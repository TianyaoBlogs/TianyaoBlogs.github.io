<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianyaoblogs.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="TianyaoBlogs">
<meta property="og:url" content="https://tianyaoblogs.github.io/index.html">
<meta property="og:site_name" content="TianyaoBlogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://tianyaoblogs.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>TianyaoBlogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TianyaoBlogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/11/10/phys5120-hw4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/11/10/phys5120-hw4/" class="post-title-link" itemprop="url">phys-5120-hw4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-11-10 21:00:00 / 修改时间：00:48:33" itemprop="dateCreated datePublished" datetime="2025-11-10T21:00:00+08:00">2025-11-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/hw/" itemprop="url" rel="index"><span itemprop="name">hw</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          这是一篇加密文章
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/11/10/phys5120-hw4/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/11/08/GCL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/11/08/GCL/" class="post-title-link" itemprop="url">Code Review - Graph Contrastive Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-11-08 21:00:00" itemprop="dateCreated datePublished" datetime="2025-11-08T21:00:00+08:00">2025-11-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-11-09 00:47:00" itemprop="dateModified" datetime="2025-11-09T00:47:00+08:00">2025-11-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          这是一篇加密文章
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/11/08/GCL/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/11/08/papers_11_8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/11/08/papers_11_8/" class="post-title-link" itemprop="url">Paper Overview</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-11-08 21:00:00 / 修改时间：16:32:43" itemprop="dateCreated datePublished" datetime="2025-11-08T21:00:00+08:00">2025-11-08</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/QM9-papers-collections/" itemprop="url" rel="index"><span itemprop="name">QM9 papers collections</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          这是一篇加密文章
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/11/08/papers_11_8/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/11/07/5120c9-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/11/07/5120c9-2/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W9-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-11-07 22:00:00 / 修改时间：19:01:30" itemprop="dateCreated datePublished" datetime="2025-11-07T22:00:00+08:00">2025-11-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - 计算能源材料和电子结构模拟 Lecture</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<p>在前面，我们看到了“理想”的<strong>无轨道 DFT
(OF-DFT)</strong>，它试图将能量 <span class="math inline">\(E\)</span>
直接写成密度 <span class="math inline">\(\rho\)</span> 的泛函 <span
class="math inline">\(E[\rho]\)</span>。但它失败了，因为我们无法找到一个足够精确的<strong>动能泛函
<span class="math inline">\(T[\rho]\)</span></strong>。</p>
<p>接下来介绍的是<strong>Kohn-Sham (KS)
DFT</strong>，它采用了一种“妥协”方案，彻底绕开了这个难题。</p>
<h3 id="概念回顾无轨道-dft-的困境">1. 概念回顾：无轨道 DFT 的困境</h3>
<p>首先回顾我们已知的（也是 OF-DFT 的）总能量泛函：</p>
<p><span class="math display">\[E[\rho] = \underbrace{T[\rho]}_{\text{①
动能}} + \int V_{ext}(\vec{r})\rho(\vec{r})d\vec{r} +
\underbrace{\frac{e^2}{2}\iint d\vec{r}d\vec{r}&#39;
\frac{\rho(\vec{r})\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|}}_{\text{Hartree
能量}} + \underbrace{E_{xc}[\rho]}_{\text{② 交换关联能}}\]</span></p>
<ul>
<li><strong>问题所在：</strong>
<ul>
<li><strong>① <span class="math inline">\(T[\rho]\)</span>
(动能)：</strong>
这是最大的难题。动能是一个与波函数<strong>曲率</strong> (curvature)
相关的量子力学量 (<span class="math inline">\(\hat{T} \propto
\nabla^2\)</span>)。我们无法简单地通过密度 <span
class="math inline">\(\rho\)</span>（一个标量场）来精确描述它。前面推导的
<span class="math inline">\(T[\rho] \propto \int \rho^{5/3}
d\vec{r}\)</span> 只是一个非常粗糙的近似。</li>
<li><strong>② <span class="math inline">\(E_{xc}[\rho]\)</span>
(交换关联能)：</strong>
这是第二个难题。它包含了所有复杂的、非经典的电子-电子相互作用（泡利不相容原理的交换效应、电子相互躲避的关联效应）。</li>
</ul></li>
</ul>
<p>KS-DFT 的目标就是<strong>同时解决这两个问题</strong>。</p>
<h3 id="核心概念kohn-sham-辅助系统">2. 核心概念：Kohn-Sham 辅助系统</h3>
<p>Kohn-Sham 的核心思想是：“我承认 <span
class="math inline">\(T[\rho]\)</span>
太难了，所以我干脆<strong>不去近似它</strong>。”</p>
<blockquote>
<p><strong>Kohn-Sham 的核心假设：</strong>
对于<em>任何</em>一个我们关心的、有相互作用的<strong>真实系统</strong>（其基态密度为
<span
class="math inline">\(\rho_{true}\)</span>），我们<em>总能</em>构建一个<strong>虚构的
(fictitious)</strong>、<strong>无相互作用的</strong>辅助系统，这个系统被设计为<strong>恰好具有与真实系统完全相同的基态密度
<span class="math inline">\(\rho(\vec{r}) =
\rho_{true}(\vec{r})\)</span></strong>。</p>
</blockquote>
<p>这个想法彻底改变了规则。</p>
<ul>
<li><strong>公式 1：Kohn-Sham 哈密顿量 <span
class="math inline">\(\hat{H}_{KS}\)</span></strong>
<ul>
<li><code>Ĥ_KS = -ħ²/2m ∇² + V_KS(r)</code></li>
<li>这是一个描述 <span class="math inline">\(N\)</span>
个<strong>无相互作用</strong>电子的哈密顿量，它们都在一个共同的<strong>有效势
<span class="math inline">\(V_{KS}(\vec{r})\)</span></strong>
中运动。</li>
<li><strong>关键点：</strong>
这个哈密顿量中<strong>没有</strong>电子-电子相互作用项（如 <span
class="math inline">\(e^2/|\vec{r}_i -
\vec{r}_j|\)</span>）。这使得它在数学上变得极其简单。</li>
</ul></li>
<li><strong>公式 2：Kohn-Sham 波函数 <span
class="math inline">\(\Phi\)</span></strong>
<ul>
<li>一个斯莱特行列式：<code>Φ = 1/√N! | ... |</code></li>
<li><strong>详细解释：</strong> 由于 <span
class="math inline">\(\hat{H}_{KS}\)</span> 是 <span
class="math inline">\(N\)</span> 个无相互作用粒子的哈密顿量（<span
class="math inline">\(\hat{H}_{KS} = \sum_i
\hat{h}_i\)</span>），它的基态波函数 <span
class="math inline">\(\Phi\)</span>
可以被<strong>精确地</strong>写成一个由 <span
class="math inline">\(N\)</span> 个最低能量的单电子<strong>Kohn-Sham
轨道 <span class="math inline">\(\phi_i\)</span></strong>
构成的<strong>斯莱特行列式 (Slater Determinant)</strong>。</li>
<li>这些轨道 <span class="math inline">\(\phi_i\)</span>
是单粒子薛定谔方程（即 <strong>Kohn-Sham 方程</strong>）的解： <span
class="math display">\[\left( -\frac{\hbar^2}{2m}\nabla^2 +
V_{KS}(\vec{r}) \right) \phi_i(\vec{r}) = \epsilon_i
\phi_i(\vec{r})\]</span></li>
</ul></li>
</ul>
<h3 id="关键公式密度和动能">3. 关键公式：密度和动能</h3>
<p>现在，我们来看看这个“虚构系统”是如何帮我们解决问题的。</p>
<ul>
<li><strong>公式 3：KS 系统的电子密度 <span
class="math inline">\(\rho(\vec{r})\)</span></strong>
<ul>
<li><code>ρ(r) = ⟨Φ| Σ δ(r-ri) |Φ⟩ = Σ_&#123;i=1&#125;^N |φ_i|²</code></li>
<li><strong>详细解释：</strong>
<ul>
<li><code>⟨Φ| Σ δ(r-ri) |Φ⟩</code>：这是密度 <span
class="math inline">\(\rho(\vec{r})\)</span> 的标准量子力学定义，即“在
<span class="math inline">\(\vec{r}\)</span>
处找到<em>任意</em>一个电子的概率”。</li>
<li><code>= Σ_&#123;i=1&#125;^N |φ_i|²</code>：这是最关键的简化。对于一个斯莱特行列式波函数
<span
class="math inline">\(\Phi\)</span>，总的电子密度<strong>精确地等于</strong>所有被占据的
KS 轨道的概率密度之和。</li>
</ul></li>
<li><strong>KS 假设的重申：</strong> 我们假设存在一个 <span
class="math inline">\(V_{KS}(\vec{r})\)</span>，使得这个 <span
class="math inline">\(\rho(\vec{r})\)</span>
<strong>恒等于</strong>我们想研究的<strong>真实系统</strong>的密度 <span
class="math inline">\(\rho_{true}(\vec{r})\)</span>。</li>
</ul></li>
<li><strong>公式 4：无相互作用动能 <span
class="math inline">\(T_s[\rho]\)</span></strong>
<ul>
<li><code>Ts[ρ] = ⟨Φ| Σ -ħ²/2m ∇_i² |Φ⟩ = Σ_&#123;i=1&#125;^N ⟨φ_i| -ħ²/2m ∇² |φ_i⟩</code></li>
<li><strong>详细解释：</strong>
<ul>
<li><strong>这就是 KS 方案的全部意义！</strong></li>
<li>我们想知道这个虚构系统的总动能 <span
class="math inline">\(T_s\)</span> (s 代表 ‘single-particle’ 或
‘non-interacting’)。</li>
<li><span class="math inline">\(T_s = \langle \Phi | \hat{T} | \Phi
\rangle\)</span>。</li>
<li>与密度一样，由于 <span class="math inline">\(\Phi\)</span>
是斯莱特行列式，<span class="math inline">\(\hat{T}\)</span>
是单体算符，总动能<strong>精确地等于</strong>所有被占据的 KS
轨道的动能之和。</li>
</ul></li>
<li><strong>结论：</strong> 我们成功地<strong>避免了对 <span
class="math inline">\(T[\rho]\)</span>
的近似</strong>。我们现在不去近似它，而是通过求解 KS 轨道 <span
class="math inline">\(\phi_i\)</span>
来<strong>精确计算</strong>动能的主要部分 <span
class="math inline">\(T_s\)</span>。</li>
</ul></li>
</ul>
<h3 id="证明t_srho-与-t_truerho-的关系">4. “证明”：<span
class="math inline">\(T_s[\rho]\)</span> 与 <span
class="math inline">\(T_{true}[\rho]\)</span> 的关系</h3>
<p>提出了一个问题和一个潦草的证明：</p>
<ul>
<li><p><strong>问题：</strong> <code>Ts[ρ] ≤ T_true[ρ] ?</code></p>
<ul>
<li>我们通过轨道计算的<strong>无相互作用动能 <span
class="math inline">\(T_s\)</span></strong>
与<strong>真实系统</strong>的<strong>真正动能 <span
class="math inline">\(T_{true}\)</span></strong> 相比，哪个更大？</li>
</ul></li>
<li><p><strong>答案：<span class="math inline">\(T_s[\rho] \le
T_{true}[\rho]\)</span> 恒成立。</strong></p></li>
<li><p><strong>证明</strong>：</p>
<ul>
<li><code>Proof: ⟨Φ_ks| ...</code>
这里的字迹非常潦草，似乎是想用变分原理来论证，但写得并不清楚。</li>
</ul></li>
<li><p><strong>一个更清晰的、概念性的证明：</strong></p>
<ol type="1">
<li><span class="math inline">\(T_s[\rho]\)</span>
是一个<strong>无相互作用</strong>系统在密度为 <span
class="math inline">\(\rho\)</span> 时的基态动能。</li>
<li><span class="math inline">\(T_{true}[\rho]\)</span>
是一个<strong>有相互作用</strong>系统在密度为 <span
class="math inline">\(\rho\)</span> 时的基态动能。</li>
<li>在真实系统中，电子不仅受 <span
class="math inline">\(V_{ext}\)</span> 束缚，它们还必须<strong>相互排斥
(correlation)</strong>。为了“躲避”彼此，它们的波函数必须变得更加“弯曲”或“扭动”。</li>
<li>在量子力学中，<strong>动能 <span class="math inline">\(\propto \int
|\nabla \Psi|^2\)</span></strong>，它衡量的是波函数的“弯曲程度”。</li>
<li>真实电子为了相互躲避而增加的额外“弯曲”，导致了<strong>额外的动能</strong>。</li>
<li>因此，对于同一个密度 <span
class="math inline">\(\rho\)</span>，真实系统的动能 <span
class="math inline">\(T_{true}\)</span>
必然<strong>大于或等于</strong>那个不需要考虑相互躲避的、虚构的无相互作用系统的动能
<span class="math inline">\(T_s\)</span>。</li>
</ol></li>
<li><p><strong>这引出了最终的 KS-DFT 能量划分：</strong></p>
<ol type="1">
<li><p>真实动能 <span class="math inline">\(T_{true}\)</span>
被拆分为：<span class="math inline">\(T_{true}[\rho] = T_s[\rho] +
T_c[\rho]\)</span></p>
<ul>
<li><span
class="math inline">\(T_s[\rho]\)</span>：无相互作用动能（<strong>我们用轨道精确计算</strong>）。</li>
<li><span
class="math inline">\(T_c[\rho]\)</span>：<strong>动能的关联部分</strong>（<span
class="math inline">\(T_{true} - T_s\)</span>，这是个未知的小量）。</li>
</ul></li>
<li><p>现在，Kohn-Sham 方案将所有未知项——<span
class="math inline">\(T_c[\rho]\)</span> 和 <span
class="math inline">\(E_{xc}[\rho]\)</span>（来自 OF-DFT）——
<strong>全部打包</strong> 进一个<strong>新的</strong>交换关联泛函 <span
class="math inline">\(E_{xc}^{KS}[\rho]\)</span> 中。</p></li>
<li><p><strong>Kohn-Sham 总能量泛函：</strong> <span
class="math display">\[E[\rho] = \mathbf{T_s[\{\phi_i\}]} + \int
V_{ext}(\vec{r})\rho(\vec{r})d\vec{r} + E_H[\rho] +
\mathbf{E_{xc}^{KS}[\rho]}\]</span></p></li>
</ol></li>
</ul>
<p><strong>总结：</strong> KS-DFT
的赌注是：通过轨道<strong>精确计算</strong> <span
class="math inline">\(T_s\)</span>，剩下的那个包含 <span
class="math inline">\(T_c\)</span> 的<strong>新 <span
class="math inline">\(E_{xc}^{KS}[\rho]\)</span></strong>，会比原来那个包含整个
<span class="math inline">\(T_{true}\)</span> 的 OF-DFT
泛函<strong>容易得多</strong>。</p>
<p>历史证明，这个赌注赢了。</p>
<p><strong>现代 DFT 计算的核心——Kohn-Sham 方程</strong>。</p>
<p>它回答了两个终极问题： 1. 我们把所有“脏活累活”都塞进 <span
class="math inline">\(E_{xc}\)</span> (交换关联能) 里，那这个 <span
class="math inline">\(E_{xc}\)</span> <strong>到底是什么</strong>？ 2.
我们如何<strong>求解</strong>这个 KS 系统来找到轨道 <span
class="math inline">\(\phi_i\)</span> 和密度 <span
class="math inline">\(\rho\)</span>？</p>
<h3 id="概念e_xc-的正式定义-左侧">1. 概念：<span
class="math inline">\(E_{xc}\)</span> 的正式定义 (左侧)</h3>
<p>接下来给出了 Kohn-Sham 交换关联能 <span
class="math inline">\(E_{xc}\)</span>
的<strong>精确定义</strong>。它是一个“垃圾堆”泛函，包含了所有真实系统与虚构
KS 系统之间的差异。</p>
<ul>
<li><strong><span class="math inline">\(E_{xc}[\rho] = (T_{true}[\rho] -
T_s[\rho]) + (\langle \Psi_{true} | \hat{V}_{ee} | \Psi_{true} \rangle -
E_{Hartree}[\rho])\)</span></strong>
<ul>
<li><strong>第一部分：<span class="math inline">\((T_{true}[\rho] -
T_s[\rho])\)</span></strong>
<ul>
<li>这是<strong>动能的关联部分 <span
class="math inline">\(T_c\)</span></strong>。</li>
<li><span class="math inline">\(T_{true}\)</span>
是真实系统的（未知的）总动能。</li>
<li><span class="math inline">\(T_s\)</span> 是我们用 KS
轨道<strong>精确计算</strong>的无相互作用动能。</li>
<li><span class="math inline">\(E_{xc}\)</span>
必须包含这个动能差。</li>
</ul></li>
<li><strong>第二部分：<span class="math inline">\((\langle \Psi_{true} |
\hat{V}_{ee} | \Psi_{true} \rangle -
E_{Hartree}[\rho])\)</span></strong>
<ul>
<li>这是<strong>势能的交换与关联部分</strong>。</li>
<li><span class="math inline">\(\langle \Psi_{true} | \hat{V}_{ee} |
\Psi_{true} \rangle\)</span> 是真实系统中电子-电子相互作用 <span
class="math inline">\(\hat{V}_{ee}\)</span>
的完整（未知的）期望值。</li>
<li><span class="math inline">\(E_{Hartree}[\rho]\)</span>
是我们<strong>可以精确计算</strong>的经典静电排斥能（哈特里能量）。</li>
<li><span class="math inline">\(E_{xc}\)</span>
包含了真实相互作用与经典排斥之间的<strong>差值</strong>，这部分就是纯粹的量子效应（交换
+ 关联）。</li>
</ul></li>
</ul></li>
</ul>
<p><strong>一句话总结：<span class="math inline">\(E_{xc}\)</span>
包含了所有我们不知道的动能和势能的复杂量子效应。</strong></p>
<h3 id="核心公式kohn-sham-总能量">2. 核心公式：Kohn-Sham 总能量</h3>
<p>有了 <span class="math inline">\(E_{xc}\)</span> 的定义，Kohn-Sham
的<strong>总能量泛函</strong>现在可以被<strong>精确地</strong>（在形式上）写为：</p>
<p><span class="math display">\[E_{KS}[\rho] = \mathbf{T_s[\{\phi_i\}]}
+ \int d\vec{r} V_{ext}(\vec{r})\rho(\vec{r}) + \frac{1}{2}\iint
d\vec{r}d\vec{r}&#39;
\frac{\rho(\vec{r})\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|} +
\mathbf{E_{xc}[\rho]}\]</span></p>
<ul>
<li><strong><span
class="math inline">\(T_s[\{\phi_i\}]\)</span></strong>：无相互作用动能（<strong>通过轨道精确计算</strong>）。</li>
<li><strong><span class="math inline">\(\int V_{ext}
\rho\)</span></strong>：外势能（例如原子核吸引，已知）。</li>
<li><strong><span class="math inline">\(\frac{1}{2}\iint
...\)</span></strong>：哈特里能量（经典静电排斥，已知）。</li>
<li><strong><span
class="math inline">\(E_{xc}[\rho]\)</span></strong>：交换关联能（<strong>这是唯一需要近似的部分！</strong>）。</li>
</ul>
<p><strong>这是整个 KS-DFT 的基石。</strong> 我们成功地将一个无法解决的
<span class="math inline">\(T_{true}\)</span>
问题，转化为了一个<strong>可以被近似</strong>的 <span
class="math inline">\(E_{xc}\)</span> 问题。</p>
<h3 id="推导kohn-sham-方程">3. 推导：Kohn-Sham 方程</h3>
<p>我们如何找到使 <span class="math inline">\(E_{KS}[\rho]\)</span>
最小的轨道 <span class="math inline">\(\phi_i\)</span> 呢？
答案是使用<strong>变分法</strong>：我们对总能量 <span
class="math inline">\(E_{KS}\)</span> 求关于轨道 <span
class="math inline">\(\phi_i^*\)</span> 的泛函导数，并令其为零。</p>
<ul>
<li><strong><span class="math inline">\(\frac{\delta}{\delta\phi_i^*}
\left( E_{KS}[\rho] - \sum_j \epsilon_j (\int |\phi_j|^2 d\vec{r} - 1)
\right) = 0\)</span></strong>
<ul>
<li>这就是带有<strong>约束条件</strong>（每个轨道必须归一化）的能量最小化。</li>
<li><span class="math inline">\(\epsilon_j\)</span>
是拉格朗日乘子。</li>
</ul></li>
<li><strong>对 <span class="math inline">\(E_{KS}\)</span>
的每一项求导：</strong>
<ul>
<li><span class="math inline">\(\frac{\delta
T_s}{\delta\phi_i^*}\)</span> <span
class="math inline">\(\rightarrow\)</span> <span
class="math inline">\(-\frac{\hbar^2}{2m}\nabla^2
\phi_i(\vec{r})\)</span></li>
<li><span class="math inline">\(\frac{\delta
E_{V_{ext}}}{\delta\phi_i^*}\)</span> <span
class="math inline">\(\rightarrow\)</span> <span
class="math inline">\(V_{ext}(\vec{r}) \phi_i(\vec{r})\)</span></li>
<li><span class="math inline">\(\frac{\delta
E_{Hartree}}{\delta\phi_i^*}\)</span> <span
class="math inline">\(\rightarrow\)</span> <span
class="math inline">\(\left( \int
\frac{\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|} d\vec{r}&#39; \right)
\phi_i(\vec{r})\)</span> (即 <span
class="math inline">\(V_H(\vec{r})\phi_i(\vec{r})\)</span>)</li>
<li><span class="math inline">\(\frac{\delta
E_{xc}}{\delta\phi_i^*}\)</span> <span
class="math inline">\(\rightarrow\)</span> <span
class="math inline">\(\left( \frac{\delta E_{xc}}{\delta\rho} \right)
\phi_i(\vec{r})\)</span> (即 <span
class="math inline">\(V_{xc}(\vec{r})\phi_i(\vec{r})\)</span>)</li>
<li><span class="math inline">\(\frac{\delta
(\text{约束项})}{\delta\phi_i^*}\)</span> <span
class="math inline">\(\rightarrow\)</span> <span
class="math inline">\(\epsilon_i \phi_i(\vec{r})\)</span></li>
</ul></li>
<li><strong>把所有项合并，我们就得到了最终的 Kohn-Sham 方程：</strong>
<span class="math display">\[\left[ -\frac{\hbar^2}{2m}\nabla^2 +
V_{ext}(\vec{r}) + \int
\frac{\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|} d\vec{r}&#39; +
V_{xc}(\vec{r}) \right] \phi_i(\vec{r}) = \epsilon_i
\phi_i(\vec{r})\]</span>
<ul>
<li><span
class="math inline">\(\epsilon_i\)</span>（拉格朗日乘子）的物理意义是
<strong>Kohn-Sham 轨道 <span class="math inline">\(\phi_i\)</span>
的能量</strong>。</li>
</ul></li>
</ul>
<h3 id="结论v_ks-的最终形式">4. 结论：<span
class="math inline">\(V_{KS}\)</span> 的最终形式</h3>
<p><strong>Kohn-Sham
方程</strong>本质上是一个<strong>单粒子薛定谔方程</strong>
<code>Ĥ_KS φ_i = ε_i φ_i</code>。</p>
<p>通过上面的推导，我们明确地找到了这个虚构的 <strong>Kohn-Sham 势 <span
class="math inline">\(V_{KS}\)</span></strong> 到底是什么：</p>
<p><span class="math display">\[V_{KS}(\vec{r}) = V_{ext}(\vec{r}) +
V_H(\vec{r}) + V_{xc}(\vec{r})\]</span></p>
<p><span class="math display">\[V_{KS}(\vec{r}) =
\underbrace{V_{ext}(\vec{r})}_{\text{原子核势}} + \underbrace{\int
\frac{\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|}
d\vec{r}&#39;}_{\text{哈特里势 (经典排斥)}} + \underbrace{\frac{\delta
E_{xc}[\rho]}{\delta\rho}}_{\text{交换关联势 (量子效应)}}\]</span></p>
<h3 id="最终总结">最终总结</h3>
<p>这个方程完美地闭合了整个理论： * 我们想求解 <span
class="math inline">\(V_{KS}\)</span> 中的 <span
class="math inline">\(\phi_i\)</span>。 * 但 <span
class="math inline">\(V_{KS}\)</span> 本身又依赖于 <span
class="math inline">\(V_H\)</span> 和 <span
class="math inline">\(V_{xc}\)</span>。 * <span
class="math inline">\(V_H\)</span> 和 <span
class="math inline">\(V_{xc}\)</span> 又依赖于 <span
class="math inline">\(\rho\)</span>（密度）。 * 而 <span
class="math inline">\(\rho\)</span> 又是由 <span
class="math inline">\(\phi_i\)</span>（<span class="math inline">\(\rho
= \sum |\phi_i|^2\)</span>）构成的。</p>
<p>这是一个<strong>“鸡生蛋，蛋生鸡”</strong>的问题。
在实践中，我们必须通过<strong>自洽迭代 (self-consistent loop)</strong>
来求解： 1. <strong>猜测</strong>一个初始密度 <span
class="math inline">\(\rho_{in}\)</span>。 2. 用 <span
class="math inline">\(\rho_{in}\)</span> 计算 <span
class="math inline">\(V_H\)</span> 和 <span
class="math inline">\(V_{xc}\)</span>，得到 <span
class="math inline">\(V_{KS}\)</span>。 3. 求解 KS 方程 <span
class="math inline">\(\hat{H}_{KS} \phi_i = \epsilon_i \phi_i\)</span>
得到新的轨道 <span class="math inline">\(\phi_i\)</span>。 4. 用新的
<span class="math inline">\(\phi_i\)</span>
计算出<strong>新的密度</strong> <span class="math inline">\(\rho_{out} =
\sum |\phi_i|^2\)</span>。 5. 比较 <span
class="math inline">\(\rho_{out}\)</span> 和 <span
class="math inline">\(\rho_{in}\)</span>。如果它们不一致，就混合新旧密度，重复步骤
2。 6. 循环往复，直到 <span class="math inline">\(\rho_{out} =
\rho_{in}\)</span>，达到<strong>自洽</strong>，计算完成。</p>
<p>在前面，我们推导出了一切都取决于一个<strong>未知的</strong>、需要<strong>近似</strong>的能量项：<strong>交换关联泛函
<span class="math inline">\(E_{xc}[\rho]\)</span></strong>。</p>
<p>接下来展示的就是在实践中<strong>如何近似 <span
class="math inline">\(E_{xc}[\rho]\)</span></strong>
的两种最基本、最重要的方法：<strong>LDA</strong> 和
<strong>GGA</strong>。</p>
<h3 id="局域密度近似-local-density-approximation-lda">1. 局域密度近似
(Local Density Approximation, LDA)</h3>
<p>这是对 <span class="math inline">\(E_{xc}[\rho]\)</span>
最简单、最基础的近似。</p>
<ul>
<li><strong>图示：</strong>
白板左上角画了一个图：一个真实的、密度不均匀的分子（或固体），但在某一点
<span class="math inline">\(\vec{r}\)</span>
处，我们“假装”它周围是一片均匀的电子海洋（自由电子气）。</li>
<li><strong>核心思想 (LDA)：</strong> 系统在 <span
class="math inline">\(\vec{r}\)</span>
处的交换关联能量密度，<strong>仅仅</strong>取决于<strong>该点 <span
class="math inline">\(\vec{r}\)</span> 处的电子密度 <span
class="math inline">\(\rho(\vec{r})\)</span></strong>。
我们假设它与具有相同密度的<strong>均匀电子气 (free electron
gas)</strong> 的交换关联能量密度 <span
class="math inline">\(\epsilon_{xc}^{unif}(\rho)\)</span>
完全相同。</li>
<li><strong>公式 1：<span
class="math inline">\(E_{xc}^{LDA}[\rho]\)</span></strong> <span
class="math display">\[E_{xc}^{LDA}[\rho] = \int d\vec{r} \rho(\vec{r})
\epsilon_{xc}^{unif}(\rho(\vec{r}))\]</span>
<ul>
<li><strong><span
class="math inline">\(\epsilon_{xc}^{unif}(\rho)\)</span></strong>：这就是我们在<strong>第
3
张白板</strong>上推导过的<strong>均匀电子气</strong>的（单粒子）交换关联能量。这是一个已知的、关于
<span class="math inline">\(\rho\)</span>
的函数（通过量子蒙特卡洛等方法可以精确算出）。</li>
<li><strong><span
class="math inline">\(\rho(\vec{r})\)</span></strong>：该点的电子密度。</li>
<li><strong>含义：</strong> 我们在空间中逐点计算该点的 <span
class="math inline">\(\rho\)</span> 对应的 <span
class="math inline">\(\epsilon_{xc}\)</span>，然后乘以该点的密度 <span
class="math inline">\(\rho\)</span>，最后在整个空间积分（求和）。</li>
</ul></li>
<li><strong>公式 2：<span class="math inline">\(V_{xc}^{LDA}\)</span>
(交换关联势)</strong> <span class="math display">\[V_{xc} = \frac{\delta
E_{xc}}{\delta \rho} = \epsilon_{xc}(\rho, \vec{r}) + \rho(\vec{r})
\frac{\partial \epsilon_{xc}}{\partial \rho}\]</span>
<ul>
<li>这是将 <span class="math inline">\(E_{xc}^{LDA}\)</span>
代入我们在<strong>第 5 张白板</strong>上定义的 <span
class="math inline">\(V_{xc} = \delta E_{xc} / \delta \rho\)</span>
中，通过链式法则推导出来的结果。</li>
</ul></li>
<li><strong>“Perdew-Zunger (1981)”</strong>
<ul>
<li>这是一个<strong>具体的 LDA 泛函</strong>的名称。Perdew 和 Zunger 在
1981 年利用高精度的均匀电子气数据（来自 Ceperley-Alder 的 QMC
计算），拟合出了一套非常精确和实用的 <span
class="math inline">\(\epsilon_{xc}^{unif}(\rho)\)</span>
参数化公式。这至今仍是 LDA 计算的标准。</li>
</ul></li>
</ul>
<h3 id="广义梯度近似-generalized-gradient-approximation-gga">2.
广义梯度近似 (Generalized-Gradient Approximation, GGA)</h3>
<p>LDA 假设在 <span class="math inline">\(\vec{r}\)</span>
处的能量只取决于 <span
class="math inline">\(\rho(\vec{r})\)</span>，这是一个<strong>非常强</strong>（且通常不正确）的假设。真实系统（如分子）的密度变化非常快。</p>
<ul>
<li><strong>核心思想 (GGA)：</strong>
一个更智能的近似，不仅要考虑<strong>该点的密度 <span
class="math inline">\(\rho(\vec{r})\)</span></strong>，还应该考虑<strong>该点密度的变化率</strong>，即<strong>梯度的模
<span class="math inline">\(|\nabla\rho|\)</span></strong>。
<ul>
<li>如果 <span class="math inline">\(|\nabla\rho|\)</span>
很大，说明密度变化剧烈（如在分子键合区域或原子边缘），<span
class="math inline">\(\epsilon_{xc}\)</span> 应该与 LDA 不同。</li>
</ul></li>
<li><strong>公式 3：<span
class="math inline">\(E_{xc}^{GGA}[\rho^\uparrow,
\rho^\downarrow]\)</span></strong> <span
class="math display">\[E_{xc}^{GGA}[\rho^\uparrow, \rho^\downarrow] =
\int d\vec{r} \rho(\vec{r}) \epsilon_{xc}(\rho^\uparrow,
\rho^\downarrow, |\nabla\rho^\uparrow|, |\nabla\rho^\downarrow|,
\vec{r})\]</span>
<ul>
<li><strong><span class="math inline">\(\rho^\uparrow,
\rho^\downarrow\)</span></strong>：自旋向上和自旋向下的电子密度（更完整的表述）。</li>
<li><strong><span class="math inline">\(|\nabla\rho^\uparrow|,
|\nabla\rho^\downarrow|\)</span></strong>：<strong>新加入的项！</strong>
密度梯度的信息被包含了进来。</li>
<li><strong><span
class="math inline">\(\epsilon_{xc}(...)\)</span></strong>：现在是一个更复杂的函数，它不仅是
<span class="math inline">\(\rho\)</span> 的函数，还是 <span
class="math inline">\(|\nabla\rho|\)</span> 的函数。</li>
</ul></li>
<li><strong>“PBE”, “BLYP” …</strong>
<ul>
<li>这些都是<strong>具体的 GGA 泛函</strong>的名称。它们就像 <span
class="math inline">\(\epsilon_{xc}\)</span> 的不同“配方”。</li>
<li><strong>BLYP</strong> = Becke (交换) + Lee, Yang, Parr (关联)。</li>
<li><strong>PBE</strong> = Perdew, Burke, Ernzerhof。</li>
<li>PBE 和 BLYP 是化学和材料科学中<strong>最常用、最成功</strong>的 GGA
泛函之二。</li>
</ul></li>
</ul>
<p>接下来介绍的是“Jacob’s Ladder”（DFT
近似的雅各天梯）的更高几层：<strong>meta-GGA</strong> 和 <strong>Hybrid
functionals (混合泛函)</strong>。</p>
<h3 id="密度泛函近似-dfa">1. 密度泛函近似 (DFA)</h3>
<ul>
<li><strong>DFA (Density Functional Approximation):</strong>
<ul>
<li>这是一个总称，泛指我们对 <span
class="math inline">\(E_{xc}[\rho]\)</span>（交换关联泛函）所做的<strong>所有近似</strong>，包括
LDA, GGA 等。</li>
</ul></li>
</ul>
<h3 id="meta-gga">2. Meta-GGA</h3>
<p>这是超越 GGA 的“天梯”的下一级。</p>
<ul>
<li><strong><span class="math inline">\(\Delta\)</span> meta
GGA:</strong>
<ul>
<li><strong>核心思想：</strong> GGA 只用了 <span
class="math inline">\(\rho\)</span>（密度）和 <span
class="math inline">\(|\nabla\rho|\)</span>（密度梯度）。为了获得更高精度，meta-GGA
引入了<strong>第三种信息</strong>。</li>
<li><strong>成分：<span class="math inline">\(\rho, |\nabla\rho|,
\nabla^2\rho? |\nabla\phi_i|^2\)</span></strong>
<ul>
<li><span class="math inline">\(\rho\)</span> (密度)</li>
<li><span class="math inline">\(|\nabla\rho|\)</span> (密度梯度)</li>
<li><code>∇²ρ?</code> (密度的拉普拉斯，一种可能的成分)</li>
<li><strong><span
class="math inline">\(|\nabla\phi_i|^2\)</span></strong> (Kohn-Sham
轨道的动能密度 <span class="math inline">\(\tau\)</span>)：这是现代
meta-GGA
泛函中<strong>最关键</strong>的成分。它使得泛函<strong>间接地</strong>依赖于轨道，从而能“感知”更复杂的电子结构信息（例如，区分是单键、双键还是孤对电子）。</li>
</ul></li>
</ul></li>
<li><strong>“SCAN”</strong>
<ul>
<li>这是一个<strong>具体的 meta-GGA
泛函</strong>的名称，是目前最流行、最成功的 meta-GGA 之一。</li>
</ul></li>
</ul>
<h3 id="一个关键问题自相互作用误差-sie">3. 一个关键问题：自相互作用误差
(SIE)</h3>
<p>为什么我们需要更高级的泛函？因为 LDA 和 GGA
有一个<strong>根本性的缺陷</strong>。</p>
<ul>
<li><strong>“self-interaction error” (自相互作用误差, SIE):</strong>
<ul>
<li><strong>问题来源：</strong> 在 DFT 中，一个电子的密度 <span
class="math inline">\(\rho\)</span> 是其自身的总密度 <span
class="math inline">\(\rho\)</span> 的一部分。在计算哈特里能量 <span
class="math inline">\(E_H[\rho] \propto \iint \rho \rho&#39;
...\)</span>
时，这个电子<strong>错误地与它自己</strong>产生了静电排斥。</li>
<li><strong>本应：</strong> <span
class="math inline">\(E_{xc}\)</span>（交换能）应该<strong>完美地抵消</strong>掉这个虚假的自排斥。</li>
<li><strong>现实：</strong> <strong>LDA 和 GGA
泛函都未能</strong>完美抵消它。</li>
</ul></li>
<li><strong>“charge too delocalized” (电荷过度离域):</strong>
<ul>
<li><strong>后果：</strong>
由于电子错误地“排斥”自己，系统为了降低能量，会倾向于将电子“涂抹”或“离域”
(delocalize) 到尽可能大的空间中，以减小这种虚假的自排斥。</li>
<li><strong>图示：</strong> 白板上的山峰（或势垒）图示说明：SIE
导致电荷在过渡态的局域化（电子集中在某处）变得困难，因此 LDA/GGA
总是<strong>低估</strong>化学反应的<strong>能垒</strong>。</li>
<li><strong>“Cl- — nH₂O”</strong>
<ul>
<li>这是一个具体例子：一个 Cl⁻ 离子被水分子包围。LDA/GGA 会错误地将 Cl⁻
上的负电荷“泄露”或“离域”到周围的水分子上，从而导致错误的体系结构和能量。</li>
</ul></li>
</ul></li>
</ul>
<h3 id="解决方案混合泛函-hybrid-functional">4. 解决方案：混合泛函
(Hybrid Functional)</h3>
<p>这是“天梯”的第四级，是目前在化学计算中<strong>最标准、最常用</strong>的高精度方法。</p>
<ul>
<li><strong><span class="math inline">\(\Delta\)</span> Hybrid
functional:</strong>
<ul>
<li><strong>核心思想：</strong> 如何修复 SIE？
<ul>
<li>我们知道，在 <strong>Hartree-Fock (HF) 理论</strong>中，其“交换能”
<span class="math inline">\(E_x^{HF}\)</span>
是通过轨道<strong>精确计算</strong>的，并且它<strong>完美地</strong>抵消了自相互作用。</li>
<li><strong>混合 (Hybrid) 思想：</strong> 让我们把 DFT (如 PBE)
的交换项拿掉一部分，<strong>替换</strong>为同一比例的“精确”的 HF
交换项。</li>
</ul></li>
</ul></li>
<li><strong>“PBE0” (一个具体的混合泛函名称):</strong>
<ul>
<li>这是最著名的混合泛函之一。</li>
<li><strong><span class="math inline">\(E_{xc} = \frac{1}{4} E_x^{HF} +
\frac{3}{4} E_x^{PBE} + E_c^{PBE}\)</span></strong></li>
<li><strong>公式分解：</strong>
<ul>
<li><strong><span class="math inline">\(\frac{1}{4}
E_x^{HF}\)</span></strong>：用 <strong>25%</strong> 的<strong>精确 HF
交换</strong>。</li>
<li><strong><span class="math inline">\(\frac{3}{4}
E_x^{PBE}\)</span></strong>：用 <strong>75%</strong> 的 <strong>PBE
(GGA) 交换</strong>。</li>
<li><strong><span class="math inline">\(E_c^{PBE}\)</span></strong>：用
<strong>100%</strong> 的 <strong>PBE (GGA) 关联</strong>。</li>
</ul></li>
<li><strong>效果：</strong> 引入 25%
的精确交换，极大地<strong>纠正</strong>了自相互作用误差
(SIE)，使其在计算能垒、带隙、分子性质等方面远比纯 GGA 准确。</li>
</ul></li>
</ul>
<p>接下来介绍了当今计算化学和材料科学中<strong>最先进、最常用</strong>的几种高级泛函。</p>
<p><strong>混合泛函 (Hybrid functional) 家族的“动物园”</strong>——
它们是如何被构建的，以及它们各自解决了什么问题。</p>
<h3 id="b3lyp-最著名的经验混合泛函">1. B3LYP (最著名的经验混合泛函)</h3>
<ul>
<li><strong><span class="math inline">\(\Delta\)</span> B3LYP:</strong>
<ul>
<li>这是<strong>最著名</strong>的混合泛函之一，特别是在量子化学领域。</li>
<li>它的构造是<strong>半经验的
(semi-empirical)</strong>，意味着它的混合参数是由拟合（fitting）一组精确的实验/基准化学数据（如分子的原子化热）而确定的。</li>
</ul></li>
<li><strong>公式：</strong> <span class="math display">\[E_{xc} =
E_x^{LDA} + a_0(E_x^{HF} - E_x^{LDA}) + a_x(E_x^{Becke} - E_x^{LDA}) +
E_c^{LDA} + a_c(E_c^{LYP} - E_c^{LDA})\]</span></li>
<li><strong>解释：</strong>
<ul>
<li>B3LYP (Becke, 3-parameter, Lee-Yang-Parr)
是一个复杂的“鸡尾酒”。</li>
<li>它混合了 <strong>LDA</strong> 的交换和关联、<strong>Hartree-Fock
(HF)</strong> 的精确交换，以及 <strong>GGA</strong> 的交换 (Becke88)
和关联 (LYP)。</li>
<li><code>a_0</code>, <code>a_x</code>, <code>a_c</code>
是三个被拟合的参数，用于确定每种成分的“配比”。</li>
</ul></li>
</ul>
<h3 id="自洽混合泛函-self-consistent-hybrid">2. 自洽混合泛函
(Self-consistent hybrid)</h3>
<ul>
<li><strong><span class="math inline">\(\Delta\)</span> Self-consistent
hybrid:</strong>
<ul>
<li><strong>思想：</strong> 与其像 PBE0（固定 25%）或
B3LYP（经验拟合）那样<em>指定</em>一个混合参数 <span
class="math inline">\(\alpha\)</span>，我们是否能从<strong>第一性原理
(ab initio)</strong> 出发，让系统<strong>自己决定</strong>应该混合多少
HF 交换？</li>
</ul></li>
<li><strong>公式：</strong>
<ul>
<li><span class="math inline">\(E_{xc} = \alpha E_x^{HF} + (1-\alpha)
E_x^{GGA} + E_c^{GGA}\)</span> (这与 PBE0 的形式相同)</li>
</ul></li>
<li><strong>关键创新：</strong>
<ul>
<li><strong><span class="math inline">\(\alpha =
\frac{1}{\epsilon_\infty}\)</span></strong></li>
<li><strong>解释：</strong> 混合比例 <span
class="math inline">\(\alpha\)</span> 被设定为材料<strong>高频介电常数
<span class="math inline">\(\epsilon_\infty\)</span>
的倒数</strong>。</li>
<li><strong>物理意义：</strong> <span
class="math inline">\(\epsilon_\infty\)</span>
描述了材料中电子<strong>屏蔽 (screening)</strong> 库仑相互作用的能力。
<ul>
<li>绝缘体/半导体：<span class="math inline">\(\epsilon_\infty\)</span>
较小（例如 2-10），<span class="math inline">\(\alpha\)</span>
较大（例如 10-50%），需要更多 HF 交换来打开带隙。</li>
<li>金属：<span class="math inline">\(\epsilon_\infty \to
\infty\)</span>，<span class="math inline">\(\alpha \to
0\)</span>，不需要 HF 交换（退化为纯 GGA）。</li>
</ul></li>
<li>这是一个<strong>自洽</strong>过程：<span
class="math inline">\(\alpha\)</span> 决定了电子结构，而电子结构（通过
<code>GW</code> 等理论）又决定了 <span
class="math inline">\(\epsilon_\infty\)</span>。</li>
</ul></li>
</ul>
<h3 id="范围分离混合泛函-range-separated-hybrid">3. 范围分离混合泛函
(Range-separated hybrid)</h3>
<ul>
<li><strong><span class="math inline">\(\Delta\)</span> Range-separated
hybrid:</strong>
<ul>
<li><strong>思想：</strong> 电子-电子排斥 <span
class="math inline">\(1/r\)</span>
在短距离和长距离下表现不同。也许我们不需要“一刀切”的混合。</li>
<li><strong>策略：</strong> 将 <span class="math inline">\(1/r\)</span>
分割为<strong>短程 (short-range, SR)</strong> 和<strong>长程
(long-range, LR)</strong>
两部分，并对它们使用<strong>不同</strong>的泛函。</li>
<li>旁边的涂鸦 <code>... ~ ...</code> 和
<code>screening</code>（屏蔽）形象地说明了这种思想：在长程，相互作用被“屏蔽”了。</li>
</ul></li>
<li><strong>公式 (以 HSE06 为例):</strong>
<ul>
<li>白板上写了 <code>HSE06'</code>（HSE 泛函的 2006
年版本），并给出了其参数：</li>
<li><strong><span class="math inline">\(\alpha = 0, \beta =
1/4\)</span></strong> (代入白板上那个复杂的通用公式)</li>
</ul></li>
<li><strong>HSE06 泛函的通俗解释：</strong>
<ul>
<li>它将 PBE (GGA) 泛函的<strong>交换部分 <span
class="math inline">\(E_x^{PBE}\)</span></strong> 进行了范围分离：</li>
<li><strong>在短程 (SR):</strong> <span class="math inline">\(E_x =
\frac{1}{4} E_x^{HF,SR} + \frac{3}{4} E_x^{PBE,SR}\)</span>
<ul>
<li>(在短距离内，它是一个 PBE0 泛函，混合了 25% 的 HF 精确交换)</li>
</ul></li>
<li><strong>在长程 (LR):</strong> <span class="math inline">\(E_x =
E_x^{PBE,LR}\)</span>
<ul>
<li>(在长距离处，它退化为 100% 的 PBE 纯 GGA 交换)</li>
</ul></li>
<li><strong>关联部分 <span class="math inline">\(E_c\)</span></strong>
始终是 100% 的 PBE 关联。</li>
</ul></li>
<li><strong>为什么这样做？</strong>
<ul>
<li><strong>物理上：</strong> 修正了长程的自相互作用误差。</li>
<li><strong>计算上：</strong> 在<strong>固体 (solid)</strong>
计算中<strong>极其重要</strong>。HF
交换的计算量非常大，尤其是在长程。HSE
泛函通过在长程<strong>关闭</strong>HF
交换（即“屏蔽”它），使得计算<strong>速度</strong>大幅提升，同时保留了混合泛函在短程（如化学键）的<strong>高精度</strong>。</li>
</ul></li>
</ul>
<h3 id="总结">总结</h3>
<ol type="1">
<li><strong>1 (HK 定理):</strong> 奠定了<strong>理论基石</strong>——
<span class="math inline">\(E = E[\rho]\)</span>。</li>
<li><strong>2 (OF-DFT):</strong> 提出了<strong>理想</strong>的无轨道
DFT，并指出了其 <span class="math inline">\(T[\rho]\)</span>
的<strong>困难</strong>。</li>
<li><strong>3 (自由电子气):</strong> <strong>推导</strong>了 <span
class="math inline">\(T[\rho]\)</span> 的<strong>最简近似</strong> <span
class="math inline">\(T \propto \int \rho^{5/3}\)</span>。</li>
<li><strong>4 (OF-DFT 求解):</strong>
展示了如何通过<strong>变分法</strong> <span class="math inline">\(\delta
E / \delta \rho = \mu\)</span> 求解 <span
class="math inline">\(\rho_0\)</span>。</li>
<li><strong>5 (Kohn-Sham):</strong> 引入了<strong>实用方案
(KS-DFT)</strong>，用“轨道” <span class="math inline">\(\phi_i\)</span>
<strong>精确计算</strong>动能 <span
class="math inline">\(T_s\)</span>，将未知项塞入 <span
class="math inline">\(E_{xc}\)</span>。</li>
<li><strong>6 (LDA/GGA):</strong> 展示了如何<strong>近似 <span
class="math inline">\(E_{xc}\)</span></strong>（“雅各天梯”的第一、二层），即
LDA（依赖 <span class="math inline">\(\rho\)</span>）和 GGA（依赖 <span
class="math inline">\(\rho, |\nabla\rho|\)</span>）。</li>
<li><strong>7 (Meta-GGA / Hybrid):</strong> 攀登天梯的更高层 (Meta-GGA,
Hybrid-PBE0)，并指出了 LDA/GGA
的<strong>根本缺陷</strong>（自相互作用误差 SIE）。</li>
<li><strong>8 (B3LYP / HSE):</strong>
展示了<strong>最先进的泛函</strong> (B3LYP, HSE06)
是如何通过经验拟合或范围分离等技巧，在精度和计算效率之间达到最佳平衡的。</li>
</ol>
<p>这完整地勾勒出了 DFT 从 1960
年代的抽象理论到当今最前沿的计算工具的整个发展脉络。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/11/07/5120c9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/11/07/5120c9/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W9-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-11-07 21:00:00 / 修改时间：17:17:51" itemprop="dateCreated datePublished" datetime="2025-11-07T21:00:00+08:00">2025-11-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - 计算能源材料和电子结构模拟 Lecture</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<p><strong>密度泛函理论 (Density Functional Theory, DFT)</strong>
的核心概念笔记，这是一种在物理和化学领域，特别是量子化学和凝聚态物理中，用于研究多电子体系电子结构的计算方法。</p>
<h3 id="密度泛函理论-density-functional-theory">密度泛函理论 (Density
Functional Theory)</h3>
<ul>
<li><strong>N-electron (N 电子体系):</strong>
<ul>
<li>波函数 (Wavefunction): <span class="math inline">\(\Psi(\vec{r}_1,
\vec{r}_2, ..., \vec{r}_N): \mathbb{R}^{3N} \to \mathbb{C}\)</span>
<ul>
<li>这是一个包含 N 个电子的体系，其波函数 <span
class="math inline">\(\Psi\)</span> 是 <span
class="math inline">\(3N\)</span>
个空间坐标（每个电子有3个坐标）的函数，值域为复数 <span
class="math inline">\(\mathbb{C}\)</span>。这是一个非常高维度的复杂函数。</li>
</ul></li>
<li>电子密度 (Electron density): <span
class="math inline">\(\rho(\vec{r}): \mathbb{R}^3 \to
\mathbb{R}\)</span>
<ul>
<li>电子密度 <span class="math inline">\(\rho\)</span> 只是空间中一点
<span
class="math inline">\(\vec{r}\)</span>（3个坐标）的函数，值域为实数
<span class="math inline">\(\mathbb{R}\)</span>。</li>
<li>DFT 的核心思想就是用这个更简单的 <span
class="math inline">\(\rho(\vec{r})\)</span> 来代替复杂的 <span
class="math inline">\(\Psi\)</span> 作为基本变量。</li>
</ul></li>
</ul></li>
<li><strong>H.K. (Hohenberg-Kohn) 定理:</strong>
<ul>
<li>这是 DFT 的理论基石。白板上的图示 ① 和 ② 总结了这两个定理。</li>
<li><strong>图示 ① (H.K. 第一定理):</strong>
<ul>
<li><code>Vext/H</code> <span
class="math inline">\(\leftrightarrow\)</span> <code>ρ(r)</code> <span
class="math inline">\(\leftrightarrow\)</span>
<code>Ψi(r1, r2, ...)</code></li>
<li><strong>含义:</strong> 体系的外势 <span
class="math inline">\(V_{ext}\)</span>
（通常指原子核对电子的吸引势，它决定了体系的哈密顿量 <span
class="math inline">\(H\)</span>）与基态电子密度 <span
class="math inline">\(\rho(\vec{r})\)</span> 之间存在一一对应关系。</li>
<li><strong>推论:</strong> 由于 <span
class="math inline">\(V_{ext}\)</span> 决定了波函数 <span
class="math inline">\(\Psi\)</span>，因此，基态电子密度 <span
class="math inline">\(\rho_0\)</span> 唯一地决定了基态波函数 <span
class="math inline">\(\Psi_0\)</span> 以及体系的所有性质。</li>
</ul></li>
<li><strong>图示 ② (H.K. 第二定理):</strong>
<ul>
<li><code>min E[ρ(r)] → ρ₀, Egs</code></li>
<li><strong>含义:</strong> 能量泛函 <span
class="math inline">\(E[\rho]\)</span>
（能量是电子密度的函数）在正确的基态密度 <span
class="math inline">\(\rho_0\)</span>
处取得最小值，这个最小值就是体系的基态能量 <span
class="math inline">\(E_{gs}\)</span>。</li>
</ul></li>
</ul></li>
<li><strong>Levy-Lieb 泛函 (Levy-Lieb functional):</strong>
<ul>
<li>也称为 <strong>Levy 约束搜索 (constrained search)
泛函</strong>。这是对 H.K. 定理中能量泛函 <span
class="math inline">\(E[\rho]\)</span> 的一种更严格和普适的定义。</li>
<li><code>ρ ← Vext : v-representability</code>
<ul>
<li>这提出了一个问题：什么样的密度 <span
class="math inline">\(\rho\)</span> 可以对应于某个外势 <span
class="math inline">\(V_{ext}\)</span>？这被称为“v-可表征性”问题。</li>
</ul></li>
<li><code>Δ</code> (三角符号通常表示 “定义为”)</li>
<li><code>N</code> (圆圈) <span class="math inline">\(\supset\)</span>
<code>K</code> (圆圈)
<ul>
<li><em>这个符号旁边的字迹有些潦草，但结合上下文，这可能是在区分
N-representability（N-可表征性）和
v-representability（v-可表Vext表征性）。</em></li>
</ul></li>
</ul></li>
</ul>
<h3 id="能量泛函与计算">能量泛函与计算</h3>
<ul>
<li><strong>约束搜索 (Constrained search):</strong>
<ul>
<li><span class="math inline">\(\int \rho(\vec{r}) d\vec{r} = N\)</span>
<ul>
<li>这是一个约束条件，即电子密度在全空间积分必须等于体系的总电子数 <span
class="math inline">\(N\)</span>。</li>
</ul></li>
</ul></li>
<li><strong>① 能量泛函 <span class="math inline">\(E_{LL}[\rho]\)</span>
(Levy-Lieb):</strong>
<ul>
<li><span class="math inline">\(E_{LL}[\rho] = \min_{\Psi \to \rho}
\langle \Psi | \hat{T} + \hat{V}_{ee} | \Psi \rangle + \int d\vec{r}
V_{ext}(\vec{r}) \rho(\vec{r})\)</span></li>
<li><strong>解释:</strong>
<ul>
<li>这个公式定义了总能量 <span class="math inline">\(E\)</span>
如何作为密度 <span class="math inline">\(\rho\)</span> 的泛函。</li>
<li>它分为两部分：
<ol type="1">
<li><span class="math inline">\(\int d\vec{r} V_{ext}(\vec{r})
\rho(\vec{r})\)</span>: 电子在外势 <span
class="math inline">\(V_{ext}\)</span> 中的能量。这部分是已知的（如果
<span class="math inline">\(V_{ext}\)</span> 和 <span
class="math inline">\(\rho\)</span> 已知）。</li>
<li><span class="math inline">\(\min_{\Psi \to \rho} \langle \Psi |
\hat{T} + \hat{V}_{ee} | \Psi \rangle\)</span>: 这是
<strong>Hohenberg-Kohn 泛函</strong> <span
class="math inline">\(F_{HK}[\rho]\)</span>（也常被称为 Levy-Lieb
泛函），它代表动能 <span class="math inline">\(\hat{T}\)</span>
和电子-电子相互作用能 <span class="math inline">\(\hat{V}_{ee}\)</span>
的总和。</li>
</ol></li>
<li><strong>约束搜索 (min <span class="math inline">\(\Psi \to
\rho\)</span>)</strong>： <span
class="math inline">\(F_{HK}[\rho]\)</span>
的值是通过搜索所有能够产生该密度 <span
class="math inline">\(\rho\)</span> 的波函数 <span
class="math inline">\(\Psi\)</span>，并从中找出使 <span
class="math inline">\(\langle \hat{T} + \hat{V}_{ee} \rangle\)</span>
最小的那个 <span class="math inline">\(\Psi\)</span> 来确定的。这个泛函
<span class="math inline">\(F_{HK}[\rho]\)</span> 是<strong>普适
(Universal)</strong> 的，因为它不依赖于 <span
class="math inline">\(V_{ext}\)</span>。</li>
</ul></li>
</ul></li>
<li><strong>② 求解基态 (Finding the Ground State):</strong>
<ul>
<li><span class="math inline">\(E_{gs} = \min_{\rho}
E_{LL}[\rho]\)</span>
<ul>
<li><strong>含义 (H.K. 第二定理的变分原理):</strong> 体系的基态能量
<span class="math inline">\(E_{gs}\)</span> 可以通过最小化总能量泛函
<span class="math inline">\(E_{LL}[\rho]\)</span> 来获得。</li>
</ul></li>
<li><span class="math inline">\(\rho_0 \to \{\Psi_0^{(1)}, \Psi_0^{(2)},
...\} \Rightarrow V_{ext}\)</span>
<ul>
<li><strong>含义:</strong> 当找到最优的基态密度 <span
class="math inline">\(\rho_0\)</span> 时，我们就同时确定了基态能量 <span
class="math inline">\(E_{gs}\)</span>。通过这个 <span
class="math inline">\(\rho_0\)</span>（以及它对应的波函数集合），原则上也可以反推出唯一确定它的外势
<span class="math inline">\(V_{ext}\)</span>。</li>
</ul></li>
</ul></li>
</ul>
<h3 id="其他笔记">其他笔记</h3>
<ul>
<li><strong><span class="math inline">\(p_i \propto e^{-\beta E_i /
Z}\)</span> finite temperature</strong>
<ul>
<li>这是白板右下角的一行字，与上面的 DFT 主题略有不同。</li>
<li><strong>含义:</strong> 这描述的是<strong>有限温度 (finite
temperature)</strong> 下的<strong>正则系综 (canonical
ensemble)</strong>。</li>
<li><span class="math inline">\(p_i\)</span> 是体系处于能量为 <span
class="math inline">\(E_i\)</span> 的某个状态的概率。</li>
<li><span class="math inline">\(\beta = 1 / (k_B T)\)</span>，<span
class="math inline">\(k_B\)</span> 是玻尔兹曼常数，<span
class="math inline">\(T\)</span> 是温度。</li>
<li><span class="math inline">\(Z\)</span> 是配分函数 (Partition
function)。</li>
<li>这个公式（玻尔兹曼分布）是统计力学的基础，用于将 DFT 推广到 <span
class="math inline">\(T &gt; 0\)</span> K 的情况（即 Mermin
泛函）。</li>
</ul></li>
</ul>
<h3 id="总结">总结</h3>
<p>总结了密度泛函理论 (DFT) 的数学和物理基础，从 Hohenberg-Kohn
定理（能量和密度的一一对应及变分原理）讲到了 Levy-Lieb
的约束搜索构造方法，这是现代 DFT
理论的核心。右下角的笔记则暗示了如何将此理论推广到有限温度体系。</p>
<p>这是（基态 DFT）内容的延续，主题是<strong>有限温度和系综 DFT (Finite
temperature and ensemble DFT)</strong>，以及<strong>无轨道 DFT
(Orbital-free DFT)</strong>。</p>
<p>以下是白板上内容的逐条中文转录和解释：</p>
<h3 id="有限温度和系综-dft-mermin-dft">有限温度和系综 DFT (Mermin
DFT)</h3>
<p>这部分内容将 DFT 从 <span
class="math inline">\(T=0\text{K}\)</span>（绝对零度，只关心基态）推广到
<span class="math inline">\(T &gt;
0\text{K}\)</span>（有限温度，需要考虑热激发和系综平均）。</p>
<ul>
<li><strong>对比 <span class="math inline">\(T=0\text{K}\)</span> 和
<span class="math inline">\(T\)</span> finite (有限温度):</strong>
<ul>
<li><strong><span class="math inline">\(T=0K\)</span> (左栏):</strong>
<ul>
<li><span class="math inline">\(\rho_0\)</span> (基态密度)</li>
<li><span class="math inline">\(|\Psi\rangle\)</span> (基态波函数)</li>
<li><span class="math inline">\(\langle \hat{O} \rangle = \langle \Psi |
\hat{O} | \Psi \rangle\)</span> (零温下的期望值)</li>
</ul></li>
<li><strong><span class="math inline">\(T\)</span> finite
(中栏):</strong>
<ul>
<li><span class="math inline">\(p_e\)</span> (系综密度 /
热力学密度)</li>
<li><span class="math inline">\(\hat{\rho} = \sum_i f_i
|\Psi_i\rangle\langle\Psi_i|\)</span> (密度矩阵)</li>
<li><span class="math inline">\(\langle \hat{O} \rangle =
\text{Tr}(\hat{\rho}\hat{O})\)</span>
(有限温度下的期望值，即系综平均)</li>
</ul></li>
</ul></li>
<li><strong>Mermin 泛函 (Mermin Functional):</strong>
<ul>
<li>这是 Mermin (1965年) 对 DFT 的推广，用于描述有限温度下的体系。</li>
<li>目标是最小化<strong>自由能 (Free
Energy)</strong>，而不是总能量。</li>
<li><strong><span class="math inline">\(F_{Mermin}[\rho] =
\min_{\hat{\rho} \to \rho} \text{Tr} \{ \hat{\rho} [ \hat{H} +
\frac{1}{\beta}\ln\hat{\rho} ] \}\)</span></strong>
<ul>
<li><strong>解释:</strong>
<ul>
<li>这里的 <span class="math inline">\(F\)</span> 不是指 Hohenberg-Kohn
泛函，而是指<strong>亥姆霍兹自由能 (Helmholtz free energy)</strong>
<span class="math inline">\(\Omega\)</span>（或 <span
class="math inline">\(A\)</span>）。白板上写 <span
class="math inline">\(F\)</span> 可能是指普适泛函部分。</li>
<li><span class="math inline">\(\hat{H}\)</span> 是哈密顿量（不含 <span
class="math inline">\(V_{ext}\)</span> 的部分，即 <span
class="math inline">\(\hat{T} + \hat{V}_{ee}\)</span>）。</li>
<li><span class="math inline">\(\frac{1}{\beta}\ln\hat{\rho}\)</span>
这一项与<strong>熵 (Entropy)</strong> 相关 (<span
class="math inline">\(S = -k_B
\text{Tr}(\hat{\rho}\ln\hat{\rho})\)</span>)。</li>
<li><span class="math inline">\(\beta = 1 / (k_B T)\)</span>。</li>
<li>整个 <span class="math inline">\(\text{Tr}\{...\}\)</span>
表达式代表系统的亥姆霍兹自由能 <span
class="math inline">\(A\)</span>。</li>
<li><span class="math inline">\(\min_{\hat{\rho} \to \rho}\)</span>：与
Levy-Lieb 约束搜索类似，这里是搜索所有能产生密度 <span
class="math inline">\(\rho\)</span> 的密度矩阵 <span
class="math inline">\(\hat{\rho}\)</span>，并找到使自由能最小的那个。</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>② 自由能最小化:</strong>
<ul>
<li><strong><span class="math inline">\(F_e = \min_{\hat{\rho}_e}
f_{mermin}[\rho], \rho_e\)</span></strong>
<ul>
<li><em>这行字迹有些潦草，但结合上下文，它表达的意思是：</em>
体系的平衡自由能 <span class="math inline">\(F_e\)</span> (或 <span
class="math inline">\(\Omega_e\)</span>) 是通过最小化 Mermin
泛函得到的，此时对应的密度是平衡态密度 <span
class="math inline">\(\rho_e\)</span>。</li>
</ul></li>
<li><strong><span class="math inline">\(\hat{\rho}_e = \frac{e^{-\beta
\hat{H}}}{\text{Tr}(e^{-\beta \hat{H}})} = \frac{e^{-\beta
\hat{H}}}{Z}\)</span></strong>
<ul>
<li>这是热力学平衡态下的<strong>正则系综密度矩阵 (equilibrium density
matrix)</strong>。<span class="math inline">\(Z\)</span>
是配分函数。</li>
</ul></li>
<li><strong><span class="math inline">\(\hat{H} =
-\frac{1}{\beta}\ln(\hat{\rho}_e)\)</span></strong>
<ul>
<li>这是上一个公式的简单变形，反解出哈密顿量 <span
class="math inline">\(\hat{H}\)</span>。</li>
</ul></li>
</ul></li>
<li><strong>应用场景 (右上角):</strong>
<ul>
<li><strong><span class="math inline">\(T &gt; 10,000K\)</span>
warm-dense materials</strong>
<ul>
<li>指出这种有限温度 DFT 理论常用于研究<strong>温稠密物质</strong>
(WDM)，如行星核心或惯性约束聚变 (ICF) 实验中的状态。</li>
</ul></li>
<li><strong>Fermi sea (费米海):</strong>
<ul>
<li>旁边的图示 (一个方框和坐标轴)
可能是在示意费米面在高温下变得模糊（即费米-狄拉克分布不再是 <span
class="math inline">\(T=0\)</span> 时的阶跃函数）。</li>
</ul></li>
</ul></li>
</ul>
<h3 id="칠-无轨道-dft-orbital-free-dft">칠 无轨道 DFT (Orbital-free
DFT)</h3>
<p>这部分回到了 <span class="math inline">\(T=0\)</span>
的情况（或稍作修改也可用于有限温度），但介绍了一种计算上更简化的 DFT
方法。</p>
<ul>
<li><strong><span class="math inline">\(E[\rho]\)</span> ?</strong>
<ul>
<li>提出一个问题：能量泛函 <span class="math inline">\(E[\rho]\)</span>
到底是什么样子的？</li>
</ul></li>
<li><strong><span class="math inline">\(\Delta\)</span> Orbital-free DFT
(OF-DFT):</strong>
<ul>
<li><strong>动机:</strong> Kohn-Sham DFT (标准的 DFT)
仍然需要求解一组单电子轨道，计算量随系统增大而急剧上升 (通常是 <span
class="math inline">\(N^3\)</span> 或更高)。OF-DFT
试图完全避免求解轨道，只依赖于密度 <span
class="math inline">\(\rho\)</span> 本身。</li>
<li><strong>Thomas-Fermi-Dirac approximation (托马斯-费米-狄拉克
近似):</strong>
<ul>
<li>这是 OF-DFT 中最早期和最简单的近似。</li>
<li><strong><span class="math inline">\(E_{TF}[\rho] = T[\rho] + \int
V_{ext}(\vec{r})\rho(\vec{r})d\vec{r} + \frac{e^2}{2} \iint
\frac{\rho(\vec{r})\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|}d\vec{r}d\vec{r}&#39;
+ E_{xc}[\rho]\)</span> ?</strong></li>
<li><strong>泛函的组成：</strong>
<ol type="1">
<li><strong><span class="math inline">\(T[\rho]\)</span>:</strong>
动能泛函。在 OF-DFT 中，最大的挑战就是找到 <span
class="math inline">\(T[\rho]\)</span>
的一个精确的、仅依赖于密度的表达式。Thomas-Fermi
理论给出了第一个近似（<span class="math inline">\(T_{TF}[\rho] \propto
\int \rho^{5/3} d\vec{r}\)</span>）。</li>
<li><strong><span class="math inline">\(\int
V_{ext}(\vec{r})\rho(\vec{r})d\vec{r}\)</span>:</strong> 外势能量
(与标准 DFT 相同)。</li>
<li><strong><span class="math inline">\(\frac{e^2}{2} \iint
...\)</span>:</strong> 电子-电子相互作用的<strong>哈特里 (Hartree)
能量</strong>，即经典的静电排斥能 (与标准 DFT 相同)。</li>
<li><strong><span class="math inline">\(E_{xc}[\rho]\)</span>
?:</strong> <strong>交换关联 (Exchange-Correlation) 能量</strong>。
<ul>
<li><em>白板上在这一项后面打了一个问号</em>，表示这部分也需要一个仅依赖于
<span class="math inline">\(\rho\)</span> 的近似泛函（例如 Local Density
Approximation, LDA，其中 <span class="math inline">\(E_x \propto \int
\rho^{4/3} d\vec{r}\)</span>）。</li>
<li>当 <span class="math inline">\(E_{xc}\)</span> 包含狄拉克 (Dirac)
的交换能近似时，就称为 Thomas-Fermi-Dirac (TFD) 理论。</li>
</ul></li>
</ol></li>
</ul></li>
</ul></li>
</ul>
<h3 id="总结-1">总结</h3>
<p>从 <span class="math inline">\(T=0\)</span> 的基态 DFT
出发，探讨了两个进阶主题： 1. <strong>Mermin DFT:</strong> 如何将 DFT
框架从 <span class="math inline">\(T=0\)</span> 的总能量 <span
class="math inline">\(E\)</span> 推广到 <span class="math inline">\(T
&gt; 0\)</span> 的自由能 <span
class="math inline">\(F\)</span>，这对于描述高温系统（如温稠密物质）至关重要。
2. <strong>Orbital-free DFT:</strong>
一种计算上可能更高效（但目前精度较低）的 DFT 方法，它试图避免使用
Kohn-Sham 轨道，而是直接构建总能量（特别是动能 <span
class="math inline">\(T[\rho]\)</span>）作为电子密度的显式泛函。</p>
<p><strong>Orbital-free DFT (OF-DFT)</strong> 是一种“理想中”的
DFT，而我们通常在实践中（例如在 Google 上搜索“DFT 计算”）谈论的几乎都是
<strong>Kohn-Sham DFT (KS-DFT)</strong>。</p>
<p>它们都基于相同的 Hohenberg-Kohn
定理，但实现这个定理的<strong>策略</strong>完全不同。</p>
<h3 id="kohn-sham-ks-dft实用的妥协方案">Kohn-Sham (KS)
DFT：实用的妥协方案</h3>
<p>KS-DFT 的天才之处在于它<strong>没有</strong>试图直接解决那个最棘手的
<strong><span
class="math inline">\(T[\rho]\)</span></strong>（动能泛函）。</p>
<p><strong>1. 核心思想：引入“虚拟系统”</strong></p>
<p>Kohn 和 Sham
提出：我们不去处理那个复杂、强相互作用的<strong>真实电子系统</strong>，而是构建一个<strong>虚拟的、无相互作用的电子系统</strong>。</p>
<p>这个虚拟系统被设计为<strong>恰好</strong>具有与真实系统<strong>完全相同的基态电子密度
<span class="math inline">\(\rho_0(\vec{r})\)</span></strong>。</p>
<p><strong>2. 为什么这样做有好处？</strong></p>
<ul>
<li>对于<strong>无相互作用</strong>的系统，我们<strong>精确地知道</strong>如何计算其动能！</li>
<li>动能 <span class="math inline">\(T_s\)</span> 就是所有虚拟粒子（称为
Kohn-Sham 轨道 <span
class="math inline">\(\phi_i\)</span>）动能的总和。</li>
<li>这个系统的波函数就是一个简单的斯莱特行列式（Slater
determinant），密度 <span class="math inline">\(\rho(\vec{r}) = \sum_i^N
|\phi_i(\vec{r})|^2\)</span>。</li>
</ul>
<p><strong>3. Kohn-Sham 能量泛函</strong></p>
<p>KS-DFT 将总能量 <span class="math inline">\(E[\rho]\)</span>
重新划分为以下几项：</p>
<p><span class="math display">\[E_{KS}[\rho] = T_s[\{\phi_i\}] + \int
V_{ext} \rho(\vec{r}) d\vec{r} + E_H[\rho] + E_{xc}[\rho]\]</span></p>
<ul>
<li><span class="math inline">\(T_s[\{\phi_i\}]\)</span>:
<strong>无相互作用动能</strong>。这是通过求解轨道 <span
class="math inline">\(\phi_i\)</span>
来<strong>精确计算</strong>的，而不是作为 <span
class="math inline">\(\rho\)</span> 的泛函来近似。</li>
<li><span class="math inline">\(\int V_{ext} \rho(\vec{r})
d\vec{r}\)</span>: 外势能 (与 OF-DFT 相同)。</li>
<li><span class="math inline">\(E_H[\rho]\)</span>: 电子-电子静电排斥能
(Hartree 能量，与 OF-DFT 相同)。</li>
<li><span class="math inline">\(E_{xc}[\rho]\)</span>:
<strong>交换关联能</strong>。</li>
</ul>
<p><strong>4. 在 <span class="math inline">\(E_{xc}\)</span>
中</strong></p>
<p>现在，未知被藏在了 <span class="math inline">\(E_{xc}[\rho]\)</span>
这一项里。它包含： 1. <strong>交换能</strong> (纯量子效应)。 2.
<strong>关联能</strong> (电子如何相互“躲避”)。 3.
<strong>动能差</strong>：真实系统的动能 <span
class="math inline">\(T\)</span> 和我们计算的无相互作用动能 <span
class="math inline">\(T_s\)</span> 之间的差值 <span
class="math inline">\((T - T_s)\)</span>。</p>
<p>KS-DFT 的<strong>巨大成功</strong>在于：<strong><span
class="math inline">\(E_{xc}[\rho]\)</span>
这一项（虽然仍然未知且必须近似）被证明比直接近似 <span
class="math inline">\(T[\rho]\)</span> 要容易得多！</strong></p>
<p>KS-DFT 的工作就是求解一组单电子方程（Kohn-Sham 方程），找到轨道 <span
class="math inline">\(\phi_i\)</span>，从而构建 <span
class="math inline">\(\rho\)</span>，并计算总能量。</p>
<h3 id="对比kohn-sham-dft-vs.-orbital-free-dft">对比：Kohn-Sham DFT
vs. Orbital-Free DFT</h3>
<p>这张表格总结了两者最关键的区别：</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">特征</th>
<th style="text-align: left;">🔵 Kohn-Sham DFT (KS-DFT)</th>
<th style="text-align: left;">🟠 Orbital-Free DFT (OF-DFT)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>基本变量</strong></td>
<td style="text-align: left;">Kohn-Sham <strong>轨道</strong> <span
class="math inline">\(\{\phi_i\}\)</span> (用来构建密度 <span
class="math inline">\(\rho\)</span>)</td>
<td style="text-align: left;"><strong>电子密度</strong> <span
class="math inline">\(\rho(\vec{r})\)</span></td>
</tr>
<tr>
<td style="text-align: left;"><strong>核心挑战</strong></td>
<td style="text-align: left;">近似<strong>交换关联泛函 <span
class="math inline">\(E_{xc}[\rho]\)</span></strong></td>
<td style="text-align: left;">近似<strong>动能泛函 <span
class="math inline">\(T[\rho]\)</span></strong> (以及 <span
class="math inline">\(E_{xc}[\rho]\)</span>)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>动能处理</strong></td>
<td style="text-align: left;"><strong>间接计算</strong>：通过求解轨道
<span class="math inline">\(\phi_i\)</span>
精确计算<strong>无相互作用动能 <span
class="math inline">\(T_s\)</span></strong>。</td>
<td style="text-align: left;"><strong>直接近似</strong>：必须找到一个
<span class="math inline">\(T[\rho]\)</span> 的表达式 (例如 <span
class="math inline">\(T \propto \int \rho^{5/3}\)</span>)。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>计算成本</strong></td>
<td
style="text-align: left;"><strong>高</strong>。求解轨道是计算瓶颈，计算量通常随系统大小
<span class="math inline">\(N\)</span> 呈 <span
class="math inline">\(N^3\)</span> 增长。</td>
<td style="text-align: left;"><strong>非常低</strong>。原则上可以 <span
class="math inline">\(N \log N\)</span> 或 <span
class="math inline">\(N\)</span> (线性标度)，因为它只处理 3D 变量 <span
class="math inline">\(\rho\)</span>。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>当前精度</strong></td>
<td
style="text-align: left;"><strong>高</strong>。是量子化学和材料科学的标准方法。</td>
<td style="text-align: left;"><strong>低</strong>。找到一个普适且精确的
<span class="math inline">\(T[\rho]\)</span>
泛函被证明<strong>极其困难</strong>。</td>
</tr>
</tbody>
</table>
<h3 id="总结-2">总结</h3>
<ul>
<li><strong>OF-DFT </strong>
一个完全依赖于密度的理论，计算速度极快，但苦于找不到准确的<strong>动能泛函</strong>
<span class="math inline">\(T[\rho]\)</span>。</li>
<li><strong>KS-DFT (标准方法)</strong>
是一个务实的“妥协”。它引入了<strong>轨道</strong>，用 <span
class="math inline">\(T_s\)</span>
这一项精确地处理了大部分动能，把更小的、更“好近似”的动能差 <span
class="math inline">\((T-T_s)\)</span> 连同交换关联能一起打包成 <span
class="math inline">\(E_{xc}[\rho]\)</span>。</li>
</ul>
<p>当今几乎所有的 DFT 软件（如 VASP, Gaussian, QE）都是基于 Kohn-Sham
方案的。</p>
<p>这推导的是<strong>自由电子气 (free electron gas)</strong>
的动能泛函，这个结果是<strong>无轨道 DFT (OF-DFT)</strong>
和<strong>局域密度近似 (LDA)</strong> 的理论基础。</p>
<p>这内容承接了上一张图的 <span
class="math inline">\(T[\rho]\)</span>（动能泛函）问题，展示了如何为最简单的系统——均匀的自由电子气——推导出
<span class="math inline">\(T\)</span> 和 <span
class="math inline">\(\rho\)</span> 的关系。</p>
<h3 id="详解">详解</h3>
<h4
id="左半部分自由电子气的基本设定">左半部分：自由电子气的基本设定</h4>
<ol type="1">
<li><strong>标题：free electron gas (自由电子气)</strong>
<ul>
<li>这是一个理想化模型，假设电子在无外势（或均匀正电荷背景）中自由运动。</li>
</ul></li>
<li><strong>薛定谔方程：</strong>
<ul>
<li><span class="math inline">\(-\frac{\hbar^2}{2m}\nabla^2\Psi =
E_k\Psi\)</span></li>
<li>这是自由粒子的定态薛定谔方程，其解为平面波。</li>
</ul></li>
<li><strong>波函数与能量：</strong>
<ul>
<li><span class="math inline">\(\Psi_k = \frac{1}{\sqrt{\Omega}}
e^{i\vec{k}\cdot\vec{r}}\)</span> （平面波解）</li>
<li><span class="math inline">\(\Omega = L^3\)</span> （系统体积）</li>
<li><span class="math inline">\(E_k = \frac{\hbar^2 k^2}{2m}\)</span>
（能量本征值，k 是波矢 <span
class="math inline">\(k=|\vec{k}|\)</span>）</li>
</ul></li>
<li><strong>PBC (周期性边界条件):</strong>
<ul>
<li><code>kx = 0, ±2π/L, ±4π/L, ...</code></li>
<li>这说明 <span class="math inline">\(\vec{k}\)</span>
矢量在“k空间”中不是连续的，而是形成一个晶格，每个点占据的体积是 <span
class="math inline">\((\frac{2\pi}{L})^3\)</span>。</li>
</ul></li>
<li><strong>3D 状态填充 (费米球):</strong>
<ul>
<li>图示为一个球体，半径为 <span
class="math inline">\(k_F\)</span>（费米波矢）。在 <span
class="math inline">\(T=0\text{K}\)</span> 时，所有 <span
class="math inline">\(k &lt; k_F\)</span> 的状态都被电子填满。</li>
<li><strong>计算电子总数 <span class="math inline">\(N^\sigma\)</span>
(单一自旋):</strong>
<ul>
<li><span class="math inline">\(N^\sigma =
\frac{\text{费米球体积}}{\text{单个 k 态体积}}\)</span></li>
<li><span class="math inline">\(N^\sigma = \frac{\frac{4}{3}\pi
(k_F^\sigma)^3}{(\frac{2\pi}{L})^3} = \frac{\frac{4}{3}\pi
(k_F^\sigma)^3}{(2\pi)^3/\Omega}\)</span></li>
</ul></li>
</ul></li>
</ol>
<h4 id="右半部分推导-trho-动能泛函">右半部分：推导 <span
class="math inline">\(T[\rho]\)</span> (动能泛函)</h4>
<ol type="1">
<li><strong>关联 <span class="math inline">\(k_F\)</span> 和密度 <span
class="math inline">\(\rho\)</span>:</strong>
<ul>
<li>从左侧公式整理可得：<span class="math inline">\(\rho^\sigma =
\frac{N^\sigma}{\Omega} = \frac{(k_F^\sigma)^3}{6\pi^2}\)</span>。</li>
<li>假设系统是自旋非极化的（<span class="math inline">\(\rho^\uparrow =
\rho^\downarrow\)</span>），总密度 <span class="math inline">\(\rho =
\rho^\uparrow + \rho^\downarrow = 2\rho^\sigma\)</span>，且 <span
class="math inline">\(k_F^\uparrow = k_F^\downarrow =
k_F\)</span>。</li>
<li>代入可得：<span class="math inline">\(\rho = 2 \cdot
\frac{k_F^3}{6\pi^2} = \frac{k_F^3}{3\pi^2}\)</span>。</li>
<li><strong><span class="math inline">\(\Rightarrow (k_F)^3 = 3\pi^2
\rho\)</span></strong> (白板上的关键关系)</li>
</ul></li>
<li><strong>费米能 (Fermi Energy):</strong>
<ul>
<li><code>HOMO/VBM → EF</code> (在连续能带中，最高占据能级就是费米能
<span class="math inline">\(E_F\)</span>)</li>
<li><span class="math inline">\(E_F = \frac{\hbar^2
k_F^2}{2m}\)</span></li>
</ul></li>
<li><strong>计算总动能 <span class="math inline">\(T\)</span>:</strong>
<ul>
<li>总动能 <span class="math inline">\(T\)</span>
是所有电子动能的总和。在 <span
class="math inline">\(T=0\text{K}\)</span>
时，等于对费米球内所有状态的能量 <span
class="math inline">\(E_k\)</span> 进行积分。</li>
<li><span class="math inline">\(T = T^\uparrow +
T^\downarrow\)</span></li>
<li>通过积分（白板上省略了积分步骤，直接用了熟知结论），可以得到<strong>平均动能</strong>
<span class="math inline">\(\langle E_{kin} \rangle = \frac{3}{5}
E_F\)</span>。</li>
<li>因此，总动能 <span class="math inline">\(T = N \cdot \langle E_{kin}
\rangle = N \cdot \frac{3}{5} E_F\)</span>。</li>
</ul></li>
<li><strong>T 作为 <span class="math inline">\(\rho\)</span> 的泛函
(最终推导):</strong>
<ul>
<li>这是最关键的一步：将 <span class="math inline">\(T = \frac{3}{5} N
E_F\)</span> 中的 <span class="math inline">\(N\)</span> 和 <span
class="math inline">\(E_F\)</span> 全部用密度 <span
class="math inline">\(\rho\)</span> 替换掉。</li>
<li><span class="math inline">\(N = \int \rho(\vec{r}) d\vec{r}\)</span>
(电子总数)</li>
<li><span class="math inline">\(E_F = \frac{\hbar^2 k_F^2}{2m} =
\frac{\hbar^2}{2m} (3\pi^2 \rho)^{2/3}\)</span> (将 <span
class="math inline">\(k_F\)</span> 用 <span
class="math inline">\(\rho\)</span> 替换)</li>
<li><strong>局域密度近似 (Local Density Approximation, LDA):</strong>
<ul>
<li>我们<strong>假设</strong>一个真实系统（密度<em>不</em>均匀，<span
class="math inline">\(\rho =
\rho(\vec{r})\)</span>）的总动能，可以通过在空间中每一点 <span
class="math inline">\((\vec{r})\)</span>
使用上述<strong>均匀电子气</strong>的结果，然后求和（积分）得到。</li>
<li><span class="math inline">\(T[\rho] = \int (\text{电子数密度}) \cdot
(\text{平均动能}) d\vec{r}\)</span></li>
<li><span class="math inline">\(T[\rho] = \int \rho(\vec{r}) \cdot
\frac{3}{5} E_F(\rho(\vec{r})) d\vec{r}\)</span></li>
<li><span class="math inline">\(T[\rho] = \int \rho(\vec{r}) \cdot
\frac{3}{5} \left[ \frac{\hbar^2}{2m} (3\pi^2 \rho(\vec{r}))^{2/3}
\right] d\vec{r}\)</span></li>
</ul></li>
<li><strong>整理后得到白板上的最终公式：</strong>
<ul>
<li><span class="math inline">\(T[\rho] = \frac{\hbar^2}{m} \frac{3}{10}
(3\pi^2)^{2/3} \int \rho^{5/3}(\vec{r}) d\vec{r}\)</span></li>
<li>这被称为<strong>托马斯-费米 (Thomas-Fermi) 动能泛函</strong>。</li>
</ul></li>
</ul></li>
</ol>
<h3 id="总结-3">总结</h3>
<p>这展示了 <span class="math inline">\(T[\rho] \propto \int \rho^{5/3}
d\vec{r}\)</span> 这个著名公式的来源。</p>
<ul>
<li>它为<strong>无轨道 DFT</strong>
提供了第一个（也是最简单的）<strong>动能泛函</strong> <span
class="math inline">\(T[\rho]\)</span> 近似。</li>
<li>它也是<strong>局域密度近似 (LDA)</strong> 的基础。在 Kohn-Sham DFT
中，虽然 <span class="math inline">\(T_s\)</span> (无相互作用动能)
是通过轨道精确计算的，但 <span class="math inline">\(E_{xc}\)</span>
(交换关联能) 里的交换能 <span class="math inline">\(E_x\)</span>
也是用完全相同的逻辑（自由电子气模型）推导出来的（<span
class="math inline">\(E_x[\rho] \propto \int \rho^{4/3}
d\vec{r}\)</span>）。</li>
</ul>
<p><strong>无轨道密度泛函理论 (Orbital-Free DFT)</strong>
的核心求解方程，它直接源自前面的推导。</p>
<p><strong>如何通过最小化能量泛函来找到基态密度 <span
class="math inline">\(\rho_0\)</span></strong>。</p>
<h3 id="详解-1">详解</h3>
<p><strong>1. 核心思想：约束下的最小化</strong></p>
<ul>
<li><strong><span class="math inline">\(\frac{\delta}{\delta\rho} \left(
E_{TF}[\rho] - \mu (\int \rho(\vec{r})d\vec{r} - N) \right) =
0\)</span></strong>
<ul>
<li>这是一个使用“拉格朗日乘子法”的<strong>泛函求导</strong>（或称变分）方程。</li>
<li><strong>目的：</strong> 寻找使总能量 <span
class="math inline">\(E_{TF}[\rho]\)</span>
达到<strong>最小值</strong>的那个密度 <span
class="math inline">\(\rho\)</span>。</li>
<li><strong>约束：</strong>
这个最小化必须满足一个条件，即电子密度在全空间积分必须等于总电子数 <span
class="math inline">\(N\)</span>（即 <span class="math inline">\(\int
\rho(\vec{r})d\vec{r} = N\)</span>）。</li>
<li><strong><span class="math inline">\(\mu\)</span> (mu):</strong>
就是为这个约束条件引入的“拉格朗日乘子”。</li>
</ul></li>
</ul>
<p><strong>2. 欧拉-拉格朗日方程 (Euler-Lagrange Equation)</strong></p>
<ul>
<li>下面那一大长串方程，就是执行上面那行泛函求导 <span
class="math inline">\(\frac{\delta}{\delta\rho}\)</span>
后得到的结果，其形式为 <span class="math inline">\(\frac{\delta
E_{TF}[\rho]}{\delta\rho} = \mu\)</span>。</li>
<li>让我们逐项分解 <span class="math inline">\(\frac{\delta
E_{TF}[\rho]}{\delta\rho}\)</span>：
<ul>
<li><strong><span class="math inline">\(E_{TF}[\rho] = T[\rho] +
E_{V_{ext}}[\rho] + E_H[\rho] + E_{xc}[\rho]\)</span></strong>
(这是上一张白板中定义的总能量)</li>
</ul></li>
<li><strong>方程的各项：</strong>
<ul>
<li><strong>第一项：<span class="math inline">\(\frac{\hbar^2}{m}
\frac{3}{10} (3\pi^2)^{2/3} \cdot \frac{5}{3}
\rho^{2/3}\)</span></strong>
<ul>
<li>这是对 <strong>动能泛函 <span
class="math inline">\(T[\rho]\)</span></strong> 求泛函导数的结果。</li>
<li><span class="math inline">\(T[\rho] = C_F \int \rho^{5/3}
d\vec{r}\)</span> (来自上一张白板)。</li>
<li><span class="math inline">\(\frac{\delta T[\rho]}{\delta\rho} = C_F
\cdot \frac{5}{3} \rho^{2/3}\)</span>。</li>
<li>这一项代表一种源自动能的“量子压力”。</li>
</ul></li>
<li><strong>第二项：<span
class="math inline">\(V_{ext}(\vec{r})\)</span></strong>
<ul>
<li>这是对 <strong>外势能</strong> <span
class="math inline">\(E_{V_{ext}}[\rho] = \int V_{ext}(\vec{r})
\rho(\vec{r}) d\vec{r}\)</span> 求导的结果。</li>
<li><span class="math inline">\(\frac{\delta
E_{V_{ext}}[\rho]}{\delta\rho} = V_{ext}(\vec{r})\)</span>。</li>
</ul></li>
<li><strong>第三项：<span class="math inline">\(+ e^2 \int d\vec{r}&#39;
\frac{\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|}\)</span></strong>
<ul>
<li>这是对 <strong>哈特里 (Hartree) 能量</strong> <span
class="math inline">\(E_H[\rho]\)</span>
（电子间经典静电排斥能）求导的结果。</li>
<li>这一项就是<strong>哈特里势 <span
class="math inline">\(V_H(\vec{r})\)</span></strong>，即 <span
class="math inline">\(\rho\)</span> 在 <span
class="math inline">\(\vec{r}\)</span>
处感受到的来自所有其他电子的静电势。</li>
<li><em>(注：白板上在 <span class="math inline">\(V_{ext}\)</span>
和这一项之间写的 <span class="math inline">\(V_{exc}[\rho]\)</span>
似乎是个笔误或简写，因为方程后面明确地分别写出了哈特里项和交换关联项。)</em></li>
</ul></li>
<li><strong>第四项：<span class="math inline">\(+ \frac{\delta
E_{xc}[\rho]}{\delta\rho}\)</span></strong>
<ul>
<li>这是对 <strong>交换关联 (Exchange-Correlation) 能量 <span
class="math inline">\(E_{xc}[\rho]\)</span></strong> 求导的结果。</li>
<li>这个导数本身被<strong>定义</strong>为<strong>交换关联势 <span
class="math inline">\(V_{xc}(\vec{r})\)</span></strong>。</li>
</ul></li>
</ul></li>
</ul>
<p><strong>3. 方程的物理意义</strong></p>
<ul>
<li><strong><span class="math inline">\(= \mu \text{ chemical
potential}\)</span></strong>
<ul>
<li>方程表明，在基态密度 <span class="math inline">\(\rho_0\)</span>
下，系统中所有“势”的总和在空间中处处等于一个常数 <span
class="math inline">\(\mu\)</span>。</li>
<li>这个常数 <span
class="math inline">\(\mu\)</span>（拉格朗日乘子）的物理意义是体系的<strong>化学势
(chemical potential)</strong>，即向系统中添加一个电子所需的能量。</li>
</ul></li>
</ul>
<p><strong>4. 总结 </strong></p>
<ul>
<li><strong><span class="math inline">\(\rho_0 \quad E_{TF}[\rho_0]
\quad \text{shell}\)</span></strong>
<ul>
<li><strong><span class="math inline">\(\rho_0\)</span>:</strong>
通过求解上面那个复杂的（非线性）积分-微分方程，我们就能得到<strong>基态密度
<span class="math inline">\(\rho_0\)</span></strong>。</li>
<li><strong><span
class="math inline">\(E_{TF}[\rho_0]\)</span>:</strong> 将 <span
class="math inline">\(\rho_0\)</span> 代回到 <span
class="math inline">\(E_{TF}[\rho]\)</span>
的原始表达式中，就能计算出体系的<strong>基态总能量</strong>。</li>
<li><strong><span class="math inline">\(\text{shell}\)</span>
(壳层):</strong> 这是一个非常重要的旁注。托马斯-费米 (Thomas-Fermi)
理论（即 OF-DFT
的最早版本）的一个著名<strong>缺陷</strong>是它无法预测原子中<strong>电子壳层结构</strong>（如
1s, 2s, 2p…）。它的密度 <span class="math inline">\(\rho\)</span>
曲线是平滑的，没有“Kohn-Sham
DFT”中轨道所产生的波峰和波谷。这个词很可能是在提醒这个理论的局限性。</li>
</ul></li>
</ul>
<h3 id="整个系列总结">整个系列总结</h3>
<p>这四部分构成了一个关于 DFT 基础： 1. <strong>1:</strong> 介绍了 DFT
的<strong>核心思想</strong> (Hohenberg-Kohn 定理)，即能量是密度的泛函
<span class="math inline">\(E[\rho]\)</span>。 2. <strong>2:</strong>
探讨了两种 DFT 的实现：一种用于<strong>有限温度</strong> (Mermin
DFT)，另一种是<strong>无轨道 DFT (OF-DFT)</strong>，并写出了 <span
class="math inline">\(E_{TF}[\rho]\)</span> 的一般形式。 3.
<strong>3:</strong> 详细<strong>推导</strong>了 OF-DFT
中最关键的<strong>动能泛函</strong> <span class="math inline">\(T[\rho]
\propto \int \rho^{5/3} d\vec{r}\)</span>，其基于自由电子气模型。 4.
<strong>4:</strong> 展示了如何<strong>使用</strong>这个 <span
class="math inline">\(E_{TF}[\rho]\)</span>
泛函，通过泛函求导（变分法）建立一个可解的方程（欧拉-拉格朗日方程），以求得基态密度
<span class="math inline">\(\rho_0\)</span> 和能量 <span
class="math inline">\(E_0\)</span>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/11/04/diffusion_model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/11/04/diffusion_model/" class="post-title-link" itemprop="url">Diffusion Model</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-11-04 11:00:00 / 修改时间：13:48:13" itemprop="dateCreated datePublished" datetime="2025-11-04T11:00:00+08:00">2025-11-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">扩散模型</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>第 1 部分：基础数学工具与物理背景</strong></p>
<p>最近在尝试理解扩散模型所依赖的核心数学概念：高斯分布的性质、朗之万动力学（SDE）及其离散化。</p>
<h3 id="核心数学工具高斯分布-gaussian-distribution">1.
核心数学工具：高斯分布 (Gaussian Distribution)</h3>
<p>扩散模型（尤其是 DDPM）的全部推导都建立在高斯分布的美妙性质之上。</p>
<h4 id="定义">1.1 定义</h4>
<p>一个一维高斯分布（正态分布）由均值 <span
class="math inline">\(\mu\)</span> 和方差 <span
class="math inline">\(\sigma^2\)</span> 定义，其概率密度函数 (PDF) 为：
<span class="math display">\[
\mathcal{N}(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
\]</span> 对于 <span class="math inline">\(D\)</span> 维向量 <span
class="math inline">\(\mathbf{x}\)</span>（例如视频中 <span
class="math inline">\(32 \times 32 = 1024\)</span>
维的图片向量），多维高斯分布由均值向量 <span
class="math inline">\(\boldsymbol{\mu}\)</span> 和协方差矩阵 <span
class="math inline">\(\boldsymbol{\Sigma}\)</span> 定义： <span
class="math display">\[
\mathcal{N}(\mathbf{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) =
\frac{1}{\sqrt{(2\pi)^D \det(\boldsymbol{\Sigma})}}
\exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^T
\boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})\right)
\]</span> &gt; <strong>关键简化：</strong> 在 DDPM
中，我们通常假设协方差矩阵是<strong>对角矩阵</strong> <span
class="math inline">\(\boldsymbol{\Sigma} = \sigma^2
\mathbf{I}\)</span>，其中 <span
class="math inline">\(\mathbf{I}\)</span>
是单位矩阵。这意味着向量的每个维度（每个像素）是独立同分布的（IID）。</p>
<h4 id="关键性质-1重参数化技巧-reparameterization-trick">1.2 关键性质
1：重参数化技巧 (Reparameterization Trick)</h4>
<p><strong>问题：</strong> 我们如何从 <span
class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span>
中采样？直接对这个分布采样是困难的，且在神经网络中无法传递梯度。</p>
<p><strong>技巧：</strong> 我们可以从一个<strong>标准高斯分布</strong>
<span class="math inline">\(\epsilon \sim \mathcal{N}(0, 1)\)</span>
中采样，然后通过线性变换得到 <span class="math inline">\(x\)</span>：
<span class="math display">\[
x = \mu + \sigma \cdot \epsilon
\]</span> <strong>推导：</strong> * <strong>1.：</strong> 设 <span
class="math inline">\(\epsilon \sim \mathcal{N}(0, 1)\)</span>，即 <span
class="math inline">\(E[\epsilon] = 0, \text{Var}(\epsilon) =
1\)</span>。 * <strong>2.：</strong> 我们构造 <span
class="math inline">\(x = \mu + \sigma \epsilon\)</span>。 * 计算 <span
class="math inline">\(x\)</span> 的均值：<span
class="math inline">\(E[x] = E[\mu + \sigma \epsilon] = E[\mu] + \sigma
E[\epsilon] = \mu + \sigma \cdot 0 = \mu\)</span>。 *
<strong>3.：</strong> 计算 <span class="math inline">\(x\)</span>
的方差：<span class="math inline">\(\text{Var}(x) = \text{Var}(\mu +
\sigma \epsilon) = \text{Var}(\sigma \epsilon) = \sigma^2
\text{Var}(\epsilon) = \sigma^2 \cdot 1 = \sigma^2\)</span>。 *
<strong>4.：</strong> 由于高斯分布的线性变换仍然是高斯分布，因此 <span
class="math inline">\(x \sim \mathcal{N}(\mu, \sigma^2)\)</span>。</p>
<p><strong>意义：</strong> 这使得采样过程可微。<span
class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span> 可以是神经网络的输出，<span
class="math inline">\(\epsilon\)</span>
作为外部噪声输入，梯度可以反向传播回 <span
class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span>。</p>
<h4 id="关键性质-2两个独立高斯分布之和-sum-of-independent-gaussians">1.3
关键性质 2：两个独立高斯分布之和 (Sum of Independent Gaussians)</h4>
<p><strong>问题：</strong>
两个独立高斯分布相加会怎样？这是前向加噪过程（Forward
Process）的核心。</p>
<p><strong>性质：</strong> 如果 <span class="math inline">\(X_1 \sim
\mathcal{N}(\mu_1, \sigma_1^2)\)</span> 且 <span
class="math inline">\(X_2 \sim \mathcal{N}(\mu_2, \sigma_2^2)\)</span>
相互独立，那么它们的和 <span class="math inline">\(Y = X_1 +
X_2\)</span> 仍然是高斯分布： <span class="math display">\[
Y \sim \mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
\]</span> <strong>推导：</strong> * <strong>均值：</strong> <span
class="math inline">\(E[Y] = E[X_1 + X_2] = E[X_1] + E[X_2] = \mu_1 +
\mu_2\)</span>。 * <strong>方差：</strong> 由于 <span
class="math inline">\(X_1\)</span> 和 <span
class="math inline">\(X_2\)</span> 相互独立，<span
class="math inline">\(\text{Cov}(X_1, X_2) = 0\)</span>。 <span
class="math inline">\(\text{Var}(Y) = \text{Var}(X_1 + X_2) =
\text{Var}(X_1) + \text{Var}(X_2) + 2 \text{Cov}(X_1, X_2) = \sigma_1^2
+ \sigma_2^2\)</span>。 *
（两个独立高斯变量之和仍为高斯分布，这可以通过矩生成函数或特征函数严格证明，这里我们接受这个结论。）</p>
<p><strong>意义：</strong> 这使得我们可以计算前向过程中任意 <span
class="math inline">\(t\)</span> 时刻 <span
class="math inline">\(x_t\)</span> 从 <span
class="math inline">\(x_0\)</span> 开始累积加噪的结果。</p>
<h3 id="物理与sde朗之万动力学-langevin-dynamics">2.
物理与SDE：朗之万动力学 (Langevin Dynamics)</h3>
<p>这是扩散模型的物理原型。</p>
<h4 id="随机微分方程-sde">2.1 随机微分方程 (SDE)</h4>
<p>朗之万动力学描述了一个粒子（在我们的例子中是图片向量 <span
class="math inline">\(\mathbf{x}\)</span>）在势场 <span
class="math inline">\(U(\mathbf{x})\)</span>
中运动，同时受到随机力（如布朗运动）的影响。其 SDE 形式 为： <span
class="math display">\[
d\mathbf{x}_t = \mathbf{f}(\mathbf{x}_t, t)dt + g(t)d\mathbf{w}_t
\]</span> * <span class="math inline">\(\mathbf{x}_t\)</span>：<span
class="math inline">\(t\)</span> 时刻的粒子位置（图片向量）。 * <span
class="math inline">\(\mathbf{f}(\mathbf{x}_t,
t)\)</span>：<strong>漂移项 (Drift)</strong>。代表确定性的力，如视频中
提到的 “吸引回原点的线性运动”（例如 <span
class="math inline">\(\mathbf{f}(\mathbf{x}, t) = -\beta
\mathbf{x}\)</span>，使分布趋向原点）。它对应能量函数的负梯度 <span
class="math inline">\(-\nabla U(\mathbf{x})\)</span>。 * <span
class="math inline">\(g(t)\)</span>：<strong>扩散项
(Diffusion)</strong>。控制随机噪声的强度。 * <span
class="math inline">\(d\mathbf{w}_t\)</span>：<strong>维纳过程 (Wiener
Process)</strong> 或布朗运动。它是一个随机项，其增量 <span
class="math inline">\(d\mathbf{w}_t\)</span> 在 <span
class="math inline">\(dt\)</span> 时间内服从高斯分布 <span
class="math inline">\(d\mathbf{w}_t \sim \mathcal{N}(0, \mathbf{I}
dt)\)</span>。</p>
<h4 id="sde-的离散化欧拉-丸山法-euler-maruyama">2.2 SDE
的离散化：欧拉-丸山法 (Euler-Maruyama)</h4>
<p><strong>问题：</strong> 计算机无法处理连续时间 <span
class="math inline">\(dt\)</span>。我们如何模拟这个 SDE？</p>
<p><strong>方法：</strong> 我们使用欧拉近似法（在 SDE 中称为
Euler-Maruyama）将其离散化为小的时间步 <span
class="math inline">\(\Delta t\)</span>。 <span class="math display">\[
\mathbf{x}_{t+\Delta t} - \mathbf{x}_t \approx \mathbf{f}(\mathbf{x}_t,
t)\Delta t + g(t) (\mathbf{w}_{t+\Delta t} - \mathbf{w}_t)
\]</span> 根据维纳过程的性质，在 <span class="math inline">\(\Delta
t\)</span> 时间内的增量 <span
class="math inline">\((\mathbf{w}_{t+\Delta t} - \mathbf{w}_t)\)</span>
服从 <span class="math inline">\(\mathcal{N}(0, \mathbf{I} \Delta
t)\)</span>。 根据<strong>性质 1.2（重参数化）</strong>，<span
class="math inline">\(\mathcal{N}(0, \mathbf{I} \Delta t)\)</span>
可以写成 <span class="math inline">\(\sqrt{\Delta t} \cdot
\mathbf{z}\)</span>，其中 <span class="math inline">\(\mathbf{z} \sim
\mathcal{N}(0, \mathbf{I})\)</span>。</p>
<p><strong>离散迭代公式：</strong> <span class="math display">\[
\mathbf{x}_{t+\Delta t} \approx \mathbf{x}_t + \mathbf{f}(\mathbf{x}_t,
t)\Delta t + g(t) \sqrt{\Delta t} \mathbf{z}_t
\]</span> 其中 <span class="math inline">\(\mathbf{z}_t \sim
\mathcal{N}(0, \mathbf{I})\)</span> 是在 <span
class="math inline">\(t\)</span> 时刻采样的标准高斯噪声。</p>
<p>这就是 DDPM <strong>前向加噪过程 (Forward Process)</strong>
的数学原型。</p>
<h3 id="核心数学工具贝叶斯公式-bayes-theorem">3.
核心数学工具：贝叶斯公式 (Bayes’ Theorem)</h3>
<p>贝叶斯公式是连接<strong>前向过程</strong>（加噪）和<strong>反向过程</strong>（去噪）的桥梁。</p>
<p>对于连续变量（概率密度函数）： <span class="math display">\[
p(x|y) = \frac{p(y|x) p(x)}{p(y)}
\]</span> 其中 <span class="math inline">\(p(y) = \int p(y|x) p(x)
dx\)</span>。</p>
<p><strong>在扩散模型中的应用：</strong> * <strong>1.：</strong>
我们定义了一个简单的前向加噪过程 <span class="math inline">\(q(x_t |
x_{t-1})\)</span>（易于计算）。 * <strong>2.：</strong>
我们想要的是反向去噪过程 <span class="math inline">\(p(x_{t-1} |
x_t)\)</span>（难以计算）。 * <strong>3.：</strong>
贝叶斯公式告诉我们：<span class="math inline">\(p(x_{t-1} | x_t) \propto
p(x_t | x_{t-1}) p(x_{t-1})\)</span>。 * <strong>4.：</strong> 在 DDPM
中，我们会看到一个更复杂的形式，它利用了 <span
class="math inline">\(x_0\)</span>： <span class="math display">\[
    q(x_{t-1} | x_t, x_0) = \frac{q(x_t | x_{t-1}, x_0) q(x_{t-1} |
x_0)}{q(x_t | x_0)}
    \]</span></p>
<p><strong>小结</strong></p>
<ol type="1">
<li><strong>高斯分布的性质</strong>（重参数化、加法），能精确计算加噪后的分布。</li>
<li><strong>朗之万动力学与欧拉近似</strong>，为 “逐步加噪”
提供了物理和数学模型。</li>
<li><strong>贝叶斯公式</strong>，指明了如何从 “加噪” 倒推出
“去噪”。</li>
</ol>
<h3 id="ddpm-前向过程从图像到噪声-the-forward-process">2. DDPM
前向过程：从图像到噪声 (The Forward Process)</h3>
<p>前向过程的目标是模拟大纲
中描述的“图片在向量空间中逐步噪声化的轨迹”。我们定义一个<strong>马尔可夫过程</strong>，在该过程中，我们从原始数据
<span class="math inline">\(\mathbf{x}_0 \sim
q(\mathbf{x}_0)\)</span>（即真实图片分布）开始，在 <span
class="math inline">\(T\)</span>
个离散的时间步中逐步向其添加高斯噪声。</p>
<h4 id="单步加噪过程-qmathbfx_t-mathbfx_t-1">2.1 单步加噪过程 <span
class="math inline">\(q(\mathbf{x}_t | \mathbf{x}_{t-1})\)</span></h4>
<p>在每个 <span class="math inline">\(t\)</span> 步，我们向 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span> 添加少量噪声以生成 <span
class="math inline">\(\mathbf{x}_t\)</span>。这个过程被定义为一个高斯转变（这源于我们第
1 部分中对朗之万动力学的离散化）：</p>
<p><span class="math display">\[
q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 -
\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})
\]</span></p>
<ul>
<li><span class="math inline">\(\{\beta_t\}_{t=1}^T\)</span>
是一个预先设定的<strong>方差表 (variance schedule)</strong>。它们是
<span class="math inline">\(T\)</span> 个很小的正常数（例如，<span
class="math inline">\(\beta_1 = 10^{-4}, \beta_T =
0.02\)</span>）。</li>
<li><span class="math inline">\(\sqrt{1 - \beta_t}
\mathbf{x}_{t-1}\)</span>：这是<strong>缩放项</strong>（大纲）。我们在添加噪声之前先将前一步的图片向量“缩小”一点。</li>
<li><span class="math inline">\(\beta_t
\mathbf{I}\)</span>：这是<strong>噪声项</strong>。<span
class="math inline">\(\beta_t\)</span> 是添加的噪声的方差，<span
class="math inline">\(\mathbf{I}\)</span>
是单位矩阵，表示噪声在所有维度（像素）上是独立同分布的。</li>
</ul>
<p><strong>重参数化技巧的应用：</strong> 我们可以使用第 1
部分中的<strong>重参数化技巧</strong>来显式地写出这个采样过程： <span
class="math display">\[
\mathbf{x}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1} + \sqrt{\beta_t}
\boldsymbol{\epsilon}_{t-1}
\]</span> 其中 <span class="math inline">\(\boldsymbol{\epsilon}_{t-1}
\sim \mathcal{N}(0, \mathbf{I})\)</span> 是在 <span
class="math inline">\(t-1\)</span> 时刻采样的一个标准高斯噪声。</p>
<h4 id="累积加噪过程-qmathbfx_t-mathbfx_0-核心推导">2.2 累积加噪过程
<span class="math inline">\(q(\mathbf{x}_t | \mathbf{x}_0)\)</span>
(核心推导)</h4>
<p><strong>问题：</strong> 在训练期间（如大纲
所述），我们希望<strong>随机跳到</strong>任意 <span
class="math inline">\(t\)</span> 步并生成 <span
class="math inline">\(\mathbf{x}_t\)</span>。如果我们必须从 <span
class="math inline">\(\mathbf{x}_0\)</span> 迭代 <span
class="math inline">\(t\)</span> 次，这将非常缓慢。</p>
<p><strong>目标：</strong> 我们需要一个公式，能让我们从 <span
class="math inline">\(\mathbf{x}_0\)</span> <strong>一次性</strong>得到
<span class="math inline">\(\mathbf{x}_t\)</span> 的分布 <span
class="math inline">\(q(\mathbf{x}_t |
\mathbf{x}_0)\)</span>。这就是大纲
中提到的“构造出高斯分布的累计变换”。</p>
<p><strong>推导：</strong> 1. <strong>定义新变量：</strong>
为了简化推导，我们定义 <span class="math inline">\(\alpha_t = 1 -
\beta_t\)</span> 和 <span class="math inline">\(\bar{\alpha}_t =
\prod_{i=1}^t \alpha_i\)</span>。 * <span
class="math inline">\(\alpha_t\)</span> 是每一步的缩放因子。 * <span
class="math inline">\(\bar{\alpha}_t\)</span> 是从第 1 步到第 <span
class="math inline">\(t\)</span> 步的<strong>累积缩放因子</strong>。</p>
<ol start="2" type="1">
<li><p><strong>展开迭代 (Step-by-step Expansion)：</strong> 让我们从
<span class="math inline">\(\mathbf{x}_t\)</span> 开始，逐步代入 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span>： <span
class="math display">\[
\mathbf{x}_t = \sqrt{\alpha_t} \mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}
\boldsymbol{\epsilon}_{t-1}
\]</span> 现在，我们代入 <span class="math inline">\(\mathbf{x}_{t-1} =
\sqrt{\alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_{t-1}}
\boldsymbol{\epsilon}_{t-2}\)</span>： <span class="math display">\[
\begin{aligned}
\mathbf{x}_t &amp;= \sqrt{\alpha_t} (\sqrt{\alpha_{t-1}}
\mathbf{x}_{t-2} + \sqrt{1 - \alpha_{t-1}} \boldsymbol{\epsilon}_{t-2})
+ \sqrt{1 - \alpha_t} \boldsymbol{\epsilon}_{t-1} \\
&amp;= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{\alpha_t(1
- \alpha_{t-1})} \boldsymbol{\epsilon}_{t-2} + \sqrt{1 - \alpha_t}
\boldsymbol{\epsilon}_{t-1}
\end{aligned}
\]</span></p></li>
<li><p><strong>合并高斯噪声 (Merging Gaussians)：</strong>
注意上式的后两项：<span class="math inline">\(\sqrt{\alpha_t(1 -
\alpha_{t-1})} \boldsymbol{\epsilon}_{t-2}\)</span> 和 <span
class="math inline">\(\sqrt{1 - \alpha_t}
\boldsymbol{\epsilon}_{t-1}\)</span>。</p>
<ul>
<li><span class="math inline">\(\boldsymbol{\epsilon}_{t-2}\)</span> 和
<span class="math inline">\(\boldsymbol{\epsilon}_{t-1}\)</span>
是两个<strong>独立</strong>的标准高斯分布。</li>
<li>我们正在对两个独立的、均值为 0 的高斯分布进行线性组合。</li>
<li>根据第 1 部分的<strong>性质 1.3</strong>
(两个独立高斯分布之和)，它们的和仍然是一个均值为 0 的高斯分布。</li>
<li>这个新的高斯分布的<strong>方差</strong>是多少？ <span
class="math display">\[
  \begin{aligned}
  \text{Var}(\text{new\_noise}) &amp;= \text{Var}(\sqrt{\alpha_t(1 -
\alpha_{t-1})} \boldsymbol{\epsilon}_{t-2}) + \text{Var}(\sqrt{1 -
\alpha_t} \boldsymbol{\epsilon}_{t-1}) \\
  &amp;= (\alpha_t(1 - \alpha_{t-1})) \mathbf{I} + (1 - \alpha_t)
\mathbf{I} \\
  &amp;= (\alpha_t - \alpha_t\alpha_{t-1} + 1 - \alpha_t) \mathbf{I} \\
  &amp;= (1 - \alpha_t\alpha_{t-1}) \mathbf{I}
  \end{aligned}
  \]</span></li>
<li>根据重参数化技巧，一个方差为 <span class="math inline">\((1 -
\alpha_t\alpha_{t-1}) \mathbf{I}\)</span> 的高斯分布，可以写成 <span
class="math inline">\(\sqrt{1 - \alpha_t\alpha_{t-1}} \cdot
\bar{\boldsymbol{\epsilon}}_{t-2}\)</span>，其中 <span
class="math inline">\(\bar{\boldsymbol{\epsilon}}_{t-2} \sim
\mathcal{N}(0, \mathbf{I})\)</span> 是一个新的标准高斯噪声。</li>
</ul></li>
<li><p><strong>递归与通项公式：</strong> 我们将合并后的噪声代入第 2
步的展开式： <span class="math display">\[
\mathbf{x}_t = \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 -
\alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2}
\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{x}_t\)</span> 和 <span
class="math inline">\(\mathbf{x}_{t-2}\)</span> 的关系，与 <span
class="math inline">\(\mathbf{x}_t\)</span> 和 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span> 的关系（<span
class="math inline">\(\mathbf{x}_t = \sqrt{\alpha_t} \mathbf{x}_{t-1} +
\sqrt{1 - \alpha_t}
\boldsymbol{\epsilon}_{t-1}\)</span>）在形式上是完全一致的！只是 <span
class="math inline">\(\alpha_t\)</span> 变成了 <span
class="math inline">\(\alpha_t \alpha_{t-1}\)</span>。</li>
<li>我们可以将这个模式递归地应用 <span class="math inline">\(t\)</span>
次： <span class="math display">\[
  \begin{aligned}
  \mathbf{x}_t &amp;= \sqrt{(\alpha_t \alpha_{t-1} \cdots \alpha_1)}
\mathbf{x}_0 + \sqrt{1 - (\alpha_t \alpha_{t-1} \cdots \alpha_1)}
\boldsymbol{\epsilon} \\
  &amp;= \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}
\boldsymbol{\epsilon}
  \end{aligned}
  \]</span></li>
<li>其中 <span class="math inline">\(\boldsymbol{\epsilon} \sim
\mathcal{N}(0, \mathbf{I})\)</span> 是一个（合并了 <span
class="math inline">\(t\)</span> 次的）标准高斯噪声。</li>
</ul></li>
</ol>
<h4 id="前向过程的最终公式">2.3 前向过程的最终公式</h4>
<p>我们得到了前向过程中最关键的<strong>累积加噪公式</strong>： <span
class="math display">\[
q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t;
\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t) \mathbf{I})
\]</span> 这个公式的重参数化形式为： <span class="math display">\[
\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 -
\bar{\alpha}_t} \boldsymbol{\epsilon}
\]</span> <strong>意义：</strong> * <strong>训练效率：</strong>
这个公式是 DDPM 训练效率的<strong>关键</strong>。我们不需要迭代 <span
class="math inline">\(t\)</span> 次来生成 <span
class="math inline">\(\mathbf{x}_t\)</span>。 *
<strong>随机训练：</strong> 在训练神经网络时，我们可以： 1.
从数据集中拿一张清晰图片 <span
class="math inline">\(\mathbf{x}_0\)</span>。 2.
<strong>随机</strong>选择一个时间步 <span
class="math inline">\(t\)</span>（例如 <span
class="math inline">\(t=150\)</span>）。 3. 从 <span
class="math inline">\(\mathcal{N}(0, \mathbf{I})\)</span> 中采样一个噪声
<span class="math inline">\(\boldsymbol{\epsilon}\)</span>。 4.
使用上述公式<strong>一步</strong>计算出 <span
class="math inline">\(\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0
+ \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}\)</span>。 5. 将 <span
class="math inline">\((\mathbf{x}_t, t, \boldsymbol{\epsilon})\)</span>
喂给神经网络进行训练。</p>
<p>当 <span class="math inline">\(t \to T\)</span> 时（例如 <span
class="math inline">\(T=1000\)</span>），<span
class="math inline">\(\bar{\alpha}_T = \prod_{i=1}^T (1 -
\beta_i)\)</span>。由于所有的 <span class="math inline">\(\beta_i &gt;
0\)</span>，<span class="math inline">\(\bar{\alpha}_T\)</span>
会非常接近 0。 此时： <span class="math display">\[
\mathbf{x}_T \approx \sqrt{0} \mathbf{x}_0 + \sqrt{1 - 0}
\boldsymbol{\epsilon} = \boldsymbol{\epsilon}
\]</span> 这意味着，在 <span class="math inline">\(T\)</span>
步之后，<span class="math inline">\(\mathbf{x}_T\)</span> 的分布 <span
class="math inline">\(q(\mathbf{x}_T | \mathbf{x}_0) \approx
\mathcal{N}(0,
\mathbf{I})\)</span>，它几乎完全变成了<strong>标准高斯噪声</strong>，并且与
<span class="math inline">\(\mathbf{x}_0\)</span> 无关。</p>
<p>成功地将复杂的图片分布 <span
class="math inline">\(q(\mathbf{x}_0)\)</span>
转化为了简单的标准高斯分布 <span
class="math inline">\(q(\mathbf{x}_T)\)</span>。</p>
<p><strong>小结</strong></p>
<p>核心问题：如何<strong>逆转</strong>这个过程？如何从一张纯噪声图片
<span class="math inline">\(\mathbf{x}_T \sim \mathcal{N}(0,
\mathbf{I})\)</span> 出发，一步步去噪，最终得到一张清晰的图片 <span
class="math inline">\(\mathbf{x}_0\)</span>？</p>
<p>这需要推导<strong>反向去噪过程 (Reverse Process)</strong> <span
class="math inline">\(p_\theta(\mathbf{x}_{t-1} |
\mathbf{x}_t)\)</span>。</p>
<h3 id="反向过程从噪声到图像-the-reverse-process">3.
反向过程：从噪声到图像 (The Reverse Process)</h3>
<p>我们的目标是学习反向的马尔可夫链，即 <span
class="math inline">\(p_\theta(\mathbf{x}_{t-1} |
\mathbf{x}_t)\)</span>。</p>
<h4 id="棘手的目标-pmathbfx_t-1-mathbfx_t">3.1 棘手的目标 <span
class="math inline">\(p(\mathbf{x}_{t-1} | \mathbf{x}_t)\)</span></h4>
<p>我们想从 <span class="math inline">\(\mathbf{x}_t\)</span> 推导出
<span class="math inline">\(\mathbf{x}_{t-1}\)</span>。根据贝叶斯公式：
<span class="math display">\[
p(\mathbf{x}_{t-1} | \mathbf{x}_t) = \frac{p(\mathbf{x}_t |
\mathbf{x}_{t-1}) p(\mathbf{x}_{t-1})}{p(\mathbf{x}_t)}
\]</span> * <span class="math inline">\(p(\mathbf{x}_t |
\mathbf{x}_{t-1})\)</span> 就是前向过程 <span
class="math inline">\(q(\mathbf{x}_t |
\mathbf{x}_{t-1})\)</span>，我们已知。 * <span
class="math inline">\(p(\mathbf{x}_{t-1})\)</span> 是 <span
class="math inline">\(t-1\)</span> 时刻的边缘分布，需要对所有 <span
class="math inline">\(\mathbf{x}_0\)</span> 积分 <span
class="math inline">\(p(\mathbf{x}_{t-1}) = \int q(\mathbf{x}_{t-1} |
\mathbf{x}_0) q(\mathbf{x}_0) d\mathbf{x}_0\)</span>，这依赖于 <span
class="math inline">\(q(\mathbf{x}_0)\)</span>（真实数据分布），<strong>极其困难
(intractable)</strong>。 * <span
class="math inline">\(p(\mathbf{x}_t)\)</span> 同样难以计算。</p>
<h4 id="ddpm-的核心创见利用-mathbfx_0-对应">3.2 DDPM 的核心创见：利用
<span class="math inline">\(\mathbf{x}_0\)</span> (对应)</h4>
<p><strong>关键洞察：</strong> 虽然 <span
class="math inline">\(p(\mathbf{x}_{t-1} | \mathbf{x}_t)\)</span>
难以计算，但<strong>如果我们额外知道 <span
class="math inline">\(\mathbf{x}_0\)</span></strong>，这个后验分布 <span
class="math inline">\(q(\mathbf{x}_{t-1} | \mathbf{x}_t,
\mathbf{x}_0)\)</span> <strong>是可计算的</strong>！</p>
<p>为什么？因为我们定义了所有 <span class="math inline">\(q\)</span>
的前向步骤。我们再次使用贝叶斯公式 (对应)： <span
class="math display">\[
q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) = \frac{q(\mathbf{x}_t
| \mathbf{x}_{t-1}, \mathbf{x}_0) \cdot q(\mathbf{x}_{t-1} |
\mathbf{x}_0)}{q(\mathbf{x}_t | \mathbf{x}_0)}
\]</span> 利用马尔可夫性质 <span class="math inline">\(q(\mathbf{x}_t |
\mathbf{x}_{t-1}, \mathbf{x}_0) = q(\mathbf{x}_t |
\mathbf{x}_{t-1})\)</span>，我们得到： <span class="math display">\[
q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) \propto q(\mathbf{x}_t
| \mathbf{x}_{t-1}) \cdot q(\mathbf{x}_{t-1} | \mathbf{x}_0)
\]</span> 我们已知这三个分布都是高斯分布（在第 2 部分已推导）： 1. <span
class="math inline">\(q(\mathbf{x}_t | \mathbf{x}_{t-1}) =
\mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t} \mathbf{x}_{t-1}, \beta_t
\mathbf{I})\)</span> 2. <span class="math inline">\(q(\mathbf{x}_{t-1} |
\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \sqrt{\bar{\alpha}_{t-1}}
\mathbf{x}_0, (1 - \bar{\alpha}_{t-1}) \mathbf{I})\)</span> 3. <span
class="math inline">\(q(\mathbf{x}_t | \mathbf{x}_0) =
\mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 -
\bar{\alpha}_t) \mathbf{I})\)</span></p>
<p>我们正在用 <span class="math inline">\(\mathbf{x}_{t-1}\)</span>
作为变量，乘以两个高斯分布的概率密度函数 (PDF)。高斯 PDF 的形式是 <span
class="math inline">\(C \cdot \exp(-\frac{(x -
\mu)^2}{2\sigma^2})\)</span>。两个高斯 PDF
相乘的结果<strong>仍然是一个高斯分布</strong>。</p>
<p>通过匹配 <span class="math inline">\(\mathbf{x}_{t-1}\)</span>
的一次项和二次项系数（一个繁琐但直接的代数过程），我们可以解出这个新高斯分布的均值
<span class="math inline">\(\tilde{\boldsymbol{\mu}}_t\)</span> 和方差
<span class="math inline">\(\tilde{\beta}_t\)</span>：</p>
<p><span class="math display">\[
q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) =
\mathcal{N}(\mathbf{x}_{t-1}; \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t,
\mathbf{x}_0), \tilde{\beta}_t \mathbf{I})
\]</span> 其中： * <strong>方差 <span
class="math inline">\(\tilde{\beta}_t\)</span></strong>：不依赖于 <span
class="math inline">\(\mathbf{x}_t\)</span> 或 <span
class="math inline">\(\mathbf{x}_0\)</span>，它是一个固定的超参数。
<span class="math display">\[
    \tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t}
\cdot \beta_t
    \]</span> * <strong>均值 <span
class="math inline">\(\tilde{\boldsymbol{\mu}}_t\)</span></strong>：依赖于
<span class="math inline">\(\mathbf{x}_t\)</span> 和 <span
class="math inline">\(\mathbf{x}_0\)</span>。 <span
class="math display">\[
    \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) =
\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0
+ \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}
\mathbf{x}_t
    \]</span></p>
<h3 id="训练学习反向过程-training">4. 训练：学习反向过程 (Training)</h3>
<h4 id="神经网络的目标">4.1 神经网络的目标</h4>
<p>我们有了一个完美的目标分布 <span
class="math inline">\(q(\mathbf{x}_{t-1} | \mathbf{x}_t,
\mathbf{x}_0)\)</span>。但它有个问题：在<strong>推理
(Inference)</strong> 时，我们从 <span
class="math inline">\(\mathbf{x}_T\)</span> 开始，并不知道 <span
class="math inline">\(\mathbf{x}_0\)</span>。</p>
<p>因此，我们训练一个神经网络 <span
class="math inline">\(p_\theta\)</span>
来<strong>近似</strong>这个分布： <span class="math display">\[
p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) =
\mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t),
\boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))
\]</span> 我们的目标是让 <span class="math inline">\(p_\theta\)</span>
尽可能接近 <span class="math inline">\(q\)</span>。 * <strong>简化1
(固定方差)</strong>：DDPM 论文发现，将神经网络的方差 <span
class="math inline">\(\boldsymbol{\Sigma}_\theta\)</span> 固定为 <span
class="math inline">\(\tilde{\beta}_t \mathbf{I}\)</span> 或 <span
class="math inline">\(\beta_t \mathbf{I}\)</span>
效果最好。这极大地简化了问题：<strong>神经网络只需要学习均值 <span
class="math inline">\(\boldsymbol{\mu}_\theta\)</span></strong>。 *
<strong>简化2 (学习目标)</strong>：我们训练 <span
class="math inline">\(\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)\)</span>
来预测真实均值 <span
class="math inline">\(\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t,
\mathbf{x}_0)\)</span>。</p>
<h4 id="ddpm-的关键改进预测噪声-对应">4.2 DDPM 的关键改进：预测噪声
(对应)</h4>
<p><span class="math inline">\(\tilde{\boldsymbol{\mu}}_t\)</span>
的公式 <span
class="math inline">\(\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 -
\bar{\alpha}_t} \mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1 -
\bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t\)</span>
仍然很复杂。</p>
<p><strong>DDPM 论文提出了一个重要的的重参数化：</strong> 我们回顾第 2
部分的前向公式：<span class="math inline">\(\mathbf{x}_t =
\sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}
\boldsymbol{\epsilon}\)</span> 我们可以用它来反解 <span
class="math inline">\(\mathbf{x}_0\)</span>（在 <span
class="math inline">\(\mathbf{x}_t\)</span> 和 <span
class="math inline">\(\boldsymbol{\epsilon}\)</span> 已知的情况下）：
<span class="math display">\[
\mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} (\mathbf{x}_t - \sqrt{1 -
\bar{\alpha}_t} \boldsymbol{\epsilon})
\]</span> 现在，我们将这个 <span
class="math inline">\(\mathbf{x}_0\)</span> 的表达式代入上面 <span
class="math inline">\(\tilde{\boldsymbol{\mu}}_t\)</span> 的复杂公式中：
<span class="math display">\[
\begin{aligned}
\tilde{\boldsymbol{\mu}}_t &amp;=
\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \left(
\frac{1}{\sqrt{\bar{\alpha}_t}} (\mathbf{x}_t - \sqrt{1 -
\bar{\alpha}_t} \boldsymbol{\epsilon}) \right) + \frac{\sqrt{\alpha_t}(1
- \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t \\
&amp;= \left( \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{(1 -
\bar{\alpha}_t)\sqrt{\bar{\alpha}_t}} + \frac{\sqrt{\alpha_t}(1 -
\bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \right) \mathbf{x}_t - \left(
\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t \sqrt{1 - \bar{\alpha}_t}}{(1 -
\bar{\alpha}_t)\sqrt{\bar{\alpha}_t}} \right) \boldsymbol{\epsilon}
\end{aligned}
\]</span> (经过一系列基于 <span class="math inline">\(\bar{\alpha}_t =
\alpha_t \bar{\alpha}_{t-1}\)</span> 和 <span
class="math inline">\(\beta_t = 1 - \alpha_t\)</span> 的代数化简) <span
class="math display">\[
\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}} \left(
\mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}}
\boldsymbol{\epsilon} \right)
\]</span> <strong>分析这个优美的公式：</strong> * <span
class="math inline">\(\alpha_t\)</span>, <span
class="math inline">\(\beta_t\)</span>, <span
class="math inline">\(\bar{\alpha}_t\)</span> 都是预先设定的超参数。 *
<span class="math inline">\(\mathbf{x}_t\)</span> 是神经网络的输入。 *
<strong>唯一未知的就是 <span
class="math inline">\(\boldsymbol{\epsilon}\)</span></strong> ——
那个在第 2 部分用于从 <span class="math inline">\(\mathbf{x}_0\)</span>
生成 <span class="math inline">\(\mathbf{x}_t\)</span>
的<strong>原始噪声</strong>。</p>
<p><strong>结论（DDPM 核心思想）：</strong> (对应) 与其让神经网络 <span
class="math inline">\(\boldsymbol{\mu}_\theta\)</span>
预测那个复杂的均值 <span
class="math inline">\(\tilde{\boldsymbol{\mu}}_t\)</span>，我们可以让它转而去预测这个<strong>噪声
<span class="math inline">\(\boldsymbol{\epsilon}\)</span></strong>。
我们定义一个神经网络（通常是 U-Net 结构）<span
class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,
t)\)</span>，它的目标就是预测 <span
class="math inline">\(\boldsymbol{\epsilon}\)</span>。</p>
<h4 id="训练流程与损失函数-对应-012440">4.3 训练流程与损失函数
(对应-[01:24:40])</h4>
<ol type="1">
<li>从数据集中随机抽取一张清晰图像 <span
class="math inline">\(\mathbf{x}_0\)</span>。</li>
<li><strong>随机</strong>选择一个时间步 <span
class="math inline">\(t\)</span>（从 1 到 <span
class="math inline">\(T\)</span>）。(对应 随机训练)</li>
<li><strong>随机</strong>采样一个标准高斯噪声 <span
class="math inline">\(\boldsymbol{\epsilon} \sim \mathcal{N}(0,
\mathbf{I})\)</span>。(对应)</li>
<li>使用前向公式<strong>一步</strong>生成加噪图像：<span
class="math inline">\(\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0
+ \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}\)</span>。</li>
<li>将 <span class="math inline">\((\mathbf{x}_t, t)\)</span>
作为输入，喂给神经网络 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,
t)\)</span>，得到预测噪声 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span>。</li>
<li>计算损失函数（均方误差 MSE）：(对应) <span class="math display">\[
L(\theta) = E_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ ||
\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)
||^2 \right]
\]</span></li>
<li>使用梯度下降更新网络参数 <span
class="math inline">\(\theta\)</span>。</li>
</ol>
<h3 id="推理逐步去噪生成图像-inference">5. 推理：逐步去噪生成图像
(Inference)</h3>
<p>当训练好 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span>
后，就可以从纯噪声生成图像了：</p>
<ol type="1">
<li><strong>起始：</strong> 从标准高斯分布中采样一张纯噪声图像 <span
class="math inline">\(\mathbf{x}_T \sim \mathcal{N}(0,
\mathbf{I})\)</span>。</li>
<li><strong>迭代：</strong> <strong>从 <span class="math inline">\(t =
T\)</span> 循环到 <span class="math inline">\(t = 1\)</span></strong>：
<ol type="a">
<li>将当前的 <span class="math inline">\(\mathbf{x}_t\)</span> 和时间步
<span class="math inline">\(t\)</span> 输入网络，得到噪声预测：<span
class="math inline">\(\boldsymbol{\epsilon}_\theta =
\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\)</span>。(对应)</li>
<li>使用 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span> 作为我们对
<span class="math inline">\(\boldsymbol{\epsilon}\)</span>
的最佳估计，代入 <strong>4.2</strong> 中的均值公式，计算 <span
class="math inline">\(t-1\)</span> 步的<strong>均值</strong> <span
class="math inline">\(\boldsymbol{\mu}_\theta(\mathbf{x}_t,
t)\)</span>：(对应) <span class="math display">\[
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}}
\left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}}
\boldsymbol{\epsilon}_\theta \right)
\]</span></li>
<li>计算 <span class="math inline">\(t-1\)</span>
步的<strong>方差</strong>。我们使用固定的方差 <span
class="math inline">\(\sigma_t^2 \mathbf{I} = \tilde{\beta}_t \mathbf{I}
= \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t
\mathbf{I}\)</span>。</li>
<li><strong>采样 (Sampling)</strong> (对应)：从这个高斯分布中采样 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span>： <span
class="math display">\[
\mathbf{x}_{t-1} = \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) + \sigma_t
\mathbf{z}
\]</span> 其中 <span class="math inline">\(\mathbf{z} \sim
\mathcal{N}(0, \mathbf{I})\)</span> 是一个新采样的随机噪声。
（<em>注意：当 <span class="math inline">\(t=1\)</span> 时，<span
class="math inline">\(\mathbf{z}\)</span> 设为 0，因为 <span
class="math inline">\(\mathbf{x}_0\)</span>
应该是一个确定性的输出，不再添加噪声</em>）。</li>
</ol></li>
<li><strong>结束：</strong> 当循环结束时，<span
class="math inline">\(\mathbf{x}_0\)</span>
就是生成的清晰图像。(对应)</li>
</ol>
<p><strong>小结</strong></p>
<p>完整地推导了 DDPM 的核心数学原理： 1. <strong>前向过程 <span
class="math inline">\(q\)</span></strong>：使用 <span
class="math inline">\(q(\mathbf{x}_t | \mathbf{x}_0)\)</span> 高效加噪。
2. <strong>反向过程 <span
class="math inline">\(p_\theta\)</span></strong>：通过贝叶斯公式推导出
<span class="math inline">\(q(\mathbf{x}_{t-1} | \mathbf{x}_t,
\mathbf{x}_0)\)</span> 作为理想目标。 3. <strong>训练 <span
class="math inline">\(p_\theta\)</span></strong>：通过让 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,
t)\)</span> 预测真实噪声 <span
class="math inline">\(\boldsymbol{\epsilon}\)</span> 来简化训练目标 (MSE
Loss)。 4. <strong>推理 <span
class="math inline">\(p_\theta\)</span></strong>：从 <span
class="math inline">\(\mathbf{x}_T\)</span> 开始，利用 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span>
预测的均值，逐步采样 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span>。</p>
<p>DDPM 的一个主要缺点是<strong>推理速度慢</strong>（需要 <span
class="math inline">\(T\)</span> 步，例如 1000 步）。</p>
<p><strong>DDIM (Denoising Diffusion Implicit Models)</strong>。</p>
<p>DDPM 的效果很好，但它有两个主要缺点： 1.
<strong>推理速度慢：</strong> 它是一个马尔可夫过程，从 <span
class="math inline">\(\mathbf{x}_T\)</span> 生成 <span
class="math inline">\(\mathbf{x}_0\)</span> 必须执行 <span
class="math inline">\(T\)</span> 步（例如 1000 步）采样，非常耗时。 2.
<strong>推理是随机的：</strong> (Stochastic) 在每一步采样 <span
class="math inline">\(\mathbf{x}_{t-1} =
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) + \sigma_t \mathbf{z}\)</span>
时，都需要加入一个新的随机噪声 <span
class="math inline">\(\mathbf{z}\)</span>。这意味着即使从同一个 <span
class="math inline">\(\mathbf{x}_T\)</span> 出发，两次推理也会得到不同的
<span
class="math inline">\(\mathbf{x}_0\)</span>。这对于需要一致性的任务（如图像编辑）来说是个问题。</p>
<p>DDIM（2020年提出）巧妙地解决了这两个问题，并且<strong>无需重新训练</strong>在
DDPM 上训练好的模型。</p>
<p>这对应于您大纲中的 - 部分。</p>
<h3 id="ddim推理升级">6. DDIM：推理升级</h3>
<h4 id="ddim-的核心洞察重新审视反向过程">6.1 DDIM
的核心洞察：重新审视反向过程</h4>
<p>DDIM 的出发点是重新审视我们推导出的反向过程。DDPM 假设反向过程是
<span class="math inline">\(p_\theta(\mathbf{x}_{t-1} |
\mathbf{x}_t)\)</span>，并用它来近似 <span
class="math inline">\(q(\mathbf{x}_{t-1} | \mathbf{x}_t,
\mathbf{x}_0)\)</span>。</p>
<p>DDIM 注意到，我们训练的神经网络 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,
t)\)</span> 实际上是在预测噪声 <span
class="math inline">\(\boldsymbol{\epsilon}\)</span>。
回顾我们的前向公式： <span class="math display">\[
\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 -
\bar{\alpha}_t} \boldsymbol{\epsilon}
\]</span> 既然我们有了 <span
class="math inline">\(\mathbf{x}_t\)</span>（当前输入）和 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span>（网络预测的
<span
class="math inline">\(\boldsymbol{\epsilon}\)</span>），我们可以直接反解出<strong>对清晰图像
<span class="math inline">\(\mathbf{x}_0\)</span>
的预测</strong>，我们称之为 <span
class="math inline">\(\hat{\mathbf{x}}_0\)</span>：</p>
<p><span class="math display">\[
\hat{\mathbf{x}}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( \mathbf{x}_t
- \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t,
t) \right)
\]</span> 这个 <span class="math inline">\(\hat{\mathbf{x}}_0\)</span>
是给定 <span class="math inline">\(\mathbf{x}_t\)</span>
时，模型对最终结果 <span class="math inline">\(\mathbf{x}_0\)</span>
的“最佳猜测”。</p>
<h4 id="升级-1确定性推理-deterministic-inference-对应">6.2 升级
1：确定性推理 (Deterministic Inference) (对应)</h4>
<p>DDPM 的采样公式为： <span class="math display">\[
\mathbf{x}_{t-1} = \underbrace{\boldsymbol{\mu}_\theta(\mathbf{x}_t,
t)}_{\text{均值}} + \underbrace{\sigma_t \mathbf{z}}_{\text{随机噪声}}
\]</span> DDIM 提出，这个过程不一定是随机的。DDIM 引入了一个新的参数
<span class="math inline">\(\eta\)</span> (eta) 来控制随机性。 * 当
<span class="math inline">\(\eta=1\)</span> 时，DDIM 的采样过程与 DDPM
完全相同（随机）。 * 当 <span class="math inline">\(\eta=0\)</span>
时，采样过程中的方差 <span class="math inline">\(\sigma_t\)</span>
被设为 0。</p>
<p><strong>当 <span class="math inline">\(\eta=0\)</span>（方差 <span
class="math inline">\(\sigma_t=0\)</span>）时</strong>，采样步骤变为：
<span class="math display">\[
\mathbf{x}_{t-1} = \boldsymbol{\mu}_\theta(\mathbf{x}_t, t)
\]</span> <strong>这是确定性的！</strong> 没有随机噪声 <span
class="math inline">\(\mathbf{z}\)</span> 的介入。</p>
<p><strong>这有什么用？</strong> 这意味着从一个固定的 <span
class="math inline">\(\mathbf{x}_T\)</span>
出发，无论运行多少次，<strong>总会生成完全相同的 <span
class="math inline">\(\mathbf{x}_0\)</span></strong>。这使得扩散模型可用于图像编辑、风格转换等需要保持一致性的任务。</p>
<p>DDIM 论文推导出了一个更通用的采样公式，它不依赖于 <span
class="math inline">\(\boldsymbol{\mu}_\theta\)</span> 而是直接使用
<span class="math inline">\(\hat{\mathbf{x}}_0\)</span> 和 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span>。 当 <span
class="math inline">\(\eta=0\)</span> (即 <span
class="math inline">\(\sigma_t = 0\)</span>) 时，从 <span
class="math inline">\(\mathbf{x}_t\)</span> 到 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span>
的<strong>确定性采样公式</strong>为：</p>
<p><span class="math display">\[
\mathbf{x}_{t-1} = \underbrace{\sqrt{\bar{\alpha}_{t-1}}
\hat{\mathbf{x}}_0}_{\text{指向“预测的” } \mathbf{x}_0} +
\underbrace{\sqrt{1 - \bar{\alpha}_{t-1}} \cdot \left(
\frac{\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \hat{\mathbf{x}}_0}{\sqrt{1 -
\bar{\alpha}_t}} \right)}_{\text{指向“当前的” } \mathbf{x}_t \text{
的方向}}
\]</span> (注意：<span class="math inline">\(\frac{\mathbf{x}_t -
\sqrt{\bar{\alpha}_t} \hat{\mathbf{x}}_0}{\sqrt{1 -
\bar{\alpha}_t}}\)</span> 正好等于 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span> )
所以，确定性（<span class="math inline">\(\eta=0\)</span>）的 DDIM
采样步骤也可以写为： <span class="math display">\[
\mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \hat{\mathbf{x}}_0 +
\sqrt{1 - \bar{\alpha}_{t-1}} \cdot \boldsymbol{\epsilon}_\theta
\]</span></p>
<h4 id="升级-2跳步采样-skip-sampling-对应">6.3 升级 2：跳步采样 (Skip
Sampling) (对应)</h4>
<p>DDPM 必须一步一步 <span class="math inline">\(t \to t-1 \to t-2
\dots\)</span> 地采样，因为它是马尔可夫过程。</p>
<p>DDIM
的采样公式（如上所示）是<strong>非马尔可夫的</strong>。它不依赖于 <span
class="math inline">\(q(\mathbf{x}_{t-1} | \mathbf{x}_t,
\mathbf{x}_0)\)</span>，而是直接使用在 <span
class="math inline">\(t\)</span> 时刻预测的 <span
class="math inline">\(\hat{\mathbf{x}}_0\)</span> 来计算 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span>。</p>
<p><strong>关键洞察：</strong> 既然我们能从 <span
class="math inline">\(\mathbf{x}_t\)</span> 预测出 <span
class="math inline">\(\hat{\mathbf{x}}_0\)</span>，我们不仅能计算 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span>，我们能计算<strong>任意</strong>
<span class="math inline">\(\mathbf{x}_{\tau}\)</span> (其中 <span
class="math inline">\(\tau &lt; t\)</span>)。</p>
<p>这使得<strong>跳步采样</strong>成为可能。我们不再需要完整的 <span
class="math inline">\(T=1000\)</span>
步，我们可以定义一个更短的子序列，例如 <span
class="math inline">\(S=20\)</span> 步： <span
class="math inline">\((\tau_1, \tau_2, \dots, \tau_S) = (1, 51, 101,
\dots, 951)\)</span></p>
<p>我们的推理循环不再是 <code>for t in (T...1)</code>，而是
<code>for i in (S...1)</code>： * <strong>当前步</strong>：<span
class="math inline">\(\tau_i\)</span> (例如 <span
class="math inline">\(\tau_{20} = 951\)</span>) *
<strong>目标步</strong>：<span class="math inline">\(\tau_{i-1}\)</span>
(例如 <span class="math inline">\(\tau_{19} = 901\)</span>)</p>
<p><strong>DDIM 跳步采样（确定性）公式：</strong> (对应)</p>
<ol type="1">
<li><strong>输入：</strong> 当前噪声图像 <span
class="math inline">\(\mathbf{x}_{\tau_i}\)</span> 和时间步 <span
class="math inline">\(\tau_i\)</span>。</li>
<li><strong>预测 <span
class="math inline">\(\hat{\mathbf{x}}_0\)</span>：</strong>
(与之前相同) <span class="math display">\[
\hat{\mathbf{x}}_0 = \frac{1}{\sqrt{\bar{\alpha}_{\tau_i}}} \left(
\mathbf{x}_{\tau_i} - \sqrt{1 - \bar{\alpha}_{\tau_i}}
\boldsymbol{\epsilon}_\theta(\mathbf{x}_{\tau_i}, \tau_i) \right)
\]</span></li>
<li><strong>计算 <span
class="math inline">\(\mathbf{x}_{\tau_{i-1}}\)</span>：</strong>
(使用确定性公式，将 <span class="math inline">\(t-1\)</span> 替换为
<span class="math inline">\(\tau_{i-1}\)</span>) <span
class="math display">\[
\mathbf{x}_{\tau_{i-1}} = \sqrt{\bar{\alpha}_{\tau_{i-1}}}
\hat{\mathbf{x}}_0 + \sqrt{1 - \bar{\alpha}_{\tau_{i-1}}} \cdot
\boldsymbol{\epsilon}_\theta(\mathbf{x}_{\tau_i}, \tau_i)
\]</span></li>
</ol>
<p><strong>结果：</strong> 我们不再需要 1000
步计算，而是通过<strong>跳步</strong>，仅用 20 步就完成了从 <span
class="math inline">\(\mathbf{x}_T\)</span> 到 <span
class="math inline">\(\mathbf{x}_0\)</span> 的生成。这极大地（例如 50
倍）提升了推理速度。</p>
<p><strong>小结</strong></p>
<p>DDIM 是对 DDPM 的一次重大升级，它通过引入 <span
class="math inline">\(\hat{\mathbf{x}}_0\)</span>
预测和非马尔可夫采样，实现了： 1. <strong>确定性推理</strong>（<span
class="math inline">\(\eta=0\)</span>），增强了模型的可控性。 2.
<strong>跳步采样</strong>，极大缩短了推理时间。 3.
最重要的是，它<strong>复用</strong>了 DDPM 训练好的 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span>
模型，无需额外训练。</p>
<p>好的，我们来探讨扩散模型演进的下一个重要阶段：<strong>流匹配 (Flow
Matching)</strong>。</p>
<p>在 DDPM 和 DDIM 中，我们都依赖于一个<strong>离散时间</strong>的
SDE（随机微分方程）或其确定性版本。我们模拟了 <span
class="math inline">\(T\)</span> 个离散步骤（例如 <span
class="math inline">\(T=1000\)</span>），这在数学上是有效的，但在概念上有些繁琐，并且依赖于
<span class="math inline">\(\beta_t\)</span> (或 <span
class="math inline">\(\bar{\alpha}_t\)</span>)
这个人工设计的“噪声表”。</p>
<p>流匹配 (Flow Matching, FM)
模型（2022年及后续工作）提出了一种更简洁、更根本的视角：<strong>连续时间</strong>的<strong>常微分方程
(ODE)</strong>。</p>
<p>这对应于您大纲中的 - 部分。</p>
<h3 id="流匹配连续轨迹与速度预测">7. 流匹配：连续轨迹与速度预测</h3>
<h4 id="核心思想从-sde-到-ode">7.1 核心思想：从 SDE 到 ODE</h4>
<ul>
<li><strong>DDPM (SDE)</strong>：将图像 <span
class="math inline">\(\mathbf{x}_0\)</span> 变为噪声 <span
class="math inline">\(\mathbf{x}_T\)</span> 的过程是随机的（<span
class="math inline">\(\mathbf{x}_t = \sqrt{\alpha_t} \mathbf{x}_{t-1} +
\sqrt{\beta_t} \boldsymbol{\epsilon}\)</span>）。</li>
<li><strong>流匹配
(ODE)</strong>：我们构建一个<strong>确定性的</strong>、<strong>连续的</strong>“流”，将纯噪声
<span class="math inline">\(\mathbf{z}\)</span>（我们这里称为 <span
class="math inline">\(\mathbf{x}_0\)</span>）平滑地转变为清晰图像 <span
class="math inline">\(\mathbf{x}_1\)</span>。</li>
</ul>
<p>我们不再考虑离散步骤 <span class="math inline">\(t=1, 2, \dots,
T\)</span>，而是考虑一个连续时间 <span class="math inline">\(t \in [0,
1]\)</span>：</p>
<ul>
<li><span class="math inline">\(t=0\)</span>：<span
class="math inline">\(\mathbf{x}_0\)</span> 是从 <span
class="math inline">\(\mathcal{N}(0, \mathbf{I})\)</span>
采样的纯噪声。</li>
<li><span class="math inline">\(t=1\)</span>：<span
class="math inline">\(\mathbf{x}_1\)</span>
是我们想要生成的清晰图像。</li>
</ul>
<p>这个从 <span class="math inline">\(\mathbf{x}_0\)</span> 到 <span
class="math inline">\(\mathbf{x}_1\)</span> 的连续演变路径 <span
class="math inline">\(\mathbf{x}_t\)</span> 由一个<strong>常微分方程
(ODE)</strong> 描述：</p>
<p><span class="math display">\[
\frac{d\mathbf{x}_t}{dt} = \mathbf{v}(\mathbf{x}_t, t)
\]</span> * <span class="math inline">\(\mathbf{v}(\mathbf{x}_t,
t)\)</span> 是一个<strong>速度向量场 (velocity vector field)</strong>。
* 它告诉我们：当一个点位于位置 <span
class="math inline">\(\mathbf{x}_t\)</span> 和时间 <span
class="math inline">\(t\)</span>
时，它应该往哪个方向（向量）以多快的速度（模长）移动。 *
<strong>训练目标：</strong> 我们的神经网络 <span
class="math inline">\(\mathbf{v}_\theta(\mathbf{x}_t, t)\)</span>
的目标就是学习这个<strong>速度场 <span
class="math inline">\(\mathbf{v}\)</span></strong>，而不是像 DDPM
那样学习噪声 <span
class="math inline">\(\boldsymbol{\epsilon}\)</span>。</p>
<h4 id="训练学习速度场-对应">7.2 训练：学习速度场 (对应)</h4>
<p><strong>问题：</strong> 理论上存在一个理想的速度场 <span
class="math inline">\(\mathbf{v}\)</span>
可以将噪声分布“推向”图像分布，但这个理想的 <span
class="math inline">\(\mathbf{v}\)</span> 非常复杂，我们无法知道。</p>
<p><strong>流匹配的创见：</strong>
我们不需要知道那个复杂的理想场。我们可以<strong>自己定义</strong>无数条简单的路径，然后训练网络来学习这些简单路径的平均速度。</p>
<p><strong>1. 定义简单路径（直线模型）：</strong> 给定一个噪声 <span
class="math inline">\(\mathbf{x}_0 \sim \mathcal{N}(0,
\mathbf{I})\)</span> 和一张真实图像 <span
class="math inline">\(\mathbf{x}_1 \sim
q(\text{data})\)</span>，连接它们的最简单路径是什么？<strong>一条直线</strong>。</p>
<p><span class="math display">\[\\mathbf{x}\_t = (1 - t) \\mathbf{x}\_0
+ t \\mathbf{x}\_1
\]</span> * 当 <span class="math inline">\(t=0\)</span> 时，<span
class="math inline">\(\mathbf{x}_t = \mathbf{x}_0\)</span> (噪声)。</p>
<ul>
<li>当 <span class="math inline">\(t=1\)</span> 时，<span
class="math inline">\(\mathbf{x}_t = \mathbf{x}_1\)</span> (图像)。</li>
</ul>
<p><strong>2. 计算目标速度：</strong> 如果我们的“粒子” <span
class="math inline">\(\mathbf{x}_t\)</span> 沿着这条直线路径运动，它在
<span class="math inline">\(t\)</span> 时刻的速度 <span
class="math inline">\(\mathbf{v}_t\)</span> 是多少？我们对 <span
class="math inline">\(t\)</span> 求导：</p>
<p><span class="math display">\[
\mathbf{v}_t = \frac{d\mathbf{x}_t}{dt} = \frac{d}{dt} \left( (1 - t)
\mathbf{x}_0 + t \mathbf{x}_1 \right)
\]</span><span class="math display">\[
\mathbf{v}_t = -\mathbf{x}_0 + \mathbf{x}_1 = \mathbf{x}_1 -
\mathbf{x}_0
\]</span><strong>这就是流匹配的训练目标！</strong>
沿着这条直线路径，在任何时间 <span
class="math inline">\(t\)</span>，目标速度都是恒定的 <span
class="math inline">\(\mathbf{x}_1 - \mathbf{x}_0\)</span>。</p>
<p><strong>3. 训练流程：</strong></p>
<ol type="1">
<li>从数据集中随机抽取一张清晰图像 <span
class="math inline">\(\mathbf{x}_1\)</span>。</li>
<li>随机采样一个标准高斯噪声 <span class="math inline">\(\mathbf{x}_0
\sim \mathcal{N}(0, \mathbf{I})\)</span>。</li>
<li><strong>随机</strong>选择一个时间 <span
class="math inline">\(t\)</span>（从 <span class="math inline">\(U(0,
1)\)</span> 均匀采样）。</li>
<li>使用直线公式<strong>一步</strong>计算出路径上的点：<span
class="math inline">\(\mathbf{x}_t = (1 - t) \mathbf{x}_0 + t
\mathbf{x}_1\)</span>。</li>
<li>将 <span class="math inline">\((\mathbf{x}_t, t)\)</span>
作为输入，喂给神经网络 <span
class="math inline">\(\mathbf{v}_\theta(\mathbf{x}_t,
t)\)</span>，得到预测速度 <span
class="math inline">\(\mathbf{v}_\theta\)</span>。</li>
<li>计算损失函数（均方误差 MSE）： $$</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$$L(\\theta) = E\_&#123;t, \\mathbf&#123;x&#125;\_0, \\mathbf&#123;x&#125;\_1&#125; \\left[ || (\\mathbf&#123;x&#125;\_1 - \\mathbf&#123;x&#125;*0) - \\mathbf&#123;v&#125;*\\theta(\\mathbf&#123;x&#125;\_t, t) ||^2 \\right]</span><br><span class="line">$$</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>
<ol start="7" type="1">
<li>使用梯度下降更新网络参数 <span
class="math inline">\(\theta\)</span>。</li>
</ol>
<h4 id="推理求解-ode-对应">7.3 推理：求解 ODE (对应)</h4>
<p>当我们训练好 <span class="math inline">\(\mathbf{v}_\theta\)</span>
后，我们就有了一个完整的速度场，它知道在时空中的任何点 <span
class="math inline">\((\mathbf{x}, t)\)</span> 应该如何移动。</p>
<p><strong>问题：</strong> 如何从 <span
class="math inline">\(\mathbf{x}_0\)</span> 积分到 <span
class="math inline">\(\mathbf{x}_1\)</span>？ 我们需要求解 ODE <span
class="math inline">\(\frac{d\mathbf{x}_t}{dt} =
\mathbf{v}_\theta(\mathbf{x}_t, t)\)</span>，从 <span
class="math inline">\(t=0\)</span> 求解到 <span
class="math inline">\(t=1\)</span>。</p>
<p><strong>方法：</strong>
我们使用数值积分方法，最简单的就是<strong>欧拉近似法</strong>（我们在第
1 部分 提到过）。</p>
<ol type="1">
<li><strong>起始：</strong> 随机采样一张纯噪声图像 <span
class="math inline">\(\mathbf{x}_0 \sim \mathcal{N}(0,
\mathbf{I})\)</span>。</li>
<li><strong>离散化：</strong> 将时间 <span class="math inline">\([0,
1]\)</span> 分为 <span class="math inline">\(N\)</span> 步（例如 <span
class="math inline">\(N=20\)</span>），每一步 <span
class="math inline">\(\Delta t = 1/N\)</span>。</li>
<li><strong>迭代：</strong> <strong>从 <span class="math inline">\(t =
0\)</span> 循环到 <span class="math inline">\(t = 1 - \Delta
t\)</span></strong>：
<ol type="a">
<li>获取当前位置 <span class="math inline">\(\mathbf{x}_t\)</span>
和时间 <span class="math inline">\(t\)</span>。</li>
<li>输入网络，得到当前速度：<span
class="math inline">\(\mathbf{v}_\theta =
\mathbf{v}_\theta(\mathbf{x}_t, t)\)</span>。</li>
<li><strong>欧拉法更新：</strong> <span class="math display">\[
\]</span><span class="math display">\[\\mathbf{x}\_{t + \\Delta t} =
\\mathbf{x}*t + \\mathbf{v}*\\theta \\cdot \\Delta t
\]</span> $$$$(新位置 = 旧位置 + 速度 × 时间)</li>
</ol></li>
<li><strong>结束：</strong> 当循环结束时，<span
class="math inline">\(\mathbf{x}_1\)</span> 就是生成的清晰图像。</li>
</ol>
<p><strong>优势：</strong></p>
<ul>
<li><strong>更简单：</strong> 训练目标 <span
class="math inline">\(\mathbf{x}_1 - \mathbf{x}_0\)</span>
非常直观，摆脱了 DDPM 中复杂的 <span
class="math inline">\(\bar{\alpha}_t, \beta_t\)</span> 系数。</li>
<li><strong>更高效：</strong> ODE 路径通常比 SDE
路径“更直”，因此流匹配通常可以用更少的推理步骤（例如 10-50
步）生成高质量图像。</li>
<li><strong>更灵活：</strong> 我们可以使用比欧拉法更高级的 ODE
求解器（如 Runge-Kutta）来进一步提高精度和速度。</li>
</ul>
<p><strong>总结</strong></p>
<p>扩散模型从基础到前沿的全部核心数学：</p>
<ol type="1">
<li><strong>基础 (Part 1)</strong>：高斯分布、朗之万动力学 (SDE)
和贝叶斯公式。</li>
<li><strong>DDPM (Part 2-5)</strong>：
<ul>
<li><strong>前向 (q)</strong>：<span
class="math inline">\(q(\mathbf{x}_t | \mathbf{x}_0) =
\mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)
\mathbf{I})\)</span></li>
<li><strong>反向 (p)</strong>：<span
class="math inline">\(q(\mathbf{x}_{t-1} | \mathbf{x}_t,
\mathbf{x}_0)\)</span> 的推导。</li>
<li><strong>训练</strong>：预测噪声 <span class="math inline">\(L = ||
\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)
||^2\)</span>。</li>
<li><strong>推理</strong>：<span class="math inline">\(\mathbf{x}_{t-1}
= \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) + \sigma_t
\mathbf{z}\)</span> (随机，T 步)。</li>
</ul></li>
<li><strong>DDIM (Part 6)</strong>：
<ul>
<li><strong>核心</strong>：预测 <span
class="math inline">\(\hat{\mathbf{x}}_0\)</span>。</li>
<li><strong>推理</strong>：<span
class="math inline">\(\mathbf{x}_{\tau_{i-1}} = \dots\)</span>
(确定性，可跳步)。</li>
</ul></li>
<li><strong>流匹配 (Part 7)</strong>：
<ul>
<li><strong>核心</strong>：ODE 连续流 <span
class="math inline">\(\frac{d\mathbf{x}_t}{dt} =
\mathbf{v}(\mathbf{x}_t, t)\)</span>。</li>
<li><strong>训练</strong>：预测速度 <span class="math inline">\(L = ||
(\mathbf{x}_1 - \mathbf{x}_0) - \mathbf{v}_\theta(\mathbf{x}_t, t)
||^2\)</span>。</li>
<li><strong>推理</strong>：<span class="math inline">\(\mathbf{x}_{t +
\Delta t} = \mathbf{x}_t + \mathbf{v}_\theta \cdot \Delta t\)</span>
(ODE 求解)。</li>
</ul></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/31/5120-1031/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/31/5120-1031/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-10-31 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-31T21:00:00+08:00">2025-10-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-11-05 15:24:36" itemprop="dateModified" datetime="2025-11-05T15:24:36+08:00">2025-11-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - 计算能源材料和电子结构模拟 Lecture</p>
<h2 id="møller-plesset-mp-微扰理论">1 Møller-Plesset (MP) 微扰理论:</h2>
<ul>
<li><strong>内容</strong>:</li>
</ul>
<p>量子化学中关于 <strong>Møller-Plesset (MP) 微扰理论</strong>
的推导过程。这是一种在 Hartree-Fock (HF)
方法基础上引入电子相关效应（electron correlation）的后-HF方法。</p>
<h3 id="标准微扰理论公式">标准微扰理论公式</h3>
<p>这部分列出的是通用的 <strong>Rayleigh-Schrödinger 微扰理论</strong>
(RSPT) 的基本公式：</p>
<ol type="1">
<li><strong>哈密顿算符 (Hamiltonian) 拆分:</strong>
<ul>
<li><span class="math inline">\(\hat{H} = \hat{H}_0 +
\hat{H}&#39;\)</span></li>
<li>将体系的总哈密顿算符 <span class="math inline">\(\hat{H}\)</span>
拆分为一个可以精确求解的零阶哈密顿算符 <span
class="math inline">\(\hat{H}_0\)</span> 和一个微扰项 <span
class="math inline">\(\hat{H}&#39;\)</span>。</li>
</ul></li>
<li><strong>能量和波函数的展开:</strong>
<ul>
<li><span class="math inline">\(E_n = E_n^{(0)} + E_n^{(1)} + E_n^{(2)}
+ \dots\)</span></li>
<li><span class="math inline">\(|n\rangle = |n^{(0)}\rangle +
|n^{(1)}\rangle + |n^{(2)}\rangle + \dots\)</span></li>
<li>体系的真实能量 <span class="math inline">\(E_n\)</span> 和波函数
<span class="math inline">\(|n\rangle\)</span>
可以展开为零阶、一阶、二阶等各级修正项的总和。</li>
</ul></li>
<li><strong>一阶能量修正 (First-order energy correction):</strong>
<ul>
<li><span class="math inline">\(E_n^{(1)} = \langle n^{(0)} |
\hat{H}&#39; | n^{(0)} \rangle\)</span></li>
<li>一阶能量修正是微扰算符 <span
class="math inline">\(\hat{H}&#39;\)</span> 在零阶波函数 <span
class="math inline">\(|n^{(0)}\rangle\)</span> 下的期望值。</li>
</ul></li>
<li><strong>二阶能量修正 (Second-order energy correction):</strong>
<ul>
<li><span class="math inline">\(E_n^{(2)} = \sum_{k \neq n}
\frac{|\langle k^{(0)} | \hat{H}&#39; | n^{(0)} \rangle|^2}{E_n^{(0)} -
E_k^{(0)}}\)</span></li>
<li>二阶能量修正涉及所有其他零阶本征态 <span
class="math inline">\(|k^{(0)}\rangle\)</span>。</li>
</ul></li>
</ol>
<h3 id="møller-plesset-理论的定义">Møller-Plesset 理论的定义</h3>
<p>这部分将上述通用公式应用到 MP 理论中，定义了 <span
class="math inline">\(\hat{H}_0\)</span> 和 <span
class="math inline">\(\hat{H}&#39;\)</span>：</p>
<ol type="1">
<li><strong>MP 理论的哈密顿算符定义:</strong>
<ul>
<li><strong>零阶哈密顿 <span
class="math inline">\(\hat{H}_0\)</span>:</strong>
<ul>
<li><span class="math inline">\(\hat{H}_0 = \hat{F} = \sum_{i=1}^N
\hat{f}(i)\)</span> (注：<span class="math inline">\(\hat{F}\)</span> 是
Fock 算符，<span class="math inline">\(\hat{H}_0\)</span> 被定义为 <span
class="math inline">\(N\)</span> 个电子的 Fock 算符之和)</li>
</ul></li>
<li><strong>微扰项 <span
class="math inline">\(\hat{H}&#39;\)</span>:</strong>
<ul>
<li><span class="math inline">\(\hat{H}&#39; = \hat{H} -
\hat{F}\)</span> (即 <span class="math inline">\(\hat{H} -
\hat{H}_0\)</span>)</li>
<li>微扰项是真实的哈密顿算符 <span
class="math inline">\(\hat{H}\)</span> (包含完整的电子-电子排斥) 与 Fock
算符 <span class="math inline">\(\hat{F}\)</span>
(只包含平均化的电子排斥) 之间的差值。这个差值就是 HF
理论所忽略的“电子相关”。</li>
</ul></li>
</ul></li>
<li><strong>零阶波函数和能量:</strong>
<ul>
<li>$|_0= $ HF Slater determinant</li>
<li>零阶波函数（即 <span class="math inline">\(\hat{H}_0\)</span>
的本征函数）被选为 <strong>Hartree-Fock (HF)
斯莱特行列式</strong>。</li>
<li><span class="math inline">\(\hat{H}_0 |\Phi_0\rangle = (\sum_{i=1}^N
\varepsilon_i) |\Phi_0\rangle\)</span></li>
<li>零阶能量 <span class="math inline">\(E^{(0)}\)</span> (即 <span
class="math inline">\(E_{MP0}\)</span>) 是所有被占据分子轨道 <span
class="math inline">\(\varepsilon_i\)</span> 的能量总和。</li>
<li><strong><span class="math inline">\(E_{MP0} = \langle \Phi_0 |
\hat{H}_0 | \Phi_0 \rangle = \sum_{i=1}^N
\varepsilon_i\)</span></strong></li>
</ul></li>
<li><strong>MP1 能量 (一阶能量修正):</strong>
<ul>
<li><span class="math inline">\(E_{MP1} = \langle \Phi_0 | \hat{H}&#39;
| \Phi_0 \rangle = \langle \Phi_0 | \hat{H} - \hat{F} | \Phi_0
\rangle\)</span></li>
<li><span class="math inline">\(E_{MP1} = \langle \Phi_0 | \hat{H} |
\Phi_0 \rangle - \langle \Phi_0 | \hat{F} | \Phi_0 \rangle\)</span></li>
<li>由于 <span class="math inline">\(\langle \Phi_0 | \hat{H} | \Phi_0
\rangle\)</span> 正是 Hartree-Fock 能量 <span
class="math inline">\(E_{HF}\)</span>，而 <span
class="math inline">\(\langle \Phi_0 | \hat{F} | \Phi_0 \rangle =
E_{MP0} = \sum \varepsilon_i\)</span>。</li>
<li>因此：<strong><span class="math inline">\(E_{MP1} = E_{HF} -
\sum_{i=1}^N \varepsilon_i\)</span></strong></li>
</ul></li>
</ol>
<blockquote>
<p><strong>重要结论：</strong> 零阶能量 (<span
class="math inline">\(E_{MP0}\)</span>) 和一阶能量 (<span
class="math inline">\(E_{MP1}\)</span>) 的总和恰好等于 Hartree-Fock
能量： <span class="math inline">\(E_{MP0} + E_{MP1} = (\sum
\varepsilon_i) + (E_{HF} - \sum \varepsilon_i) = E_{HF}\)</span>
这意味着 <strong>MP1 理论得到的总能量就是 HF 能量</strong>。要获得对 HF
能量的第一个修正（即电子相关能），我们必须计算到二阶，即
<strong>MP2</strong>。</p>
</blockquote>
<h2 id="mp2-能量推导">2. MP2 能量推导</h2>
<p>这部分是 MP 理论的核心，推导了 <strong>MP2
能量（二阶能量修正）</strong>：</p>
<ol type="1">
<li><strong>MP2 通用公式:</strong>
<ul>
<li><span class="math inline">\(E_{MP2} = \sum_{k \neq 0} \frac{|\langle
k | \hat{H}&#39; | 0 \rangle|^2}{E_0 - E_k}\)</span></li>
<li>这是将左侧的 <span class="math inline">\(E_n^{(2)}\)</span>
公式应用于基态 (<span class="math inline">\(n=0\)</span>)。</li>
</ul></li>
<li><strong>关键简化 (Brillouin 定理):</strong>
<ul>
<li>根据 Brillouin 定理，对于所有<strong>单激发</strong>行列式 <span
class="math inline">\(|\Phi_i^a\rangle\)</span> (即一个电子从占据轨道
<span class="math inline">\(i\)</span> 跃迁到空轨道 <span
class="math inline">\(a\)</span>)，矩阵元 <span
class="math inline">\(\langle \Phi_i^a | \hat{H}&#39; | \Phi_0 \rangle =
0\)</span>。</li>
<li>这意味着在求和 <span class="math inline">\(\sum_{k \neq 0}\)</span>
时，所有单激发的项都为零。</li>
<li>因此，对 MP2
能量有贡献的<strong>第一类非零项</strong>来自<strong>双激发</strong>行列式
(Doubly-excited determinants)，记为 <span
class="math inline">\(|\Phi_{ij}^{ab}\rangle\)</span>（即电子 <span
class="math inline">\(i, j\)</span> 激发到空轨道 <span
class="math inline">\(a, b\)</span>）。</li>
<li>白板中间的图示 <code>|| -&gt; #</code> (两条线变到两条更高阶的线)
正是形象地表示了这种双激发。</li>
</ul></li>
<li><strong>MP2 最终公式:</strong>
<ul>
<li><span class="math inline">\(E_{MP2} = \sum_{i&lt;j}^{occ}
\sum_{a&lt;b}^{vir} \frac{|\langle \Phi_{ij}^{ab} | \hat{H}&#39; |
\Phi_0 \rangle|^2}{\varepsilon_i + \varepsilon_j - \varepsilon_a -
\varepsilon_b}\)</span></li>
<li><strong>求和:</strong> 遍历所有占据轨道对 (<span
class="math inline">\(i, j\)</span>) 和所有空轨道（虚拟轨道）对 (<span
class="math inline">\(a, b\)</span>)。</li>
<li><strong>分母:</strong> <span class="math inline">\(E_0 - E_k = (\sum
\varepsilon) - (\sum \varepsilon - \varepsilon_i - \varepsilon_j +
\varepsilon_a + \varepsilon_b) = \varepsilon_i + \varepsilon_j -
\varepsilon_a -
\varepsilon_b\)</span>。这是零阶能量差，即激发所消耗的轨道能量。</li>
<li><strong>分子:</strong> <span class="math inline">\(\langle
\Phi_{ij}^{ab} | \hat{H}&#39; | \Phi_0 \rangle\)</span>
可以被简化为一个关于分子轨道的双电子积分 <span
class="math inline">\(\langle ij || ab \rangle\)</span>。</li>
</ul></li>
</ol>
<p><strong>总结：</strong> Møller-Plesset
微扰理论的标准推导，核心思想是将 HF
解作为零阶近似，并将电子相关的“剩余部分”作为微扰。推导表明，MP1
能量只是重现了 HF 能量，而 <strong>MP2
能量是第一个真正包含电子相关效应的修正项</strong>，它通过计算所有可能的双激发对总能量的贡献来实现。</p>
<p>从 Møller-Plesset (MP)
理论过渡到了另一种更高级的量子化学方法：<strong>耦合簇理论 (Coupled
Cluster Theory)</strong>。</p>
<h3 id="mp2-能量的深入探讨">MP2 能量的深入探讨</h3>
<p>这部分继续讨论 MP2 能量（二阶能量修正）：</p>
<ol type="1">
<li><strong>MP2 能量公式 (重复):</strong>
<ul>
<li><span class="math inline">\(E_{MP2} = \sum_{k \neq 0} \frac{|\langle
\Phi_0 | \hat{H}&#39; | \Phi_k \rangle|^2}{E_{MP0} -
E_k^{(0)}}\)</span></li>
<li>这是 MP2 能量的通用形式。</li>
</ul></li>
<li><strong>微扰的来源:</strong>
<ul>
<li><strong><span class="math inline">\(\frac{e^2}{|r_i -
r_j|}\)</span></strong> (被圈出)</li>
<li>这是电子-电子间的库仑排斥算符。这个项是 <span
class="math inline">\(\hat{H}\)</span>（真实哈密顿算符）和 <span
class="math inline">\(\hat{H}_0\)</span>（Fock
算符，仅包含平均场排斥）之间差异的核心。正是这个“瞬时相关”的相互作用导致了
<span class="math inline">\(\hat{H}&#39;\)</span>（微扰）的存在，也是 MP
理论试图修正的能量来源。</li>
</ul></li>
<li><strong>Brillouin 定理的应用:</strong>
<ul>
<li><span class="math inline">\(\langle \Phi_0 | \hat{H}&#39; | \Phi_i^a
\rangle = 0\)</span></li>
<li>这再次强调了上一张白板的结论：微扰算符 <span
class="math inline">\(\hat{H}&#39;\)</span> 在基态 <span
class="math inline">\(\Phi_0\)</span>
和任何<strong>单激发</strong>行列式 <span
class="math inline">\(\Phi_i^a\)</span> 之间的矩阵元为零。</li>
<li><span class="math inline">\(\langle \Phi_0 | \hat{H} - \hat{F} |
\Phi_i^a \rangle = 0\)</span> (这是 <span
class="math inline">\(\hat{H}&#39;\)</span> 的展开)</li>
<li><span class="math inline">\(\langle \Phi_a | \hat{f} | \Phi_i
\rangle = 0\)</span> (注：<span class="math inline">\(\Phi_i\)</span> 和
<span class="math inline">\(\Phi_a\)</span> 应为轨道 <span
class="math inline">\(\phi_i\)</span> 和 <span
class="math inline">\(\phi_a\)</span>)</li>
<li>这一行解释了为什么 Brillouin 定理在 MP 理论中成立：因为在标准的
Hartree-Fock (HF) 方法中，占据轨道 (<span
class="math inline">\(\phi_i\)</span>) 和空轨道 (<span
class="math inline">\(\phi_a\)</span>) 之间的 Fock 矩阵元（即 <span
class="math inline">\(\langle \phi_a | \hat{f} | \phi_i
\rangle\)</span>）被设为零。</li>
</ul></li>
<li><strong>MP2 最终实用公式:</strong>
<ul>
<li><span class="math inline">\(E_{MP2} = \sum_{i&lt;j}^{occ}
\sum_{a&lt;b}^{vir} \frac{|\langle \Phi_0 | \hat{H}&#39; |
\Phi_{ij}^{ab} \rangle|^2}{E_{MP0} - E_{ij}^{ab}}\)</span></li>
<li>由于单激发项为零，求和 <span class="math inline">\(k \neq 0\)</span>
中幸存下来的第一类项就是<strong>双激发</strong>项 <span
class="math inline">\(\Phi_{ij}^{ab}\)</span>（电子从 <span
class="math inline">\(i,j\)</span> 轨道激发到 <span
class="math inline">\(a,b\)</span> 轨道）。</li>
<li><span class="math inline">\(E_{ij}^{ab}\)</span>
是双激发组态的零阶能量。</li>
</ul></li>
<li><strong>MP2 能量的性质:</strong>
<ul>
<li><strong><span class="math inline">\(&lt; 0\)</span></strong>: MP2
修正能量<strong>总是负值</strong>。这意味着 MP2 能量总是在 HF
能量（<span class="math inline">\(E_{MP0} +
E_{MP1}\)</span>）的基础上进一步降低总能量。这是符合物理直觉的，因为电子相关效应允许电子更好地相互“躲避”，从而降低体系的总能量。</li>
<li><strong>“80%” (大约)</strong>:
这是一个常见的经验之谈，即对于许多小分子，MP2 方法大约能“恢复”80% 到 90%
的电子相关能。</li>
</ul></li>
</ol>
<h2 id="耦合簇理论-coupled-cluster-theory">3. 耦合簇理论 (Coupled
Cluster Theory)</h2>
<p>这部分介绍了一种更强大、更准确（也更昂贵）的后-HF方法。</p>
<ol type="1">
<li><p><strong>标题:</strong>
<code>Coupled Cluster Theory</code></p></li>
<li><p><strong>CC 波函数拟设 (Ansatz):</strong></p>
<ul>
<li><span class="math inline">\(\Psi_{CC} = e^{\hat{T}}
\Phi_{HF}\)</span> (这里 <span class="math inline">\(\Phi_{HF}\)</span>
即 <span class="math inline">\(\Phi_0\)</span>)</li>
<li>这是 CC 理论的核心！它假设真实的波函数 <span
class="math inline">\(\Psi_{CC}\)</span> 可以通过一个“指数算符” <span
class="math inline">\(e^{\hat{T}}\)</span> 作用在 HF 参考波函数 <span
class="math inline">\(\Phi_0\)</span> 上得到。</li>
</ul></li>
<li><p><strong>指数算符 <span
class="math inline">\(e^{\hat{T}}\)</span>:</strong></p>
<ul>
<li><span class="math inline">\(e^{\hat{T}} = \hat{1} + \hat{T} +
\frac{1}{2!} \hat{T}^2 + \frac{1}{3!} \hat{T}^3 + \dots\)</span></li>
<li>指数算符通过泰勒级数展开。这种指数形式具有非常重要的特性，即它能自动包含“非关联”的高阶激发（例如，两次独立的双激发
<span class="math inline">\(\hat{T}_2^2\)</span> 会产生四激发），这使得
CC 方法具有<strong>大小一致性 (size-consistency)</strong>，这是 MP
理论（在某些阶数上）所缺乏的重要特性。</li>
</ul></li>
<li><p><strong>簇算符 (Cluster Operator) <span
class="math inline">\(\hat{T}\)</span>:</strong></p>
<ul>
<li><span class="math inline">\(\hat{T} = \hat{T}_1 + \hat{T}_2 +
\hat{T}_3 + \dots\)</span></li>
<li>簇算符 <span class="math inline">\(\hat{T}\)</span>
本身是所有可能的激发算符的总和。</li>
</ul></li>
<li><p><strong>激发算符的定义:</strong></p>
<ul>
<li><strong><span class="math inline">\(\hat{T}_1\)</span>
(单激发):</strong>
<ul>
<li><span class="math inline">\(\hat{T}_1 \Phi_0 = \sum_{i}^{occ}
\sum_{a}^{vir} t_i^a \Phi_i^a\)</span></li>
<li><span class="math inline">\(\hat{T}_1\)</span>
产生所有可能的单激发态的线性组合。<span
class="math inline">\(t_i^a\)</span> 是未知的“振幅
(amplitudes)”，即激发的重要性权重，需要通过求解 CC 方程得到。</li>
</ul></li>
<li><strong><span class="math inline">\(\hat{T}_2\)</span>
(双激发):</strong>
<ul>
<li><span class="math inline">\(\hat{T}_2 \Phi_0 = \sum_{i&lt;j}^{occ}
\sum_{a&lt;b}^{vir} t_{ij}^{ab} \Phi_{ij}^{ab}\)</span></li>
<li><span class="math inline">\(\hat{T}_2\)</span>
产生所有可能的双激发态的线性组合。<span
class="math inline">\(t_{ij}^{ab}\)</span> 是双激发的振幅。</li>
</ul></li>
</ul></li>
</ol>
<p><strong>总结：</strong> 这张白板从 MP2
理论（一种基于微扰的方法）过渡到了耦合簇理论（一种基于指数拟设的非微扰方法）。</p>
<ul>
<li><strong>MP2</strong>
通过二阶微扰，只显式地考虑了<strong>双激发</strong>对能量的贡献。</li>
<li><strong>Coupled Cluster</strong> (例如 CCSD，即 <span
class="math inline">\(\hat{T} = \hat{T}_1 + \hat{T}_2\)</span>)
则系统地包含了 <span class="math inline">\(\hat{T}_1\)</span> (单激发)
和 <span class="math inline">\(\hat{T}_2\)</span> (双激发)
及其所有乘积（如 <span class="math inline">\(\hat{T}_1^2, \hat{T}_1
\hat{T}_2, \hat{T}_2^2\)</span>
等），因此它隐式地包含了某些更高阶的激发（如四激发），使其成为比 MP2
更准确、更鲁棒的方法。</li>
</ul>
<p>当然可以。这是一个非常核心的量子化学概念。</p>
<p>简单来说：<strong>双激发 (Double Excitations)
是描述电子“躲避”彼此这一行为的<em>最主要</em>、<em>最基本</em>的数学方式。</strong></p>
<p>MP2 和 CCSD 都高度关注双激发，因为它们是捕获“电子相关能” (Electron
Correlation Energy) 的关键。</p>
<h3 id="问题的根源hf-理论错过了什么">问题的根源：HF
理论错过了什么？</h3>
<p>在白板的 MP 理论推导中，我们从 <strong>Hartree-Fock (HF)
波函数</strong> (<span class="math inline">\(\Phi_0\)</span>) 开始。</p>
<ul>
<li><strong>HF 的问题：</strong> HF
是一种“平均场”理论。它假设一个电子（例如电子 <span
class="math inline">\(i\)</span>）只感受到所有其他电子（例如电子 <span
class="math inline">\(j\)</span>）的<em>平均</em>电荷分布，而不是它们在某一<em>瞬时</em>的真实位置。</li>
<li><strong>物理现实：</strong>
电子是带负电的粒子，它们会<em>瞬时</em>地相互排斥。如果电子 <span
class="math inline">\(i\)</span> 在分子的 A 点，电子 <span
class="math inline">\(j\)</span> 会倾向于<em>避开</em> A 点，跑到 B
点去。这种为了“躲避”对方而调整自己行为的现象，就叫做 <strong>电子相关
(Electron Correlation)</strong>。</li>
</ul>
<p>HF 理论忽略了这种瞬时躲避，因此 HF
能量总是高于真实的基态能量。我们所说的“电子相关能”，就是这个能量差。</p>
<h3 id="解决方案双激发phi_ijab">解决方案：双激发（<span
class="math inline">\(\Phi_{ij}^{ab}\)</span>）</h3>
<p>我们如何用数学来描述“电子 <span class="math inline">\(i\)</span> 和
<span class="math inline">\(j\)</span> 相互躲避”？</p>
<p>想象一下，在 HF 基态 (<span class="math inline">\(\Phi_0\)</span>)
中，电子 <span class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span> 在它们各自的轨道里。</p>
<ul>
<li>为了描述它们“躲开”彼此，我们需要在波函数中“混入”一个新的组态
(configuration)。</li>
<li>在这个新组态中，电子 <span class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span> <em>同时</em>从它们原来的轨道（占据轨道
<span class="math inline">\(i,
j\)</span>）“跳”到了两个新的、能量更高的空轨道（虚拟轨道 <span
class="math inline">\(a, b\)</span>）。</li>
<li>这个“双重跳跃”的态，就是白板上的 <strong>双激发态 <span
class="math inline">\(\Phi_{ij}^{ab}\)</span></strong>。</li>
</ul>
<p>通过将这个双激发态 <span
class="math inline">\(\Phi_{ij}^{ab}\)</span> 线性叠加到基态 <span
class="math inline">\(\Phi_0\)</span>
中，我们的总波函数就能描述这样一种情形：电子 <span
class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span>
有一定概率<em>不在</em>它们“应该”在的地方，而是跑到了别处——这就是它们相互躲避的数学表达。</p>
<p><strong>为什么单激发 (<span class="math inline">\(\Phi_i^a\)</span>)
不行？</strong> 如白板所示，Brillouin 定理（<span
class="math inline">\(\langle \Phi_0 | \hat{H}&#39; | \Phi_i^a \rangle =
0\)</span>）告诉我们，在 MP 理论框架下，HF
基态和单激发态之间没有直接的相互作用。单激发主要描述的是轨道本身的形状调整（轨道弛豫），而不是电子<em>之间</em>的相关。</p>
<h3 id="mp2-和-ccsd-的联系与区别">MP2 和 CCSD 的联系与区别</h3>
<p>MP2 和 CCSD
都是基于这个核心思想，但实现方式的复杂程度和准确性完全不同。</p>
<h4 id="mp2-møller-plesset-2nd-order">🔹 MP2 (Møller-Plesset 2nd
Order)</h4>
<ul>
<li><strong>它做了什么：</strong> MP2
是一种<strong>微扰</strong>方法。它把双激发当作一种“微小的扰动”。</li>
<li><strong>如何工作：</strong> 它使用二阶微扰公式（白板上的 <span
class="math inline">\(E_{MP2}\)</span>）来计算：<em>“如果我允许体系中的每一对电子
(i, j) 发生一次双激发 (到 a, b)，这会使体系的总能量降低多少？”</em></li>
<li><strong>局限性：</strong>
<ol type="1">
<li>它只计算<em>一次</em>激发的影响。</li>
<li>它只考虑了双激发。</li>
<li>它假设这种扰动很“小”。</li>
</ol></li>
</ul>
<p>MP2
是最简单、计算最便宜的包含电子相关的方法，它抓住了相关能的“大头”（
<code>~80%</code>）。</p>
<h4 id="ccsd-coupled-cluster-singles-and-doubles">🔹 CCSD (Coupled
Cluster Singles and Doubles)</h4>
<ul>
<li><strong>它做了什么：</strong> CCSD
是一种<strong>非微扰</strong>方法。它不认为相关是“小扰动”，而是从根本上重新构建波函数。</li>
<li><strong>如何工作：</strong> 它使用指数拟设 <span
class="math inline">\(\Psi_{CC} = e^{\hat{T}_1 + \hat{T}_2}
\Phi_0\)</span>。
<ul>
<li><span class="math inline">\(\hat{T}_2\)</span>
算符（白板上有写）就是用来产生所有双激发的。</li>
<li><span class="math inline">\(\hat{T}_1\)</span>
算符（白板上有写）用来产生所有单激发（用于轨道弛豫）。</li>
</ul></li>
<li><strong>关键区别（指数的魔力）：</strong>
<ul>
<li>当指数 <span class="math inline">\(e^{\hat{T}}\)</span> 展开时
(<span class="math inline">\(e^{\hat{T}} = 1 + \hat{T} +
\frac{1}{2}\hat{T}^2 + \dots\)</span>)，你会得到像 <span
class="math inline">\(\frac{1}{2}(\hat{T}_2)^2\)</span> 这样的项。</li>
<li><span class="math inline">\(\frac{1}{2}(\hat{T}_2)^2\)</span>
意味着<strong>两次独立的双激发</strong>。这在物理上代表了<strong>四激发</strong>（例如，分子中两对互不相干的电子同时在各自“躲避”）。</li>
<li><strong>这就是 CCSD 远比 MP2 准确的原因</strong>：它不仅包含了 <span
class="math inline">\(\hat{T}_2\)</span>（双激发），还通过指数形式<em>自动包含</em>了由
<span class="math inline">\(\hat{T}_2\)</span>
组合产生的高阶激发（如四激发、六激发等）。它正确地描述了多个电子对同时发生相关行为的情况。</li>
</ul></li>
</ul>
<h3 id="总结对比">总结对比</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">特性</th>
<th style="text-align: left;">MP2</th>
<th style="text-align: left;">CCSD ( <span class="math inline">\(\hat{T}
= \hat{T}_1 + \hat{T}_2\)</span> )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>方法类型</strong></td>
<td style="text-align: left;">微扰理论 (Perturbative)</td>
<td style="text-align: left;">非微扰 (Non-perturbative)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>核心思想</strong></td>
<td
style="text-align: left;">计算<strong>双激发</strong>对能量的<em>二阶</em>修正。</td>
<td
style="text-align: left;">用指数算符包含所有<strong>单、双激发</strong>。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>包含的激发</strong></td>
<td style="text-align: left;">仅显式包含<strong>双激发</strong>。</td>
<td style="text-align: left;">显式包含<strong>单、双激发</strong>。<br>
<em>隐式</em>包含高阶激发（四、六等）。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>准确性</strong></td>
<td style="text-align: left;">良好（约 80-90% 相关能）</td>
<td style="text-align: left;">优秀（“黄金标准”之一）</td>
</tr>
<tr>
<td style="text-align: left;"><strong>数学联系</strong></td>
<td style="text-align: left;">MP2 的能量公式可以被证明是 CCSD
方程的<em>最低阶近似</em>。</td>
<td style="text-align: left;">更完整、更高级的理论。</td>
</tr>
</tbody>
</table>
<p><strong>总结：</strong></p>
<p><strong>双激发是描述电子相关（相互躲避）的物理核心。MP2
用最简单的方式估算了它的能量贡献，而 CCSD
则用一种更完备、更强大的数学（指数）形式将其及组合效应系统地包含了进来。</strong></p>
<h2 id="量子蒙特卡洛-quantum-monte-carlo">4. 量子蒙特卡洛 (Quantum Monte
Carlo)</h2>
<h3 id="耦合簇理论-coupled-cluster">耦合簇理论 (Coupled Cluster)</h3>
<h4 id="ccsdtt-的含义">1. CCSD(T)：“(T)” 的含义</h4>
<ul>
<li><strong>回顾 <code>CCSD</code>：</strong> 正如标题
<code>Coupled Cluster Singles and Doubles (CCSD)</code> 所示，CCSD
方法只完整地包含了单激发算符 (<span
class="math inline">\(\hat{T}_1\)</span>) 和双激发算符 (<span
class="math inline">\(\hat{T}_2\)</span>)。它通过指数形式 <span
class="math inline">\(e^{(\hat{T}_1 +
\hat{T}_2)}\)</span>，已经能间接地包含一些高阶激发（如四激发 <span
class="math inline">\(\hat{T}_2^2\)</span>）。</li>
<li><strong>缺失的项：</strong> CCSD <strong>没有</strong> 显式地包含
<strong>“连通”(connected) 三激发算符 <span
class="math inline">\(\hat{T}_3\)</span></strong>。<span
class="math inline">\(\hat{T}_3\)</span>
描述的是三个电子<em>同时</em>相互关联并激发到三个空轨道。</li>
<li><strong><code>(T)</code> 的含义：</strong> <code>(T)</code> 代表
<strong>“微扰三激发” (perturbative Triples)</strong>。
<ul>
<li><strong>为什么不直接用 <code>CCSDT</code>？</strong> 完整地求解包含
<span class="math inline">\(\hat{T}_3\)</span> 的方程（即 CCSDT
方法）在计算上极其昂贵（计算量随体系大小的 <span
class="math inline">\(N^8\)</span> 增长）。</li>
<li><strong><code>(T)</code> 的解决之道：</strong> <code>CCSD(T)</code>
是一种巧妙的折中。它首先执行一个完整的、较便宜的 CCSD 计算（求解 <span
class="math inline">\(\hat{T}_1\)</span> 和 <span
class="math inline">\(\hat{T}_2\)</span>）。然后，它使用这些已知的 <span
class="math inline">\(\hat{T}_1\)</span> 和 <span
class="math inline">\(\hat{T}_2\)</span>
振幅，通过<strong>微扰理论</strong>（就像第一张白板上的 MP
理论一样！）来<em>估算</em> <span
class="math inline">\(\hat{T}_3\)</span>
会对总能量产生的<em>修正</em>。</li>
</ul></li>
<li><strong>“黄金标准” (Gold Standard)：</strong> <code>CCSD(T)</code>
方法被广泛认为是量子化学的“黄金标准”。它以可承受的计算成本（<span
class="math inline">\(N^7\)</span>
增长）提供了接近“完美”的能量，是绝大多数高精度计算的基准。</li>
<li><strong>“95%” 注释：</strong> 白板上的 <code>95%</code>
注释很可能是讲师的一个经验之谈，即 CCSD 大约能恢复 95%
的电子相关能（相比 MP2 的 ~80%），而 <code>(T)</code>
修正则能将这个数字推向 99% 甚至更高。</li>
</ul>
<h4 id="耦合簇方程-cc-equations">2. 耦合簇方程 (CC Equations)</h4>
<p>白板的中间部分展示了如何<em>求解</em> CCSD 方程以获得能量 <span
class="math inline">\(E\)</span> 和振幅 <span
class="math inline">\(t_i^a, t_{ij}^{ab}\)</span>。</p>
<ul>
<li><p><strong>总薛定谔方程：</strong></p>
<ul>
<li><span class="math inline">\(H e^{\hat{T}} | \Phi_0 \rangle = E
e^{\hat{T}} | \Phi_0 \rangle\)</span></li>
<li>这是 CC 理论要解的薛定谔方程 (<span class="math inline">\(\hat{H}
|\Psi_{CC}\rangle = E |\Psi_{CC}\rangle\)</span>)。</li>
</ul></li>
<li><p><strong>求解方法（投影法）：</strong> 为了解出未知的 <span
class="math inline">\(E\)</span>, <span
class="math inline">\(\hat{T}_1\)</span>, <span
class="math inline">\(\hat{T}_2\)</span>，我们将这个总方程“投影”到不同的激发空间上：</p>
<ol type="1">
<li><strong>能量 <span class="math inline">\(E\)</span>：</strong>
<ul>
<li><span class="math inline">\(\langle \Phi_0 |
e^{-(\hat{T}_1+\hat{T}_2)} \hat{H} e^{(\hat{T}_1+\hat{T}_2)} | \Phi_0
\rangle = E\)</span></li>
<li>将总方程左乘 <span class="math inline">\(\langle \Phi_0
|\)</span>（HF 基态）并积分，可以直接得到总能量 <span
class="math inline">\(E\)</span>。</li>
</ul></li>
<li><strong>求解 <span class="math inline">\(\hat{T}_1\)</span>（方程
①）：</strong>
<ul>
<li><span class="math inline">\(\langle \Phi_i^a |
e^{-(\hat{T}_1+\hat{T}_2)} \hat{H} e^{(\hat{T}_1+\hat{T}_2)} | \Phi_0
\rangle = 0\)</span></li>
<li>将总方程左乘 <span class="math inline">\(\langle \Phi_i^a
|\)</span>（所有<strong>单激发</strong>态）。这会产生一系列方程，求解它们可以得到所有
<span class="math inline">\(\hat{T}_1\)</span> 的振幅 <span
class="math inline">\(t_i^a\)</span>。</li>
</ul></li>
<li><strong>求解 <span class="math inline">\(\hat{T}_2\)</span>（方程
②）：</strong>
<ul>
<li><span class="math inline">\(\langle \Phi_{ij}^{ab} |
e^{-(\hat{T}_1+\hat{T}_2)} \hat{H} e^{(\hat{T}_1+\hat{T}_2)} | \Phi_0
\rangle = 0\)</span></li>
<li>将总方程左乘 <span class="math inline">\(\langle \Phi_{ij}^{ab}
|\)</span>（所有<strong>双激发</strong>态）。这会产生另一系列方程，求解它们可以得到所有
<span class="math inline">\(\hat{T}_2\)</span> 的振幅 <span
class="math inline">\(t_{ij}^{ab}\)</span>。</li>
</ul></li>
</ol></li>
</ul>
<p><strong>总结：</strong> CCSD
是一个复杂的非线性方程组。我们通过求解方程 ① 和 ②
得到所有激发振幅，然后将这些振幅代入能量方程，得到最终的 CCSD 能量。</p>
<h3 id="量子蒙特卡洛-quantum-monte-carlo-qmc">量子蒙特卡洛 (Quantum
Monte Carlo, QMC)</h3>
<p>这部分介绍了一种<strong>完全不同</strong>的、不依赖于轨道和激发的方法。</p>
<ul>
<li><strong>标题：</strong> <code>Quantum Monte Carlo</code></li>
<li><strong>子标题：</strong> <code>Variational / Diffusion MC</code>
(变分蒙特卡洛 / 扩散蒙特卡洛)</li>
</ul>
<h4 id="变分蒙特卡洛-vmc">变分蒙特卡洛 (VMC)</h4>
<p>VMC 的核心思想，基于<strong>变分原理</strong>：</p>
<ol type="1">
<li><strong>能量期望值：</strong>
<ul>
<li><span class="math inline">\(E(\theta) = \frac{\langle \Psi(\theta) |
\hat{H} | \Psi(\theta) \rangle}{\langle \Psi(\theta) | \Psi(\theta)
\rangle}\)</span></li>
<li>任何一个“试验波函数” <span
class="math inline">\(\Psi(\theta)\)</span>（<span
class="math inline">\(\theta\)</span> 是函数中的可调参数）所计算出的能量
<span class="math inline">\(E(\theta)\)</span>
永远<strong>大于或等于</strong>真实的基态能量 <span
class="math inline">\(E_{\text{ground}}\)</span>。</li>
<li><strong>目标：</strong>
<code>min $E(\theta)$ = $E_&#123;\text&#123;ground&#125;&#125;$</code> (更准确地说是 <span
class="math inline">\(E_{\text{approx}}\)</span>)</li>
<li>通过调整参数 <span class="math inline">\(\theta\)</span> 来最小化
<span
class="math inline">\(E(\theta)\)</span>，我们可以获得对基态能量的最佳近似。</li>
</ul></li>
<li><strong>蒙特卡洛方法如何计算？</strong>
<ul>
<li>上述能量公式是一个极其复杂的高维积分（积分维度 = 3 <span
class="math inline">\(\times\)</span> 电子数）。</li>
<li>QMC
它<strong>不直接计算这个积分</strong>，而是使用<strong>随机抽样</strong>（“蒙特卡洛”方法）来估算它。</li>
</ul></li>
<li><strong>VMC 计算步骤（如白板所示）：</strong>
<ul>
<li><strong>a. 重写积分：</strong> <span class="math inline">\(E =
\frac{\int |\Psi(\vec{x}, \theta)|^2 \left[ \frac{\hat{H} \Psi(\vec{x},
\theta)}{\Psi(\vec{x}, \theta)} \right] d\vec{x}}{\int |\Psi(\vec{x},
\theta)|^2 d\vec{x}}\)</span>
<ul>
<li><span class="math inline">\(\vec{x}\)</span> 代表所有电子的坐标
(<span class="math inline">\(r_1, r_2, \dots\)</span>)。</li>
<li><span class="math inline">\(\frac{|\Psi(\vec{x})|^2}{\int
|\Psi(\vec{x})|^2 d\vec{x}}\)</span> 是电子在 <span
class="math inline">\(\vec{x}\)</span>
处被发现的<strong>概率密度</strong> <span
class="math inline">\(P(\vec{x})\)</span>。</li>
<li><span class="math inline">\(E_L(\vec{x}) = \frac{\hat{H}
\Psi(\vec{x}, \theta)}{\Psi(\vec{x}, \theta)}\)</span> 被称为
<strong>“局域能量” (Local Energy)</strong>。</li>
</ul></li>
<li><strong>b. 抽样：</strong>
<ul>
<li>整个积分就变成了在 <span class="math inline">\(P(\vec{x})\)</span>
概率分布下，对 <span class="math inline">\(E_L(\vec{x})\)</span>
求平均值。</li>
<li>VMC 方法通过一种算法（如 Metropolis 算法）产生大量的、符合 <span
class="math inline">\(|\Psi|^2\)</span>
分布的随机电子构型（“walkers”）。</li>
</ul></li>
<li><strong>c. 求平均：</strong>
<ul>
<li><span class="math inline">\(E \approx \frac{1}{N}
\sum_{\text{samples } \vec{x}} E_L(\vec{x})\)</span></li>
<li>最终的能量就是所有采样点上“局域能量”的简单平均值。</li>
</ul></li>
</ul></li>
</ol>
<p><strong>总结：</strong> VMC
通过在“真实空间”中随机移动电子来直接估算能量，完全绕过了 MP 或 CC
理论中复杂的轨道和激发概念。<code>Diffusion MC</code> (DMC)
是其更高级的变体，原则上可以找到精确的基态能量。</p>
<p>这张白板标志着一个<strong>重大的理论转变</strong>：从前面讨论的“波函数方法”（Wavefunction
Methods, 如 MP2,
CCSD）转向了一种完全不同的、在计算化学和物理中占主导地位的方法——<strong>密度泛函理论
(Density Functional Theory, DFT)</strong>。</p>
<h2 id="波函数方法的维度灾难">5. 波函数方法的“维度灾难”</h2>
<p>一个生动的例子，说明了为什么基于波函数的方法（如
CCSD）在计算上极其昂贵，甚至是不可能的。</p>
<ol type="1">
<li><strong>“wavefunction method” (波函数方法):</strong>
<ul>
<li>这类方法（如 HF, MP2, CCSD）的中心目标是求解体系的 <span
class="math inline">\(N\)</span> 电子波函数 <span
class="math inline">\(\Psi(\vec{r}_1, \vec{r}_2, \dots,
\vec{r}_N)\)</span>。</li>
<li>这个波函数 <span class="math inline">\(\Psi\)</span>
是一个极其复杂的对象，它是一个 <span class="math inline">\(3N\)</span>
维度的函数（每个电子有 3 个空间坐标 <span class="math inline">\(x, y,
z\)</span>）。</li>
</ul></li>
<li><strong>计算成本的“立方体”比喻：</strong>
<ul>
<li>假设我们想在一个 3D 空间中存储<em>一个</em>电子的波函数。</li>
<li>如果我们在每个维度（x, y, z）上只使用 <strong>10</strong>
个网格点，我们就需要 <span class="math inline">\(10 \times 10 \times 10
= 10^3\)</span> 个点来描述这<em>一个</em>电子。</li>
<li>现在，考虑一个有 <strong>30</strong>
个电子的体系（一个中等大小的分子）。</li>
<li>由于总波函数 <span class="math inline">\(\Psi\)</span> 是 <span
class="math inline">\(3 \times 30 = 90\)</span> 维的，我们需要 <span
class="math inline">\((10^3)^{30} = 10^{90}\)</span>
个网格点来存储这个波函数。</li>
<li><strong><span class="math inline">\(10^{90}\)</span>
是一个天文数字！</strong> 白板上的 <span
class="math inline">\(10^{77}\)</span> 或 <span
class="math inline">\(10^{80}\)</span>
可能是用来比较的数字（例如，可观测宇宙中的原子总数约 <span
class="math inline">\(10^{80}\)</span>）。这个数字 (<span
class="math inline">\(10^{90}\)</span>) 意味着直接存储或计算 N
电子波函数在计算上是<strong>绝对不可能</strong>的。</li>
<li>这就是所谓的“<strong>维度灾难 (Curse of
Dimensionality)</strong>”。</li>
</ul></li>
<li><strong>DFT：解决方案登场</strong>
<ul>
<li>在指出了波函数方法的根本困难后，白板上写下了
<strong>DFT</strong>，预示着它是一种解决方案。</li>
</ul></li>
<li><strong>体系的哈密顿算符 (<span
class="math inline">\(\hat{H}\)</span>):</strong>
<ul>
<li><span class="math inline">\(\hat{H} = \underbrace{-\sum_i
\frac{\hbar^2}{2m} \nabla_i^2}_{\text{电子动能}} + \underbrace{\sum_i
V_{\text{ext}}(\vec{r}_i)}_{\text{电子-原子核吸引}} +
\underbrace{\frac{1}{2} \sum_{i \neq j} \frac{e^2}{|\vec{r}_i -
\vec{r}_j|}}_{\text{电子-电子排斥}}\)</span></li>
<li>这是任何分子体系的完整（非相对论）哈密顿算符。</li>
</ul></li>
<li><strong>关键的简化：</strong>
<ul>
<li><span class="math inline">\(\langle \Psi | \sum_i
V_{\text{ext}}(\vec{r}_i) | \Psi \rangle = \int V_{\text{ext}}(\vec{r})
\rho(\vec{r}) d\vec{r}\)</span></li>
<li>这一行展示了一个至关重要的简化：<span
class="math inline">\(N\)</span> 电子的“电子-原子核吸引能”的期望值（一个
<span class="math inline">\(3N\)</span>
维积分），可以被<em>精确地</em>重写为一个只涉及<strong>电子密度 <span
class="math inline">\(\rho(\vec{r})\)</span></strong> 的** 3
维积分**。</li>
<li>这就引出了一个问题：我们是否能用这个简单的 <span
class="math inline">\(\rho(\vec{r})\)</span> 来代替 <span
class="math inline">\(\Psi\)</span> 呢？</li>
</ul></li>
</ol>
<h3 id="dft-的理论基石">DFT 的理论基石</h3>
<p>这部分介绍了 DFT 的核心定理</p>
<ol type="1">
<li><strong>电子密度 (Electron Density, <span
class="math inline">\(\rho(\vec{r})\)</span>):</strong>
<ul>
<li><span class="math inline">\(\int \rho(\vec{r}) d\vec{r} =
N\)</span></li>
<li>电子密度 <span class="math inline">\(\rho(\vec{r})\)</span>
是一个<strong>在 3D 空间中的函数</strong>。它描述了在任意空间点 <span
class="math inline">\(\vec{r}\)</span> 处找到一个电子的概率。</li>
<li>无论体系有多少电子（<span class="math inline">\(N=30\)</span> 或
<span class="math inline">\(N=1000\)</span>），<span
class="math inline">\(\rho(\vec{r})\)</span>
<strong>始终</strong>是一个简单的 3 维函数。这与 <span
class="math inline">\(3N\)</span> 维的波函数 <span
class="math inline">\(\Psi\)</span> 形成了鲜明对比。</li>
</ul></li>
<li><strong>Hohenberg-Kohn (H-K) 定理：</strong>
<ul>
<li>这是 DFT 的全部理论基础。白板上的图示完美地总结了第一个 H-K
定理：</li>
<li><strong>标准路径（上 <span
class="math inline">\(\rightarrow\)</span> 下）：</strong> <span
class="math inline">\(V_{\text{ext}}(\vec{r})\)</span>（外势，即原子核的位置和电荷）<span
class="math inline">\(\implies \Psi(\vec{r})\)</span>（基态波函数）<span
class="math inline">\(\implies \rho(\vec{r})\)</span>（基态密度）。
<ul>
<li><em>解释：</em> 原子核的位置决定了 <span
class="math inline">\(\hat{H}\)</span>，求解 <span
class="math inline">\(\hat{H}\)</span> 得到 <span
class="math inline">\(\Psi\)</span>，由 <span
class="math inline">\(\Psi\)</span> 可以计算出 <span
class="math inline">\(\rho\)</span>。</li>
</ul></li>
<li><strong>H-K 革命性路径（下 <span
class="math inline">\(\leftrightarrow\)</span> 上）：</strong> <span
class="math inline">\(V_{\text{ext}}(\vec{r}) \Longleftrightarrow
\rho(\vec{r})\)</span>
<ul>
<li><strong>第一 H-K 定理</strong>证明：体系的<strong>基态电子密度 <span
class="math inline">\(\rho(\vec{r})\)</span></strong>
<em>唯一地</em>决定了<strong>外势 <span
class="math inline">\(V_{\text{ext}}(\vec{r})\)</span></strong>（因此也唯一决定了
<span class="math inline">\(\hat{H}\)</span>、<span
class="math inline">\(\Psi\)</span> 和总能量 <span
class="math inline">\(E\)</span>）。</li>
</ul></li>
</ul></li>
<li><strong>H-K 定理的重大意义：</strong>
<ul>
<li><strong>所有信息都在 <span
class="math inline">\(\rho(\vec{r})\)</span> 中！</strong>
体系的基态总能量 <span class="math inline">\(E\)</span> 是基态密度 <span
class="math inline">\(\rho\)</span> 的一个<strong>泛函
(Functional)</strong>，记为 <span
class="math inline">\(E[\rho]\)</span>。</li>
<li><strong>目标转变：</strong> 我们不再需要去求解那个 <span
class="math inline">\(10^{90}\)</span> 维的波函数 <span
class="math inline">\(\Psi\)</span>！我们只需要找到那个能使总能量 <span
class="math inline">\(E[\rho]\)</span> 最小的 3 维密度 <span
class="math inline">\(\rho(\vec{r})\)</span>。</li>
</ul></li>
</ol>
<p><strong>总结：</strong>
首先论证了“波函数方法”在计算上的不可能性（维度灾难），然后引入了
<strong>DFT</strong> 作为解决方案，其理论依据是 <strong>H-K
定理</strong>——即体系的所有信息都包含在简单的 3 维电子密度 <span
class="math inline">\(\rho(\vec{r})\)</span> 中。</p>
<h2
id="证明-第一-hohenberg-kohn-h-k-定理的经典证明这个定理是密度泛函理论-dft-的基石">6：证明
<strong>第一 Hohenberg-Kohn (H-K)
定理的经典证明</strong>。这个定理是密度泛函理论 (DFT) 的基石。</h2>
<p>这个证明使用的是 <strong>“Proof by contradiction”
(反证法)</strong>。</p>
<p><strong>定理内容：</strong> 体系的基态电子密度 <span
class="math inline">\(\rho_0(\vec{r})\)</span> 唯一地决定了其外势 <span
class="math inline">\(V_{ext}(\vec{r})\)</span>（即原子核的位置和电荷），并因此唯一地决定了体系的哈密顿算符
<span class="math inline">\(\hat{H}\)</span> 和波函数 <span
class="math inline">\(\Psi\)</span>。</p>
<h3 id="证明步骤详解">证明步骤详解</h3>
<h4 id="假设-为反证法设置">1. 假设 (为反证法设置)</h4>
<p>我们假设 H-K 定理是<strong>错</strong>的。</p>
<p>这意味着我们假设存在<strong>两个不同 (different)</strong> 的外势
<span class="math inline">\(V_{ext}^{(1)}\)</span> 和 <span
class="math inline">\(V_{ext}^{(2)}\)</span>，它们分别对应各自的哈密顿算符
<span class="math inline">\(\hat{H}^{(1)}\)</span>、<span
class="math inline">\(\hat{H}^{(2)}\)</span> 和基态波函数 <span
class="math inline">\(\Psi^{(1)}\)</span>、<span
class="math inline">\(\Psi^{(2)}\)</span>。</p>
<p>但我们<strong>假设</strong>，这两个完全不同的体系却碰巧产生了<strong>完全相同
(same)</strong> 的基态电子密度 <span
class="math inline">\(\rho_0(\vec{r})\)</span>。</p>
<blockquote>
<p><strong>假设总结：</strong> * <span
class="math inline">\(V_{ext}^{(1)} \neq V_{ext}^{(2)}\)</span> （因此
<span class="math inline">\(\hat{H}^{(1)} \neq \hat{H}^{(2)}\)</span> 且
<span class="math inline">\(\Psi^{(1)} \neq \Psi^{(2)}\)</span>） * 但是
<span class="math inline">\(\rho^{(1)}(\vec{r}) = \rho^{(2)}(\vec{r}) =
\rho_0(\vec{r})\)</span> * <span class="math inline">\(E^{(1)}\)</span>
是体系1的基态能量。 * <span class="math inline">\(E^{(2)}\)</span>
是体系2的基态能量。</p>
</blockquote>
<h4 id="应用变分原理-第-1-步">2. 应用变分原理 (第 1 步)</h4>
<p>变分原理指出：任何一个“试验波函数” <span
class="math inline">\(\Psi_{\text{trial}}\)</span> 对某个哈密顿算符
<span class="math inline">\(\hat{H}\)</span>
的能量期望值，永远高于或等于该 <span
class="math inline">\(\hat{H}\)</span> 的真实基态能量 <span
class="math inline">\(E_{GS}\)</span>。</p>
<ul>
<li>我们将 <span class="math inline">\(\Psi^{(2)}\)</span>
(体系2的基态波函数) 作为体系1的<strong>试验波函数</strong>。</li>
<li>根据变分原理，<span class="math inline">\(\Psi^{(2)}\)</span>
计算出的 <span class="math inline">\(\hat{H}^{(1)}\)</span>
的能量必定高于 <span class="math inline">\(\hat{H}^{(1)}\)</span>
的真实基态能量 <span
class="math inline">\(E^{(1)}\)</span>（因为我们假设了 <span
class="math inline">\(\Psi^{(1)} \neq \Psi^{(2)}\)</span>）。</li>
<li><strong><span class="math inline">\(E^{(1)} &lt; \langle \Psi^{(2)}
| \hat{H}^{(1)} | \Psi^{(2)} \rangle\)</span></strong>
(白板上的第3行)</li>
</ul>
<h4 id="展开能量-第-2-步">3. 展开能量 (第 2 步)</h4>
<p>我们来展开 <span class="math inline">\(\langle \Psi^{(2)} |
\hat{H}^{(1)} | \Psi^{(2)} \rangle\)</span> 这一项。</p>
<ul>
<li><p>我们知道 <span class="math inline">\(\hat{H}^{(1)} =
\hat{H}^{(2)} + (V_{ext}^{(1)} - V_{ext}^{(2)})\)</span>。</p></li>
<li><p>代入上式： <span class="math inline">\(\langle \Psi^{(2)} |
\hat{H}^{(1)} | \Psi^{(2)} \rangle = \langle \Psi^{(2)} | \hat{H}^{(2)}
+ V_{ext}^{(1)} - V_{ext}^{(2)} | \Psi^{(2)} \rangle\)</span></p></li>
<li><p>拆分它： <span class="math inline">\(= \langle \Psi^{(2)} |
\hat{H}^{(2)} | \Psi^{(2)} \rangle + \langle \Psi^{(2)} | V_{ext}^{(1)}
- V_{ext}^{(2)} | \Psi^{(2)} \rangle\)</span></p></li>
<li><p>第一项 <span class="math inline">\(\langle \Psi^{(2)} |
\hat{H}^{(2)} | \Psi^{(2)} \rangle\)</span> 正是体系2的基态能量 <span
class="math inline">\(E^{(2)}\)</span>。</p></li>
<li><p>第二项（如上一张白板所示）可以写成关于密度的积分： <span
class="math inline">\(\int (V_{ext}^{(1)} - V_{ext}^{(2)})
\rho^{(2)}(\vec{r}) d\vec{r}\)</span></p></li>
<li><p>根据我们的初始假设，<span
class="math inline">\(\rho^{(2)}(\vec{r}) =
\rho_0(\vec{r})\)</span>。</p></li>
<li><p>将这些组合起来，第1步的变分不等式 <span
class="math inline">\(E^{(1)} &lt; \dots\)</span> 就变成了：
<strong><span class="math inline">\(E^{(1)} &lt; E^{(2)} + \int
(V_{ext}^{(1)} - V_{ext}^{(2)}) \rho_0(\vec{r})
d\vec{r}\)</span></strong> (白板上的第5行)</p></li>
</ul>
<h4 id="对称地应用变分原理-第-3-步">4. 对称地应用变分原理 (第 3 步)</h4>
<p>现在我们反过来，将 <span class="math inline">\(\Psi^{(1)}\)</span>
作为体系2的<strong>试验波函数</strong>。</p>
<ul>
<li><p>根据变分原理： <strong><span class="math inline">\(E^{(2)} &lt;
\langle \Psi^{(1)} | \hat{H}^{(2)} | \Psi^{(1)}
\rangle\)</span></strong></p></li>
<li><p>我们用同样的方法展开 <span class="math inline">\(\hat{H}^{(2)} =
\hat{H}^{(1)} + (V_{ext}^{(2)} - V_{ext}^{(1)})\)</span>： <span
class="math inline">\(\langle \Psi^{(1)} | \hat{H}^{(2)} | \Psi^{(1)}
\rangle = \langle \Psi^{(1)} | \hat{H}^{(1)} | \Psi^{(1)} \rangle +
\langle \Psi^{(1)} | V_{ext}^{(2)} - V_{ext}^{(1)} | \Psi^{(1)}
\rangle\)</span></p></li>
<li><p>这等于： <span class="math inline">\(= E^{(1)} + \int
(V_{ext}^{(2)} - V_{ext}^{(1)}) \rho^{(1)}(\vec{r})
d\vec{r}\)</span></p></li>
<li><p>再次使用我们的假设 <span
class="math inline">\(\rho^{(1)}(\vec{r}) =
\rho_0(\vec{r})\)</span>。</p></li>
<li><p>因此，我们得到了第二个不等式： <strong><span
class="math inline">\(E^{(2)} &lt; E^{(1)} + \int (V_{ext}^{(2)} -
V_{ext}^{(1)}) \rho_0(\vec{r}) d\vec{r}\)</span></strong>
(白板上的第6行)</p></li>
</ul>
<h4 id="导出矛盾-第-4-步">5. 导出矛盾 (第 4 步)</h4>
<p>现在我们把两个不等式（第5行和第6行）相加：</p>
<p><span class="math inline">\(E^{(1)} + E^{(2)} &lt; \left[ E^{(2)} +
\int (V_{ext}^{(1)} - V_{ext}^{(2)}) \rho_0 d\vec{r} \right] + \left[
E^{(1)} + \int (V_{ext}^{(2)} - V_{ext}^{(1)}) \rho_0 d\vec{r}
\right]\)</span></p>
<p>我们来合并右侧的项：</p>
<p><span class="math inline">\(E^{(1)} + E^{(2)} &lt; (E^{(1)} +
E^{(2)}) + \int \underbrace{[(V_{ext}^{(1)} - V_{ext}^{(2)}) +
(V_{ext}^{(2)} - V_{ext}^{(1)})]}_{= 0} \rho_0 d\vec{r}\)</span></p>
<p>右侧的两个积分项<strong>完全抵消</strong>，变成了 0。</p>
<ul>
<li>于是我们得到了最终的荒谬结论： <strong><span
class="math inline">\(E^{(1)} + E^{(2)} &lt; E^{(1)} +
E^{(2)}\)</span></strong> (白板上的最后一行)</li>
</ul>
<h4 id="结论">6. 结论</h4>
<p>“一个数严格小于它自己” (<span class="math inline">\(A &lt;
A\)</span>) 是一个数学上<strong>不可能</strong>的悖论。</p>
<p>这个悖论证明了我们的<strong>初始假设一定是错误的</strong>。</p>
<p>因此，两个不同的外势 <span
class="math inline">\(V_{ext}^{(1)}\)</span> 和 <span
class="math inline">\(V_{ext}^{(2)}\)</span> <strong>不可能</strong>
产生相同的基态密度 <span class="math inline">\(\rho_0\)</span>。</p>
<p><strong>证明完毕：</strong> 体系的基态电子密度 <span
class="math inline">\(\rho_0(\vec{r})\)</span>
<strong>唯一地</strong>决定了其外势 <span
class="math inline">\(V_{ext}(\vec{r})\)</span>，并因此决定了体系的所有基态性质。</p>
<h2 id="更多">7. 更多</h2>
<p>Hohenberg-Kohn (H-K) 定理证明了 <span
class="math inline">\(E[\rho]\)</span>（能量是密度的泛函）的<em>存在性</em>，但它没有告诉我们这个泛函到底长什么样。</p>
<p><strong>问题在于：</strong> 我们不知道电子动能 <span
class="math inline">\(T[\rho]\)</span> 和电子-电子排斥能 <span
class="math inline">\(V_{ee}[\rho]\)</span> 的精确泛函形式。</p>
<p><strong>Kohn-Sham (KS) 理论</strong>
就是为了解决这个问题而提出的。</p>
<p>这个理论的核心思想是：<strong>“我们假装在解一个简单问题，然后把所有的复杂性都藏在一个我们去近似的项里。”</strong></p>
<p>以下是这个实践过程的分解：</p>
<h3 id="引入一个虚拟的无相互作用体系">1.
引入一个“虚拟”的无相互作用体系</h3>
<p>Kohn 和 Sham
假设存在一个<em>虚拟的</em>、<strong>无相互作用</strong>（non-interacting）的电子体系。</p>
<p>这个虚拟体系有一个关键的约束：它被设计为与<em>真实的</em>、有相互作用的体系具有<strong>完全相同的基态电子密度
<span class="math inline">\(\rho(\vec{r})\)</span></strong>。</p>
<p><strong>这为什么有帮助？</strong>
因为对于一个<strong>无相互作用</strong>的体系，我们<strong>精确地知道</strong>它的动能泛函
<span class="math inline">\(T_s[\rho]\)</span>！它就是所有单电子轨道
<span class="math inline">\(\phi_i\)</span> 的动能之和。（<span
class="math inline">\(s\)</span> 代表 “single-particle” 或
“non-interacting”）。</p>
<h3 id="重写总能量泛函-erho">2. 重写总能量泛函 <span
class="math inline">\(E[\rho]\)</span></h3>
<p>现在，Kohn-Sham 将<em>真实</em>体系的总能量 <span
class="math inline">\(E[\rho]\)</span> 重新组织为以下四项：</p>
<p><span class="math inline">\(E[\rho] = T_s[\rho] + \int
V_{ext}(\vec{r}) \rho(\vec{r}) d\vec{r} + E_H[\rho] +
E_{xc}[\rho]\)</span></p>
<p>我们来逐项分析：</p>
<ol type="1">
<li><strong><span
class="math inline">\(T_s[\rho]\)</span>：无相互作用动能</strong>
<ul>
<li>这是我们刚刚引入的<em>虚拟</em>体系的动能。我们<strong>可以精确计算</strong>它（通过求解轨道
<span class="math inline">\(\phi_i\)</span>）。</li>
<li><em>（注：这</em>不是<em>真实体系的动能 <span
class="math inline">\(T[\rho]\)</span>，但它通常是 <span
class="math inline">\(T[\rho]\)</span> 的一个很好的近似。）</em></li>
</ul></li>
<li><strong><span class="math inline">\(\int V_{ext}(\vec{r})
\rho(\vec{r}) d\vec{r}\)</span>：外势能</strong>
<ul>
<li>这是电子-原子核的吸引能。这个泛函是<strong>精确已知</strong>的（就像上一张白板展示的）。</li>
</ul></li>
<li><strong><span class="math inline">\(E_H[\rho]\)</span>：哈特里
(Hartree) 能量</strong>
<ul>
<li><span class="math inline">\(E_H[\rho] = \frac{1}{2} \iint
\frac{\rho(\vec{r})\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|}
d\vec{r}d\vec{r}&#39;\)</span></li>
<li>这是电子密度与其自身相互作用的经典库仑排斥能。这个泛函也是<strong>精确已知</strong>的。</li>
</ul></li>
<li><strong><span class="math inline">\(E_{xc}[\rho]\)</span>：交换-相关
(Exchange-Correlation) 泛函</strong>
<ul>
<li><strong>这是 DFT 实践的核心！</strong></li>
<li><span class="math inline">\(E_{xc}[\rho]\)</span>
被定义为一个“垃圾桶”，它包含了所有我们不知道的、以及我们故意用近似替换掉的<em>所有</em>复杂物理：
<ul>
<li>(<span class="math inline">\(T[\rho] -
T_s[\rho]\)</span>)：真实动能与无相互作用动能之间的差值（即动能的相关部分）。</li>
<li>(<span class="math inline">\(V_{ee}[\rho] -
E_H[\rho]\)</span>)：总电子排斥能与经典库仑排斥能之间的差值（即所有非经典的交换效应和相关效应）。</li>
</ul></li>
</ul></li>
</ol>
<h3 id="kohn-sham-方程实践的工具">3. Kohn-Sham 方程：实践的工具</h3>
<p>现在我们有了能量表达式。根据变分原理（Hohenberg-Kohn
第二定理），我们通过最小化 <span class="math inline">\(E[\rho]\)</span>
来寻找基态密度 <span class="math inline">\(\rho(\vec{r})\)</span>。</p>
<p>对这个能量泛函 <span class="math inline">\(E[\rho]\)</span>
应用变分法，最终会得到一组<strong>类似于薛定谔方程</strong>的单电子方程，这就是著名的
<strong>Kohn-Sham (KS) 方程</strong>：</p>
<p><span class="math inline">\(\left( -\frac{\hbar^2}{2m} \nabla^2 +
V_{eff}(\vec{r}) \right) \phi_i(\vec{r}) = \varepsilon_i
\phi_i(\vec{r})\)</span></p>
<ul>
<li><span class="math inline">\(\phi_i(\vec{r})\)</span> 就是“Kohn-Sham
轨道”，电子密度由它们构成：<span class="math inline">\(\rho(\vec{r}) =
\sum_i |\phi_i(\vec{r})|^2\)</span>。</li>
<li><span class="math inline">\(V_{eff}(\vec{r})\)</span>
是一个“有效势”，无相互作用的电子在这个势场中运动： <span
class="math inline">\(V_{eff}(\vec{r}) = V_{ext}(\vec{r}) + V_H(\vec{r})
+ V_{xc}(\vec{r})\)</span>
<ul>
<li><span class="math inline">\(V_{ext}\)</span>：原子核的势。</li>
<li><span class="math inline">\(V_H\)</span>：电子间的经典库仑势（来自
<span class="math inline">\(E_H\)</span>）。</li>
<li><span
class="math inline">\(V_{xc}\)</span>：<strong>交换-相关势</strong>（来自
<span class="math inline">\(E_{xc}\)</span>）。</li>
</ul></li>
</ul>
<h3 id="唯一的近似泛函动物园">4. 唯一的近似：“泛函动物园”</h3>
<p><strong>Kohn-Sham 理论在形式上是精确的。</strong>
如果我们知道了<em>精确的</em> <span
class="math inline">\(E_{xc}[\rho]\)</span>
泛函，我们将得到体系的精确基态能量和密度。</p>
<p><strong>但在实践中，我们不知道精确的 <span
class="math inline">\(E_{xc}[\rho]\)</span>。</strong></p>
<p>因此，<strong>所有实用的 DFT 计算都变成了对 <span
class="math inline">\(E_{xc}[\rho]\)</span> 的近似。</strong></p>
<p>这就是您可能听说过的所有“泛函”（functionals）的来源，它们是对 <span
class="math inline">\(E_{xc}[\rho]\)</span>
的不同近似，构成了所谓的“泛函动物园”(Functional Zoo)：</p>
<ul>
<li><strong>LDA (局域密度近似):</strong> 最简单的近似，只依赖于 <span
class="math inline">\(\rho\)</span>。</li>
<li><strong>GGA (广义梯度近似):</strong> 依赖于 <span
class="math inline">\(\rho\)</span> 和它的梯度 <span
class="math inline">\(\nabla\rho\)</span> (例如 PBE, BLYP)。</li>
<li><strong>Hybrid (杂化) 泛函:</strong> 混合了一部分 Hartree-Fock
的精确交换（例如 B3LYP, PBE0）。</li>
</ul>
<h3 id="总结实践中的-dft">总结：实践中的 DFT</h3>
<ol type="1">
<li><strong>选择一个近似的 <span
class="math inline">\(E_{xc}[\rho]\)</span> 泛函</strong>（例如
B3LYP）。</li>
<li><strong>猜测</strong>一个初始的电子密度 <span
class="math inline">\(\rho_{guess}\)</span>。</li>
<li>根据 <span class="math inline">\(\rho_{guess}\)</span> 计算出有效势
<span class="math inline">\(V_{eff}\)</span>。</li>
<li>求解 Kohn-Sham 方程，得到一组新的轨道 <span
class="math inline">\(\phi_i\)</span>。</li>
<li>根据新的 <span class="math inline">\(\phi_i\)</span>
计算出一个新的电子密度 <span
class="math inline">\(\rho_{new}\)</span>。</li>
<li>比较 <span class="math inline">\(\rho_{new}\)</span> 和 <span
class="math inline">\(\rho_{guess}\)</span>。如果它们足够接近，计算完成（“自洽”）。</li>
<li>如果不同，则混合新旧密度，返回第 3
步，<strong>循环迭代</strong>直到收敛。</li>
</ol>
<p>Kohn-Sham 理论的伟大之处在于，它将一个极其复杂的 <span
class="math inline">\(N\)</span> 电子问题（如 <span
class="math inline">\(10^{90}\)</span>
维度），<strong>在数学上等价地</strong>转化为了一个（原则上精确的）求解
<span class="math inline">\(N\)</span> 个电子在有效势场中运动的 3
维问题。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/24/5120c7-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/24/5120c7-2/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W7-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-10-24 22:00:00" itemprop="dateCreated datePublished" datetime="2025-10-24T22:00:00+08:00">2025-10-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-11-10 02:03:01" itemprop="dateModified" datetime="2025-11-10T02:03:01+08:00">2025-11-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - 计算能源材料和电子结构模拟 Lecture</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="i">I:</h2>
<h3 id="基本概念与定义">1. 基本概念与定义</h3>
<h4 id="hf-theory-hartree-fock-理论">“HF theory” (Hartree-Fock
理论)</h4>
<p>这是一种在量子化学中用于近似求解多电子体系（如分子）薛定谔方程的<strong>从头计算法
(ab initio method)</strong>。</p>
<ul>
<li><strong>核心思想</strong>:
它将复杂的多电子问题简化为一系列独立的单电子问题。它假设每个电子都在一个由原子核和其他所有电子共同产生的<strong>平均场</strong>中运动。</li>
<li><strong>波函数</strong>: 它使用一个<strong>斯莱特行列式 (Slater
determinant)</strong> 来描述 N
电子体系的波函数，这自动满足了泡利不相容原理（即波函数在交换任意两个电子时反号）。</li>
</ul>
<h4 id="e_hf-neq-sum_i-epsilon_i"><span class="math inline">\(E_{HF}
\neq \sum_{i} \epsilon_i\)</span></h4>
<ul>
<li><strong><span class="math inline">\(E_{HF}\)</span></strong>:
<strong>HF 总能量</strong>。这是整个 N
电子体系的<strong>总能量</strong>，包含了所有电子的动能、电子与原子核的吸引能、以及电子与电子之间的排斥能。</li>
<li><strong><span class="math inline">\(\epsilon_i\)</span></strong>:
<strong>轨道能量 (Orbital Energy)</strong>。这是第 <span
class="math inline">\(i\)</span> 个电子在所有其他 <span
class="math inline">\(N-1\)</span> 个电子的平均场中所具有的能量。</li>
<li><strong><span class="math inline">\(\sum_{i}
\epsilon_i\)</span></strong>:
所有<strong>被占据</strong>轨道能量的总和。</li>
<li><strong>为什么不相等?</strong>:
<ul>
<li>在计算总能量 <span class="math inline">\(E_{HF}\)</span>
时，电子-电子排斥能 <span class="math inline">\(V_{ee}\)</span>
只计算一次。</li>
<li>在计算每个轨道能量 <span class="math inline">\(\epsilon_i\)</span>
时，它包含了第 <span class="math inline">\(i\)</span>
个电子与<strong>所有</strong> <span class="math inline">\(j\)</span>
电子（<span class="math inline">\(j \neq i\)</span>）的排斥。</li>
<li>当你把所有的 <span class="math inline">\(\epsilon_i\)</span> 相加时
(<span class="math inline">\(\sum_i \epsilon_i\)</span>)，每一对电子
(<span class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span>)
之间的排斥能被计算了<strong>两次</strong>（一次在 <span
class="math inline">\(\epsilon_i\)</span> 中，一次在 <span
class="math inline">\(\epsilon_j\)</span> 中）。</li>
<li><strong>正确的公式是</strong>: <span class="math inline">\(E_{HF} =
\sum_{i} \epsilon_i - V_{ee}\)</span>，或者更准确地写为： <span
class="math display">\[E_{HF} = \sum_{i} \epsilon_i - \frac{1}{2}
\sum_{i,j} (J_{ij} - K_{ij})\]</span> (这里的 <span
class="math inline">\(J_{ij}\)</span> 和 <span
class="math inline">\(K_{ij}\)</span>
是下面会讲到的库仑积分和交换积分)。这个 <span
class="math inline">\(\frac{1}{2}\)</span> 就是为了修正重复计算。</li>
</ul></li>
</ul>
<h4 id="band-gap-带隙">“band gap” (带隙)</h4>
<p>这个图显示了两个关键的分子轨道： * <strong>HOMO</strong>:
<strong>Highest Occupied Molecular Orbital</strong>
(最高占据分子轨道)。这是在体系基态下，能量最高的、包含电子的轨道。 *
<strong>LUMO</strong>: <strong>Lowest Unoccupied Molecular
Orbital</strong>
(最低未占分子轨道)。这是在体系基态下，能量最低的、没有电子的轨道。 *
<strong>HOMO-LUMO 隙</strong>: <span class="math inline">\(E_{LUMO} -
E_{HOMO}\)</span>。在分子中，这通常被粗略地称为“带隙”，它大致对应于将电子从基态激发到第一激发态所需的最低能量（光学带隙）。</p>
<h4 id="fundamental-band-gap-i---a">“fundamental band gap = I - A”</h4>
<ul>
<li><strong>解释</strong>:
这是<strong>基本带隙</strong>（或称准粒子带隙）的<strong>严格定义</strong>。它和
HOMO-LUMO 隙是<strong>不同</strong>的概念。</li>
<li>它代表了在不考虑电子-空穴相互作用（激子效应）的情况下，产生一个分离的电子和一个空穴所需的能量。</li>
<li><span class="math inline">\(I - A = [E(N-1) - E(N)] - [E(N) -
E(N+1)] = E(N-1) + E(N+1) - 2E(N)\)</span>。</li>
</ul>
<h4 id="ionization-energy-i-en-1---en-0">“ionization energy: I = E(N-1)
- E(N) &gt; 0”</h4>
<ul>
<li><strong>电离能 (I)</strong>:
<ul>
<li><strong><span class="math inline">\(E(N)\)</span></strong>: 具有 N
个电子的中性体系的基态总能量。</li>
<li><strong><span class="math inline">\(E(N-1)\)</span></strong>:
移走一个电子后，具有 (N-1) 个电子的阳离子的基态总能量。</li>
<li><strong>定义</strong>:
从一个体系中移走一个电子所需的<strong>最小能量</strong>。</li>
<li>因为电子被原子核束缚，移走它需要外界提供能量，所以 <span
class="math inline">\(I\)</span> 总是大于 0。</li>
</ul></li>
</ul>
<h4 id="electron-affinity-a-en---en1-0">“electron affinity: A = E(N) -
E(N+1) &gt; 0”</h4>
<ul>
<li><strong>电子亲和能 (A)</strong>:
<ul>
<li><strong><span class="math inline">\(E(N+1)\)</span></strong>:
增加一个电子后，具有 (N+1) 个电子的阴离子的基态总能量。</li>
<li><strong>定义</strong>:
一个中性体系获得一个电子并形成稳定阴离子时所<strong>释放</strong>的能量。</li>
<li><strong>注意</strong>: 如果 <span class="math inline">\(E(N+1) &lt;
E(N)\)</span>（即阴离子更稳定），则 <span class="math inline">\(A &gt;
0\)</span>，体系释放能量。如果阴离子不稳定（<span
class="math inline">\(E(N+1) &gt; E(N)\)</span>），则 <span
class="math inline">\(A &lt; 0\)</span>。白板上的 <span
class="math inline">\(&gt; 0\)</span> 假设了形成稳定阴离子的情况。</li>
</ul></li>
</ul>
<h3 id="hartree-fock-总能量公式">2. Hartree-Fock 总能量公式</h3>
<p><strong><span class="math inline">\(E_{HF}\)</span>（HF
总能量）的完整表达式</strong>。它由三部分组成：</p>
<h4 id="第一行单电子能量-one-electron-energy">第一行：单电子能量
(One-Electron Energy)</h4>
<p><span class="math display">\[\sum_{i\sigma} \int d\vec{r}
\phi_i^*(\vec{r}) \left( -\frac{\hbar^2}{2m} \nabla^2 + V_{ext} \right)
\phi_i(\vec{r})\]</span></p>
<ul>
<li><strong><span
class="math inline">\(\sum_{i\sigma}\)</span></strong>:
对所有被占据的<strong>自旋轨道 (spin-orbital)</strong> <span
class="math inline">\(i\)</span> (自旋为 <span
class="math inline">\(\sigma\)</span>) 求和。</li>
<li><strong><span class="math inline">\(\int d\vec{r}\)</span></strong>:
对空间坐标 <span class="math inline">\(\vec{r}\)</span> 积分。</li>
<li><strong><span
class="math inline">\(\phi_i(\vec{r})\)</span></strong>: 第 <span
class="math inline">\(i\)</span> 个自旋轨道的波函数。<span
class="math inline">\(\phi_i^*\)</span> 是它的复共轭。</li>
<li><strong><span class="math inline">\(-\frac{\hbar^2}{2m}
\nabla^2\)</span></strong>: <strong>动能算符</strong>。<span
class="math inline">\(\hbar\)</span> 是约化普朗克常数，<span
class="math inline">\(m\)</span> 是电子质量，<span
class="math inline">\(\nabla^2\)</span> 是拉普拉斯算符。</li>
<li><strong><span class="math inline">\(V_{ext}\)</span></strong>:
<strong>外势场</strong>。这是来自所有原子核对电子的<strong>库仑吸引势</strong>。</li>
<li><strong>含义</strong>:
这一整行代表了所有电子的<strong>动能</strong>与它们受到原子核<strong>吸引势能</strong>的总和。</li>
</ul>
<h4 id="第二行库仑项-coulomb-term-j">第二行：库仑项 (Coulomb Term,
J)</h4>
<p><span class="math display">\[+ \frac{e^2}{2} \sum_{i\sigma,
j\sigma&#39;} \iint d\vec{r} d\vec{r}&#39; \frac{|\phi_i(\vec{r})|^2
|\phi_j(\vec{r}&#39;)|^2}{|\vec{r} - \vec{r}&#39;|}\]</span></p>
<ul>
<li><strong><span class="math inline">\(\frac{e^2}{2} \sum_{i\sigma,
j\sigma&#39;}\)</span></strong>: 对<strong>所有</strong>电子对 (<span
class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span>，无论自旋 <span
class="math inline">\(\sigma\)</span> 和 <span
class="math inline">\(\sigma&#39;\)</span> 是否相同) 求和。<span
class="math inline">\(e\)</span> 是电子电荷。</li>
<li><strong><span class="math inline">\(\frac{1}{2}\)</span></strong>:
修正因子，因为求和时 (<span class="math inline">\(i,j\)</span>) 和
(<span class="math inline">\(j,i\)</span>)
被计算了两次，而它们是同一种相互作用。</li>
<li><strong><span
class="math inline">\(|\phi_i(\vec{r})|^2\)</span></strong>: 电子 <span
class="math inline">\(i\)</span> 在 <span
class="math inline">\(\vec{r}\)</span>
处出现的<strong>概率密度</strong>（即电荷密度）。</li>
<li><strong><span class="math inline">\(\frac{...}{|\vec{r} -
\vec{r}&#39;|}\)</span></strong>: 两个点电荷之间的库仑排斥。</li>
<li><strong>含义</strong>: 这是电子 <span
class="math inline">\(i\)</span> 的电荷云和电子 <span
class="math inline">\(j\)</span>
的电荷云之间的<strong>经典静电排斥能</strong>。这是一个纯粹的经典概念。</li>
</ul>
<h4 id="第三行交换项-exchange-term-k">第三行：交换项 (Exchange Term,
K)</h4>
<p><span class="math display">\[- \frac{e^2}{2} \sum_{i\sigma, j\sigma}
\iint d\vec{r} d\vec{r}&#39; \frac{\phi_i^*(\vec{r}) \phi_j(\vec{r})
\phi_j^*(\vec{r}&#39;) \phi_i(\vec{r}&#39;)}{|\vec{r} -
\vec{r}&#39;|}\]</span> <em>（注意：白板上的公式在 <span
class="math inline">\(\phi\)</span>
的变量上似乎有些笔误，这里写的是标准形式）</em> * <strong><span
class="math inline">\(\sum_{i\sigma, j\sigma}\)</span></strong>:
<strong>关键区别！</strong> 这里的求和<strong>只对自旋相同</strong>
(<span class="math inline">\(\sigma\)</span>) 的电子对进行。 *
<strong>含义</strong>:
这是一个纯粹的<strong>量子力学效应</strong>，没有经典对应。它源于波函数必须满足泡利不相容原理（反对称性）。
*
它修正了库仑项。由于泡利不相容，自旋相同的电子有“避开”彼此的倾向（称为<strong>费米空穴
(Fermi
hole)</strong>），这导致它们的实际排斥能<strong>小于</strong>经典的库仑排斥能。交换项
<span class="math inline">\(K\)</span>
是一个正值，因此在总能量中它是一个<strong>负的贡献</strong>（使能量更低，体系更稳定）。</p>
<h3 id="核心对比delta-scf-与-koopmans-定理">3. 核心对比：<span
class="math inline">\(\Delta SCF\)</span> 与 Koopmans 定理</h3>
<p>核心是比较计算 <span class="math inline">\(I\)</span> 和 <span
class="math inline">\(A\)</span> 的两种方法。</p>
<h4 id="delta-scf-delta-scf">“<span class="math inline">\(\Delta
SCF\)</span>” (Delta SCF)</h4>
<ul>
<li><strong>含义</strong>: “<span class="math inline">\(\Delta\)</span>”
(Delta) 指的是<strong>差值</strong>，“SCF” (Self-Consistent Field,
自洽场) 是求解 HF 方程的计算过程。</li>
<li><strong>方法</strong>:
<ol type="1">
<li>对 N 电子体系进行一次<strong>完整的 SCF 计算</strong>，得到总能量
<span class="math inline">\(E(N)\)</span>。</li>
<li>对 (N-1) 电子体系（阳离子）进行<strong>另一次完整的 SCF
计算</strong>，得到总能量 <span
class="math inline">\(E(N-1)\)</span>。</li>
<li><span class="math inline">\(I = E(N-1) - E(N)\)</span>。</li>
</ol></li>
<li><strong>特点</strong>:
<ul>
<li><strong>准确</strong>: 这是在 HF
理论框架内计算电离能的<strong>最准确</strong>方法。</li>
<li><strong>计算量大</strong>: 需要进行两次（或多次）独立的、昂贵的 SCF
计算。</li>
</ul></li>
</ul>
<h4 id="koopmans-theorem-koopmans-定理">“Koopmans’ theorem” (Koopmans
定理)</h4>
<ul>
<li><strong>含义</strong>:
这是一个<strong>近似</strong>方法，它将电离能/电子亲和能与<strong>单次</strong>
HF 计算得到的<strong>轨道能量</strong>直接关联起来。</li>
<li><strong>定理内容</strong>:
<ul>
<li><strong>电离能</strong>: <span class="math inline">\(I \approx
-\epsilon_{HOMO}\)</span> (电离能约等于 HOMO 轨道能量的负值)</li>
<li><strong>电子亲和能</strong>: <span class="math inline">\(A \approx
-\epsilon_{LUMO}\)</span> (电子亲和能约等于 LUMO 轨道能量的负值)</li>
</ul></li>
<li><strong>特点</strong>:
<ul>
<li><strong>近似</strong>: 结果不如 <span class="math inline">\(\Delta
SCF\)</span> 准确。</li>
<li><strong>计算量小</strong>: 只需要对 N
电子体系进行<strong>一次</strong> SCF
计算，就能“免费”得到所有轨道能量，从而估算出 <span
class="math inline">\(I\)</span> 和 <span
class="math inline">\(A\)</span>。</li>
</ul></li>
</ul>
<h4 id="frozen-orbital-approximation-冻结轨道近似">“frozen orbital
approximation” (冻结轨道近似)</h4>
<ul>
<li><strong>含义</strong>: 这是 Koopmans
定理背后的<strong>核心假设</strong>。</li>
<li><strong>内容</strong>: 它假设当你从 N 电子体系中移出一个电子（例如从
HOMO 移出）后，剩下的 (N-1) 个电子的轨道（<span
class="math inline">\(\phi\)</span>）<strong>完全不发生任何改变</strong>（即被“冻结”了）。</li>
<li><strong>推导</strong>: 白板上的 <span class="math inline">\(-I =
E(N) - E(N-1)\)</span> 以及指向 <span
class="math inline">\(E_{HF}\)</span>
公式的箭头，就是在演示这个推导。如果你假设 <span
class="math inline">\(E(N-1)\)</span> 只是 <span
class="math inline">\(E(N)\)</span> 的公式中去掉了所有与 HOMO (设为
<span class="math inline">\(k\)</span> 轨道) 相关的项，那么 <span
class="math inline">\(E(N) - E(N-1)\)</span>
经过数学推导后<strong>恰好等于</strong> <span
class="math inline">\(\epsilon_k\)</span>（<span
class="math inline">\(k\)</span> 轨道的轨道能量）。因此 <span
class="math inline">\(I = -\epsilon_k\)</span>。</li>
</ul>
<h4 id="orbital-relaxation-轨道弛豫">“orbital relaxation”
(轨道弛豫)</h4>
<ul>
<li><strong>含义</strong>: 这是 Koopmans
定理<strong>出错的原因</strong>，也是“冻结轨道近似”所<strong>忽略</strong>的物理现实。</li>
<li><strong>内容</strong>:
<ul>
<li><strong>真实情况是</strong>：当你移出一个电子后，电子-电子排斥<strong>减少</strong>了。</li>
<li>剩下的 (N-1)
个电子会“感觉”到更强的来自原子核的净吸引力，导致它们的轨道会<strong>收缩</strong>并<strong>重新排布</strong>（即“弛豫”），以达到一个新的、能量更低的稳定状态。</li>
<li>因此，通过 <span class="math inline">\(\Delta SCF\)</span>
计算得到的<strong>真实</strong> <span
class="math inline">\(E(N-1)\)</span>（弛豫后的能量）<strong>总是低于</strong>
Koopmans 定理所假设的 <span
class="math inline">\(E(N-1)_{frozen}\)</span>（冻结轨道的能量）。</li>
</ul></li>
<li><strong>结论</strong>:
<ul>
<li><span class="math inline">\(I_{\Delta SCF} = E(N-1)_{relaxed} -
E(N)\)</span></li>
<li><span class="math inline">\(I_{Koopmans} = E(N-1)_{frozen} -
E(N)\)</span></li>
<li>因为 <span class="math inline">\(E(N-1)_{relaxed} &lt;
E(N-1)_{frozen}\)</span>，所以 <strong><span
class="math inline">\(I_{\Delta SCF} &lt;
I_{Koopmans}\)</span></strong>。</li>
<li>换句话说，<strong>Koopmans
定理总是高估（overestimate）电离能</strong>。</li>
</ul></li>
</ul>
<h3 id="总结">总结</h3>
<ul>
<li><strong><span class="math inline">\(\Delta SCF\)</span></strong>:
准确（在 HF
理论内）、计算量大。它通过计算两个不同体系的<strong>总能量之差</strong>来求
<span
class="math inline">\(I\)</span>，并<strong>包含了轨道弛豫</strong>效应。</li>
<li><strong>Koopmans 定理</strong>:
近似、计算量小。它通过一次计算的<strong>轨道能量</strong>来估算 <span
class="math inline">\(I\)</span>，它基于<strong>冻结轨道近似</strong>，<strong>忽略了轨道弛豫</strong>。</li>
</ul>
<p><strong>Koopmans 定理的数学推导</strong>，并将其与<strong>带隙 (band
gap)</strong> 的概念联系起来。</p>
<h2 id="ii">II:</h2>
<h3 id="koopmans-定理的数学推导电离能-i">1. Koopmans
定理的数学推导（电离能 I）</h3>
<p>在数学上证明为什么 <span class="math inline">\(I \approx
-\epsilon_{HOMO}\)</span>。</p>
<ul>
<li><strong>起点</strong>: <span class="math inline">\(-I = E(N) -
E(N-1)\)</span>
<ul>
<li>这是电离能 (I)
定义的变形。我们现在要计算的是，在<strong>冻结轨道近似</strong>下，N
电子体系的总能量 <span class="math inline">\(E(N)\)</span> 与 (N-1)
电子体系的总能量 <span class="math inline">\(E(N-1)_{frozen}\)</span>
之差。</li>
<li>我们假设被移走的电子来自 HOMO 轨道，我们称之为轨道 <span
class="math inline">\(k\)</span>（或白板上的 <span
class="math inline">\(N\)</span>）。</li>
</ul></li>
<li><strong>中间的大型公式</strong>:
<ul>
<li><span class="math inline">\(E(N) - E(N-1)_{frozen}\)</span>
的结果就是白板上写的这些项。当你从总能量 <span
class="math inline">\(E_{HF}\)</span>（在第一张图中有）中减去一个“冻结”的
(N-1)
体系能量时，你剩下的<strong>恰好</strong>是与被移走的那个电子（来自轨道
<span class="math inline">\(k\)</span>）相关的所有能量项。</li>
<li>这些项是：
<ol type="1">
<li><strong>单电子能量</strong>: <span class="math inline">\(\langle
\phi_k | \hat{h} | \phi_k \rangle\)</span> (白板上用 <span
class="math inline">\(\hat{f}(\vec{r})\)</span> 等符号表示)。这是 <span
class="math inline">\(k\)</span>
电子自身的动能和它受到的所有原子核的吸引能。</li>
<li><strong>库仑项 (J)</strong>: <span class="math inline">\(+ e^2
\sum_{j} \iint ...\)</span>。这是 <span class="math inline">\(k\)</span>
电子与<strong>所有</strong>其他 <span class="math inline">\(j\)</span>
电子之间的经典库仑排斥能。</li>
<li><strong>交换项 (K)</strong>: <span class="math inline">\(- e^2
\sum_{j} \iint ...\)</span>。这是 <span class="math inline">\(k\)</span>
电子与所有<strong>同自旋</strong>的 <span
class="math inline">\(j\)</span> 电子之间的量子交换能。</li>
</ol></li>
</ul></li>
<li><strong>关键等式</strong>: <span class="math inline">\(\hat{f}_i
\phi_i = \epsilon_i \phi_i\)</span>
<ul>
<li>这是 <strong>Hartree-Fock 方程</strong>。它定义了<strong>轨道能量
<span class="math inline">\(\epsilon_i\)</span></strong>。</li>
<li><span class="math inline">\(\hat{f}_i\)</span> 是 <strong>Fock
算符</strong>，它本身就包含了上述的三部分能量：单电子能量算符 (<span
class="math inline">\(\hat{h}\)</span>)、所有其他电子的库仑算符 (<span
class="math inline">\(\hat{J}\)</span>) 和交换算符 (<span
class="math inline">\(\hat{K}\)</span>)。</li>
<li>因此，上面那一大堆积分（<span class="math inline">\(k\)</span>
电子的单电子能量 + 它与所有其他电子的 J 和 K
相互作用）<strong>根据定义，就等于 <span
class="math inline">\(\epsilon_k\)</span></strong>！</li>
</ul></li>
<li><strong>推导结论</strong>: <span class="math inline">\(= \epsilon_k
= \epsilon_{HOMO}\)</span>
<ul>
<li>因为 <span class="math inline">\(E(N) - E(N-1)_{frozen} =
\epsilon_k\)</span>（<span class="math inline">\(k\)</span> 是 HOMO
轨道）。</li>
<li>所以 <span class="math inline">\(-I =
\epsilon_{HOMO}\)</span>。</li>
<li><strong>最终得到: <span class="math inline">\(I =
-\epsilon_{HOMO}\)</span></strong></li>
<li><strong>总结</strong>: Koopmans
定理的推导，其核心就是<strong>冻结轨道近似</strong>。这个近似使得 <span
class="math inline">\(E(N)\)</span> 和 <span
class="math inline">\(E(N-1)\)</span>
之间的能量差，<strong>恰好</strong>等于被移走的那个电子的<strong>轨道能量</strong>。</li>
</ul></li>
</ul>
<h3 id="koopmans-定理电子亲和能-a">2. Koopmans 定理（电子亲和能 A）</h3>
<ul>
<li><strong><span class="math inline">\(-A = E(N+1) -
E(N)\)</span></strong>:
<ul>
<li>这是电子亲和能 <span class="math inline">\(A = E(N) -
E(N+1)\)</span> 的变形。</li>
<li>我们现在考虑在<strong>冻结轨道近似</strong>下，向 N 电子体系的 LUMO
轨道<strong>加入</strong>一个电子。</li>
</ul></li>
<li><strong><span class="math inline">\(=
\epsilon_{LUMO}\)</span></strong>:
<ul>
<li><strong>逻辑</strong>: 同样使用冻结轨道近似，我们假设 N
电子体系中所有原来的轨道在加入新电子后<strong>不发生改变</strong>（不弛豫）。</li>
<li>那么，(N+1) 电子体系的总能量 <span
class="math inline">\(E(N+1)_{frozen}\)</span> 与 N 电子体系的总能量
<span class="math inline">\(E(N)\)</span>
之差，就<strong>恰好</strong>等于这个新电子被加入的那个轨道（即
LUMO）的<strong>轨道能量</strong>。</li>
<li>因此，<span class="math inline">\(-A =
\epsilon_{LUMO}\)</span>。</li>
<li><strong>最终得到: <span class="math inline">\(A =
-\epsilon_{LUMO}\)</span></strong></li>
</ul></li>
</ul>
<h3 id="带隙-band-gap">3. 带隙 (Band Gap)</h3>
<ul>
<li><strong><code>E_&#123;gap&#125; = I - A</code></strong>:
<ul>
<li>这是<strong>基本带隙 (Fundamental Gap)</strong>
的<strong>严格定义</strong>。</li>
<li>它代表了从体系中移出一个电子并将其放置到远离体系的无穷远处，然后再从无穷远处拿一个电子放回体系中（假设的），这两个过程的能量差。</li>
<li>它是一个<strong>总能量 (Total Energy)</strong> 的差值，需要三次
<span class="math inline">\(\Delta SCF\)</span> 计算（<span
class="math inline">\(E(N)\)</span>, <span
class="math inline">\(E(N-1)\)</span>, <span
class="math inline">\(E(N+1)\)</span>）才能精确得到。</li>
</ul></li>
<li><strong><code>= \epsilon_&#123;LUMO&#125; - \epsilon_&#123;HOMO&#125;</code></strong>:
<ul>
<li>这是<strong>Koopmans 定理对基本带隙的近似</strong>。</li>
<li>通过代入我们刚刚推导出的近似值：
<ul>
<li><span class="math inline">\(I \approx -\epsilon_{HOMO}\)</span></li>
<li><span class="math inline">\(A \approx -\epsilon_{LUMO}\)</span></li>
</ul></li>
<li>我们得到：<span class="math inline">\(E_{gap} = I - A \approx
(-\epsilon_{HOMO}) - (-\epsilon_{LUMO}) = \epsilon_{LUMO} -
\epsilon_{HOMO}\)</span></li>
<li><strong>关键结论</strong>: <span
class="math inline">\(\epsilon_{LUMO} - \epsilon_{HOMO}\)</span>（即
<strong>HOMO-LUMO 隙</strong>）是<strong>基本带隙</strong> <span
class="math inline">\(I - A\)</span> 的一个近似值。</li>
<li><strong>为什么是近似？</strong> 因为它完全忽略了计算 <span
class="math inline">\(I\)</span> 和 <span
class="math inline">\(A\)</span>
时的<strong>轨道弛豫</strong>效应。</li>
</ul></li>
</ul>
<h3 id="右下角的图示">4. 右下角的图示</h3>
<ul>
<li><strong>HOMO / LUMO</strong>:
显示了最高占据和最低未占两个轨道能级。</li>
<li><strong><code>~1.1 - 1.3 eV</code></strong>:
这是一个<strong>示例</strong>数值，用来表示 HOMO-LUMO
隙的典型大小（这个数值接近于硅的带隙，可能是一个具体的例子）。</li>
<li><strong>上面的竖线</strong>: 代表了 LUMO
之上的一系列能量更高、未被占据的<strong>虚拟轨道 (virtual
orbitals)</strong>。在固体物理中，这对应于<strong>导带 (Conduction
Band)</strong>。</li>
<li><strong>HOMO 下的能级 (未画出)</strong>:
代表了所有能量更低、已被占据的轨道。在固体物理中，这对应于<strong>价带
(Valence Band)</strong>。</li>
</ul>
<h3 id="总结-1">总结</h3>
<p>以上是Koopmans
定理的数学论证，并得出了一个在计算化学和材料科学中非常重要（虽然是近似的）的结论：</p>
<blockquote>
<p><strong>基本带隙 (<span class="math inline">\(I - A\)</span>)
可以通过单次 HF 计算得到的 HOMO-LUMO 隙 (<span
class="math inline">\(\epsilon_{LUMO} - \epsilon_{HOMO}\)</span>)
来估算。</strong></p>
</blockquote>
<p>这个近似的准确性取决于 “冻结轨道近似” 带来的误差（忽略轨道弛豫）与 HF
理论本身带来的误差（忽略电子相关能）在多大程度上相互抵消。</p>
<h2 id="iii">III:</h2>
<p><strong>“后-HF” (post-Hartree-Fock) 方法</strong>。</p>
<p>在前面的讨论中，我们确定了 HF 理论的两个主要缺陷： 1.
<strong>忽略了轨道弛豫</strong>（导致 Koopmans 定理不准）。 2.
<strong>忽略了电子相关能</strong>（HF
是一个平均场理论，没有考虑电子的瞬时“躲避”行为）。</p>
<p>介绍<strong>如何修正第二个（也是更根本的）缺陷</strong>。这个方法叫做
<strong>“Configuration Interaction (CI)” (组态相互作用)</strong>。</p>
<h3 id="phi_hf-frac1sqrtn-dots-slater-determinant">1. 📖 <span
class="math inline">\(\Phi_{HF} = \frac{1}{\sqrt{N!}} | \dots |\)</span>
(Slater Determinant)</h3>
<ul>
<li><strong>含义</strong>: 这是 Hartree-Fock (HF) 波函数 (<span
class="math inline">\(\Phi_{HF}\)</span>)
的<strong>数学定义</strong>。</li>
<li><strong><span
class="math inline">\(\frac{1}{\sqrt{N!}}\)</span></strong>:
归一化常数。</li>
<li><strong><span class="math inline">\(| \dots |\)</span></strong>:
这是一个<strong>斯莱特行列式 (Slater Determinant)</strong>。
<ul>
<li>例如，对于一个 2 电子体系（如氦原子），它写作： <span
class="math display">\[\Phi(1,2) = \frac{1}{\sqrt{2}} \begin{vmatrix}
\phi_1(1) &amp; \phi_2(1) \\ \phi_1(2) &amp; \phi_2(2) \end{vmatrix} =
\frac{1}{\sqrt{2}} [\phi_1(1)\phi_2(2) -
\phi_1(2)\phi_2(1)]\]</span></li>
<li><span class="math inline">\(\phi_1(1)\)</span> 表示电子 1 处于 <span
class="math inline">\(\phi_1\)</span> 自旋轨道。</li>
</ul></li>
<li><strong>目的</strong>:
这种行列式形式是满足<strong>泡利不相容原理</strong>（波函数在交换任意两个电子时必须反号）的最简洁的数学工具。</li>
</ul>
<p>HF 理论假设体系的<strong>真实波函数 <span
class="math inline">\(\Psi\)</span></strong>
可以被<strong>单个</strong>斯莱特行列式 <span
class="math inline">\(\Phi_{HF}\)</span>
<strong>很好地近似</strong>。</p>
<h3 id="configuration-interaction-ci-组态相互作用">2. ⚡ “Configuration
Interaction (CI)” (组态相互作用)</h3>
<ul>
<li><strong>核心思想</strong>: HF 理论的“平均场”假设（即 <span
class="math inline">\(\Psi \approx
\Phi_{HF}\)</span>）是一个<strong>近似</strong>。一个<strong>更精确</strong>的波函数
<span class="math inline">\(\Psi\)</span>
应该是一个<strong>线性组合</strong>，它不仅包含 HF 基态组态 <span
class="math inline">\(\Phi_{HF}\)</span>，还包含所有可能的<strong>激发组态
(excited configurations)</strong>。</li>
<li><strong>能级图</strong>:
<ul>
<li>图显示了 HF 基态。轨道 1 和 2 被电子占据（<strong>占据轨道,
occ</strong>），轨道 3, 4, 5… 是空的（<strong>虚拟轨道,
virt</strong>）。</li>
<li>这个基态组态就是 <span
class="math inline">\(\Phi_{HF}\)</span>。</li>
</ul></li>
<li><strong>什么是激发组态?</strong>
<ul>
<li><strong>单激发 (Singles, S)</strong>: 将一个电子从一个占据轨道
(<span class="math inline">\(i\)</span>) 激发到一个虚拟轨道 (<span
class="math inline">\(a\)</span>)。记作 <span
class="math inline">\(\Phi_i^a\)</span>。</li>
<li><strong>双激发 (Doubles, D)</strong>: 将两个电子从占据轨道 (<span
class="math inline">\(i, j\)</span>) 激发到虚拟轨道 (<span
class="math inline">\(a, b\)</span>)。记作 <span
class="math inline">\(\Phi_{ij}^{ab}\)</span>。</li>
<li><strong>三激发 (Triples, T)</strong>: …以此类推。</li>
</ul></li>
<li><strong>CI 波函数</strong>: 真正的基态波函数 <span
class="math inline">\(\Psi\)</span> 是所有这些可能组态的叠加： <span
class="math display">\[\Psi = C_0 \Phi_{HF} + \sum_{i,a} C_i^a \Phi_i^a
+ \sum_{i,j,a,b} C_{ij}^{ab} \Phi_{ij}^{ab} + \sum_{i,j,k,a,b,c}
C_{ijk}^{abc} \Phi_{ijk}^{abc} + \dots\]</span>
<ul>
<li><span class="math inline">\(C_0\)</span> 是 HF 基态的系数（通常接近
1）。</li>
<li><span class="math inline">\(C_i^a\)</span>, <span
class="math inline">\(C_{ij}^{ab}\)</span>…
是各种激发组态的<strong>系数</strong>（或“权重”）。</li>
<li><strong>CI
方法的目的</strong>就是通过求解一个巨大的矩阵本征值问题来找出所有这些
<span class="math inline">\(C\)</span> 系数，从而获得更精确的波函数
<span class="math inline">\(\Psi\)</span> 和能量 <span
class="math inline">\(E\)</span>。</li>
</ul></li>
</ul>
<h3 id="ci-公式">3. CI 公式</h3>
<p>总波函数 <span class="math inline">\(\Psi\)</span>
的各个组成部分：</p>
<ul>
<li><strong><span class="math inline">\(\Psi^{(1)} =
\sum_{i}^{\text{occ}} \sum_{a}^{\text{virt}} C_i^a
\Phi_i^a\)</span></strong>
<ul>
<li>这是波函数中所有<strong>单激发 (S)</strong> 组态的总和。</li>
</ul></li>
<li><strong><span class="math inline">\(\Psi^{(2)} =
\sum_{i&lt;j}^{\text{occ}} \sum_{a&lt;b}^{\text{virt}} C_{ij}^{ab}
\Phi_{ij}^{ab}\)</span></strong>
<ul>
<li>这是波函数中所有<strong>双激发 (D)</strong> 组态的总和。</li>
</ul></li>
<li><strong><span class="math inline">\(\Psi^{(3)} =
\sum_{i&lt;j&lt;k}^{\text{occ}} \sum_{a&lt;b&lt;c}^{\text{virt}}
C_{ijk}^{abc} \Phi_{ijk}^{abc}\)</span></strong>
<ul>
<li>这是波函数中所有<strong>三激发 (T)</strong> 组态的总和。</li>
</ul></li>
</ul>
<h3 id="关键概念">4. 🔑 关键概念</h3>
<ul>
<li><strong><span class="math inline">\(\langle \Phi_{HF} | \Phi_i^a
\rangle = 0\)</span></strong>
<ul>
<li><strong>含义</strong>: HF
基态波函数与<strong>任意一个</strong>单激发态波函数都是<strong>正交</strong>的（即它们的重叠积分为
0）。</li>
<li><strong>重要推论 (Brillouin 定理)</strong>: 这意味着 HF 基态 <span
class="math inline">\(\Phi_{HF}\)</span> 不会与单激发态 <span
class="math inline">\(\Phi_i^a\)</span> 直接混合。换句话说，在 CI
波函数中，所有的 <span class="math inline">\(C_i^a\)</span> 系数都将为
0（<em>如果</em> <span class="math inline">\(\Phi_{HF}\)</span> 是严格的
HF 波函数）。</li>
<li><strong>结论</strong>: 对 HF
基态能量的<strong>第一个修正</strong>来自于<strong>双激发
(Doubles)</strong>。这就是为什么双激发在电子相关能中如此重要。</li>
</ul></li>
<li><strong>Full CI (FCI, 全组态相互作用)</strong>
<ul>
<li>如果你在 CI
展开式中包含了<strong>所有</strong>可能的激发（单、双、三、…直到 N
激发），你就得到了 <strong>Full CI</strong>。</li>
<li>FCI 在给定的基组下，是求解薛定谔方程的<strong>精确解</strong>。</li>
<li><strong>问题</strong>:
计算量是<strong>天文数字</strong>（随体系大小呈指数增长），只能用于几个电子的微小体系。</li>
</ul></li>
<li><strong>Truncated CI (截断 CI)</strong>
<ul>
<li>由于 FCI 不可行，人们在实际中会<strong>截断</strong>这个求和。</li>
<li><strong>CISD</strong>: 只包含<strong>单激发 (S)</strong> 和
<strong>双激发 (D)</strong>。这是最常见的 CI 方法之一。</li>
<li><strong>CISDT</strong>: 包含单、双、三激发。</li>
</ul></li>
</ul>
<h3 id="总结-2">总结</h3>
<p>从 HF 理论（一个<strong>单行列式</strong>近似）转向了 <strong>CI
理论</strong>（一个<strong>多行列式</strong>方法）。</p>
<p><strong>CI 的核心目的</strong>：通过将激发态组态（<span
class="math inline">\(\Phi_i^a\)</span>, <span
class="math inline">\(\Phi_{ij}^{ab}\)</span> 等）“混合”到 HF 基态组态
(<span class="math inline">\(\Phi_{HF}\)</span>)
中，来系统地<strong>恢复</strong> HF
理论所忽略的<strong>电子相关能</strong>。</p>
<p><strong>“二次量子化” (Second Quantization)</strong>。</p>
<p>它是一种用来处理多粒子系统（如此处的电子）的强大工具，特别擅长描述粒子的产生和湮灭。</p>
<h3 id="核心工具产生与湮灭算符">1. 核心工具：产生与湮灭算符</h3>
<p>在二次量子化中，我们不再纠结于写出完整的、庞大的斯莱特行列式（像
<span class="math inline">\(\Phi_{HF}\)</span>
那样），而是定义两个基本的操作符：</p>
<ul>
<li><strong><span class="math inline">\(a_p^\dagger\)</span> (产生算符,
Creation Operator)</strong>:
<ul>
<li><strong>作用</strong>: 当它作用在一个波函数上时，它会在<strong>轨道
<span class="math inline">\(p\)</span></strong>
中<strong>产生</strong>一个电子。</li>
<li>例如，如果 <span class="math inline">\(p\)</span> 轨道是空的， <span
class="math inline">\(a_p^\dagger\)</span> 会把一个电子放进去。</li>
<li>如果 <span class="math inline">\(p\)</span>
轨道已经被占据，根据泡利不相容原理， <span
class="math inline">\(a_p^\dagger |\phi_p\rangle = 0\)</span>
(你不能在同一个自旋轨道上放两个电子)。</li>
</ul></li>
<li><strong><span class="math inline">\(a_p\)</span> (湮灭算符,
Annihilation Operator)</strong>:
<ul>
<li><strong>作用</strong>: 当它作用在一个波函数上时，它会从<strong>轨道
<span class="math inline">\(p\)</span></strong>
中<strong>湮灭</strong>（或移除）一个电子。</li>
<li>例如，如果 <span class="math inline">\(p\)</span> 轨道是占据的，
<span class="math inline">\(a_p\)</span> 会把这个电子移走。</li>
<li>如果 <span class="math inline">\(p\)</span> 轨道本来就是空的， <span
class="math inline">\(a_p |\text{empty}\rangle = 0\)</span>
(你不能从一个空轨道中移走电子)。</li>
</ul></li>
</ul>
<h3 id="定义基态-phi_hf">2. 定义基态 <span
class="math inline">\(\Phi_{HF}\)</span></h3>
<ul>
<li>首先，我们定义一个真正的<strong>“真空态” (vacuum
state)</strong>，记作 <span
class="math inline">\(|0\rangle\)</span>，表示一个<strong>完全没有电子</strong>的空荡荡的体系。</li>
<li>那么，<strong>HF 基态波函数 <span
class="math inline">\(\Phi_{HF}\)</span></strong>（即所有占据轨道 <span
class="math inline">\(i, j, k, \dots\)</span>
都被填满的状态）就可以通过从真空态开始，不断“产生”电子来构建： <span
class="math display">\[|\Phi_{HF}\rangle = a_1^\dagger a_2^\dagger
a_3^\dagger \dots a_N^\dagger |0\rangle\]</span> (这里 <span
class="math inline">\(1, 2, \dots, N\)</span> 是所有被占据的轨道)。</li>
</ul>
<h3 id="解释">3. 解释</h3>
<p>。它们展示了如何从已知的 HF 基态 <span
class="math inline">\(|\Phi_{HF}\rangle\)</span>
<strong>构建</strong>出各种激发态。</p>
<h4
id="phi_ia-a_adagger-a_i-phi_hf"><code>| $\Phi_i^a$ &gt; = $a_a^\dagger a_i$ | $\Phi_&#123;HF&#125;$ &gt;</code></h4>
<ul>
<li><strong>含义</strong>: 这个公式在说：“单激发态 <span
class="math inline">\(\Phi_i^a\)</span> 是如何得到的？”</li>
<li><strong>操作</strong>:
<ol type="1">
<li>从 HF 基态 <span class="math inline">\(|\Phi_{HF}\rangle\)</span>
开始。</li>
<li>首先，应用<strong>湮灭算符 <span
class="math inline">\(a_i\)</span></strong>。这会从<strong>占据轨道
<span class="math inline">\(i\)</span></strong>
中<strong>移走</strong>一个电子。</li>
<li>然后，应用<strong>产生算符 <span
class="math inline">\(a_a^\dagger\)</span></strong>。这会在<strong>虚拟轨道
<span class="math inline">\(a\)</span></strong>
中<strong>产生</strong>一个电子。</li>
</ol></li>
<li><strong>结果</strong>: 这个两步操作（“先湮灭 <span
class="math inline">\(i\)</span>，再产生 <span
class="math inline">\(a\)</span>”）的效果，就是将一个电子从轨道 <span
class="math inline">\(i\)</span> 激发到了轨道 <span
class="math inline">\(a\)</span>。这<strong>正是</strong>单激发态 <span
class="math inline">\(\Phi_i^a\)</span> 的定义。</li>
</ul>
<h4
id="phi_ijab-a_adagger-a_bdagger-a_j-a_i-phi_hf"><code>| $\Phi_&#123;ij&#125;^&#123;ab&#125;$ &gt; = $a_a^\dagger a_b^\dagger a_j a_i$ | $\Phi_&#123;HF&#125;$ &gt;</code></h4>
<p><em>(白板上这个公式被部分遮挡了，但这是它的标准形式)</em></p>
<ul>
<li><strong>含义</strong>: 这个公式在说：“双激发态 <span
class="math inline">\(\Phi_{ij}^{ab}\)</span> 是如何得到的？”</li>
<li><strong>操作</strong>:
<ol type="1">
<li>从 HF 基态 <span class="math inline">\(|\Phi_{HF}\rangle\)</span>
开始。</li>
<li>应用 <span class="math inline">\(a_i\)</span>：从<strong>占据轨道
<span class="math inline">\(i\)</span></strong>
中<strong>移走</strong>一个电子。</li>
<li>应用 <span class="math inline">\(a_j\)</span>：从<strong>占据轨道
<span class="math inline">\(j\)</span></strong>
中<strong>移走</strong>另一个电子。</li>
<li>应用 <span
class="math inline">\(a_b^\dagger\)</span>：在<strong>虚拟轨道 <span
class="math inline">\(b\)</span></strong>
中<strong>产生</strong>一个电子。</li>
<li>应用 <span
class="math inline">\(a_a^\dagger\)</span>：在<strong>虚拟轨道 <span
class="math inline">\(a\)</span></strong>
中<strong>产生</strong>另一个电子。</li>
</ol></li>
<li><strong>结果</strong>: 这个四步操作（“湮灭 <span
class="math inline">\(i, j\)</span>；产生 <span class="math inline">\(a,
b\)</span>”）的效果，就是将两个电子从轨道 <span class="math inline">\(i,
j\)</span> 激发到了轨道 <span class="math inline">\(a,
b\)</span>。这<strong>正是</strong>双激发态 <span
class="math inline">\(\Phi_{ij}^{ab}\)</span> 的定义。</li>
</ul>
<h3 id="为什么这个方法更好">4. 为什么这个方法更好？</h3>
<ul>
<li><strong>简洁性</strong>: 你不需要再写出庞大的 <span
class="math inline">\(N \times N\)</span> 行列式。你只需要简单地写 <span
class="math inline">\(a_a^\dagger a_i |\Phi_{HF}\rangle\)</span>
就可以<strong>精确地代表</strong>那个单激发的斯莱特行列式。</li>
<li><strong>自动处理“交换 (exchange)”</strong>: 白板上提到了
“exchange”。这些产生/湮灭算符被定义为<strong>费米子算符</strong>，它们自动满足一个称为<strong>“反对易关系”
(anti-commutation relation)</strong> 的规则。
<ul>
<li>例如：<span class="math inline">\(a_i a_j = -a_j a_i\)</span> 且
<span class="math inline">\(a_i^\dagger a_j^\dagger = -a_j
a_i^\dagger\)</span>。</li>
<li>这个负号<strong>自动地</strong>包含了泡利不相容原理和波函数的反对称性（即“交换”效应）。你不需要再手动去操心行列式的正负号问题。</li>
</ul></li>
</ul>
<p><strong>总结</strong>:
<strong>二次量子化</strong>的语言，它是一种更强大、更数学化的方式，用来描述
CI（组态相互作用）理论中如何从 HF 基态构建出各种激发态。</p>
<h2 id="iv">IV:</h2>
<p>CI（组态相互作用）理论的总结性笔记。</p>
<h3 id="波函数基组">1. 波函数基组</h3>
<p><code>&#123; $\Phi_&#123;HF&#125;, \Phi_i^a, \Phi_&#123;ij&#125;^&#123;ab&#125;, \Phi_&#123;ijk&#125;^&#123;abc&#125;, \dots$ &#125;</code></p>
<ul>
<li><strong>含义</strong>: 这是一个<strong>集合
(set)</strong>。它代表了在给定的单电子轨道基组下，所有可能构建出来的 N
电子<strong>组态 (configurations)</strong>。</li>
<li><strong><span class="math inline">\(\Phi_{HF}\)</span></strong>: HF
基态组态（即电子占据能量最低的 N 个轨道）。</li>
<li><strong><span class="math inline">\(\Phi_i^a\)</span></strong>:
所有的<strong>单激发 (Singles)</strong> 组态。</li>
<li><strong><span
class="math inline">\(\Phi_{ij}^{ab}\)</span></strong>:
所有的<strong>双激发 (Doubles)</strong> 组态。</li>
<li><strong><span
class="math inline">\(\Phi_{ijk}^{abc}\)</span></strong>:
所有的<strong>三激发 (Triples)</strong> 组态。</li>
<li><strong>…</strong>: 一直持续到 N 激发（即所有电子都被激发）。</li>
<li><strong>重要性</strong>:
这个无穷（但在有限基组下是有限的）集合构成了一个<strong>完备的 N
电子基组</strong>。这意味着<strong>任何</strong>一个真实的 N 电子波函数
<span
class="math inline">\(\Psi\)</span>（即薛定谔方程的精确解），都可以被<strong>精确地</strong>表示为这个集合中所有组态的线性组合：
<span class="math display">\[\Psi = C_0 \Phi_{HF} + \sum C_i^a \Phi_i^a
+ \sum C_{ij}^{ab} \Phi_{ij}^{ab} + \dots\]</span></li>
</ul>
<h3 id="exchange-交换-vs-correlation-相关">2. “exchange” (交换) vs
“correlation” (相关)</h3>
<ul>
<li><strong>“exchange” (交换能)</strong>:
<ul>
<li>白板上将 “exchange” 指向了 HF 基态 <span
class="math inline">\(\Phi_{HF}\)</span>。</li>
<li><strong>含义</strong>: 这是 HF
理论<strong>已经包含</strong>在内的量子效应。</li>
<li>它源于泡利不相容原理，即两个自旋相同的电子不能占据同一空间位置。这导致同自旋电子之间存在一个“费米空穴
(Fermi hole)”，使它们倾向于“避开”彼此。</li>
<li>在 HF 能量公式中（见第一张白板），这表现为那个<strong>负的交换积分
(-K)</strong> 项。</li>
</ul></li>
<li><strong>“correlation” (相关能)</strong>:
<ul>
<li>白板上将 “correlation” 指向了<strong>所有激发态</strong>（<span
class="math inline">\(\Phi_i^a, \Phi_{ij}^{ab}, \dots\)</span>）。</li>
<li><strong>含义</strong>: 这是 HF
理论<strong>所忽略</strong>的效应。</li>
<li><strong>HF 的缺陷</strong>: HF
只考虑了同自旋电子的“交换”相关，但它<strong>忽略了不同自旋电子</strong>之间的瞬时相互作用。在
HF（平均场）理论中，一个自旋向上的电子只感受到一个自旋向下电子云的<strong>平均</strong>排斥，而没有根据它俩的瞬时位置来“躲避”对方。</li>
<li><strong>CI 的修正</strong>:
通过在波函数中<strong>混入激发态</strong>（如 <span
class="math inline">\(\Phi_{ij}^{ab}\)</span>），CI
方法允许电子（包括不同自旋的电子）的运动<strong>相互关联</strong>起来，从而更有效地“躲避”彼此，进一步降低体系的总能量。</li>
<li><strong>定义</strong>: <strong>相关能 = 真实能量 - HF
能量</strong>。</li>
<li><strong>结论</strong>: <strong>CI 方法（以及其他 post-HF
方法）的目的就是为了“恢复” HF 理论所丢失的电子相关能</strong>。</li>
</ul></li>
</ul>
<h3 id="能量图示">3. 能量图示</h3>
<p>能量图：</p>
<ul>
<li><strong><span class="math inline">\(\uparrow\)</span></strong>:
能量轴，能量向上增加。</li>
<li><strong><code>HF</code></strong>: <strong>Hartree-Fock 能量（<span
class="math inline">\(E_{HF}\)</span>）</strong>。
<ul>
<li>这是通过<strong>单个</strong>斯莱特行列式 <span
class="math inline">\(\Phi_{HF}\)</span> 计算得到的能量。</li>
<li>这是一个<strong>近似</strong>能量，它<strong>高于</strong>真实的基态能量。</li>
</ul></li>
<li><strong><code>Full CI</code></strong>:
<strong>全组态相互作用能量（<span
class="math inline">\(E_{FCI}\)</span>）</strong>。
<ul>
<li>这是通过将 <span class="math inline">\(\Psi\)</span>
表示为<strong>所有</strong>可能组态（<span
class="math inline">\(\Phi_{HF}, \Phi_i^a,
\dots\)</span>）的线性组合，并求解得到的<strong>精确</strong>基态能量（在给定基组下的）。</li>
<li><strong><span class="math inline">\(E_{FCI}\)</span>
是我们能得到的最低、最准确的能量</strong>。</li>
</ul></li>
<li><strong><code>&#123;</code> (之间的能量差)</strong>:
<ul>
<li><strong><span class="math inline">\(E_{FCI}\)</span> 和 <span
class="math inline">\(E_{HF}\)</span>
之间的能量差，正是我们前面定义的“电子相关能 (Correlation
Energy)”</strong>。</li>
<li><span class="math display">\[E_{\text{corr}} = E_{FCI} -
E_{HF}\]</span></li>
</ul></li>
</ul>
<h3 id="二次量子化复习">4. 二次量子化（复习）</h3>
<ul>
<li><strong><code>| $\Phi_i^a$ &gt; = $a_a^\dagger a_i$ | $\Phi_&#123;HF&#125;$ &gt;</code></strong>
(单激发)</li>
<li><strong><code>| $\Phi_&#123;ij&#125;^&#123;ab&#125;$ &gt; = $a_a^\dagger a_b^\dagger a_j a_i$ | $\Phi_&#123;HF&#125;$ &gt;</code></strong>
(双激发)
<ul>
<li>这重申了上一张白板的概念：使用<strong>产生 (<span
class="math inline">\(a^\dagger\)</span>)</strong> 和<strong>湮灭 (<span
class="math inline">\(a\)</span>)</strong> 算符，是从 HF
基态构建所有其他激发态的最简洁的数学方式。</li>
</ul></li>
<li><strong><code>&#123; $a_i, a_j^\dagger$ &#125; = $\delta_&#123;ij&#125;$</code></strong>
<ul>
<li><strong>含义</strong>: 这是费米子算符的<strong>反对易关系
(Anti-commutation relation)</strong>。</li>
<li><strong><code>&#123;A, B&#125;</code></strong> 是一个简写，表示 <span
class="math inline">\(AB + BA\)</span>。</li>
<li>所以，这个公式的完整形式是：<strong><span class="math inline">\(a_i
a_j^\dagger + a_j^\dagger a_i = \delta_{ij}\)</span></strong></li>
<li><strong><span class="math inline">\(\delta_{ij}\)</span></strong>:
<strong>克罗内克 delta 符号 (Kronecker delta)</strong>。
<ul>
<li>如果 <span class="math inline">\(i = j\)</span>，<span
class="math inline">\(\delta_{ij} = 1\)</span>。</li>
<li>如果 <span class="math inline">\(i \neq j\)</span>，<span
class="math inline">\(\delta_{ij} = 0\)</span>。</li>
</ul></li>
<li><strong>物理意义</strong>:
<ul>
<li><strong>如果 <span class="math inline">\(i \neq j\)</span></strong>:
<span class="math inline">\(a_i a_j^\dagger = -a_j^\dagger
a_i\)</span>。这表示“在 <span class="math inline">\(j\)</span>
轨道产生一个电子，再在 <span class="math inline">\(i\)</span>
轨道湮灭一个电子”与“先在 <span class="math inline">\(i\)</span>
湮灭，再在 <span class="math inline">\(j\)</span>
产生”这两个操作的顺序是<strong>相反</strong>的（导致波函数变号）。</li>
<li><strong>如果 <span class="math inline">\(i = j\)</span></strong>:
<span class="math inline">\(a_i a_i^\dagger + a_i^\dagger a_i =
1\)</span>。这个规则用于计算占据数，并确保了泡利不相容原理的正确执行。</li>
</ul></li>
</ul></li>
</ul>
<h3 id="总结-3">总结</h3>
<p>清晰地总结了： 1. <strong>HF
理论</strong>只是一种<strong>近似</strong>，它包含了<strong>交换能</strong>。
2. 为了获得<strong>精确解 (Full CI)</strong>，必须将 HF
基态与所有可能的<strong>激发态</strong>相混合。 3.
这个混合过程所恢复的能量，就是 HF
理论所忽略的<strong>电子相关能</strong>。 4. 二次量子化（<span
class="math inline">\(a^\dagger, a\)</span>
算符）是实现这一目标的标准数学工具。</p>
<p>这一系列白板笔记非常连贯地从 HF 理论的基础，讲到了 Koopmans
定理的近似，最后引入了更高级的 CI 方法来修正 HF
的根本缺陷（即电子相关能）。</p>
<h2 id="量子化学指南从-hartree-fock-到组态相互作用">量子化学指南：从
Hartree-Fock 到组态相互作用</h2>
<p>这份指南涵盖了量子化学中的两个核心主题： 1. <strong>Hartree-Fock (HF)
理论</strong>：一种基础的、近似求解多电子体系的方法。 2.
<strong>组态相互作用 (CI)</strong>：一种 “后-HF” 方法，用于系统地修正 HF
理论的缺陷，以逼近精确解。</p>
<h3 id="第一部分hartree-fock-hf-理论基础">第一部分：Hartree-Fock (HF)
理论基础</h3>
<p>HF 理论是求解多电子体系薛定谔方程的第一个、也是最重要的<strong>平均场
(Mean-Field)</strong> 近似。</p>
<h4 id="核心思想与波函数">1. 核心思想与波函数</h4>
<ul>
<li><strong>核心思想</strong>：HF
理论将复杂的多电子问题（电子的运动是相互关联的）简化为一系列独立的单电子问题。它假设每个电子都在一个由原子核和其他所有电子共同产生的<strong>平均</strong>静电场中运动。</li>
<li><strong>波函数 (<span
class="math inline">\(\Phi_{HF}\)</span>)</strong>：HF
理论使用<strong>单个斯莱特行列式 (Slater Determinant)</strong> 来描述 N
电子体系的基态波函数。 <span class="math display">\[\Phi_{HF} =
\frac{1}{\sqrt{N!}} \begin{vmatrix} \phi_1(1) &amp; \phi_2(1) &amp;
\dots \\ \phi_1(2) &amp; \phi_2(2) &amp; \dots \\ \vdots &amp; \vdots
&amp; \ddots \end{vmatrix}\]</span>
这种形式自动满足了泡利不相容原理（即波函数在交换任意两个电子时反号）。</li>
</ul>
<h4 id="hf-总能量-e_hf">2. HF 总能量 (<span
class="math inline">\(E_{HF}\)</span>)</h4>
<p>HF 体系的总能量 <span
class="math inline">\(E_{HF}\)</span>（如第一张白板上的公式所示）由三部分组成：</p>
<ol type="1">
<li><strong>单电子能量</strong>：所有电子的动能，以及它们与所有原子核的吸引势能。</li>
<li><strong>库仑能 (J)</strong>：电子 <span
class="math inline">\(i\)</span> 的电荷云与电子 <span
class="math inline">\(j\)</span>
的电荷云之间的<strong>经典静电排斥能</strong>。</li>
<li><strong>交换能
(K)</strong>：一个纯粹的量子效应，源自泡利不相容原理。它仅存在于<strong>自旋相同</strong>的电子之间。这个能量项是<strong>负值</strong>，它降低了体系的总能量，可以理解为同自旋电子有“避开”彼此的倾向（<strong>费米空穴,
Fermi Hole</strong>）。</li>
</ol>
<h4 id="轨道能量-epsilon_i-vs-总能量-e_hf">3. 轨道能量 (<span
class="math inline">\(\epsilon_i\)</span>) vs 总能量 (<span
class="math inline">\(E_{HF}\)</span>)</h4>
<ul>
<li><strong>轨道能量 (<span
class="math inline">\(\epsilon_i\)</span>)</strong>：是在求解 HF
方程（<span class="math inline">\(\hat{f}\phi_i =
\epsilon_i\phi_i\)</span>）时得到的本征值。它代表了电子 <span
class="math inline">\(i\)</span> 在<strong>所有</strong>其他 (N-1)
个电子的平均场中的能量。</li>
<li><strong>关键区别</strong>：总能量 <strong>不等于</strong>
轨道能量之和。 <span class="math display">\[E_{HF} \neq \sum_{i}
\epsilon_i\]</span> <strong>原因</strong>：在 <span
class="math inline">\(\sum \epsilon_i\)</span>
中，每一对电子之间的排斥能（库仑和交换）都被计算了<strong>两次</strong>。正确的公式是
<span class="math inline">\(E_{HF} = \sum \epsilon_i -
V_{ee}\)</span>，其中 <span class="math inline">\(V_{ee}\)</span>
是电子-电子排斥能项，用于修正重复计算。</li>
</ul>
<h3 id="第二部分物理可观测量-i-和-a">第二部分：物理可观测量 (I 和
A)</h3>
<p>HF 理论计算完成后，我们希望从中提取有物理意义的数据，例如电离能 (I)
和电子亲和能 (A)。有两种主要方法：</p>
<h4 id="deltascf-delta-scf-方法总能量之差">1. <span
class="math inline">\(\Delta\)</span>SCF (Delta-SCF)
方法：总能量之差</h4>
<p>这是在 HF 理论框架内<strong>最准确</strong>的方法。“<span
class="math inline">\(\Delta\)</span>” (Delta) 指的就是”差值”。</p>
<ul>
<li><strong>电离能 (I)</strong>：从 N 电子体系移出一个电子所需的能量。
&gt; <span class="math inline">\(I = E(N-1) - E(N)\)</span>
<ul>
<li><strong>计算</strong>：你需要分别对 N 电子体系和 (N-1)
电子体系（阳离子）进行<strong>两次独立</strong>的 SCF
计算，然后求能量差。</li>
</ul></li>
<li><strong>电子亲和能 (A)</strong>：N
电子体系获得一个电子所释放的能量。 &gt; <span class="math inline">\(A =
E(N) - E(N+1)\)</span>
<ul>
<li><strong>计算</strong>：你需要分别对 N 电子体系和 (N+1)
电子体系（阴离子）进行<strong>两次独立</strong>的 SCF 计算。</li>
</ul></li>
<li><strong>基本带隙 (Fundamental Gap)</strong>：被严格定义为 <span
class="math inline">\(I - A\)</span>。 &gt; <span
class="math inline">\(E_{gap} = I - A = E(N-1) + E(N+1) -
2E(N)\)</span></li>
</ul>
<h4 id="koopmans-定理轨道能量近似">2. Koopmans 定理：轨道能量近似</h4>
<p>这是一个非常有用，但<strong>计算上更便宜</strong>的<strong>近似</strong>方法。它<strong>仅需一次</strong>
N 电子体系的 SCF 计算。</p>
<ul>
<li><strong>定理内容</strong>：
<ul>
<li><span class="math inline">\(I \approx -\epsilon_{HOMO}\)</span>
(电离能约等于 HOMO 轨道能量的负值)</li>
<li><span class="math inline">\(A \approx -\epsilon_{LUMO}\)</span>
(电子亲和能约等于 LUMO 轨道能量的负值)</li>
</ul></li>
<li><strong>带隙近似</strong>：因此，基本带隙 <span
class="math inline">\(I - A\)</span> 可以被 <strong>HOMO-LUMO
隙</strong> 所近似。 &gt; <span class="math inline">\(E_{gap} = I - A
\approx (-\epsilon_{HOMO}) - (-\epsilon_{LUMO}) = \epsilon_{LUMO} -
\epsilon_{HOMO}\)</span></li>
</ul>
<h4 id="为什么-deltascf-和-koopmans-定理不同">3. 为什么 <span
class="math inline">\(\Delta\)</span>SCF 和 Koopmans’ 定理不同？</h4>
<p>答案在于两个关键概念：</p>
<ul>
<li><strong>“Frozen Orbital Approximation” (冻结轨道近似)</strong>：
Koopmans
定理的<strong>数学假设</strong>。它假设当你移走（或加入）一个电子时，所有其他
(N-1) 个电子的轨道波函数<strong>完全不发生改变</strong>。</li>
<li><strong>“Orbital Relaxation” (轨道弛豫)</strong>：
<strong>物理现实</strong>。当你移走一个电子，电子间排斥力减小，剩下的
(N-1)
个电子会“感觉”到更强的核吸引力，它们的轨道会<strong>收缩</strong>并<strong>重新排布</strong>（即“弛豫”），以达到一个新的、能量更低的稳定状态。</li>
</ul>
<p><strong>结论</strong>： * <strong><span
class="math inline">\(\Delta\)</span>SCF</strong>
方法<strong>考虑了</strong>轨道弛豫，因为它为 (N-1)
体系重新进行了完整的计算。 * <strong>Koopmans’</strong>
定理<strong>忽略了</strong>轨道弛豫。 * 由于弛豫会使 (N-1)
体系的能量<strong>进一步降低</strong>，Koopmans’ 定理（<span
class="math inline">\(I \approx
-\epsilon_{HOMO}\)</span>）<strong>总是高估</strong>真实的电离能。</p>
<h3 id="第三部分hf-理论的根本缺陷相关能">第三部分：HF
理论的根本缺陷（相关能）</h3>
<p>HF 理论本身就是一个近似，它最大的缺陷是忽略了<strong>电子相关能
(Electron Correlation Energy)</strong>。</p>
<ul>
<li><strong>“Exchange” (交换能)</strong>：HF
理论<strong>已经包含</strong>。它只解决了<strong>同自旋</strong>电子（如
<span class="math inline">\(\uparrow
\uparrow\)</span>）因泡利不相容原理而相互“避开”的问题（费米空穴）。</li>
<li><strong>“Correlation” (相关能)</strong>：HF
理论<strong>完全忽略</strong>。这是指所有电子（特别是<strong>不同自旋</strong>的电子，如
<span class="math inline">\(\uparrow
\downarrow\)</span>）为了瞬时地“躲避”彼此而产生的运动关联（<strong>库仑空穴,
Coulomb Hole</strong>）。HF
作为一个平均场理论，只让电子感受到彼此的“平均”电荷云。</li>
</ul>
<p><strong>定义</strong>：电子相关能 <span
class="math inline">\(E_{\text{corr}}\)</span>
是体系的<strong>精确基态能量</strong> <span
class="math inline">\(E_{\text{exact}}\)</span> 与 <strong>HF
基态能量</strong> <span class="math inline">\(E_{HF}\)</span>
之间的差值。</p>
<blockquote>
<p><strong><span class="math inline">\(E_{\text{corr}} =
E_{\text{exact}} - E_{HF}\)</span></strong></p>
</blockquote>
<ul>
<li>由于 HF 理论（平均场）高估了电子排斥，HF
能量<strong>总是高于</strong>真实能量，所以<strong>相关能 <span
class="math inline">\(E_{\text{corr}}\)</span>
永远是负值</strong>。</li>
</ul>
<h3 id="第四部分组态相互作用-ci-修正-hf">第四部分：组态相互作用 (CI) ——
修正 HF</h3>
<p>CI 是一种系统地“恢复” HF 所丢失的相关能的方法。</p>
<h4 id="核心思想">1. 核心思想</h4>
<p>HF 理论假设基态波函数<strong>只是</strong>一个 <span
class="math inline">\(\Phi_{HF}\)</span> 行列式。 CI
理论认为，<strong>精确的</strong>基态波函数 <span
class="math inline">\(\Psi\)</span> 应该是<strong>所有</strong>可能的 N
电子组态的<strong>线性叠加</strong>。</p>
<p>这个组态的完备基组包括： * <strong><span
class="math inline">\(\Phi_{HF}\)</span></strong> (HF 基态组态) *
<strong><span class="math inline">\(\Phi_i^a\)</span></strong>
(单激发组态：电子 <span class="math inline">\(i \to a\)</span>) *
<strong><span class="math inline">\(\Phi_{ij}^{ab}\)</span></strong>
(双激发组态：电子 <span class="math inline">\(i, j \to a, b\)</span>) *
<strong><span class="math inline">\(\Phi_{ijk}^{abc}\)</span></strong>
(三激发组态) * … 一直到 N 激发</p>
<h4 id="ci-波函数">2. CI 波函数</h4>
<p>精确的波函数 <span class="math inline">\(\Psi\)</span> 被展开为：</p>
<blockquote>
<p><span class="math inline">\(\Psi = C_0 \Phi_{HF} + \sum_{i,a} C_i^a
\Phi_i^a + \sum_{i,j,a,b} C_{ij}^{ab} \Phi_{ij}^{ab} +
\sum_{i,j,k,a,b,c} C_{ijk}^{abc} \Phi_{ijk}^{abc} + \dots\)</span></p>
</blockquote>
<ul>
<li><span class="math inline">\(C_0, C_i^a, C_{ij}^{ab} \dots\)</span>
是混合系数，表示每个组态对真实波函数的“贡献”大小。</li>
<li><strong>Full CI (FCI,
全组态相互作用)</strong>：如果这个展开式包含了<strong>所有</strong>可能的激发态，那么它就是在给定基组下的<strong>精确解</strong>。</li>
<li><strong>Truncated CI (截断 CI)</strong>：由于 FCI
的计算量是天文数字，实际中通常会截断，例如
<strong>CISD</strong>（只包含单激发和双激发）。</li>
</ul>
<h4 id="能量总结">3. 能量总结</h4>
<p>白板上的能量图清晰地展示了这一点：</p>
<ul>
<li><strong><span class="math inline">\(E_{HF}\)</span></strong>：HF
能量，是 <span class="math inline">\(E_{\text{exact}}\)</span>
的一个<strong>上限</strong>（近似值）。</li>
<li><strong><span class="math inline">\(E_{Full
CI}\)</span></strong>：Full CI 能量，是<strong>精确值</strong> <span
class="math inline">\(E_{\text{exact}}\)</span>。</li>
<li><strong><span class="math inline">\(E_{HF} - E_{Full
CI}\)</span></strong>：这个能量差就是<strong>电子相关能 <span
class="math inline">\(E_{\text{corr}}\)</span></strong>。</li>
</ul>
<h4 id="高级工具二次量子化">4. 高级工具：二次量子化</h4>
<p>CI 理论在数学上通常用<strong>产生算符 <span
class="math inline">\(a^\dagger\)</span></strong> 和<strong>湮灭算符
<span class="math inline">\(a\)</span></strong> 来表述。</p>
<ul>
<li><strong><span
class="math inline">\(a_p^\dagger\)</span></strong>：在轨道 <span
class="math inline">\(p\)</span> 中<strong>产生</strong>一个电子。</li>
<li><strong><span class="math inline">\(a_p\)</span></strong>：在轨道
<span class="math inline">\(p\)</span>
中<strong>湮灭</strong>一个电子。</li>
</ul>
<p>使用这个工具，构建激发态变得非常简洁： *
<strong>单激发</strong>：<span class="math inline">\(|\Phi_i^a\rangle =
a_a^\dagger a_i |\Phi_{HF}\rangle\)</span> （含义：在 HF
基态上，先湮灭轨道 <span class="math inline">\(i\)</span>
的电子，再产生轨道 <span class="math inline">\(a\)</span> 的电子） *
<strong>双激发</strong>：<span
class="math inline">\(|\Phi_{ij}^{ab}\rangle = a_a^\dagger a_b^\dagger
a_j a_i |\Phi_{HF}\rangle\)</span> （含义：湮灭 <span
class="math inline">\(i, j\)</span> 的电子，产生 <span
class="math inline">\(a, b\)</span> 的电子）</p>
<p>这些算符自动满足<strong>反对易关系</strong>（如 <span
class="math inline">\(\{ a_i, a_j^\dagger \} =
\delta_{ij}\)</span>），这保证了波函数自动满足泡利不相容原理。</p>
<h3 id="总结幸运的误差抵消">总结：“幸运的误差抵消”</h3>
<p>最后，一个有趣的问题：为什么 Koopmans 定理（<span
class="math inline">\(I \approx
-\epsilon_{HOMO}\)</span>）这个“粗糙”的近似，在实际中经常比“更准确”的
<span class="math inline">\(\Delta SCF\)</span> 方法（<span
class="math inline">\(I = E(N-1) -
E(N)\)</span>）<strong>更接近实验值</strong>？</p>
<p>答案是 <strong>“幸运的误差抵消”</strong>：</p>
<ol type="1">
<li><strong>误差 1 (轨道弛豫)</strong>：Koopmans’
定理忽略了轨道弛豫，这使得它计算的 <span
class="math inline">\(I\)</span> <strong>偏高</strong>。</li>
<li><strong>误差 2 (电子相关能)</strong>：Koopmans’ 定理（作为 HF
理论的一部分）忽略了电子相关能。相关能对 N
电子体系的稳定化（能量降低）比对 (N-1) 体系更显著，这使得 HF 计算的
<span class="math inline">\(I\)</span> <strong>偏低</strong>。</li>
</ol>
<p>因此，<strong>Koopmans’ 定理（<span class="math inline">\(I \approx
-\epsilon_{HOMO}\)</span>）</strong> 在与<strong>实验值</strong>比较时：
* <strong>误差 1（高估 <span class="math inline">\(I\)</span>）</strong>
和 <strong>误差 2（低估 <span
class="math inline">\(I\)</span>）</strong>
在一定程度上<strong>相互抵消</strong>了！ *
这使得这个“错上加错”的近似，最终给出了一个出奇“准确”的结果。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/22/5120c7-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/22/5120c7-1/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W7-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-10-22 22:00:00" itemprop="dateCreated datePublished" datetime="2025-10-22T22:00:00+08:00">2025-10-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-11-10 02:46:38" itemprop="dateModified" datetime="2025-11-10T02:46:38+08:00">2025-11-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - 计算能源材料和电子结构模拟 Lecture</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="i">I:</h2>
<p>这些内容共同构成了量子化学中用于计算分子电子结构的基石——<strong>Hartree-Fock
(HF) 自洽场 (SCF)</strong> 方法。</p>
<h3 id="核心概念hartree-fock-hf-近似">核心概念：Hartree-Fock (HF)
近似</h3>
<ul>
<li><strong>目标</strong>：求解多电子体系（如分子）的薛定谔方程。但这个方程非常复杂，无法精确求解。</li>
<li><strong>核心思想 (HF 近似)</strong>：
<ol type="1">
<li><strong>波恩-奥本海默近似</strong>：假定原子核固定不动，我们只关心电子的运动。</li>
<li><strong>单电子近似</strong>：<em>这是 HF
近似最关键的一步</em>。它假设每个电子的运动只受到一个“平均场”的影响，这个平均场是由原子核和其他所有电子共同产生的。</li>
<li><strong>反对称性</strong>：电子是费米子，必须满足泡利不相容原理。HF
方法通过一个称为“斯莱特行列式 (Slater
Determinant)”的数学工具来构造总波函数，以自动满足这个要求。</li>
</ol></li>
</ul>
<h3 id="基本算符与方程">⬅基本算符与方程</h3>
<h4 id="hartree-fock-方程f-φi-εi-φi">1. Hartree-Fock
方程：<code>f̂ |φi&gt; = εi |φi&gt;</code></h4>
<p>它在形式上是一个本征方程。</p>
<ul>
<li><strong><code>|φi&gt;</code> (分子轨道)</strong>：这是我们要求解的第
<code>i</code> 个单电子波函数（或称为轨道）。</li>
<li><strong><code>f̂</code> (Fock
算符)</strong>：这是一个等效的单电子哈密顿算符。它代表了一个电子在原子核和所有其他电子的“平均场”中所感受到的总能量。</li>
<li><strong><code>εi</code> (轨道能量)</strong>：这是第 <code>i</code>
个分子轨道的能量，即电子占据该轨道时的能量。</li>
</ul>
<h4 id="fock-算符的构成f-h₁r-σj-2jjr---kjr">2. Fock
算符的构成：<code>f̂ = ĥ₁(r) + Σ[j] (2Ĵj(r) - K̂j(r))</code></h4>
<p>这个公式详细定义了 Fock 算符 <code>f̂</code> 是由什么组成的。</p>
<ul>
<li><strong><code>ĥ₁(r)</code>
(核心哈密顿算符)</strong>：这是“单电子”部分，与电子间的相互作用无关。它只包括：
<ol type="1">
<li><strong>电子的动能</strong>：白板上的 <code>-(ħ²/2m)∇²</code>
项。</li>
<li><strong>电子与所有原子核的吸引势能</strong>：白板上的
<code>-Σ[I] (Z_I e² / |r - R_I|)</code> 项。</li>
</ol></li>
<li><strong><code>Σ[j] (2Ĵj(r) - K̂j(r))</code>
(双电子相互作用)</strong>：这是描述电子 <code>i</code> 与其他所有电子
<code>j</code> 之间平均相互作用的项（求和 <code>Σ[j]</code>
遍历所有被占据的轨道）。
<ul>
<li><strong><code>Ĵj(r)</code> (库仑算符)</strong>：
<ul>
<li><strong>物理意义</strong>：描述了电子 <code>i</code> 与轨道
<code>j</code> 上的电子云（密度为
<code>|φj|²</code>）之间的<strong>经典静电排斥</strong>。</li>
<li><strong>白板上的定义</strong>：<code>Ĵj(ψ) = &lt;φj | e² / |r - r'| | φj&gt; ψ</code>。这意味着
<code>Ĵj</code> 作用在一个函数 <code>ψ(r)</code> 上时，它会计算
<code>ψ(r)</code> 与 <code>φj(r')</code> 之间的平均排斥能。</li>
</ul></li>
<li><strong><code>K̂j(r)</code> (交换算符)</strong>：
<ul>
<li><strong>物理意义</strong>：这是一个纯粹的量子力学效应，<strong>没有经典对应</strong>。它源于波函数的反对称性要求（泡利不相容原理）。它修正了电子“自我排斥”的错误（因为
<code>Ĵj</code> 包含了 <code>j=i</code>
的情况），并降低了自旋平行电子相遇的概率，从而降低了能量。</li>
<li><strong>白板上的定义</strong>：<code>K̂j(ψ) = &lt;φj | e² / |r - r'| | ψ&gt; φj</code>。注意
<code>ψ</code> 和 <code>φj</code>
在积分符号内的位置发生了“交换”，因此得名。</li>
</ul></li>
<li><strong>系数 “2”</strong>：在闭壳层（Closed-Shell）HF
方法中，我们假设每个分子轨道 <code>j</code>
都被两个自旋相反的电子（一个自旋向上 <code>α</code>，一个自旋向下
<code>β</code>）占据。因此，<code>Ĵj</code> 的排斥作用要乘以 2。而
<code>K̂j</code> 的交换作用只发生在自旋相同的电子之间，因此只有一个
<code>K̂j</code> 被减去（例如，自旋向上的电子 <code>i</code>
只与自旋向上的电子 <code>j</code> 发生交换）。</li>
</ul></li>
</ul>
<h4 id="homo-lumo">3. HOMO / LUMO</h4>
<ul>
<li><strong>HOMO (Highest Occupied Molecular
Orbital)</strong>：最高占据分子轨道。这是 HF 计算得到的 <code>εi</code>
能量中，能量最高但仍被电子占据的那个轨道。</li>
<li><strong>LUMO (Lowest Unoccupied Molecular
Orbital)</strong>：最低未占分子轨道。这是 <code>εi</code>
能量中，能量最低但没有电子占据的那个轨道。</li>
<li><strong>意义</strong>：HOMO 和 LUMO
统称为“前线轨道”。它们的能量差（HOMO-LUMO
Gap）和形状在化学反应中至关重要，决定了分子倾向于从哪里给出电子 (HOMO)
和从哪里接受电子 (LUMO)。</li>
</ul>
<h3 id="矩阵化-roothaan-hall-方法">矩阵化 (Roothaan-Hall 方法)</h3>
<p>直接求解上面的 Hartree-Fock 方程（它是微分-积分方程）非常困难。C. C.
J. Roothaan 和 G. G. Hall
提出了一种将其转换为标准矩阵代数问题的方法。</p>
<h4 id="lcao-展开">1. LCAO 展开</h4>
<p>我们假设未知的分子轨道 <code>|φi&gt;</code>
可以由一组已知的<strong>原子轨道 (Atomic Orbitals, AOs)</strong>
<code>|χμ&gt;</code> 线性组合而成：
<code>|φi&gt; = Σ[μ] Cμi |χμ&gt;</code> 其中 <code>Cμi</code>
是我们要解的<strong>系数</strong>。</p>
<h4 id="roothaan-hall-方程f-c-s-c-ε">2. Roothaan-Hall
方程：<code>F C = S C ε</code></h4>
<p>这是将 LCAO 展开代入 HF 方程后得到的<strong>矩阵方程</strong>。</p>
<ul>
<li><strong><code>F</code> (Fock
矩阵)</strong>：<code>Fμν = &lt;χμ | f̂ | χν&gt;</code>。这是 Fock 算符
<code>f̂</code> 在原子轨道基组下的矩阵表示。</li>
<li><strong><code>C</code> (系数矩阵)</strong>：矩阵的每一列
<code>i</code> 都是一个分子轨道 <code>φi</code> 的展开系数
<code>Cμi</code>。</li>
<li><strong><code>S</code>
(重叠矩阵)</strong>：<code>Sμν = &lt;χμ | χν&gt;</code>。它描述了原子轨道基函数之间的重叠程度。如果基函数是“正交”的，<code>S</code>
就是单位矩阵。</li>
<li><strong><code>ε</code>
(轨道能量矩阵)</strong>：这是一个对角矩阵，对角线上的元素
<code>εi</code> 就是我们要求的分子轨道能量。</li>
</ul>
<h4 id="fock-矩阵元的计算fμν-hμνcore-gμν">3. Fock
矩阵元的计算：<code>Fμν = Hμν^core + Gμν</code></h4>
<p>这是求解的核心，它把 <code>F</code> 矩阵的计算分为两部分：</p>
<ul>
<li><strong>① <code>Hμν^core = &lt;χμ | ĥ₁ | χν&gt;</code>
(核心哈密顿矩阵元)</strong>：
<ul>
<li><strong>物理意义</strong>：这是“单电子”部分，只包含电子动能和电子-原子核吸引能。</li>
<li><strong>计算</strong>：这部分在整个计算过程中<strong>只用计算一次</strong>，因为它不依赖于电子的分布。</li>
</ul></li>
<li><strong><code>Gμν</code> (双电子积分项)</strong>：
<ul>
<li><strong>物理意义</strong>：这是“双电子”排斥部分，对应于
<code>Σ[j] (2Ĵj - K̂j)</code>。</li>
<li><strong>问题</strong>：计算 <code>Gμν</code>
需要知道库仑和交换算符，而这些算符又依赖于分子轨道
<code>|φj&gt;</code>，而 <code>|φj&gt;</code> 又是由我们要求解的系数
<code>C</code> 决定的。这就形成了一个“鸡生蛋，蛋生鸡”的循环问题。</li>
</ul></li>
</ul>
<h4 id="密度矩阵与-scf-循环">4. 密度矩阵与 SCF 循环</h4>
<p>为了解决这个循环问题，我们引入了<strong>密度矩阵
<code>P</code></strong>。</p>
<ul>
<li><strong>③ 密度矩阵 (Density Matrix) <code>P</code></strong>：
<ul>
<li><strong>白板上的定义</strong>：<code>Pαβ = 2 Σ[j=1 to N/2] Cαj* Cβj</code></li>
<li><strong>物理意义</strong>：<code>P</code>
描述了电子在原子轨道基组上的分布情况。它由系数矩阵 <code>C</code>
构造。</li>
</ul></li>
<li><strong>使用 <code>P</code> 构建 Fock 矩阵 <code>F</code></strong>：
<ul>
<li><strong>白板上的公式</strong>：<code>Fμν = Hμν^core + Σ[αβ] Pαβ * (...)</code>
（白板上 <code>(...)</code> 部分代表了 <code>(μν|αβ) - 1/2(μα|νβ)</code>
这样的双电子积分项）。</li>
<li><strong>关键点</strong>：<code>F</code>
矩阵（代表电子间的相互作用）现在被表示为密度矩阵
<code>P</code>（代表电子的分布）的函数。<code>F = F(P)</code>。</li>
</ul></li>
</ul>
<h3 id="总结自洽场-scf-的完整流程">总结：自洽场 (SCF) 的完整流程</h3>
<p>完整地描述了 <strong>“自洽场” (Self-Consistent Field, SCF)</strong>
的计算流程：</p>
<ol type="1">
<li><strong>第 0 步</strong>：选择原子轨道基组
<code>|χμ&gt;</code>，并计算所有<strong>不变的</strong>积分，如重叠矩阵
<code>S</code> 和核心哈密顿矩阵 <code>H^core</code>。</li>
<li><strong>第 1 步 (猜测)</strong>：对系数矩阵 <code>C</code>
做一个初始<strong>猜测</strong>（例如，通过一个更简单的方法得到），并用它来构建一个初始的密度矩阵
<code>P^(0)</code>。</li>
<li><strong>第 2 步 (构建)</strong>：使用当前的密度矩阵
<code>P^(k)</code> 来<strong>构建</strong> Fock 矩阵
<code>F^(k)</code>。 <code>Fμν = Hμν^core + Gμν(P^(k))</code></li>
<li><strong>第 3 步 (求解)</strong>：求解 Roothaan-Hall 矩阵方程
<code>F^(k) C^(k+1) = S C^(k+1) ε^(k+1)</code>，得到一组<strong>新的</strong>系数矩阵
<code>C^(k+1)</code> 和新的轨道能量 <code>ε^(k+1)</code>。</li>
<li><strong>第 4 步 (更新)</strong>：使用新的系数 <code>C^(k+1)</code>
来计算一个<strong>新的</strong>密度矩阵 <code>P^(k+1)</code>。</li>
<li><strong>第 5 步
(检查自洽)</strong>：比较<strong>新的</strong>密度矩阵
<code>P^(k+1)</code> 和<strong>旧的</strong>密度矩阵
<code>P^(k)</code>。
<ul>
<li><strong>如果</strong> <code>P^(k+1)</code> 和 <code>P^(k)</code>
几乎相同（即“自洽”了），说明我们找到的电子分布 <code>P</code>
所产生的“平均场” <code>F</code>，反过来再求解这个 <code>F</code>
得到的电子分布恰好就是 <code>P</code>
本身。计算<strong>收敛</strong>，循环结束。</li>
<li><strong>如果</strong>它们不相同，就令
<code>k = k+1</code>，返回<strong>第 2 步</strong>，用新的
<code>P</code> 继续迭代。</li>
</ul></li>
</ol>
<p>这个迭代过程，就是 Hartree-Fock 自洽场方法的核心。</p>
<h2 id="ii">II:</h2>
<p>两个关键部分：</p>
<ol type="1">
<li><strong>1.</strong>：继续详细推导如何使用<strong>密度矩阵 (Density
Matrix)</strong> 来构建 Fock 矩阵中的双电子相互作用项。</li>
<li><strong>2.</strong>：展示了如何从数学上求解 Roothaan-Hall 方程
(<code>F C = S C ε</code>)，这是一个“广义本征值问题”，并将其转换为计算机可以轻松处理的“标准本征值问题”。</li>
</ol>
<h3 id="fock-矩阵元的最终形式">Fock 矩阵元的最终形式</h3>
<p>这部分是整个 HF-SCF
方法中最核心的数学推导之一。它展示了如何将复杂的双电子相互作用（<code>Gμν</code>）表示为密度矩阵
<code>P</code> 和一堆预先计算好的积分的乘积。</p>
<ul>
<li><strong><code>Pαβ = 2 Σ[j=1 to N/2] Cαj* Cβj</code></strong>
<ul>
<li>重申<strong>密度矩阵 <code>P</code></strong> 的定义。它由系数矩阵
<code>C</code> 构建。</li>
</ul></li>
<li><strong>推导
<code>③</code>：<code>&lt;χμ | Σ[j] ... | χν&gt;</code></strong>
<ul>
<li>这一长串推导（从 <code>③</code> 开始，一直到
<code>= Σ[αβ] Pαβ (...)</code>）是白板上最复杂的部分。</li>
<li><strong>目标</strong>：计算 Fock 矩阵的双电子部分
<code>Gμν = &lt;χμ | Σ[j] (2Ĵj - K̂j) | χν&gt;</code>。</li>
<li><strong>步骤</strong>：
<ol type="1">
<li>将分子轨道 <code>|φj&gt;</code> 用原子轨道 <code>|χ&gt;</code>
展开：<code>|φj&gt; = Σ[α] Cαj |χα&gt;</code>。</li>
<li>将这个展开式代入 <code>Ĵj</code> 和 <code>K̂j</code>
的积分定义中。</li>
<li>这会产生涉及四个原子轨道基函数的积分，称为<strong>双电子积分
(Two-Electron Integrals)</strong>，通常写作 <code>(μν|αβ)</code>。</li>
<li>经过复杂的代数重排（将 <code>C</code> 和 <code>Σ[j]</code>
重新组合），推导发现 <code>Gμν</code> 可以被写成：
<code>Gμν = Σ[αβ] Pαβ * [ (μν|αβ) - 1/2 (μα|νβ) ]</code>
<ul>
<li><code>(μν|αβ)</code> 是库仑积分。</li>
<li><code>(μα|νβ)</code> 是交换积分。</li>
</ul></li>
</ol></li>
</ul></li>
<li><strong><code>Fμν = Hμν^core + Σ[αβ] Pαβ (...)</code></strong>
<ul>
<li><strong>最终的 Fock 矩阵元公式</strong>。</li>
<li><code>Fμν</code>（Fock 矩阵）=
<code>Hμν^core</code>（核心哈密顿矩阵，只算一次）+
<code>Gμν</code>（双电子排斥项）。</li>
<li>关键在于 <code>Gμν</code> 是<strong>密度矩阵 <code>P</code>
的线性函数</strong>。</li>
<li>这完美地建立了 SCF 的循环关系：<code>C</code> → <code>P</code> →
<code>F</code> → 求解 <code>F</code> 得到新的 <code>C</code>。</li>
</ul></li>
</ul>
<h3 id="roothaan-hall-方程的求解问题">Roothaan-Hall 方程的求解问题</h3>
<p>这部分转向了一个纯粹的数学（线性代数）问题：如何求解我们建立的矩阵方程。</p>
<ul>
<li><strong><code>S† F C = C ε</code> (笔误)</strong>
<ul>
<li>白板上划掉的 <code>S† F C = C ε</code> 及其旁边的推导
<code>(S†F)† = ...</code>
看起来像是一个错误的尝试或旁注，试图探索这个矩阵的厄米性
(Hermiticity)。</li>
<li><strong>正确的方程</strong>（在它下面）是
<code>F C = S C ε</code>。</li>
</ul></li>
<li><strong><code>F C = S C ε</code> (Roothaan-Hall 方程)</strong>
<ul>
<li><strong>问题</strong>：这不是一个“标准本征值问题”
(<code>A x = λ x</code>)，因为在等式右边多了一个<strong>重叠矩阵
<code>S</code></strong>。<code>S</code> 不是单位矩阵
<code>I</code>，因为原子轨道基组 <code>|χ&gt;</code>
通常不是正交的。</li>
<li><strong>术语</strong>：这被称为“<strong>广义本征值问题</strong>”。</li>
</ul></li>
<li><strong><code>S is positive definite</code> (S 是正定矩阵)</strong>
<ul>
<li>这是一个关键的数学性质。<code>S</code>
是正定的，意味着它所有的本征值（<code>λi</code>）都大于零。</li>
<li><strong>意义</strong>：因为 <code>S</code>
是正定的，所以它保证是可逆的（<code>S⁻¹</code>
存在），并且我们可以对它进行“开方”，即找到 <code>S^(1/2)</code> 和
<code>S^(-1/2)</code>。</li>
</ul></li>
<li><strong><code>S = R† R</code> 或 <code>S = U† Λ U</code> (S
的分解)</strong>
<ul>
<li>这是对 <code>S</code> 矩阵进行对角化或分解的标准方法。</li>
<li><code>S = U† Λ U</code> 是<strong>谱分解</strong>：
<ul>
<li><code>U</code> 是 <code>S</code> 的本征向量矩阵。</li>
<li><code>Λ</code> 是由 <code>S</code> 的本征值 <code>λi</code>
组成的对角矩阵。</li>
</ul></li>
</ul></li>
<li><strong><code>S^(1/2) = U† √Λ U</code> 和
<code>S^(-1/2) = U† (1/√Λ) U</code></strong>
<ul>
<li>（右侧白板上有 <code>S^(-1/2)</code> 的定义）。</li>
<li>这是利用 <code>S</code> 的分解来定义它的 <code>1/2</code> 次方和
<code>-1/2</code> 次方矩阵。<code>√Λ</code> 就是简单地将对角矩阵
<code>Λ</code> 上的每个元素 <code>λi</code> 都开方。</li>
</ul></li>
</ul>
<h3 id="变换为标准本征值问题">变换为标准本征值问题</h3>
<p>这部分展示了如何利用 <code>S^(-1/2)</code> 矩阵来“清除”方程中的
<code>S</code>，将其变为标准本征值问题。这个过程称为<strong>正交化
(Orthogonalization)</strong>。</p>
<ol type="1">
<li><strong>目标</strong>：将 <code>F C = S C ε</code> 变换为
<code>F' C' = C' ε</code> 的形式。</li>
<li><strong>定义新的系数矩阵 <code>C'</code></strong>：
我们定义一组新的、在正交化基组下的系数 <code>C'</code>，它与原始系数
<code>C</code> 的关系是： <code>C' = S^(1/2) C</code> （因此
<code>C = S^(-1/2) C'</code>）</li>
<li><strong>代入原方程</strong>： 将 <code>C = S^(-1/2) C'</code> 代入
<code>F C = S C ε</code>：
<code>F (S^(-1/2) C') = S (S^(-1/2) C') ε</code></li>
<li><strong>两边左乘 <code>S^(-1/2)</code></strong>：
<code>(S^(-1/2) F S^(-1/2)) C' = (S^(-1/2) S S^(-1/2)) C' ε</code></li>
<li><strong>简化</strong>：
<ul>
<li><strong>右侧</strong>：如白板所示，<code>S^(-1/2) S S^(-1/2) = S^(-1/2) S^(1/2) S^(1/2) S^(-1/2) = I</code>（单位矩阵）。</li>
<li><strong>左侧</strong>：我们定义一个新的、变换后的 Fock 矩阵
<code>F'</code>：
<strong><code>F' = S^(-1/2) F S^(-1/2)</code></strong></li>
</ul></li>
<li><strong>最终的标准本征值方程</strong>：
<strong><code>F' C' = C' ε</code></strong></li>
</ol>
<h3 id="总结scf-循环的完整计算步骤">总结：SCF 循环的完整计算步骤</h3>
<p>一个完整的 SCF 迭代步骤如下：</p>
<ol type="1">
<li><strong>猜测</strong> <code>C^(0)</code> (或
<code>P^(0)</code>)。</li>
<li><strong>构建 P</strong>：使用 <code>C^(k)</code> 计算密度矩阵
<code>P^(k)</code>。</li>
<li><strong>构建 F</strong>：使用 <code>P^(k)</code> 构建 Fock 矩阵
<code>F^(k)</code>（如左侧推导所示）。</li>
<li><strong>构建 F’</strong>：使用
<code>S^(-1/2)</code>（它在计算开始前就算好了）和 <code>F^(k)</code>
来构建变换后的 Fock 矩阵
<strong><code>F'^(k) = S^(-1/2) F^(k) S^(-1/2)</code></strong>。</li>
<li><strong>求解</strong>：<strong><code>F'^(k) C'^(k+1) = C'^(k+1) ε^(k+1)</code></strong>。这是一个标准本征值问题，计算机可以高效求解，得到新的
<code>C'</code> 和 <code>ε</code>。</li>
<li><strong>反变换</strong>：通过
<strong><code>C^(k+1) = S^(-1/2) C'^(k+1)</code></strong>
得到我们<strong>真正</strong>的系数矩阵 <code>C</code>。</li>
<li><strong>检查收敛</strong>：用 <code>C^(k+1)</code> 计算新的
<code>P^(k+1)</code>，与 <code>P^(k)</code> 比较。如果不收敛，返回第 2
步。</li>
</ol>
<p>在数学上解决了如何在非正交基组下求解 HF 方程的实际计算问题。</p>
<h2 id="iii">III:</h2>
<p>两个主要部分：</p>
<ol type="1">
<li><strong>上部</strong>：完成 Roothaan-Hall 方程的数学求解变换。</li>
<li><strong>下部</strong>：引入一个全新且至关重要的概念——<strong>Hartree-Fock
(HF) 的总能量</strong>。</li>
</ol>
<h3 id="方程的最终求解形式">方程的最终求解形式</h3>
<p>“标准本征值问题”变换的总结和补充。</p>
<ul>
<li><strong><code>S^(-1/2) = U† (1/√λ ... 0; 0 ... 1/λ_m) U</code></strong>
<ul>
<li>这是 <strong><code>S^(-1/2)</code>
矩阵</strong>的明确计算方法，即通过对重叠矩阵 <code>S</code>
进行“谱分解”：
<ol type="1">
<li>对角化 <code>S</code> 得到其本征向量矩阵 <code>U</code>
和本征值对角矩阵 <code>Λ</code> (对角元为 <code>λ_i</code>)。</li>
<li>计算 <code>Λ^(-1/2)</code>（即把每个 <code>λ_i</code> 替换为
<code>1/√λ_i</code>）。</li>
<li>重新组合 <code>U† Λ^(-1/2) U</code> 得到
<code>S^(-1/2)</code>。</li>
</ol></li>
</ul></li>
<li><strong><code>(blas, lapack)</code></strong>
<ul>
<li>这是一个非常实际的课堂笔记。<code>BLAS</code> (Basic Linear Algebra
Subprograms) 和 <code>LAPACK</code> (Linear Algebra Package)
是用于高性能科学计算（如矩阵对角化、求逆等）的<strong>黄金标准软件包</strong>。</li>
<li>教授在这里的意思是：“这个矩阵运算（<code>S^(-1/2)</code>）我们手算不了，但计算机上的量子化学软件会调用
<code>LAPACK</code> 库来高效地完成它。”</li>
</ul></li>
<li><strong><code>F' C' = C' ε</code></strong>
<ul>
<li><strong>最终方程</strong>。重申上一张白板的结论：我们已经成功地将广义本征值问题
<code>F C = S C ε</code> 转换为了标准本征值问题
<code>F' C' = C' ε</code>。</li>
<li>这是计算机可以（通过 <code>LAPACK</code>）直接求解的。</li>
</ul></li>
<li><strong><code>F' = S^(-1/2) F S^(-1/2) = (F')†</code></strong>
<ul>
<li>这一行在确认一个重要的数学性质：<strong><code>F'</code> 矩阵也是厄米
(Hermitian) 的</strong>。</li>
<li><code>†</code> (dagger) 符号代表“厄米共轭”（转置并取复共轭）。</li>
<li>因为 <code>F</code> 是厄米的 (<code>F† = F</code>)，<code>S</code>
也是厄米的，所以 <code>F'</code>
保证是厄米的。这确保了我们求解得到的轨道能量 <code>ε</code>
必定是实数，这在物理上是必需的。</li>
</ul></li>
</ul>
<h3 id="hartree-fock-hf-总能量">Hartree-Fock (HF) 总能量</h3>
<p>这是本张白板的核心。在 SCF 迭代收敛后，我们得到了所有的轨道能量
<code>ε_i</code>。那么，分子的<strong>总能量</strong> <code>E_HF</code>
是多少？</p>
<p><strong>一个常见的陷阱</strong>：你可能会认为总能量就是所有占据轨道的能量之和（<code>2 Σ ε_i</code>，因为每个轨道
2 个电子）。<strong>白板明确指出：这是错误的！</strong></p>
<ul>
<li><p><strong><code>E_HF ≠ 2 Σ[i=1 to N/2] ε_i</code> (总能量 ≠
轨道能量之和)</strong></p>
<ul>
<li><strong>为什么？</strong></li>
<li>白板上的最后一行给出了答案。<strong>轨道能量
<code>ε_i</code></strong> 的定义是：
<code>ε_i = &lt;φ_i | f̂ | φ_i&gt; = ε_ii + Σ[j=1 to N/2] (2J_ij - K_ij)</code></li>
<li><strong>物理含义</strong>：<code>ε_i</code> 不仅仅是电子
<code>i</code> 的能量，它代表了将一个电子<strong>加入</strong>到轨道
<code>i</code> 中所需要的能量。这个能量包括了：
<ol type="1">
<li><code>ε_ii</code>：电子 <code>i</code>
自己的动能和它与<strong>所有原子核</strong>的吸引能。</li>
<li><code>Σ[j] (2J_ij - K_ij)</code>：电子 <code>i</code>
与<strong>所有其他</strong> <code>j</code>
轨道电子的库仑排斥和交换作用。</li>
</ol></li>
<li><strong>双重计算问题</strong>：
<ul>
<li><code>ε_i</code> 包含了 <code>i</code> 与 <code>j</code>
的排斥能。</li>
<li><code>ε_j</code> 包含了 <code>j</code> 与 <code>i</code>
的排斥能。</li>
<li>如果你简单地将它们相加 <code>(ε_i + ε_j)</code>，你就把
<code>i</code> 和 <code>j</code>
之间的排斥能<strong>计算了两遍</strong>。</li>
</ul></li>
<li>因此，<code>2 Σ ε_i</code>
会<strong>双倍计算</strong>所有的电子-电子排斥能，导致结果错误。</li>
</ul></li>
<li><p><strong>正确的 HF 总能量公式</strong>
白板给出了两个等价的正确公式：</p>
<ol type="1">
<li><strong><code>E_HF = 2 Σ[i] ε_ii + Σ[i,j] (2J_ij - K_ij)</code></strong>
(白板第一行)
<ul>
<li><code>2 Σ[i] ε_ii</code>：所有电子的动能 +
电子-原子核吸引能（<code>ε_ii</code> 在这里是核心哈密顿积分
<code>h_ii</code>）。</li>
<li><code>Σ[i,j] (2J_ij - K_ij)</code>：所有电子对之间的排斥/交换能（<strong>只计算一次！</strong>）。</li>
</ul></li>
<li><strong><code>E_HF = Σ[i=1 to N/2] (ε_ii + ε_i)</code></strong>
(白板中间行)
<ul>
<li>这是一个更巧妙、更简洁的公式。</li>
<li>它将总能量表示为：对所有占据轨道 <code>i</code>
求和，每一项是（<strong>核心哈密顿积分 <code>ε_ii</code></strong> +
<strong>轨道能量 <code>ε_i</code></strong>）。</li>
<li>这个公式通过只加一次 <code>ε_ii</code>（单电子项）和一次
<code>ε_i</code>（包含单电子项和双电子项），巧妙地修正了双重计算问题，最终结果与公式
1 完全等价。</li>
</ul></li>
</ol></li>
</ul>
<h3 id="总结">总结</h3>
<p>从“如何求解”到“如何获取最终能量”的过渡。它展示了求解
<code>F C = S C ε</code> 的实用计算方法，并着重强调了 HF 总能量
<code>E_HF</code> 和轨道能量 <code>ε_i</code> 之间的关键区别。</p>
<h2 id="iv">IV</h2>
<p>它用一个清晰的<strong>流程图
(Flowchart)</strong>，前面包含的所有复杂的数学公式和概念，总结成了一个完整的<strong>计算算法</strong>。</p>
<p>这就是 <strong>Hartree-Fock 自洽场 (Self-Consistent Field,
SCF)</strong> 迭代循环的<strong>标准计算流程</strong>。</p>
<h3 id="scf-流程图详解">💡 SCF 流程图详解</h3>
<p><img src="/imgs/5120/scf.jpg" alt="fusion"></p>
<p>这个流程图展示了量子化学程序是如何一步步“猜”出正确答案的。</p>
<h4 id="准备工作-循环开始前">1. 准备工作 (循环开始前)</h4>
<ul>
<li><strong><code>Calculate one, two-electron Integrals</code></strong>
<ul>
<li>这是计算的“第 0
步”，在循环开始前<strong>只需要做一次</strong>。</li>
<li>程序会计算所有需要的“积木块”：
<ol type="1">
<li><strong>单电子积分</strong>：重叠矩阵 <code>S</code>
和核心哈密顿矩阵 <code>H_core</code>。</li>
<li><strong>双电子积分</strong>：所有 <code>(μν|αβ)</code>
形式的积分。这些积分数量极其庞大，是 HF 计算中最耗时的一步。</li>
</ol></li>
</ul></li>
<li><strong><code>[ S -&gt; S^(-1/2) ]</code></strong>
<ul>
<li>利用第一步算出的 <code>S</code> 矩阵，计算出用于正交化的
<code>S^(-1/2)</code> 矩阵。这也<strong>只需要做一次</strong>。</li>
</ul></li>
</ul>
<h4 id="scf-迭代循环-the-loop">2. SCF 迭代循环 (The Loop)</h4>
<ul>
<li><strong><code>[ P_αβ ]</code> (起始点)</strong>
<ul>
<li><strong>第 1
步：猜测</strong>。循环开始，我们必须提供一个初始的<strong>密度矩阵
<code>P</code></strong>。</li>
<li>白板上的 <code>P_αβ^ini = 0</code>
是一个最简单的“零猜测”，实际程序通常会用更高级的猜测方法（如
<code>H_core</code> 猜测）。</li>
</ul></li>
<li><strong><code>[ F_μν ]</code></strong>
<ul>
<li><strong>第 2 步：构建 Fock 矩阵</strong>。</li>
<li>使用<strong>当前</strong>的密度矩阵
<code>P_αβ</code>，根据我们在第二张白板上的公式
<code>F = H_core + G(P)</code> 来构建<strong>当前</strong>的 Fock 矩阵
<code>F</code>。</li>
</ul></li>
<li><strong><code>[ F' C' = C' ε ]</code></strong>
<ul>
<li><strong>第 3 步：求解 Roothaan-Hall 方程</strong>。</li>
<li>正如第三张白板所示，我们不直接解 <code>F C = S C ε</code>。</li>
<li>我们先进行变换：<strong><code>F' = S^(-1/2) F S^(-1/2)</code></strong>。</li>
<li>然后求解这个“标准本征值问题”，得到新的<strong>轨道能量
<code>ε</code></strong> 和<strong>变换后的系数
<code>C'</code></strong>。</li>
</ul></li>
<li><strong><code>[ C = S^(-1/2) C' ]</code></strong>
<ul>
<li><strong>第 4 步：反变换</strong>。</li>
<li>用 <code>S^(-1/2)</code> 矩阵将 <code>C'</code>
转换回我们真正需要的、在原子轨道基组下的<strong>系数矩阵
<code>C</code></strong>。</li>
</ul></li>
<li><strong><code>[ P_αβ^old ~ P_αβ^new ]</code> (决策点)</strong>
<ul>
<li><strong>第 5 步：检查自洽性 (Convergence Check)</strong>。</li>
<li>使用第 4 步得到的<strong>新
<code>C</code></strong>，计算出一个<strong>新的密度矩阵
<code>P^new</code></strong>。</li>
<li>比较这个 <code>P^new</code> 和我们在第 2 步中使用的
<code>P^old</code>。</li>
<li><strong><code>N</code> (No)：</strong> 如果 <code>P^new</code> 和
<code>P^old</code> 差别很大（未收敛），则自洽尚未达成。
<ul>
<li><strong>循环</strong>：将 <code>P^new</code>
作为<strong>下一次</strong>迭代的 <code>P</code>，<strong>返回第 2
步</strong>（<code>[ F_μν ]</code>），用这个新的 <code>P</code>
去构建新的 <code>F</code>。</li>
</ul></li>
<li><strong><code>Y</code> (Yes，未画出)：</strong> 如果
<code>P^new</code> 和 <code>P^old</code>
几乎完全相同（差值小于某个阈值，例如 10⁻⁸），则说明“自洽”达成！
<ul>
<li><strong>循环结束</strong>。</li>
</ul></li>
</ul></li>
</ul>
<h3 id="总结-1">总结</h3>
<p>以上内容从“为什么”（HF 近似）到“是什么”（HF
方程和矩阵）再到“怎么做”（SCF 流程图）。</p>
<p>SCF 流程是所有基于 Hartree-Fock 方法（以及更高级的后 HF
方法）的计算化学软件的核心算法。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/13/5054C6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/13/5054C6/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L6</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-10-13 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-13T21:00:00+08:00">2025-10-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-10-20 03:45:06" itemprop="dateModified" datetime="2025-10-20T03:45:06+08:00">2025-10-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>统计机器学习Lecture-6</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1
id="linear-model-selection-and-regularization-线性模型选择与正则化">1.
Linear Model Selection and Regularization 线性模型选择与正则化</h1>
<h2 id="summary-of-core-concepts">Summary of Core Concepts</h2>
<p><strong>Chapter 6: Linear Model Selection and
Regularization</strong>, focusing specifically on <strong>Section 6.1:
Subset Selection</strong>.
<strong>第六章：线性模型选择与正则化</strong>，<strong>6.1节：子集选择</strong></p>
<ul>
<li><p><strong>The Problem:</strong> You have a dataset with many
potential predictor variables (features). If you include all of them
(like <strong>Model 1</strong> with <span
class="math inline">\(p\)</span> predictors in slide
<code>...221320.png</code>), you risk including “noise” variables. These
irrelevant features can decrease model accuracy (overfitting) and make
the model difficult to interpret.
数据集包含许多潜在的预测变量（特征）。如果包含所有这些变量（例如幻灯片“…221320.png”中带有<span
class="math inline">\(p\)</span>个预测变量的<strong>模型1</strong>），则可能会包含“噪声”变量。这些不相关的特征会降低模型的准确率（过拟合），并使模型难以解释。</p></li>
<li><p><strong>The Goal:</strong> Identify a smaller subset of variables
that are truly related to the response. This creates a simpler, more
interpretable, and often more accurate model (like <strong>Model
2</strong> with <span class="math inline">\(q\)</span> predictors).
找出一个与响应真正相关的较小变量子集。这将创建一个更简单、更易于解释且通常更准确的模型（例如带有<span
class="math inline">\(q\)</span>个预测变量的<strong>模型2</strong>）。</p></li>
<li><p><strong>The Main Method Discussed: Best Subset
Selection</strong></p></li>
<li><p><strong>主要讨论的方法：最佳子集选择</strong> This is an
<em>exhaustive search</em> algorithm. It checks <em>every possible
combination</em> of predictors to find the “best” model. With <span
class="math inline">\(p\)</span> variables, this means checking <span
class="math inline">\(2^p\)</span> total models.
这是一种<em>穷举搜索</em>算法。它检查<em>所有可能的预测变量组合</em>，以找到“最佳”模型。对于
<span class="math inline">\(p\)</span> 个变量，这意味着需要检查总共
<span class="math inline">\(2^p\)</span> 个模型。</p>
<p>The algorithm (from slide <code>...221333.png</code>) works in three
steps:</p>
<ol type="1">
<li><p><strong>Step 1:</strong> Fit the “null model” <span
class="math inline">\(M_0\)</span>, which has no predictors (it just
predicts the average of the response). 拟合“空模型”<span
class="math inline">\(M_0\)</span>，它没有预测变量（它只预测响应的平均值）。</p></li>
<li><p><strong>Step 2:</strong> For each <span
class="math inline">\(k\)</span> (from 1 to <span
class="math inline">\(p\)</span>):</p>
<ul>
<li><p>Fit all <span class="math inline">\(\binom{p}{k}\)</span> models
that contain exactly <span class="math inline">\(k\)</span> predictors.
(e.g., fit all models with 1 predictor, then all models with 2
predictors, etc.).</p></li>
<li><p>拟合所有包含 <span class="math inline">\(k\)</span> 个预测变量的
<span class="math inline">\(\binom{p}{k}\)</span>
个模型。（例如，先拟合所有包含 1 个预测变量的模型，然后拟合所有包含 2
个预测变量的模型，等等）。</p></li>
<li><p>From this group, select the single best model <em>for that size
<span class="math inline">\(k\)</span></em>. This “best” model is the
one with the highest <strong><span
class="math inline">\(R^2\)</span></strong> (or lowest
<strong>RSS</strong> - Residual Sum of Squares) on the <em>training
data</em>. Call this model <span
class="math inline">\(M_k\)</span>.</p></li>
<li><p>从这组中，选择 <em>对于该规模 <span
class="math inline">\(k\)</span></em> 的最佳模型。这个“最佳”模型是在
<em>训练数据</em> 上具有最高 <strong><span
class="math inline">\(R^2\)</span></strong>（或最低 <strong>RSS</strong>
- 残差平方和）的模型。将此模型称为 <span
class="math inline">\(M_k\)</span>。</p></li>
</ul></li>
<li><p><strong>Step 3:</strong> You now have <span
class="math inline">\(p+1\)</span> models: <span
class="math inline">\(M_0, M_1, \dots, M_p\)</span>. You must select the
single best one from this list. To do this, you <strong>cannot</strong>
use training <span class="math inline">\(R^2\)</span> (as it will always
pick the biggest model <span class="math inline">\(M_p\)</span>).
Instead, you must use a metric that estimates <em>test error</em>, such
as: <strong>现在你有 <span class="math inline">\(p+1\)</span>
个模型：<span class="math inline">\(M_0, M_1, \dots,
M_p\)</span>。你必须从列表中选择一个最佳模型。为此，你</strong>不能**使用训练
<span class="math inline">\(R^2\)</span>（因为它总是会选择最大的模型
<span
class="math inline">\(M_p\)</span>）。相反，你必须使用一个能够估计<em>测试误差</em>的指标，例如：</p>
<ul>
<li><strong>Cross-Validation (CV) 交叉验证 (CV)</strong> (This is what
the Python code uses)</li>
<li><strong>AIC</strong> (Akaike Information Criterion
赤池信息准则)</li>
<li><strong>BIC</strong> (Bayesian Information Criterion
贝叶斯信息准则)</li>
<li><strong>Adjusted <span class="math inline">\(R^2\)</span> 调整后的
<span class="math inline">\(R^2\)</span></strong></li>
</ul></li>
</ol></li>
<li><p><strong>Key Takeaway:</strong> The slides show this “subset
selection” concept can be applied <em>beyond</em> linear models. The
Python code demonstrates this by applying best subset selection to a
<strong>K-Nearest Neighbors (KNN) Regressor</strong>, a non-linear
model.“子集选择”的概念可以应用于线性模型<em>之外</em>。</p></li>
</ul>
<h2
id="mathematical-understanding-key-questions-数学理解与关键问题">Mathematical
Understanding &amp; Key Questions 数学理解与关键问题</h2>
<p>This section directly answers the questions posed on your slides.</p>
<h3 id="how-to-compare-which-model-is-better">How to compare which model
is better?</h3>
<p>(From slides <code>...221320.png</code> and
<code>...221326.png</code>)</p>
<p>You cannot use <strong>training error</strong> (like <span
class="math inline">\(R^2\)</span> or RSS) to compare models with
<em>different numbers of predictors</em>. A model with more predictors
will almost always have a better <em>training</em> score, even if those
extra predictors are just noise. This is called
<strong>overfitting</strong>. 不能使用<strong>训练误差</strong>（例如
<span class="math inline">\(R^2\)</span> 或
RSS）来比较具有<em>不同数量预测变量</em>的模型。具有更多预测变量的模型几乎总是具有更好的<em>训练</em>分数，即使这些额外的预测变量只是噪声。这被称为<strong>过拟合</strong>。</p>
<p>To compare models of different sizes (like Model 1 vs. Model 2, or
<span class="math inline">\(M_2\)</span> vs. <span
class="math inline">\(M_5\)</span>), you <strong>must</strong> use a
method that estimates <strong>test error</strong> (how the model
performs on new, unseen data). The slides mention:
要比较不同大小的模型（例如模型 1 与模型 2，或 <span
class="math inline">\(M_2\)</span> 与 <span
class="math inline">\(M_5\)</span>），您<strong>必须</strong>使用一种估算<strong>测试误差</strong>（模型在新的、未见过的数据上的表现）的方法。</p>
<ul>
<li><p><strong>Cross-Validation (CV):</strong> This is the gold
standard. You split your data into “folds,” train the model on some
folds, and test it on the remaining fold. You repeat this and average
the test scores. The model with the best (e.g., lowest) average CV error
is chosen.
将数据分成“折叠”，在一些折叠上训练模型，然后在剩余的折叠上测试模型。重复此操作并取测试分数的平均值。选择平均
CV 误差最小（例如，最小）的模型。</p></li>
<li><p><strong>AIC &amp; BIC:</strong> These are mathematical
adjustments to the training error (like RSS) that add a <em>penalty</em>
for having more predictors. They balance model <em>fit</em> with model
<em>complexity</em>. 这些是对训练误差（如
RSS）的数学调整，会因预测变量较多而增加<em>惩罚</em>。它们平衡了模型<em>拟合度</em>和模型<em>复杂度</em>。</p></li>
</ul>
<h3 id="why-use-r2-in-step-2">Why use <span
class="math inline">\(R^2\)</span> in Step 2?</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p>In Step 2, you are only comparing models <strong>of the same
size</strong> (i.e., all models that have exactly <span
class="math inline">\(k\)</span> predictors). For models with the same
number of parameters, a higher <span class="math inline">\(R^2\)</span>
(or lower RSS) on the training data directly corresponds to a better
fit. You don’t need to penalize for complexity because all models being
compared <em>have the same complexity</em>.
只比较<strong>大小相同</strong>的模型（即所有恰好具有 <span
class="math inline">\(k\)</span>
个预测变量的模型）。对于参数数量相同的模型，训练数据上更高的 <span
class="math inline">\(R^2\)</span>（或更低的
RSS）直接对应着更好的拟合度。您不需要对复杂度进行惩罚，因为所有被比较的模型<em>都具有相同的复杂度</em>。</p>
<h3 id="why-cant-we-use-training-error-in-step-3">Why can’t we use
training error in Step 3?</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p>In Step 3, you are comparing models <strong>of different
sizes</strong> (<span class="math inline">\(M_0\)</span> vs. <span
class="math inline">\(M_1\)</span> vs. <span
class="math inline">\(M_2\)</span>, etc.). As you add predictors, the
training <span class="math inline">\(R^2\)</span> will <em>always</em>
go up (or stay the same), and the training RSS will <em>always</em> go
down (or stay the same). If you used <span
class="math inline">\(R^2\)</span> to pick the best model in Step 3, you
would <em>always</em> pick the most complex model <span
class="math inline">\(M_p\)</span>, which is almost certainly overfit.
将比较<strong>不同大小</strong>的模型（例如 <span
class="math inline">\(M_0\)</span> vs. <span
class="math inline">\(M_1\)</span> vs. <span
class="math inline">\(M_2\)</span> 等）。随着您添加预测变量，训练 <span
class="math inline">\(R^2\)</span>
将<em>始终</em>上升（或保持不变），而训练 RSS
将<em>始终</em>下降（或保持不变）。如果您在步骤 3 中使用 <span
class="math inline">\(R^2\)</span>
来选择最佳模型，那么您<em>始终</em>会选择最复杂的模型 <span
class="math inline">\(M_p\)</span>，而该模型几乎肯定会过拟合。</p>
<p>Therefore, you <em>must</em> use a metric that estimates test error
(like CV) or penalizes for complexity (like AIC, BIC, or Adjusted <span
class="math inline">\(R^2\)</span>) to find the right balance between
fit and simplicity. 因此，您<em>必须</em>使用一个可以估算测试误差（例如
CV）或惩罚复杂度（例如 AIC、BIC 或调整后的 <span
class="math inline">\(R^2\)</span>）的指标来找到拟合度和简单性之间的平衡。</p>
<h2 id="code-analysis">Code Analysis</h2>
<p>The Python code (slides <code>...221249.jpg</code> and
<code>...221303.jpg</code>) implements the <strong>Best Subset
Selection</strong> algorithm using <strong>KNN Regression</strong>.</p>
<h3 id="key-functions">Key Functions</h3>
<ul>
<li><code>main()</code>:
<ol type="1">
<li><strong>Loads Data:</strong> Reads the <code>Credit.csv</code>
file.</li>
<li><strong>Preprocesses Data:</strong>
<ul>
<li>Converts categorical features (‘Gender’, ‘Student’, ‘Married’,
‘Ethnicity’) into numerical ones (dummy variables).
将分类特征（“性别”、“学生”、“已婚”、“种族”）转换为数值特征（虚拟变量）。</li>
<li>Creates the feature matrix <code>X</code> and target variable
<code>y</code> (‘Balance’). 创建特征矩阵 <code>X</code> 和目标变量
<code>y</code>（“余额”）。</li>
<li><strong>Scales</strong> the features using
<code>StandardScaler</code>. This is crucial for KNN, which is sensitive
to the scale of features. 用 <code>StandardScaler</code>
对特征进行<strong>缩放</strong>。这对于 KNN
至关重要，因为它对特征的缩放非常敏感。</li>
</ul></li>
<li><strong>Adds Noise (in the second example):</strong> Slide
<code>...221303.jpg</code> shows code that <em>adds 20 new “noisy”
columns</em> to the data. This is to test if the selection algorithm is
smart enough to ignore them. 向数据中添加 20
个新的“噪声”列的代码。这是为了测试选择算法是否足够智能，能够忽略它们。</li>
<li><strong>Runs Selection:</strong> Calls
<code>best_subset_selection_parallel</code> to do the main work.</li>
<li><strong>Prints Results:</strong> Finds the best subset (lowest
error) and prints the top 20 best-performing subsets.
找到最佳子集（误差最小），并打印出表现最佳的前 20 个子集。</li>
<li><strong>Final Evaluation:</strong> It re-trains a KNN model on
<em>only</em> the best subset and calculates the final cross-validated
RMSE. 仅基于最佳子集重新训练 KNN 模型，并计算最终的交叉验证 RMSE。</li>
</ol></li>
<li><code>evaluate_subset(subset, ...)</code>:
<ul>
<li>This is the “worker” function. It’s called for <em>every single</em>
possible subset.</li>
<li>It takes a <code>subset</code> (a list of feature names, e.g.,
<code>['Income', 'Limit']</code>).</li>
<li>It creates a new <code>X_subset</code> containing <em>only</em>
those columns.</li>
<li>It runs 5-fold cross-validation (<code>cross_val_score</code>) on a
KNN model using this <code>X_subset</code>.</li>
<li>It uses <code>'neg_mean_squared_error'</code> as the metric. This is
negative MSE; a <em>higher</em> score (closer to 0) is better.
它会创建一个新的“X_subset”<em>，仅包含这些列。 它会使用此“X_subset”在
KNN 模型上运行 5 倍交叉验证（“cross_val_score”）。
它使用“neg_mean_squared_error”作为度量标准。这是负
MSE；</em>更高*的分数（越接近 0）越好。</li>
<li>It returns the subset and its average CV score.</li>
</ul></li>
<li><code>best_subset_selection_parallel(model, ...)</code>:
<ul>
<li>This is the “manager” function.这是“管理器”函数。</li>
<li>It iterates from <code>k=1</code> up to the total number of
features.它从“k=1”迭代到特征总数。</li>
<li>For each <code>k</code>, it generates <em>all combinations</em> of
features of that size (this is the <span
class="math inline">\(\binom{p}{k}\)</span> part).
对于每个“k”，它会生成该大小的特征的<em>所有组合</em>（这是 <span
class="math inline">\(\binom{p}{k}\)</span> 部分）。</li>
<li>It uses <code>Parallel</code> and <code>delayed</code> (from
<code>joblib</code>) to run <code>evaluate_subset</code> for all these
combinations <em>in parallel</em>, speeding up the process
significantly. 它使用 <code>Parallel</code> 和
<code>delayed</code>（来自
<code>joblib</code>）对所有这些组合<em>并行</em>运行
<code>evaluate_subset</code>，从而显著加快了处理速度。</li>
<li>It collects all the results and returns
them.它收集所有结果并返回。</li>
</ul></li>
</ul>
<h3 id="analysis-of-the-output">Analysis of the Output</h3>
<ul>
<li><strong>Slide <code>...221255.png</code> (Original Data):</strong>
<ul>
<li>The code runs subset selection on the original dataset.</li>
<li>The “Top 20 Best Feature Subsets” are shown. The CV scores are
negative (they are <code>neg_mean_squared_error</code>), so the scores
<em>closest to zero</em> (smallest magnitude) are best.</li>
<li>The <strong>Best feature subset</strong> is found to be
<code>('Income', 'Limit', 'Rating', 'Student')</code>.</li>
<li>The final cross-validated RMSE for this model is
<strong>105.41</strong>.</li>
</ul></li>
<li><strong>Slide <code>...221309.png</code> (Data with 20 Noisy
Variables):</strong>
<ul>
<li>The code is re-run after adding 20 useless “Noisy” features.</li>
<li>The algorithm <em>still</em> works. It correctly identifies that the
“Noisy” variables are useless.</li>
<li>The <strong>Best feature subset</strong> is now
<code>('Income', 'Limit', 'Student')</code>. (Note: ‘Rating’ was
dropped, likely because it’s highly correlated with ‘Limit’, and the
noisy data made the simpler model perform slightly better in CV).</li>
<li>The final RMSE is <strong>114.94</strong>. This is <em>higher</em>
than the original 105.41, which is expected—the presence of so many
noise variables makes the selection problem harder, but the final model
is still good and, most importantly, <em>it successfully excluded all 20
noisy features</em>. 最终的 RMSE 为 <strong>114.94</strong>。这比最初的
105.41<em>更高</em>，这是预期的——如此多的噪声变量的存在使得选择问题更加困难，但最终模型仍然很好，最重要的是，<em>它成功地排除了所有
20 个噪声特征</em>。</li>
</ul></li>
</ul>
<h2 id="conceptual-overview-the-why">Conceptual Overview: The “Why”</h2>
<p>Slides cover <strong>Chapter 6: Linear Model Selection and
Regularization</strong>, which is all about a fundamental trade-off in
machine learning: the <strong>bias-variance trade-off</strong>.
该部分主要讨论机器学习中的一个基本权衡：<strong>偏差-方差权衡</strong>。</p>
<ul>
<li><p><strong>The Problem (Slide <code>...221320.png</code>):</strong>
Imagine you have a dataset with 50 predictors (<span
class="math inline">\(p=50\)</span>). You want to predict a response
<span class="math inline">\(y\)</span>. 假设你有一个包含 50
个预测变量（p=50）的数据集。你想要预测响应 <span
class="math inline">\(y\)</span>。</p>
<ul>
<li><strong>Model 1 (Full Model):</strong> You use all 50 predictors.
This model is very <strong>flexible</strong>. It will fit the
<em>training data</em> extremely well, resulting in a low
<strong>bias</strong>. However, it’s highly likely that many of those 50
predictors are just “noise” (random, unrelated variables). By fitting to
this noise, the model will be <strong>overfit</strong>. When you show it
new, unseen data (the <em>test data</em>), it will perform poorly. This
is called <strong>high variance</strong>. 你使用了所有 50
个预测变量。这个模型非常<strong>灵活</strong>。它能很好地拟合<em>训练数据</em>，从而产生较低的<strong>偏差</strong>。然而，这
50
个预测变量中很可能有很多只是“噪声”（随机的、不相关的变量）。由于拟合这些噪声，模型会<strong>过拟合</strong>。当你向它展示新的、未见过的数据（<em>测试数据</em>）时，它的表现会很差。这被称为<strong>高方差</strong>。</li>
<li><strong>Model 2 (Subset Model):</strong> You intelligently select
only the 3 predictors (<span class="math inline">\(q=3\)</span>) that
are <em>actually</em> related to <span class="math inline">\(y\)</span>.
This model is less flexible. It won’t fit the <em>training data</em> as
perfectly as Model 1 (it has higher <strong>bias</strong>). But, because
it’s <em>not</em> fitting the noise, it will generalize much better to
new data. It will have a much lower <strong>variance</strong>, and thus
a lower overall <em>test error</em>. 你智能地只选择与 <span
class="math inline">\(y\)</span> <em>真正</em>相关的 3 个预测变量 (<span
class="math inline">\(q=3\)</span>)。这个模型的灵活性较差。它对
<em>训练数据</em> 的拟合度不如模型 1
完美（它的<strong>偏差</strong>更高）。但是，由于它对噪声的拟合度更高，因此对新数据的泛化能力会更好。它的<strong>方差</strong>会更低，因此总体的<em>测试误差</em>也会更低。</li>
</ul></li>
<li><p><strong>The Goal:</strong> The goal is to find the model that has
the <strong>lowest test error</strong>. We need a formal method to
<em>find</em> the best subset (like Model 2) without just guessing.
<strong>目标是找到</strong>测试误差**最低的模型。我们需要一个正式的方法来<em>找到</em>最佳子集（例如模型
2），而不是仅仅靠猜测。</p></li>
<li><p><strong>Two Main Strategies (Slide
<code>...221314.png</code>):</strong></p>
<ol type="1">
<li><p><strong>Subset Selection (Section 6.1):</strong> This is what
we’re focused on. It’s an “all-or-nothing” approach. You either
<em>keep</em> a variable in the model or you <em>discard</em> it
completely. The “Best Subset Selection” algorithm is the most extreme,
“brute-force” way to do this.
是我们关注的重点。这是一种“全有或全无”的方法。你要么在模型中“保留”一个变量，要么“彻底丢弃”它。“最佳子集选择”算法是最极端、最“暴力”的做法。</p></li>
<li><p><strong>Shrinkage/Regularization (Section 6.2):</strong> This is
a more subtle approach (e.g., Ridge Regression, LASSO). Instead of
discarding variables, you <em>keep all <span
class="math inline">\(p\)</span> variables</em> but add a penalty to the
model that “shrinks” the coefficients (<span
class="math inline">\(\beta\)</span>) of the useless variables towards
zero.
这是一种更巧妙的方法（例如，岭回归、LASSO）。你不是丢弃变量，而是<em>保留所有
<span class="math inline">\(p\)</span>
个变量</em>，但会给模型添加一个惩罚项，将无用变量的系数（<span
class="math inline">\(\beta\)</span>）“收缩”到零。</p></li>
</ol></li>
</ul>
<h2 id="questions">Questions 🎯</h2>
<h3 id="q1-how-to-compare-which-model-is-better">Q1: “How to compare
which model is better?”</h3>
<p>(From slides <code>...221320.png</code> and
<code>...221326.png</code>)</p>
<p>This is the most important question. You <strong>cannot</strong> use
metrics based on <em>training data</em> (like <span
class="math inline">\(R^2\)</span> or RSS - Residual Sum of Squares) to
compare models with <em>different numbers of predictors</em>.
这是最重要的问题。您<strong>不能</strong>使用基于<em>训练数据</em>的指标（例如
R^2 或 RSS - 残差平方和）来比较具有<em>不同数量预测变量</em>的模型。</p>
<ul>
<li><p><strong>The Trap:</strong> A model with more predictors will
<em>always</em> have a higher <span class="math inline">\(R^2\)</span>
(or lower RSS) on the data it was trained on. <span
class="math inline">\(R^2\)</span> will <em>always</em> increase as you
add variables, even if they are pure noise. If you used <span
class="math inline">\(R^2\)</span> to compare a 3-predictor model to a
10-predictor model, the 10-predictor model would <em>always</em> look
better on paper, even if it’s terribly overfit.
具有更多预测变量的模型在其训练数据上<em>总是</em>具有更高的
R^2（或更低的 RSS）。随着变量的增加，R^2
会<em>总是</em>增加，即使这些变量是纯噪声。如果您使用 R^2 来比较 3
个预测变量的模型和 10 个预测变量的模型，那么 10
个预测变量的模型在纸面上<em>总是</em>看起来更好，即使它严重过拟合。</p></li>
<li><p><strong>The Correct Way:</strong> You must use a metric that
estimates the <strong>test error</strong>. The slides and code show two
ways:您必须使用一个能够估计<strong>测试误差</strong>的指标。</p>
<ol type="1">
<li><strong>Cross-Validation (CV):</strong> This is the method used in
your Python code. It works by:
<ul>
<li>Splitting your training data into <span
class="math inline">\(k\)</span> “folds” (e.g., 5 folds).
将训练数据拆分成 <span class="math inline">\(k\)</span> 个“折叠”（例如 5
个折叠）。</li>
<li>Training the model on 4 folds and testing it on the 5th fold.
使用其中 4 个折叠训练模型，并使用第 5 个折叠进行测试。</li>
<li>Repeating this 5 times, so each fold gets to be the test set once.
重复此操作 5 次，使每个折叠都作为测试集一次。</li>
<li>Averaging the 5 test errors. 对 5 个测试误差求平均值。 This gives
you a robust estimate of how your model will perform on <em>unseen
data</em>. You then choose the model with the best (lowest) average CV
error.
这可以让你对模型在<em>未见数据</em>上的表现有一个稳健的估计。然后，你可以选择平均
CV 误差最小（最佳）的模型。</li>
</ul></li>
<li><strong>Mathematical Adjustments (AIC, BIC, Adjusted <span
class="math inline">\(R^2\)</span>):</strong> These are formulas that
take the training error (like RSS) and add a <em>penalty</em> for each
predictor (<span class="math inline">\(k\)</span>) you add.
<ul>
<li><span class="math inline">\(AIC \approx RSS +
2k\sigma^2\)</span></li>
<li><span class="math inline">\(BIC \approx RSS +
\log(n)k\sigma^2\)</span> A model with more predictors (larger <span
class="math inline">\(k\)</span>) gets a bigger penalty. To be chosen, a
more complex model must <em>significantly</em> improve the RSS to
overcome this penalty. 预测变量越多（k
越大）的模型，惩罚越大。要被选中，更复杂的模型必须<em>显著</em>提升 RSS
以克服此惩罚。</li>
</ul></li>
</ol></li>
</ul>
<h3 id="q2-why-using-r2-for-step-2">Q2: “Why using <span
class="math inline">\(R^2\)</span> for step 2?”</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p><strong>Step 2</strong> of the “Best Subset Selection” algorithm
says: “For <span class="math inline">\(k = 1, \dots, p\)</span>: Fit all
<span class="math inline">\(\binom{p}{k}\)</span> models… Pick the best
model, that with the largest <span class="math inline">\(R^2\)</span>, …
and call it <span class="math inline">\(M_k\)</span>.” “对于 <span
class="math inline">\(k = 1, \dots, p\)</span>：拟合所有 <span
class="math inline">\(\binom{p}{k}\)</span> 个模型……选择具有最大 <span
class="math inline">\(R^2\)</span> 的最佳模型……并将其命名为 <span
class="math inline">\(M_k\)</span>。”</p>
<ul>
<li><strong>The Reason:</strong> In Step 2, you are <em>only</em>
comparing models <strong>of the same size</strong>. For example, when
<span class="math inline">\(k=3\)</span>, you are comparing all possible
3-predictor models: 步骤 2
中，您<em>仅</em>比较**相同大小的模型。例如，当 <span
class="math inline">\(k=3\)</span> 时，您将比较所有可能的 3
预测变量模型：
<ul>
<li>Model A: (<span class="math inline">\(X_1, X_2, X_3\)</span>)</li>
<li>Model B: (<span class="math inline">\(X_1, X_2, X_4\)</span>)</li>
<li>Model C: (<span class="math inline">\(X_1, X_3, X_5\)</span>)</li>
<li>…and so on.</li>
</ul>
Since all these models have the <em>exact same complexity</em> (they all
have <span class="math inline">\(k=3\)</span> predictors), there is no
risk of unfairly favoring a more complex model. Therefore, you are free
to use a training metric like <span class="math inline">\(R^2\)</span>
(or RSS). The model with the highest <span
class="math inline">\(R^2\)</span> is, by definition, the one that
<em>best fits the training data</em> for that specific size <span
class="math inline">\(k\)</span>.
由于所有这些模型都具有<em>完全相同的复杂度</em>（它们都具有 <span
class="math inline">\(k=3\)</span>
个预测变量），因此不存在不公平地偏向更复杂模型的风险。因此，您可以自由使用像
<span class="math inline">\(R^2\)</span>（或
RSS）这样的训练指标。根据定义，具有最高 <span
class="math inline">\(R^2\)</span> 的模型就是在特定大小 <span
class="math inline">\(k\)</span>
下<em>与训练数据拟合度</em>最高的模型。</li>
</ul>
<h3
id="q3-cannot-use-training-error-in-step-3.-why-not-步骤-3-中不能使用训练误差-为什么">Q3:
“Cannot use training error in Step 3.” Why not? “步骤 3
中不能使用训练误差。” 为什么？</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p><strong>Step 3</strong> says: “Select a single best model from <span
class="math inline">\(M_0, M_1, \dots, M_p\)</span> by cross validation,
AIC, or BIC.”“通过交叉验证、AIC 或 BIC，从 <span
class="math inline">\(M_0、M_1、\dots、M_p\)</span>
中选择一个最佳模型。”</p>
<ul>
<li><p><strong>The Reason:</strong> In Step 3, you are now comparing
models <strong>of different sizes</strong>. You are comparing the best
1-predictor model (<span class="math inline">\(M_1\)</span>) vs. the
best 2-predictor model (<span class="math inline">\(M_2\)</span>)
vs. the best 3-predictor model (<span
class="math inline">\(M_3\)</span>), and so on, all the way up to <span
class="math inline">\(M_p\)</span>. 在步骤 3
中，您正在比较<strong>不同大小</strong>的模型。您正在比较最佳的单预测模型
(<span class="math inline">\(M_1\)</span>)、最佳的双预测模型 (<span
class="math inline">\(M_2\)</span>) 和最佳的三预测模型 (<span
class="math inline">\(M_3\)</span>)，依此类推，直到 <span
class="math inline">\(M_p\)</span>。</p>
<p>As explained in Q1, if you used a training error metric like <span
class="math inline">\(R^2\)</span> here, the <span
class="math inline">\(R^2\)</span> would just keep going up, and you
would <em>always</em> select the largest, most complex model, <span
class="math inline">\(M_p\)</span>. This completely defeats the purpose
of model selection. 如问题 1 所述，如果您在此处使用像 <span
class="math inline">\(R^2\)</span> 这样的训练误差指标，那么 <span
class="math inline">\(R^2\)</span>
会持续上升，并且您<em>总是</em>会选择最大、最复杂的模型 <span
class="math inline">\(M_p\)</span>。这完全违背了模型选择的目的。</p>
<p>Therefore, in Step 3, you <em>must</em> use a method that estimates
<strong>test error</strong> (like Cross-Validation) or one that
<strong>penalizes for complexity</strong> (like AIC or BIC) to find the
“sweet spot” model that balances fit and simplicity. 因此，在步骤 3
中，您<em>必须</em>使用一种估算<strong>测试误差</strong>的方法（例如交叉验证）或<strong>惩罚复杂性</strong>的方法（例如
AIC 或
BIC），以找到在拟合度和简单性之间取得平衡的“最佳点”模型。</p></li>
</ul>
<h2 id="mathematical-deep-dive">Mathematical Deep Dive 🧮</h2>
<ul>
<li><strong><span class="math inline">\(Y = \beta_0 + \beta_1X_1 + \dots
+ \beta_pX_p + \epsilon\)</span>:</strong> The full linear model. The
goal of subset selection is to find a subset of <span
class="math inline">\(X_j\)</span>’s where <span
class="math inline">\(\beta_j \neq 0\)</span> and set all other <span
class="math inline">\(\beta\)</span>’s to 0.
完整的线性模型。子集选择的目标是找到 <span
class="math inline">\(X_j\)</span> 的一个子集，其中 $_j 等于
0，并将所有其他 <span class="math inline">\(\beta\)</span> 设置为
0。</li>
<li><strong><span class="math inline">\(2^p\)</span>
combinations:</strong> (Slide <code>...221333.png</code>) This is the
total number of models you have to check. For each of the <span
class="math inline">\(p\)</span> variables, you have two choices: either
it is <strong>IN</strong> the model or it is
<strong>OUT</strong>.这是你需要检查的模型总数。对于每个 <span
class="math inline">\(p\)</span>
个变量，你有两个选择：要么它在模型<strong>内部</strong>，要么它在模型<strong>外部</strong>。
<ul>
<li>Example: <span class="math inline">\(p=3\)</span> (variables <span
class="math inline">\(X_1, X_2, X_3\)</span>)</li>
<li>The <span class="math inline">\(2^3 = 8\)</span> possible models
are:
<ol type="1">
<li>{} (The null model, <span class="math inline">\(M_0\)</span>)</li>
<li>{ <span class="math inline">\(X_1\)</span> }</li>
<li>{ <span class="math inline">\(X_2\)</span> }</li>
<li>{ <span class="math inline">\(X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_2\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_2, X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_2, X_3\)</span> } (The full
model, <span class="math inline">\(M_3\)</span>)</li>
</ol></li>
<li>This is why this method is called an <strong>“exhaustive
search”</strong>. It literally checks every single one. For <span
class="math inline">\(p=20\)</span>, <span
class="math inline">\(2^{20}\)</span> is over a million
models!这就是该方法被称为<strong>“穷举搜索”</strong>的原因。它实际上会检查每一个模型。对于
<span class="math inline">\(p=20\)</span>，<span
class="math inline">\(2^{20}\)</span> 就超过一百万个模型！</li>
</ul></li>
<li><strong><span class="math inline">\(\binom{p}{k} =
\frac{p!}{k!(p-k)!}\)</span>:</strong> (Slide
<code>...221333.png</code>) This is the “combinations” formula. It tells
you <em>how many</em> models you fit <em>in Step 2</em> for a specific
<span
class="math inline">\(k\)</span>.这是“组合”公式。它告诉你，对于特定的
<span class="math inline">\(k\)</span>，<em>在步骤 2</em>中，你拟合了
<em>多少</em> 个模型。
<ul>
<li>Example: <span class="math inline">\(p=10\)</span> total
predictors.</li>
<li>For <span class="math inline">\(k=1\)</span>: You fit <span
class="math inline">\(\binom{10}{1} = 10\)</span> models.</li>
<li>For <span class="math inline">\(k=2\)</span>: You fit <span
class="math inline">\(\binom{10}{2} = \frac{10 \times 9}{2 \times 1} =
45\)</span> models.</li>
<li>For <span class="math inline">\(k=3\)</span>: You fit <span
class="math inline">\(\binom{10}{3} = \frac{10 \times 9 \times 8}{3
\times 2 \times 1} = 120\)</span> models.</li>
<li>…and so on. The sum of all these <span
class="math inline">\(\binom{p}{k}\)</span> from <span
class="math inline">\(k=0\)</span> to <span
class="math inline">\(k=p\)</span> equals <span
class="math inline">\(2^p\)</span>.</li>
</ul></li>
</ul>
<h2 id="detailed-code-analysis">Detailed Code Analysis 💻</h2>
<p>Your slides show Python code that applies the <strong>Best Subset
Selection algorithm</strong> to a <strong>KNN Regressor</strong>. This
is a great example of how the <em>selection algorithm</em> is
independent of the <em>model type</em> (as mentioned in slide
<code>...221314.png</code>).</p>
<h3 id="key-functions-1">Key Functions</h3>
<ul>
<li><strong><code>main()</code></strong>
<ol type="1">
<li><strong>Load &amp; Preprocess:</strong> Reads
<code>Credit.csv</code>. The most important step here is converting
categorical text (like ‘Male’/‘Female’) into numbers (1/0).</li>
<li><strong>Scale Data:</strong> <code>scaler = StandardScaler()</code>
and <code>X_scaled = scaler.fit_transform(X)</code>.
<ul>
<li><strong>WHY?</strong> This is <strong>CRITICAL</strong> for KNN. KNN
works by measuring distance. If ‘Income’ (e.g., 50,000) is on a vastly
different scale than ‘Cards’ (e.g., 3), the ‘Income’ feature will
completely dominate the distance calculation, making ‘Cards’ irrelevant.
Scaling resizes all features to have a mean of 0 and standard deviation
of 1, so they all contribute fairly.</li>
</ul></li>
<li><strong>Handle Noisy Data (Slide
<code>...221303.jpg</code>):</strong> This version of the code
<em>intentionally</em> adds 20 columns of useless, random numbers. This
is a test to see if the algorithm is smart enough to ignore them.</li>
<li><strong>Run Selection:</strong>
<code>results_df = best_subset_selection_parallel(...)</code>. This
function does all the heavy lifting (explained next).</li>
<li><strong>Find Best Model:</strong>
<code>results_df.sort_values(by='CV_Score', ascending=False)</code>.
<ul>
<li><strong>WHY <code>ascending=False</code>?</strong> The code uses the
metric <code>'neg_mean_squared_error'</code>. This is MSE, but negative
(e.g., -15000). A <em>better</em> model has an error closer to 0 (e.g.,
-10000). Since -10000 is <em>greater than</em> -15000, you sort in
descending (high-to-low) order to put the best models at the top.</li>
</ul></li>
<li><strong>Final Evaluation (Step 3):</strong>
<code>final_scores = cross_val_score(knn, X_best, y, ...)</code>
<ul>
<li>This is the implementation of Step 3. It takes <em>only</em> the
single best subset (<code>X_best</code>) and runs a <em>new</em>
cross-validation on it. This gives a final, unbiased estimate of how
good that one model is.</li>
</ul></li>
<li><strong>Print RMSE:</strong>
<code>final_rmse = np.sqrt(-final_scores)</code>. It converts the
negative MSE back into a positive RMSE (Root Mean Squared Error), which
is in the same units as the target <span
class="math inline">\(y\)</span> (in this case, ‘Balance’ in
dollars).</li>
</ol></li>
<li><strong><code>best_subset_selection_parallel(model, ...)</code></strong>
<ol type="1">
<li>This is the “manager” function. It implements the loop from Step
2.</li>
<li><code>for k in range(1, n_features + 1):</code> This is the loop
“For <span class="math inline">\(k = 1, \dots, p\)</span>”.</li>
<li><code>subsets = list(combinations(feature_names, k))</code>: This
generates the <span class="math inline">\(\binom{p}{k}\)</span>
combinations for the current <span
class="math inline">\(k\)</span>.</li>
<li><code>results = Parallel(n_jobs=n_jobs)(...)</code>: This is a
non-core, “speed-up” command. It uses the <code>joblib</code> library to
run the evaluations on all your computer’s CPU cores at once (in
parallel). Without this, checking millions of models would take
days.</li>
<li><code>subset_scores = ... [delayed(evaluate_subset)(...) ...]</code>
This line farms out the <em>actual work</em> to the
<code>evaluate_subset</code> function for every single subset.</li>
</ol></li>
<li><strong><code>evaluate_subset(subset, ...)</code></strong>
<ol type="1">
<li>This is the “worker” function. It gets called thousands or millions
of times.</li>
<li>Its job is to evaluate <em>one single subset</em> (e.g.,
<code>('Income', 'Limit', 'Student')</code>).</li>
<li><code>X_subset = X[list(subset)]</code>: It slices the data to get
<em>only</em> these columns.</li>
<li><code>scores = cross_val_score(model, X_subset, ...)</code>:
<strong>This is the most important line.</strong> It takes the subset
and performs a full 5-fold cross-validation on it.</li>
<li><code>return (subset, np.mean(scores))</code>: It returns the subset
and its average CV score.</li>
</ol></li>
</ul>
<h3 id="summary-of-outputs-slides-...221255.png-...221309.png">Summary
of Outputs (Slides <code>...221255.png</code> &amp;
<code>...221309.png</code>)</h3>
<ul>
<li><strong>Original Data (Slide <code>...221255.png</code>):</strong>
<ul>
<li><strong>Best Subset:</strong>
<code>('Income', 'Limit', 'Rating', 'Student')</code></li>
<li><strong>Final RMSE:</strong> ~105.4</li>
</ul></li>
<li><strong>Data with 20 “Noisy” Variables (Slide
<code>...221309.png</code>):</strong>
<ul>
<li><strong>Best Subset:</strong>
<code>('Income', 'Limit', 'Student')</code></li>
<li><strong>Result:</strong> The algorithm <em>successfully</em>
identified that all 20 “Noisy” variables were useless and
<strong>excluded every single one of them</strong> from the best
models.</li>
<li><strong>Final RMSE:</strong> ~114.9</li>
<li><strong>Key Takeaway:</strong> The RMSE is slightly higher, which
makes sense because the selection problem was much harder. But the
<em>method worked perfectly</em>. It filtered all the “noise” and found
a simple, powerful model, just as the theory on slide
<code>...221320.png</code> predicted.</li>
</ul></li>
</ul>
<h1
id="the-core-problem-training-error-vs.-test-error-核心问题训练误差-vs.-测试误差">2.
The Core Problem: Training Error vs. Test Error 核心问题：训练误差
vs. 测试误差</h1>
<p>The central theme of these slides is finding the “best” model. The
problem is that a model with more predictors (more complex) will
<em>always</em> fit the data it was trained on better. This is a trap.
寻找“最佳”模型。问题在于，预测因子越多（越复杂）的模型<em>总是</em>能更好地拟合训练数据。这是一个陷阱。</p>
<ul>
<li><strong>Training Error:</strong> How well the model fits the data we
used to build it. <strong><span class="math inline">\(R^2\)</span> and
<span class="math inline">\(RSS\)</span> measure this.</strong>
模型与我们构建模型时所用数据的拟合程度。<strong><span
class="math inline">\(R^2\)</span> 和 <span
class="math inline">\(RSS\)</span> 衡量了这一点。</strong></li>
<li><strong>Test Error:</strong> How well the model predicts new, unseen
data. This is what we <em>actually</em> care about. A model that is too
complex (e.g., has 10 predictors when only 3 are useful) will have low
training error but very high test error. This is called
<strong>overfitting</strong>.
模型预测新的、未见过的数据的准确程度。这才是我们<em>真正</em>关心的。过于复杂的模型（例如，有
10 个预测因子，但只有 3
个有用）的训练误差会很低，但测试误差会很高。这被称为<strong>过拟合</strong>。</li>
</ul>
<p>The goal is to choose a model that has the lowest <em>test
error</em>. The metrics below (Adjusted <span
class="math inline">\(R^2\)</span>, AIC, BIC) are all attempts to
<em>estimate</em> this test error without having to actually collect new
data. They do this by adding a <strong>penalty</strong> for complexity.
目标是选择一个具有最低<em>测试误差</em>的模型。以下指标（调整后的 <span
class="math inline">\(R^2\)</span>、AIC、BIC）都是在无需实际收集新数据的情况下尝试<em>估计</em>此测试误差。他们通过增加<strong>复杂度惩罚</strong>来实现这一点。</p>
<h2 id="basic-metrics-measures-of-fit">Basic Metrics (Measures of
Fit)</h2>
<p>These formulas from slide 13 describe how well a model fits the
<em>training data</em>.</p>
<h3 id="residue-error-残差误差">Residue (Error) 残差（误差）</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\epsilon}_i = y_i - \hat{y}_i = y_i -
\hat{\beta}_0 - \sum_{j=1}^{p} \hat{\beta}_j x_{ij}\)</span></li>
<li><strong>Concept:</strong> This is the most basic building block.
It’s the difference between the <em>actual</em> observed value (<span
class="math inline">\(y_i\)</span>) and the value your model
<em>predicted</em> (<span class="math inline">\(\hat{y}_i\)</span>). It
is the “error” for a single data point.
这是最基本的构建块。它是<em>实际</em>观测值 (<span
class="math inline">\(y_i\)</span>) 与模型*预测值 (<span
class="math inline">\(\hat{y}_i\)</span>)
之间的差值。它是单个数据点的“误差”。</li>
</ul>
<h3 id="residual-sum-of-squares-rss-残差平方和-rss">Residual Sum of
Squares (RSS) 残差平方和 (RSS)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(RSS =
\sum_{i=1}^{n} \hat{\epsilon}_i^2\)</span></li>
<li><strong>Concept:</strong> This is the overall measure of model
error. You square all the individual errors (residues) to make them
positive and then add them all up.
这是模型误差的总体度量。将所有单个误差（残差）平方，使其为正，然后将它们全部相加。</li>
<li><strong>Goal:</strong> The entire process of linear regression
(called “Ordinary Least Squares”) is designed to find the <span
class="math inline">\(\hat{\beta}\)</span> coefficients that make this
<strong>RSS value as small as possible</strong>.
整个线性回归过程（称为“普通最小二乘法”）旨在找到使<strong>RSS
值尽可能小</strong>的 <span class="math inline">\(\hat{\beta}\)</span>
个系数。</li>
<li><strong>The Flaw 缺陷:</strong> <span
class="math inline">\(RSS\)</span> will <em>always</em> decrease (or
stay the same) as you add more predictors (<span
class="math inline">\(p\)</span>). A model with all 10 predictors will
have a lower <span class="math inline">\(RSS\)</span> than a model with
9, even if that 10th predictor is useless. Therefore, <span
class="math inline">\(RSS\)</span> is useless for choosing
<em>between</em> models of different sizes. 随着预测变量 (<span
class="math inline">\(p\)</span>) 的增加，<span
class="math inline">\(RSS\)</span>
总是会减小（或保持不变）。一个包含所有 10 个预测变量的模型的 <span
class="math inline">\(RSS\)</span> 会低于一个包含 9
个预测变量的模型，即使第 10 个预测变量毫无用处。因此，<span
class="math inline">\(RSS\)</span>
对于在不同规模的模型之间进行选择毫无用处。</li>
</ul>
<h3 id="r-squared-r2">R-squared (<span
class="math inline">\(R^2\)</span>)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(R^2 = 1 -
\frac{SS_{error}}{SS_{total}} = 1 - \frac{RSS}{\sum_{i=1}^{n} (y_i -
\bar{y})^2}\)</span></li>
<li><strong>Concept:</strong> This metric reframes <span
class="math inline">\(RSS\)</span> into a more interpretable
percentage.此指标将 <span class="math inline">\(RSS\)</span>
重新定义为更易于解释的百分比。
<ul>
<li><span class="math inline">\(SS_{total}\)</span> (the denominator)
represents the <em>total variance</em> of the data. It’s the error you
would get if your “model” was just guessing the average value (<span
class="math inline">\(\bar{y}\)</span>) for every single observation.
（分母）表示数据的<em>总方差</em>。如果你的“模型”只是猜测每个观测值的平均值
(<span
class="math inline">\(\bar{y}\)</span>)，那么你就会得到这个误差。</li>
<li><span class="math inline">\(SS_{error}\)</span> (the <span
class="math inline">\(RSS\)</span>) is the error <em>after</em> using
your model. 是“模型解释的总方差的比例”。 <span
class="math inline">\(R^2\)</span> 为 0.75
意味着你的模型可以解释响应变量 75% 的变异。</li>
<li><span class="math inline">\(R^2\)</span> is the “proportion of total
variance explained by the model.” An <span
class="math inline">\(R^2\)</span> of 0.75 means your model can explain
75% of the variation in the response variable.</li>
</ul></li>
<li><strong>The Flaw 缺陷:</strong> Just like <span
class="math inline">\(RSS\)</span>, <span
class="math inline">\(R^2\)</span> will <em>always</em> increase (or
stay the same) as you add more predictors. This is visually confirmed in
Figure 6.1, where the red line for <span
class="math inline">\(R^2\)</span> only goes up. It will always pick the
most complex model. 与 <span class="math inline">\(RSS\)</span>
一样，随着预测变量的增加，<span class="math inline">\(R^2\)</span>
会<em>始终</em>增加（或保持不变）。图 6.1 直观地证实了这一点，其中 <span
class="math inline">\(R^2\)</span>
的红线只会上升。它总是会选择最复杂的模型。</li>
</ul>
<h2
id="advanced-metrics-for-model-selection-高级指标用于模型选择">Advanced
Metrics (For Model Selection) 高级指标（用于模型选择）</h2>
<p>These metrics “fix” the flaw of <span
class="math inline">\(R^2\)</span> by including a penalty for the number
of predictors.</p>
<h3 id="adjusted-r2">Adjusted <span
class="math inline">\(R^2\)</span></h3>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
  \text{Adjusted } R^2 = 1 - \frac{RSS / (n - p - 1)}{SS_{total} / (n -
1)}
  \]</span></li>
<li><strong>Mathematical Concept:</strong> This formula replaces the
“Sum of Squares” (<span class="math inline">\(SS\)</span>) with “Mean
Squares” (<span class="math inline">\(MS\)</span>).
<ul>
<li><span class="math inline">\(MS_{error} =
\frac{RSS}{n-p-1}\)</span></li>
<li><span class="math inline">\(MS_{total} =
\frac{SS_{total}}{n-1}\)</span></li>
</ul></li>
<li><strong>The “Penalty” Explained:</strong> The penalty is
<strong>degrees of freedom</strong>.
<ul>
<li><span class="math inline">\(n\)</span> = number of data points.</li>
<li><span class="math inline">\(p\)</span> = number of predictors.</li>
<li>The term <span class="math inline">\(n-p-1\)</span> is the degrees
of freedom for the residuals. You start with <span
class="math inline">\(n\)</span> data points, but you “use up” one
degree of freedom to estimate the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) and <span
class="math inline">\(p\)</span> more to estimate the <span
class="math inline">\(p\)</span> slopes.</li>
</ul></li>
<li><strong>How it Works:</strong>
<ol type="1">
<li>When you add a new predictor (increase <span
class="math inline">\(p\)</span>), <span
class="math inline">\(RSS\)</span> goes down, which makes the numerator
(<span class="math inline">\(MS_{error}\)</span>) smaller.</li>
<li>…But, increasing <span class="math inline">\(p\)</span>
<em>also</em> decreases the denominator (<span
class="math inline">\(n-p-1\)</span>), which makes the numerator (<span
class="math inline">\(MS_{error}\)</span>) <em>larger</em>.</li>
</ol>
<ul>
<li>This creates a “tug-of-war.” If the new predictor is
<strong>useful</strong>, it will drop <span
class="math inline">\(RSS\)</span> a lot, and Adjusted <span
class="math inline">\(R^2\)</span> will <strong>increase</strong>. If
the new predictor is <strong>useless</strong>, <span
class="math inline">\(RSS\)</span> will barely change, and the penalty
from decreasing the denominator will win, causing Adjusted <span
class="math inline">\(R^2\)</span> to <strong>decrease</strong>.</li>
</ul></li>
<li><strong>Goal:</strong> You select the model with the
<strong>highest</strong> Adjusted <span
class="math inline">\(R^2\)</span>.</li>
</ul>
<h3 id="akaike-information-criterion-aic">Akaike Information Criterion
(AIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Concept Breakdown:</strong>
<ul>
<li><span class="math inline">\(\ell(\hat{\theta})\)</span>: This is the
<strong>Maximized Likelihood Function</strong>.
<ul>
<li>The <strong>Likelihood Function</strong> <span
class="math inline">\(\ell(\theta)\)</span> asks: “Given a set of model
parameters <span class="math inline">\(\theta\)</span>, how probable is
the data we observed?”</li>
<li>The <strong>Maximum Likelihood Estimate (MLE)</strong> <span
class="math inline">\(\hat{\theta}\)</span> is the specific set of
parameters (the <span class="math inline">\(\hat{\beta}\)</span>’s) that
<em>maximizes</em> this probability.</li>
</ul></li>
<li><span class="math inline">\(\log \ell(\hat{\theta})\)</span>: The
<strong>log-likelihood</strong>. This is just a number that represents
the <em>best possible fit</em> the model can achieve for the data. A
higher number is a better fit.</li>
<li><span class="math inline">\(-2 \log \ell(\hat{\theta})\)</span>:
This is the <strong>Deviance</strong>. Since a higher log-likelihood is
better, a <em>lower</em> deviance is better. This term measures
<strong>poorness-of-fit</strong>.</li>
<li><span class="math inline">\(d\)</span>: The number of parameters
estimated by the model. (e.g., <span class="math inline">\(p\)</span>
predictors + 1 intercept).</li>
<li><span class="math inline">\(2d\)</span>: This is the <strong>Penalty
Term</strong>.</li>
</ul></li>
<li><strong>How it Works:</strong> <span class="math inline">\(AIC =
(\text{Poorness-of-Fit}) + (\text{Complexity Penalty})\)</span>. As you
add predictors, the fit gets better (the deviance term goes down), but
the penalty term (<span class="math inline">\(2d\)</span>) goes up.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> AIC.</li>
</ul>
<h3 id="bayesian-information-criterion-bic">Bayesian Information
Criterion (BIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(BIC =
-2 \log \ell(\hat{\theta}) + \log(n)d\)</span></li>
<li><strong>Concept:</strong> This is mathematically identical to AIC,
but the penalty term is different.
<ul>
<li><strong>AIC Penalty:</strong> <span
class="math inline">\(2d\)</span></li>
<li><strong>BIC Penalty:</strong> <span
class="math inline">\(\log(n)d\)</span></li>
</ul></li>
<li><strong>Comparison:</strong>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations
in your dataset.</li>
<li>As long as your dataset has 8 or more observations (<span
class="math inline">\(n \ge 8\)</span>), <span
class="math inline">\(\log(n)\)</span> will be greater than 2.</li>
<li>This means <strong>BIC applies a much harsher penalty for
complexity</strong> than AIC.</li>
</ul></li>
<li><strong>Consequence:</strong> BIC will tend to choose
<em>simpler</em> models (fewer predictors) than AIC.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> BIC.</li>
</ul>
<h2 id="the-deeper-theory-why-aic-works">The Deeper Theory: Why AIC
Works</h2>
<p>Slide 27 (“Understanding AIC”) gives the deep mathematical
justification.</p>
<ul>
<li><strong>Goal:</strong> We have a <em>true</em>, unknown process
<span class="math inline">\(p\)</span> that generates our data. We are
creating a model <span class="math inline">\(\hat{p}_j\)</span>. We want
our model to be as “close” to the truth as possible.</li>
<li><strong>Kullback-Leibler (K-L) Distance:</strong> This is a function
<span class="math inline">\(K(p, \hat{p}_j)\)</span> that measures the
“information lost” when you use your model <span
class="math inline">\(\hat{p}_j\)</span> to approximate the truth <span
class="math inline">\(p\)</span>. You want to <em>minimize</em> this
distance.</li>
<li><strong>The Math:</strong>
<ol type="1">
<li><span class="math inline">\(K(p, \hat{p}_j) = \int p(y) \log \left(
\frac{p(y)}{\hat{p}_j(y)} \right) dy\)</span></li>
<li>This splits into: <span class="math inline">\(K(p, \hat{p}_j) =
\underbrace{\int p(y) \log(p(y)) dy}_{\text{Constant}} -
\underbrace{\int p(y) \log(\hat{p}_j(y)) dy}_{\text{This is what we need
to maximize}}\)</span></li>
</ol></li>
<li><strong>The Problem:</strong> We can’t calculate that second term
because it requires knowing the <em>true</em> function <span
class="math inline">\(p\)</span>.</li>
<li><strong>Akaike’s Insight:</strong> Akaike proved that the
log-likelihood we <em>can</em> calculate, <span
class="math inline">\(\log \ell(\hat{\theta})\)</span>, is a
<em>biased</em> estimator of that target. He also proved that the bias
is approximately <span class="math inline">\(-d\)</span>.</li>
<li><strong>The Solution:</strong> An <em>unbiased</em> estimate of the
target is <span class="math inline">\(\log \ell(\hat{\theta}) -
d\)</span>.</li>
<li><strong>Final Step:</strong> For historical and statistical reasons,
he multiplied this by <span class="math inline">\(-2\)</span> to create
the final AIC formula.</li>
<li><strong>Conclusion:</strong> AIC is not just a random formula. It is
a carefully derived estimate of how much information your model loses
compared to the “truth” (i.e., its expected performance on new
data).</li>
</ul>
<h2 id="aicbic-for-linear-regression">AIC/BIC for Linear Regression</h2>
<p>Slide 26 shows how these general formulas simplify for linear
regression (assuming normal, Gaussian errors).</p>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Linear Regression Formula:</strong> <span
class="math inline">\(AIC = \frac{1}{n\hat{\sigma}^2}(RSS +
2d\hat{\sigma}^2)\)</span></li>
</ul>
<p><strong>Key Insight:</strong> For linear regression, the
“poorness-of-fit” term (<span class="math inline">\(-2 \log
\ell(\hat{\theta})\)</span>) is <em>directly proportional to</em> the
<span class="math inline">\(RSS\)</span>.</p>
<p>This makes it much easier to understand. You can just think of the
formulas as: * <strong>AIC <span class="math inline">\(\approx\)</span>
<span class="math inline">\(RSS + 2d\hat{\sigma}^2\)</span></strong> *
<strong>BIC <span class="math inline">\(\approx\)</span> <span
class="math inline">\(RSS + \log(n)d\hat{\sigma}^2\)</span></strong></p>
<p>(Here <span class="math inline">\(\hat{\sigma}^2\)</span> is an
estimate of the error variance, which can often be treated as a
constant).</p>
<p>This clearly shows the trade-off: We want a model with a low
<strong><span class="math inline">\(RSS\)</span></strong> (good fit) and
a low <strong><span class="math inline">\(d\)</span></strong> (low
complexity). These two goals are in direct competition.</p>
<p><strong>Mallow’s <span class="math inline">\(C_p\)</span>:</strong>
The slide notes that <span class="math inline">\(C_p\)</span> is
equivalent to AIC for linear regression. The <span
class="math inline">\(C_p\)</span> formula is <span
class="math inline">\(C_p = \frac{1}{n}(RSS +
2d\hat{\sigma}^2_{full})\)</span>, where <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> is the error
variance estimated from the <em>full</em> model. Since <span
class="math inline">\(n\)</span> and <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> are constants,
minimizing <span class="math inline">\(C_p\)</span> is mathematically
identical to minimizing <span class="math inline">\(RSS +
2d\hat{\sigma}^2_{full}\)</span>, which is the same logic as AIC.</p>
<p>Here is a detailed breakdown of the mathematical formulas and
concepts from your slides.</p>
<h2 id="the-core-problem-training-error-vs.-test-error">The Core
Problem: Training Error vs. Test Error</h2>
<p>The central theme of these slides is finding the “best” model. The
problem is that a model with more predictors (more complex) will
<em>always</em> fit the data it was trained on better. This is a
trap.</p>
<ul>
<li><strong>Training Error:</strong> How well the model fits the data we
used to build it. <strong><span class="math inline">\(R^2\)</span> and
<span class="math inline">\(RSS\)</span> measure this.</strong></li>
<li><strong>Test Error:</strong> How well the model predicts new, unseen
data. This is what we <em>actually</em> care about. A model that is too
complex (e.g., has 10 predictors when only 3 are useful) will have low
training error but very high test error. This is called
<strong>overfitting</strong>.</li>
</ul>
<p>The goal is to choose a model that has the lowest <em>test
error</em>. The metrics below (Adjusted <span
class="math inline">\(R^2\)</span>, AIC, BIC) are all attempts to
<em>estimate</em> this test error without having to actually collect new
data. They do this by adding a <strong>penalty</strong> for
complexity.</p>
<h2 id="basic-metrics-measures-of-fit-1">Basic Metrics (Measures of
Fit)</h2>
<p>These formulas from slide 13 describe how well a model fits the
<em>training data</em>.</p>
<h3 id="residue-error">Residue (Error)</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\epsilon}_i = y_i - \hat{y}_i = y_i -
\hat{\beta}_0 - \sum_{j=1}^{p} \hat{\beta}_j x_{ij}\)</span></li>
<li><strong>Concept:</strong> This is the most basic building block.
It’s the difference between the <em>actual</em> observed value (<span
class="math inline">\(y_i\)</span>) and the value your model
<em>predicted</em> (<span class="math inline">\(\hat{y}_i\)</span>). It
is the “error” for a single data point.</li>
</ul>
<h3 id="residual-sum-of-squares-rss">Residual Sum of Squares (RSS)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(RSS =
\sum_{i=1}^{n} \hat{\epsilon}_i^2\)</span></li>
<li><strong>Concept:</strong> This is the overall measure of model
error. You square all the individual errors (residues) to make them
positive and then add them all up.</li>
<li><strong>Goal:</strong> The entire process of linear regression
(called “Ordinary Least Squares”) is designed to find the <span
class="math inline">\(\hat{\beta}\)</span> coefficients that make this
<strong>RSS value as small as possible</strong>.</li>
<li><strong>The Flaw:</strong> <span class="math inline">\(RSS\)</span>
will <em>always</em> decrease (or stay the same) as you add more
predictors (<span class="math inline">\(p\)</span>). A model with all 10
predictors will have a lower <span class="math inline">\(RSS\)</span>
than a model with 9, even if that 10th predictor is useless. Therefore,
<span class="math inline">\(RSS\)</span> is useless for choosing
<em>between</em> models of different sizes.</li>
</ul>
<h3 id="r-squared-r2-1">R-squared (<span
class="math inline">\(R^2\)</span>)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(R^2 = 1 -
\frac{SS_{error}}{SS_{total}} = 1 - \frac{RSS}{\sum_{i=1}^{n} (y_i -
\bar{y})^2}\)</span></li>
<li><strong>Concept:</strong> This metric reframes <span
class="math inline">\(RSS\)</span> into a more interpretable percentage.
<ul>
<li><span class="math inline">\(SS_{total}\)</span> (the denominator)
represents the <em>total variance</em> of the data. It’s the error you
would get if your “model” was just guessing the average value (<span
class="math inline">\(\bar{y}\)</span>) for every single
observation.</li>
<li><span class="math inline">\(SS_{error}\)</span> (the <span
class="math inline">\(RSS\)</span>) is the error <em>after</em> using
your model.</li>
<li><span class="math inline">\(R^2\)</span> is the “proportion of total
variance explained by the model.” An <span
class="math inline">\(R^2\)</span> of 0.75 means your model can explain
75% of the variation in the response variable.</li>
</ul></li>
<li><strong>The Flaw:</strong> Just like <span
class="math inline">\(RSS\)</span>, <span
class="math inline">\(R^2\)</span> will <em>always</em> increase (or
stay the same) as you add more predictors. This is visually confirmed in
Figure 6.1, where the red line for <span
class="math inline">\(R^2\)</span> only goes up. It will always pick the
most complex model.</li>
</ul>
<h2 id="advanced-metrics-for-model-selection">Advanced Metrics (For
Model Selection)</h2>
<p>These metrics “fix” the flaw of <span
class="math inline">\(R^2\)</span> by including a penalty for the number
of predictors.</p>
<h3 id="adjusted-r2-1">Adjusted <span
class="math inline">\(R^2\)</span></h3>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
  \text{Adjusted } R^2 = 1 - \frac{RSS / (n - p - 1)}{SS_{total} / (n -
1)}
  \]</span></li>
<li><strong>Mathematical Concept:</strong> This formula replaces the
“Sum of Squares” (<span class="math inline">\(SS\)</span>) with “Mean
Squares” (<span class="math inline">\(MS\)</span>).
<ul>
<li><span class="math inline">\(MS_{error} =
\frac{RSS}{n-p-1}\)</span></li>
<li><span class="math inline">\(MS_{total} =
\frac{SS_{total}}{n-1}\)</span></li>
</ul></li>
<li><strong>The “Penalty” Explained:</strong> The penalty is
<strong>degrees of freedom</strong>.
<ul>
<li><span class="math inline">\(n\)</span> = number of data points.</li>
<li><span class="math inline">\(p\)</span> = number of predictors.</li>
<li>The term <span class="math inline">\(n-p-1\)</span> is the degrees
of freedom for the residuals. You start with <span
class="math inline">\(n\)</span> data points, but you “use up” one
degree of freedom to estimate the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) and <span
class="math inline">\(p\)</span> more to estimate the <span
class="math inline">\(p\)</span> slopes.</li>
</ul></li>
<li><strong>How it Works:</strong>
<ol type="1">
<li>When you add a new predictor (increase <span
class="math inline">\(p\)</span>), <span
class="math inline">\(RSS\)</span> goes down, which makes the numerator
(<span class="math inline">\(MS_{error}\)</span>) smaller.</li>
<li>…But, increasing <span class="math inline">\(p\)</span>
<em>also</em> decreases the denominator (<span
class="math inline">\(n-p-1\)</span>), which makes the numerator (<span
class="math inline">\(MS_{error}\)</span>) <em>larger</em>.</li>
</ol>
<ul>
<li>This creates a “tug-of-war.” If the new predictor is
<strong>useful</strong>, it will drop <span
class="math inline">\(RSS\)</span> a lot, and Adjusted <span
class="math inline">\(R^2\)</span> will <strong>increase</strong>. If
the new predictor is <strong>useless</strong>, <span
class="math inline">\(RSS\)</span> will barely change, and the penalty
from decreasing the denominator will win, causing Adjusted <span
class="math inline">\(R^2\)</span> to <strong>decrease</strong>.</li>
</ul></li>
<li><strong>Goal:</strong> You select the model with the
<strong>highest</strong> Adjusted <span
class="math inline">\(R^2\)</span>.</li>
</ul>
<h3 id="akaike-information-criterion-aic-1">Akaike Information Criterion
(AIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Concept Breakdown:</strong>
<ul>
<li><span class="math inline">\(\ell(\hat{\theta})\)</span>: This is the
<strong>Maximized Likelihood Function</strong>.
<ul>
<li>The <strong>Likelihood Function</strong> <span
class="math inline">\(\ell(\theta)\)</span> asks: “Given a set of model
parameters <span class="math inline">\(\theta\)</span>, how probable is
the data we observed?”</li>
<li>The <strong>Maximum Likelihood Estimate (MLE)</strong> <span
class="math inline">\(\hat{\theta}\)</span> is the specific set of
parameters (the <span class="math inline">\(\hat{\beta}\)</span>’s) that
<em>maximizes</em> this probability.</li>
</ul></li>
<li><span class="math inline">\(\log \ell(\hat{\theta})\)</span>: The
<strong>log-likelihood</strong>. This is just a number that represents
the <em>best possible fit</em> the model can achieve for the data. A
higher number is a better fit.</li>
<li><span class="math inline">\(-2 \log \ell(\hat{\theta})\)</span>:
This is the <strong>Deviance</strong>. Since a higher log-likelihood is
better, a <em>lower</em> deviance is better. This term measures
<strong>poorness-of-fit</strong>.</li>
<li><span class="math inline">\(d\)</span>: The number of parameters
estimated by the model. (e.g., <span class="math inline">\(p\)</span>
predictors + 1 intercept).</li>
<li><span class="math inline">\(2d\)</span>: This is the <strong>Penalty
Term</strong>.</li>
</ul></li>
<li><strong>How it Works:</strong> <span class="math inline">\(AIC =
(\text{Poorness-of-Fit}) + (\text{Complexity Penalty})\)</span>. As you
add predictors, the fit gets better (the deviance term goes down), but
the penalty term (<span class="math inline">\(2d\)</span>) goes up.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> AIC.</li>
</ul>
<h3 id="bayesian-information-criterion-bic-1">Bayesian Information
Criterion (BIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(BIC =
-2 \log \ell(\hat{\theta}) + \log(n)d\)</span></li>
<li><strong>Concept:</strong> This is mathematically identical to AIC,
but the penalty term is different.
<ul>
<li><strong>AIC Penalty:</strong> <span
class="math inline">\(2d\)</span></li>
<li><strong>BIC Penalty:</strong> <span
class="math inline">\(\log(n)d\)</span></li>
</ul></li>
<li><strong>Comparison:</strong>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations
in your dataset.</li>
<li>As long as your dataset has 8 or more observations (<span
class="math inline">\(n \ge 8\)</span>), <span
class="math inline">\(\log(n)\)</span> will be greater than 2.</li>
<li>This means <strong>BIC applies a much harsher penalty for
complexity</strong> than AIC.</li>
</ul></li>
<li><strong>Consequence:</strong> BIC will tend to choose
<em>simpler</em> models (fewer predictors) than AIC.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> BIC.</li>
</ul>
<h2 id="the-deeper-theory-why-aic-works-1">The Deeper Theory: Why AIC
Works</h2>
<p>Slide 27 (“Understanding AIC”) gives the deep mathematical
justification.</p>
<ul>
<li><strong>Goal:</strong> We have a <em>true</em>, unknown process
<span class="math inline">\(p\)</span> that generates our data. We are
creating a model <span class="math inline">\(\hat{p}_j\)</span>. We want
our model to be as “close” to the truth as possible.</li>
<li><strong>Kullback-Leibler (K-L) Distance:</strong> This is a function
<span class="math inline">\(K(p, \hat{p}_j)\)</span> that measures the
“information lost” when you use your model <span
class="math inline">\(\hat{p}_j\)</span> to approximate the truth <span
class="math inline">\(p\)</span>. You want to <em>minimize</em> this
distance.</li>
<li><strong>The Math:</strong>
<ol type="1">
<li><span class="math inline">\(K(p, \hat{p}_j) = \int p(y) \log \left(
\frac{p(y)}{\hat{p}_j(y)} \right) dy\)</span></li>
<li>This splits into: <span class="math inline">\(K(p, \hat{p}_j) =
\underbrace{\int p(y) \log(p(y)) dy}_{\text{Constant}} -
\underbrace{\int p(y) \log(\hat{p}_j(y)) dy}_{\text{This is what we need
to maximize}}\)</span></li>
</ol></li>
<li><strong>The Problem:</strong> We can’t calculate that second term
because it requires knowing the <em>true</em> function <span
class="math inline">\(p\)</span>.</li>
<li><strong>Akaike’s Insight:</strong> Akaike proved that the
log-likelihood we <em>can</em> calculate, <span
class="math inline">\(\log \ell(\hat{\theta})\)</span>, is a
<em>biased</em> estimator of that target. He also proved that the bias
is approximately <span class="math inline">\(-d\)</span>.</li>
<li><strong>The Solution:</strong> An <em>unbiased</em> estimate of the
target is <span class="math inline">\(\log \ell(\hat{\theta}) -
d\)</span>.</li>
<li><strong>Final Step:</strong> For historical and statistical reasons,
he multiplied this by <span class="math inline">\(-2\)</span> to create
the final AIC formula.</li>
<li><strong>Conclusion:</strong> AIC is not just a random formula. It is
a carefully derived estimate of how much information your model loses
compared to the “truth” (i.e., its expected performance on new
data).</li>
</ul>
<h2 id="aicbic-for-linear-regression-1">AIC/BIC for Linear
Regression</h2>
<p>Slide 26 shows how these general formulas simplify for linear
regression (assuming normal, Gaussian errors).</p>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Linear Regression Formula:</strong> <span
class="math inline">\(AIC = \frac{1}{n\hat{\sigma}^2}(RSS +
2d\hat{\sigma}^2)\)</span></li>
</ul>
<p><strong>Key Insight:</strong> For linear regression, the
“poorness-of-fit” term (<span class="math inline">\(-2 \log
\ell(\hat{\theta})\)</span>) is <em>directly proportional to</em> the
<span class="math inline">\(RSS\)</span>.</p>
<p>This makes it much easier to understand. You can just think of the
formulas as: * <strong>AIC <span class="math inline">\(\approx\)</span>
<span class="math inline">\(RSS + 2d\hat{\sigma}^2\)</span></strong> *
<strong>BIC <span class="math inline">\(\approx\)</span> <span
class="math inline">\(RSS + \log(n)d\hat{\sigma}^2\)</span></strong></p>
<p>(Here <span class="math inline">\(\hat{\sigma}^2\)</span> is an
estimate of the error variance, which can often be treated as a
constant).</p>
<p>This clearly shows the trade-off: We want a model with a low
<strong><span class="math inline">\(RSS\)</span></strong> (good fit) and
a low <strong><span class="math inline">\(d\)</span></strong> (low
complexity). These two goals are in direct competition.</p>
<p><strong>Mallow’s <span class="math inline">\(C_p\)</span>:</strong>
The slide notes that <span class="math inline">\(C_p\)</span> is
equivalent to AIC for linear regression. The <span
class="math inline">\(C_p\)</span> formula is <span
class="math inline">\(C_p = \frac{1}{n}(RSS +
2d\hat{\sigma}^2_{full})\)</span>, where <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> is the error
variance estimated from the <em>full</em> model. Since <span
class="math inline">\(n\)</span> and <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> are constants,
minimizing <span class="math inline">\(C_p\)</span> is mathematically
identical to minimizing <span class="math inline">\(RSS +
2d\hat{\sigma}^2_{full}\)</span>, which is the same logic as AIC.</p>
<h1 id="variable-selection">3. Variable Selection</h1>
<h2 id="core-concept-the-problem-of-variable-selection">Core Concept:
The Problem of Variable Selection</h2>
<p>In regression, we want to model a response variable <span
class="math inline">\(Y\)</span> using a set of <span
class="math inline">\(p\)</span> predictor variables <span
class="math inline">\(X_1, X_2, ..., X_p\)</span>.</p>
<ul>
<li><p><strong>The “Kitchen Sink” Problem:</strong> A common temptation
is to include all available predictors in the model: <span
class="math display">\[Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... +
\beta_pX_p + \epsilon\]</span> This often leads to
<strong>overfitting</strong>. The model may fit the training data well
but will perform poorly on new, unseen data. It’s also hard to interpret
a model with dozens of predictors.</p></li>
<li><p><strong>The Solution: Subset Selection.</strong> The goal is to
find a smaller subset of the predictors that builds a model that is:</p>
<ol type="1">
<li><strong>Accurate:</strong> Has low prediction error.</li>
<li><strong>Parsimonious:</strong> Uses the fewest predictors
necessary.</li>
<li><strong>Interpretable:</strong> Is simple enough for a human to
understand.</li>
</ol></li>
</ul>
<p>Your slides present two main methods to achieve this: <strong>Best
Subset Selection</strong> and <strong>Forward Stepwise
Selection</strong>.</p>
<h2 id="method-1-best-subset-selection-bss">Method 1: Best Subset
Selection (BSS)</h2>
<p>This is the “brute force” approach. It considers <em>every single
possible model</em>.</p>
<h3 id="conceptual-algorithm">Conceptual Algorithm</h3>
<ol type="1">
<li>Fit all models with <span class="math inline">\(k=1\)</span>
predictor (there are <span class="math inline">\(p\)</span> of these).
Find the best one (lowest RSS) and call it <span
class="math inline">\(M_1\)</span>.</li>
<li>Fit all models with <span class="math inline">\(k=2\)</span>
predictors (there are <span class="math inline">\(\binom{p}{2}\)</span>
of these). Find the best one and call it <span
class="math inline">\(M_2\)</span>.</li>
<li>…</li>
<li>Fit the one model with <span class="math inline">\(k=p\)</span>
predictors (the full model), <span
class="math inline">\(M_p\)</span>.</li>
<li>You now have a list of <span class="math inline">\(p\)</span> “best”
models: <span class="math inline">\(M_1, M_2, ..., M_p\)</span>.</li>
<li>Use a selection criterion (like <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>BIC</strong>,
<strong>AIC</strong>, or <strong><span
class="math inline">\(C_p\)</span></strong>) to choose the single best
model from this list.</li>
</ol>
<h3
id="mathematical-computational-cost-from-slide-225641.png">Mathematical
&amp; Computational Cost (from slide <code>225641.png</code>)</h3>
<ul>
<li>For each predictor, there are two possibilities: it’s either
<strong>IN</strong> the model or <strong>OUT</strong>.</li>
<li>With <span class="math inline">\(p\)</span> predictors, the total
number of models to test is <span class="math inline">\(2 \times 2
\times ... \times 2\)</span> (<span class="math inline">\(p\)</span>
times).</li>
<li><strong>Total Models = <span
class="math inline">\(2^p\)</span></strong></li>
<li>This is a “combinatorial explosion.” As the slide notes, if <span
class="math inline">\(p=20\)</span>, <span class="math inline">\(2^{20}
= 1,048,576\)</span> models. This is computationally infeasible for
large <span class="math inline">\(p\)</span>.</li>
</ul>
<h2 id="method-2-forward-stepwise-selection-fss">Method 2: Forward
Stepwise Selection (FSS)</h2>
<p>This is a “greedy” algorithm. It’s an efficient alternative to BSS
that does <em>not</em> test every model.</p>
<h3
id="conceptual-algorithm-from-slides-225645.png-225648.png">Conceptual
Algorithm (from slides <code>225645.png</code> &amp;
<code>225648.png</code>)</h3>
<ul>
<li><p><strong>Step 1:</strong> Start with the <strong>null
model</strong>, <span class="math inline">\(M_0\)</span>, which has no
predictors. <span class="math display">\[M_0: Y = \beta_0 +
\epsilon\]</span> The prediction is just the sample mean of <span
class="math inline">\(Y\)</span>.</p></li>
<li><p><strong>Step 2 (Iterative):</strong></p>
<ul>
<li><strong>For <span class="math inline">\(k=0\)</span> (to get <span
class="math inline">\(M_1\)</span>):</strong> Fit all <span
class="math inline">\(p\)</span> models that add <em>one</em> predictor
to <span class="math inline">\(M_0\)</span>. Choose the best one (lowest
<strong>RSS</strong> or highest <strong><span
class="math inline">\(R^2\)</span></strong>). This is <span
class="math inline">\(M_1\)</span>. Let’s say it contains <span
class="math inline">\(X_1\)</span>.</li>
<li><strong>For <span class="math inline">\(k=1\)</span> (to get <span
class="math inline">\(M_2\)</span>):</strong> <em>Keep</em> <span
class="math inline">\(X_1\)</span> in the model. Fit all <span
class="math inline">\(p-1\)</span> models that add <em>one more</em>
predictor to <span class="math inline">\(M_1\)</span> (e.g., <span
class="math inline">\(M_1+X_2\)</span>, <span
class="math inline">\(M_1+X_3\)</span>, …). Choose the best of these.
This is <span class="math inline">\(M_2\)</span>.</li>
<li><strong>Repeat:</strong> Continue this process, adding one variable
at a time, until all <span class="math inline">\(p\)</span> predictors
are in the model <span class="math inline">\(M_p\)</span>.</li>
</ul></li>
<li><p><strong>Step 3:</strong> You now have a sequence of <span
class="math inline">\(p+1\)</span> models: <span
class="math inline">\(M_0, M_1, ..., M_p\)</span>. Choose the single
best model from this sequence using <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>AIC</strong>,
<strong>BIC</strong>, or <strong><span
class="math inline">\(C_p\)</span></strong>.</p></li>
</ul>
<h3
id="mathematical-computational-cost-from-slide-225651.png">Mathematical
&amp; Computational Cost (from slide <code>225651.png</code>)</h3>
<ul>
<li>To find <span class="math inline">\(M_1\)</span>, you fit <span
class="math inline">\(p\)</span> models.</li>
<li>To find <span class="math inline">\(M_2\)</span>, you fit <span
class="math inline">\(p-1\)</span> models.</li>
<li>To find <span class="math inline">\(M_p\)</span>, you fit <span
class="math inline">\(1\)</span> model.</li>
<li>The null model <span class="math inline">\(M_0\)</span> is 1
model.</li>
<li><strong>Total Models = <span class="math inline">\(1 +
\sum_{k=0}^{p-1} (p-k) = 1 + p + (p-1) + ... + 1 = 1 +
\frac{p(p+1)}{2}\)</span></strong></li>
<li>As the slide notes, if <span class="math inline">\(p=20\)</span>,
this is only <span class="math inline">\(1 + 20(21)/2 = 211\)</span>
models. This is vastly more efficient than BSS.</li>
<li><strong>Key weakness:</strong> The method is “greedy.” If it adds
<span class="math inline">\(X_1\)</span> in Step 1, it can
<em>never</em> be removed. It’s possible the true best 2-variable model
is <span class="math inline">\((X_2, X_3)\)</span>, but if FSS chose
<span class="math inline">\(X_1\)</span> as the best 1-variable model,
it will never find <span class="math inline">\((X_2, X_3)\)</span>.</li>
</ul>
<h2 id="how-to-choose-the-best-model-the-criteria">4. How to Choose the
“Best” Model: The Criteria</h2>
<p>You can’t use <strong>RSS</strong> or <strong><span
class="math inline">\(R^2\)</span></strong> to compare models with
<em>different numbers of predictors</em> (<span
class="math inline">\(k\)</span>). This is because RSS always decreases
(and <span class="math inline">\(R^2\)</span> always increases) as you
add more variables. You <em>must</em> use a criterion that penalizes
complexity.</p>
<ul>
<li><p><strong>RSS (Residual Sum of Squares):</strong> Goal is to
<strong>minimize</strong>. <span class="math display">\[RSS =
\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span> Good for comparing models
<em>of the same size <span
class="math inline">\(k\)</span></em>.</p></li>
<li><p><strong>Adjusted R-squared (<span class="math inline">\(Adj.
R^2\)</span>):</strong> Goal is to <strong>maximize</strong>. <span
class="math display">\[Adj. R^2 = 1 -
\frac{(1-R^2)(n-1)}{n-p-1}\]</span> This “adjusts” <span
class="math inline">\(R^2\)</span> by adding a penalty for having more
predictors (<span class="math inline">\(p\)</span>). Adding a useless
predictor will make <span class="math inline">\(Adj. R^2\)</span> go
down.</p></li>
<li><p><strong>Mallow’s <span
class="math inline">\(C_p\)</span>:</strong> Goal is to
<strong>minimize</strong>. <span class="math display">\[C_p \approx
\frac{1}{n}(RSS + 2p\hat{\sigma}^2)\]</span> Here, <span
class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the
error variance from the <em>full model</em> (with all <span
class="math inline">\(p\)</span> predictors). A good model will have
<span class="math inline">\(C_p \approx p\)</span>.</p></li>
<li><p><strong>AIC (Akaike Information Criterion) &amp; BIC (Bayesian
Information Criterion):</strong> Goal is to <strong>minimize</strong>.
<span class="math display">\[AIC = 2p - 2\ln(\hat{L})\]</span> <span
class="math display">\[BIC = p\ln(n) - 2\ln(\hat{L})\]</span> Here,
<span class="math inline">\(\hat{L}\)</span> is the maximized likelihood
of the model. You don’t need to calculate this by hand; software
provides it.</p>
<ul>
<li><strong>Key difference:</strong> BIC’s penalty for <span
class="math inline">\(p\)</span> is <span
class="math inline">\(p\ln(n)\)</span>, while AIC’s is <span
class="math inline">\(2p\)</span>. Since <span
class="math inline">\(\ln(n)\)</span> is almost always <span
class="math inline">\(&gt; 2\)</span> (for <span
class="math inline">\(n&gt;7\)</span>), <strong>BIC applies a much
heavier penalty for complexity</strong>.</li>
<li>This means <strong>BIC tends to choose smaller, more parsimonious
models</strong> than AIC or <span class="math inline">\(Adj.
R^2\)</span>.</li>
</ul></li>
</ul>
<h2 id="python-code-analysis-slide-225546.jpg">5. Python Code Analysis
(Slide <code>225546.jpg</code>)</h2>
<p>This slide shows the Python code for <strong>Best Subset
Selection</strong> (BSS).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations <span class="comment"># &lt;-- This is the BSS engine</span></span><br></pre></td></tr></table></figure>
<h3 id="block-1-load-the-credit-dataset">Block 1: Load the Credit
dataset</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Load the Credit dataset</span></span><br><span class="line">Credit = pd.read_csv(<span class="string">&#x27;Credit.csv&#x27;</span>)</span><br><span class="line">Credit[<span class="string">&#x27;ID&#x27;</span>] = Credit[<span class="string">&#x27;ID&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">(num_samples, num_predictors) = Credit.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert categorical text data to numerical (dummy variables)</span></span><br><span class="line">Credit[<span class="string">&#x27;Gender&#x27;</span>] = Credit[<span class="string">&#x27;Gender&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Male&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Female&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Student&#x27;</span>] = Credit[<span class="string">&#x27;Student&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Yes&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;No&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Married&#x27;</span>] = Credit[<span class="string">&#x27;Married&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Yes&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;No&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Ethnicity&#x27;</span>] = Credit[<span class="string">&#x27;Ethnicity&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Asian&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Caucasian&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;African American&#x27;</span>: <span class="number">0</span>&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>pd.read_csv</code></strong>: Reads the data into a
<code>pandas</code> DataFrame.</li>
<li><strong><code>.map()</code></strong>: This is a crucial
preprocessing step. Regression models require numbers, not text like
‘Yes’ or ‘Male’. This line converts those strings into <code>1</code>s
and <code>0</code>s.</li>
</ul>
<h3 id="block-2-plot-scatterplot-matrix">Block 2: Plot scatterplot
matrix</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. Plot scatterplot matrix</span></span><br><span class="line">selected_columns = [<span class="string">&#x27;Balance&#x27;</span>, <span class="string">&#x27;Education&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Cards&#x27;</span>, <span class="string">&#x27;Rating&#x27;</span>, <span class="string">&#x27;Limit&#x27;</span>, <span class="string">&#x27;Income&#x27;</span>]</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&quot;ticks&quot;</span>)</span><br><span class="line">sns.pairplot(Credit[selected_columns], diag_kind=<span class="string">&#x27;kde&#x27;</span>)</span><br><span class="line">plt.suptitle(<span class="string">&#x27;Scatterplot Matrix&#x27;</span>, y=<span class="number">1.02</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>sns.pairplot</code></strong>: A powerful visualization
from the <code>seaborn</code> library. The resulting plot (right side of
the slide) is a grid.
<ul>
<li><strong>Diagonal plots (kde)</strong>: Show the distribution (Kernel
Density Estimate) of a single variable (e.g., ‘Balance’ is skewed
right).</li>
<li><strong>Off-diagonal plots (scatter)</strong>: Show the relationship
between two variables (e.g., ‘Limit’ and ‘Rating’ are almost perfectly
linear). This helps you visually spot potentially strong
predictors.</li>
</ul></li>
</ul>
<h3 id="block-3-best-subset-selection">Block 3: Best Subset
Selection</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Best Subset Selection</span></span><br><span class="line"><span class="comment"># (This code is incomplete on the slide, I&#x27;ll fill in the logic)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define target and predictors</span></span><br><span class="line">target = <span class="string">&#x27;Balance&#x27;</span></span><br><span class="line">predictors = [col <span class="keyword">for</span> col <span class="keyword">in</span> Credit.columns <span class="keyword">if</span> col != target] </span><br><span class="line">nvmax = <span class="number">10</span> <span class="comment"># Max number of predictors to test (up to 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize lists to store model statistics</span></span><br><span class="line">model_stats = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over number of predictors from 1 to nvmax</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, nvmax + <span class="number">1</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Generate all possible combinations of predictors of size k</span></span><br><span class="line">    <span class="comment"># This is the core of BSS</span></span><br><span class="line">    <span class="keyword">for</span> subset <span class="keyword">in</span> <span class="built_in">list</span>(combinations(predictors, k)):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the design matrix (X)</span></span><br><span class="line">        X_subset = Credit[<span class="built_in">list</span>(subset)]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Add a constant (intercept) term to the model</span></span><br><span class="line">        <span class="comment"># Y = B0 + B1*X1 -&gt; statsmodels needs B0 to be added manually</span></span><br><span class="line">        X_subset_const = sm.add_constant(X_subset)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the target variable (y)</span></span><br><span class="line">        y_target = Credit[target]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Fit the Ordinary Least Squares (OLS) model</span></span><br><span class="line">        model = sm.OLS(y_target, X_subset_const).fit()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate RSS</span></span><br><span class="line">        RSS = ((model.resid) ** <span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (The full code would also calculate R-squared, Adj. R-sq, BIC, etc. here)</span></span><br><span class="line">        <span class="comment"># model_stats.append(&#123;&#x27;k&#x27;: k, &#x27;subset&#x27;: subset, &#x27;RSS&#x27;: RSS, ...&#125;)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>for k in range(1, nvmax + 1)</code></strong>: This is
the <em>outer</em> loop that iterates from <span
class="math inline">\(k=1\)</span> (1 predictor) to <span
class="math inline">\(k=10\)</span> (10 predictors).</li>
<li><strong><code>list(combinations(predictors, k))</code></strong>:
This is the <em>inner</em> loop and the <strong>most important
line</strong>. The <code>itertools.combinations</code> function is a
highly efficient way to generate all unique subsets.
<ul>
<li>When <span class="math inline">\(k=1\)</span>, it returns
<code>[('Income',), ('Limit',), ('Rating',), ...]</code>.</li>
<li>When <span class="math inline">\(k=2\)</span>, it returns
<code>[('Income', 'Limit'), ('Income', 'Rating'), ('Limit', 'Rating'), ...]</code>.</li>
<li>This is what generates the <span class="math inline">\(2^p\)</span>
(or in this case, <span class="math inline">\(\sum_{k=1}^{10}
\binom{p}{k}\)</span>) models to test.</li>
</ul></li>
<li><strong><code>sm.add_constant(X_subset)</code></strong>: Your
regression equation is <span class="math inline">\(Y = \beta_0 +
\beta_1X_1\)</span>. The <span class="math inline">\(X_1\)</span> is
your <code>X_subset</code>. The <code>sm.add_constant</code> function
adds a column of <code>1</code>s to your data, which allows the
<code>statsmodels</code> library to estimate the <span
class="math inline">\(\beta_0\)</span> (intercept) term.</li>
<li><strong><code>sm.OLS(y_target, X_subset_const).fit()</code></strong>:
This fits the Ordinary Least Squares (OLS) model, which finds the <span
class="math inline">\(\beta\)</span> coefficients that <strong>minimize
the RSS</strong>.</li>
<li><strong><code>model.resid</code></strong>: This attribute of the
fitted model contains the residuals (<span class="math inline">\(e_i =
y_i - \hat{y}_i\)</span>) for each data point.</li>
<li><strong><code>((model.resid) ** 2).sum()</code></strong>: This line
is the direct code implementation of the formula <span
class="math inline">\(RSS = \sum e_i^2\)</span>.</li>
</ul>
<h2 id="synthesizing-the-results-the-plots">Synthesizing the Results
(The Plots)</h2>
<p>After running the BSS code, you get the data used in the plots and
the table.</p>
<ul>
<li><p><strong>Image <code>225550.png</code> (Adjusted
R-squared)</strong></p>
<ul>
<li><strong>Goal:</strong> Maximize.</li>
<li><strong>What it shows:</strong> The gray dots are <em>all</em> the
models tested for each <span class="math inline">\(k\)</span>. The red
line connects the single <em>best</em> model for each <span
class="math inline">\(k\)</span>.</li>
<li><strong>Conclusion:</strong> The plot shows a sharp “elbow.” The
<span class="math inline">\(Adj. R^2\)</span> increases dramatically up
to <span class="math inline">\(k=4\)</span>, then increases very slowly.
The maximum is around <span class="math inline">\(k=6\)</span> or <span
class="math inline">\(k=7\)</span>, but the gain after <span
class="math inline">\(k=4\)</span> is minimal.</li>
</ul></li>
<li><p><strong>Image <code>225554.png</code> (BIC)</strong></p>
<ul>
<li><strong>Goal:</strong> Minimize.</li>
<li><strong>What it shows:</strong> BIC heavily penalizes
complexity.</li>
<li><strong>Conclusion:</strong> The plot shows a very clear minimum.
The BIC value plummets from <span class="math inline">\(k=2\)</span> to
<span class="math inline">\(k=3\)</span> and hits its lowest point at
<strong><span class="math inline">\(k=4\)</span></strong>. After <span
class="math inline">\(k=4\)</span>, the penalty for adding more
variables is <em>larger</em> than the benefit in model fit, so the BIC
score starts to rise. This is a very strong vote for the 4-predictor
model.</li>
</ul></li>
<li><p><strong>Image <code>225635.png</code> (Mallow’s <span
class="math inline">\(C_p\)</span>)</strong></p>
<ul>
<li><strong>Goal:</strong> Minimize.</li>
<li><strong>What it shows:</strong> A very similar story to BIC.</li>
<li><strong>Conclusion:</strong> The <span
class="math inline">\(C_p\)</span> value drops significantly and hits
its minimum at <strong><span
class="math inline">\(k=4\)</span></strong>.</li>
</ul></li>
<li><p><strong>Image <code>225638.png</code> (Summary
Table)</strong></p>
<ul>
<li>This is the <strong>most important image</strong> for the final
conclusion. It summarizes the red line from all the plots.</li>
<li>Look at the row for <code>Num_Predictors = 4</code>. The predictors
are <strong>(Income, Limit, Cards, Student)</strong>.</li>
<li>Now look at the columns for <code>BIC</code> and <code>Cp</code>.
<ul>
<li><strong>BIC:</strong> <code>4841.615607</code>. This is the lowest
value in the entire <code>BIC</code> column (the value at <span
class="math inline">\(k=3\)</span> is <code>4865.352851</code>).</li>
<li><strong>Cp:</strong> <code>7.122228</code>. This is also the lowest
value in the <code>Cp</code> column.</li>
</ul></li>
<li>The <code>Adj_R_squared</code> at <span
class="math inline">\(k=4\)</span> is <code>0.953580</code>, which is
very close to its maximum of <code>~0.954</code> at <span
class="math inline">\(k=7-10\)</span>.</li>
</ul></li>
</ul>
<p><strong>Final Conclusion:</strong> All three “penalized” criteria
(Adjusted <span class="math inline">\(R^2\)</span>, BIC, and <span
class="math inline">\(C_p\)</span>) point to the same conclusion. While
<span class="math inline">\(Adj. R^2\)</span> is a bit ambiguous,
<strong>BIC and <span class="math inline">\(C_p\)</span> provide a clear
signal that the best, most parsimonious model is the 4-predictor model
using <code>Income</code>, <code>Limit</code>, <code>Cards</code>, and
<code>Student</code></strong>.</p>
<h1 id="subset-selection">4. Subset Selection</h1>
<h2 id="summary-of-subset-selection">Summary of Subset Selection</h2>
<p>These slides introduce <strong>subset selection</strong>, a process
in statistical learning used to identify the best subset of predictors
(variables) for a regression model. The goal is to find a model that has
low prediction error and avoids overfitting by excluding irrelevant
variables.</p>
<p>The slides cover two main “greedy” (stepwise) algorithms and the
criteria used to select the final best model.</p>
<h2 id="stepwise-selection-algorithms">Stepwise Selection
Algorithms</h2>
<p>Instead of testing all <span class="math inline">\(2^p\)</span>
possible models (which is “best subset selection” and computationally
unfeasible), stepwise methods build a single path of models.</p>
<h3 id="forward-stepwise-selection">Forward Stepwise Selection</h3>
<p>This is an <strong>additive</strong> (bottom-up) approach:</p>
<ol type="1">
<li><strong>Start</strong> with the null model (no predictors).</li>
<li><strong>Find</strong> the best 1-variable model (the one that gives
the lowest Residual Sum of Squares, or RSS).</li>
<li><strong>Add</strong> the single variable that, when added to the
current model, results in the <em>new</em> best model (lowest RSS).</li>
<li><strong>Repeat</strong> this process until all <span
class="math inline">\(p\)</span> predictors are in the model.</li>
<li>This generates a sequence of <span
class="math inline">\(p+1\)</span> models, from <span
class="math inline">\(\mathcal{M}_0\)</span> to <span
class="math inline">\(\mathcal{M}_p\)</span>.</li>
</ol>
<h3 id="backward-stepwise-selection">Backward Stepwise Selection</h3>
<p>This is a <strong>subtractive</strong> (top-down) approach:</p>
<ol type="1">
<li><strong>Start</strong> with the full model containing all <span
class="math inline">\(p\)</span> predictors.</li>
<li><strong>Find</strong> the best <span
class="math inline">\((p-1)\)</span>-variable model by <em>removing</em>
the single variable that results in the <em>lowest RSS</em> (or highest
<span class="math inline">\(R^2\)</span>). This variable is considered
the least significant.</li>
<li><strong>Remove</strong> the next variable that, when removed from
the current best model, gives the new best model.</li>
<li><strong>Repeat</strong> until only the null model remains.</li>
<li>This also generates a sequence of <span
class="math inline">\(p+1\)</span> models.</li>
</ol>
<h4 id="pros-and-cons-backward-selection">Pros and Cons (Backward
Selection)</h4>
<ul>
<li><strong>Pro:</strong> Computationally efficient compared to best
subset. It fits <span class="math inline">\(1 + \sum_{k=0}^{p-1}(p-k) =
\mathbf{1 + p(p+1)/2}\)</span> models, which is much less than <span
class="math inline">\(2^p\)</span>. (e.g., for <span
class="math inline">\(p=20\)</span>, it’s 211 models vs. &gt;1
million).</li>
<li><strong>Con:</strong> <strong>Cannot be used if <span
class="math inline">\(p &gt; n\)</span></strong> (more predictors than
observations), because the initial full model cannot be fit.</li>
<li><strong>Con (for both):</strong> These methods are
<strong>greedy</strong>. A variable added in forward selection is
<em>never removed</em>, and a variable removed in backward selection is
<em>never added back</em>. This means they are not guaranteed to find
the true best model.</li>
</ul>
<h2 id="choosing-the-final-best-model">Choosing the Final Best
Model</h2>
<p>Both forward and backward selection give you a set of candidate
models (e.g., the best 1-variable model, best 2-variable model, etc.).
You must then choose the <em>single best</em> one. The slides show two
main approaches:</p>
<h3 id="a.-direct-error-estimation">A. Direct Error Estimation</h3>
<p>Use a validation set or cross-validation (CV) to estimate the test
error for each model (e.g., the 1-variable, 2-variable… models).
<strong>Choose the model with the lowest estimated test
error.</strong></p>
<h3 id="b.-adjusted-metrics-penalizing-for-complexity">B. Adjusted
Metrics (Penalizing for Complexity)</h3>
<p>Standard RSS and <span class="math inline">\(R^2\)</span> will always
improve as you add variables, leading to overfitting. Instead, use
metrics that <em>penalize</em> the model for having too many
predictors.</p>
<ul>
<li><p><strong>Mallows’ <span
class="math inline">\(C_p\)</span>:</strong> An estimate of test Mean
Squared Error (MSE). <span class="math display">\[C_p = \frac{1}{n} (RSS
+ 2d\hat{\sigma}^2)\]</span> (where <span
class="math inline">\(d\)</span> is the number of predictors, and <span
class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the
error variance). <strong>You want to find the model with the
<em>minimum</em> <span
class="math inline">\(C_p\)</span>.</strong></p></li>
<li><p><strong>BIC (Bayesian Information Criterion):</strong> <span
class="math display">\[BIC = \frac{1}{n} (RSS +
\log(n)d\hat{\sigma}^2)\]</span> BIC’s penalty <span
class="math inline">\(\log(n)\)</span> is stronger than <span
class="math inline">\(C_p\)</span>’s (or AIC’s) penalty of <span
class="math inline">\(2\)</span>, so it tends to select <em>smaller</em>
(more parsimonious) models. <strong>You want to find the model with the
<em>minimum</em> BIC.</strong></p></li>
<li><p><strong>Adjusted <span
class="math inline">\(R^2\)</span>:</strong> <span
class="math display">\[R^2_{adj} = 1 -
\frac{RSS/(n-d-1)}{TSS/(n-1)}\]</span> (where <span
class="math inline">\(TSS\)</span> is the Total Sum of Squares). Unlike
<span class="math inline">\(R^2\)</span>, this metric can decrease if
adding a variable doesn’t help enough. <strong>You want to find the
model with the <em>maximum</em> Adjusted <span
class="math inline">\(R^2\)</span>.</strong></p></li>
</ul>
<h2 id="python-code-understanding">Python Code Understanding</h2>
<p>The slides use the <code>regsubsets()</code> function from the
<code>leaps</code> package in <strong>R</strong>.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># R Code from slides</span></span><br><span class="line">library<span class="punctuation">(</span>leaps<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Forward Selection</span></span><br><span class="line">regfit.fwd <span class="operator">&lt;-</span> regsubsets<span class="punctuation">(</span>Balance<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Credit<span class="punctuation">,</span> method<span class="operator">=</span><span class="string">&quot;forward&quot;</span><span class="punctuation">,</span> nvmax<span class="operator">=</span><span class="number">11</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Backward Selection</span></span><br><span class="line">regfit.bwd <span class="operator">&lt;-</span> regsubsets<span class="punctuation">(</span>Balance<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Credit<span class="punctuation">,</span> method<span class="operator">=</span><span class="string">&quot;backward&quot;</span><span class="punctuation">,</span> nvmax<span class="operator">=</span><span class="number">11</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>In <strong>Python</strong>, the standard tool for this is
<code>SequentialFeatureSelector</code> from
<strong><code>scikit-learn</code></strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SequentialFeatureSelector</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Credit&#x27; is a pandas DataFrame with &#x27;Balance&#x27; as the target</span></span><br><span class="line">X = Credit.drop(<span class="string">&#x27;Balance&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = Credit[<span class="string">&#x27;Balance&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the linear regression estimator</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Forward Selection ---</span></span><br><span class="line"><span class="comment"># direction=&#x27;forward&#x27; starts with 0 features and adds them</span></span><br><span class="line"><span class="comment"># To get the best 4-variable model, for example:</span></span><br><span class="line">sfs_forward = SequentialFeatureSelector(</span><br><span class="line">    model,</span><br><span class="line">    n_features_to_select=<span class="number">4</span>,</span><br><span class="line">    direction=<span class="string">&#x27;forward&#x27;</span>,</span><br><span class="line">    cv=<span class="literal">None</span> <span class="comment"># Or use cross-validation, e.g., cv=10</span></span><br><span class="line">)</span><br><span class="line">sfs_forward.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Forward selection best 4 features:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(sfs_forward.get_feature_names_out())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Backward Selection ---</span></span><br><span class="line"><span class="comment"># direction=&#x27;backward&#x27; starts with all features and removes them</span></span><br><span class="line">sfs_backward = SequentialFeatureSelector(</span><br><span class="line">    model,</span><br><span class="line">    n_features_to_select=<span class="number">4</span>,</span><br><span class="line">    direction=<span class="string">&#x27;backward&#x27;</span>,</span><br><span class="line">    cv=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line">sfs_backward.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nBackward selection best 4 features:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(sfs_backward.get_feature_names_out())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: To replicate the plots, you would loop this process,</span></span><br><span class="line"><span class="comment"># changing &#x27;n_features_to_select&#x27; from 1 to p,</span></span><br><span class="line"><span class="comment"># record the model scores (e.g., RSS, AIC, BIC) at each step,</span></span><br><span class="line"><span class="comment"># and then plot the results.</span></span><br></pre></td></tr></table></figure>
<h2 id="important-images">Important Images</h2>
<ol type="1">
<li><p><strong>Slide <code>...230014.png</code> (Forward Selection
Plots) &amp; <code>...230036.png</code> (Backward Selection
Plots):</strong></p>
<ul>
<li><strong>What they are:</strong> These <span class="math inline">\(2
\times 2\)</span> plot grids are the most important visuals. They show
<strong>Residual Sum of Squares (RSS)</strong>, <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>BIC</strong>, and
<strong>Mallows’ <span class="math inline">\(C_p\)</span></strong>
plotted against the <em>Number of Variables</em>.</li>
<li><strong>Why they’re important:</strong> They are the
<strong>decision-making tool</strong>. You use these plots to choose the
best model.
<ul>
<li>You look for the “elbow” or <strong>minimum</strong> value for BIC
and <span class="math inline">\(C_p\)</span>.</li>
<li>You look for the “peak” or <strong>maximum</strong> value for
Adjusted <span class="math inline">\(R^2\)</span>.</li>
<li>(RSS is not used for selection as it always decreases).</li>
</ul></li>
</ul></li>
<li><p><strong>Slide <code>...230040.png</code> (Find the best
model):</strong></p>
<ul>
<li><strong>What it is:</strong> This slide shows a close-up of the
<span class="math inline">\(C_p\)</span>, BIC, and Adjusted <span
class="math inline">\(R^2\)</span> plots, with the “best” model (the
min/max) marked with a blue ‘x’.</li>
<li><strong>Why it’s important:</strong> It explicitly states the
selection criteria. The text highlights that BIC suggests a 4-variable
model, while the other two are “rather flat” after 4, making the choice
less obvious but pointing to a simple model.</li>
</ul></li>
<li><p><strong>Slide <code>...230045.png</code> (BIC vs. Validation
vs. CV):</strong></p>
<ul>
<li><strong>What it is:</strong> This shows three plots for selecting
the best model using different criteria: BIC, Validation Set Error, and
Cross-Validation Error.</li>
<li><strong>Why it’s important:</strong> It shows that <strong>different
selection criteria can lead to different “best” models</strong>. Here,
BIC (a mathematical adjustment) picks a 4-variable model, while
validation and CV (direct error estimation) both pick a 6-variable
model.</li>
</ul></li>
</ol>
<p>The slides use the <code>Credit</code> dataset to demonstrate two key
tasks: 1. <strong>Running</strong> different subset selection algorithms
(forward, backward, best). 2. <strong>Using</strong> various statistical
metrics (BIC, <span class="math inline">\(C_p\)</span>, CV error) to
choose the single best model.</p>
<h2 id="comparing-selection-algorithms-the-path">Comparing Selection
Algorithms (The Path)</h2>
<p>This part of the example compares the <em>sequence</em> of models
selected by “Forward Stepwise” selection versus “Best Subset”
selection.</p>
<p><strong>Key Result (from Table 6.1):</strong></p>
<p>This table is the most important result for comparing the
algorithms.</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Variables</th>
<th style="text-align: left;">Best Subset</th>
<th style="text-align: left;">Forward Stepwise</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>one</strong></td>
<td style="text-align: left;"><code>rating</code></td>
<td style="text-align: left;"><code>rating</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>two</strong></td>
<td style="text-align: left;"><code>rating</code>,
<code>income</code></td>
<td style="text-align: left;"><code>rating</code>,
<code>income</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>three</strong></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>four</strong></td>
<td style="text-align: left;"><code>cards</code>, <code>income</code>,
<code>student</code>, <code>limit</code></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code>, <code>limit</code></td>
</tr>
</tbody>
</table>
<p><strong>Summary of this result:</strong></p>
<ul>
<li><strong>Identical for 1, 2, and 3 variables:</strong> Both methods
agree on the best one-variable model (<code>rating</code>), the best
two-variable model (<code>rating</code>, <code>income</code>), and the
best three-variable model (<code>rating</code>, <code>income</code>,
<code>student</code>).</li>
<li><strong>They Diverge at 4 variables:</strong>
<ul>
<li><strong>Forward selection</strong> is <em>greedy</em>. It started
with <code>rating</code>, <code>income</code>, <code>student</code> and
was “stuck” with them. It then added <code>limit</code>, as that was the
best variable to <em>add</em> to its existing 3-variable model.</li>
<li><strong>Best subset selection</strong> is <em>not</em> greedy. It
tests all possible 4-variable combinations. It discovered that the model
<code>cards</code>, <code>income</code>, <code>student</code>,
<code>limit</code> has a slightly lower RSS than the model forward
selection found.</li>
</ul></li>
<li><strong>Main Takeaway:</strong> This demonstrates the limitation of
a greedy algorithm. Forward selection missed the “true” best 4-variable
model because it was locked into its previous choices and couldn’t “swap
out” <code>rating</code> for <code>cards</code>.</li>
</ul>
<h2 id="choosing-the-single-best-model-the-destination">Choosing the
Single Best Model (The Destination)</h2>
<p>This is the most critical part of the analysis. After running a
selection algorithm (like forward, backward, or best subset), you get a
list of the “best” models for each size (best 1-variable, best
2-variable, etc.). Now you must decide: <strong>is the best model the
4-variable one, the 6-variable one, or another?</strong></p>
<p>The slides show several plots to help make this decision, all plotted
against the “Number of Predictors.”</p>
<p><strong>Summary of Plot Results:</strong></p>
<p>Here’s what each plot tells you:</p>
<ul>
<li><strong>Residual Sum of Squares (RSS)</strong> (e.g., in slide
<code>...230014.png</code>, top-left)
<ul>
<li><strong>What it shows:</strong> RSS <em>always</em> decreases as you
add more variables. It drops sharply until 4 variables, then flattens
out.</li>
<li><strong>Conclusion:</strong> This plot is <strong>not useful for
picking the best model</strong> because it will always pick the full
model, which is overfit. It’s only used to see the diminishing returns
of adding new variables.</li>
</ul></li>
<li><strong>Adjusted <span class="math inline">\(R^2\)</span></strong>
(e.g., in slide <code>...230040.png</code>, right)
<ul>
<li><strong>What it shows:</strong> This metric penalizes adding useless
variables. The plot rises quickly, then flattens, peaking at its
<strong>maximum value around 6 or 7 variables</strong>.</li>
<li><strong>Conclusion:</strong> This metric suggests a <strong>6 or
7-variable model</strong>.</li>
</ul></li>
<li><strong>Mallows’ <span class="math inline">\(C_p\)</span></strong>
(e.g., in slide <code>...230040.png</code>, left)
<ul>
<li><strong>What it shows:</strong> This is an estimate of test error.
We want the model with the <strong>minimum <span
class="math inline">\(C_p\)</span></strong>. The plot drops to a low
value at 4 variables and stays low, with its absolute minimum around
<strong>6 or 7 variables</strong>.</li>
<li><strong>Conclusion:</strong> This metric also suggests a <strong>6
or 7-variable model</strong>.</li>
</ul></li>
<li><strong>BIC (Bayesian Information Criterion)</strong> (e.g., in
slide <code>...230040.png</code>, center)
<ul>
<li><strong>What it shows:</strong> This is another estimate of test
error, but it has a <em>stronger penalty</em> for model complexity. The
plot shows a clear “U” shape, reaching its <strong>minimum value at 4
variables</strong> and then <em>increasing</em> afterward.</li>
<li><strong>Conclusion:</strong> This metric strongly suggests a
<strong>4-variable model</strong>.</li>
</ul></li>
<li><strong>Validation Set &amp; Cross-Validation (CV) Error</strong>
(Slide <code>...230045.png</code>)
<ul>
<li><strong>What it shows:</strong> These plots show the <em>direct</em>
estimate of test error (not a mathematical adjustment like BIC or <span
class="math inline">\(C_p\)</span>). Both the validation set error and
the 10-fold CV error show a “U” shape.</li>
<li><strong>Conclusion:</strong> Both methods reach their
<strong>minimum error at 6 variables</strong>. This is considered a very
reliable result.</li>
</ul></li>
</ul>
<h2 id="final-summary-of-results">Final Summary of Results</h2>
<p>The analysis of the <code>Credit</code> dataset reveals two strong
candidates for the “best” model, depending on your goal:</p>
<ol type="1">
<li><p><strong>The 6-Variable Model:</strong> This model is supported by
the <strong>Adjusted <span class="math inline">\(R^2\)</span></strong>,
<strong>Mallows’ <span class="math inline">\(C_p\)</span></strong>, and
(most importantly) the <strong>Validation Set</strong> and
<strong>10-fold Cross-Validation</strong> results. These metrics all
indicate that the 6-variable model has the <strong>lowest prediction
error</strong> on new data.</p></li>
<li><p><strong>The 4-Variable Model:</strong> This model is supported by
<strong>BIC</strong>. Because BIC penalizes complexity more heavily, it
selects a simpler (more <em>parsimonious</em>) model.</p></li>
</ol>
<p><strong>Overall Conclusion:</strong> If your primary goal is
<strong>maximum predictive accuracy</strong>, you should choose the
<strong>6-variable model</strong>. If your goal is a <strong>simpler,
more interpretable model</strong> that is still very good (and avoids
any risk of overfitting), the <strong>4-variable model</strong> is an
excellent choice.</p>
<h1
id="two-main-strategies-for-controlling-model-complexity-in-linear-regression">5.
Two main strategies for controlling model complexity in linear
regression</h1>
<p>This presentation covers two main strategies for controlling model
complexity in linear regression: <strong>Subset Selection</strong>
(choosing <em>which</em> variables to include) and <strong>Shrinkage
Methods</strong> (keeping all variables but <em>reducing the impact</em>
of their coefficients).</p>
<h2 id="subset-selection-1">Subset Selection</h2>
<p>This method involves selecting a subset of the <span
class="math inline">\(p\)</span> total predictors to use in the
model.</p>
<h3 id="key-concepts-formulas">Key Concepts &amp; Formulas</h3>
<ul>
<li><p><strong>The Model:</strong> The standard linear regression model
is represented in matrix form: <span class="math display">\[\mathbf{y} =
\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\]</span> The goal
of subset selection is to find a coefficient vector <span
class="math inline">\(\boldsymbol{\beta}\)</span> that is
<strong>sparse</strong>, meaning it has many zero entries.</p></li>
<li><p><strong>Forward Selection:</strong> This is a <em>greedy
algorithm</em> that starts with an empty model and iteratively adds the
single predictor that most improves the fit.</p></li>
<li><p><strong>Theoretical Guarantee:</strong> Can forward selection
find the <em>true</em> sparse set of variables?</p>
<ul>
<li>Yes, <em>if</em> the predictors are not strongly correlated.</li>
<li>This is quantified by the <strong>Mutual Coherence
Condition</strong>. Assuming the predictors <span
class="math inline">\(\mathbf{x}_i\)</span> are normalized, the method
is guaranteed to work if: <span class="math display">\[\mu = \max_{i
\neq j} |\langle \mathbf{x}_i, \mathbf{x}_j \rangle| &lt; \frac{1}{2s -
1}\]</span> where <span class="math inline">\(s\)</span> is the number
of true non-zero coefficients and <span class="math inline">\(\langle
\mathbf{x}_i, \mathbf{x}_j \rangle\)</span> represents the correlation
between predictors.</li>
</ul></li>
</ul>
<h3 id="practical-application-finding-the-best-model-size">Practical
Application: Finding the Best Model Size</h3>
<p>How do you know whether to choose a model with 3, 4, or 5 variables?
You use <strong>Cross-Validation (CV)</strong>.</p>
<ul>
<li><p><strong>Important Image:</strong> The plot titled “10-fold CV”
(from the first slide) is the most important visual. It plots the
estimated test error (CV Error) on the y-axis against the number of
variables in the model on the x-axis.</p></li>
<li><p><strong>The “One Standard Deviation Rule”:</strong> Looking at
the plot, the error drops sharply and then flattens. The absolute
minimum error might be at 6 variables, but it’s only slightly better
than the 3-variable model.</p>
<ol type="1">
<li>Find the model with the <em>lowest</em> CV error.</li>
<li>Calculate the standard error for that error estimate.</li>
<li>Select the <strong>simplest model</strong> (fewest variables) whose
error is <em>within one standard deviation</em> of the minimum.</li>
<li>This follows <strong>Occam’s razor</strong>: choose the simplest
explanation (model) that fits the data well enough. In the example
given, this rule selects the 3-variable model.</li>
</ol></li>
</ul>
<h3 id="code-interpretation-r-vs.-python">Code Interpretation (R
vs. Python)</h3>
<p>The R code in the first slide performs this 10-fold CV manually for
forward selection:</p>
<ol type="1">
<li>It loops from <code>p = 1</code> to <code>10</code> (model
sizes).</li>
<li>Inside the loop, it identifies the <code>p</code> variables chosen
by a pre-computed forward selection model
(<code>regfit.fwd</code>).</li>
<li>It fits a new model (<code>glm.fit</code>) using <em>only</em> those
<code>p</code> variables.</li>
<li>It runs 10-fold CV (<code>cv.glm</code>) on <em>that specific
model</em> to get its test error.</li>
<li>It stores the error in <code>CV10.err[p]</code>.</li>
<li>Finally, it plots the results.</li>
</ol>
<p><strong>In Python (with <code>scikit-learn</code>):</strong> This
entire process is often automated.</p>
<ul>
<li>You would use <code>sklearn.feature_selection.RFECV</code>
(Recursive Feature Elimination with Cross-Validation).</li>
<li><code>RFECV</code> automatically performs cross-validation to find
the optimal number of features, effectively producing the same plot and
result as the R code.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Conceptual Python equivalent for finding the best model size</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFECV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"></span><br><span class="line"><span class="comment"># X, y = load_your_data()</span></span><br><span class="line">X, y = make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">10</span>, n_informative=<span class="number">3</span>, noise=<span class="number">10</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">estimator = LinearRegression()</span><br><span class="line"><span class="comment"># RFECV will test models with 1 feature, 2 features, etc.,</span></span><br><span class="line"><span class="comment"># and use cross-validation (cv=10) to find the best number.</span></span><br><span class="line">selector = RFECV(estimator, step=<span class="number">1</span>, cv=<span class="number">10</span>, scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>)</span><br><span class="line">selector = selector.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal number of features: <span class="subst">&#123;selector.n_features_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># You can plot selector.cv_results_[&#x27;mean_test_score&#x27;] to get the CV curve</span></span><br></pre></td></tr></table></figure>
<h2 id="shrinkage-methods-regularization">Shrinkage Methods
(Regularization)</h2>
<p>Instead of explicitly removing variables, shrinkage methods keep all
<span class="math inline">\(p\)</span> variables but <em>shrink</em>
their coefficients <span class="math inline">\(\beta_j\)</span> towards
zero.</p>
<h3 id="ridge-regression">Ridge Regression</h3>
<p>Ridge regression is a prime example of a shrinkage method.</p>
<ul>
<li><p><strong>Objective Function:</strong> It finds the coefficients
<span class="math inline">\(\boldsymbol{\beta}\)</span> that minimize a
new quantity: <span class="math display">\[\underbrace{\sum_{i=1}^{n}
(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2}_{\text{RSS (Goodness
of Fit)}} + \underbrace{\lambda \sum_{j=1}^{p}
\beta_j^2}_{\text{$\ell_2$ Penalty (Shrinkage)}}\]</span></p></li>
<li><p><strong>The <span class="math inline">\(\lambda\)</span> Tuning
Parameter:</strong> This parameter controls the strength of the
penalty:</p>
<ul>
<li><strong>If <span class="math inline">\(\lambda =
0\)</span>:</strong> The penalty term disappears. Ridge regression is
identical to standard Ordinary Least Squares (OLS).</li>
<li><strong>If <span class="math inline">\(\lambda \to
\infty\)</span>:</strong> The penalty is “infinitely” strong. To
minimize the function, all coefficients <span
class="math inline">\(\beta_j\)</span> (for <span
class="math inline">\(j=1...p\)</span>) are forced to be zero. The model
becomes an intercept-only model.</li>
<li><strong>Note:</strong> The intercept <span
class="math inline">\(\beta_0\)</span> is <em>not penalized</em>.</li>
</ul></li>
<li><p><strong>The Bias-Variance Trade-off:</strong> This is the core
concept of regularization.</p>
<ul>
<li>Standard OLS has low bias but can have high variance (it
overfits).</li>
<li>Ridge regression adds a <em>small amount of bias</em> (the
coefficients are “wrong” on purpose) to <strong>significantly reduce the
model’s variance</strong>.</li>
<li>This trade-off often leads to a model with a lower overall test
error.</li>
</ul></li>
<li><p><strong>Matrix Solution:</strong> The discussion slide asks “What
is the solution?”. While OLS has the solution <span
class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>, the Ridge
solution is: <span class="math display">\[\hat{\boldsymbol{\beta}}^R =
(\mathbf{X}^T\mathbf{X} + \lambda
\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</span> where <span
class="math inline">\(\mathbf{I}\)</span> is the identity matrix. The
<span class="math inline">\(\lambda \mathbf{I}\)</span> term adds a
“ridge” to the diagonal, making the matrix invertible even if <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is singular (which
happens if <span class="math inline">\(p &gt; n\)</span> or predictors
are collinear).</p></li>
</ul>
<h3 id="an-essential-step-standardization">An Essential Step:
Standardization</h3>
<ul>
<li><strong>Problem:</strong> The <span
class="math inline">\(\ell_2\)</span> penalty <span
class="math inline">\(\lambda \sum \beta_j^2\)</span> is applied equally
to all coefficients. If predictor <span
class="math inline">\(x_1\)</span> (e.g., house size in sq-ft) is on a
much larger scale than <span class="math inline">\(x_2\)</span> (e.g.,
number of rooms), its coefficient <span
class="math inline">\(\beta_1\)</span> will naturally be much smaller
than <span class="math inline">\(\beta_2\)</span>. The penalty will
unfairly punish <span class="math inline">\(\beta_2\)</span> more.</li>
<li><strong>Solution:</strong> You <strong>must standardize</strong>
your inputs <em>before</em> fitting a Ridge model.</li>
<li><strong>Formula:</strong> For each predictor <span
class="math inline">\(X_j\)</span>, all its observations <span
class="math inline">\(x_{ij}\)</span> are rescaled: <span
class="math display">\[\tilde{x}_{ij} = \frac{x_{ij} -
\bar{x}_j}{\sigma_j}\]</span> (where <span
class="math inline">\(\bar{x}_j\)</span> is the mean of the predictor
and <span class="math inline">\(\sigma_j\)</span> is its standard
deviation). This puts all predictors on a common scale (mean=0,
std=1).</li>
</ul>
<p><strong>In Python (with <code>scikit-learn</code>):</strong></p>
<ul>
<li>You use <code>sklearn.preprocessing.StandardScaler</code> to
standardize your data.</li>
<li>You use <code>sklearn.linear_model.Ridge</code> to fit the
model.</li>
<li>You use <code>sklearn.linear_model.RidgeCV</code> to automatically
find the best value for <span class="math inline">\(\lambda\)</span>
(called <code>alpha</code> in scikit-learn) using cross-validation.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Conceptual Python code for Ridge Regression</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># X, y = load_your_data()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a pipeline that first standardizes the data,</span></span><br><span class="line"><span class="comment"># then fits a Ridge model.</span></span><br><span class="line"><span class="comment"># RidgeCV tests a range of alphas (lambdas) automatically.</span></span><br><span class="line">model = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    RidgeCV(alphas=[<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">100.0</span>], scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best alpha (lambda): <span class="subst">&#123;model.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].alpha_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model coefficients: <span class="subst">&#123;model.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].coef_&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="subset-selection-2">Subset Selection</h2>
<p>This section is about choosing <em>which</em> predictors (variables)
to include in your linear model. The main idea is to find a “sparse”
model (one with few variables) that performs well.</p>
<h3 id="the-model-and-the-goal">The Model and The Goal</h3>
<ul>
<li><strong>Slide: “Forward selection in Linear
Regression”</strong></li>
<li><strong>Formula:</strong> The standard linear regression model is
<span class="math inline">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} +
\boldsymbol{\epsilon}\)</span>
<ul>
<li><span class="math inline">\(\mathbf{y}\)</span> is the <span
class="math inline">\(n \times 1\)</span> vector of outcomes.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is the <span
class="math inline">\(n \times (p+1)\)</span> matrix of predictors (with
a leading column of 1s for the intercept).</li>
<li><span class="math inline">\(\boldsymbol{\beta}\)</span> is the <span
class="math inline">\((p+1) \times 1\)</span> vector of coefficients
(<span class="math inline">\(\beta_0, \beta_1, ...,
\beta_p\)</span>).</li>
<li><span class="math inline">\(\boldsymbol{\epsilon}\)</span> is the
<span class="math inline">\(n \times 1\)</span> vector of irreducible
error.</li>
</ul></li>
<li><strong>Key Question:</strong> “If <span
class="math inline">\(\boldsymbol{\beta}\)</span> is sparse with at most
<span class="math inline">\(s\)</span> non-zero entries, can forward
selection find those variables?”
<ul>
<li><strong>Sparse</strong> means most coefficients are zero.</li>
<li><strong>Forward Selection</strong> is a <em>greedy algorithm</em>:
<ol type="1">
<li>Start with no variables.</li>
<li>Add the one variable that gives the best fit.</li>
<li>Add the <em>next</em> best variable to the existing model.</li>
<li>Repeat until you have a model with <span
class="math inline">\(s\)</span> variables.</li>
</ol></li>
<li>The slide suggests the answer is <strong>yes</strong>, but only
under certain conditions.</li>
</ul></li>
</ul>
<h3 id="the-condition-for-success">The Condition for Success</h3>
<ul>
<li><strong>Slide: “Orthogonal Matching Pursuit”</strong></li>
<li><strong>Key Concept:</strong> Forward selection can provably find
the correct variables if those variables are not strongly
correlated.</li>
<li><strong>Formula:</strong> This is formalized by the <strong>Mutual
Coherence Condition</strong>: <span class="math display">\[\mu = \max_{i
\neq j} |\langle \mathbf{x}_i, \mathbf{x}_j \rangle| &lt; \frac{1}{2s -
1}\]</span>
<ul>
<li><strong>What it means:</strong>
<ul>
<li><code>assuming $\mathbf&#123;x&#125;_i$'s are normalized</code> means we’ve
scaled them to have a length of 1.</li>
<li><span class="math inline">\(\langle \mathbf{x}_i, \mathbf{x}_j
\rangle\)</span> is the dot product, which is just their
<strong>correlation</strong> since they are normalized.</li>
<li><span class="math inline">\(\mu\)</span> (mu) is the <strong>largest
absolute correlation</strong> you can find between any two
<em>different</em> predictors.</li>
<li><span class="math inline">\(s\)</span> is the true number of
important variables.</li>
</ul></li>
<li><strong>In English:</strong> If the maximum correlation between any
of your predictors is less than this threshold, the greedy forward
selection algorithm is guaranteed to find the true, sparse set of
variables.</li>
</ul></li>
</ul>
<h3 id="how-to-choose-the-model-size-practice">How to Choose the Model
Size (Practice)</h3>
<p>The theory is nice, but in practice, you don’t know <span
class="math inline">\(s\)</span>. How many variables should you
pick?</p>
<ul>
<li><p><strong>Slide: “10-fold CV Errors”</strong></p></li>
<li><p><strong>This is the most important practical slide for this
section.</strong></p></li>
<li><p><strong>What the plot shows:</strong></p>
<ul>
<li><strong>X-axis:</strong> “Number of Variables” (from 1 to 10).</li>
<li><strong>Y-axis:</strong> “CV Error” (the 10-fold cross-validated
Mean Squared Error).</li>
<li><strong>The Curve:</strong> The error drops very fast as we add the
first 2-3 variables. Then, it flattens out. Adding more than 3 variables
doesn’t really help much.</li>
</ul></li>
<li><p><strong>Slide: “The one standard deviation
rule”</strong></p></li>
<li><p>This rule helps you pick the “best” model from the CV plot.</p>
<ol type="1">
<li>Find the model with the absolute <em>minimum</em> CV error (in the
plot, this looks to be around 6 or 7 variables).</li>
<li>Calculate the standard error of that minimum CV error.</li>
<li>Draw a “tolerance” line at
<code>(minimum error) + (one standard error)</code>.</li>
<li>Choose the <strong>simplest model</strong> (fewest variables) whose
CV error is <em>below</em> this tolerance line.</li>
</ol>
<!-- end list -->
<ul>
<li>The slide states this rule “gives the model with 3 variable” for
this example. This is because the 3-variable model is much simpler than
the 6-variable one, and its error is “good enough” (within one standard
deviation of the minimum). This is an application of <strong>Occam’s
razor</strong>.</li>
</ul></li>
</ul>
<h3 id="code-r-vs.-python">Code: R vs. Python</h3>
<p>The R code on the “10-fold CV Errors” slide generates that exact
plot.</p>
<ul>
<li><p><strong>R Code Explained:</strong></p>
<ul>
<li><code>library(boot)</code>: Loads the cross-validation library.</li>
<li><code>CV10.err=rep(0,10)</code>: Creates an empty vector to store
the 10 error scores.</li>
<li><code>for(p in 1:10)</code>: A loop that will test model sizes from
1 to 10.</li>
<li><code>x&lt;-which(summary(regfit.fwd)$which[p,])</code>: Gets the
<em>names</em> of the <span class="math inline">\(p\)</span> variables
chosen by a pre-run forward selection (<code>regfit.fwd</code>).</li>
<li><code>glm.fit=glm(Balance~.,data=newCred)</code>: Fits a model using
<em>only</em> those <span class="math inline">\(p\)</span>
variables.</li>
<li><code>cv.err=cv.glm(newCred,glm.fit,K=10)</code>: Performs 10-fold
CV on <em>that specific <span class="math inline">\(p\)</span>-variable
model</em>.</li>
<li><code>CV10.err[p]&lt;-cv.err$delta[1]</code>: Stores the CV
error.</li>
<li><code>plot(...)</code>: Plots the 10 errors against the 10 model
sizes.</li>
</ul></li>
<li><p><strong>Python Equivalent (Conceptual):</strong></p>
<ul>
<li>In <code>scikit-learn</code>, this process is often automated. You
wouldn’t write the CV loop yourself.</li>
<li>You would use <code>sklearn.feature_selection.RFECV</code>
(Recursive Feature Elimination with Cross-Validation). This tool
automatically wraps a model (like <code>LinearRegression</code>),
performs cross-validation, and finds the optimal number of features,
effectively producing the same plot and result.</li>
</ul></li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- Python equivalent for 6.1 ---</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFECV</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="comment"># Assume X and y are your data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create a pipeline</span></span><br><span class="line"><span class="comment"># (Note: It&#x27;s good practice to scale, even for OLS, if you&#x27;re comparing)</span></span><br><span class="line">pipeline = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    LinearRegression()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create the RFECV (Recursive Feature Elimination w/ CV) object</span></span><br><span class="line"><span class="comment"># This is an *alternative* to forward selection, but serves the same purpose</span></span><br><span class="line"><span class="comment"># It will test models with 1, 2, 3... features using 10-fold CV</span></span><br><span class="line">feature_selector = RFECV(</span><br><span class="line">    estimator=pipeline, </span><br><span class="line">    min_features_to_select=<span class="number">1</span>, </span><br><span class="line">    step=<span class="number">1</span>, </span><br><span class="line">    cv=<span class="number">10</span>, </span><br><span class="line">    scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span> <span class="comment"># We want to minimize error</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit it</span></span><br><span class="line">feature_selector.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal number of features found: <span class="subst">&#123;feature_selector.n_features_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You could then plot feature_selector.cv_results_[&#x27;mean_test_score&#x27;]</span></span><br><span class="line"><span class="comment"># to replicate the R plot.</span></span><br></pre></td></tr></table></figure>
<h2 id="shrinkage-methods-by-regularization">Shrinkage Methods by
Regularization</h2>
<p>This is a different approach. Instead of <em>removing</em> variables,
we keep all <span class="math inline">\(p\)</span> variables but
<em>shrink</em> their coefficients <span
class="math inline">\(\beta_j\)</span> towards 0.</p>
<h3 id="ridge-regression-the-core-idea">Ridge Regression: The Core
Idea</h3>
<ul>
<li><strong>Slide: “Ridge regression”</strong></li>
<li><strong>Formula:</strong> Ridge regression minimizes a new objective
function: <span class="math display">\[\min_{\boldsymbol{\beta}} \left(
\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 +
\lambda \sum_{j=1}^{p} \beta_j^2 \right)\]</span>
<ul>
<li><strong>Term 1: <span class="math inline">\(\text{RSS}\)</span>
(Residual Sum of Squares).</strong> This is the original OLS “goodness
of fit” term. We want this to be small.</li>
<li><strong>Term 2: <span class="math inline">\(\lambda \sum
\beta_j^2\)</span>.</strong> This is the <strong><span
class="math inline">\(\ell_2\)</span> penalty</strong> or “shrinkage
penalty”. It adds a “cost” for having large coefficients.</li>
</ul></li>
<li><strong>The <span class="math inline">\(\lambda\)</span> (lambda)
Parameter:</strong>
<ul>
<li>This is the <strong>tuning parameter</strong> that controls the
trade-off between fit and simplicity.</li>
<li><code>$\lambda = 0$</code>: No penalty. The objective is just to
minimize RSS. The solution <span
class="math inline">\(\hat{\boldsymbol{\beta}}^R\)</span> is identical
to the OLS solution <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</li>
<li><code>$\lambda = \infty$</code>: Infinite penalty. The only way to
minimize the cost is to make all <span class="math inline">\(\beta_j =
0\)</span> (for <span class="math inline">\(j \ge 1\)</span>). The model
becomes an intercept-only model.</li>
<li><code>Large $\lambda$</code>: Heavy penalty, more shrinkage.</li>
<li><strong>Crucial Note:</strong> The intercept <span
class="math inline">\(\beta_0\)</span> is <strong>not
penalized</strong>. This is because <span
class="math inline">\(\beta_0\)</span> just represents the mean of <span
class="math inline">\(y\)</span> when all <span
class="math inline">\(x\)</span>’s are 0; shrinking it makes no
sense.</li>
</ul></li>
</ul>
<h3 id="the-need-for-standardization">The Need for Standardization</h3>
<ul>
<li><strong>Slide: “Standardize the inputs”</strong></li>
<li><strong>Problem:</strong> The penalty <span
class="math inline">\(\lambda \sum \beta_j^2\)</span> is applied to all
coefficients. But what if <span class="math inline">\(x_1\)</span> is
“house size in sq-ft” (values 1000-5000) and <span
class="math inline">\(x_2\)</span> is “number of bedrooms” (values 1-5)?
<ul>
<li>The coefficient <span class="math inline">\(\beta_1\)</span> for
house size will naturally be <em>tiny</em>, while the coefficient <span
class="math inline">\(\beta_2\)</span> for bedrooms will be
<em>large</em>, even if they are equally important.</li>
<li>Ridge regression would unfairly and heavily penalize <span
class="math inline">\(\beta_2\)</span> while barely touching <span
class="math inline">\(\beta_1\)</span>.</li>
</ul></li>
<li><strong>Solution:</strong> You <strong>must</strong> standardize all
predictors <em>before</em> fitting a Ridge model.</li>
<li><strong>Formula:</strong> For each observation <span
class="math inline">\(i\)</span> of each predictor <span
class="math inline">\(j\)</span>: <span
class="math display">\[\tilde{x}_{ij} = \frac{x_{ij} -
\bar{x}_j}{\sqrt{(1/n) \sum_{i=1}^{n} (x_{ij} - \bar{x}_j)^2}}\]</span>
<ul>
<li>This formula rescales every predictor to have a mean of 0 and a
standard deviation of 1.</li>
<li>Now, all coefficients <span class="math inline">\(\beta_j\)</span>
are on a “level playing field” and can be penalized fairly.</li>
</ul></li>
</ul>
<h3 id="answering-the-discussion-questions">Answering the Discussion
Questions</h3>
<ul>
<li><strong>Slide: “DISCUSSION”</strong>
<ul>
<li><code>What is the solution of Ridge regression?</code></li>
<li><code>What is the bias and the variance?</code></li>
</ul></li>
</ul>
<h4 id="what-is-the-solution-of-ridge-regression">1. What is the
solution of Ridge regression?</h4>
<p>The solution can be written in matrix form, which is very
elegant.</p>
<ul>
<li><p><strong>Standard OLS Solution:</strong> The coefficients <span
class="math inline">\(\hat{\boldsymbol{\beta}}^{\text{OLS}}\)</span>
that minimize RSS are found by: <span
class="math display">\[\hat{\boldsymbol{\beta}}^{\text{OLS}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p></li>
<li><p><strong>Ridge Regression Solution:</strong> The coefficients
<span class="math inline">\(\hat{\boldsymbol{\beta}}^{R}\)</span> that
minimize the Ridge objective are: <span
class="math display">\[\hat{\boldsymbol{\beta}}^{R} =
(\mathbf{X}^T\mathbf{X} + \lambda
\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><strong>Explanation:</strong>
<ul>
<li><span class="math inline">\(\mathbf{I}\)</span> is the
<strong>identity matrix</strong> (a matrix of 1s on the diagonal, 0s
everywhere else).</li>
<li>By adding <span class="math inline">\(\lambda\mathbf{I}\)</span>, we
are adding a positive value <span class="math inline">\(\lambda\)</span>
to the <em>diagonal</em> of the <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> matrix.</li>
<li>This addition <strong>stabilizes</strong> the matrix. <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> might not be
invertible (if <span class="math inline">\(p &gt; n\)</span> or if
predictors are perfectly collinear), but <span
class="math inline">\((\mathbf{X}^T\mathbf{X} + \lambda
\mathbf{I})\)</span> is <em>always</em> invertible for <span
class="math inline">\(\lambda &gt; 0\)</span>.</li>
<li>This addition is what mathematically “shrinks” the coefficients
toward zero.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="what-is-the-bias-and-the-variance">2. What is the bias and the
variance?</h4>
<p>This is the <strong>most important concept</strong> in
regularization. It’s the <strong>bias-variance trade-off</strong>.</p>
<ul>
<li><p><strong>Standard OLS (where <span
class="math inline">\(\lambda=0\)</span>):</strong></p>
<ul>
<li><strong>Bias: Low.</strong> The OLS estimator is
<strong>unbiased</strong>, meaning that if you took many samples and fit
many OLS models, their average <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> would be the
<em>true</em> <span
class="math inline">\(\boldsymbol{\beta}\)</span>.</li>
<li><strong>Variance: High.</strong> The OLS solution can be
<em>highly</em> sensitive to the training data. If you change a few data
points, the coefficients can swing wildly. This is especially true if
<span class="math inline">\(p\)</span> is large or predictors are
correlated. This “sensitivity” is high variance, which leads to
<strong>overfitting</strong>.</li>
</ul></li>
<li><p><strong>Ridge Regression (where <span
class="math inline">\(\lambda &gt; 0\)</span>):</strong></p>
<ul>
<li><strong>Bias: High(er).</strong> Ridge regression is a
<strong>biased</strong> estimator. By adding the penalty, we are
<em>purposefully</em> pulling the coefficients away from the OLS
solution and towards zero. The average <span
class="math inline">\(\hat{\boldsymbol{\beta}}^R\)</span> from many
samples will <em>not</em> equal the true <span
class="math inline">\(\boldsymbol{\beta}\)</span>. We have
<em>introduced</em> bias into our model.</li>
<li><strong>Variance: Low(er).</strong> In exchange for this bias, we
get a massive <em>reduction in variance</em>. The <span
class="math inline">\(\lambda\mathbf{I}\)</span> term stabilizes the
solution. The coefficients won’t change wildly even if the training data
changes. The model is more robust and less sensitive.</li>
</ul></li>
</ul>
<p><strong>The Trade-off:</strong> The total expected test error of a
model is: <span class="math inline">\(\text{Error} = \text{Bias}^2 +
\text{Variance} + \text{Irreducible Error}\)</span></p>
<p>By using Ridge regression, we <em>increase</em> the <span
class="math inline">\(\text{Bias}^2\)</span> term a little, but we
<em>decrease</em> the <span
class="math inline">\(\text{Variance}\)</span> term a lot. The goal is
to find a <span class="math inline">\(\lambda\)</span> where the
<em>total error</em> is minimized. Ridge regression reduces variance
<em>at the cost of</em> increased bias.</p>
<h3 id="python-equivalent-for-6.2">Python Equivalent for 6.2</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- Python equivalent for 6.2 ---</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="comment"># Assume X and y are your data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create a pipeline that AUTOMATICALLY</span></span><br><span class="line"><span class="comment">#    - Standardizes the data</span></span><br><span class="line"><span class="comment">#    - Fits a Ridge Regression model</span></span><br><span class="line"><span class="comment">#    - Uses Cross-Validation to find the BEST lambda (alpha in scikit-learn)</span></span><br><span class="line">alphas_to_test = [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">100.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># RidgeCV handles everything for us</span></span><br><span class="line">pipeline = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    RidgeCV(alphas=alphas_to_test, scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>, cv=<span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit the pipeline</span></span><br><span class="line">pipeline.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Get the results</span></span><br><span class="line">best_lambda = pipeline.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].alpha_</span><br><span class="line">ridge_coefficients = pipeline.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].coef_</span><br><span class="line">intercept = pipeline.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].intercept_</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha) found by CV: <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model intercept (beta_0): <span class="subst">&#123;intercept&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model coefficients (beta_j): <span class="subst">&#123;ridge_coefficients&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="the-why-of-ridge-regression">6. The “Why” of Ridge
Regression</h1>
<h2 id="core-concepts-the-why-of-ridge-regression">Core Concepts: The
“Why” of Ridge Regression</h2>
<p>Your slides explain that ridge regression is a “shrinkage method”
designed to solve a major problem with standard Ordinary Least Squares
(OLS) regression: <strong>high variance</strong>.</p>
<h3 id="the-bias-variance-tradeoff-slide-3">The Bias-Variance Tradeoff
(Slide 3)</h3>
<p>This is the most important theoretical concept. In prediction, the
total error (Mean Squared Error, or MSE) of a model is composed of three
parts: <span class="math inline">\(\text{Error} = \text{Variance} +
\text{Bias}^2 + \text{Irreducible Error}\)</span></p>
<ul>
<li><strong>Ordinary Least Squares (OLS):</strong> Aims to be unbiased
(low bias). However, when you have many predictors (<span
class="math inline">\(p\)</span>), especially if they are correlated, or
if <span class="math inline">\(p\)</span> is large compared to the
number of samples <span class="math inline">\(n\)</span> (<span
class="math inline">\(p \approx n\)</span> or <span
class="math inline">\(p &gt; n\)</span>), the OLS model becomes highly
<em>unstable</em>. A small change in the training data can cause the
coefficients to change wildly. This is <strong>high variance</strong>.
(See Slide 6, “Remarks”).</li>
<li><strong>Ridge Regression:</strong> By adding a penalty, ridge
<em>intentionally</em> introduces a small amount of
<strong>bias</strong> (it pulls coefficients away from their “true” OLS
values). In return, it achieves a <em>massive</em> reduction in
<strong>variance</strong>.</li>
</ul>
<p>As <strong>Slide 3</strong> shows:</p>
<ul>
<li>The <strong>green line (Variance)</strong> starts very high for low
<span class="math inline">\(\lambda\)</span> (left side) and drops
quickly.</li>
<li>The <strong>black line (Squared Bias)</strong> starts at zero (for
OLS at <span class="math inline">\(\lambda=0\)</span>) and slowly
increases as <span class="math inline">\(\lambda\)</span> grows.</li>
<li>The <strong>purple line (Test MSE)</strong> is the sum of the two.
It’s U-shaped. The goal of ridge is to find the <span
class="math inline">\(\lambda\)</span> (marked by the ‘x’) at the
<em>bottom</em> of this “U,” which gives the lowest possible total
error.</li>
</ul>
<h3 id="why-is-it-called-ridge-the-3d-spatial-meaning-slide-5">Why Is It
Called “Ridge”? The 3D Spatial Meaning (Slide 5)</h3>
<p>This slide explains the problem of <strong>collinearity</strong> and
the origin of the name.</p>
<ul>
<li><strong>Left Plot (Least Squares):</strong> Imagine a model with two
correlated predictors, <span class="math inline">\(\beta_1\)</span> and
<span class="math inline">\(\beta_2\)</span>. The y-axis (SS1) is the
error (RSS). Because the predictors are correlated, there isn’t one
single “point” that is the minimum. Instead, there’s a long, flat
<em>valley</em> or <em>trough</em> (marked “unstable”). Many different
combinations of <span class="math inline">\(\beta_1\)</span> and <span
class="math inline">\(\beta_2\)</span> along this valley give a
similarly low error. The OLS solution is unstable because it can pick
<em>any</em> point in this flat-bottomed valley.</li>
<li><strong>Right Plot (Ridge):</strong> The ridge objective function
adds a penalty term: <span class="math inline">\(\lambda(\beta_1^2 +
\beta_2^2)\)</span>. This penalty term, by itself, is a perfect circular
bowl centered at (0,0). When you add this “bowl” to the OLS “valley,” it
<em>stabilizes</em> the function. It pulls the minimum towards (0,0) and
creates a single, stable, well-defined minimum.</li>
<li><strong>The “Ridge” Name:</strong> The penalty <span
class="math inline">\(\lambda\mathbf{I}\)</span> (from the matrix
formula) adds a “ridge” of values to the diagonal of the <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> matrix, which
geometrically turns the unstable flat valley into a stable bowl.</li>
</ul>
<h2 id="mathematical-formulas">Mathematical Formulas</h2>
<p>The key difference between OLS and Ridge is the function they try to
minimize.</p>
<ol type="1">
<li><p><strong>OLS Objective Function:</strong> Minimize the Residual
Sum of Squares (RSS). <span class="math display">\[\text{RSS} =
\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}
\right)^2\]</span></p></li>
<li><p><strong>Ridge Objective Function (Slide 6):</strong> Minimize the
RSS <em>plus</em> an L2 penalty term. <span
class="math display">\[\text{Minimize: } \left[ \sum_{i=1}^{n} \left(
y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 \right] +
\lambda \sum_{j=1}^{p} \beta_j^2\]</span></p>
<ul>
<li><span class="math inline">\(\lambda\)</span> is the <strong>tuning
parameter</strong> controlling the penalty strength.</li>
<li><span class="math inline">\(\sum_{j=1}^{p} \beta_j^2\)</span> is the
<strong>L2-norm</strong> (squared) of the coefficients. It penalizes
large coefficients.</li>
</ul></li>
<li><p><strong>L2 Norm (Slide 1):</strong> The L2 norm of a vector <span
class="math inline">\(\mathbf{a}\)</span> is its standard Euclidean
length. The plot on Slide 1 uses this to show the <em>total
magnitude</em> of the ridge coefficients. <span
class="math display">\[\|\mathbf{a}\|_2 = \sqrt{\sum_{j=1}^p
a_j^2}\]</span></p></li>
<li><p><strong>Matrix Solution (Slide 6):</strong> This is the
“closed-form” solution for the ridge coefficients <span
class="math inline">\(\hat{\beta}^R\)</span>. <span
class="math display">\[\hat{\beta}^R = (\mathbf{X}^T\mathbf{X} +
\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{I}\)</span> is the identity
matrix.</li>
<li>The term <span class="math inline">\(\lambda\mathbf{I}\)</span> is
what stabilizes the <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> matrix, making it
invertible even if it’s singular (due to <span class="math inline">\(p
&gt; n\)</span> or collinearity).</li>
</ul></li>
</ol>
<h2 id="walkthrough-of-the-credit-data-example-all-slides">Walkthrough
of the “Credit Data” Example (All Slides)</h2>
<p>Here is the logical story of the R code, from start to finish.</p>
<h3 id="step-1-data-preparation-slide-8">Step 1: Data Preparation (Slide
8)</h3>
<ul>
<li><code>x=scale(model.matrix(Balance~., Credit)[,-1])</code>
<ul>
<li><code>model.matrix(...)</code> creates the predictor matrix
<code>x</code>.</li>
<li><code>scale(...)</code> is <strong>critically important</strong>. It
standardizes all predictors to have a mean of 0 and a standard deviation
of 1. This is necessary because the ridge penalty <span
class="math inline">\(\lambda \sum \beta_j^2\)</span> is
<em>unit-dependent</em>. If <code>Income</code> (in 10,000s) and
<code>Cards</code> (1-10) were unscaled, the penalty would unfairly
crush the <code>Income</code> coefficient. Scaling puts all predictors
on a level playing field.</li>
</ul></li>
<li><code>y=Credit$Balance</code>
<ul>
<li>This sets the <code>y</code> (target) variable.</li>
</ul></li>
</ul>
<h3 id="step-2-fit-the-ridge-model-slide-8">Step 2: Fit the Ridge Model
(Slide 8)</h3>
<ul>
<li><code>grid=10^seq(4,-2,length=100)</code>
<ul>
<li>This creates a <em>grid</em> of 100 <span
class="math inline">\(\lambda\)</span> values to test, ranging from
<span class="math inline">\(10^4\)</span> (a huge penalty) down to <span
class="math inline">\(10^{-2}\)</span> (a tiny penalty).</li>
</ul></li>
<li><code>ridge.mod=glmnet(x,y,alpha=0,lambda=grid)</code>
<ul>
<li>This is the main command. It fits a <em>separate</em> ridge model
for <em>every single <span class="math inline">\(\lambda\)</span></em>
in the <code>grid</code>.</li>
<li><code>alpha=0</code> is the specific command that tells
<code>glmnet</code> to perform <strong>Ridge Regression</strong>.
(Setting <code>alpha=1</code> would be LASSO).</li>
</ul></li>
<li><code>coef(ridge.mod)[,50]</code>
<ul>
<li>This inspects the model. It pulls out the vector of coefficients for
the 50th <span class="math inline">\(\lambda\)</span> in the grid (which
is <span class="math inline">\(\lambda=10.72\)</span>).</li>
</ul></li>
</ul>
<h3
id="step-3-visualize-the-coefficient-solution-path-slides-1-4-9">Step 3:
Visualize the Coefficient “Solution Path” (Slides 1, 4, 9)</h3>
<p>These plots all show the same thing: how the coefficients change as
<span class="math inline">\(\lambda\)</span> changes.</p>
<ul>
<li><strong>Slide 9 Plot:</strong> This plots the standardized
coefficients for 4 predictors (<code>Income</code>, <code>Limit</code>,
<code>Rating</code>, <code>Student</code>) against the <em>index</em> (1
to 100). Index 1 (left) is the largest <span
class="math inline">\(\lambda\)</span>, and index 100 (right) is the
smallest <span class="math inline">\(\lambda\)</span> (closest to OLS).
You can see the coefficients “grow” from 0 as the penalty (<span
class="math inline">\(\lambda\)</span>) gets smaller.</li>
<li><strong>Slide 1 (Left Plot):</strong> This is the <em>same plot</em>
as Slide 9, but more professional. It plots the coefficients against
<span class="math inline">\(\lambda\)</span> on a log scale. You can
clearly see all coefficients (gray lines) being “shrunk” toward zero as
<span class="math inline">\(\lambda\)</span> increases (moves right).
The key predictors (<code>Income</code>, <code>Rating</code>, etc.) are
highlighted.</li>
<li><strong>Slide 1 (Right Plot):</strong> This is the <em>exact same
data</em> again, but with a different x-axis: <span
class="math inline">\(\|\hat{\beta}_\lambda^R\|_2 /
\|\hat{\beta}\|_2\)</span>.
<ul>
<li><strong>1.0</strong> on the right means <span
class="math inline">\(\lambda=0\)</span>. The ratio of the ridge norm to
the OLS norm is 1 (they are the same).</li>
<li><strong>0.0</strong> on the left means <span
class="math inline">\(\lambda=\infty\)</span>. The ridge coefficients
are all 0, so their norm is 0.</li>
<li>This axis shows the “fraction” of the full OLS coefficient magnitude
that the model is using.</li>
</ul></li>
<li><strong>Slide 4 Plot:</strong> This plots the <em>total L2 norm</em>
of <em>all</em> coefficients (<span
class="math inline">\(\|\hat{\beta}_\lambda^R\|_2\)</span>) against the
index. As the index goes from 1 to 100 (i.e., <span
class="math inline">\(\lambda\)</span> gets smaller), the total
magnitude of the coefficients gets larger, which is exactly what we
expect.</li>
</ul>
<h3
id="step-4-find-the-best-lambda-using-cross-validation-slides-4-7">Step
4: Find the <em>Best</em> <span class="math inline">\(\lambda\)</span>
using Cross-Validation (Slides 4 &amp; 7)</h3>
<p>We have 100 models. Which one is best?</p>
<ul>
<li><p><strong>The “Manual” Way (Slide 4):</strong></p>
<ul>
<li>The code splits the data into a <code>train</code> and
<code>test</code> set.</li>
<li>It fits a model <em>only</em> on the <code>train</code> set.</li>
<li>It tests two <span class="math inline">\(\lambda\)</span> values:
<ul>
<li><code>s=4</code>: Gives a test MSE of <code>10293.33</code>.</li>
<li><code>s=10</code>: Gives a test MSE of <code>168981.1</code> (much
worse!).</li>
</ul></li>
<li>This shows that <span class="math inline">\(\lambda=4\)</span> is
better than <span class="math inline">\(\lambda=10\)</span>, but we
don’t know if it’s the <em>best</em>.</li>
</ul></li>
<li><p><strong>The “Automatic” Way (Slide 7):</strong></p>
<ul>
<li><code>cv.out=cv.glmnet(x[train,], y[train], alpha=0)</code></li>
<li>This runs <strong>10-fold Cross-Validation</strong> on the training
set. It automatically splits the training set into 10 “folds,” trains on
9, tests on 1, and repeats this 10 times for <em>every <span
class="math inline">\(\lambda\)</span></em>.</li>
<li><strong>The Plot:</strong> The plot on this slide is the result. It
shows the average MSE (y-axis) for each <span
class="math inline">\(\log(\lambda)\)</span> (x-axis). This is the
<em>real-data version</em> of the theoretical purple curve from Slide
3.</li>
<li><code>bestlam=cv.out$lambda.min</code></li>
<li>This command finds the <span class="math inline">\(\lambda\)</span>
at the <em>very bottom</em> of the U-shaped curve. The output shows
<code>bestlam</code> is <strong>41.6</strong>.</li>
<li><code>ridge.pred=predict(ridge.mod, s=bestlam, newx=x[test,])</code></li>
<li>Now, we use this <em>one best <span
class="math inline">\(\lambda\)</span></em> to make predictions on our
held-out <code>test</code> set.</li>
<li><code>mean((ridge.pred-y.test)^2)</code></li>
<li>The final, reliable test MSE is <strong>16129.68</strong>. This is
our best estimate of how the model will perform on new, unseen
data.</li>
</ul></li>
</ul>
<h2 id="python-scikit-learn-equivalents">Python
(<code>scikit-learn</code>) Equivalents</h2>
<p>Here is how you would perform the entire R workflow from your slides
in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. Load and Prepare Data (like Slide 8) ---</span></span><br><span class="line"><span class="comment"># Assuming &#x27;Credit&#x27; is a pandas DataFrame</span></span><br><span class="line"><span class="comment"># X = Credit.drop(&#x27;Balance&#x27;, axis=1)</span></span><br><span class="line"><span class="comment"># y = Credit[&#x27;Balance&#x27;]</span></span><br><span class="line"><span class="comment"># ... (need to handle categorical variables first, e.g., with pd.get_dummies) ...</span></span><br><span class="line"><span class="comment"># For this example, let&#x27;s assume X and y are already loaded and numeric.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the predictors (CRITICAL)</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. Train/Test Split (like Slide 4) ---</span></span><br><span class="line"><span class="comment"># test_size=0.5 and random_state=1 mimic the R code</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X_scaled, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 3. Find Best Lambda (alpha) with Cross-Validation (like Slide 7) ---</span></span><br><span class="line"><span class="comment"># Create the same log-spaced grid of lambdas (sklearn calls it &#x27;alpha&#x27;)</span></span><br><span class="line">lambda_grid = np.logspace(<span class="number">4</span>, -<span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># RidgeCV performs cross-validation to find the best alpha</span></span><br><span class="line"><span class="comment"># cv=10 matches the 10-fold CV</span></span><br><span class="line"><span class="comment"># store_cv_values=True is needed to plot the CV error curve</span></span><br><span class="line">cv_model = RidgeCV(alphas=lambda_grid, store_cv_values=<span class="literal">True</span>, cv=<span class="number">10</span>)</span><br><span class="line">cv_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best lambda found</span></span><br><span class="line">best_lambda = cv_model.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha) found by CV: <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the CV error curve (like Slide 7 plot)</span></span><br><span class="line"><span class="comment"># cv_model.cv_values_ has shape (n_samples, n_alphas)</span></span><br><span class="line"><span class="comment"># We need to average over the samples for each alpha</span></span><br><span class="line">mse_path = np.mean(cv_model.cv_values_, axis=<span class="number">0</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(np.log10(cv_model.alphas_), mse_path, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Log(lambda)&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Mean Squared Error&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Cross-Validation Error Path&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 4. Evaluate on Test Set (like Slide 7) ---</span></span><br><span class="line"><span class="comment"># &#x27;cv_model&#x27; is already refit on the full training set using the best_lambda</span></span><br><span class="line">test_pred = cv_model.predict(X_test)</span><br><span class="line">final_test_mse = mean_squared_error(y_test, test_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Final Test MSE with best lambda: <span class="subst">&#123;final_test_mse&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 5. Get Final Coefficients (like Slide 7, bottom) ---</span></span><br><span class="line"><span class="comment"># The coefficients from the CV-trained model:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Intercept: <span class="subst">&#123;cv_model.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Coefficients:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> coef, feature <span class="keyword">in</span> <span class="built_in">zip</span>(cv_model.coef_, X.columns):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;  <span class="subst">&#123;feature&#125;</span>: <span class="subst">&#123;coef&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 6. Plot the Solution Path (like Slide 1) ---</span></span><br><span class="line"><span class="comment"># To do this, we fit a Ridge model for each lambda and store the coefficients</span></span><br><span class="line">coefs = []</span><br><span class="line"><span class="keyword">for</span> lam <span class="keyword">in</span> lambda_grid:</span><br><span class="line">    model = Ridge(alpha=lam)</span><br><span class="line">    model.fit(X_scaled, y)  <span class="comment"># Fit on all data</span></span><br><span class="line">    coefs.append(model.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(np.log10(lambda_grid), coefs)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Log(lambda)&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Standardized Coefficients&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Ridge Solution Path&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="shrinkage-methods-regularization-1">7. Shrinkage Methods
(Regularization)</h1>
<p>These slides cover <strong>Shrinkage Methods</strong>, also known as
<strong>Regularization</strong>, which are techniques used to improve on
the standard least squares model, particularly when dealing with many
variables or multicollinearity. The main focus is on
<strong>LASSO</strong> regression.</p>
<h2 id="key-mathematical-formulas">Key Mathematical Formulas</h2>
<p>The slides present two main, but equivalent, ways to formulate these
methods.</p>
<h3 id="penalized-formulation-slide-1">1. Penalized Formulation (Slide
1)</h3>
<p>This is the most common formulation. The goal is to minimize a
function that is a combination of the <strong>Residual Sum of Squares
(RSS)</strong> and a <strong>penalty term</strong>. The penalty
discourages large coefficients.</p>
<ul>
<li><strong>LASSO (Least Absolute Shrinkage and Selection
Operator):</strong> The goal is to find coefficients (<span
class="math inline">\(\beta_0, \beta_j\)</span>) that minimize: <span
class="math display">\[\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p}
\beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j|\]</span>
<ul>
<li><strong>Penalty:</strong> The <span
class="math inline">\(L_1\)</span> norm (<span
class="math inline">\(\|\beta\|_1\)</span>), which is the sum of the
<em>absolute values</em> of the coefficients.</li>
<li><strong>Key Property:</strong> This penalty can force some
coefficients to be <strong>exactly zero</strong>, effectively performing
automatic variable selection.</li>
</ul></li>
</ul>
<h3 id="constrained-formulation-slide-2">2. Constrained Formulation
(Slide 2)</h3>
<p>This alternative formulation minimizes the RSS <em>subject to a
constraint</em> (a “budget”) on the size of the coefficients.</p>
<ul>
<li><p><strong>For Lasso:</strong> Minimize RSS subject to: <span
class="math display">\[\sum_{j=1}^{p} |\beta_j| \le s\]</span> (The sum
of the absolute values of the coefficients must be less than some budget
<span class="math inline">\(s\)</span>.)</p></li>
<li><p><strong>For Ridge:</strong> Minimize RSS subject to: <span
class="math display">\[\sum_{j=1}^{p} \beta_j^2 \le s\]</span> (The sum
of the <em>squares</em> of the coefficients (<span
class="math inline">\(L_2\)</span> norm) must be less than <span
class="math inline">\(s\)</span>.)</p></li>
</ul>
<p><strong>Equivalence (Slide 3):</strong> For any penalty value <span
class="math inline">\(\lambda\)</span> used in the first formulation,
there is a corresponding budget <span class="math inline">\(s\)</span>
in the second formulation that will give the exact same set of
coefficients. <span class="math inline">\(\lambda\)</span> and <span
class="math inline">\(s\)</span> are inversely related: a large <span
class="math inline">\(\lambda\)</span> (high penalty) corresponds to a
small <span class="math inline">\(s\)</span> (small budget).</p>
<h2 id="important-plots-and-interpretation">Important Plots and
Interpretation</h2>
<p>Your slides show the two most important plots for understanding and
using LASSO.</p>
<h3 id="the-cross-validation-cv-plot-slide-5">1. The Cross-Validation
(CV) Plot (Slide 5)</h3>
<p>This plot is crucial for <strong>choosing the best tuning parameter
(<span class="math inline">\(\lambda\)</span>)</strong>.</p>
<ul>
<li><strong>X-axis:</strong> <span
class="math inline">\(\text{Log}(\lambda)\)</span>. This is the penalty
strength.
<ul>
<li><strong>Right side (high <span
class="math inline">\(\lambda\)</span>):</strong> High penalty, simple
model (many coefficients are 0), high bias, high Mean-Squared Error
(MSE).</li>
<li><strong>Left side (low <span
class="math inline">\(\lambda\)</span>):</strong> Low penalty, complex
model (like standard linear regression), high variance, MSE starts to
increase (overfitting).</li>
</ul></li>
<li><strong>Y-axis:</strong> Mean-Squared Error (MSE) from
cross-validation.</li>
<li><strong>Goal:</strong> Find the <span
class="math inline">\(\lambda\)</span> at the <strong>bottom of the “U”
shape</strong>, which gives the <em>lowest</em> MSE. This is the optimal
trade-off between bias and variance. The top axis shows how many
variables are included in the model at each <span
class="math inline">\(\lambda\)</span>.</li>
</ul>
<h3 id="the-coefficient-path-plot-slide-6">2. The Coefficient Path Plot
(Slide 6)</h3>
<p>This plot is the best visualization for <strong>understanding what
LASSO does</strong>.</p>
<ul>
<li><strong>Left Plot (vs. <span
class="math inline">\(\lambda\)</span>):</strong>
<ul>
<li><strong>X-axis:</strong> The penalty strength <span
class="math inline">\(\lambda\)</span>.</li>
<li><strong>Y-axis:</strong> The standardized value of each
coefficient.</li>
<li><strong>How to read it:</strong> Start from the
<strong>right</strong> (high <span
class="math inline">\(\lambda\)</span>). All coefficients are 0. As you
move <strong>left</strong>, <span class="math inline">\(\lambda\)</span>
<em>decreases</em>, and the penalty is relaxed. Variables “enter” the
model one by one (their coefficients become non-zero). You can see that
‘Rating’, ‘Income’, and ‘Student’ are the most important variables, as
they are the first to become non-zero.</li>
</ul></li>
<li><strong>Right Plot (vs. <span class="math inline">\(L_1\)</span>
Norm Ratio):</strong>
<ul>
<li>This shows the exact same information as the left plot, but the
x-axis is reversed and rescaled. An axis value of 0.0 means full penalty
(all <span class="math inline">\(\beta=0\)</span>), and 1.0 means no
penalty.</li>
</ul></li>
</ul>
<h2 id="code-understanding-r-to-python">Code Understanding (R to
Python)</h2>
<p>The slides use the <code>glmnet</code> package in R. The equivalent
and most popular library in Python is <strong>scikit-learn</strong>.</p>
<h3 id="finding-the-best-lambda-cv">1. Finding the Best <span
class="math inline">\(\lambda\)</span> (CV)</h3>
<p>The R code <code>cv.out=cv.glmnet(x[train,],y[train],alpha=1)</code>
performs cross-validation to find the best <span
class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><strong>Python Equivalent:</strong> Use <code>LassoCV</code>. It
does the same thing: tests many <span
class="math inline">\(\lambda\)</span> values (called
<code>alphas</code> in scikit-learn) and picks the best one.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the LassoCV object</span></span><br><span class="line"><span class="comment"># cv=5 means 5-fold cross-validation</span></span><br><span class="line">lasso_cv = LassoCV(cv=<span class="number">5</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model to the training data</span></span><br><span class="line">lasso_cv.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best lambda (called alpha_ in sklearn)</span></span><br><span class="line">best_lambda = lasso_cv.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha): <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the MSEs</span></span><br><span class="line"><span class="comment"># This is what&#x27;s plotted in the CV plot</span></span><br><span class="line"><span class="built_in">print</span>(lasso_cv.mse_path_)</span><br></pre></td></tr></table></figure>
<h3 id="fitting-with-the-best-lambda-and-getting-coefficients">2.
Fitting with the Best <span class="math inline">\(\lambda\)</span> and
Getting Coefficients</h3>
<p>The R code
<code>lasso.coef=predict(out,type="coefficients",s=bestlam)</code> gets
the coefficients for the best <span
class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><strong>Python Equivalent:</strong> The <code>LassoCV</code> object
is <em>already</em> refitted on the full training data using the best
<span class="math inline">\(\lambda\)</span>. You can also fit a new
<code>Lasso</code> model with that specific <span
class="math inline">\(\lambda\)</span>.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Option 1: Use the already-fitted LassoCV object ---</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Coefficients from LassoCV:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(lasso_cv.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on the test set</span></span><br><span class="line">y_pred = lasso_cv.predict(X_test)</span><br><span class="line">test_mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test MSE: <span class="subst">&#123;test_mse&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Option 2: Fit a new Lasso model with the best lambda ---</span></span><br><span class="line">final_lasso = Lasso(alpha=best_lambda)</span><br><span class="line">final_lasso.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get coefficients (Slide 7 shows this)</span></span><br><span class="line"><span class="comment"># Note how some are 0!</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nCoefficients from new Lasso model:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(final_lasso.coef_)</span><br></pre></td></tr></table></figure>
<h2 id="the-core-problem-two-equivalent-formulas">The Core Problem: Two
Equivalent Formulas</h2>
<p>The slides show two ways of writing the <em>same problem</em>.
Understanding this equivalence is key.</p>
<h3 id="formulation-1-the-penalized-method-slides-1-4">Formulation 1:
The Penalized Method (Slides 1 &amp; 4)</h3>
<ul>
<li><p><strong>Formula:</strong> <span
class="math display">\[\min_{\beta} \left( \sum_{i=1}^{n} (y_i -
\mathbf{x}_i^T \beta)^2 + \lambda \|\beta\|_1 \right)\]</span></p>
<ul>
<li><strong><span class="math inline">\(\sum (y_i - \mathbf{x}_i^T
\beta)^2\)</span></strong>: This is the normal <strong>Residual Sum of
Squares (RSS)</strong>. We want to make this small (fit the data
well).</li>
<li><strong><span class="math inline">\(\lambda
\|\beta\|_1\)</span></strong>: This is the <strong><span
class="math inline">\(L_1\)</span> penalty</strong>.
<ul>
<li><span class="math inline">\(\|\beta\|_1 = \sum_{j=1}^{p}
|\beta_j|\)</span> is the sum of the absolute values of the
coefficients.</li>
<li><span class="math inline">\(\lambda\)</span> (lambda) is a tuning
parameter. Think of it as a <strong>“penalty knob”</strong>.</li>
</ul></li>
</ul></li>
<li><p><strong>How to think about <span
class="math inline">\(\lambda\)</span></strong>:</p>
<ul>
<li><strong>If <span class="math inline">\(\lambda =
0\)</span>:</strong> There is no penalty. This is just standard Ordinary
Least Squares (OLS) regression. The model will likely overfit.</li>
<li><strong>If <span class="math inline">\(\lambda\)</span> is
<em>small</em>:</strong> There’s a small penalty. Coefficients will
shrink a <em>little</em> bit.</li>
<li><strong>If <span class="math inline">\(\lambda\)</span> is <em>very
large</em>:</strong> The penalty is severe. The <em>only</em> way to
make the penalty term small is to make the coefficients (<span
class="math inline">\(\beta\)</span>) themselves small. The model will
eventually shrink all coefficients to <strong>exactly 0</strong>.</li>
</ul></li>
</ul>
<h3 id="formulation-2-the-constrained-method-slides-2-3">Formulation 2:
The Constrained Method (Slides 2 &amp; 3)</h3>
<ul>
<li><p><strong>Formula:</strong> <span
class="math display">\[\min_{\beta} \sum_{i=1}^{n} (y_i - \mathbf{x}_i^T
\beta)^2 \quad \text{subject to} \quad \|\beta\|_1 \le
s\]</span></p></li>
<li><p><strong>How to think about <span
class="math inline">\(s\)</span></strong>:</p>
<ul>
<li>This says: “Find the best-fitting model (minimize RSS) <em>but</em>
you have a limited <strong>‘budget’ <span
class="math inline">\(s\)</span></strong> for the total size of your
coefficients.”</li>
<li><strong>If <span class="math inline">\(s\)</span> is <em>very
large</em>:</strong> The budget is huge. This constraint does nothing.
You get the standard OLS solution.</li>
<li><strong>If <span class="math inline">\(s\)</span> is
<em>small</em>:</strong> The budget is tight. You <em>must</em> shrink
your coefficients to stay under the budget <span
class="math inline">\(s\)</span>. To get the best fit, the model will be
forced to set unimportant coefficients to 0 and only “spend” its budget
on the most important variables.</li>
</ul></li>
</ul>
<p><strong>The Equivalence:</strong> These two forms are equivalent. For
any <span class="math inline">\(\lambda\)</span> you pick, there’s a
corresponding budget <span class="math inline">\(s\)</span> that gives
the <em>exact same solution</em>.</p>
<ul>
<li>High <span class="math inline">\(\lambda\)</span> (strong penalty)
<span class="math inline">\(\iff\)</span> Small <span
class="math inline">\(s\)</span> (tight budget)</li>
<li>Low <span class="math inline">\(\lambda\)</span> (weak penalty)
<span class="math inline">\(\iff\)</span> Large <span
class="math inline">\(s\)</span> (loose budget)</li>
</ul>
<p>This equivalence is why you see plots with both <span
class="math inline">\(\lambda\)</span> and <span
class="math inline">\(L_1\)</span> Norm on the x-axis. They are just two
different ways of looking at the same “penalty” spectrum.</p>
<h2 id="detailed-plot-code-analysis">Detailed Plot &amp; Code
Analysis</h2>
<p>Let’s look at the plots and code, which answer the practical
questions: <strong>(1)</strong> How do we pick the <em>best</em> <span
class="math inline">\(\lambda\)</span>? and <strong>(2)</strong> What
does LASSO <em>do</em> to the coefficients?</p>
<h3 id="question-1-how-to-pick-the-best-lambda-slide-5">Question 1: How
to pick the best <span class="math inline">\(\lambda\)</span>? (Slide
5)</h3>
<p>This is the <strong>Cross-Validation (CV) Plot</strong>. Its one and
only job is to help you find the optimal <span
class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><strong>R Code:</strong>
<code>cv.out=cv.glmnet(x[train,],y[train],alpha=1)</code>
<ul>
<li><code>cv.glmnet</code>: This R function <em>automatically</em> does
K-fold cross-validation. <code>alpha=1</code> explicitly tells it to use
<strong>LASSO</strong> (alpha=0 would be Ridge).</li>
<li>It tries a whole range of <span
class="math inline">\(\lambda\)</span> values, calculates the
Mean-Squared Error (MSE) for each, and stores the results in
<code>cv.out</code>.</li>
</ul></li>
<li><strong>Plot Analysis:</strong>
<ul>
<li><strong>X-axis:</strong> <span
class="math inline">\(\text{Log}(\lambda)\)</span>. The penalty
strength. <strong>Right = High Penalty</strong> (simple model),
<strong>Left = Low Penalty</strong> (complex model).</li>
<li><strong>Y-axis:</strong> Mean-Squared Error (MSE). <strong>Lower is
better.</strong></li>
<li><strong>Red Dots:</strong> The average MSE for each <span
class="math inline">\(\lambda\)</span>.</li>
<li><strong>Gray Bars:</strong> The error bars (standard error).</li>
<li><strong>The “U” Shape:</strong> This is the classic
<strong>bias-variance trade-off</strong>.
<ul>
<li><strong>Right Side (High <span
class="math inline">\(\lambda\)</span>):</strong> The model is <em>too
simple</em> (too many coefficients are 0). It’s “underfitting.” The
error is high (high bias).</li>
<li><strong>Left Side (High <span
class="math inline">\(\lambda\)</span>):</strong> The model is <em>too
complex</em> (low penalty, like OLS). It’s “overfitting” the training
data. The error on new data is high (high variance).</li>
<li><strong>Bottom of the “U”:</strong> This is the “sweet spot.” The
<span class="math inline">\(\lambda\)</span> at the very bottom (marked
by the left vertical dotted line) gives the <strong>lowest possible
MSE</strong>. This is <code>lambda.min</code>.</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Answer:</strong> You pick the <span
class="math inline">\(\lambda\)</span> that corresponds to the lowest
point on this graph.</p>
<h3 id="question-2-what-does-lasso-do-slides-5-6-7">Question 2: What
does LASSO <em>do</em>? (Slides 5, 6, 7)</h3>
<p>These slides all show the <em>effect</em> of LASSO.</p>
<p><strong>A. The Coefficient Path Plots (Slides 5 &amp; 6)</strong></p>
<p>These plots visualize how coefficients change. They show the <em>same
information</em> just with different x-axes.</p>
<ul>
<li><strong>Left Plot (Slide 6) vs. <span
class="math inline">\(\lambda\)</span>:</strong>
<ul>
<li><strong>How to read:</strong> Read from <strong>RIGHT to
LEFT</strong>.</li>
<li>At the far right (<span class="math inline">\(\lambda\)</span> is
large), all coefficients are 0.</li>
<li>As you move left, <span class="math inline">\(\lambda\)</span> gets
smaller, and the penalty is relaxed. Variables “enter” the model one by
one as their coefficients become non-zero.</li>
<li>You can see ‘Rating’ (red-dashed), ‘Student’ (black-solid), and
‘Income’ (blue-dotted) are the first to enter, suggesting they are the
most important predictors.</li>
</ul></li>
<li><strong>Right Plot (Slide 6) vs. <span
class="math inline">\(L_1\)</span> Norm Ratio:</strong>
<ul>
<li>This is the <em>same plot</em>, just flipped and rescaled. The
x-axis is <span class="math inline">\(\|\hat{\beta}_\lambda\|_1 /
\|\hat{\beta}_{OLS}\|_1\)</span>.</li>
<li><strong>How to read:</strong> Read from <strong>LEFT to
RIGHT</strong>.</li>
<li><strong>At 0.0:</strong> This is a “0% budget” (like <span
class="math inline">\(s=0\)</span> or <span
class="math inline">\(\lambda=\infty\)</span>). All coefficients are
0.</li>
<li><strong>At 1.0:</strong> This is a “100% budget” (like <span
class="math inline">\(s=\infty\)</span> or <span
class="math inline">\(\lambda=0\)</span>). This is the full OLS
model.</li>
<li>This view clearly shows the coefficients “growing” from 0 as their
“budget” (<span class="math inline">\(L_1\)</span> Norm) increases.</li>
</ul></li>
</ul>
<p><strong>B. The Code Output (Slide 7) - This is the most important
“answer”</strong></p>
<p>This slide <em>explicitly demonstrates</em> variable selection by
comparing the coefficients from two different <span
class="math inline">\(\lambda\)</span> values.</p>
<ul>
<li><p><strong>First Block (The “Optimal” Model):</strong></p>
<ul>
<li><code>bestlam.cv &lt;- cv.out$lambda.min</code>: This gets the <span
class="math inline">\(\lambda\)</span> from the bottom of the “U” in the
CV plot.</li>
<li><code>lasso.conf &lt;- predict(out,type="coefficients",s=bestlam.cv)[1:12,]</code>:
This gets the coefficients using that <em>best</em> <span
class="math inline">\(\lambda\)</span>.</li>
<li><code>lasso.conf[lasso.conf!=0]</code>: This R command filters the
list to show <em>only the non-zero coefficients</em>.</li>
<li><strong>Result:</strong> The optimal model <em>still keeps 10
variables</em> (‘Income’, ‘Limit’, ‘Rating’, etc.). It has shrunk them,
but it hasn’t set many to 0.</li>
</ul></li>
<li><p><strong>Second Block (The “High Penalty” Model):</strong></p>
<ul>
<li>The slide text says “if we choose a larger regularization
parameter.” Here, they’ve picked an arbitrary <em>larger</em> value,
<code>s=10</code>. (Note: R’s <code>predict.glmnet</code> can be
confusing; <code>s=10</code> here means <span
class="math inline">\(\lambda=10\)</span>).</li>
<li><code>lasso.conf &lt;- predict(out,type="coefficients",s=10)[1:12,]</code>:
This gets the coefficients using a <em>stronger penalty</em> (<span
class="math inline">\(\lambda=10\)</span>).</li>
<li><code>lasso.conf[lasso.conf!=0]</code>: Again, show only the
non-zero coefficients.</li>
<li><strong>Result:</strong> Look! The list is much shorter. The
coefficients for ‘Age’, ‘Education’, ‘GenderFemale’, ‘MarriedYes’, and
‘Ethnicity’ are <em>all gone</em> (shrunk to 0.000000). The model has
decided these are not important enough to “spend” budget on.</li>
</ul></li>
</ul>
<p><strong>Conclusion:</strong> LASSO performs <strong>automatic
variable selection</strong>. By increasing <span
class="math inline">\(\lambda\)</span>, you create a
<strong>sparser</strong> (simpler) model. Slide 7 is the concrete
proof.</p>
<h2 id="python-equivalents-in-more-detail">Python Equivalents (in more
detail)</h2>
<p>Here is how you would replicate the <em>entire</em> workflow from the
slides in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso, LassoCV, lasso_path</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Assume X_train, y_train, X_test, y_test are loaded ---</span></span><br><span class="line"><span class="comment"># Example: </span></span><br><span class="line"><span class="comment"># data = pd.read_csv(&#x27;Credit.csv&#x27;)</span></span><br><span class="line"><span class="comment"># X = pd.get_dummies(data.drop([&#x27;ID&#x27;, &#x27;Balance&#x27;], axis=1), drop_first=True)</span></span><br><span class="line"><span class="comment"># y = data[&#x27;Balance&#x27;]</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It&#x27;s CRITICAL to scale data before regularization</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train_scaled = scaler.fit_transform(X_train)</span><br><span class="line">X_test_scaled = scaler.transform(X_test)</span><br><span class="line">feature_names = X.columns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Replicate the CV Plot (Slide 5: ...000200.png)</span></span><br><span class="line"><span class="comment"># LassoCV does what cv.glmnet does: finds the best lambda (alpha)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Running LassoCV to find best lambda (alpha)...&quot;</span>)</span><br><span class="line"><span class="comment"># &#x27;alphas&#x27; is the list of lambdas to try. We can let it choose automatically.</span></span><br><span class="line"><span class="comment"># cv=10 means 10-fold cross-validation.</span></span><br><span class="line">lasso_cv = LassoCV(cv=<span class="number">10</span>, random_state=<span class="number">1</span>, max_iter=<span class="number">10000</span>)</span><br><span class="line">lasso_cv.fit(X_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The best lambda found</span></span><br><span class="line">best_lambda = lasso_cv.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha) found: <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Plotting the CV (MSE vs. Log(Lambda)) ---</span></span><br><span class="line"><span class="comment"># This recreates the R plot</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"><span class="comment"># lasso_cv.mse_path_ is a (n_alphas, n_folds) array of MSEs</span></span><br><span class="line"><span class="comment"># We take the mean across the folds (axis=1)</span></span><br><span class="line">mean_mses = np.mean(lasso_cv.mse_path_, axis=<span class="number">1</span>)</span><br><span class="line">log_lambdas = np.log10(lasso_cv.alphas_)</span><br><span class="line"></span><br><span class="line">plt.plot(log_lambdas, mean_mses, <span class="string">&#x27;r.-&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Log(Lambda / Alpha)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Mean-Squared Error&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;LASSO Cross-Validation Path (Replicating R Plot)&#x27;</span>)</span><br><span class="line"><span class="comment"># Plot a vertical line at the best lambda</span></span><br><span class="line">plt.axvline(np.log10(best_lambda), linestyle=<span class="string">&#x27;--&#x27;</span>, color=<span class="string">&#x27;k&#x27;</span>, label=<span class="string">f&#x27;Best Lambda (alpha) = <span class="subst">&#123;best_lambda:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.gca().invert_xaxis() <span class="comment"># High lambda is on the right in R plot</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Replicate the Coefficient Path Plot (Slide 6: ...000206.png)</span></span><br><span class="line"><span class="comment"># We can use the lasso_path function, or just use the CV object</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The lasso_cv object already calculated the paths!</span></span><br><span class="line">coefs = lasso_cv.path(X_train_scaled, y_train, alphas=lasso_cv.alphas_)[<span class="number">1</span>].T</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X_train_scaled.shape[<span class="number">1</span>]):</span><br><span class="line">    plt.plot(log_lambdas, coefs[:, i], label=feature_names[i])</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Log(Lambda / Alpha)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Standardized Coefficients&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;LASSO Coefficient Path (Replicating R Plot)&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">plt.gca().invert_xaxis()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Replicate the Code Output (Slide 7: ...000202.png)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Replicating R Output ---&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- First Block: Coefficients with BEST lambda ---</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Coefficients using best lambda (alpha = <span class="subst">&#123;best_lambda:<span class="number">.4</span>f&#125;</span>):&quot;</span>)</span><br><span class="line"><span class="comment"># The lasso_cv object is already fitted with the best lambda</span></span><br><span class="line">best_coefs = lasso_cv.coef_</span><br><span class="line">coef_series_best = pd.Series(best_coefs, index=feature_names)</span><br><span class="line"><span class="comment"># This is like R&#x27;s `lasso.conf[lasso.conf != 0]`</span></span><br><span class="line"><span class="built_in">print</span>(coef_series_best[coef_series_best != <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Second Block: Coefficients with a LARGER lambda ---</span></span><br><span class="line"><span class="comment"># Let&#x27;s pick a larger lambda, e.g., 10 (like the slide)</span></span><br><span class="line">large_lambda = <span class="number">10</span> </span><br><span class="line">lasso_high_penalty = Lasso(alpha=large_lambda)</span><br><span class="line">lasso_high_penalty.fit(X_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nCoefficients using larger lambda (alpha = <span class="subst">&#123;large_lambda&#125;</span>):&quot;</span>)</span><br><span class="line">high_pen_coefs = lasso_high_penalty.coef_</span><br><span class="line">coef_series_high = pd.Series(high_pen_coefs, index=feature_names)</span><br><span class="line"><span class="comment"># This is the second R command: `lasso.conf[lasso.conf != 0]`</span></span><br><span class="line"><span class="built_in">print</span>(coef_series_high[coef_series_high != <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Final Prediction ---</span></span><br><span class="line"><span class="comment"># This is R&#x27;s `mean((lasso.pred-y.test)^2)`</span></span><br><span class="line">y_pred = lasso_cv.predict(X_test_scaled)</span><br><span class="line">test_mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nTest MSE using best lambda: <span class="subst">&#123;test_mse:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="the-game-of-regularization">The “Game” of Regularization</h3>
<p>First, let’s understand what these plots are showing. This is a “map”
of a constrained optimization problem.</p>
<ul>
<li><strong>The Red Ellipses (RSS Contours):</strong> Think of these as
contour lines on a topographic map.
<ul>
<li><strong>The Center (<span
class="math inline">\(\hat{\beta}\)</span>):</strong> This point is the
“bottom of the valley.” It represents the <em>perfect</em>,
unconstrained solution—the standard Ordinary Least Squares (OLS)
coefficients. This point has the lowest possible Residual Sum of Squares
(RSS), or error.</li>
<li><strong>The Lines:</strong> Every point on a single red ellipse has
the <em>exact same</em> RSS. As the ellipses get bigger (moving away
from the center <span class="math inline">\(\hat{\beta}\)</span>), the
error gets higher.</li>
</ul></li>
<li><strong>The Blue Shaded Area (Constraint Region):</strong> This is
the “rule” of the game.
<ul>
<li>This is our “budget.” We are <em>only allowed</em> to pick a
solution (<span class="math inline">\(\beta_1, \beta_2\)</span>) from
<em>inside or on the boundary</em> of this blue shape.</li>
<li><strong>LASSO:</strong> The constraint is <span
class="math inline">\(|\beta_1| + |\beta_2| \le s\)</span>. This
equation forms a <strong>diamond</strong> (or a rotated square).</li>
<li><strong>Ridge:</strong> The constraint is <span
class="math inline">\(\beta_1^2 + \beta_2^2 \le s\)</span>. This
equation forms a <strong>circle</strong>.</li>
</ul></li>
<li><strong>The Goal:</strong> Find the “best” point that is <em>inside
the blue area</em>.
<ul>
<li>The “best” point is the one with the lowest possible error
(RSS).</li>
<li>Geometrically, this means we start at the center (<span
class="math inline">\(\hat{\beta}\)</span>) and expand our ellipse
outward. The <em>very first point</em> where the ellipse
<strong>touches</strong> the blue constraint region is our
solution.</li>
</ul></li>
</ul>
<h3 id="why-lasso-performs-variable-selection-the-diamond">Why LASSO
Performs Variable Selection (The Diamond) 🎯</h3>
<p>This is the most important concept. Look at the LASSO diagrams.</p>
<ul>
<li><strong>The Shape:</strong> The LASSO constraint is a
<strong>diamond</strong>.</li>
<li><strong>The Key Feature:</strong> This diamond has <strong>sharp
corners</strong> (vertices). And most importantly, these corners lie
<strong>exactly on the axes</strong>.
<ul>
<li>The top corner is at <span class="math inline">\((\beta_1=0,
\beta_2=s)\)</span>.</li>
<li>The right corner is at <span class="math inline">\((\beta_1=s,
\beta_2=0)\)</span>.</li>
</ul></li>
<li><strong>The “Collision”:</strong> Now, imagine the red ellipses
(representing our error) expanding from the OLS solution (<span
class="math inline">\(\hat{\beta}\)</span>). They will almost always
“hit” the blue diamond at one of its <strong>sharp corners</strong>.
<ul>
<li>Look at your textbook diagram (slide <code>...000304.png</code>).
The ellipse clearly makes contact with the diamond at the top corner,
where <span class="math inline">\(\beta_1 = 0\)</span>.</li>
<li>Look at your example (slide <code>...000259.jpg</code>). The center
of the ellipses is at (4, 0.1). The closest point on the diamond that
the expanding ellipses will hit is the corner at (2, 0). At this
solution, <strong><span class="math inline">\(y\)</span> is exactly
0</strong>.</li>
</ul></li>
</ul>
<p><strong>Conclusion:</strong> Because the <span
class="math inline">\(L_1\)</span> “diamond” has corners on the axes,
the optimal solution is very likely to land on one of them. When it
does, the coefficient for the <em>other</em> axis is set to
<strong>exactly zero</strong>. This is the <strong>variable selection
property</strong>.</p>
<h3 id="why-ridge-regression-only-shrinks-the-circle">Why Ridge
Regression Only Shrinks (The Circle) 🤏</h3>
<p>Now, look at the Ridge regression diagram.</p>
<ul>
<li><strong>The Shape:</strong> The Ridge constraint is a
<strong>circle</strong>.</li>
<li><strong>The Key Feature:</strong> A circle is perfectly smooth and
has <strong>no corners</strong>.</li>
<li><strong>The “Collision”:</strong> Imagine the same ellipses
expanding and hitting the blue circle. The contact point will be a
<em>tangent</em> point.
<ul>
<li>Because the circle is round, this tangent point can be
<em>anywhere</em> on its circumference.</li>
<li>It is <em>extremely unlikely</em> that the contact point will be
exactly on an axis (e.g., at <span class="math inline">\((\beta_1=0,
\beta_2=s)\)</span>). This would only happen if the OLS solution <span
class="math inline">\(\hat{\beta}\)</span> was <em>already</em>
perfectly aligned with that axis.</li>
</ul></li>
<li><strong>Conclusion:</strong> The Ridge solution will find a point
where <em>both</em> <span class="math inline">\(\beta_1\)</span> and
<span class="math inline">\(\beta_2\)</span> are non-zero. The
coefficients are “shrunk” (pulled in from <span
class="math inline">\(\hat{\beta}\)</span> towards the origin), but they
<strong>never become zero</strong>. This is why Ridge is called a
“shrinkage” method, but not a “variable selection” method.</li>
</ul>
<h3 id="summary-diamond-vs.-circle">Summary: Diamond vs. Circle</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">LASSO (<span
class="math inline">\(L_1\)</span> Norm)</th>
<th style="text-align: left;">Ridge (<span
class="math inline">\(L_2\)</span> Norm)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Constraint Shape</strong></td>
<td style="text-align: left;"><strong>Diamond</strong> (or
hyper-rhombus)</td>
<td style="text-align: left;"><strong>Circle</strong> (or
hypersphere)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Key Feature</strong></td>
<td style="text-align: left;"><strong>Sharp corners</strong> on the
axes</td>
<td style="text-align: left;"><strong>Smooth curve</strong> with no
corners</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Geometric Solution</strong></td>
<td style="text-align: left;">Ellipses hit the
<strong>corners</strong></td>
<td style="text-align: left;">Ellipses hit a <strong>smooth
part</strong></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Result</strong></td>
<td style="text-align: left;">Forces some coefficients to
<strong>exactly 0</strong></td>
<td style="text-align: left;">Shrinks all coefficients <em>towards</em>
0</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Name</strong></td>
<td style="text-align: left;"><strong>Variable Selection</strong></td>
<td style="text-align: left;"><strong>Shrinkage</strong></td>
</tr>
</tbody>
</table>
<p>The “space meaning” is that the <strong>sharp corners of the <span
class="math inline">\(L_1\)</span> diamond are what make variable
selection possible</strong>. The smooth circle of the <span
class="math inline">\(L_2\)</span> norm does not have these corners and
thus cannot force coefficients to zero.</p>
<h1 id="shrinkage-methods-lasso-vs.-ridge">8. Shrinkage Methods (Lasso
vs. Ridge)</h1>
<h2 id="core-concept-shrinkage-methods">Core Concept: Shrinkage
Methods</h2>
<p>Both <strong>Ridge (L2)</strong> and <strong>Lasso (L1)</strong> are
regularization techniques used to improve upon standard <strong>Ordinary
Least Squares (OLS)</strong> regression.</p>
<p>Their main goal is to manage the <strong>bias-variance
tradeoff</strong>. OLS often has low bias but very high variance,
especially when you have many predictors (<span
class="math inline">\(p\)</span>) or when predictors are correlated.
Ridge and Lasso improve prediction accuracy by <em>shrinking</em> the
regression coefficients towards zero. This adds a small amount of bias
but significantly <em>reduces</em> the variance, leading to a lower
overall Test Mean Squared Error (MSE).</p>
<h2 id="the-key-difference-math-how-they-shrink">The Key Difference:
Math &amp; How They Shrink</h2>
<p>The slides show that the two methods use different penalties, which
leads to very different mathematical forms and practical outcomes.</p>
<ul>
<li><strong>Ridge Regression (L2 Penalty):</strong> Minimizes <span
class="math inline">\(RSS + \lambda \sum_{j=1}^{p}
\beta_j^2\)</span></li>
<li><strong>Lasso Regression (L1 Penalty):</strong> Minimizes <span
class="math inline">\(RSS + \lambda \sum_{j=1}^{p}
|\beta_j|\)</span></li>
</ul>
<p>Slide 80 provides the exact formulas for their coefficient estimates
in a simple, orthogonal case (where predictors are independent):</p>
<h3 id="ridge-regression-proportional-shrinkage">Ridge Regression
(Proportional Shrinkage)</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\beta}_j^R = \hat{\beta}_j^{LSE} / (1 +
\lambda)\)</span></li>
<li><strong>What this means:</strong> Ridge <em>shrinks</em> every least
squares coefficient by a proportional amount. It will make coefficients
<em>smaller</em>, but it will <strong>never set them to exactly
zero</strong> (unless <span class="math inline">\(\lambda\)</span> is
<span class="math inline">\(\infty\)</span>).</li>
</ul>
<h3 id="lasso-regression-soft-thresholding">Lasso Regression
(Soft-Thresholding)</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\beta}_j^L =
\text{sign}(\hat{\beta}_j^{LSE})(|\hat{\beta}_j^{LSE}| -
\lambda/2)_+\)</span></li>
<li><strong>What this means:</strong> This is a “soft-thresholding”
operator.
<ul>
<li>If the original coefficient <span
class="math inline">\(\hat{\beta}_j^{LSE}\)</span> is small (its
absolute value is less than <span
class="math inline">\(\lambda/2\)</span>), Lasso <strong>sets it to
exactly zero</strong>.</li>
<li>If the coefficient is large, Lasso subtracts <span
class="math inline">\(\lambda/2\)</span> from its absolute value,
shrinking it towards zero.</li>
</ul></li>
<li><strong>Key Property:</strong> Because of this, Lasso performs
<strong>automatic feature selection</strong> by eliminating
predictors.</li>
</ul>
<h2 id="important-images-explained">Important Images Explained</h2>
<h3 id="most-important-figure-6.10-slide-82">Most Important: Figure 6.10
(Slide 82)</h3>
<p>This is the best visual for understanding the <em>mathematical
difference</em> from the formulas above.</p>
<ul>
<li><strong>Left (Ridge):</strong> The red line shows the Ridge estimate
vs. the OLS estimate. It’s a straight, diagonal line with a slope less
than 1. It shrinks everything <em>proportionally</em>.</li>
<li><strong>Right (Lasso):</strong> The red line shows the Lasso
estimate. It’s “flat” at zero for a range, showing it <strong>sets small
coefficients to zero</strong>. Then, it slopes up, but it’s shifted (it
shrinks the large coefficients by a fixed amount).</li>
</ul>
<h3 id="scenario-1-figure-6.8-slide-76">Scenario 1: Figure 6.8 (Slide
76)</h3>
<p>This plot shows what happens when <strong>all 45 predictors are truly
related to the response</strong>.</p>
<ul>
<li><strong>Result (Slide 77):</strong> <strong>Ridge performs slightly
better</strong> (has a lower minimum MSE, shown by the dotted purple
line).</li>
<li><strong>Why:</strong> Lasso’s assumption (that some coefficients are
zero) is <em>wrong</em> in this case. By forcing some relevant
predictors to zero, it adds too much bias. Ridge, by just
<em>shrinking</em> all of them, finds a better balance.</li>
</ul>
<h3 id="scenario-2-figure-6.9-slide-78">Scenario 2: Figure 6.9 (Slide
78)</h3>
<p>This plot shows the <em>opposite</em> scenario: <strong>only 2 out of
45 predictors are truly related</strong> (a “sparse” model).</p>
<ul>
<li><strong>Result:</strong> <strong>Lasso performs much better</strong>
(its solid purple line has a much lower minimum MSE).</li>
<li><strong>Why:</strong> Lasso’s assumption is <em>correct</em>. It
successfully sets the 43 “noise” predictors to zero, which dramatically
reduces variance, while correctly keeping the 2 important ones.</li>
</ul>
<h2 id="python-code-understanding-1">Python &amp; Code
Understanding</h2>
<p>The slides don’t contain Python code, but they describe the exact
concepts you would use, primarily in <code>scikit-learn</code>.</p>
<ul>
<li><p><strong>Implementing Ridge &amp; Lasso:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, Lasso, RidgeCV, LassoCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># It&#x27;s crucial to scale data before regularization</span></span><br><span class="line"><span class="comment"># alpha is the same as the λ (lambda) in your slides</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Ridge ---</span></span><br><span class="line"><span class="comment"># The math for Ridge is a &quot;closed-form solution&quot; (Slide 80)</span></span><br><span class="line"><span class="comment"># ridge_model = make_pipeline(StandardScaler(), Ridge(alpha=1.0))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Lasso ---</span></span><br><span class="line"><span class="comment"># Lasso requires a numerical solver (like coordinate descent)</span></span><br><span class="line"><span class="comment"># lasso_model = make_pipeline(StandardScaler(), Lasso(alpha=0.1))</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>The Soft-Thresholding Formula:</strong> The math from
Slide 80, <span class="math inline">\(\text{sign}(y)(|y| -
\lambda/2)_+\)</span>, is the core operation in the “coordinate descent”
algorithm used to solve Lasso. You could write it in Python/Numpy:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">soft_threshold</span>(<span class="params">x, lambda_val</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements the Lasso soft-thresholding formula.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">return</span> np.sign(x) * np.maximum(<span class="number">0</span>, np.<span class="built_in">abs</span>(x) - (lambda_val / <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example:</span></span><br><span class="line"><span class="comment"># ols_coefficient = 1.5</span></span><br><span class="line"><span class="comment"># threshold = 4.0</span></span><br><span class="line"><span class="comment"># lasso_coefficient = soft_threshold(ols_coefficient, threshold) </span></span><br><span class="line"><span class="comment"># print(lasso_coefficient) # Output: 0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ols_coefficient = 3.0</span></span><br><span class="line"><span class="comment"># threshold = 4.0</span></span><br><span class="line"><span class="comment"># lasso_coefficient = soft_threshold(ols_coefficient, threshold) </span></span><br><span class="line"><span class="comment"># print(lasso_coefficient) # Output: 1.0 (it was 3.0, shrunk by 4/2 = 2)</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>Choosing <span class="math inline">\(\lambda\)</span>
(alpha):</strong> Slide 79 says to “Use cross validation to determine
which one has better prediction.” In <code>scikit-learn</code>, this is
done for you with <code>RidgeCV</code> and <code>LassoCV</code>, which
automatically test a range of <code>alpha</code> values.</p></li>
</ul>
<h2 id="summary-lasso-vs.-ridge">Summary: Lasso vs. Ridge</h2>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Ridge (L2)</th>
<th style="text-align: left;">Lasso (L1)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Penalty</strong></td>
<td style="text-align: left;"><span class="math inline">\(L_2\)</span>
norm: <span class="math inline">\(\lambda \sum \beta_j^2\)</span></td>
<td style="text-align: left;"><span class="math inline">\(L_1\)</span>
norm: <span class="math inline">\(\lambda \sum |\beta_j|\)</span></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Coefficient
Shrinkage</strong></td>
<td style="text-align: left;">Proportional; shrinks all coefficients,
but never to <em>exactly</em> zero.</td>
<td style="text-align: left;">Soft-thresholding; can force coefficients
to be <em>exactly</em> zero.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Feature Selection?</strong></td>
<td style="text-align: left;">No</td>
<td style="text-align: left;"><strong>Yes</strong>, this is its main
advantage.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Interpretability</strong></td>
<td style="text-align: left;">Less interpretable (keeps all <span
class="math inline">\(p\)</span> variables).</td>
<td style="text-align: left;">More interpretable (produces a “sparse”
model with fewer variables).</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Best Used When…</strong></td>
<td style="text-align: left;">…most predictors are useful. (e.g., Slide
76: 45/45 relevant).</td>
<td style="text-align: left;">…many predictors are “noise” and only a
few are strong. (e.g., Slide 78: 2/45 relevant).</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Computation</strong></td>
<td style="text-align: left;">Has a simple, closed-form solution.</td>
<td style="text-align: left;">Requires numerical optimization (e.g.,
coordinate descent).</td>
</tr>
</tbody>
</table>
<h1 id="shrinkage-methods-ridge-lasso">9. Shrinkage Methods (Ridge &amp;
LASSO)</h1>
<h2 id="summary-of-shrinkage-methods-ridge-lasso">Summary of Shrinkage
Methods (Ridge &amp; LASSO)</h2>
<p>These slides introduce <strong>shrinkage methods</strong>, also known
as <strong>regularization</strong>, a technique used in regression (like
linear regression) to improve model performance. The main idea is to add
a <em>penalty</em> to the model’s loss function to “shrink” the size of
the coefficients. This helps to reduce model variance and prevent
overfitting, especially when you have many features.</p>
<p>The two main methods discussed are <strong>Ridge Regression</strong>
(<span class="math inline">\(L_2\)</span> penalty) and
<strong>LASSO</strong> (<span class="math inline">\(L_1\)</span>
penalty).</p>
<h2 id="key-mathematical-formulas-1">Key Mathematical Formulas</h2>
<ol type="1">
<li><p><strong>Standard Linear Model:</strong> The problem starts with
the standard linear regression model (from slide 1):</p>
<p><span class="math display">\[
\]</span>$$\mathbf{y} = \mathbf{X}\beta + \epsilon</p>
<p><span class="math display">\[
\]</span>$$ * <span class="math inline">\(\mathbf{y}\)</span> is the
<span class="math inline">\(n \times 1\)</span> vector of observed
outcomes.</p>
<ul>
<li><span class="math inline">\(\mathbf{X}\)</span> is the <span
class="math inline">\(n \times p\)</span> matrix of <span
class="math inline">\(p\)</span> predictor features for <span
class="math inline">\(n\)</span> observations.</li>
<li><span class="math inline">\(\beta\)</span> is the <span
class="math inline">\(p \times 1\)</span> vector of coefficients (what
we want to find).</li>
<li><span class="math inline">\(\epsilon\)</span> is the <span
class="math inline">\(n \times 1\)</span> vector of random errors.</li>
<li>The goal of standard “Ordinary Least Squares” (OLS) regression is to
find the <span class="math inline">\(\beta\)</span> that minimizes the
loss: <span class="math inline">\(\|\mathbf{X}\beta -
\mathbf{y}\|^2_2\)</span>.</li>
</ul></li>
<li><p><strong>LASSO (L1 Regularization):</strong> LASSO (Least Absolute
Shrinkage and Selection Operator) adds a penalty based on the
<em>absolute value</em> of the coefficients (the <span
class="math inline">\(L_1\)</span>-norm). This is the key formula from
slide 1:</p>
<p><span class="math display">\[
\]</span>$$\hat{\beta}(\lambda) \leftarrow \arg \min_{\beta} \left(
|\mathbf{X}\beta - \mathbf{y}|^2_2 + \lambda|\beta|_1 \right)</p>
<p><span class="math display">\[
\]</span>$$ * <span class="math inline">\(\|\beta\|_1 = \sum_{j=1}^{p}
|\beta_j|\)</span></p>
<ul>
<li><span class="math inline">\(\lambda\)</span> (lambda) is the
<strong>tuning parameter</strong> that controls the strength of the
penalty. A larger <span class="math inline">\(\lambda\)</span> means
more shrinkage.</li>
<li><strong>Key Property (Variable Selection):</strong> The <span
class="math inline">\(L_1\)</span> penalty can force some coefficients
(<span class="math inline">\(\beta_j\)</span>) to become <strong>exactly
zero</strong>. This means LASSO simultaneously performs <em>feature
selection</em> by automatically removing irrelevant predictors.</li>
<li><strong>Support (Slide 1):</strong> The question “Can it recover the
support of <span class="math inline">\(\beta\)</span>?” is asking if
LASSO can correctly identify the set of true non-zero coefficients
(defined as <span class="math inline">\(S := \{j : \beta_j \neq
0\}\)</span>).</li>
</ul></li>
<li><p><strong>Ridge Regression (L2 Regularization):</strong> Ridge
regression (mentioned on slide 2, shown on slide 3) adds a penalty based
on the <em>squared value</em> of the coefficients (the <span
class="math inline">\(L_2\)</span>-norm).</p>
<p><span class="math display">\[
\]</span>$$\hat{\beta}(\lambda) \leftarrow \arg \min_{\beta} \left(
|\mathbf{X}\beta - \mathbf{y}|^2_2 + \lambda|\beta|^2_2 \right)</p>
<p><span class="math display">\[
\]</span>$$ * <span class="math inline">\(\|\beta\|^2_2 = \sum_{j=1}^{p}
\beta_j^2\)</span></p>
<ul>
<li><strong>Key Property (Shrinkage):</strong> The <span
class="math inline">\(L_2\)</span> penalty <em>shrinks</em> coefficients
<em>towards</em> zero but <strong>never</strong> sets them to
<em>exactly</em> zero (unless <span class="math inline">\(\lambda =
\infty\)</span>). It is effective at handling multicollinearity.</li>
</ul></li>
</ol>
<h2 id="important-images-concepts">Important Images &amp; Concepts</h2>
<p>The most important images are the plots from slides 3 and 4. They
illustrate the two most critical concepts: <strong>how to choose <span
class="math inline">\(\lambda\)</span></strong> and <strong>what the
penalty does to the coefficients</strong>.</p>
<h3 id="tuning-parameter-selection-slides-3-4-left-plots">Tuning
Parameter Selection (Slides 3 &amp; 4, Left Plots)</h3>
<ul>
<li><strong>Problem:</strong> How do you find the <em>best</em> value
for <span class="math inline">\(\lambda\)</span>?</li>
<li><strong>Solution:</strong> <strong>Cross-Validation (CV)</strong>.
The slides show 10-fold CV.</li>
<li><strong>What the Plots Show:</strong> The left plots on slides 3 and
4 show the <strong>Cross-Validation Error</strong> (like MSE) for
different values of the penalty.
<ul>
<li>The x-axis represents the penalty strength (either <span
class="math inline">\(\lambda\)</span> itself or a related measure like
the shrinkage ratio <span
class="math inline">\(\|\hat{\beta}_\lambda\|_1 /
\|\hat{\beta}\|_1\)</span>).</li>
<li>The y-axis is the prediction error.</li>
<li>The curve is typically <strong>U-shaped</strong>. The vertical
dashed line marks the <strong>minimum</strong> of this curve. This
minimum point corresponds to the <strong>optimal <span
class="math inline">\(\lambda\)</span></strong>, which provides the best
balance between bias and variance, leading to the best-performing model
on unseen data.</li>
</ul></li>
</ul>
<h3 id="coefficient-paths-slides-3-4-right-plots">Coefficient Paths
(Slides 3 &amp; 4, Right Plots)</h3>
<p>These “trace” plots are crucial for understanding the difference
between Ridge and LASSO. They show how the value of each coefficient
(y-axis) changes as the penalty strength (x-axis) changes.</p>
<ul>
<li><strong>Slide 3 (Ridge):</strong> As <span
class="math inline">\(\lambda\)</span> increases (moving right), all
coefficient values are smoothly shrunk <em>towards</em> zero, but none
of them actually hit zero.</li>
<li><strong>Slide 4 (LASSO):</strong> As the penalty increases (moving
from right to left, as the ratio <span class="math inline">\(s\)</span>
goes from 1.0 to 0.0), you can see coefficients “drop off” and become
<strong>exactly zero</strong> one by one. The model with the optimal
<span class="math inline">\(\lambda\)</span> (vertical line) has
selected only a few non-zero coefficients (the pink and teal lines),
while all the grey lines have been set to zero. This is <em>feature
selection</em> in action.</li>
</ul>
<h2 id="key-discussion-points-slide-2">Key Discussion Points (Slide
2)</h2>
<ul>
<li><strong>Non-linear models:</strong> You can apply these methods to
non-linear models by first creating non-linear features (e.g., <span
class="math inline">\(x_1^2\)</span>, <span
class="math inline">\(x_2^2\)</span>, <span class="math inline">\(x_1
\cdot x_2\)</span>) and then feeding them into a LASSO or Ridge model.
The regularization will then select which of these linear <em>or</em>
non-linear terms are important.</li>
<li><strong>Correlated Features (Multicollinearity):</strong> The
question “If <span class="math inline">\(x_j \approx x_k\)</span>, how
does LASSO behave?” is a key weakness of LASSO.
<ul>
<li><strong>LASSO:</strong> Tends to <em>arbitrarily</em> select one of
the correlated features and set the others to zero. This can make the
model unstable.</li>
<li><strong>Ridge:</strong> Tends to shrink the coefficients of
correlated features <em>together</em>, giving them similar (but smaller)
values.</li>
<li><strong>Elastic Net</strong> (not shown) is a hybrid of Ridge and
LASSO that is often used to get the best of both worlds: it can select
groups of correlated variables.</li>
</ul></li>
</ul>
<h2 id="python-code-understanding-using-scikit-learn">Python Code
Understanding (using <code>scikit-learn</code>)</h2>
<p>Here is how you would implement these concepts in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso, Ridge, LassoCV, RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Assume you have your data ---</span></span><br><span class="line"><span class="comment"># X: your feature matrix (e.g., shape 100, 20)</span></span><br><span class="line"><span class="comment"># y: your target vector (e.g., shape 100,)</span></span><br><span class="line"><span class="comment"># X, y = ... load your data ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. It&#x27;s crucial to scale your data before regularization</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Find the optimal lambda (alpha) using Cross-Validation</span></span><br><span class="line"><span class="comment"># scikit-learn uses &#x27;alpha&#x27; instead of &#x27;lambda&#x27; for the tuning parameter.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- For LASSO ---</span></span><br><span class="line"><span class="comment"># LassoCV automatically performs cross-validation (e.g., cv=10)</span></span><br><span class="line"><span class="comment"># to find the best alpha.</span></span><br><span class="line">lasso_cv_model = LassoCV(cv=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">lasso_cv_model.fit(X_scaled, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best alpha (lambda)</span></span><br><span class="line">best_alpha_lasso = lasso_cv_model.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal alpha (lambda) for LASSO: <span class="subst">&#123;best_alpha_lasso&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the final coefficients</span></span><br><span class="line">lasso_coeffs = lasso_cv_model.coef_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;LASSO coefficients: <span class="subst">&#123;lasso_coeffs&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># You will see that many of these are exactly 0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- For Ridge ---</span></span><br><span class="line"><span class="comment"># RidgeCV works similarly. It&#x27;s often good to test alphas on a log scale.</span></span><br><span class="line">ridge_alphas = np.logspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>) <span class="comment"># 100 values from 0.001 to 1000</span></span><br><span class="line">ridge_cv_model = RidgeCV(alphas=ridge_alphas, store_cv_values=<span class="literal">True</span>)</span><br><span class="line">ridge_cv_model.fit(X_scaled, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best alpha (lambda)</span></span><br><span class="line">best_alpha_ridge = ridge_cv_model.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal alpha (lambda) for Ridge: <span class="subst">&#123;best_alpha_ridge&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the final coefficients</span></span><br><span class="line">ridge_coeffs = ridge_cv_model.coef_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ridge coefficients: <span class="subst">&#123;ridge_coeffs&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># You will see these are small, but not exactly zero.</span></span><br></pre></td></tr></table></figure>
<h2 id="bias-variance-tradeoff">Bias-variance tradeoff</h2>
<h2 id="key-mathematical-formulas-concepts">Key Mathematical Formulas
&amp; Concepts</h2>
<h3 id="lasso-sign-consistency">LASSO: Sign Consistency</h3>
<p>This is the “ideal” scenario for LASSO. Sign consistency means that,
with enough data, the LASSO model not only selects the <em>correct</em>
set of features (it recovers the “support” <span
class="math inline">\(S\)</span>) but also correctly identifies the
<em>sign</em> (positive or negative) of their coefficients.</p>
<ul>
<li><p><strong>The Goal (Slide 1):</strong></p>
<p><span class="math display">\[
\]</span>$$\text{sign}(\hat{\beta}(\lambda)) = \text{sign}(\beta)</p>
<p><span class="math display">\[
\]</span>$$This means the signs of our <em>estimated</em> coefficients
<span class="math inline">\(\hat{\beta}(\lambda)\)</span> match the
signs of the <em>true</em> underlying coefficients <span
class="math inline">\(\beta\)</span>.</p></li>
<li><p><strong>The “Irrepresentable Condition” (Slide 1):</strong> This
is the mathematical guarantee required for LASSO to achieve sign
consistency.</p>
<p><span class="math display">\[
\]</span>$$|\mathbf{X}_{S<sup>c}</sup>\top \mathbf{X}_S
(\mathbf{X}_S^\top \mathbf{X}<em>S)^{-1}
\text{sign}(\beta_S)|</em>\infty &lt; 1</p>
<p><span class="math display">\[
\]</span>$$ * <strong>Plain English:</strong> This formula is a complex
way of saying: <strong>The irrelevant features (<span
class="math inline">\(\mathbf{X}_{S^c}\)</span>) cannot be too strongly
correlated with the true, relevant features (<span
class="math inline">\(\mathbf{X}_S\)</span>).</strong></p>
<ul>
<li>If an irrelevant feature is very similar (highly correlated) to a
true feature, LASSO can get “confused” and might pick the wrong one, or
its estimate will be unstable. This condition fails.</li>
</ul></li>
</ul>
<h3 id="ridge-regression-the-bias-variance-tradeoff">Ridge Regression:
The Bias-Variance Tradeoff</h3>
<ul>
<li><p><strong>The Formula (Slide 3):</strong></p>
<p><span class="math display">\[
\]</span>$$\hat{\beta}<em>{\text{ridge}}(\lambda) \leftarrow \arg
\min</em>{\beta} \left( |\mathbf{y} - \mathbf{X}\beta|^2 +
\lambda|\beta|^2 \right)</p>
<p><span class="math display">\[
\]</span>$$<em>(Note: This is the <span
class="math inline">\(L_2\)</span> penalty, so <span
class="math inline">\(\|\beta\|^2 = \sum
\beta_j^2\)</span>)</em></p></li>
<li><p><strong>The Problem it Solves: Collinearity (Slide 2)</strong>
When features are strongly correlated (e.g., <span
class="math inline">\(x_i \approx x_j\)</span>), regular methods
fail:</p>
<ul>
<li><strong>LSE (OLS):</strong> Fails because the matrix <span
class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> is
“non-invertible” (or singular), so the math for the solution <span
class="math inline">\(\hat{\beta} = (\mathbf{X}^\top
\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\)</span> breaks down.</li>
<li><strong>LASSO:</strong> Fails because the <strong>Irrepresentable
Condition</strong> is violated. LASSO will tend to <em>arbitrarily</em>
pick one of the correlated features and set the others to zero.</li>
</ul></li>
<li><p><strong>The Ridge Solution (Slide 3):</strong></p>
<ol type="1">
<li><strong>Always has a solution:</strong> Adding the <span
class="math inline">\(\lambda\)</span> penalty makes the matrix math
work, even if <span class="math inline">\(\mathbf{X}^\top
\mathbf{X}\)</span> is non-invertible.</li>
<li><strong>Groups variables:</strong> This is the key takeaway. Instead
of arbitrarily picking one feature, <strong>Ridge tends to shrink the
coefficients of collinear variables <em>together</em></strong>.</li>
<li><strong>Bias-Variance Tradeoff:</strong> Ridge <em>introduces
bias</em> into the estimates (they are “wrong” on purpose) to
<em>massively reduce variance</em> (they are more stable and less
sensitive to the specific training data). This trade-off usually leads
to a much lower overall error (Mean Squared Error).</li>
</ol></li>
</ul>
<h2 id="important-images-key-takeaways">Important Images &amp; Key
Takeaways</h2>
<ol type="1">
<li><p><strong>Slide 2 (Collinearity Failures):</strong> This is the
most important “problem” slide. It clearly explains <em>why</em> you
can’t always use standard LSE or LASSO. The fact that all three methods
(LSE, LASSO, Forward Selection) fail with strong collinearity motivates
the need for Ridge.</p></li>
<li><p><strong>Slide 3 (Ridge Properties):</strong> This is the most
important “solution” slide. The two most critical points are:</p>
<ul>
<li><code>Always unique solution for λ &gt; 0</code></li>
<li><code>Collinear variables tend to be grouped!</code> (This is the
“fix” for the problem on Slide 2).</li>
</ul></li>
</ol>
<h2 id="python-code-understanding-2">Python Code Understanding</h2>
<p>Let’s demonstrate the <strong>key difference</strong> (Slide 3) in
how LASSO and Ridge handle collinear features.</p>
<p>We will create two features, <code>x1</code> and <code>x2</code>,
that are nearly identical.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso, Ridge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create a dataset with 2 strongly correlated features</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line"><span class="comment"># x1: a standard feature</span></span><br><span class="line">x1 = np.random.randn(n_samples)</span><br><span class="line"><span class="comment"># x2: almost identical to x1</span></span><br><span class="line">x2 = x1 + <span class="number">0.01</span> * np.random.randn(n_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine into our feature matrix X</span></span><br><span class="line">X = np.c_[x1, x2]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y: The target variable (let&#x27;s say y = 2*x1 + 2*x2)</span></span><br><span class="line">y = <span class="number">2</span> * x1 + <span class="number">2</span> * x2 + np.random.randn(n_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit LASSO (alpha is the same as lambda)</span></span><br><span class="line"><span class="comment"># We use a moderate alpha</span></span><br><span class="line">lasso_model = Lasso(alpha=<span class="number">1.0</span>)</span><br><span class="line">lasso_model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit Ridge (alpha is the same as lambda)</span></span><br><span class="line">ridge_model = Ridge(alpha=<span class="number">1.0</span>)</span><br><span class="line">ridge_model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Compare the coefficients</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- Results for Correlated Features ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;True Coefficients: [2.0, 2.0]&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;LASSO Coefficients: <span class="subst">&#123;np.<span class="built_in">round</span>(lasso_model.coef_, <span class="number">2</span>)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ridge Coefficients: <span class="subst">&#123;np.<span class="built_in">round</span>(ridge_model.coef_, <span class="number">2</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="example-output">Example Output:</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--- Results for Correlated Features ---</span><br><span class="line">True Coefficients: [2.0, 2.0]</span><br><span class="line">LASSO Coefficients: [3.89 0.  ]</span><br><span class="line">Ridge Coefficients: [1.95 1.94]</span><br></pre></td></tr></table></figure>
<h3 id="code-explanation">Code Explanation:</h3>
<ul>
<li><strong>LASSO:</strong> As predicted by the slides, LASSO failed to
find the true model. It <em>arbitrarily</em> picked <code>x1</code>,
gave it a large coefficient, and <strong>set <code>x2</code> to
zero</strong>. This is unstable and not what we wanted.</li>
<li><strong>Ridge:</strong> As predicted by Slide 3, Ridge handled the
collinearity perfectly. It identified that both <code>x1</code> and
<code>x2</code> were important and <strong>“grouped” them</strong> by
assigning them nearly identical, stable coefficients (1.95 and 1.94),
which are very close to the true values of 2.0.</li>
</ul>
<h1 id="elastic-net">10. Elastic Net</h1>
<h2 id="overall-summary">Overall Summary</h2>
<p>These slides introduce <strong>Elastic Net</strong>, a modern
regression method that solves the major weaknesses of its two
predecessors, <strong>Ridge</strong> and <strong>LASSO</strong>
regression.</p>
<ul>
<li><strong>Ridge</strong> is good for <strong>collinearity</strong>
(correlated features) but can’t do <strong>variable selection</strong>
(it can’t set any feature’s coefficient to <em>exactly</em> zero).</li>
<li><strong>LASSO</strong> is good for <strong>variable
selection</strong> (it creates <em>sparse</em> models by setting
coefficients to zero) but behaves <strong>unstably</strong> when
features are correlated (it tends to randomly pick one and discard the
others).</li>
</ul>
<p><strong>Elastic Net</strong> combines the L1 penalty of LASSO and the
L2 penalty of Ridge. The result is a single, flexible model that:</p>
<ol type="1">
<li>Performs <strong>variable selection</strong> (like LASSO).</li>
<li>Handles <strong>correlated features</strong> stably by grouping them
together (like Ridge).</li>
<li>Can select more features than samples (<span class="math inline">\(p
&gt; n\)</span>), which LASSO cannot do.</li>
</ol>
<h3 id="slide-1-the-definition-and-formula-file-...020245.png">Slide 1:
The Definition and Formula (File: <code>...020245.png</code>)</h3>
<p>This slide explains <em>why</em> Elastic Net was created and defines
it <em>mathematically</em>.</p>
<ul>
<li><strong>The Problem:</strong> It states the exact trade-off:
<ul>
<li>“Ridge regression can handle collinearity, but cannot perform
variable selection;”</li>
<li>“LASSO can perform variable selection, but performs poorly when
collinearity;”</li>
</ul></li>
<li><strong>The Solution (The Formula):</strong> The core of the method
is this optimization formula: <span
class="math display">\[\hat{\beta}_{eNet}(\lambda, \alpha) \leftarrow
\arg \min_{\beta} \left( \underbrace{\|\mathbf{y} -
\mathbf{X}\beta\|^2}_{\text{Loss}} + \lambda \left(
\underbrace{\alpha\|\beta\|_1}_{\text{L1 Penalty}} +
\underbrace{\frac{1-\alpha}{2}\|\beta\|_2^2}_{\text{L2 Penalty}} \right)
\right)\]</span></li>
<li><strong>Breaking Down the Formula:</strong>
<ul>
<li><strong><span class="math inline">\(\|\mathbf{y} -
\mathbf{X}\beta\|^2\)</span></strong>: This is the standard “Residual
Sum of Squares” (RSS). We want to find coefficients (<span
class="math inline">\(\beta\)</span>) that make the model’s predictions
(<span class="math inline">\(X\beta\)</span>) as close as possible to
the true values (<span class="math inline">\(y\)</span>).</li>
<li><strong><span class="math inline">\(\lambda\)</span>
(Lambda)</strong>: This is the <strong>master knob</strong> for
<em>total regularization strength</em>. A larger <span
class="math inline">\(\lambda\)</span> means a bigger penalty, which
“shrinks” all coefficients more.</li>
<li><strong><span class="math inline">\(\alpha\)</span>
(Alpha)</strong>: This is the <strong>mixing parameter</strong> that
balances L1 and L2. This is the key innovation.
<ul>
<li><strong><span
class="math inline">\(\alpha\|\beta\|_1\)</span></strong>: This is the
<strong>L1 (LASSO)</strong> part. It forces weak coefficients to become
exactly zero, thus selecting variables.</li>
<li><strong><span
class="math inline">\(\frac{1-\alpha}{2}\|\beta\|_2^2\)</span></strong>:
This is the <strong>L2 (Ridge)</strong> part. It shrinks all
coefficients and, crucially, encourages correlated features to have
similar coefficients (the grouping effect).</li>
</ul></li>
</ul></li>
<li><strong>The Special Cases:</strong>
<ul>
<li>If <strong><span class="math inline">\(\alpha = 0\)</span></strong>,
the L1 term vanishes, and the model becomes pure <strong>Ridge
Regression</strong>.</li>
<li>If <strong><span class="math inline">\(\alpha = 1\)</span></strong>,
the L2 term vanishes, and the model becomes pure <strong>LASSO
Regression</strong>.</li>
<li>If <strong><span class="math inline">\(0 &lt; \alpha &lt;
1\)</span></strong>, you get <strong>Elastic Net</strong>, which
“encourages grouping of correlated variables” <em>and</em> “can perform
variable selection.”</li>
</ul></li>
</ul>
<h3
id="slide-2-the-intuition-and-the-grouping-effect-file-...020249.jpg">Slide
2: The Intuition and The Grouping Effect (File:
<code>...020249.jpg</code>)</h3>
<p>This slide gives you the <em>visual intuition</em> and the
<em>practical proof</em> of why Elastic Net works. It has two parts.</p>
<h4 id="part-1-the-three-graphs-geometric-intuition">Part 1: The Three
Graphs (Geometric Intuition)</h4>
<p>These graphs show the <em>constraint region</em> (the shaded shape)
for each penalty. The model tries to find the best coefficients (<span
class="math inline">\(\theta_{opt}\)</span>), and the final solution
(the green dot) is the first point where the cost function (the blue
ellipses) “touches” the constraint region.</p>
<ul>
<li><strong>L1 Norm (LASSO):</strong> The region is a
<strong>diamond</strong>. Because of its <strong>sharp corners</strong>,
the ellipses are very likely to hit a corner first. At a corner, one of
the coefficients (e.g., <span class="math inline">\(\theta_1\)</span>)
is zero. This is a visual explanation of how LASSO creates
<strong>sparsity</strong> (variable selection).</li>
<li><strong>L2 Norm (Ridge):</strong> The region is a
<strong>circle</strong>. It has <strong>no corners</strong>. The
ellipses will hit a “smooth” point on the circle, shrinking both
coefficients (<span class="math inline">\(\theta_1\)</span> and <span
class="math inline">\(\theta_2\)</span>) but not setting either to zero.
This is <strong>weight sharing</strong>.</li>
<li><strong>L1 + L2 (Elastic Net):</strong> The region is a
<strong>“rounded square”</strong>. It’s the perfect compromise.
<ul>
<li>It has “corners” (like LASSO) so it can still set coefficients to
zero.</li>
<li>It has “curved edges” (like Ridge) so it’s more stable and handles
correlated variables by finding a solution on an edge rather than a
single sharp corner.</li>
</ul></li>
</ul>
<h4 id="part-2-the-formula-the-grouping-effect">Part 2: The Formula (The
Grouping Effect)</h4>
<p>The text at the bottom explains Elastic Net’s “grouping effect.”</p>
<ul>
<li><strong>The Implication:</strong> “If <span
class="math inline">\(x_j \approx x_k\)</span>, then <span
class="math inline">\(\hat{\beta}_j \approx
\hat{\beta}_k\)</span>.”</li>
<li><strong>Meaning:</strong> If two features (<span
class="math inline">\(x_j\)</span> and <span
class="math inline">\(x_k\)</span>) are highly correlated (their values
are very similar), Elastic Net will force their <em>coefficients</em>
(<span class="math inline">\(\hat{\beta}_j\)</span> and <span
class="math inline">\(\hat{\beta}_k\)</span>) to also be very
similar.</li>
<li><strong>Why this is good:</strong> This is the <em>opposite</em> of
LASSO. LASSO would be unstable and might arbitrarily set <span
class="math inline">\(\hat{\beta}_j\)</span> to a large value and <span
class="math inline">\(\hat{\beta}_k\)</span> to zero. Elastic Net
“groups” them: it will either keep <em>both</em> in the model with
similar importance, or it will shrink <em>both</em> of them out of the
model together. This is a much more stable and realistic result.</li>
<li><strong>The Warning:</strong> “LASSO may be unstable in this case!”
This directly highlights the problem that Elastic Net solves.</li>
</ul>
<h3 id="slide-3-the-feature-comparison-table-file-...020255.png">Slide
3: The Feature Comparison Table (File: <code>...020255.png</code>)</h3>
<p>This table is your “cheat sheet” for choosing the right model. It
compares Ridge, LASSO, and Elastic Net on all their key properties.</p>
<ul>
<li><strong>Penalty:</strong> Shows the L2, L1, and combined
penalties.</li>
<li><strong>Sparsity:</strong> Can the model set coefficients to 0?
<ul>
<li>Ridge: <strong>No ❌</strong></li>
<li>LASSO: <strong>Yes ✅</strong></li>
<li>Elastic Net: <strong>Yes ✅</strong></li>
</ul></li>
<li><strong>Variable Selection:</strong> This is a <em>crucial</em> row.
<ul>
<li>LASSO: <strong>Yes ✅</strong>, BUT it has a major limitation: if
you have more features than samples (<span class="math inline">\(p &gt;
n\)</span>), LASSO can select <em>at most</em> <span
class="math inline">\(n\)</span> features.</li>
<li>Elastic Net: <strong>Yes ✅</strong>, and it <strong>can select more
than <span class="math inline">\(n\)</span> variables</strong>. This
makes it the clear choice for “wide” data problems (e.g., in genomics,
where <span class="math inline">\(p=20,000\)</span> features and <span
class="math inline">\(n=100\)</span> samples).</li>
</ul></li>
<li><strong>Grouping Effect:</strong> How does it handle correlated
features?
<ul>
<li>Ridge: <strong>Strong ✅</strong></li>
<li>LASSO: <strong>Weak ❌</strong> (it “picks one”)</li>
<li>Elastic Net: <strong>Strong ✅</strong></li>
</ul></li>
<li><strong>Solution Uniqueness:</strong> Is the answer stable?
<ul>
<li>Ridge: <strong>Always ✅</strong></li>
<li>LASSO: <strong>No ❌</strong> (not if <span
class="math inline">\(X\)</span> is “rank-deficient,” e.g., <span
class="math inline">\(p &gt; n\)</span> or correlated features)</li>
<li>Elastic Net: <strong>Always ✅</strong> (as long as <span
class="math inline">\(\alpha &lt; 1\)</span>, the Ridge component
guarantees a unique, stable solution).</li>
</ul></li>
<li><strong>Use Case:</strong> When should you use each?
<ul>
<li><strong>Ridge:</strong> For prediction, especially with
<strong>multicollinearity</strong>.</li>
<li><strong>LASSO:</strong> For <strong>interpretability</strong> and
creating <strong>sparse models</strong> (when you think only a few
features matter).</li>
<li><strong>Elastic Net:</strong> The best all-arounder. Use it for
<strong>correlated predictors</strong>, when <strong><span
class="math inline">\(p \gg n\)</span></strong>, or when you need both
<strong>sparsity + stability</strong>.</li>
</ul></li>
</ul>
<h3 id="code-understanding-python-scikit-learn">Code Understanding
(Python <code>scikit-learn</code>)</h3>
<p>When you use this in Python, be aware of a common confusion in the
parameter names:</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Concept (from your slides)</th>
<th style="text-align: left;"><code>scikit-learn</code> Parameter</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong><span
class="math inline">\(\lambda\)</span></strong> (Lambda)</td>
<td style="text-align: left;"><code>alpha</code></td>
<td style="text-align: left;">The <strong>overall strength</strong> of
regularization.</td>
</tr>
<tr>
<td style="text-align: left;"><strong><span
class="math inline">\(\alpha\)</span></strong> (Alpha)</td>
<td style="text-align: left;"><code>l1_ratio</code></td>
<td style="text-align: left;">The <strong>mixing parameter</strong>
between L1 and L2.</td>
</tr>
</tbody>
</table>
<p><strong>Example:</strong> An <code>l1_ratio</code> of <code>0</code>
is Ridge. An <code>l1_ratio</code> of <code>1</code> is LASSO. An
<code>l1_ratio</code> of <code>0.5</code> is a 50/50 mix.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet, ElasticNetCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Initialize a specific model</span></span><br><span class="line"><span class="comment"># This uses 0.5 for lambda (slide&#x27;s alpha) and 0.1 for lambda (slide&#x27;s lambda)</span></span><br><span class="line">model = ElasticNet(alpha=<span class="number">0.1</span>, l1_ratio=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. A much better way: Find the best parameters automatically</span></span><br><span class="line"><span class="comment"># This will test l1_ratios of 0.1, 0.5, and 0.9</span></span><br><span class="line"><span class="comment"># and automatically find the best &#x27;alpha&#x27; (strength) for each.</span></span><br><span class="line">cv_model = ElasticNetCV(</span><br><span class="line">    l1_ratio=[<span class="number">.1</span>, <span class="number">.5</span>, <span class="number">.9</span>],</span><br><span class="line">    cv=<span class="number">5</span>  <span class="comment"># 5-fold cross-validation</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit the model to your data (X_train, y_train)</span></span><br><span class="line"><span class="comment"># cv_model.fit(X_train, y_train)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. See the best parameters it found</span></span><br><span class="line"><span class="comment"># print(f&quot;Best l1_ratio (slide&#x27;s alpha): &#123;cv_model.l1_ratio_&#125;&quot;)</span></span><br><span class="line"><span class="comment"># print(f&quot;Best alpha (slide&#x27;s lambda): &#123;cv_model.alpha_&#125;&quot;)</span></span><br></pre></td></tr></table></figure>
<h1 id="high-dimensional-data-analysis">11. High-Dimensional Data
Analysis</h1>
<h2 id="the-core-problem-large-p-small-n">The Core Problem: Large <span
class="math inline">\(p\)</span>, Small <span
class="math inline">\(n\)</span></h2>
<p>The slides introduce the challenge of high-dimensional data, which is
defined by having <strong>many more features (predictors) <span
class="math inline">\(p\)</span> than observations (samples) <span
class="math inline">\(n\)</span></strong>. This is often written as
<strong><span class="math inline">\(p \gg n\)</span></strong>.</p>
<ul>
<li><strong>Example:</strong> Predicting blood pressure (the response
<span class="math inline">\(y\)</span>) using millions of genetic
markers (SNPs) as features <span class="math inline">\(X\)</span>, but
only having data from a few hundred patients.</li>
<li><strong>Troubles:</strong>
<ul>
<li><strong>Overfitting:</strong> Models become “too flexible” and learn
the noise in the training data, rather than the true underlying
pattern.</li>
<li><strong>Non-Unique Solution:</strong> When <span
class="math inline">\(p &gt; n\)</span>, the standard least squares
linear regression model doesn’t even have a unique solution.</li>
<li><strong>Misleading Metrics:</strong> This leads to a common symptom:
a very small <strong>training error</strong> (or high <span
class="math inline">\(R^2\)</span>) but a very large <strong>test
error</strong>.</li>
</ul></li>
</ul>
<h2 id="most-important-image-the-overfitting-trap-figure-6.23">Most
Important Image: The Overfitting Trap (Figure 6.23)</h2>
<p>Figure 6.23 (from the first uploaded image) is the most critical
visual for understanding the <em>problem</em>. It shows what happens
when you add features (variables) that are <em>completely unrelated</em>
to the outcome.</p>
<ul>
<li><strong>Left Plot (R²):</strong> The <span
class="math inline">\(R^2\)</span> on the training data increases
towards 1. This <em>looks</em> like a perfect fit.</li>
<li><strong>Center Plot (Training MSE):</strong> The Mean Squared Error
on the <em>training</em> data decreases to 0. This also <em>looks</em>
perfect.</li>
<li><strong>Right Plot (Test MSE):</strong> The Mean Squared Error on
the <em>test</em> data (new, unseen data) explodes. This reveals the
model is garbage and has just memorized the training set.</li>
</ul>
<p>⚠️ <strong>This is the key takeaway:</strong> In high dimensions,
<span class="math inline">\(R^2\)</span> and training MSE are
<strong>useless</strong> and <strong>misleading</strong> metrics for
model quality.</p>
<h2 id="the-solution-regularization-model-selection">The Solution:
Regularization &amp; Model Selection</h2>
<p>To combat overfitting, we must use <strong>less flexible
models</strong>. The main strategy is <strong>regularization</strong>
(also called shrinkage), which involves adding a penalty term to the
cost function to “shrink” the model coefficients (<span
class="math inline">\(\beta\)</span>).</p>
<h3 id="mathematical-formulas-python-code">Mathematical Formulas &amp;
Python Code 🐍</h3>
<p>The standard <strong>Least Squares</strong> cost function you try to
minimize is: <span class="math display">\[\text{RSS} = \sum_{i=1}^n
\left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j\right)^2 \quad
\text{or} \quad \|y - X\beta\|^2_2\]</span> This fails when <span
class="math inline">\(p &gt; n\)</span>. The solutions modify this:</p>
<h4 id="a.-ridge-regression-l_2-penalty">A. Ridge Regression (<span
class="math inline">\(L_2\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> Shrinks all coefficients towards zero, but
never <em>to</em> zero. It’s good when many features are related to the
outcome.</li>
<li><strong>Math Formula:</strong> <span
class="math display">\[\text{Minimize: } \left( \|y - X\beta\|^2_2 +
\lambda \sum_{j=1}^p \beta_j^2 \right)\]</span>
<ul>
<li>The <span class="math inline">\(\lambda \sum_{j=1}^p
\beta_j^2\)</span> is the <strong><span
class="math inline">\(L_2\)</span> penalty</strong>.</li>
<li><span class="math inline">\(\lambda\)</span> (lambda) is a
<em>tuning parameter</em> that controls the penalty strength. A larger
<span class="math inline">\(\lambda\)</span> means more shrinkage.</li>
</ul></li>
<li><strong>Python (Scikit-learn):</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># alpha is the lambda (λ) tuning parameter</span></span><br><span class="line"><span class="comment"># We find the best alpha using cross-validation</span></span><br><span class="line">ridge_model = Ridge(alpha=<span class="number">1.0</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">ridge_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate using test error (e.g., MSE on test set)</span></span><br><span class="line"><span class="comment"># NOT with training R-squared</span></span><br><span class="line">test_score = ridge_model.score(X_test, y_test) </span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="b.-the-lasso-l_1-penalty">B. The Lasso (<span
class="math inline">\(L_1\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> This is a very important method. The <span
class="math inline">\(L_1\)</span> penalty can force coefficients to be
<strong>exactly zero</strong>. This means Lasso performs
<strong>automatic feature selection</strong>, creating a <em>sparse</em>
model.</li>
<li><strong>Math Formula:</strong> <span
class="math display">\[\text{Minimize: } \left( \|y - X\beta\|^2_2 +
\lambda \sum_{j=1}^p |\beta_j| \right)\]</span>
<ul>
<li>The <span class="math inline">\(\lambda \sum_{j=1}^p
|\beta_j|\)</span> is the <strong><span
class="math inline">\(L_1\)</span> penalty</strong>.</li>
<li>Again, <span class="math inline">\(\lambda\)</span> is the tuning
parameter.</li>
</ul></li>
<li><strong>Python (Scikit-learn):</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"></span><br><span class="line"><span class="comment"># alpha is the lambda (λ) tuning parameter</span></span><br><span class="line">lasso_model = Lasso(alpha=<span class="number">0.1</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">lasso_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The model automatically selects features</span></span><br><span class="line"><span class="comment"># Coefficients that are zero were &#x27;dropped&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(lasso_model.coef_) </span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="c.-other-methods">C. Other Methods</h4>
<p>The slides also mention:</p>
<ul>
<li><strong>Forward Stepwise Selection:</strong> A different approach
where you start with no features and add them one by one, picking the
one that improves the model most (based on a criterion like
cross-validation error).</li>
<li><strong>Principal Components Regression (PCR):</strong> A
dimensionality reduction technique.</li>
</ul>
<h2 id="the-curse-of-dimensionality-figure-6.24">The Curse of
Dimensionality (Figure 6.24)</h2>
<p>This example (Figures 6.24 and its description) shows a more subtle
problem.</p>
<ul>
<li><strong>Setup:</strong> A model with <span
class="math inline">\(n=100\)</span> observations and 20 <em>true</em>
features.</li>
<li><strong>Plots:</strong> They test Lasso by adding more and more
<em>irrelevant</em> features:
<ul>
<li><strong><span class="math inline">\(p=20\)</span> (Left):</strong>
Lasso performs well. The lowest test MSE is found with minimal
regularization.</li>
<li><strong><span class="math inline">\(p=50\)</span> (Center):</strong>
Lasso still works well, but it needs more regularization (a smaller
“Degrees of Freedom”) to filter out the 30 junk features.</li>
<li><strong><span class="math inline">\(p=2000\)</span>
(Right):</strong> This is the <strong>curse of dimensionality</strong>.
Even with a good method like Lasso, the 1,980 irrelevant features add so
much noise that the model <strong>performs poorly regardless</strong> of
the tuning parameter. The true signal is “lost in the noise.”</li>
</ul></li>
</ul>
<h2 id="summary-cautions-for-p-n">Summary: Cautions for <span
class="math inline">\(p &gt; n\)</span></h2>
<p>The final slide gives the most important rules to follow:</p>
<ol type="1">
<li><strong>Beware Extreme Multicollinearity:</strong> When <span
class="math inline">\(p &gt; n\)</span>, your features are
mathematically guaranteed to be linearly related, which breaks standard
regression.</li>
<li><strong>Don’t Overstate Results:</strong> A model you find (e.g.,
with Lasso) is just <em>one</em> of many potentially good models.</li>
<li><strong>🚫 DO NOT USE</strong> training <span
class="math inline">\(R^2\)</span>, <span
class="math inline">\(p\)</span>-values, or training MSE to justify your
model. As Figure 6.23 showed, they are misleading.</li>
<li><strong>✅ DO USE</strong> <strong>test error</strong> and
<strong>cross-validation error</strong> to choose your model and assess
its performance.</li>
</ol>
<h2 id="the-core-problem-p-gg-n-the-troubles-slide">The Core Problem:
<span class="math inline">\(p \gg n\)</span> (The “Troubles” Slide)</h2>
<p>This slide (filename: <code>...020259.png</code>) sets up the entire
problem. The issue isn’t just “overfitting”; it’s a fundamental
mathematical breakdown of standard methods.</p>
<ul>
<li><strong>“Large <span class="math inline">\(p\)</span> makes our
linear regression model too flexible”</strong>: This is an
understatement. It leads to a problem called an <strong>underdetermined
system</strong>.</li>
<li><strong>“If <span class="math inline">\(p &gt; n\)</span>, the LSE
is not even uniquely determined”</strong>: This is the most important
technical point.
<ul>
<li><strong>Mathematical Reason:</strong> The standard solution for
Ordinary Least Squares (OLS) is <span class="math inline">\(\hat{\beta}
= (X^T X)^{-1} X^T y\)</span>.</li>
<li><span class="math inline">\(X\)</span> is the data matrix with <span
class="math inline">\(n\)</span> rows (observations) and <span
class="math inline">\(p\)</span> columns (features).</li>
<li>The matrix <span class="math inline">\(X^T X\)</span> has dimensions
<span class="math inline">\(p \times p\)</span>.</li>
<li>When <span class="math inline">\(p &gt; n\)</span>, the <span
class="math inline">\(X^T X\)</span> matrix is
<strong>singular</strong>, which means its determinant is zero and it
<strong>cannot be inverted</strong>. The <span
class="math inline">\((X^T X)^{-1}\)</span> term does not exist.</li>
<li><strong>“Extreme multicollinearity”</strong> (from slide
<code>...020744.png</code>) is the direct cause. When <span
class="math inline">\(p &gt; n\)</span>, the columns of <span
class="math inline">\(X\)</span> (the features) are <em>guaranteed</em>
to be linearly dependent. There are infinite combinations of the
features that can explain the data.</li>
</ul></li>
</ul>
<h2 id="the-simplest-example-n2-figure-6.22">The Simplest Example: <span
class="math inline">\(n=2\)</span> (Figure 6.22)</h2>
<p>This slide (filename: <code>...020728.png</code>) is the
<em>perfect</em> illustration of the “not uniquely determined”
problem.</p>
<ul>
<li><strong>Left Plot (Low-D):</strong> Many points (<span
class="math inline">\(n\)</span>), only two parameters (<span
class="math inline">\(p=2\)</span>: intercept <span
class="math inline">\(\beta_0\)</span> and slope <span
class="math inline">\(\beta_1\)</span>). The line is a “best fit” that
balances the errors. The training error (RSS) is non-zero.</li>
<li><strong>Right Plot (High-D):</strong> We have <span
class="math inline">\(n=2\)</span> observations and <span
class="math inline">\(p=2\)</span> parameters.
<ul>
<li>You have two equations (one for each point) and two unknowns (<span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>).</li>
<li>The model has <em>exactly</em> enough flexibility to pass
<em>perfectly</em> through both points.</li>
<li>The result is <strong>zero training error</strong>.</li>
<li>This “perfect” fit is an illusion. If you got a <em>new</em> data
point, this line would almost certainly be a terrible predictor. This is
the essence of overfitting.</li>
</ul></li>
</ul>
<h2 id="the-consequence-misleading-metrics-figure-6.23">The Consequence:
Misleading Metrics (Figure 6.23)</h2>
<p>This slide (filename: <code>...020730.png</code>) scales up the
problem from <span class="math inline">\(n=2\)</span> to <span
class="math inline">\(n=20\)</span> and shows <em>why</em> you must be
cautious.</p>
<ul>
<li><strong>The Setup:</strong> <span
class="math inline">\(n=20\)</span> observations. We start with 1
feature and add more and more <em>irrelevant, junk</em> features.</li>
<li><strong>Left Plot (<span
class="math inline">\(R^2\)</span>):</strong> The <span
class="math inline">\(R^2\)</span> on the training data steadily
increases towards 1 as we add features. This is because, by pure chance,
each new junk feature can explain a tiny bit more of the noise in the
training set.</li>
<li><strong>Center Plot (Training MSE):</strong> The training error
drops to 0. This is the same as the <span
class="math inline">\(n=2\)</span> plot. Once the number of features
(<span class="math inline">\(p\)</span>) gets close to the number of
observations (<span class="math inline">\(n=20\)</span>), the model can
perfectly fit the 20 data points, even if the features are random
noise.</li>
<li><strong>Right Plot (Test MSE):</strong> This is the “truth.” The
<em>actual</em> error on new, unseen data gets worse and worse. By
adding noise features, we are just “memorizing” the training set, and
our model’s ability to generalize is destroyed.</li>
<li><strong>Key Lesson:</strong> (from slide <code>...020744.png</code>)
This is why you must <strong>“Avoid using… <span
class="math inline">\(p\)</span>-values, <span
class="math inline">\(R^2\)</span>, or other traditional measures of
model on training as evidence of good fit.”</strong> They are guaranteed
to lie to you when <span class="math inline">\(p &gt; n\)</span>.</li>
</ul>
<h2 id="the-solutions-the-deal-with-slide">The Solutions (The “Deal
with…” Slide)</h2>
<p>This slide (filename: <code>...020734.png</code>) lists the
strategies to fix this. The core idea is <strong>regularization</strong>
(or shrinkage). We add a “penalty” to the cost function to stop the
<span class="math inline">\(\beta\)</span> coefficients from getting too
large or too numerous.</p>
<h4 id="a.-ridge-regression-l_2-penalty-1">A. Ridge Regression (<span
class="math inline">\(L_2\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> Keeps all <span
class="math inline">\(p\)</span> features, but shrinks their
coefficients. It’s excellent for handling multicollinearity.</li>
<li><strong>Math:</strong> <span class="math inline">\(\text{Minimize: }
\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j\right)^2 +
\lambda \sum_{j=1}^p \beta_j^2\)</span>
<ul>
<li>The first part is the standard RSS.</li>
<li>The <span class="math inline">\(\lambda \sum \beta_j^2\)</span> is
the <strong><span class="math inline">\(L_2\)</span> penalty</strong>.
It punishes large coefficient values.</li>
</ul></li>
<li><strong><span class="math inline">\(\lambda\)</span>
(Lambda):</strong> This is the <strong>tuning parameter</strong>.
<ul>
<li>If <span class="math inline">\(\lambda=0\)</span>, it’s just OLS
(which fails).</li>
<li>If <span class="math inline">\(\lambda \to \infty\)</span>, all
<span class="math inline">\(\beta\)</span>’s are shrunk to 0.</li>
<li>The right <span class="math inline">\(\lambda\)</span> is chosen via
<strong>cross-validation</strong>.</li>
</ul></li>
</ul>
<h4 id="b.-the-lasso-l_1-penalty-1">B. The Lasso (<span
class="math inline">\(L_1\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> This is often preferred because it
performs <strong>automatic feature selection</strong>. It shrinks many
coefficients to be <strong>exactly zero</strong>.</li>
<li><strong>Math:</strong> <span class="math inline">\(\text{Minimize: }
\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j\right)^2 +
\lambda \sum_{j=1}^p |\beta_j|\)</span>
<ul>
<li>The <span class="math inline">\(\lambda \sum |\beta_j|\)</span> is
the <strong><span class="math inline">\(L_1\)</span> penalty</strong>.
This absolute value penalty is what allows coefficients to become
exactly 0.</li>
</ul></li>
<li><strong>Benefit:</strong> The final model is <em>sparse</em> (e.g.,
it might say “out of 2,000 features, only these 15 matter”).</li>
</ul>
<h4 id="c.-tuning-parameter-choice-the-real-work">C. Tuning Parameter
Choice (The <em>Real</em> Work)</h4>
<p>How do you pick the best <span
class="math inline">\(\lambda\)</span>? You must use the data you have.
The slides mention this and “cross validation error” (from
<code>...020744.png</code>).</p>
<ul>
<li><strong>Python Code (Scikit-learn):</strong> You don’t just guess
<code>alpha</code> (which is <span
class="math inline">\(\lambda\)</span> in scikit-learn). You use a tool
like <code>LassoCV</code> or <code>GridSearchCV</code> to find the best
one. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a high-dimensional dataset</span></span><br><span class="line">X, y = make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">500</span>, n_informative=<span class="number">10</span>, noise=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># LassoCV automatically performs cross-validation to find the best alpha (lambda)</span></span><br><span class="line"><span class="comment"># cv=10 means 10-fold cross-validation</span></span><br><span class="line">lasso_cv_model = LassoCV(cv=<span class="number">10</span>, random_state=<span class="number">0</span>, max_iter=<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">lasso_cv_model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is the best lambda (alpha) it found:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best alpha (lambda): <span class="subst">&#123;lasso_cv_model.alpha_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can now see the coefficients</span></span><br><span class="line"><span class="comment"># Most of the 500 coefficients will be 0.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of non-zero features: <span class="subst">&#123;np.<span class="built_in">sum</span>(lasso_cv_model.coef_ != <span class="number">0</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="a-final-warning-the-curse-of-dimensionality-figure-6.24">A Final
Warning: The Curse of Dimensionality (Figure 6.24)</h2>
<p>This final set of slides (filenames: <code>...020738.png</code> and
<code>...020741.jpg</code>) provides a crucial, subtle warning:
<strong>Regularization is not magic.</strong></p>
<ul>
<li><strong>The Setup:</strong> <span
class="math inline">\(n=100\)</span> observations. There are <strong>20
real features</strong> that truly affect the response.</li>
<li><strong>The Experiment:</strong> They run Lasso three times, adding
more and more <em>noise</em> features:
<ul>
<li><strong>Left Plot (<span
class="math inline">\(p=20\)</span>):</strong> All 20 features are real.
The lowest test MSE is found with minimal regularization (high “Degrees
of Freedom,” meaning many non-zero coefficients). This makes sense; you
want to keep all 20 real features.</li>
<li><strong>Center Plot (<span
class="math inline">\(p=50\)</span>):</strong> Now we have 20 real
features + 30 noise features. Lasso still works! The best model is found
with more regularization (fewer “Degrees of Freedom”). Lasso
successfully “zeroed out” many of the 30 noise features.</li>
<li><strong>Right Plot (<span
class="math inline">\(p=2000\)</span>):</strong> This is the
<strong>curse of dimensionality</strong>. We have 20 real features +
1980 noise features. The <em>noise</em> has completely overwhelmed the
<em>signal</em>. <strong>Lasso fails.</strong> The test MSE is high
<em>no matter what</em> tuning parameter you choose. The model cannot
distinguish the 20 real features from the 1980 junk ones.</li>
</ul></li>
</ul>
<p><strong>Final Takeaway:</strong> Even with advanced methods like
Lasso, if your <span class="math inline">\(p \gg n\)</span> problem is
<em>too</em> extreme (i.S. the signal-to-noise ratio is too low), it may
be impossible to build a good predictive model.</p>
<h2 id="the-goal-collaborative-filtering">The Goal: “Collaborative
Filtering”</h2>
<p>The first slide (<code>...021218.png</code>) uses the term
<strong>Collaborative Filtering</strong>. This is the key concept. The
model “collaborates” by using the ratings of <em>all</em> users to fill
in the blanks for a <em>single</em> user.</p>
<ul>
<li><strong>How it works:</strong> The model assumes your “taste”
(vector <span class="math inline">\(\mathbf{u}_i\)</span>) can be
described as a combination of <span class="math inline">\(r\)</span>
“latent features” (e.g., <span class="math inline">\(r=3\)</span>: %
action, % comedy, % drama). It <em>also</em> assumes each movie (vector
<span class="math inline">\(\mathbf{v}_j\)</span>) has a profile on
these same features.</li>
<li>Your predicted rating for a movie is the dot product of your taste
vector and the movie’s feature vector.</li>
<li>The model finds the best “taste” vectors <span
class="math inline">\(\mathbf{U}\)</span> and “movie” vectors <span
class="math inline">\(\mathbf{V}\)</span> that explain all the known
ratings <em>simultaneously</em>. It’s collaborative because Lee’s
ratings help define the features of “Bullet Train” (<span
class="math inline">\(\mathbf{v}_2\)</span>), which in turn helps
predict Yang’s rating for that same movie.</li>
</ul>
<h2 id="the-hard-problem-and-its-2-flavors">The Hard Problem (and its 2
Flavors)</h2>
<p>The second slide (<code>...021222.png</code>) presents the intuitive,
but computationally <em>very</em> hard, way to frame the problem.</p>
<h4 id="detail-1-noise-vs.-no-noise">Detail 1: Noise vs. No Noise</h4>
<p>The slide shows <span class="math inline">\(\mathbf{Y} = \mathbf{M} +
\mathbf{E}\)</span>. This is critical. * <span
class="math inline">\(\mathbf{M}\)</span> is the “true,” “clean,”
underlying low-rank matrix of everyone’s “true” preferences. * <span
class="math inline">\(\mathbf{E}\)</span> is a matrix of random noise.
(e.g., your true rating is 4.3, but you entered a 4; or you were in a
bad mood and rated a 3). * <span
class="math inline">\(\mathbf{Y}\)</span> is the <em>noisy data</em> we
actually observe.</p>
<p>Because of this noise, we don’t expect to find a matrix <span
class="math inline">\(\mathbf{N}\)</span> that <em>perfectly</em>
matches our data. Instead, we try to find a low-rank <span
class="math inline">\(\mathbf{N}\)</span> that is <em>as close as
possible</em>. This leads to the formula: <span
class="math display">\[\underset{\text{rank}(\mathbf{N}) \le
r}{\text{minimize}} \quad \left\| \mathcal{P}_{\mathcal{O}}(\mathbf{Y} -
\mathbf{N}) \right\|_{\text{F}}^2\]</span> This says: “Find a matrix
<span class="math inline">\(\mathbf{N}\)</span> (of rank <span
class="math inline">\(r\)</span> or less) that minimizes the sum of
squared errors <em>only on the ratings we observed</em> (<span
class="math inline">\(\mathcal{O}\)</span>).”</p>
<h4
id="detail-2-why-is-textrankmathbfn-le-r-a-non-convex-constraint">Detail
2: Why is <span class="math inline">\(\text{rank}(\mathbf{N}) \le
r\)</span> a “Non-convex constraint”?</h4>
<p>This is the “difficult to optimize” part. A convex problem is
(simplistically) one with a single valley, making it easy to find the
single lowest point. A non-convex problem has many local valleys, and an
algorithm can get stuck in a “pretty good” valley instead of the “best”
one.</p>
<p>The rank constraint is non-convex. For example, the average of two
rank-1 matrices is <em>not</em> necessarily a rank-1 matrix (it could be
rank-2). This lack of a “smooth valley” property makes the problem
NP-hard.</p>
<h4 id="detail-3-the-number-of-parameters-rd_1-d_2">Detail 3: The Number
of Parameters: <span class="math inline">\(r(d_1 + d_2)\)</span></h4>
<p>The slide asks, “how many entries are needed?” The answer is based on
the number of unknown parameters. * A rank-<span
class="math inline">\(r\)</span> matrix <span
class="math inline">\(\mathbf{M}\)</span> can be factored into <span
class="math inline">\(\mathbf{U}\)</span> (which is <span
class="math inline">\(d_1 \times r\)</span>) and <span
class="math inline">\(\mathbf{V}^T\)</span> (which is <span
class="math inline">\(r \times d_2\)</span>). * The number of entries in
<span class="math inline">\(\mathbf{U}\)</span> is <span
class="math inline">\(d_1 \times r\)</span>. * The number of entries in
<span class="math inline">\(\mathbf{V}\)</span> is <span
class="math inline">\(d_2 \times r\)</span>. * Total “unknowns” to solve
for: <span class="math inline">\(d_1 r + d_2 r = r(d_1 + d_2)\)</span>.
* This means we must have <em>at least</em> <span
class="math inline">\(r(d_1 + d_2)\)</span> observed ratings to have any
hope of uniquely solving for <span
class="math inline">\(\mathbf{U}\)</span> and <span
class="math inline">\(\mathbf{V}\)</span>. If our number of observations
<span class="math inline">\(|\mathcal{O}|\)</span> is less than this,
the problem is hopelessly underdetermined.</p>
<h2 id="the-magic-solution-convex-relaxation">The “Magic” Solution:
Convex Relaxation</h2>
<p>The final slide (<code>...021225.png</code>) presents the
groundbreaking solution from Candès and Recht. This solution cleverly
<em>changes the problem</em> to one that is convex and solvable.</p>
<h4
id="detail-1-the-l1-norm-analogy-this-is-the-most-important-concept">Detail
1: The L1-Norm Analogy (This is the most important concept)</h4>
<p>This is the key to understanding <em>why</em> this works.</p>
<ul>
<li><strong>In Vectors (Lasso):</strong>
<ul>
<li><strong>Hard Problem:</strong> Find the <em>sparsest</em> vector
<span class="math inline">\(\beta\)</span> (fewest non-zeros). This is
<span class="math inline">\(L_0\)</span> norm, <span
class="math inline">\(\text{minimize } \|\beta\|_0\)</span>. This is
non-convex.</li>
<li><strong>Easy Problem:</strong> Minimize the <span
class="math inline">\(L_1\)</span> norm, <span
class="math inline">\(\text{minimize } \|\beta\|_1 = \sum
|\beta_j|\)</span>. This is convex, and it’s a “relaxation” that
<em>also</em> produces sparse solutions.</li>
</ul></li>
<li><strong>In Matrices (Matrix Completion):</strong>
<ul>
<li><strong>Hard Problem:</strong> Find the <em>lowest-rank</em> matrix
<span class="math inline">\(\mathbf{X}\)</span>. Rank is the number of
non-zero singular values. This is <span
class="math inline">\(\text{minimize } \text{rank}(\mathbf{X})\)</span>.
This is non-convex.</li>
<li><strong>Easy Problem:</strong> Minimize the <strong>Nuclear
Norm</strong>, <span class="math inline">\(\text{minimize }
\|\mathbf{X}\|_* = \sum \sigma_i(\mathbf{X})\)</span> (where <span
class="math inline">\(\sigma_i\)</span> are the singular values). This
is convex, and it’s the “matrix equivalent” of the <span
class="math inline">\(L_1\)</span> norm. It’s a relaxation that
<em>also</em> produces low-rank solutions.</li>
</ul></li>
</ul>
<h4 id="detail-2-noiseless-vs.-noisy-again">Detail 2: Noiseless
vs. Noisy (Again)</h4>
<p>Notice the <em>constraint</em> in this new problem: <span
class="math display">\[\text{Minimize } \quad \|\mathbf{X}\|_*\]</span>
<span class="math display">\[\text{Subject to } \quad X_{ij} = M_{ij},
\quad (i, j) \in \mathcal{O}\]</span></p>
<p>This formulation is for the <strong>noiseless</strong> case. It
assumes the <span class="math inline">\(M_{ij}\)</span> we observed are
<em>perfectly accurate</em>. It demands that our solution <span
class="math inline">\(\mathbf{X}\)</span> <em>exactly matches</em> the
known ratings. This is different from the optimization problem on the
previous slide, which just tried to get <em>close</em> to the noisy data
<span class="math inline">\(\mathbf{Y}\)</span>.</p>
<p>(In practice, you solve a noisy-aware version that combines both
ideas, but the slide shows the original, “exact completion”
problem.)</p>
<h4 id="detail-3-the-guarantee-what-the-math-at-the-bottom-means">Detail
3: The Guarantee (What the math at the bottom means)</h4>
<p><span class="math display">\[\text{If } \mathcal{O} \text{ is
randomly sampled and } |\mathcal{O}| \gg r(d_1+d_2)\log(d_1+d_2),
\text{... then the solution is unique and } \mathbf{M}
\text{...}\]</span></p>
<p>This is the punchline. The Candès paper <em>proved</em> that if you
have <em>enough</em> (but still very few) <em>randomly</em> sampled
ratings, solving this easy convex problem (minimizing the nuclear norm)
will <em>magically give you the exact, true, low-rank matrix <span
class="math inline">\(\mathbf{M}\)</span></em>.</p>
<ul>
<li><strong><span class="math inline">\(|\mathcal{O}| \gg
r(d_1+d_2)\)</span></strong>: This part makes sense. We need <em>at
least</em> as many observations as our <span
class="math inline">\(r(d_1+d_2)\)</span> degrees of freedom.</li>
<li><strong><span class="math inline">\(\log(d_1+d_2)\)</span></strong>:
This “log” factor is the “price” we pay for not knowing <em>where</em>
the information is. It’s an astonishingly small price.</li>
<li><strong>Example:</strong> For a 1,000,000 user x 10,000 movie matrix
(like Netflix) with <span class="math inline">\(r=10\)</span>, you don’t
need <span class="math inline">\(\approx 10^{10}\)</span> ratings. You
need a number closer to <span class="math inline">\(10 \times (10^6 +
10^4) \times \log(\dots)\)</span>, which is <em>dramatically</em>
smaller. This is why this method is practical.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
