<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianyaoblogs.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="TianyaoBlogs">
<meta property="og:url" content="https://tianyaoblogs.github.io/index.html">
<meta property="og:site_name" content="TianyaoBlogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://tianyaoblogs.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>TianyaoBlogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TianyaoBlogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/11/10/phys5120-hw4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/11/10/phys5120-hw4/" class="post-title-link" itemprop="url">phys-5120-hw4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-11-10 21:00:00 / 修改时间：00:41:19" itemprop="dateCreated datePublished" datetime="2025-11-10T21:00:00+08:00">2025-11-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/hw/" itemprop="url" rel="index"><span itemprop="name">hw</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          这是一篇加密文章
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/11/10/phys5120-hw4/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/11/08/GCL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/11/08/GCL/" class="post-title-link" itemprop="url">Code Review - Graph Contrastive Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-11-08 21:00:00" itemprop="dateCreated datePublished" datetime="2025-11-08T21:00:00+08:00">2025-11-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-11-09 00:47:00" itemprop="dateModified" datetime="2025-11-09T00:47:00+08:00">2025-11-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          这是一篇加密文章
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/11/08/GCL/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/11/08/papers_11_8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/11/08/papers_11_8/" class="post-title-link" itemprop="url">Paper Overview</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-11-08 21:00:00 / 修改时间：16:32:43" itemprop="dateCreated datePublished" datetime="2025-11-08T21:00:00+08:00">2025-11-08</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/QM9-papers-collections/" itemprop="url" rel="index"><span itemprop="name">QM9 papers collections</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          这是一篇加密文章
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2025/11/08/papers_11_8/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/11/07/5120c9-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/11/07/5120c9-2/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W9-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-11-07 22:00:00 / 修改时间：19:01:30" itemprop="dateCreated datePublished" datetime="2025-11-07T22:00:00+08:00">2025-11-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - 计算能源材料和电子结构模拟 Lecture</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<p>在前面，我们看到了“理想”的<strong>无轨道 DFT
(OF-DFT)</strong>，它试图将能量 <span class="math inline">\(E\)</span>
直接写成密度 <span class="math inline">\(\rho\)</span> 的泛函 <span
class="math inline">\(E[\rho]\)</span>。但它失败了，因为我们无法找到一个足够精确的<strong>动能泛函
<span class="math inline">\(T[\rho]\)</span></strong>。</p>
<p>接下来介绍的是<strong>Kohn-Sham (KS)
DFT</strong>，它采用了一种“妥协”方案，彻底绕开了这个难题。</p>
<h3 id="概念回顾无轨道-dft-的困境">1. 概念回顾：无轨道 DFT 的困境</h3>
<p>首先回顾我们已知的（也是 OF-DFT 的）总能量泛函：</p>
<p><span class="math display">\[E[\rho] = \underbrace{T[\rho]}_{\text{①
动能}} + \int V_{ext}(\vec{r})\rho(\vec{r})d\vec{r} +
\underbrace{\frac{e^2}{2}\iint d\vec{r}d\vec{r}&#39;
\frac{\rho(\vec{r})\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|}}_{\text{Hartree
能量}} + \underbrace{E_{xc}[\rho]}_{\text{② 交换关联能}}\]</span></p>
<ul>
<li><strong>问题所在：</strong>
<ul>
<li><strong>① <span class="math inline">\(T[\rho]\)</span>
(动能)：</strong>
这是最大的难题。动能是一个与波函数<strong>曲率</strong> (curvature)
相关的量子力学量 (<span class="math inline">\(\hat{T} \propto
\nabla^2\)</span>)。我们无法简单地通过密度 <span
class="math inline">\(\rho\)</span>（一个标量场）来精确描述它。前面推导的
<span class="math inline">\(T[\rho] \propto \int \rho^{5/3}
d\vec{r}\)</span> 只是一个非常粗糙的近似。</li>
<li><strong>② <span class="math inline">\(E_{xc}[\rho]\)</span>
(交换关联能)：</strong>
这是第二个难题。它包含了所有复杂的、非经典的电子-电子相互作用（泡利不相容原理的交换效应、电子相互躲避的关联效应）。</li>
</ul></li>
</ul>
<p>KS-DFT 的目标就是<strong>同时解决这两个问题</strong>。</p>
<h3 id="核心概念kohn-sham-辅助系统">2. 核心概念：Kohn-Sham 辅助系统</h3>
<p>Kohn-Sham 的核心思想是：“我承认 <span
class="math inline">\(T[\rho]\)</span>
太难了，所以我干脆<strong>不去近似它</strong>。”</p>
<blockquote>
<p><strong>Kohn-Sham 的核心假设：</strong>
对于<em>任何</em>一个我们关心的、有相互作用的<strong>真实系统</strong>（其基态密度为
<span
class="math inline">\(\rho_{true}\)</span>），我们<em>总能</em>构建一个<strong>虚构的
(fictitious)</strong>、<strong>无相互作用的</strong>辅助系统，这个系统被设计为<strong>恰好具有与真实系统完全相同的基态密度
<span class="math inline">\(\rho(\vec{r}) =
\rho_{true}(\vec{r})\)</span></strong>。</p>
</blockquote>
<p>这个想法彻底改变了规则。</p>
<ul>
<li><strong>公式 1：Kohn-Sham 哈密顿量 <span
class="math inline">\(\hat{H}_{KS}\)</span></strong>
<ul>
<li><code>Ĥ_KS = -ħ²/2m ∇² + V_KS(r)</code></li>
<li>这是一个描述 <span class="math inline">\(N\)</span>
个<strong>无相互作用</strong>电子的哈密顿量，它们都在一个共同的<strong>有效势
<span class="math inline">\(V_{KS}(\vec{r})\)</span></strong>
中运动。</li>
<li><strong>关键点：</strong>
这个哈密顿量中<strong>没有</strong>电子-电子相互作用项（如 <span
class="math inline">\(e^2/|\vec{r}_i -
\vec{r}_j|\)</span>）。这使得它在数学上变得极其简单。</li>
</ul></li>
<li><strong>公式 2：Kohn-Sham 波函数 <span
class="math inline">\(\Phi\)</span></strong>
<ul>
<li>一个斯莱特行列式：<code>Φ = 1/√N! | ... |</code></li>
<li><strong>详细解释：</strong> 由于 <span
class="math inline">\(\hat{H}_{KS}\)</span> 是 <span
class="math inline">\(N\)</span> 个无相互作用粒子的哈密顿量（<span
class="math inline">\(\hat{H}_{KS} = \sum_i
\hat{h}_i\)</span>），它的基态波函数 <span
class="math inline">\(\Phi\)</span>
可以被<strong>精确地</strong>写成一个由 <span
class="math inline">\(N\)</span> 个最低能量的单电子<strong>Kohn-Sham
轨道 <span class="math inline">\(\phi_i\)</span></strong>
构成的<strong>斯莱特行列式 (Slater Determinant)</strong>。</li>
<li>这些轨道 <span class="math inline">\(\phi_i\)</span>
是单粒子薛定谔方程（即 <strong>Kohn-Sham 方程</strong>）的解： <span
class="math display">\[\left( -\frac{\hbar^2}{2m}\nabla^2 +
V_{KS}(\vec{r}) \right) \phi_i(\vec{r}) = \epsilon_i
\phi_i(\vec{r})\]</span></li>
</ul></li>
</ul>
<h3 id="关键公式密度和动能">3. 关键公式：密度和动能</h3>
<p>现在，我们来看看这个“虚构系统”是如何帮我们解决问题的。</p>
<ul>
<li><strong>公式 3：KS 系统的电子密度 <span
class="math inline">\(\rho(\vec{r})\)</span></strong>
<ul>
<li><code>ρ(r) = ⟨Φ| Σ δ(r-ri) |Φ⟩ = Σ_&#123;i=1&#125;^N |φ_i|²</code></li>
<li><strong>详细解释：</strong>
<ul>
<li><code>⟨Φ| Σ δ(r-ri) |Φ⟩</code>：这是密度 <span
class="math inline">\(\rho(\vec{r})\)</span> 的标准量子力学定义，即“在
<span class="math inline">\(\vec{r}\)</span>
处找到<em>任意</em>一个电子的概率”。</li>
<li><code>= Σ_&#123;i=1&#125;^N |φ_i|²</code>：这是最关键的简化。对于一个斯莱特行列式波函数
<span
class="math inline">\(\Phi\)</span>，总的电子密度<strong>精确地等于</strong>所有被占据的
KS 轨道的概率密度之和。</li>
</ul></li>
<li><strong>KS 假设的重申：</strong> 我们假设存在一个 <span
class="math inline">\(V_{KS}(\vec{r})\)</span>，使得这个 <span
class="math inline">\(\rho(\vec{r})\)</span>
<strong>恒等于</strong>我们想研究的<strong>真实系统</strong>的密度 <span
class="math inline">\(\rho_{true}(\vec{r})\)</span>。</li>
</ul></li>
<li><strong>公式 4：无相互作用动能 <span
class="math inline">\(T_s[\rho]\)</span></strong>
<ul>
<li><code>Ts[ρ] = ⟨Φ| Σ -ħ²/2m ∇_i² |Φ⟩ = Σ_&#123;i=1&#125;^N ⟨φ_i| -ħ²/2m ∇² |φ_i⟩</code></li>
<li><strong>详细解释：</strong>
<ul>
<li><strong>这就是 KS 方案的全部意义！</strong></li>
<li>我们想知道这个虚构系统的总动能 <span
class="math inline">\(T_s\)</span> (s 代表 ‘single-particle’ 或
‘non-interacting’)。</li>
<li><span class="math inline">\(T_s = \langle \Phi | \hat{T} | \Phi
\rangle\)</span>。</li>
<li>与密度一样，由于 <span class="math inline">\(\Phi\)</span>
是斯莱特行列式，<span class="math inline">\(\hat{T}\)</span>
是单体算符，总动能<strong>精确地等于</strong>所有被占据的 KS
轨道的动能之和。</li>
</ul></li>
<li><strong>结论：</strong> 我们成功地<strong>避免了对 <span
class="math inline">\(T[\rho]\)</span>
的近似</strong>。我们现在不去近似它，而是通过求解 KS 轨道 <span
class="math inline">\(\phi_i\)</span>
来<strong>精确计算</strong>动能的主要部分 <span
class="math inline">\(T_s\)</span>。</li>
</ul></li>
</ul>
<h3 id="证明t_srho-与-t_truerho-的关系">4. “证明”：<span
class="math inline">\(T_s[\rho]\)</span> 与 <span
class="math inline">\(T_{true}[\rho]\)</span> 的关系</h3>
<p>提出了一个问题和一个潦草的证明：</p>
<ul>
<li><p><strong>问题：</strong> <code>Ts[ρ] ≤ T_true[ρ] ?</code></p>
<ul>
<li>我们通过轨道计算的<strong>无相互作用动能 <span
class="math inline">\(T_s\)</span></strong>
与<strong>真实系统</strong>的<strong>真正动能 <span
class="math inline">\(T_{true}\)</span></strong> 相比，哪个更大？</li>
</ul></li>
<li><p><strong>答案：<span class="math inline">\(T_s[\rho] \le
T_{true}[\rho]\)</span> 恒成立。</strong></p></li>
<li><p><strong>证明</strong>：</p>
<ul>
<li><code>Proof: ⟨Φ_ks| ...</code>
这里的字迹非常潦草，似乎是想用变分原理来论证，但写得并不清楚。</li>
</ul></li>
<li><p><strong>一个更清晰的、概念性的证明：</strong></p>
<ol type="1">
<li><span class="math inline">\(T_s[\rho]\)</span>
是一个<strong>无相互作用</strong>系统在密度为 <span
class="math inline">\(\rho\)</span> 时的基态动能。</li>
<li><span class="math inline">\(T_{true}[\rho]\)</span>
是一个<strong>有相互作用</strong>系统在密度为 <span
class="math inline">\(\rho\)</span> 时的基态动能。</li>
<li>在真实系统中，电子不仅受 <span
class="math inline">\(V_{ext}\)</span> 束缚，它们还必须<strong>相互排斥
(correlation)</strong>。为了“躲避”彼此，它们的波函数必须变得更加“弯曲”或“扭动”。</li>
<li>在量子力学中，<strong>动能 <span class="math inline">\(\propto \int
|\nabla \Psi|^2\)</span></strong>，它衡量的是波函数的“弯曲程度”。</li>
<li>真实电子为了相互躲避而增加的额外“弯曲”，导致了<strong>额外的动能</strong>。</li>
<li>因此，对于同一个密度 <span
class="math inline">\(\rho\)</span>，真实系统的动能 <span
class="math inline">\(T_{true}\)</span>
必然<strong>大于或等于</strong>那个不需要考虑相互躲避的、虚构的无相互作用系统的动能
<span class="math inline">\(T_s\)</span>。</li>
</ol></li>
<li><p><strong>这引出了最终的 KS-DFT 能量划分：</strong></p>
<ol type="1">
<li><p>真实动能 <span class="math inline">\(T_{true}\)</span>
被拆分为：<span class="math inline">\(T_{true}[\rho] = T_s[\rho] +
T_c[\rho]\)</span></p>
<ul>
<li><span
class="math inline">\(T_s[\rho]\)</span>：无相互作用动能（<strong>我们用轨道精确计算</strong>）。</li>
<li><span
class="math inline">\(T_c[\rho]\)</span>：<strong>动能的关联部分</strong>（<span
class="math inline">\(T_{true} - T_s\)</span>，这是个未知的小量）。</li>
</ul></li>
<li><p>现在，Kohn-Sham 方案将所有未知项——<span
class="math inline">\(T_c[\rho]\)</span> 和 <span
class="math inline">\(E_{xc}[\rho]\)</span>（来自 OF-DFT）——
<strong>全部打包</strong> 进一个<strong>新的</strong>交换关联泛函 <span
class="math inline">\(E_{xc}^{KS}[\rho]\)</span> 中。</p></li>
<li><p><strong>Kohn-Sham 总能量泛函：</strong> <span
class="math display">\[E[\rho] = \mathbf{T_s[\{\phi_i\}]} + \int
V_{ext}(\vec{r})\rho(\vec{r})d\vec{r} + E_H[\rho] +
\mathbf{E_{xc}^{KS}[\rho]}\]</span></p></li>
</ol></li>
</ul>
<p><strong>总结：</strong> KS-DFT
的赌注是：通过轨道<strong>精确计算</strong> <span
class="math inline">\(T_s\)</span>，剩下的那个包含 <span
class="math inline">\(T_c\)</span> 的<strong>新 <span
class="math inline">\(E_{xc}^{KS}[\rho]\)</span></strong>，会比原来那个包含整个
<span class="math inline">\(T_{true}\)</span> 的 OF-DFT
泛函<strong>容易得多</strong>。</p>
<p>历史证明，这个赌注赢了。</p>
<p><strong>现代 DFT 计算的核心——Kohn-Sham 方程</strong>。</p>
<p>它回答了两个终极问题： 1. 我们把所有“脏活累活”都塞进 <span
class="math inline">\(E_{xc}\)</span> (交换关联能) 里，那这个 <span
class="math inline">\(E_{xc}\)</span> <strong>到底是什么</strong>？ 2.
我们如何<strong>求解</strong>这个 KS 系统来找到轨道 <span
class="math inline">\(\phi_i\)</span> 和密度 <span
class="math inline">\(\rho\)</span>？</p>
<h3 id="概念e_xc-的正式定义-左侧">1. 概念：<span
class="math inline">\(E_{xc}\)</span> 的正式定义 (左侧)</h3>
<p>接下来给出了 Kohn-Sham 交换关联能 <span
class="math inline">\(E_{xc}\)</span>
的<strong>精确定义</strong>。它是一个“垃圾堆”泛函，包含了所有真实系统与虚构
KS 系统之间的差异。</p>
<ul>
<li><strong><span class="math inline">\(E_{xc}[\rho] = (T_{true}[\rho] -
T_s[\rho]) + (\langle \Psi_{true} | \hat{V}_{ee} | \Psi_{true} \rangle -
E_{Hartree}[\rho])\)</span></strong>
<ul>
<li><strong>第一部分：<span class="math inline">\((T_{true}[\rho] -
T_s[\rho])\)</span></strong>
<ul>
<li>这是<strong>动能的关联部分 <span
class="math inline">\(T_c\)</span></strong>。</li>
<li><span class="math inline">\(T_{true}\)</span>
是真实系统的（未知的）总动能。</li>
<li><span class="math inline">\(T_s\)</span> 是我们用 KS
轨道<strong>精确计算</strong>的无相互作用动能。</li>
<li><span class="math inline">\(E_{xc}\)</span>
必须包含这个动能差。</li>
</ul></li>
<li><strong>第二部分：<span class="math inline">\((\langle \Psi_{true} |
\hat{V}_{ee} | \Psi_{true} \rangle -
E_{Hartree}[\rho])\)</span></strong>
<ul>
<li>这是<strong>势能的交换与关联部分</strong>。</li>
<li><span class="math inline">\(\langle \Psi_{true} | \hat{V}_{ee} |
\Psi_{true} \rangle\)</span> 是真实系统中电子-电子相互作用 <span
class="math inline">\(\hat{V}_{ee}\)</span>
的完整（未知的）期望值。</li>
<li><span class="math inline">\(E_{Hartree}[\rho]\)</span>
是我们<strong>可以精确计算</strong>的经典静电排斥能（哈特里能量）。</li>
<li><span class="math inline">\(E_{xc}\)</span>
包含了真实相互作用与经典排斥之间的<strong>差值</strong>，这部分就是纯粹的量子效应（交换
+ 关联）。</li>
</ul></li>
</ul></li>
</ul>
<p><strong>一句话总结：<span class="math inline">\(E_{xc}\)</span>
包含了所有我们不知道的动能和势能的复杂量子效应。</strong></p>
<h3 id="核心公式kohn-sham-总能量">2. 核心公式：Kohn-Sham 总能量</h3>
<p>有了 <span class="math inline">\(E_{xc}\)</span> 的定义，Kohn-Sham
的<strong>总能量泛函</strong>现在可以被<strong>精确地</strong>（在形式上）写为：</p>
<p><span class="math display">\[E_{KS}[\rho] = \mathbf{T_s[\{\phi_i\}]}
+ \int d\vec{r} V_{ext}(\vec{r})\rho(\vec{r}) + \frac{1}{2}\iint
d\vec{r}d\vec{r}&#39;
\frac{\rho(\vec{r})\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|} +
\mathbf{E_{xc}[\rho]}\]</span></p>
<ul>
<li><strong><span
class="math inline">\(T_s[\{\phi_i\}]\)</span></strong>：无相互作用动能（<strong>通过轨道精确计算</strong>）。</li>
<li><strong><span class="math inline">\(\int V_{ext}
\rho\)</span></strong>：外势能（例如原子核吸引，已知）。</li>
<li><strong><span class="math inline">\(\frac{1}{2}\iint
...\)</span></strong>：哈特里能量（经典静电排斥，已知）。</li>
<li><strong><span
class="math inline">\(E_{xc}[\rho]\)</span></strong>：交换关联能（<strong>这是唯一需要近似的部分！</strong>）。</li>
</ul>
<p><strong>这是整个 KS-DFT 的基石。</strong> 我们成功地将一个无法解决的
<span class="math inline">\(T_{true}\)</span>
问题，转化为了一个<strong>可以被近似</strong>的 <span
class="math inline">\(E_{xc}\)</span> 问题。</p>
<h3 id="推导kohn-sham-方程">3. 推导：Kohn-Sham 方程</h3>
<p>我们如何找到使 <span class="math inline">\(E_{KS}[\rho]\)</span>
最小的轨道 <span class="math inline">\(\phi_i\)</span> 呢？
答案是使用<strong>变分法</strong>：我们对总能量 <span
class="math inline">\(E_{KS}\)</span> 求关于轨道 <span
class="math inline">\(\phi_i^*\)</span> 的泛函导数，并令其为零。</p>
<ul>
<li><strong><span class="math inline">\(\frac{\delta}{\delta\phi_i^*}
\left( E_{KS}[\rho] - \sum_j \epsilon_j (\int |\phi_j|^2 d\vec{r} - 1)
\right) = 0\)</span></strong>
<ul>
<li>这就是带有<strong>约束条件</strong>（每个轨道必须归一化）的能量最小化。</li>
<li><span class="math inline">\(\epsilon_j\)</span>
是拉格朗日乘子。</li>
</ul></li>
<li><strong>对 <span class="math inline">\(E_{KS}\)</span>
的每一项求导：</strong>
<ul>
<li><span class="math inline">\(\frac{\delta
T_s}{\delta\phi_i^*}\)</span> <span
class="math inline">\(\rightarrow\)</span> <span
class="math inline">\(-\frac{\hbar^2}{2m}\nabla^2
\phi_i(\vec{r})\)</span></li>
<li><span class="math inline">\(\frac{\delta
E_{V_{ext}}}{\delta\phi_i^*}\)</span> <span
class="math inline">\(\rightarrow\)</span> <span
class="math inline">\(V_{ext}(\vec{r}) \phi_i(\vec{r})\)</span></li>
<li><span class="math inline">\(\frac{\delta
E_{Hartree}}{\delta\phi_i^*}\)</span> <span
class="math inline">\(\rightarrow\)</span> <span
class="math inline">\(\left( \int
\frac{\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|} d\vec{r}&#39; \right)
\phi_i(\vec{r})\)</span> (即 <span
class="math inline">\(V_H(\vec{r})\phi_i(\vec{r})\)</span>)</li>
<li><span class="math inline">\(\frac{\delta
E_{xc}}{\delta\phi_i^*}\)</span> <span
class="math inline">\(\rightarrow\)</span> <span
class="math inline">\(\left( \frac{\delta E_{xc}}{\delta\rho} \right)
\phi_i(\vec{r})\)</span> (即 <span
class="math inline">\(V_{xc}(\vec{r})\phi_i(\vec{r})\)</span>)</li>
<li><span class="math inline">\(\frac{\delta
(\text{约束项})}{\delta\phi_i^*}\)</span> <span
class="math inline">\(\rightarrow\)</span> <span
class="math inline">\(\epsilon_i \phi_i(\vec{r})\)</span></li>
</ul></li>
<li><strong>把所有项合并，我们就得到了最终的 Kohn-Sham 方程：</strong>
<span class="math display">\[\left[ -\frac{\hbar^2}{2m}\nabla^2 +
V_{ext}(\vec{r}) + \int
\frac{\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|} d\vec{r}&#39; +
V_{xc}(\vec{r}) \right] \phi_i(\vec{r}) = \epsilon_i
\phi_i(\vec{r})\]</span>
<ul>
<li><span
class="math inline">\(\epsilon_i\)</span>（拉格朗日乘子）的物理意义是
<strong>Kohn-Sham 轨道 <span class="math inline">\(\phi_i\)</span>
的能量</strong>。</li>
</ul></li>
</ul>
<h3 id="结论v_ks-的最终形式">4. 结论：<span
class="math inline">\(V_{KS}\)</span> 的最终形式</h3>
<p><strong>Kohn-Sham
方程</strong>本质上是一个<strong>单粒子薛定谔方程</strong>
<code>Ĥ_KS φ_i = ε_i φ_i</code>。</p>
<p>通过上面的推导，我们明确地找到了这个虚构的 <strong>Kohn-Sham 势 <span
class="math inline">\(V_{KS}\)</span></strong> 到底是什么：</p>
<p><span class="math display">\[V_{KS}(\vec{r}) = V_{ext}(\vec{r}) +
V_H(\vec{r}) + V_{xc}(\vec{r})\]</span></p>
<p><span class="math display">\[V_{KS}(\vec{r}) =
\underbrace{V_{ext}(\vec{r})}_{\text{原子核势}} + \underbrace{\int
\frac{\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|}
d\vec{r}&#39;}_{\text{哈特里势 (经典排斥)}} + \underbrace{\frac{\delta
E_{xc}[\rho]}{\delta\rho}}_{\text{交换关联势 (量子效应)}}\]</span></p>
<h3 id="最终总结">最终总结</h3>
<p>这个方程完美地闭合了整个理论： * 我们想求解 <span
class="math inline">\(V_{KS}\)</span> 中的 <span
class="math inline">\(\phi_i\)</span>。 * 但 <span
class="math inline">\(V_{KS}\)</span> 本身又依赖于 <span
class="math inline">\(V_H\)</span> 和 <span
class="math inline">\(V_{xc}\)</span>。 * <span
class="math inline">\(V_H\)</span> 和 <span
class="math inline">\(V_{xc}\)</span> 又依赖于 <span
class="math inline">\(\rho\)</span>（密度）。 * 而 <span
class="math inline">\(\rho\)</span> 又是由 <span
class="math inline">\(\phi_i\)</span>（<span class="math inline">\(\rho
= \sum |\phi_i|^2\)</span>）构成的。</p>
<p>这是一个<strong>“鸡生蛋，蛋生鸡”</strong>的问题。
在实践中，我们必须通过<strong>自洽迭代 (self-consistent loop)</strong>
来求解： 1. <strong>猜测</strong>一个初始密度 <span
class="math inline">\(\rho_{in}\)</span>。 2. 用 <span
class="math inline">\(\rho_{in}\)</span> 计算 <span
class="math inline">\(V_H\)</span> 和 <span
class="math inline">\(V_{xc}\)</span>，得到 <span
class="math inline">\(V_{KS}\)</span>。 3. 求解 KS 方程 <span
class="math inline">\(\hat{H}_{KS} \phi_i = \epsilon_i \phi_i\)</span>
得到新的轨道 <span class="math inline">\(\phi_i\)</span>。 4. 用新的
<span class="math inline">\(\phi_i\)</span>
计算出<strong>新的密度</strong> <span class="math inline">\(\rho_{out} =
\sum |\phi_i|^2\)</span>。 5. 比较 <span
class="math inline">\(\rho_{out}\)</span> 和 <span
class="math inline">\(\rho_{in}\)</span>。如果它们不一致，就混合新旧密度，重复步骤
2。 6. 循环往复，直到 <span class="math inline">\(\rho_{out} =
\rho_{in}\)</span>，达到<strong>自洽</strong>，计算完成。</p>
<p>在前面，我们推导出了一切都取决于一个<strong>未知的</strong>、需要<strong>近似</strong>的能量项：<strong>交换关联泛函
<span class="math inline">\(E_{xc}[\rho]\)</span></strong>。</p>
<p>接下来展示的就是在实践中<strong>如何近似 <span
class="math inline">\(E_{xc}[\rho]\)</span></strong>
的两种最基本、最重要的方法：<strong>LDA</strong> 和
<strong>GGA</strong>。</p>
<h3 id="局域密度近似-local-density-approximation-lda">1. 局域密度近似
(Local Density Approximation, LDA)</h3>
<p>这是对 <span class="math inline">\(E_{xc}[\rho]\)</span>
最简单、最基础的近似。</p>
<ul>
<li><strong>图示：</strong>
白板左上角画了一个图：一个真实的、密度不均匀的分子（或固体），但在某一点
<span class="math inline">\(\vec{r}\)</span>
处，我们“假装”它周围是一片均匀的电子海洋（自由电子气）。</li>
<li><strong>核心思想 (LDA)：</strong> 系统在 <span
class="math inline">\(\vec{r}\)</span>
处的交换关联能量密度，<strong>仅仅</strong>取决于<strong>该点 <span
class="math inline">\(\vec{r}\)</span> 处的电子密度 <span
class="math inline">\(\rho(\vec{r})\)</span></strong>。
我们假设它与具有相同密度的<strong>均匀电子气 (free electron
gas)</strong> 的交换关联能量密度 <span
class="math inline">\(\epsilon_{xc}^{unif}(\rho)\)</span>
完全相同。</li>
<li><strong>公式 1：<span
class="math inline">\(E_{xc}^{LDA}[\rho]\)</span></strong> <span
class="math display">\[E_{xc}^{LDA}[\rho] = \int d\vec{r} \rho(\vec{r})
\epsilon_{xc}^{unif}(\rho(\vec{r}))\]</span>
<ul>
<li><strong><span
class="math inline">\(\epsilon_{xc}^{unif}(\rho)\)</span></strong>：这就是我们在<strong>第
3
张白板</strong>上推导过的<strong>均匀电子气</strong>的（单粒子）交换关联能量。这是一个已知的、关于
<span class="math inline">\(\rho\)</span>
的函数（通过量子蒙特卡洛等方法可以精确算出）。</li>
<li><strong><span
class="math inline">\(\rho(\vec{r})\)</span></strong>：该点的电子密度。</li>
<li><strong>含义：</strong> 我们在空间中逐点计算该点的 <span
class="math inline">\(\rho\)</span> 对应的 <span
class="math inline">\(\epsilon_{xc}\)</span>，然后乘以该点的密度 <span
class="math inline">\(\rho\)</span>，最后在整个空间积分（求和）。</li>
</ul></li>
<li><strong>公式 2：<span class="math inline">\(V_{xc}^{LDA}\)</span>
(交换关联势)</strong> <span class="math display">\[V_{xc} = \frac{\delta
E_{xc}}{\delta \rho} = \epsilon_{xc}(\rho, \vec{r}) + \rho(\vec{r})
\frac{\partial \epsilon_{xc}}{\partial \rho}\]</span>
<ul>
<li>这是将 <span class="math inline">\(E_{xc}^{LDA}\)</span>
代入我们在<strong>第 5 张白板</strong>上定义的 <span
class="math inline">\(V_{xc} = \delta E_{xc} / \delta \rho\)</span>
中，通过链式法则推导出来的结果。</li>
</ul></li>
<li><strong>“Perdew-Zunger (1981)”</strong>
<ul>
<li>这是一个<strong>具体的 LDA 泛函</strong>的名称。Perdew 和 Zunger 在
1981 年利用高精度的均匀电子气数据（来自 Ceperley-Alder 的 QMC
计算），拟合出了一套非常精确和实用的 <span
class="math inline">\(\epsilon_{xc}^{unif}(\rho)\)</span>
参数化公式。这至今仍是 LDA 计算的标准。</li>
</ul></li>
</ul>
<h3 id="广义梯度近似-generalized-gradient-approximation-gga">2.
广义梯度近似 (Generalized-Gradient Approximation, GGA)</h3>
<p>LDA 假设在 <span class="math inline">\(\vec{r}\)</span>
处的能量只取决于 <span
class="math inline">\(\rho(\vec{r})\)</span>，这是一个<strong>非常强</strong>（且通常不正确）的假设。真实系统（如分子）的密度变化非常快。</p>
<ul>
<li><strong>核心思想 (GGA)：</strong>
一个更智能的近似，不仅要考虑<strong>该点的密度 <span
class="math inline">\(\rho(\vec{r})\)</span></strong>，还应该考虑<strong>该点密度的变化率</strong>，即<strong>梯度的模
<span class="math inline">\(|\nabla\rho|\)</span></strong>。
<ul>
<li>如果 <span class="math inline">\(|\nabla\rho|\)</span>
很大，说明密度变化剧烈（如在分子键合区域或原子边缘），<span
class="math inline">\(\epsilon_{xc}\)</span> 应该与 LDA 不同。</li>
</ul></li>
<li><strong>公式 3：<span
class="math inline">\(E_{xc}^{GGA}[\rho^\uparrow,
\rho^\downarrow]\)</span></strong> <span
class="math display">\[E_{xc}^{GGA}[\rho^\uparrow, \rho^\downarrow] =
\int d\vec{r} \rho(\vec{r}) \epsilon_{xc}(\rho^\uparrow,
\rho^\downarrow, |\nabla\rho^\uparrow|, |\nabla\rho^\downarrow|,
\vec{r})\]</span>
<ul>
<li><strong><span class="math inline">\(\rho^\uparrow,
\rho^\downarrow\)</span></strong>：自旋向上和自旋向下的电子密度（更完整的表述）。</li>
<li><strong><span class="math inline">\(|\nabla\rho^\uparrow|,
|\nabla\rho^\downarrow|\)</span></strong>：<strong>新加入的项！</strong>
密度梯度的信息被包含了进来。</li>
<li><strong><span
class="math inline">\(\epsilon_{xc}(...)\)</span></strong>：现在是一个更复杂的函数，它不仅是
<span class="math inline">\(\rho\)</span> 的函数，还是 <span
class="math inline">\(|\nabla\rho|\)</span> 的函数。</li>
</ul></li>
<li><strong>“PBE”, “BLYP” …</strong>
<ul>
<li>这些都是<strong>具体的 GGA 泛函</strong>的名称。它们就像 <span
class="math inline">\(\epsilon_{xc}\)</span> 的不同“配方”。</li>
<li><strong>BLYP</strong> = Becke (交换) + Lee, Yang, Parr (关联)。</li>
<li><strong>PBE</strong> = Perdew, Burke, Ernzerhof。</li>
<li>PBE 和 BLYP 是化学和材料科学中<strong>最常用、最成功</strong>的 GGA
泛函之二。</li>
</ul></li>
</ul>
<p>接下来介绍的是“Jacob’s Ladder”（DFT
近似的雅各天梯）的更高几层：<strong>meta-GGA</strong> 和 <strong>Hybrid
functionals (混合泛函)</strong>。</p>
<h3 id="密度泛函近似-dfa">1. 密度泛函近似 (DFA)</h3>
<ul>
<li><strong>DFA (Density Functional Approximation):</strong>
<ul>
<li>这是一个总称，泛指我们对 <span
class="math inline">\(E_{xc}[\rho]\)</span>（交换关联泛函）所做的<strong>所有近似</strong>，包括
LDA, GGA 等。</li>
</ul></li>
</ul>
<h3 id="meta-gga">2. Meta-GGA</h3>
<p>这是超越 GGA 的“天梯”的下一级。</p>
<ul>
<li><strong><span class="math inline">\(\Delta\)</span> meta
GGA:</strong>
<ul>
<li><strong>核心思想：</strong> GGA 只用了 <span
class="math inline">\(\rho\)</span>（密度）和 <span
class="math inline">\(|\nabla\rho|\)</span>（密度梯度）。为了获得更高精度，meta-GGA
引入了<strong>第三种信息</strong>。</li>
<li><strong>成分：<span class="math inline">\(\rho, |\nabla\rho|,
\nabla^2\rho? |\nabla\phi_i|^2\)</span></strong>
<ul>
<li><span class="math inline">\(\rho\)</span> (密度)</li>
<li><span class="math inline">\(|\nabla\rho|\)</span> (密度梯度)</li>
<li><code>∇²ρ?</code> (密度的拉普拉斯，一种可能的成分)</li>
<li><strong><span
class="math inline">\(|\nabla\phi_i|^2\)</span></strong> (Kohn-Sham
轨道的动能密度 <span class="math inline">\(\tau\)</span>)：这是现代
meta-GGA
泛函中<strong>最关键</strong>的成分。它使得泛函<strong>间接地</strong>依赖于轨道，从而能“感知”更复杂的电子结构信息（例如，区分是单键、双键还是孤对电子）。</li>
</ul></li>
</ul></li>
<li><strong>“SCAN”</strong>
<ul>
<li>这是一个<strong>具体的 meta-GGA
泛函</strong>的名称，是目前最流行、最成功的 meta-GGA 之一。</li>
</ul></li>
</ul>
<h3 id="一个关键问题自相互作用误差-sie">3. 一个关键问题：自相互作用误差
(SIE)</h3>
<p>为什么我们需要更高级的泛函？因为 LDA 和 GGA
有一个<strong>根本性的缺陷</strong>。</p>
<ul>
<li><strong>“self-interaction error” (自相互作用误差, SIE):</strong>
<ul>
<li><strong>问题来源：</strong> 在 DFT 中，一个电子的密度 <span
class="math inline">\(\rho\)</span> 是其自身的总密度 <span
class="math inline">\(\rho\)</span> 的一部分。在计算哈特里能量 <span
class="math inline">\(E_H[\rho] \propto \iint \rho \rho&#39;
...\)</span>
时，这个电子<strong>错误地与它自己</strong>产生了静电排斥。</li>
<li><strong>本应：</strong> <span
class="math inline">\(E_{xc}\)</span>（交换能）应该<strong>完美地抵消</strong>掉这个虚假的自排斥。</li>
<li><strong>现实：</strong> <strong>LDA 和 GGA
泛函都未能</strong>完美抵消它。</li>
</ul></li>
<li><strong>“charge too delocalized” (电荷过度离域):</strong>
<ul>
<li><strong>后果：</strong>
由于电子错误地“排斥”自己，系统为了降低能量，会倾向于将电子“涂抹”或“离域”
(delocalize) 到尽可能大的空间中，以减小这种虚假的自排斥。</li>
<li><strong>图示：</strong> 白板上的山峰（或势垒）图示说明：SIE
导致电荷在过渡态的局域化（电子集中在某处）变得困难，因此 LDA/GGA
总是<strong>低估</strong>化学反应的<strong>能垒</strong>。</li>
<li><strong>“Cl- — nH₂O”</strong>
<ul>
<li>这是一个具体例子：一个 Cl⁻ 离子被水分子包围。LDA/GGA 会错误地将 Cl⁻
上的负电荷“泄露”或“离域”到周围的水分子上，从而导致错误的体系结构和能量。</li>
</ul></li>
</ul></li>
</ul>
<h3 id="解决方案混合泛函-hybrid-functional">4. 解决方案：混合泛函
(Hybrid Functional)</h3>
<p>这是“天梯”的第四级，是目前在化学计算中<strong>最标准、最常用</strong>的高精度方法。</p>
<ul>
<li><strong><span class="math inline">\(\Delta\)</span> Hybrid
functional:</strong>
<ul>
<li><strong>核心思想：</strong> 如何修复 SIE？
<ul>
<li>我们知道，在 <strong>Hartree-Fock (HF) 理论</strong>中，其“交换能”
<span class="math inline">\(E_x^{HF}\)</span>
是通过轨道<strong>精确计算</strong>的，并且它<strong>完美地</strong>抵消了自相互作用。</li>
<li><strong>混合 (Hybrid) 思想：</strong> 让我们把 DFT (如 PBE)
的交换项拿掉一部分，<strong>替换</strong>为同一比例的“精确”的 HF
交换项。</li>
</ul></li>
</ul></li>
<li><strong>“PBE0” (一个具体的混合泛函名称):</strong>
<ul>
<li>这是最著名的混合泛函之一。</li>
<li><strong><span class="math inline">\(E_{xc} = \frac{1}{4} E_x^{HF} +
\frac{3}{4} E_x^{PBE} + E_c^{PBE}\)</span></strong></li>
<li><strong>公式分解：</strong>
<ul>
<li><strong><span class="math inline">\(\frac{1}{4}
E_x^{HF}\)</span></strong>：用 <strong>25%</strong> 的<strong>精确 HF
交换</strong>。</li>
<li><strong><span class="math inline">\(\frac{3}{4}
E_x^{PBE}\)</span></strong>：用 <strong>75%</strong> 的 <strong>PBE
(GGA) 交换</strong>。</li>
<li><strong><span class="math inline">\(E_c^{PBE}\)</span></strong>：用
<strong>100%</strong> 的 <strong>PBE (GGA) 关联</strong>。</li>
</ul></li>
<li><strong>效果：</strong> 引入 25%
的精确交换，极大地<strong>纠正</strong>了自相互作用误差
(SIE)，使其在计算能垒、带隙、分子性质等方面远比纯 GGA 准确。</li>
</ul></li>
</ul>
<p>接下来介绍了当今计算化学和材料科学中<strong>最先进、最常用</strong>的几种高级泛函。</p>
<p><strong>混合泛函 (Hybrid functional) 家族的“动物园”</strong>——
它们是如何被构建的，以及它们各自解决了什么问题。</p>
<h3 id="b3lyp-最著名的经验混合泛函">1. B3LYP (最著名的经验混合泛函)</h3>
<ul>
<li><strong><span class="math inline">\(\Delta\)</span> B3LYP:</strong>
<ul>
<li>这是<strong>最著名</strong>的混合泛函之一，特别是在量子化学领域。</li>
<li>它的构造是<strong>半经验的
(semi-empirical)</strong>，意味着它的混合参数是由拟合（fitting）一组精确的实验/基准化学数据（如分子的原子化热）而确定的。</li>
</ul></li>
<li><strong>公式：</strong> <span class="math display">\[E_{xc} =
E_x^{LDA} + a_0(E_x^{HF} - E_x^{LDA}) + a_x(E_x^{Becke} - E_x^{LDA}) +
E_c^{LDA} + a_c(E_c^{LYP} - E_c^{LDA})\]</span></li>
<li><strong>解释：</strong>
<ul>
<li>B3LYP (Becke, 3-parameter, Lee-Yang-Parr)
是一个复杂的“鸡尾酒”。</li>
<li>它混合了 <strong>LDA</strong> 的交换和关联、<strong>Hartree-Fock
(HF)</strong> 的精确交换，以及 <strong>GGA</strong> 的交换 (Becke88)
和关联 (LYP)。</li>
<li><code>a_0</code>, <code>a_x</code>, <code>a_c</code>
是三个被拟合的参数，用于确定每种成分的“配比”。</li>
</ul></li>
</ul>
<h3 id="自洽混合泛函-self-consistent-hybrid">2. 自洽混合泛函
(Self-consistent hybrid)</h3>
<ul>
<li><strong><span class="math inline">\(\Delta\)</span> Self-consistent
hybrid:</strong>
<ul>
<li><strong>思想：</strong> 与其像 PBE0（固定 25%）或
B3LYP（经验拟合）那样<em>指定</em>一个混合参数 <span
class="math inline">\(\alpha\)</span>，我们是否能从<strong>第一性原理
(ab initio)</strong> 出发，让系统<strong>自己决定</strong>应该混合多少
HF 交换？</li>
</ul></li>
<li><strong>公式：</strong>
<ul>
<li><span class="math inline">\(E_{xc} = \alpha E_x^{HF} + (1-\alpha)
E_x^{GGA} + E_c^{GGA}\)</span> (这与 PBE0 的形式相同)</li>
</ul></li>
<li><strong>关键创新：</strong>
<ul>
<li><strong><span class="math inline">\(\alpha =
\frac{1}{\epsilon_\infty}\)</span></strong></li>
<li><strong>解释：</strong> 混合比例 <span
class="math inline">\(\alpha\)</span> 被设定为材料<strong>高频介电常数
<span class="math inline">\(\epsilon_\infty\)</span>
的倒数</strong>。</li>
<li><strong>物理意义：</strong> <span
class="math inline">\(\epsilon_\infty\)</span>
描述了材料中电子<strong>屏蔽 (screening)</strong> 库仑相互作用的能力。
<ul>
<li>绝缘体/半导体：<span class="math inline">\(\epsilon_\infty\)</span>
较小（例如 2-10），<span class="math inline">\(\alpha\)</span>
较大（例如 10-50%），需要更多 HF 交换来打开带隙。</li>
<li>金属：<span class="math inline">\(\epsilon_\infty \to
\infty\)</span>，<span class="math inline">\(\alpha \to
0\)</span>，不需要 HF 交换（退化为纯 GGA）。</li>
</ul></li>
<li>这是一个<strong>自洽</strong>过程：<span
class="math inline">\(\alpha\)</span> 决定了电子结构，而电子结构（通过
<code>GW</code> 等理论）又决定了 <span
class="math inline">\(\epsilon_\infty\)</span>。</li>
</ul></li>
</ul>
<h3 id="范围分离混合泛函-range-separated-hybrid">3. 范围分离混合泛函
(Range-separated hybrid)</h3>
<ul>
<li><strong><span class="math inline">\(\Delta\)</span> Range-separated
hybrid:</strong>
<ul>
<li><strong>思想：</strong> 电子-电子排斥 <span
class="math inline">\(1/r\)</span>
在短距离和长距离下表现不同。也许我们不需要“一刀切”的混合。</li>
<li><strong>策略：</strong> 将 <span class="math inline">\(1/r\)</span>
分割为<strong>短程 (short-range, SR)</strong> 和<strong>长程
(long-range, LR)</strong>
两部分，并对它们使用<strong>不同</strong>的泛函。</li>
<li>旁边的涂鸦 <code>... ~ ...</code> 和
<code>screening</code>（屏蔽）形象地说明了这种思想：在长程，相互作用被“屏蔽”了。</li>
</ul></li>
<li><strong>公式 (以 HSE06 为例):</strong>
<ul>
<li>白板上写了 <code>HSE06'</code>（HSE 泛函的 2006
年版本），并给出了其参数：</li>
<li><strong><span class="math inline">\(\alpha = 0, \beta =
1/4\)</span></strong> (代入白板上那个复杂的通用公式)</li>
</ul></li>
<li><strong>HSE06 泛函的通俗解释：</strong>
<ul>
<li>它将 PBE (GGA) 泛函的<strong>交换部分 <span
class="math inline">\(E_x^{PBE}\)</span></strong> 进行了范围分离：</li>
<li><strong>在短程 (SR):</strong> <span class="math inline">\(E_x =
\frac{1}{4} E_x^{HF,SR} + \frac{3}{4} E_x^{PBE,SR}\)</span>
<ul>
<li>(在短距离内，它是一个 PBE0 泛函，混合了 25% 的 HF 精确交换)</li>
</ul></li>
<li><strong>在长程 (LR):</strong> <span class="math inline">\(E_x =
E_x^{PBE,LR}\)</span>
<ul>
<li>(在长距离处，它退化为 100% 的 PBE 纯 GGA 交换)</li>
</ul></li>
<li><strong>关联部分 <span class="math inline">\(E_c\)</span></strong>
始终是 100% 的 PBE 关联。</li>
</ul></li>
<li><strong>为什么这样做？</strong>
<ul>
<li><strong>物理上：</strong> 修正了长程的自相互作用误差。</li>
<li><strong>计算上：</strong> 在<strong>固体 (solid)</strong>
计算中<strong>极其重要</strong>。HF
交换的计算量非常大，尤其是在长程。HSE
泛函通过在长程<strong>关闭</strong>HF
交换（即“屏蔽”它），使得计算<strong>速度</strong>大幅提升，同时保留了混合泛函在短程（如化学键）的<strong>高精度</strong>。</li>
</ul></li>
</ul>
<h3 id="总结">总结</h3>
<ol type="1">
<li><strong>1 (HK 定理):</strong> 奠定了<strong>理论基石</strong>——
<span class="math inline">\(E = E[\rho]\)</span>。</li>
<li><strong>2 (OF-DFT):</strong> 提出了<strong>理想</strong>的无轨道
DFT，并指出了其 <span class="math inline">\(T[\rho]\)</span>
的<strong>困难</strong>。</li>
<li><strong>3 (自由电子气):</strong> <strong>推导</strong>了 <span
class="math inline">\(T[\rho]\)</span> 的<strong>最简近似</strong> <span
class="math inline">\(T \propto \int \rho^{5/3}\)</span>。</li>
<li><strong>4 (OF-DFT 求解):</strong>
展示了如何通过<strong>变分法</strong> <span class="math inline">\(\delta
E / \delta \rho = \mu\)</span> 求解 <span
class="math inline">\(\rho_0\)</span>。</li>
<li><strong>5 (Kohn-Sham):</strong> 引入了<strong>实用方案
(KS-DFT)</strong>，用“轨道” <span class="math inline">\(\phi_i\)</span>
<strong>精确计算</strong>动能 <span
class="math inline">\(T_s\)</span>，将未知项塞入 <span
class="math inline">\(E_{xc}\)</span>。</li>
<li><strong>6 (LDA/GGA):</strong> 展示了如何<strong>近似 <span
class="math inline">\(E_{xc}\)</span></strong>（“雅各天梯”的第一、二层），即
LDA（依赖 <span class="math inline">\(\rho\)</span>）和 GGA（依赖 <span
class="math inline">\(\rho, |\nabla\rho|\)</span>）。</li>
<li><strong>7 (Meta-GGA / Hybrid):</strong> 攀登天梯的更高层 (Meta-GGA,
Hybrid-PBE0)，并指出了 LDA/GGA
的<strong>根本缺陷</strong>（自相互作用误差 SIE）。</li>
<li><strong>8 (B3LYP / HSE):</strong>
展示了<strong>最先进的泛函</strong> (B3LYP, HSE06)
是如何通过经验拟合或范围分离等技巧，在精度和计算效率之间达到最佳平衡的。</li>
</ol>
<p>这完整地勾勒出了 DFT 从 1960
年代的抽象理论到当今最前沿的计算工具的整个发展脉络。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/11/07/5120c9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/11/07/5120c9/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W9-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-11-07 21:00:00 / 修改时间：17:17:51" itemprop="dateCreated datePublished" datetime="2025-11-07T21:00:00+08:00">2025-11-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - 计算能源材料和电子结构模拟 Lecture</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<p><strong>密度泛函理论 (Density Functional Theory, DFT)</strong>
的核心概念笔记，这是一种在物理和化学领域，特别是量子化学和凝聚态物理中，用于研究多电子体系电子结构的计算方法。</p>
<h3 id="密度泛函理论-density-functional-theory">密度泛函理论 (Density
Functional Theory)</h3>
<ul>
<li><strong>N-electron (N 电子体系):</strong>
<ul>
<li>波函数 (Wavefunction): <span class="math inline">\(\Psi(\vec{r}_1,
\vec{r}_2, ..., \vec{r}_N): \mathbb{R}^{3N} \to \mathbb{C}\)</span>
<ul>
<li>这是一个包含 N 个电子的体系，其波函数 <span
class="math inline">\(\Psi\)</span> 是 <span
class="math inline">\(3N\)</span>
个空间坐标（每个电子有3个坐标）的函数，值域为复数 <span
class="math inline">\(\mathbb{C}\)</span>。这是一个非常高维度的复杂函数。</li>
</ul></li>
<li>电子密度 (Electron density): <span
class="math inline">\(\rho(\vec{r}): \mathbb{R}^3 \to
\mathbb{R}\)</span>
<ul>
<li>电子密度 <span class="math inline">\(\rho\)</span> 只是空间中一点
<span
class="math inline">\(\vec{r}\)</span>（3个坐标）的函数，值域为实数
<span class="math inline">\(\mathbb{R}\)</span>。</li>
<li>DFT 的核心思想就是用这个更简单的 <span
class="math inline">\(\rho(\vec{r})\)</span> 来代替复杂的 <span
class="math inline">\(\Psi\)</span> 作为基本变量。</li>
</ul></li>
</ul></li>
<li><strong>H.K. (Hohenberg-Kohn) 定理:</strong>
<ul>
<li>这是 DFT 的理论基石。白板上的图示 ① 和 ② 总结了这两个定理。</li>
<li><strong>图示 ① (H.K. 第一定理):</strong>
<ul>
<li><code>Vext/H</code> <span
class="math inline">\(\leftrightarrow\)</span> <code>ρ(r)</code> <span
class="math inline">\(\leftrightarrow\)</span>
<code>Ψi(r1, r2, ...)</code></li>
<li><strong>含义:</strong> 体系的外势 <span
class="math inline">\(V_{ext}\)</span>
（通常指原子核对电子的吸引势，它决定了体系的哈密顿量 <span
class="math inline">\(H\)</span>）与基态电子密度 <span
class="math inline">\(\rho(\vec{r})\)</span> 之间存在一一对应关系。</li>
<li><strong>推论:</strong> 由于 <span
class="math inline">\(V_{ext}\)</span> 决定了波函数 <span
class="math inline">\(\Psi\)</span>，因此，基态电子密度 <span
class="math inline">\(\rho_0\)</span> 唯一地决定了基态波函数 <span
class="math inline">\(\Psi_0\)</span> 以及体系的所有性质。</li>
</ul></li>
<li><strong>图示 ② (H.K. 第二定理):</strong>
<ul>
<li><code>min E[ρ(r)] → ρ₀, Egs</code></li>
<li><strong>含义:</strong> 能量泛函 <span
class="math inline">\(E[\rho]\)</span>
（能量是电子密度的函数）在正确的基态密度 <span
class="math inline">\(\rho_0\)</span>
处取得最小值，这个最小值就是体系的基态能量 <span
class="math inline">\(E_{gs}\)</span>。</li>
</ul></li>
</ul></li>
<li><strong>Levy-Lieb 泛函 (Levy-Lieb functional):</strong>
<ul>
<li>也称为 <strong>Levy 约束搜索 (constrained search)
泛函</strong>。这是对 H.K. 定理中能量泛函 <span
class="math inline">\(E[\rho]\)</span> 的一种更严格和普适的定义。</li>
<li><code>ρ ← Vext : v-representability</code>
<ul>
<li>这提出了一个问题：什么样的密度 <span
class="math inline">\(\rho\)</span> 可以对应于某个外势 <span
class="math inline">\(V_{ext}\)</span>？这被称为“v-可表征性”问题。</li>
</ul></li>
<li><code>Δ</code> (三角符号通常表示 “定义为”)</li>
<li><code>N</code> (圆圈) <span class="math inline">\(\supset\)</span>
<code>K</code> (圆圈)
<ul>
<li><em>这个符号旁边的字迹有些潦草，但结合上下文，这可能是在区分
N-representability（N-可表征性）和
v-representability（v-可表Vext表征性）。</em></li>
</ul></li>
</ul></li>
</ul>
<h3 id="能量泛函与计算">能量泛函与计算</h3>
<ul>
<li><strong>约束搜索 (Constrained search):</strong>
<ul>
<li><span class="math inline">\(\int \rho(\vec{r}) d\vec{r} = N\)</span>
<ul>
<li>这是一个约束条件，即电子密度在全空间积分必须等于体系的总电子数 <span
class="math inline">\(N\)</span>。</li>
</ul></li>
</ul></li>
<li><strong>① 能量泛函 <span class="math inline">\(E_{LL}[\rho]\)</span>
(Levy-Lieb):</strong>
<ul>
<li><span class="math inline">\(E_{LL}[\rho] = \min_{\Psi \to \rho}
\langle \Psi | \hat{T} + \hat{V}_{ee} | \Psi \rangle + \int d\vec{r}
V_{ext}(\vec{r}) \rho(\vec{r})\)</span></li>
<li><strong>解释:</strong>
<ul>
<li>这个公式定义了总能量 <span class="math inline">\(E\)</span>
如何作为密度 <span class="math inline">\(\rho\)</span> 的泛函。</li>
<li>它分为两部分：
<ol type="1">
<li><span class="math inline">\(\int d\vec{r} V_{ext}(\vec{r})
\rho(\vec{r})\)</span>: 电子在外势 <span
class="math inline">\(V_{ext}\)</span> 中的能量。这部分是已知的（如果
<span class="math inline">\(V_{ext}\)</span> 和 <span
class="math inline">\(\rho\)</span> 已知）。</li>
<li><span class="math inline">\(\min_{\Psi \to \rho} \langle \Psi |
\hat{T} + \hat{V}_{ee} | \Psi \rangle\)</span>: 这是
<strong>Hohenberg-Kohn 泛函</strong> <span
class="math inline">\(F_{HK}[\rho]\)</span>（也常被称为 Levy-Lieb
泛函），它代表动能 <span class="math inline">\(\hat{T}\)</span>
和电子-电子相互作用能 <span class="math inline">\(\hat{V}_{ee}\)</span>
的总和。</li>
</ol></li>
<li><strong>约束搜索 (min <span class="math inline">\(\Psi \to
\rho\)</span>)</strong>： <span
class="math inline">\(F_{HK}[\rho]\)</span>
的值是通过搜索所有能够产生该密度 <span
class="math inline">\(\rho\)</span> 的波函数 <span
class="math inline">\(\Psi\)</span>，并从中找出使 <span
class="math inline">\(\langle \hat{T} + \hat{V}_{ee} \rangle\)</span>
最小的那个 <span class="math inline">\(\Psi\)</span> 来确定的。这个泛函
<span class="math inline">\(F_{HK}[\rho]\)</span> 是<strong>普适
(Universal)</strong> 的，因为它不依赖于 <span
class="math inline">\(V_{ext}\)</span>。</li>
</ul></li>
</ul></li>
<li><strong>② 求解基态 (Finding the Ground State):</strong>
<ul>
<li><span class="math inline">\(E_{gs} = \min_{\rho}
E_{LL}[\rho]\)</span>
<ul>
<li><strong>含义 (H.K. 第二定理的变分原理):</strong> 体系的基态能量
<span class="math inline">\(E_{gs}\)</span> 可以通过最小化总能量泛函
<span class="math inline">\(E_{LL}[\rho]\)</span> 来获得。</li>
</ul></li>
<li><span class="math inline">\(\rho_0 \to \{\Psi_0^{(1)}, \Psi_0^{(2)},
...\} \Rightarrow V_{ext}\)</span>
<ul>
<li><strong>含义:</strong> 当找到最优的基态密度 <span
class="math inline">\(\rho_0\)</span> 时，我们就同时确定了基态能量 <span
class="math inline">\(E_{gs}\)</span>。通过这个 <span
class="math inline">\(\rho_0\)</span>（以及它对应的波函数集合），原则上也可以反推出唯一确定它的外势
<span class="math inline">\(V_{ext}\)</span>。</li>
</ul></li>
</ul></li>
</ul>
<h3 id="其他笔记">其他笔记</h3>
<ul>
<li><strong><span class="math inline">\(p_i \propto e^{-\beta E_i /
Z}\)</span> finite temperature</strong>
<ul>
<li>这是白板右下角的一行字，与上面的 DFT 主题略有不同。</li>
<li><strong>含义:</strong> 这描述的是<strong>有限温度 (finite
temperature)</strong> 下的<strong>正则系综 (canonical
ensemble)</strong>。</li>
<li><span class="math inline">\(p_i\)</span> 是体系处于能量为 <span
class="math inline">\(E_i\)</span> 的某个状态的概率。</li>
<li><span class="math inline">\(\beta = 1 / (k_B T)\)</span>，<span
class="math inline">\(k_B\)</span> 是玻尔兹曼常数，<span
class="math inline">\(T\)</span> 是温度。</li>
<li><span class="math inline">\(Z\)</span> 是配分函数 (Partition
function)。</li>
<li>这个公式（玻尔兹曼分布）是统计力学的基础，用于将 DFT 推广到 <span
class="math inline">\(T &gt; 0\)</span> K 的情况（即 Mermin
泛函）。</li>
</ul></li>
</ul>
<h3 id="总结">总结</h3>
<p>总结了密度泛函理论 (DFT) 的数学和物理基础，从 Hohenberg-Kohn
定理（能量和密度的一一对应及变分原理）讲到了 Levy-Lieb
的约束搜索构造方法，这是现代 DFT
理论的核心。右下角的笔记则暗示了如何将此理论推广到有限温度体系。</p>
<p>这是（基态 DFT）内容的延续，主题是<strong>有限温度和系综 DFT (Finite
temperature and ensemble DFT)</strong>，以及<strong>无轨道 DFT
(Orbital-free DFT)</strong>。</p>
<p>以下是白板上内容的逐条中文转录和解释：</p>
<h3 id="有限温度和系综-dft-mermin-dft">有限温度和系综 DFT (Mermin
DFT)</h3>
<p>这部分内容将 DFT 从 <span
class="math inline">\(T=0\text{K}\)</span>（绝对零度，只关心基态）推广到
<span class="math inline">\(T &gt;
0\text{K}\)</span>（有限温度，需要考虑热激发和系综平均）。</p>
<ul>
<li><strong>对比 <span class="math inline">\(T=0\text{K}\)</span> 和
<span class="math inline">\(T\)</span> finite (有限温度):</strong>
<ul>
<li><strong><span class="math inline">\(T=0K\)</span> (左栏):</strong>
<ul>
<li><span class="math inline">\(\rho_0\)</span> (基态密度)</li>
<li><span class="math inline">\(|\Psi\rangle\)</span> (基态波函数)</li>
<li><span class="math inline">\(\langle \hat{O} \rangle = \langle \Psi |
\hat{O} | \Psi \rangle\)</span> (零温下的期望值)</li>
</ul></li>
<li><strong><span class="math inline">\(T\)</span> finite
(中栏):</strong>
<ul>
<li><span class="math inline">\(p_e\)</span> (系综密度 /
热力学密度)</li>
<li><span class="math inline">\(\hat{\rho} = \sum_i f_i
|\Psi_i\rangle\langle\Psi_i|\)</span> (密度矩阵)</li>
<li><span class="math inline">\(\langle \hat{O} \rangle =
\text{Tr}(\hat{\rho}\hat{O})\)</span>
(有限温度下的期望值，即系综平均)</li>
</ul></li>
</ul></li>
<li><strong>Mermin 泛函 (Mermin Functional):</strong>
<ul>
<li>这是 Mermin (1965年) 对 DFT 的推广，用于描述有限温度下的体系。</li>
<li>目标是最小化<strong>自由能 (Free
Energy)</strong>，而不是总能量。</li>
<li><strong><span class="math inline">\(F_{Mermin}[\rho] =
\min_{\hat{\rho} \to \rho} \text{Tr} \{ \hat{\rho} [ \hat{H} +
\frac{1}{\beta}\ln\hat{\rho} ] \}\)</span></strong>
<ul>
<li><strong>解释:</strong>
<ul>
<li>这里的 <span class="math inline">\(F\)</span> 不是指 Hohenberg-Kohn
泛函，而是指<strong>亥姆霍兹自由能 (Helmholtz free energy)</strong>
<span class="math inline">\(\Omega\)</span>（或 <span
class="math inline">\(A\)</span>）。白板上写 <span
class="math inline">\(F\)</span> 可能是指普适泛函部分。</li>
<li><span class="math inline">\(\hat{H}\)</span> 是哈密顿量（不含 <span
class="math inline">\(V_{ext}\)</span> 的部分，即 <span
class="math inline">\(\hat{T} + \hat{V}_{ee}\)</span>）。</li>
<li><span class="math inline">\(\frac{1}{\beta}\ln\hat{\rho}\)</span>
这一项与<strong>熵 (Entropy)</strong> 相关 (<span
class="math inline">\(S = -k_B
\text{Tr}(\hat{\rho}\ln\hat{\rho})\)</span>)。</li>
<li><span class="math inline">\(\beta = 1 / (k_B T)\)</span>。</li>
<li>整个 <span class="math inline">\(\text{Tr}\{...\}\)</span>
表达式代表系统的亥姆霍兹自由能 <span
class="math inline">\(A\)</span>。</li>
<li><span class="math inline">\(\min_{\hat{\rho} \to \rho}\)</span>：与
Levy-Lieb 约束搜索类似，这里是搜索所有能产生密度 <span
class="math inline">\(\rho\)</span> 的密度矩阵 <span
class="math inline">\(\hat{\rho}\)</span>，并找到使自由能最小的那个。</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>② 自由能最小化:</strong>
<ul>
<li><strong><span class="math inline">\(F_e = \min_{\hat{\rho}_e}
f_{mermin}[\rho], \rho_e\)</span></strong>
<ul>
<li><em>这行字迹有些潦草，但结合上下文，它表达的意思是：</em>
体系的平衡自由能 <span class="math inline">\(F_e\)</span> (或 <span
class="math inline">\(\Omega_e\)</span>) 是通过最小化 Mermin
泛函得到的，此时对应的密度是平衡态密度 <span
class="math inline">\(\rho_e\)</span>。</li>
</ul></li>
<li><strong><span class="math inline">\(\hat{\rho}_e = \frac{e^{-\beta
\hat{H}}}{\text{Tr}(e^{-\beta \hat{H}})} = \frac{e^{-\beta
\hat{H}}}{Z}\)</span></strong>
<ul>
<li>这是热力学平衡态下的<strong>正则系综密度矩阵 (equilibrium density
matrix)</strong>。<span class="math inline">\(Z\)</span>
是配分函数。</li>
</ul></li>
<li><strong><span class="math inline">\(\hat{H} =
-\frac{1}{\beta}\ln(\hat{\rho}_e)\)</span></strong>
<ul>
<li>这是上一个公式的简单变形，反解出哈密顿量 <span
class="math inline">\(\hat{H}\)</span>。</li>
</ul></li>
</ul></li>
<li><strong>应用场景 (右上角):</strong>
<ul>
<li><strong><span class="math inline">\(T &gt; 10,000K\)</span>
warm-dense materials</strong>
<ul>
<li>指出这种有限温度 DFT 理论常用于研究<strong>温稠密物质</strong>
(WDM)，如行星核心或惯性约束聚变 (ICF) 实验中的状态。</li>
</ul></li>
<li><strong>Fermi sea (费米海):</strong>
<ul>
<li>旁边的图示 (一个方框和坐标轴)
可能是在示意费米面在高温下变得模糊（即费米-狄拉克分布不再是 <span
class="math inline">\(T=0\)</span> 时的阶跃函数）。</li>
</ul></li>
</ul></li>
</ul>
<h3 id="칠-无轨道-dft-orbital-free-dft">칠 无轨道 DFT (Orbital-free
DFT)</h3>
<p>这部分回到了 <span class="math inline">\(T=0\)</span>
的情况（或稍作修改也可用于有限温度），但介绍了一种计算上更简化的 DFT
方法。</p>
<ul>
<li><strong><span class="math inline">\(E[\rho]\)</span> ?</strong>
<ul>
<li>提出一个问题：能量泛函 <span class="math inline">\(E[\rho]\)</span>
到底是什么样子的？</li>
</ul></li>
<li><strong><span class="math inline">\(\Delta\)</span> Orbital-free DFT
(OF-DFT):</strong>
<ul>
<li><strong>动机:</strong> Kohn-Sham DFT (标准的 DFT)
仍然需要求解一组单电子轨道，计算量随系统增大而急剧上升 (通常是 <span
class="math inline">\(N^3\)</span> 或更高)。OF-DFT
试图完全避免求解轨道，只依赖于密度 <span
class="math inline">\(\rho\)</span> 本身。</li>
<li><strong>Thomas-Fermi-Dirac approximation (托马斯-费米-狄拉克
近似):</strong>
<ul>
<li>这是 OF-DFT 中最早期和最简单的近似。</li>
<li><strong><span class="math inline">\(E_{TF}[\rho] = T[\rho] + \int
V_{ext}(\vec{r})\rho(\vec{r})d\vec{r} + \frac{e^2}{2} \iint
\frac{\rho(\vec{r})\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|}d\vec{r}d\vec{r}&#39;
+ E_{xc}[\rho]\)</span> ?</strong></li>
<li><strong>泛函的组成：</strong>
<ol type="1">
<li><strong><span class="math inline">\(T[\rho]\)</span>:</strong>
动能泛函。在 OF-DFT 中，最大的挑战就是找到 <span
class="math inline">\(T[\rho]\)</span>
的一个精确的、仅依赖于密度的表达式。Thomas-Fermi
理论给出了第一个近似（<span class="math inline">\(T_{TF}[\rho] \propto
\int \rho^{5/3} d\vec{r}\)</span>）。</li>
<li><strong><span class="math inline">\(\int
V_{ext}(\vec{r})\rho(\vec{r})d\vec{r}\)</span>:</strong> 外势能量
(与标准 DFT 相同)。</li>
<li><strong><span class="math inline">\(\frac{e^2}{2} \iint
...\)</span>:</strong> 电子-电子相互作用的<strong>哈特里 (Hartree)
能量</strong>，即经典的静电排斥能 (与标准 DFT 相同)。</li>
<li><strong><span class="math inline">\(E_{xc}[\rho]\)</span>
?:</strong> <strong>交换关联 (Exchange-Correlation) 能量</strong>。
<ul>
<li><em>白板上在这一项后面打了一个问号</em>，表示这部分也需要一个仅依赖于
<span class="math inline">\(\rho\)</span> 的近似泛函（例如 Local Density
Approximation, LDA，其中 <span class="math inline">\(E_x \propto \int
\rho^{4/3} d\vec{r}\)</span>）。</li>
<li>当 <span class="math inline">\(E_{xc}\)</span> 包含狄拉克 (Dirac)
的交换能近似时，就称为 Thomas-Fermi-Dirac (TFD) 理论。</li>
</ul></li>
</ol></li>
</ul></li>
</ul></li>
</ul>
<h3 id="总结-1">总结</h3>
<p>从 <span class="math inline">\(T=0\)</span> 的基态 DFT
出发，探讨了两个进阶主题： 1. <strong>Mermin DFT:</strong> 如何将 DFT
框架从 <span class="math inline">\(T=0\)</span> 的总能量 <span
class="math inline">\(E\)</span> 推广到 <span class="math inline">\(T
&gt; 0\)</span> 的自由能 <span
class="math inline">\(F\)</span>，这对于描述高温系统（如温稠密物质）至关重要。
2. <strong>Orbital-free DFT:</strong>
一种计算上可能更高效（但目前精度较低）的 DFT 方法，它试图避免使用
Kohn-Sham 轨道，而是直接构建总能量（特别是动能 <span
class="math inline">\(T[\rho]\)</span>）作为电子密度的显式泛函。</p>
<p><strong>Orbital-free DFT (OF-DFT)</strong> 是一种“理想中”的
DFT，而我们通常在实践中（例如在 Google 上搜索“DFT 计算”）谈论的几乎都是
<strong>Kohn-Sham DFT (KS-DFT)</strong>。</p>
<p>它们都基于相同的 Hohenberg-Kohn
定理，但实现这个定理的<strong>策略</strong>完全不同。</p>
<h3 id="kohn-sham-ks-dft实用的妥协方案">Kohn-Sham (KS)
DFT：实用的妥协方案</h3>
<p>KS-DFT 的天才之处在于它<strong>没有</strong>试图直接解决那个最棘手的
<strong><span
class="math inline">\(T[\rho]\)</span></strong>（动能泛函）。</p>
<p><strong>1. 核心思想：引入“虚拟系统”</strong></p>
<p>Kohn 和 Sham
提出：我们不去处理那个复杂、强相互作用的<strong>真实电子系统</strong>，而是构建一个<strong>虚拟的、无相互作用的电子系统</strong>。</p>
<p>这个虚拟系统被设计为<strong>恰好</strong>具有与真实系统<strong>完全相同的基态电子密度
<span class="math inline">\(\rho_0(\vec{r})\)</span></strong>。</p>
<p><strong>2. 为什么这样做有好处？</strong></p>
<ul>
<li>对于<strong>无相互作用</strong>的系统，我们<strong>精确地知道</strong>如何计算其动能！</li>
<li>动能 <span class="math inline">\(T_s\)</span> 就是所有虚拟粒子（称为
Kohn-Sham 轨道 <span
class="math inline">\(\phi_i\)</span>）动能的总和。</li>
<li>这个系统的波函数就是一个简单的斯莱特行列式（Slater
determinant），密度 <span class="math inline">\(\rho(\vec{r}) = \sum_i^N
|\phi_i(\vec{r})|^2\)</span>。</li>
</ul>
<p><strong>3. Kohn-Sham 能量泛函</strong></p>
<p>KS-DFT 将总能量 <span class="math inline">\(E[\rho]\)</span>
重新划分为以下几项：</p>
<p><span class="math display">\[E_{KS}[\rho] = T_s[\{\phi_i\}] + \int
V_{ext} \rho(\vec{r}) d\vec{r} + E_H[\rho] + E_{xc}[\rho]\]</span></p>
<ul>
<li><span class="math inline">\(T_s[\{\phi_i\}]\)</span>:
<strong>无相互作用动能</strong>。这是通过求解轨道 <span
class="math inline">\(\phi_i\)</span>
来<strong>精确计算</strong>的，而不是作为 <span
class="math inline">\(\rho\)</span> 的泛函来近似。</li>
<li><span class="math inline">\(\int V_{ext} \rho(\vec{r})
d\vec{r}\)</span>: 外势能 (与 OF-DFT 相同)。</li>
<li><span class="math inline">\(E_H[\rho]\)</span>: 电子-电子静电排斥能
(Hartree 能量，与 OF-DFT 相同)。</li>
<li><span class="math inline">\(E_{xc}[\rho]\)</span>:
<strong>交换关联能</strong>。</li>
</ul>
<p><strong>4. 在 <span class="math inline">\(E_{xc}\)</span>
中</strong></p>
<p>现在，未知被藏在了 <span class="math inline">\(E_{xc}[\rho]\)</span>
这一项里。它包含： 1. <strong>交换能</strong> (纯量子效应)。 2.
<strong>关联能</strong> (电子如何相互“躲避”)。 3.
<strong>动能差</strong>：真实系统的动能 <span
class="math inline">\(T\)</span> 和我们计算的无相互作用动能 <span
class="math inline">\(T_s\)</span> 之间的差值 <span
class="math inline">\((T - T_s)\)</span>。</p>
<p>KS-DFT 的<strong>巨大成功</strong>在于：<strong><span
class="math inline">\(E_{xc}[\rho]\)</span>
这一项（虽然仍然未知且必须近似）被证明比直接近似 <span
class="math inline">\(T[\rho]\)</span> 要容易得多！</strong></p>
<p>KS-DFT 的工作就是求解一组单电子方程（Kohn-Sham 方程），找到轨道 <span
class="math inline">\(\phi_i\)</span>，从而构建 <span
class="math inline">\(\rho\)</span>，并计算总能量。</p>
<h3 id="对比kohn-sham-dft-vs.-orbital-free-dft">对比：Kohn-Sham DFT
vs. Orbital-Free DFT</h3>
<p>这张表格总结了两者最关键的区别：</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">特征</th>
<th style="text-align: left;">🔵 Kohn-Sham DFT (KS-DFT)</th>
<th style="text-align: left;">🟠 Orbital-Free DFT (OF-DFT)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>基本变量</strong></td>
<td style="text-align: left;">Kohn-Sham <strong>轨道</strong> <span
class="math inline">\(\{\phi_i\}\)</span> (用来构建密度 <span
class="math inline">\(\rho\)</span>)</td>
<td style="text-align: left;"><strong>电子密度</strong> <span
class="math inline">\(\rho(\vec{r})\)</span></td>
</tr>
<tr>
<td style="text-align: left;"><strong>核心挑战</strong></td>
<td style="text-align: left;">近似<strong>交换关联泛函 <span
class="math inline">\(E_{xc}[\rho]\)</span></strong></td>
<td style="text-align: left;">近似<strong>动能泛函 <span
class="math inline">\(T[\rho]\)</span></strong> (以及 <span
class="math inline">\(E_{xc}[\rho]\)</span>)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>动能处理</strong></td>
<td style="text-align: left;"><strong>间接计算</strong>：通过求解轨道
<span class="math inline">\(\phi_i\)</span>
精确计算<strong>无相互作用动能 <span
class="math inline">\(T_s\)</span></strong>。</td>
<td style="text-align: left;"><strong>直接近似</strong>：必须找到一个
<span class="math inline">\(T[\rho]\)</span> 的表达式 (例如 <span
class="math inline">\(T \propto \int \rho^{5/3}\)</span>)。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>计算成本</strong></td>
<td
style="text-align: left;"><strong>高</strong>。求解轨道是计算瓶颈，计算量通常随系统大小
<span class="math inline">\(N\)</span> 呈 <span
class="math inline">\(N^3\)</span> 增长。</td>
<td style="text-align: left;"><strong>非常低</strong>。原则上可以 <span
class="math inline">\(N \log N\)</span> 或 <span
class="math inline">\(N\)</span> (线性标度)，因为它只处理 3D 变量 <span
class="math inline">\(\rho\)</span>。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>当前精度</strong></td>
<td
style="text-align: left;"><strong>高</strong>。是量子化学和材料科学的标准方法。</td>
<td style="text-align: left;"><strong>低</strong>。找到一个普适且精确的
<span class="math inline">\(T[\rho]\)</span>
泛函被证明<strong>极其困难</strong>。</td>
</tr>
</tbody>
</table>
<h3 id="总结-2">总结</h3>
<ul>
<li><strong>OF-DFT </strong>
一个完全依赖于密度的理论，计算速度极快，但苦于找不到准确的<strong>动能泛函</strong>
<span class="math inline">\(T[\rho]\)</span>。</li>
<li><strong>KS-DFT (标准方法)</strong>
是一个务实的“妥协”。它引入了<strong>轨道</strong>，用 <span
class="math inline">\(T_s\)</span>
这一项精确地处理了大部分动能，把更小的、更“好近似”的动能差 <span
class="math inline">\((T-T_s)\)</span> 连同交换关联能一起打包成 <span
class="math inline">\(E_{xc}[\rho]\)</span>。</li>
</ul>
<p>当今几乎所有的 DFT 软件（如 VASP, Gaussian, QE）都是基于 Kohn-Sham
方案的。</p>
<p>这推导的是<strong>自由电子气 (free electron gas)</strong>
的动能泛函，这个结果是<strong>无轨道 DFT (OF-DFT)</strong>
和<strong>局域密度近似 (LDA)</strong> 的理论基础。</p>
<p>这内容承接了上一张图的 <span
class="math inline">\(T[\rho]\)</span>（动能泛函）问题，展示了如何为最简单的系统——均匀的自由电子气——推导出
<span class="math inline">\(T\)</span> 和 <span
class="math inline">\(\rho\)</span> 的关系。</p>
<h3 id="详解">详解</h3>
<h4
id="左半部分自由电子气的基本设定">左半部分：自由电子气的基本设定</h4>
<ol type="1">
<li><strong>标题：free electron gas (自由电子气)</strong>
<ul>
<li>这是一个理想化模型，假设电子在无外势（或均匀正电荷背景）中自由运动。</li>
</ul></li>
<li><strong>薛定谔方程：</strong>
<ul>
<li><span class="math inline">\(-\frac{\hbar^2}{2m}\nabla^2\Psi =
E_k\Psi\)</span></li>
<li>这是自由粒子的定态薛定谔方程，其解为平面波。</li>
</ul></li>
<li><strong>波函数与能量：</strong>
<ul>
<li><span class="math inline">\(\Psi_k = \frac{1}{\sqrt{\Omega}}
e^{i\vec{k}\cdot\vec{r}}\)</span> （平面波解）</li>
<li><span class="math inline">\(\Omega = L^3\)</span> （系统体积）</li>
<li><span class="math inline">\(E_k = \frac{\hbar^2 k^2}{2m}\)</span>
（能量本征值，k 是波矢 <span
class="math inline">\(k=|\vec{k}|\)</span>）</li>
</ul></li>
<li><strong>PBC (周期性边界条件):</strong>
<ul>
<li><code>kx = 0, ±2π/L, ±4π/L, ...</code></li>
<li>这说明 <span class="math inline">\(\vec{k}\)</span>
矢量在“k空间”中不是连续的，而是形成一个晶格，每个点占据的体积是 <span
class="math inline">\((\frac{2\pi}{L})^3\)</span>。</li>
</ul></li>
<li><strong>3D 状态填充 (费米球):</strong>
<ul>
<li>图示为一个球体，半径为 <span
class="math inline">\(k_F\)</span>（费米波矢）。在 <span
class="math inline">\(T=0\text{K}\)</span> 时，所有 <span
class="math inline">\(k &lt; k_F\)</span> 的状态都被电子填满。</li>
<li><strong>计算电子总数 <span class="math inline">\(N^\sigma\)</span>
(单一自旋):</strong>
<ul>
<li><span class="math inline">\(N^\sigma =
\frac{\text{费米球体积}}{\text{单个 k 态体积}}\)</span></li>
<li><span class="math inline">\(N^\sigma = \frac{\frac{4}{3}\pi
(k_F^\sigma)^3}{(\frac{2\pi}{L})^3} = \frac{\frac{4}{3}\pi
(k_F^\sigma)^3}{(2\pi)^3/\Omega}\)</span></li>
</ul></li>
</ul></li>
</ol>
<h4 id="右半部分推导-trho-动能泛函">右半部分：推导 <span
class="math inline">\(T[\rho]\)</span> (动能泛函)</h4>
<ol type="1">
<li><strong>关联 <span class="math inline">\(k_F\)</span> 和密度 <span
class="math inline">\(\rho\)</span>:</strong>
<ul>
<li>从左侧公式整理可得：<span class="math inline">\(\rho^\sigma =
\frac{N^\sigma}{\Omega} = \frac{(k_F^\sigma)^3}{6\pi^2}\)</span>。</li>
<li>假设系统是自旋非极化的（<span class="math inline">\(\rho^\uparrow =
\rho^\downarrow\)</span>），总密度 <span class="math inline">\(\rho =
\rho^\uparrow + \rho^\downarrow = 2\rho^\sigma\)</span>，且 <span
class="math inline">\(k_F^\uparrow = k_F^\downarrow =
k_F\)</span>。</li>
<li>代入可得：<span class="math inline">\(\rho = 2 \cdot
\frac{k_F^3}{6\pi^2} = \frac{k_F^3}{3\pi^2}\)</span>。</li>
<li><strong><span class="math inline">\(\Rightarrow (k_F)^3 = 3\pi^2
\rho\)</span></strong> (白板上的关键关系)</li>
</ul></li>
<li><strong>费米能 (Fermi Energy):</strong>
<ul>
<li><code>HOMO/VBM → EF</code> (在连续能带中，最高占据能级就是费米能
<span class="math inline">\(E_F\)</span>)</li>
<li><span class="math inline">\(E_F = \frac{\hbar^2
k_F^2}{2m}\)</span></li>
</ul></li>
<li><strong>计算总动能 <span class="math inline">\(T\)</span>:</strong>
<ul>
<li>总动能 <span class="math inline">\(T\)</span>
是所有电子动能的总和。在 <span
class="math inline">\(T=0\text{K}\)</span>
时，等于对费米球内所有状态的能量 <span
class="math inline">\(E_k\)</span> 进行积分。</li>
<li><span class="math inline">\(T = T^\uparrow +
T^\downarrow\)</span></li>
<li>通过积分（白板上省略了积分步骤，直接用了熟知结论），可以得到<strong>平均动能</strong>
<span class="math inline">\(\langle E_{kin} \rangle = \frac{3}{5}
E_F\)</span>。</li>
<li>因此，总动能 <span class="math inline">\(T = N \cdot \langle E_{kin}
\rangle = N \cdot \frac{3}{5} E_F\)</span>。</li>
</ul></li>
<li><strong>T 作为 <span class="math inline">\(\rho\)</span> 的泛函
(最终推导):</strong>
<ul>
<li>这是最关键的一步：将 <span class="math inline">\(T = \frac{3}{5} N
E_F\)</span> 中的 <span class="math inline">\(N\)</span> 和 <span
class="math inline">\(E_F\)</span> 全部用密度 <span
class="math inline">\(\rho\)</span> 替换掉。</li>
<li><span class="math inline">\(N = \int \rho(\vec{r}) d\vec{r}\)</span>
(电子总数)</li>
<li><span class="math inline">\(E_F = \frac{\hbar^2 k_F^2}{2m} =
\frac{\hbar^2}{2m} (3\pi^2 \rho)^{2/3}\)</span> (将 <span
class="math inline">\(k_F\)</span> 用 <span
class="math inline">\(\rho\)</span> 替换)</li>
<li><strong>局域密度近似 (Local Density Approximation, LDA):</strong>
<ul>
<li>我们<strong>假设</strong>一个真实系统（密度<em>不</em>均匀，<span
class="math inline">\(\rho =
\rho(\vec{r})\)</span>）的总动能，可以通过在空间中每一点 <span
class="math inline">\((\vec{r})\)</span>
使用上述<strong>均匀电子气</strong>的结果，然后求和（积分）得到。</li>
<li><span class="math inline">\(T[\rho] = \int (\text{电子数密度}) \cdot
(\text{平均动能}) d\vec{r}\)</span></li>
<li><span class="math inline">\(T[\rho] = \int \rho(\vec{r}) \cdot
\frac{3}{5} E_F(\rho(\vec{r})) d\vec{r}\)</span></li>
<li><span class="math inline">\(T[\rho] = \int \rho(\vec{r}) \cdot
\frac{3}{5} \left[ \frac{\hbar^2}{2m} (3\pi^2 \rho(\vec{r}))^{2/3}
\right] d\vec{r}\)</span></li>
</ul></li>
<li><strong>整理后得到白板上的最终公式：</strong>
<ul>
<li><span class="math inline">\(T[\rho] = \frac{\hbar^2}{m} \frac{3}{10}
(3\pi^2)^{2/3} \int \rho^{5/3}(\vec{r}) d\vec{r}\)</span></li>
<li>这被称为<strong>托马斯-费米 (Thomas-Fermi) 动能泛函</strong>。</li>
</ul></li>
</ul></li>
</ol>
<h3 id="总结-3">总结</h3>
<p>这展示了 <span class="math inline">\(T[\rho] \propto \int \rho^{5/3}
d\vec{r}\)</span> 这个著名公式的来源。</p>
<ul>
<li>它为<strong>无轨道 DFT</strong>
提供了第一个（也是最简单的）<strong>动能泛函</strong> <span
class="math inline">\(T[\rho]\)</span> 近似。</li>
<li>它也是<strong>局域密度近似 (LDA)</strong> 的基础。在 Kohn-Sham DFT
中，虽然 <span class="math inline">\(T_s\)</span> (无相互作用动能)
是通过轨道精确计算的，但 <span class="math inline">\(E_{xc}\)</span>
(交换关联能) 里的交换能 <span class="math inline">\(E_x\)</span>
也是用完全相同的逻辑（自由电子气模型）推导出来的（<span
class="math inline">\(E_x[\rho] \propto \int \rho^{4/3}
d\vec{r}\)</span>）。</li>
</ul>
<p><strong>无轨道密度泛函理论 (Orbital-Free DFT)</strong>
的核心求解方程，它直接源自前面的推导。</p>
<p><strong>如何通过最小化能量泛函来找到基态密度 <span
class="math inline">\(\rho_0\)</span></strong>。</p>
<h3 id="详解-1">详解</h3>
<p><strong>1. 核心思想：约束下的最小化</strong></p>
<ul>
<li><strong><span class="math inline">\(\frac{\delta}{\delta\rho} \left(
E_{TF}[\rho] - \mu (\int \rho(\vec{r})d\vec{r} - N) \right) =
0\)</span></strong>
<ul>
<li>这是一个使用“拉格朗日乘子法”的<strong>泛函求导</strong>（或称变分）方程。</li>
<li><strong>目的：</strong> 寻找使总能量 <span
class="math inline">\(E_{TF}[\rho]\)</span>
达到<strong>最小值</strong>的那个密度 <span
class="math inline">\(\rho\)</span>。</li>
<li><strong>约束：</strong>
这个最小化必须满足一个条件，即电子密度在全空间积分必须等于总电子数 <span
class="math inline">\(N\)</span>（即 <span class="math inline">\(\int
\rho(\vec{r})d\vec{r} = N\)</span>）。</li>
<li><strong><span class="math inline">\(\mu\)</span> (mu):</strong>
就是为这个约束条件引入的“拉格朗日乘子”。</li>
</ul></li>
</ul>
<p><strong>2. 欧拉-拉格朗日方程 (Euler-Lagrange Equation)</strong></p>
<ul>
<li>下面那一大长串方程，就是执行上面那行泛函求导 <span
class="math inline">\(\frac{\delta}{\delta\rho}\)</span>
后得到的结果，其形式为 <span class="math inline">\(\frac{\delta
E_{TF}[\rho]}{\delta\rho} = \mu\)</span>。</li>
<li>让我们逐项分解 <span class="math inline">\(\frac{\delta
E_{TF}[\rho]}{\delta\rho}\)</span>：
<ul>
<li><strong><span class="math inline">\(E_{TF}[\rho] = T[\rho] +
E_{V_{ext}}[\rho] + E_H[\rho] + E_{xc}[\rho]\)</span></strong>
(这是上一张白板中定义的总能量)</li>
</ul></li>
<li><strong>方程的各项：</strong>
<ul>
<li><strong>第一项：<span class="math inline">\(\frac{\hbar^2}{m}
\frac{3}{10} (3\pi^2)^{2/3} \cdot \frac{5}{3}
\rho^{2/3}\)</span></strong>
<ul>
<li>这是对 <strong>动能泛函 <span
class="math inline">\(T[\rho]\)</span></strong> 求泛函导数的结果。</li>
<li><span class="math inline">\(T[\rho] = C_F \int \rho^{5/3}
d\vec{r}\)</span> (来自上一张白板)。</li>
<li><span class="math inline">\(\frac{\delta T[\rho]}{\delta\rho} = C_F
\cdot \frac{5}{3} \rho^{2/3}\)</span>。</li>
<li>这一项代表一种源自动能的“量子压力”。</li>
</ul></li>
<li><strong>第二项：<span
class="math inline">\(V_{ext}(\vec{r})\)</span></strong>
<ul>
<li>这是对 <strong>外势能</strong> <span
class="math inline">\(E_{V_{ext}}[\rho] = \int V_{ext}(\vec{r})
\rho(\vec{r}) d\vec{r}\)</span> 求导的结果。</li>
<li><span class="math inline">\(\frac{\delta
E_{V_{ext}}[\rho]}{\delta\rho} = V_{ext}(\vec{r})\)</span>。</li>
</ul></li>
<li><strong>第三项：<span class="math inline">\(+ e^2 \int d\vec{r}&#39;
\frac{\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|}\)</span></strong>
<ul>
<li>这是对 <strong>哈特里 (Hartree) 能量</strong> <span
class="math inline">\(E_H[\rho]\)</span>
（电子间经典静电排斥能）求导的结果。</li>
<li>这一项就是<strong>哈特里势 <span
class="math inline">\(V_H(\vec{r})\)</span></strong>，即 <span
class="math inline">\(\rho\)</span> 在 <span
class="math inline">\(\vec{r}\)</span>
处感受到的来自所有其他电子的静电势。</li>
<li><em>(注：白板上在 <span class="math inline">\(V_{ext}\)</span>
和这一项之间写的 <span class="math inline">\(V_{exc}[\rho]\)</span>
似乎是个笔误或简写，因为方程后面明确地分别写出了哈特里项和交换关联项。)</em></li>
</ul></li>
<li><strong>第四项：<span class="math inline">\(+ \frac{\delta
E_{xc}[\rho]}{\delta\rho}\)</span></strong>
<ul>
<li>这是对 <strong>交换关联 (Exchange-Correlation) 能量 <span
class="math inline">\(E_{xc}[\rho]\)</span></strong> 求导的结果。</li>
<li>这个导数本身被<strong>定义</strong>为<strong>交换关联势 <span
class="math inline">\(V_{xc}(\vec{r})\)</span></strong>。</li>
</ul></li>
</ul></li>
</ul>
<p><strong>3. 方程的物理意义</strong></p>
<ul>
<li><strong><span class="math inline">\(= \mu \text{ chemical
potential}\)</span></strong>
<ul>
<li>方程表明，在基态密度 <span class="math inline">\(\rho_0\)</span>
下，系统中所有“势”的总和在空间中处处等于一个常数 <span
class="math inline">\(\mu\)</span>。</li>
<li>这个常数 <span
class="math inline">\(\mu\)</span>（拉格朗日乘子）的物理意义是体系的<strong>化学势
(chemical potential)</strong>，即向系统中添加一个电子所需的能量。</li>
</ul></li>
</ul>
<p><strong>4. 总结 </strong></p>
<ul>
<li><strong><span class="math inline">\(\rho_0 \quad E_{TF}[\rho_0]
\quad \text{shell}\)</span></strong>
<ul>
<li><strong><span class="math inline">\(\rho_0\)</span>:</strong>
通过求解上面那个复杂的（非线性）积分-微分方程，我们就能得到<strong>基态密度
<span class="math inline">\(\rho_0\)</span></strong>。</li>
<li><strong><span
class="math inline">\(E_{TF}[\rho_0]\)</span>:</strong> 将 <span
class="math inline">\(\rho_0\)</span> 代回到 <span
class="math inline">\(E_{TF}[\rho]\)</span>
的原始表达式中，就能计算出体系的<strong>基态总能量</strong>。</li>
<li><strong><span class="math inline">\(\text{shell}\)</span>
(壳层):</strong> 这是一个非常重要的旁注。托马斯-费米 (Thomas-Fermi)
理论（即 OF-DFT
的最早版本）的一个著名<strong>缺陷</strong>是它无法预测原子中<strong>电子壳层结构</strong>（如
1s, 2s, 2p…）。它的密度 <span class="math inline">\(\rho\)</span>
曲线是平滑的，没有“Kohn-Sham
DFT”中轨道所产生的波峰和波谷。这个词很可能是在提醒这个理论的局限性。</li>
</ul></li>
</ul>
<h3 id="整个系列总结">整个系列总结</h3>
<p>这四部分构成了一个关于 DFT 基础： 1. <strong>1:</strong> 介绍了 DFT
的<strong>核心思想</strong> (Hohenberg-Kohn 定理)，即能量是密度的泛函
<span class="math inline">\(E[\rho]\)</span>。 2. <strong>2:</strong>
探讨了两种 DFT 的实现：一种用于<strong>有限温度</strong> (Mermin
DFT)，另一种是<strong>无轨道 DFT (OF-DFT)</strong>，并写出了 <span
class="math inline">\(E_{TF}[\rho]\)</span> 的一般形式。 3.
<strong>3:</strong> 详细<strong>推导</strong>了 OF-DFT
中最关键的<strong>动能泛函</strong> <span class="math inline">\(T[\rho]
\propto \int \rho^{5/3} d\vec{r}\)</span>，其基于自由电子气模型。 4.
<strong>4:</strong> 展示了如何<strong>使用</strong>这个 <span
class="math inline">\(E_{TF}[\rho]\)</span>
泛函，通过泛函求导（变分法）建立一个可解的方程（欧拉-拉格朗日方程），以求得基态密度
<span class="math inline">\(\rho_0\)</span> 和能量 <span
class="math inline">\(E_0\)</span>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/11/04/diffusion_model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/11/04/diffusion_model/" class="post-title-link" itemprop="url">Diffusion Model</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-11-04 11:00:00 / 修改时间：13:48:13" itemprop="dateCreated datePublished" datetime="2025-11-04T11:00:00+08:00">2025-11-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">扩散模型</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>第 1 部分：基础数学工具与物理背景</strong></p>
<p>最近在尝试理解扩散模型所依赖的核心数学概念：高斯分布的性质、朗之万动力学（SDE）及其离散化。</p>
<h3 id="核心数学工具高斯分布-gaussian-distribution">1.
核心数学工具：高斯分布 (Gaussian Distribution)</h3>
<p>扩散模型（尤其是 DDPM）的全部推导都建立在高斯分布的美妙性质之上。</p>
<h4 id="定义">1.1 定义</h4>
<p>一个一维高斯分布（正态分布）由均值 <span
class="math inline">\(\mu\)</span> 和方差 <span
class="math inline">\(\sigma^2\)</span> 定义，其概率密度函数 (PDF) 为：
<span class="math display">\[
\mathcal{N}(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
\]</span> 对于 <span class="math inline">\(D\)</span> 维向量 <span
class="math inline">\(\mathbf{x}\)</span>（例如视频中 <span
class="math inline">\(32 \times 32 = 1024\)</span>
维的图片向量），多维高斯分布由均值向量 <span
class="math inline">\(\boldsymbol{\mu}\)</span> 和协方差矩阵 <span
class="math inline">\(\boldsymbol{\Sigma}\)</span> 定义： <span
class="math display">\[
\mathcal{N}(\mathbf{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) =
\frac{1}{\sqrt{(2\pi)^D \det(\boldsymbol{\Sigma})}}
\exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^T
\boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})\right)
\]</span> &gt; <strong>关键简化：</strong> 在 DDPM
中，我们通常假设协方差矩阵是<strong>对角矩阵</strong> <span
class="math inline">\(\boldsymbol{\Sigma} = \sigma^2
\mathbf{I}\)</span>，其中 <span
class="math inline">\(\mathbf{I}\)</span>
是单位矩阵。这意味着向量的每个维度（每个像素）是独立同分布的（IID）。</p>
<h4 id="关键性质-1重参数化技巧-reparameterization-trick">1.2 关键性质
1：重参数化技巧 (Reparameterization Trick)</h4>
<p><strong>问题：</strong> 我们如何从 <span
class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span>
中采样？直接对这个分布采样是困难的，且在神经网络中无法传递梯度。</p>
<p><strong>技巧：</strong> 我们可以从一个<strong>标准高斯分布</strong>
<span class="math inline">\(\epsilon \sim \mathcal{N}(0, 1)\)</span>
中采样，然后通过线性变换得到 <span class="math inline">\(x\)</span>：
<span class="math display">\[
x = \mu + \sigma \cdot \epsilon
\]</span> <strong>推导：</strong> * <strong>1.：</strong> 设 <span
class="math inline">\(\epsilon \sim \mathcal{N}(0, 1)\)</span>，即 <span
class="math inline">\(E[\epsilon] = 0, \text{Var}(\epsilon) =
1\)</span>。 * <strong>2.：</strong> 我们构造 <span
class="math inline">\(x = \mu + \sigma \epsilon\)</span>。 * 计算 <span
class="math inline">\(x\)</span> 的均值：<span
class="math inline">\(E[x] = E[\mu + \sigma \epsilon] = E[\mu] + \sigma
E[\epsilon] = \mu + \sigma \cdot 0 = \mu\)</span>。 *
<strong>3.：</strong> 计算 <span class="math inline">\(x\)</span>
的方差：<span class="math inline">\(\text{Var}(x) = \text{Var}(\mu +
\sigma \epsilon) = \text{Var}(\sigma \epsilon) = \sigma^2
\text{Var}(\epsilon) = \sigma^2 \cdot 1 = \sigma^2\)</span>。 *
<strong>4.：</strong> 由于高斯分布的线性变换仍然是高斯分布，因此 <span
class="math inline">\(x \sim \mathcal{N}(\mu, \sigma^2)\)</span>。</p>
<p><strong>意义：</strong> 这使得采样过程可微。<span
class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span> 可以是神经网络的输出，<span
class="math inline">\(\epsilon\)</span>
作为外部噪声输入，梯度可以反向传播回 <span
class="math inline">\(\mu\)</span> 和 <span
class="math inline">\(\sigma\)</span>。</p>
<h4 id="关键性质-2两个独立高斯分布之和-sum-of-independent-gaussians">1.3
关键性质 2：两个独立高斯分布之和 (Sum of Independent Gaussians)</h4>
<p><strong>问题：</strong>
两个独立高斯分布相加会怎样？这是前向加噪过程（Forward
Process）的核心。</p>
<p><strong>性质：</strong> 如果 <span class="math inline">\(X_1 \sim
\mathcal{N}(\mu_1, \sigma_1^2)\)</span> 且 <span
class="math inline">\(X_2 \sim \mathcal{N}(\mu_2, \sigma_2^2)\)</span>
相互独立，那么它们的和 <span class="math inline">\(Y = X_1 +
X_2\)</span> 仍然是高斯分布： <span class="math display">\[
Y \sim \mathcal{N}(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
\]</span> <strong>推导：</strong> * <strong>均值：</strong> <span
class="math inline">\(E[Y] = E[X_1 + X_2] = E[X_1] + E[X_2] = \mu_1 +
\mu_2\)</span>。 * <strong>方差：</strong> 由于 <span
class="math inline">\(X_1\)</span> 和 <span
class="math inline">\(X_2\)</span> 相互独立，<span
class="math inline">\(\text{Cov}(X_1, X_2) = 0\)</span>。 <span
class="math inline">\(\text{Var}(Y) = \text{Var}(X_1 + X_2) =
\text{Var}(X_1) + \text{Var}(X_2) + 2 \text{Cov}(X_1, X_2) = \sigma_1^2
+ \sigma_2^2\)</span>。 *
（两个独立高斯变量之和仍为高斯分布，这可以通过矩生成函数或特征函数严格证明，这里我们接受这个结论。）</p>
<p><strong>意义：</strong> 这使得我们可以计算前向过程中任意 <span
class="math inline">\(t\)</span> 时刻 <span
class="math inline">\(x_t\)</span> 从 <span
class="math inline">\(x_0\)</span> 开始累积加噪的结果。</p>
<h3 id="物理与sde朗之万动力学-langevin-dynamics">2.
物理与SDE：朗之万动力学 (Langevin Dynamics)</h3>
<p>这是扩散模型的物理原型。</p>
<h4 id="随机微分方程-sde">2.1 随机微分方程 (SDE)</h4>
<p>朗之万动力学描述了一个粒子（在我们的例子中是图片向量 <span
class="math inline">\(\mathbf{x}\)</span>）在势场 <span
class="math inline">\(U(\mathbf{x})\)</span>
中运动，同时受到随机力（如布朗运动）的影响。其 SDE 形式 为： <span
class="math display">\[
d\mathbf{x}_t = \mathbf{f}(\mathbf{x}_t, t)dt + g(t)d\mathbf{w}_t
\]</span> * <span class="math inline">\(\mathbf{x}_t\)</span>：<span
class="math inline">\(t\)</span> 时刻的粒子位置（图片向量）。 * <span
class="math inline">\(\mathbf{f}(\mathbf{x}_t,
t)\)</span>：<strong>漂移项 (Drift)</strong>。代表确定性的力，如视频中
提到的 “吸引回原点的线性运动”（例如 <span
class="math inline">\(\mathbf{f}(\mathbf{x}, t) = -\beta
\mathbf{x}\)</span>，使分布趋向原点）。它对应能量函数的负梯度 <span
class="math inline">\(-\nabla U(\mathbf{x})\)</span>。 * <span
class="math inline">\(g(t)\)</span>：<strong>扩散项
(Diffusion)</strong>。控制随机噪声的强度。 * <span
class="math inline">\(d\mathbf{w}_t\)</span>：<strong>维纳过程 (Wiener
Process)</strong> 或布朗运动。它是一个随机项，其增量 <span
class="math inline">\(d\mathbf{w}_t\)</span> 在 <span
class="math inline">\(dt\)</span> 时间内服从高斯分布 <span
class="math inline">\(d\mathbf{w}_t \sim \mathcal{N}(0, \mathbf{I}
dt)\)</span>。</p>
<h4 id="sde-的离散化欧拉-丸山法-euler-maruyama">2.2 SDE
的离散化：欧拉-丸山法 (Euler-Maruyama)</h4>
<p><strong>问题：</strong> 计算机无法处理连续时间 <span
class="math inline">\(dt\)</span>。我们如何模拟这个 SDE？</p>
<p><strong>方法：</strong> 我们使用欧拉近似法（在 SDE 中称为
Euler-Maruyama）将其离散化为小的时间步 <span
class="math inline">\(\Delta t\)</span>。 <span class="math display">\[
\mathbf{x}_{t+\Delta t} - \mathbf{x}_t \approx \mathbf{f}(\mathbf{x}_t,
t)\Delta t + g(t) (\mathbf{w}_{t+\Delta t} - \mathbf{w}_t)
\]</span> 根据维纳过程的性质，在 <span class="math inline">\(\Delta
t\)</span> 时间内的增量 <span
class="math inline">\((\mathbf{w}_{t+\Delta t} - \mathbf{w}_t)\)</span>
服从 <span class="math inline">\(\mathcal{N}(0, \mathbf{I} \Delta
t)\)</span>。 根据<strong>性质 1.2（重参数化）</strong>，<span
class="math inline">\(\mathcal{N}(0, \mathbf{I} \Delta t)\)</span>
可以写成 <span class="math inline">\(\sqrt{\Delta t} \cdot
\mathbf{z}\)</span>，其中 <span class="math inline">\(\mathbf{z} \sim
\mathcal{N}(0, \mathbf{I})\)</span>。</p>
<p><strong>离散迭代公式：</strong> <span class="math display">\[
\mathbf{x}_{t+\Delta t} \approx \mathbf{x}_t + \mathbf{f}(\mathbf{x}_t,
t)\Delta t + g(t) \sqrt{\Delta t} \mathbf{z}_t
\]</span> 其中 <span class="math inline">\(\mathbf{z}_t \sim
\mathcal{N}(0, \mathbf{I})\)</span> 是在 <span
class="math inline">\(t\)</span> 时刻采样的标准高斯噪声。</p>
<p>这就是 DDPM <strong>前向加噪过程 (Forward Process)</strong>
的数学原型。</p>
<h3 id="核心数学工具贝叶斯公式-bayes-theorem">3.
核心数学工具：贝叶斯公式 (Bayes’ Theorem)</h3>
<p>贝叶斯公式是连接<strong>前向过程</strong>（加噪）和<strong>反向过程</strong>（去噪）的桥梁。</p>
<p>对于连续变量（概率密度函数）： <span class="math display">\[
p(x|y) = \frac{p(y|x) p(x)}{p(y)}
\]</span> 其中 <span class="math inline">\(p(y) = \int p(y|x) p(x)
dx\)</span>。</p>
<p><strong>在扩散模型中的应用：</strong> * <strong>1.：</strong>
我们定义了一个简单的前向加噪过程 <span class="math inline">\(q(x_t |
x_{t-1})\)</span>（易于计算）。 * <strong>2.：</strong>
我们想要的是反向去噪过程 <span class="math inline">\(p(x_{t-1} |
x_t)\)</span>（难以计算）。 * <strong>3.：</strong>
贝叶斯公式告诉我们：<span class="math inline">\(p(x_{t-1} | x_t) \propto
p(x_t | x_{t-1}) p(x_{t-1})\)</span>。 * <strong>4.：</strong> 在 DDPM
中，我们会看到一个更复杂的形式，它利用了 <span
class="math inline">\(x_0\)</span>： <span class="math display">\[
    q(x_{t-1} | x_t, x_0) = \frac{q(x_t | x_{t-1}, x_0) q(x_{t-1} |
x_0)}{q(x_t | x_0)}
    \]</span></p>
<p><strong>小结</strong></p>
<ol type="1">
<li><strong>高斯分布的性质</strong>（重参数化、加法），能精确计算加噪后的分布。</li>
<li><strong>朗之万动力学与欧拉近似</strong>，为 “逐步加噪”
提供了物理和数学模型。</li>
<li><strong>贝叶斯公式</strong>，指明了如何从 “加噪” 倒推出
“去噪”。</li>
</ol>
<h3 id="ddpm-前向过程从图像到噪声-the-forward-process">2. DDPM
前向过程：从图像到噪声 (The Forward Process)</h3>
<p>前向过程的目标是模拟大纲
中描述的“图片在向量空间中逐步噪声化的轨迹”。我们定义一个<strong>马尔可夫过程</strong>，在该过程中，我们从原始数据
<span class="math inline">\(\mathbf{x}_0 \sim
q(\mathbf{x}_0)\)</span>（即真实图片分布）开始，在 <span
class="math inline">\(T\)</span>
个离散的时间步中逐步向其添加高斯噪声。</p>
<h4 id="单步加噪过程-qmathbfx_t-mathbfx_t-1">2.1 单步加噪过程 <span
class="math inline">\(q(\mathbf{x}_t | \mathbf{x}_{t-1})\)</span></h4>
<p>在每个 <span class="math inline">\(t\)</span> 步，我们向 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span> 添加少量噪声以生成 <span
class="math inline">\(\mathbf{x}_t\)</span>。这个过程被定义为一个高斯转变（这源于我们第
1 部分中对朗之万动力学的离散化）：</p>
<p><span class="math display">\[
q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 -
\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I})
\]</span></p>
<ul>
<li><span class="math inline">\(\{\beta_t\}_{t=1}^T\)</span>
是一个预先设定的<strong>方差表 (variance schedule)</strong>。它们是
<span class="math inline">\(T\)</span> 个很小的正常数（例如，<span
class="math inline">\(\beta_1 = 10^{-4}, \beta_T =
0.02\)</span>）。</li>
<li><span class="math inline">\(\sqrt{1 - \beta_t}
\mathbf{x}_{t-1}\)</span>：这是<strong>缩放项</strong>（大纲）。我们在添加噪声之前先将前一步的图片向量“缩小”一点。</li>
<li><span class="math inline">\(\beta_t
\mathbf{I}\)</span>：这是<strong>噪声项</strong>。<span
class="math inline">\(\beta_t\)</span> 是添加的噪声的方差，<span
class="math inline">\(\mathbf{I}\)</span>
是单位矩阵，表示噪声在所有维度（像素）上是独立同分布的。</li>
</ul>
<p><strong>重参数化技巧的应用：</strong> 我们可以使用第 1
部分中的<strong>重参数化技巧</strong>来显式地写出这个采样过程： <span
class="math display">\[
\mathbf{x}_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1} + \sqrt{\beta_t}
\boldsymbol{\epsilon}_{t-1}
\]</span> 其中 <span class="math inline">\(\boldsymbol{\epsilon}_{t-1}
\sim \mathcal{N}(0, \mathbf{I})\)</span> 是在 <span
class="math inline">\(t-1\)</span> 时刻采样的一个标准高斯噪声。</p>
<h4 id="累积加噪过程-qmathbfx_t-mathbfx_0-核心推导">2.2 累积加噪过程
<span class="math inline">\(q(\mathbf{x}_t | \mathbf{x}_0)\)</span>
(核心推导)</h4>
<p><strong>问题：</strong> 在训练期间（如大纲
所述），我们希望<strong>随机跳到</strong>任意 <span
class="math inline">\(t\)</span> 步并生成 <span
class="math inline">\(\mathbf{x}_t\)</span>。如果我们必须从 <span
class="math inline">\(\mathbf{x}_0\)</span> 迭代 <span
class="math inline">\(t\)</span> 次，这将非常缓慢。</p>
<p><strong>目标：</strong> 我们需要一个公式，能让我们从 <span
class="math inline">\(\mathbf{x}_0\)</span> <strong>一次性</strong>得到
<span class="math inline">\(\mathbf{x}_t\)</span> 的分布 <span
class="math inline">\(q(\mathbf{x}_t |
\mathbf{x}_0)\)</span>。这就是大纲
中提到的“构造出高斯分布的累计变换”。</p>
<p><strong>推导：</strong> 1. <strong>定义新变量：</strong>
为了简化推导，我们定义 <span class="math inline">\(\alpha_t = 1 -
\beta_t\)</span> 和 <span class="math inline">\(\bar{\alpha}_t =
\prod_{i=1}^t \alpha_i\)</span>。 * <span
class="math inline">\(\alpha_t\)</span> 是每一步的缩放因子。 * <span
class="math inline">\(\bar{\alpha}_t\)</span> 是从第 1 步到第 <span
class="math inline">\(t\)</span> 步的<strong>累积缩放因子</strong>。</p>
<ol start="2" type="1">
<li><p><strong>展开迭代 (Step-by-step Expansion)：</strong> 让我们从
<span class="math inline">\(\mathbf{x}_t\)</span> 开始，逐步代入 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span>： <span
class="math display">\[
\mathbf{x}_t = \sqrt{\alpha_t} \mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}
\boldsymbol{\epsilon}_{t-1}
\]</span> 现在，我们代入 <span class="math inline">\(\mathbf{x}_{t-1} =
\sqrt{\alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_{t-1}}
\boldsymbol{\epsilon}_{t-2}\)</span>： <span class="math display">\[
\begin{aligned}
\mathbf{x}_t &amp;= \sqrt{\alpha_t} (\sqrt{\alpha_{t-1}}
\mathbf{x}_{t-2} + \sqrt{1 - \alpha_{t-1}} \boldsymbol{\epsilon}_{t-2})
+ \sqrt{1 - \alpha_t} \boldsymbol{\epsilon}_{t-1} \\
&amp;= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{\alpha_t(1
- \alpha_{t-1})} \boldsymbol{\epsilon}_{t-2} + \sqrt{1 - \alpha_t}
\boldsymbol{\epsilon}_{t-1}
\end{aligned}
\]</span></p></li>
<li><p><strong>合并高斯噪声 (Merging Gaussians)：</strong>
注意上式的后两项：<span class="math inline">\(\sqrt{\alpha_t(1 -
\alpha_{t-1})} \boldsymbol{\epsilon}_{t-2}\)</span> 和 <span
class="math inline">\(\sqrt{1 - \alpha_t}
\boldsymbol{\epsilon}_{t-1}\)</span>。</p>
<ul>
<li><span class="math inline">\(\boldsymbol{\epsilon}_{t-2}\)</span> 和
<span class="math inline">\(\boldsymbol{\epsilon}_{t-1}\)</span>
是两个<strong>独立</strong>的标准高斯分布。</li>
<li>我们正在对两个独立的、均值为 0 的高斯分布进行线性组合。</li>
<li>根据第 1 部分的<strong>性质 1.3</strong>
(两个独立高斯分布之和)，它们的和仍然是一个均值为 0 的高斯分布。</li>
<li>这个新的高斯分布的<strong>方差</strong>是多少？ <span
class="math display">\[
  \begin{aligned}
  \text{Var}(\text{new\_noise}) &amp;= \text{Var}(\sqrt{\alpha_t(1 -
\alpha_{t-1})} \boldsymbol{\epsilon}_{t-2}) + \text{Var}(\sqrt{1 -
\alpha_t} \boldsymbol{\epsilon}_{t-1}) \\
  &amp;= (\alpha_t(1 - \alpha_{t-1})) \mathbf{I} + (1 - \alpha_t)
\mathbf{I} \\
  &amp;= (\alpha_t - \alpha_t\alpha_{t-1} + 1 - \alpha_t) \mathbf{I} \\
  &amp;= (1 - \alpha_t\alpha_{t-1}) \mathbf{I}
  \end{aligned}
  \]</span></li>
<li>根据重参数化技巧，一个方差为 <span class="math inline">\((1 -
\alpha_t\alpha_{t-1}) \mathbf{I}\)</span> 的高斯分布，可以写成 <span
class="math inline">\(\sqrt{1 - \alpha_t\alpha_{t-1}} \cdot
\bar{\boldsymbol{\epsilon}}_{t-2}\)</span>，其中 <span
class="math inline">\(\bar{\boldsymbol{\epsilon}}_{t-2} \sim
\mathcal{N}(0, \mathbf{I})\)</span> 是一个新的标准高斯噪声。</li>
</ul></li>
<li><p><strong>递归与通项公式：</strong> 我们将合并后的噪声代入第 2
步的展开式： <span class="math display">\[
\mathbf{x}_t = \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 -
\alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2}
\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{x}_t\)</span> 和 <span
class="math inline">\(\mathbf{x}_{t-2}\)</span> 的关系，与 <span
class="math inline">\(\mathbf{x}_t\)</span> 和 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span> 的关系（<span
class="math inline">\(\mathbf{x}_t = \sqrt{\alpha_t} \mathbf{x}_{t-1} +
\sqrt{1 - \alpha_t}
\boldsymbol{\epsilon}_{t-1}\)</span>）在形式上是完全一致的！只是 <span
class="math inline">\(\alpha_t\)</span> 变成了 <span
class="math inline">\(\alpha_t \alpha_{t-1}\)</span>。</li>
<li>我们可以将这个模式递归地应用 <span class="math inline">\(t\)</span>
次： <span class="math display">\[
  \begin{aligned}
  \mathbf{x}_t &amp;= \sqrt{(\alpha_t \alpha_{t-1} \cdots \alpha_1)}
\mathbf{x}_0 + \sqrt{1 - (\alpha_t \alpha_{t-1} \cdots \alpha_1)}
\boldsymbol{\epsilon} \\
  &amp;= \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}
\boldsymbol{\epsilon}
  \end{aligned}
  \]</span></li>
<li>其中 <span class="math inline">\(\boldsymbol{\epsilon} \sim
\mathcal{N}(0, \mathbf{I})\)</span> 是一个（合并了 <span
class="math inline">\(t\)</span> 次的）标准高斯噪声。</li>
</ul></li>
</ol>
<h4 id="前向过程的最终公式">2.3 前向过程的最终公式</h4>
<p>我们得到了前向过程中最关键的<strong>累积加噪公式</strong>： <span
class="math display">\[
q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t;
\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t) \mathbf{I})
\]</span> 这个公式的重参数化形式为： <span class="math display">\[
\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 -
\bar{\alpha}_t} \boldsymbol{\epsilon}
\]</span> <strong>意义：</strong> * <strong>训练效率：</strong>
这个公式是 DDPM 训练效率的<strong>关键</strong>。我们不需要迭代 <span
class="math inline">\(t\)</span> 次来生成 <span
class="math inline">\(\mathbf{x}_t\)</span>。 *
<strong>随机训练：</strong> 在训练神经网络时，我们可以： 1.
从数据集中拿一张清晰图片 <span
class="math inline">\(\mathbf{x}_0\)</span>。 2.
<strong>随机</strong>选择一个时间步 <span
class="math inline">\(t\)</span>（例如 <span
class="math inline">\(t=150\)</span>）。 3. 从 <span
class="math inline">\(\mathcal{N}(0, \mathbf{I})\)</span> 中采样一个噪声
<span class="math inline">\(\boldsymbol{\epsilon}\)</span>。 4.
使用上述公式<strong>一步</strong>计算出 <span
class="math inline">\(\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0
+ \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}\)</span>。 5. 将 <span
class="math inline">\((\mathbf{x}_t, t, \boldsymbol{\epsilon})\)</span>
喂给神经网络进行训练。</p>
<p>当 <span class="math inline">\(t \to T\)</span> 时（例如 <span
class="math inline">\(T=1000\)</span>），<span
class="math inline">\(\bar{\alpha}_T = \prod_{i=1}^T (1 -
\beta_i)\)</span>。由于所有的 <span class="math inline">\(\beta_i &gt;
0\)</span>，<span class="math inline">\(\bar{\alpha}_T\)</span>
会非常接近 0。 此时： <span class="math display">\[
\mathbf{x}_T \approx \sqrt{0} \mathbf{x}_0 + \sqrt{1 - 0}
\boldsymbol{\epsilon} = \boldsymbol{\epsilon}
\]</span> 这意味着，在 <span class="math inline">\(T\)</span>
步之后，<span class="math inline">\(\mathbf{x}_T\)</span> 的分布 <span
class="math inline">\(q(\mathbf{x}_T | \mathbf{x}_0) \approx
\mathcal{N}(0,
\mathbf{I})\)</span>，它几乎完全变成了<strong>标准高斯噪声</strong>，并且与
<span class="math inline">\(\mathbf{x}_0\)</span> 无关。</p>
<p>成功地将复杂的图片分布 <span
class="math inline">\(q(\mathbf{x}_0)\)</span>
转化为了简单的标准高斯分布 <span
class="math inline">\(q(\mathbf{x}_T)\)</span>。</p>
<p><strong>小结</strong></p>
<p>核心问题：如何<strong>逆转</strong>这个过程？如何从一张纯噪声图片
<span class="math inline">\(\mathbf{x}_T \sim \mathcal{N}(0,
\mathbf{I})\)</span> 出发，一步步去噪，最终得到一张清晰的图片 <span
class="math inline">\(\mathbf{x}_0\)</span>？</p>
<p>这需要推导<strong>反向去噪过程 (Reverse Process)</strong> <span
class="math inline">\(p_\theta(\mathbf{x}_{t-1} |
\mathbf{x}_t)\)</span>。</p>
<h3 id="反向过程从噪声到图像-the-reverse-process">3.
反向过程：从噪声到图像 (The Reverse Process)</h3>
<p>我们的目标是学习反向的马尔可夫链，即 <span
class="math inline">\(p_\theta(\mathbf{x}_{t-1} |
\mathbf{x}_t)\)</span>。</p>
<h4 id="棘手的目标-pmathbfx_t-1-mathbfx_t">3.1 棘手的目标 <span
class="math inline">\(p(\mathbf{x}_{t-1} | \mathbf{x}_t)\)</span></h4>
<p>我们想从 <span class="math inline">\(\mathbf{x}_t\)</span> 推导出
<span class="math inline">\(\mathbf{x}_{t-1}\)</span>。根据贝叶斯公式：
<span class="math display">\[
p(\mathbf{x}_{t-1} | \mathbf{x}_t) = \frac{p(\mathbf{x}_t |
\mathbf{x}_{t-1}) p(\mathbf{x}_{t-1})}{p(\mathbf{x}_t)}
\]</span> * <span class="math inline">\(p(\mathbf{x}_t |
\mathbf{x}_{t-1})\)</span> 就是前向过程 <span
class="math inline">\(q(\mathbf{x}_t |
\mathbf{x}_{t-1})\)</span>，我们已知。 * <span
class="math inline">\(p(\mathbf{x}_{t-1})\)</span> 是 <span
class="math inline">\(t-1\)</span> 时刻的边缘分布，需要对所有 <span
class="math inline">\(\mathbf{x}_0\)</span> 积分 <span
class="math inline">\(p(\mathbf{x}_{t-1}) = \int q(\mathbf{x}_{t-1} |
\mathbf{x}_0) q(\mathbf{x}_0) d\mathbf{x}_0\)</span>，这依赖于 <span
class="math inline">\(q(\mathbf{x}_0)\)</span>（真实数据分布），<strong>极其困难
(intractable)</strong>。 * <span
class="math inline">\(p(\mathbf{x}_t)\)</span> 同样难以计算。</p>
<h4 id="ddpm-的核心创见利用-mathbfx_0-对应">3.2 DDPM 的核心创见：利用
<span class="math inline">\(\mathbf{x}_0\)</span> (对应)</h4>
<p><strong>关键洞察：</strong> 虽然 <span
class="math inline">\(p(\mathbf{x}_{t-1} | \mathbf{x}_t)\)</span>
难以计算，但<strong>如果我们额外知道 <span
class="math inline">\(\mathbf{x}_0\)</span></strong>，这个后验分布 <span
class="math inline">\(q(\mathbf{x}_{t-1} | \mathbf{x}_t,
\mathbf{x}_0)\)</span> <strong>是可计算的</strong>！</p>
<p>为什么？因为我们定义了所有 <span class="math inline">\(q\)</span>
的前向步骤。我们再次使用贝叶斯公式 (对应)： <span
class="math display">\[
q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) = \frac{q(\mathbf{x}_t
| \mathbf{x}_{t-1}, \mathbf{x}_0) \cdot q(\mathbf{x}_{t-1} |
\mathbf{x}_0)}{q(\mathbf{x}_t | \mathbf{x}_0)}
\]</span> 利用马尔可夫性质 <span class="math inline">\(q(\mathbf{x}_t |
\mathbf{x}_{t-1}, \mathbf{x}_0) = q(\mathbf{x}_t |
\mathbf{x}_{t-1})\)</span>，我们得到： <span class="math display">\[
q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) \propto q(\mathbf{x}_t
| \mathbf{x}_{t-1}) \cdot q(\mathbf{x}_{t-1} | \mathbf{x}_0)
\]</span> 我们已知这三个分布都是高斯分布（在第 2 部分已推导）： 1. <span
class="math inline">\(q(\mathbf{x}_t | \mathbf{x}_{t-1}) =
\mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t} \mathbf{x}_{t-1}, \beta_t
\mathbf{I})\)</span> 2. <span class="math inline">\(q(\mathbf{x}_{t-1} |
\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \sqrt{\bar{\alpha}_{t-1}}
\mathbf{x}_0, (1 - \bar{\alpha}_{t-1}) \mathbf{I})\)</span> 3. <span
class="math inline">\(q(\mathbf{x}_t | \mathbf{x}_0) =
\mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 -
\bar{\alpha}_t) \mathbf{I})\)</span></p>
<p>我们正在用 <span class="math inline">\(\mathbf{x}_{t-1}\)</span>
作为变量，乘以两个高斯分布的概率密度函数 (PDF)。高斯 PDF 的形式是 <span
class="math inline">\(C \cdot \exp(-\frac{(x -
\mu)^2}{2\sigma^2})\)</span>。两个高斯 PDF
相乘的结果<strong>仍然是一个高斯分布</strong>。</p>
<p>通过匹配 <span class="math inline">\(\mathbf{x}_{t-1}\)</span>
的一次项和二次项系数（一个繁琐但直接的代数过程），我们可以解出这个新高斯分布的均值
<span class="math inline">\(\tilde{\boldsymbol{\mu}}_t\)</span> 和方差
<span class="math inline">\(\tilde{\beta}_t\)</span>：</p>
<p><span class="math display">\[
q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) =
\mathcal{N}(\mathbf{x}_{t-1}; \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t,
\mathbf{x}_0), \tilde{\beta}_t \mathbf{I})
\]</span> 其中： * <strong>方差 <span
class="math inline">\(\tilde{\beta}_t\)</span></strong>：不依赖于 <span
class="math inline">\(\mathbf{x}_t\)</span> 或 <span
class="math inline">\(\mathbf{x}_0\)</span>，它是一个固定的超参数。
<span class="math display">\[
    \tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t}
\cdot \beta_t
    \]</span> * <strong>均值 <span
class="math inline">\(\tilde{\boldsymbol{\mu}}_t\)</span></strong>：依赖于
<span class="math inline">\(\mathbf{x}_t\)</span> 和 <span
class="math inline">\(\mathbf{x}_0\)</span>。 <span
class="math display">\[
    \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) =
\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0
+ \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}
\mathbf{x}_t
    \]</span></p>
<h3 id="训练学习反向过程-training">4. 训练：学习反向过程 (Training)</h3>
<h4 id="神经网络的目标">4.1 神经网络的目标</h4>
<p>我们有了一个完美的目标分布 <span
class="math inline">\(q(\mathbf{x}_{t-1} | \mathbf{x}_t,
\mathbf{x}_0)\)</span>。但它有个问题：在<strong>推理
(Inference)</strong> 时，我们从 <span
class="math inline">\(\mathbf{x}_T\)</span> 开始，并不知道 <span
class="math inline">\(\mathbf{x}_0\)</span>。</p>
<p>因此，我们训练一个神经网络 <span
class="math inline">\(p_\theta\)</span>
来<strong>近似</strong>这个分布： <span class="math display">\[
p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) =
\mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t),
\boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))
\]</span> 我们的目标是让 <span class="math inline">\(p_\theta\)</span>
尽可能接近 <span class="math inline">\(q\)</span>。 * <strong>简化1
(固定方差)</strong>：DDPM 论文发现，将神经网络的方差 <span
class="math inline">\(\boldsymbol{\Sigma}_\theta\)</span> 固定为 <span
class="math inline">\(\tilde{\beta}_t \mathbf{I}\)</span> 或 <span
class="math inline">\(\beta_t \mathbf{I}\)</span>
效果最好。这极大地简化了问题：<strong>神经网络只需要学习均值 <span
class="math inline">\(\boldsymbol{\mu}_\theta\)</span></strong>。 *
<strong>简化2 (学习目标)</strong>：我们训练 <span
class="math inline">\(\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)\)</span>
来预测真实均值 <span
class="math inline">\(\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t,
\mathbf{x}_0)\)</span>。</p>
<h4 id="ddpm-的关键改进预测噪声-对应">4.2 DDPM 的关键改进：预测噪声
(对应)</h4>
<p><span class="math inline">\(\tilde{\boldsymbol{\mu}}_t\)</span>
的公式 <span
class="math inline">\(\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 -
\bar{\alpha}_t} \mathbf{x}_0 + \frac{\sqrt{\alpha_t}(1 -
\bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t\)</span>
仍然很复杂。</p>
<p><strong>DDPM 论文提出了一个重要的的重参数化：</strong> 我们回顾第 2
部分的前向公式：<span class="math inline">\(\mathbf{x}_t =
\sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}
\boldsymbol{\epsilon}\)</span> 我们可以用它来反解 <span
class="math inline">\(\mathbf{x}_0\)</span>（在 <span
class="math inline">\(\mathbf{x}_t\)</span> 和 <span
class="math inline">\(\boldsymbol{\epsilon}\)</span> 已知的情况下）：
<span class="math display">\[
\mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} (\mathbf{x}_t - \sqrt{1 -
\bar{\alpha}_t} \boldsymbol{\epsilon})
\]</span> 现在，我们将这个 <span
class="math inline">\(\mathbf{x}_0\)</span> 的表达式代入上面 <span
class="math inline">\(\tilde{\boldsymbol{\mu}}_t\)</span> 的复杂公式中：
<span class="math display">\[
\begin{aligned}
\tilde{\boldsymbol{\mu}}_t &amp;=
\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \left(
\frac{1}{\sqrt{\bar{\alpha}_t}} (\mathbf{x}_t - \sqrt{1 -
\bar{\alpha}_t} \boldsymbol{\epsilon}) \right) + \frac{\sqrt{\alpha_t}(1
- \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t \\
&amp;= \left( \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{(1 -
\bar{\alpha}_t)\sqrt{\bar{\alpha}_t}} + \frac{\sqrt{\alpha_t}(1 -
\bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \right) \mathbf{x}_t - \left(
\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t \sqrt{1 - \bar{\alpha}_t}}{(1 -
\bar{\alpha}_t)\sqrt{\bar{\alpha}_t}} \right) \boldsymbol{\epsilon}
\end{aligned}
\]</span> (经过一系列基于 <span class="math inline">\(\bar{\alpha}_t =
\alpha_t \bar{\alpha}_{t-1}\)</span> 和 <span
class="math inline">\(\beta_t = 1 - \alpha_t\)</span> 的代数化简) <span
class="math display">\[
\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}} \left(
\mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}}
\boldsymbol{\epsilon} \right)
\]</span> <strong>分析这个优美的公式：</strong> * <span
class="math inline">\(\alpha_t\)</span>, <span
class="math inline">\(\beta_t\)</span>, <span
class="math inline">\(\bar{\alpha}_t\)</span> 都是预先设定的超参数。 *
<span class="math inline">\(\mathbf{x}_t\)</span> 是神经网络的输入。 *
<strong>唯一未知的就是 <span
class="math inline">\(\boldsymbol{\epsilon}\)</span></strong> ——
那个在第 2 部分用于从 <span class="math inline">\(\mathbf{x}_0\)</span>
生成 <span class="math inline">\(\mathbf{x}_t\)</span>
的<strong>原始噪声</strong>。</p>
<p><strong>结论（DDPM 核心思想）：</strong> (对应) 与其让神经网络 <span
class="math inline">\(\boldsymbol{\mu}_\theta\)</span>
预测那个复杂的均值 <span
class="math inline">\(\tilde{\boldsymbol{\mu}}_t\)</span>，我们可以让它转而去预测这个<strong>噪声
<span class="math inline">\(\boldsymbol{\epsilon}\)</span></strong>。
我们定义一个神经网络（通常是 U-Net 结构）<span
class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,
t)\)</span>，它的目标就是预测 <span
class="math inline">\(\boldsymbol{\epsilon}\)</span>。</p>
<h4 id="训练流程与损失函数-对应-012440">4.3 训练流程与损失函数
(对应-[01:24:40])</h4>
<ol type="1">
<li>从数据集中随机抽取一张清晰图像 <span
class="math inline">\(\mathbf{x}_0\)</span>。</li>
<li><strong>随机</strong>选择一个时间步 <span
class="math inline">\(t\)</span>（从 1 到 <span
class="math inline">\(T\)</span>）。(对应 随机训练)</li>
<li><strong>随机</strong>采样一个标准高斯噪声 <span
class="math inline">\(\boldsymbol{\epsilon} \sim \mathcal{N}(0,
\mathbf{I})\)</span>。(对应)</li>
<li>使用前向公式<strong>一步</strong>生成加噪图像：<span
class="math inline">\(\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0
+ \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}\)</span>。</li>
<li>将 <span class="math inline">\((\mathbf{x}_t, t)\)</span>
作为输入，喂给神经网络 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,
t)\)</span>，得到预测噪声 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span>。</li>
<li>计算损失函数（均方误差 MSE）：(对应) <span class="math display">\[
L(\theta) = E_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ ||
\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)
||^2 \right]
\]</span></li>
<li>使用梯度下降更新网络参数 <span
class="math inline">\(\theta\)</span>。</li>
</ol>
<h3 id="推理逐步去噪生成图像-inference">5. 推理：逐步去噪生成图像
(Inference)</h3>
<p>当训练好 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span>
后，就可以从纯噪声生成图像了：</p>
<ol type="1">
<li><strong>起始：</strong> 从标准高斯分布中采样一张纯噪声图像 <span
class="math inline">\(\mathbf{x}_T \sim \mathcal{N}(0,
\mathbf{I})\)</span>。</li>
<li><strong>迭代：</strong> <strong>从 <span class="math inline">\(t =
T\)</span> 循环到 <span class="math inline">\(t = 1\)</span></strong>：
<ol type="a">
<li>将当前的 <span class="math inline">\(\mathbf{x}_t\)</span> 和时间步
<span class="math inline">\(t\)</span> 输入网络，得到噪声预测：<span
class="math inline">\(\boldsymbol{\epsilon}_\theta =
\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\)</span>。(对应)</li>
<li>使用 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span> 作为我们对
<span class="math inline">\(\boldsymbol{\epsilon}\)</span>
的最佳估计，代入 <strong>4.2</strong> 中的均值公式，计算 <span
class="math inline">\(t-1\)</span> 步的<strong>均值</strong> <span
class="math inline">\(\boldsymbol{\mu}_\theta(\mathbf{x}_t,
t)\)</span>：(对应) <span class="math display">\[
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}}
\left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}}
\boldsymbol{\epsilon}_\theta \right)
\]</span></li>
<li>计算 <span class="math inline">\(t-1\)</span>
步的<strong>方差</strong>。我们使用固定的方差 <span
class="math inline">\(\sigma_t^2 \mathbf{I} = \tilde{\beta}_t \mathbf{I}
= \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t
\mathbf{I}\)</span>。</li>
<li><strong>采样 (Sampling)</strong> (对应)：从这个高斯分布中采样 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span>： <span
class="math display">\[
\mathbf{x}_{t-1} = \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) + \sigma_t
\mathbf{z}
\]</span> 其中 <span class="math inline">\(\mathbf{z} \sim
\mathcal{N}(0, \mathbf{I})\)</span> 是一个新采样的随机噪声。
（<em>注意：当 <span class="math inline">\(t=1\)</span> 时，<span
class="math inline">\(\mathbf{z}\)</span> 设为 0，因为 <span
class="math inline">\(\mathbf{x}_0\)</span>
应该是一个确定性的输出，不再添加噪声</em>）。</li>
</ol></li>
<li><strong>结束：</strong> 当循环结束时，<span
class="math inline">\(\mathbf{x}_0\)</span>
就是生成的清晰图像。(对应)</li>
</ol>
<p><strong>小结</strong></p>
<p>完整地推导了 DDPM 的核心数学原理： 1. <strong>前向过程 <span
class="math inline">\(q\)</span></strong>：使用 <span
class="math inline">\(q(\mathbf{x}_t | \mathbf{x}_0)\)</span> 高效加噪。
2. <strong>反向过程 <span
class="math inline">\(p_\theta\)</span></strong>：通过贝叶斯公式推导出
<span class="math inline">\(q(\mathbf{x}_{t-1} | \mathbf{x}_t,
\mathbf{x}_0)\)</span> 作为理想目标。 3. <strong>训练 <span
class="math inline">\(p_\theta\)</span></strong>：通过让 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,
t)\)</span> 预测真实噪声 <span
class="math inline">\(\boldsymbol{\epsilon}\)</span> 来简化训练目标 (MSE
Loss)。 4. <strong>推理 <span
class="math inline">\(p_\theta\)</span></strong>：从 <span
class="math inline">\(\mathbf{x}_T\)</span> 开始，利用 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span>
预测的均值，逐步采样 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span>。</p>
<p>DDPM 的一个主要缺点是<strong>推理速度慢</strong>（需要 <span
class="math inline">\(T\)</span> 步，例如 1000 步）。</p>
<p><strong>DDIM (Denoising Diffusion Implicit Models)</strong>。</p>
<p>DDPM 的效果很好，但它有两个主要缺点： 1.
<strong>推理速度慢：</strong> 它是一个马尔可夫过程，从 <span
class="math inline">\(\mathbf{x}_T\)</span> 生成 <span
class="math inline">\(\mathbf{x}_0\)</span> 必须执行 <span
class="math inline">\(T\)</span> 步（例如 1000 步）采样，非常耗时。 2.
<strong>推理是随机的：</strong> (Stochastic) 在每一步采样 <span
class="math inline">\(\mathbf{x}_{t-1} =
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) + \sigma_t \mathbf{z}\)</span>
时，都需要加入一个新的随机噪声 <span
class="math inline">\(\mathbf{z}\)</span>。这意味着即使从同一个 <span
class="math inline">\(\mathbf{x}_T\)</span> 出发，两次推理也会得到不同的
<span
class="math inline">\(\mathbf{x}_0\)</span>。这对于需要一致性的任务（如图像编辑）来说是个问题。</p>
<p>DDIM（2020年提出）巧妙地解决了这两个问题，并且<strong>无需重新训练</strong>在
DDPM 上训练好的模型。</p>
<p>这对应于您大纲中的 - 部分。</p>
<h3 id="ddim推理升级">6. DDIM：推理升级</h3>
<h4 id="ddim-的核心洞察重新审视反向过程">6.1 DDIM
的核心洞察：重新审视反向过程</h4>
<p>DDIM 的出发点是重新审视我们推导出的反向过程。DDPM 假设反向过程是
<span class="math inline">\(p_\theta(\mathbf{x}_{t-1} |
\mathbf{x}_t)\)</span>，并用它来近似 <span
class="math inline">\(q(\mathbf{x}_{t-1} | \mathbf{x}_t,
\mathbf{x}_0)\)</span>。</p>
<p>DDIM 注意到，我们训练的神经网络 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t,
t)\)</span> 实际上是在预测噪声 <span
class="math inline">\(\boldsymbol{\epsilon}\)</span>。
回顾我们的前向公式： <span class="math display">\[
\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 -
\bar{\alpha}_t} \boldsymbol{\epsilon}
\]</span> 既然我们有了 <span
class="math inline">\(\mathbf{x}_t\)</span>（当前输入）和 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span>（网络预测的
<span
class="math inline">\(\boldsymbol{\epsilon}\)</span>），我们可以直接反解出<strong>对清晰图像
<span class="math inline">\(\mathbf{x}_0\)</span>
的预测</strong>，我们称之为 <span
class="math inline">\(\hat{\mathbf{x}}_0\)</span>：</p>
<p><span class="math display">\[
\hat{\mathbf{x}}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( \mathbf{x}_t
- \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t,
t) \right)
\]</span> 这个 <span class="math inline">\(\hat{\mathbf{x}}_0\)</span>
是给定 <span class="math inline">\(\mathbf{x}_t\)</span>
时，模型对最终结果 <span class="math inline">\(\mathbf{x}_0\)</span>
的“最佳猜测”。</p>
<h4 id="升级-1确定性推理-deterministic-inference-对应">6.2 升级
1：确定性推理 (Deterministic Inference) (对应)</h4>
<p>DDPM 的采样公式为： <span class="math display">\[
\mathbf{x}_{t-1} = \underbrace{\boldsymbol{\mu}_\theta(\mathbf{x}_t,
t)}_{\text{均值}} + \underbrace{\sigma_t \mathbf{z}}_{\text{随机噪声}}
\]</span> DDIM 提出，这个过程不一定是随机的。DDIM 引入了一个新的参数
<span class="math inline">\(\eta\)</span> (eta) 来控制随机性。 * 当
<span class="math inline">\(\eta=1\)</span> 时，DDIM 的采样过程与 DDPM
完全相同（随机）。 * 当 <span class="math inline">\(\eta=0\)</span>
时，采样过程中的方差 <span class="math inline">\(\sigma_t\)</span>
被设为 0。</p>
<p><strong>当 <span class="math inline">\(\eta=0\)</span>（方差 <span
class="math inline">\(\sigma_t=0\)</span>）时</strong>，采样步骤变为：
<span class="math display">\[
\mathbf{x}_{t-1} = \boldsymbol{\mu}_\theta(\mathbf{x}_t, t)
\]</span> <strong>这是确定性的！</strong> 没有随机噪声 <span
class="math inline">\(\mathbf{z}\)</span> 的介入。</p>
<p><strong>这有什么用？</strong> 这意味着从一个固定的 <span
class="math inline">\(\mathbf{x}_T\)</span>
出发，无论运行多少次，<strong>总会生成完全相同的 <span
class="math inline">\(\mathbf{x}_0\)</span></strong>。这使得扩散模型可用于图像编辑、风格转换等需要保持一致性的任务。</p>
<p>DDIM 论文推导出了一个更通用的采样公式，它不依赖于 <span
class="math inline">\(\boldsymbol{\mu}_\theta\)</span> 而是直接使用
<span class="math inline">\(\hat{\mathbf{x}}_0\)</span> 和 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span>。 当 <span
class="math inline">\(\eta=0\)</span> (即 <span
class="math inline">\(\sigma_t = 0\)</span>) 时，从 <span
class="math inline">\(\mathbf{x}_t\)</span> 到 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span>
的<strong>确定性采样公式</strong>为：</p>
<p><span class="math display">\[
\mathbf{x}_{t-1} = \underbrace{\sqrt{\bar{\alpha}_{t-1}}
\hat{\mathbf{x}}_0}_{\text{指向“预测的” } \mathbf{x}_0} +
\underbrace{\sqrt{1 - \bar{\alpha}_{t-1}} \cdot \left(
\frac{\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \hat{\mathbf{x}}_0}{\sqrt{1 -
\bar{\alpha}_t}} \right)}_{\text{指向“当前的” } \mathbf{x}_t \text{
的方向}}
\]</span> (注意：<span class="math inline">\(\frac{\mathbf{x}_t -
\sqrt{\bar{\alpha}_t} \hat{\mathbf{x}}_0}{\sqrt{1 -
\bar{\alpha}_t}}\)</span> 正好等于 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span> )
所以，确定性（<span class="math inline">\(\eta=0\)</span>）的 DDIM
采样步骤也可以写为： <span class="math display">\[
\mathbf{x}_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \hat{\mathbf{x}}_0 +
\sqrt{1 - \bar{\alpha}_{t-1}} \cdot \boldsymbol{\epsilon}_\theta
\]</span></p>
<h4 id="升级-2跳步采样-skip-sampling-对应">6.3 升级 2：跳步采样 (Skip
Sampling) (对应)</h4>
<p>DDPM 必须一步一步 <span class="math inline">\(t \to t-1 \to t-2
\dots\)</span> 地采样，因为它是马尔可夫过程。</p>
<p>DDIM
的采样公式（如上所示）是<strong>非马尔可夫的</strong>。它不依赖于 <span
class="math inline">\(q(\mathbf{x}_{t-1} | \mathbf{x}_t,
\mathbf{x}_0)\)</span>，而是直接使用在 <span
class="math inline">\(t\)</span> 时刻预测的 <span
class="math inline">\(\hat{\mathbf{x}}_0\)</span> 来计算 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span>。</p>
<p><strong>关键洞察：</strong> 既然我们能从 <span
class="math inline">\(\mathbf{x}_t\)</span> 预测出 <span
class="math inline">\(\hat{\mathbf{x}}_0\)</span>，我们不仅能计算 <span
class="math inline">\(\mathbf{x}_{t-1}\)</span>，我们能计算<strong>任意</strong>
<span class="math inline">\(\mathbf{x}_{\tau}\)</span> (其中 <span
class="math inline">\(\tau &lt; t\)</span>)。</p>
<p>这使得<strong>跳步采样</strong>成为可能。我们不再需要完整的 <span
class="math inline">\(T=1000\)</span>
步，我们可以定义一个更短的子序列，例如 <span
class="math inline">\(S=20\)</span> 步： <span
class="math inline">\((\tau_1, \tau_2, \dots, \tau_S) = (1, 51, 101,
\dots, 951)\)</span></p>
<p>我们的推理循环不再是 <code>for t in (T...1)</code>，而是
<code>for i in (S...1)</code>： * <strong>当前步</strong>：<span
class="math inline">\(\tau_i\)</span> (例如 <span
class="math inline">\(\tau_{20} = 951\)</span>) *
<strong>目标步</strong>：<span class="math inline">\(\tau_{i-1}\)</span>
(例如 <span class="math inline">\(\tau_{19} = 901\)</span>)</p>
<p><strong>DDIM 跳步采样（确定性）公式：</strong> (对应)</p>
<ol type="1">
<li><strong>输入：</strong> 当前噪声图像 <span
class="math inline">\(\mathbf{x}_{\tau_i}\)</span> 和时间步 <span
class="math inline">\(\tau_i\)</span>。</li>
<li><strong>预测 <span
class="math inline">\(\hat{\mathbf{x}}_0\)</span>：</strong>
(与之前相同) <span class="math display">\[
\hat{\mathbf{x}}_0 = \frac{1}{\sqrt{\bar{\alpha}_{\tau_i}}} \left(
\mathbf{x}_{\tau_i} - \sqrt{1 - \bar{\alpha}_{\tau_i}}
\boldsymbol{\epsilon}_\theta(\mathbf{x}_{\tau_i}, \tau_i) \right)
\]</span></li>
<li><strong>计算 <span
class="math inline">\(\mathbf{x}_{\tau_{i-1}}\)</span>：</strong>
(使用确定性公式，将 <span class="math inline">\(t-1\)</span> 替换为
<span class="math inline">\(\tau_{i-1}\)</span>) <span
class="math display">\[
\mathbf{x}_{\tau_{i-1}} = \sqrt{\bar{\alpha}_{\tau_{i-1}}}
\hat{\mathbf{x}}_0 + \sqrt{1 - \bar{\alpha}_{\tau_{i-1}}} \cdot
\boldsymbol{\epsilon}_\theta(\mathbf{x}_{\tau_i}, \tau_i)
\]</span></li>
</ol>
<p><strong>结果：</strong> 我们不再需要 1000
步计算，而是通过<strong>跳步</strong>，仅用 20 步就完成了从 <span
class="math inline">\(\mathbf{x}_T\)</span> 到 <span
class="math inline">\(\mathbf{x}_0\)</span> 的生成。这极大地（例如 50
倍）提升了推理速度。</p>
<p><strong>小结</strong></p>
<p>DDIM 是对 DDPM 的一次重大升级，它通过引入 <span
class="math inline">\(\hat{\mathbf{x}}_0\)</span>
预测和非马尔可夫采样，实现了： 1. <strong>确定性推理</strong>（<span
class="math inline">\(\eta=0\)</span>），增强了模型的可控性。 2.
<strong>跳步采样</strong>，极大缩短了推理时间。 3.
最重要的是，它<strong>复用</strong>了 DDPM 训练好的 <span
class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span>
模型，无需额外训练。</p>
<p>好的，我们来探讨扩散模型演进的下一个重要阶段：<strong>流匹配 (Flow
Matching)</strong>。</p>
<p>在 DDPM 和 DDIM 中，我们都依赖于一个<strong>离散时间</strong>的
SDE（随机微分方程）或其确定性版本。我们模拟了 <span
class="math inline">\(T\)</span> 个离散步骤（例如 <span
class="math inline">\(T=1000\)</span>），这在数学上是有效的，但在概念上有些繁琐，并且依赖于
<span class="math inline">\(\beta_t\)</span> (或 <span
class="math inline">\(\bar{\alpha}_t\)</span>)
这个人工设计的“噪声表”。</p>
<p>流匹配 (Flow Matching, FM)
模型（2022年及后续工作）提出了一种更简洁、更根本的视角：<strong>连续时间</strong>的<strong>常微分方程
(ODE)</strong>。</p>
<p>这对应于您大纲中的 - 部分。</p>
<h3 id="流匹配连续轨迹与速度预测">7. 流匹配：连续轨迹与速度预测</h3>
<h4 id="核心思想从-sde-到-ode">7.1 核心思想：从 SDE 到 ODE</h4>
<ul>
<li><strong>DDPM (SDE)</strong>：将图像 <span
class="math inline">\(\mathbf{x}_0\)</span> 变为噪声 <span
class="math inline">\(\mathbf{x}_T\)</span> 的过程是随机的（<span
class="math inline">\(\mathbf{x}_t = \sqrt{\alpha_t} \mathbf{x}_{t-1} +
\sqrt{\beta_t} \boldsymbol{\epsilon}\)</span>）。</li>
<li><strong>流匹配
(ODE)</strong>：我们构建一个<strong>确定性的</strong>、<strong>连续的</strong>“流”，将纯噪声
<span class="math inline">\(\mathbf{z}\)</span>（我们这里称为 <span
class="math inline">\(\mathbf{x}_0\)</span>）平滑地转变为清晰图像 <span
class="math inline">\(\mathbf{x}_1\)</span>。</li>
</ul>
<p>我们不再考虑离散步骤 <span class="math inline">\(t=1, 2, \dots,
T\)</span>，而是考虑一个连续时间 <span class="math inline">\(t \in [0,
1]\)</span>：</p>
<ul>
<li><span class="math inline">\(t=0\)</span>：<span
class="math inline">\(\mathbf{x}_0\)</span> 是从 <span
class="math inline">\(\mathcal{N}(0, \mathbf{I})\)</span>
采样的纯噪声。</li>
<li><span class="math inline">\(t=1\)</span>：<span
class="math inline">\(\mathbf{x}_1\)</span>
是我们想要生成的清晰图像。</li>
</ul>
<p>这个从 <span class="math inline">\(\mathbf{x}_0\)</span> 到 <span
class="math inline">\(\mathbf{x}_1\)</span> 的连续演变路径 <span
class="math inline">\(\mathbf{x}_t\)</span> 由一个<strong>常微分方程
(ODE)</strong> 描述：</p>
<p><span class="math display">\[
\frac{d\mathbf{x}_t}{dt} = \mathbf{v}(\mathbf{x}_t, t)
\]</span> * <span class="math inline">\(\mathbf{v}(\mathbf{x}_t,
t)\)</span> 是一个<strong>速度向量场 (velocity vector field)</strong>。
* 它告诉我们：当一个点位于位置 <span
class="math inline">\(\mathbf{x}_t\)</span> 和时间 <span
class="math inline">\(t\)</span>
时，它应该往哪个方向（向量）以多快的速度（模长）移动。 *
<strong>训练目标：</strong> 我们的神经网络 <span
class="math inline">\(\mathbf{v}_\theta(\mathbf{x}_t, t)\)</span>
的目标就是学习这个<strong>速度场 <span
class="math inline">\(\mathbf{v}\)</span></strong>，而不是像 DDPM
那样学习噪声 <span
class="math inline">\(\boldsymbol{\epsilon}\)</span>。</p>
<h4 id="训练学习速度场-对应">7.2 训练：学习速度场 (对应)</h4>
<p><strong>问题：</strong> 理论上存在一个理想的速度场 <span
class="math inline">\(\mathbf{v}\)</span>
可以将噪声分布“推向”图像分布，但这个理想的 <span
class="math inline">\(\mathbf{v}\)</span> 非常复杂，我们无法知道。</p>
<p><strong>流匹配的创见：</strong>
我们不需要知道那个复杂的理想场。我们可以<strong>自己定义</strong>无数条简单的路径，然后训练网络来学习这些简单路径的平均速度。</p>
<p><strong>1. 定义简单路径（直线模型）：</strong> 给定一个噪声 <span
class="math inline">\(\mathbf{x}_0 \sim \mathcal{N}(0,
\mathbf{I})\)</span> 和一张真实图像 <span
class="math inline">\(\mathbf{x}_1 \sim
q(\text{data})\)</span>，连接它们的最简单路径是什么？<strong>一条直线</strong>。</p>
<p><span class="math display">\[\\mathbf{x}\_t = (1 - t) \\mathbf{x}\_0
+ t \\mathbf{x}\_1
\]</span> * 当 <span class="math inline">\(t=0\)</span> 时，<span
class="math inline">\(\mathbf{x}_t = \mathbf{x}_0\)</span> (噪声)。</p>
<ul>
<li>当 <span class="math inline">\(t=1\)</span> 时，<span
class="math inline">\(\mathbf{x}_t = \mathbf{x}_1\)</span> (图像)。</li>
</ul>
<p><strong>2. 计算目标速度：</strong> 如果我们的“粒子” <span
class="math inline">\(\mathbf{x}_t\)</span> 沿着这条直线路径运动，它在
<span class="math inline">\(t\)</span> 时刻的速度 <span
class="math inline">\(\mathbf{v}_t\)</span> 是多少？我们对 <span
class="math inline">\(t\)</span> 求导：</p>
<p><span class="math display">\[
\mathbf{v}_t = \frac{d\mathbf{x}_t}{dt} = \frac{d}{dt} \left( (1 - t)
\mathbf{x}_0 + t \mathbf{x}_1 \right)
\]</span><span class="math display">\[
\mathbf{v}_t = -\mathbf{x}_0 + \mathbf{x}_1 = \mathbf{x}_1 -
\mathbf{x}_0
\]</span><strong>这就是流匹配的训练目标！</strong>
沿着这条直线路径，在任何时间 <span
class="math inline">\(t\)</span>，目标速度都是恒定的 <span
class="math inline">\(\mathbf{x}_1 - \mathbf{x}_0\)</span>。</p>
<p><strong>3. 训练流程：</strong></p>
<ol type="1">
<li>从数据集中随机抽取一张清晰图像 <span
class="math inline">\(\mathbf{x}_1\)</span>。</li>
<li>随机采样一个标准高斯噪声 <span class="math inline">\(\mathbf{x}_0
\sim \mathcal{N}(0, \mathbf{I})\)</span>。</li>
<li><strong>随机</strong>选择一个时间 <span
class="math inline">\(t\)</span>（从 <span class="math inline">\(U(0,
1)\)</span> 均匀采样）。</li>
<li>使用直线公式<strong>一步</strong>计算出路径上的点：<span
class="math inline">\(\mathbf{x}_t = (1 - t) \mathbf{x}_0 + t
\mathbf{x}_1\)</span>。</li>
<li>将 <span class="math inline">\((\mathbf{x}_t, t)\)</span>
作为输入，喂给神经网络 <span
class="math inline">\(\mathbf{v}_\theta(\mathbf{x}_t,
t)\)</span>，得到预测速度 <span
class="math inline">\(\mathbf{v}_\theta\)</span>。</li>
<li>计算损失函数（均方误差 MSE）： $$</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$$L(\\theta) = E\_&#123;t, \\mathbf&#123;x&#125;\_0, \\mathbf&#123;x&#125;\_1&#125; \\left[ || (\\mathbf&#123;x&#125;\_1 - \\mathbf&#123;x&#125;*0) - \\mathbf&#123;v&#125;*\\theta(\\mathbf&#123;x&#125;\_t, t) ||^2 \\right]</span><br><span class="line">$$</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>
<ol start="7" type="1">
<li>使用梯度下降更新网络参数 <span
class="math inline">\(\theta\)</span>。</li>
</ol>
<h4 id="推理求解-ode-对应">7.3 推理：求解 ODE (对应)</h4>
<p>当我们训练好 <span class="math inline">\(\mathbf{v}_\theta\)</span>
后，我们就有了一个完整的速度场，它知道在时空中的任何点 <span
class="math inline">\((\mathbf{x}, t)\)</span> 应该如何移动。</p>
<p><strong>问题：</strong> 如何从 <span
class="math inline">\(\mathbf{x}_0\)</span> 积分到 <span
class="math inline">\(\mathbf{x}_1\)</span>？ 我们需要求解 ODE <span
class="math inline">\(\frac{d\mathbf{x}_t}{dt} =
\mathbf{v}_\theta(\mathbf{x}_t, t)\)</span>，从 <span
class="math inline">\(t=0\)</span> 求解到 <span
class="math inline">\(t=1\)</span>。</p>
<p><strong>方法：</strong>
我们使用数值积分方法，最简单的就是<strong>欧拉近似法</strong>（我们在第
1 部分 提到过）。</p>
<ol type="1">
<li><strong>起始：</strong> 随机采样一张纯噪声图像 <span
class="math inline">\(\mathbf{x}_0 \sim \mathcal{N}(0,
\mathbf{I})\)</span>。</li>
<li><strong>离散化：</strong> 将时间 <span class="math inline">\([0,
1]\)</span> 分为 <span class="math inline">\(N\)</span> 步（例如 <span
class="math inline">\(N=20\)</span>），每一步 <span
class="math inline">\(\Delta t = 1/N\)</span>。</li>
<li><strong>迭代：</strong> <strong>从 <span class="math inline">\(t =
0\)</span> 循环到 <span class="math inline">\(t = 1 - \Delta
t\)</span></strong>：
<ol type="a">
<li>获取当前位置 <span class="math inline">\(\mathbf{x}_t\)</span>
和时间 <span class="math inline">\(t\)</span>。</li>
<li>输入网络，得到当前速度：<span
class="math inline">\(\mathbf{v}_\theta =
\mathbf{v}_\theta(\mathbf{x}_t, t)\)</span>。</li>
<li><strong>欧拉法更新：</strong> <span class="math display">\[
\]</span><span class="math display">\[\\mathbf{x}\_{t + \\Delta t} =
\\mathbf{x}*t + \\mathbf{v}*\\theta \\cdot \\Delta t
\]</span> $$$$(新位置 = 旧位置 + 速度 × 时间)</li>
</ol></li>
<li><strong>结束：</strong> 当循环结束时，<span
class="math inline">\(\mathbf{x}_1\)</span> 就是生成的清晰图像。</li>
</ol>
<p><strong>优势：</strong></p>
<ul>
<li><strong>更简单：</strong> 训练目标 <span
class="math inline">\(\mathbf{x}_1 - \mathbf{x}_0\)</span>
非常直观，摆脱了 DDPM 中复杂的 <span
class="math inline">\(\bar{\alpha}_t, \beta_t\)</span> 系数。</li>
<li><strong>更高效：</strong> ODE 路径通常比 SDE
路径“更直”，因此流匹配通常可以用更少的推理步骤（例如 10-50
步）生成高质量图像。</li>
<li><strong>更灵活：</strong> 我们可以使用比欧拉法更高级的 ODE
求解器（如 Runge-Kutta）来进一步提高精度和速度。</li>
</ul>
<p><strong>总结</strong></p>
<p>扩散模型从基础到前沿的全部核心数学：</p>
<ol type="1">
<li><strong>基础 (Part 1)</strong>：高斯分布、朗之万动力学 (SDE)
和贝叶斯公式。</li>
<li><strong>DDPM (Part 2-5)</strong>：
<ul>
<li><strong>前向 (q)</strong>：<span
class="math inline">\(q(\mathbf{x}_t | \mathbf{x}_0) =
\mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)
\mathbf{I})\)</span></li>
<li><strong>反向 (p)</strong>：<span
class="math inline">\(q(\mathbf{x}_{t-1} | \mathbf{x}_t,
\mathbf{x}_0)\)</span> 的推导。</li>
<li><strong>训练</strong>：预测噪声 <span class="math inline">\(L = ||
\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)
||^2\)</span>。</li>
<li><strong>推理</strong>：<span class="math inline">\(\mathbf{x}_{t-1}
= \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) + \sigma_t
\mathbf{z}\)</span> (随机，T 步)。</li>
</ul></li>
<li><strong>DDIM (Part 6)</strong>：
<ul>
<li><strong>核心</strong>：预测 <span
class="math inline">\(\hat{\mathbf{x}}_0\)</span>。</li>
<li><strong>推理</strong>：<span
class="math inline">\(\mathbf{x}_{\tau_{i-1}} = \dots\)</span>
(确定性，可跳步)。</li>
</ul></li>
<li><strong>流匹配 (Part 7)</strong>：
<ul>
<li><strong>核心</strong>：ODE 连续流 <span
class="math inline">\(\frac{d\mathbf{x}_t}{dt} =
\mathbf{v}(\mathbf{x}_t, t)\)</span>。</li>
<li><strong>训练</strong>：预测速度 <span class="math inline">\(L = ||
(\mathbf{x}_1 - \mathbf{x}_0) - \mathbf{v}_\theta(\mathbf{x}_t, t)
||^2\)</span>。</li>
<li><strong>推理</strong>：<span class="math inline">\(\mathbf{x}_{t +
\Delta t} = \mathbf{x}_t + \mathbf{v}_\theta \cdot \Delta t\)</span>
(ODE 求解)。</li>
</ul></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/31/5120-1031/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/31/5120-1031/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-10-31 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-31T21:00:00+08:00">2025-10-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-11-05 15:24:36" itemprop="dateModified" datetime="2025-11-05T15:24:36+08:00">2025-11-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - 计算能源材料和电子结构模拟 Lecture</p>
<h2 id="møller-plesset-mp-微扰理论">1 Møller-Plesset (MP) 微扰理论:</h2>
<ul>
<li><strong>内容</strong>:</li>
</ul>
<p>量子化学中关于 <strong>Møller-Plesset (MP) 微扰理论</strong>
的推导过程。这是一种在 Hartree-Fock (HF)
方法基础上引入电子相关效应（electron correlation）的后-HF方法。</p>
<h3 id="标准微扰理论公式">标准微扰理论公式</h3>
<p>这部分列出的是通用的 <strong>Rayleigh-Schrödinger 微扰理论</strong>
(RSPT) 的基本公式：</p>
<ol type="1">
<li><strong>哈密顿算符 (Hamiltonian) 拆分:</strong>
<ul>
<li><span class="math inline">\(\hat{H} = \hat{H}_0 +
\hat{H}&#39;\)</span></li>
<li>将体系的总哈密顿算符 <span class="math inline">\(\hat{H}\)</span>
拆分为一个可以精确求解的零阶哈密顿算符 <span
class="math inline">\(\hat{H}_0\)</span> 和一个微扰项 <span
class="math inline">\(\hat{H}&#39;\)</span>。</li>
</ul></li>
<li><strong>能量和波函数的展开:</strong>
<ul>
<li><span class="math inline">\(E_n = E_n^{(0)} + E_n^{(1)} + E_n^{(2)}
+ \dots\)</span></li>
<li><span class="math inline">\(|n\rangle = |n^{(0)}\rangle +
|n^{(1)}\rangle + |n^{(2)}\rangle + \dots\)</span></li>
<li>体系的真实能量 <span class="math inline">\(E_n\)</span> 和波函数
<span class="math inline">\(|n\rangle\)</span>
可以展开为零阶、一阶、二阶等各级修正项的总和。</li>
</ul></li>
<li><strong>一阶能量修正 (First-order energy correction):</strong>
<ul>
<li><span class="math inline">\(E_n^{(1)} = \langle n^{(0)} |
\hat{H}&#39; | n^{(0)} \rangle\)</span></li>
<li>一阶能量修正是微扰算符 <span
class="math inline">\(\hat{H}&#39;\)</span> 在零阶波函数 <span
class="math inline">\(|n^{(0)}\rangle\)</span> 下的期望值。</li>
</ul></li>
<li><strong>二阶能量修正 (Second-order energy correction):</strong>
<ul>
<li><span class="math inline">\(E_n^{(2)} = \sum_{k \neq n}
\frac{|\langle k^{(0)} | \hat{H}&#39; | n^{(0)} \rangle|^2}{E_n^{(0)} -
E_k^{(0)}}\)</span></li>
<li>二阶能量修正涉及所有其他零阶本征态 <span
class="math inline">\(|k^{(0)}\rangle\)</span>。</li>
</ul></li>
</ol>
<h3 id="møller-plesset-理论的定义">Møller-Plesset 理论的定义</h3>
<p>这部分将上述通用公式应用到 MP 理论中，定义了 <span
class="math inline">\(\hat{H}_0\)</span> 和 <span
class="math inline">\(\hat{H}&#39;\)</span>：</p>
<ol type="1">
<li><strong>MP 理论的哈密顿算符定义:</strong>
<ul>
<li><strong>零阶哈密顿 <span
class="math inline">\(\hat{H}_0\)</span>:</strong>
<ul>
<li><span class="math inline">\(\hat{H}_0 = \hat{F} = \sum_{i=1}^N
\hat{f}(i)\)</span> (注：<span class="math inline">\(\hat{F}\)</span> 是
Fock 算符，<span class="math inline">\(\hat{H}_0\)</span> 被定义为 <span
class="math inline">\(N\)</span> 个电子的 Fock 算符之和)</li>
</ul></li>
<li><strong>微扰项 <span
class="math inline">\(\hat{H}&#39;\)</span>:</strong>
<ul>
<li><span class="math inline">\(\hat{H}&#39; = \hat{H} -
\hat{F}\)</span> (即 <span class="math inline">\(\hat{H} -
\hat{H}_0\)</span>)</li>
<li>微扰项是真实的哈密顿算符 <span
class="math inline">\(\hat{H}\)</span> (包含完整的电子-电子排斥) 与 Fock
算符 <span class="math inline">\(\hat{F}\)</span>
(只包含平均化的电子排斥) 之间的差值。这个差值就是 HF
理论所忽略的“电子相关”。</li>
</ul></li>
</ul></li>
<li><strong>零阶波函数和能量:</strong>
<ul>
<li>$|_0= $ HF Slater determinant</li>
<li>零阶波函数（即 <span class="math inline">\(\hat{H}_0\)</span>
的本征函数）被选为 <strong>Hartree-Fock (HF)
斯莱特行列式</strong>。</li>
<li><span class="math inline">\(\hat{H}_0 |\Phi_0\rangle = (\sum_{i=1}^N
\varepsilon_i) |\Phi_0\rangle\)</span></li>
<li>零阶能量 <span class="math inline">\(E^{(0)}\)</span> (即 <span
class="math inline">\(E_{MP0}\)</span>) 是所有被占据分子轨道 <span
class="math inline">\(\varepsilon_i\)</span> 的能量总和。</li>
<li><strong><span class="math inline">\(E_{MP0} = \langle \Phi_0 |
\hat{H}_0 | \Phi_0 \rangle = \sum_{i=1}^N
\varepsilon_i\)</span></strong></li>
</ul></li>
<li><strong>MP1 能量 (一阶能量修正):</strong>
<ul>
<li><span class="math inline">\(E_{MP1} = \langle \Phi_0 | \hat{H}&#39;
| \Phi_0 \rangle = \langle \Phi_0 | \hat{H} - \hat{F} | \Phi_0
\rangle\)</span></li>
<li><span class="math inline">\(E_{MP1} = \langle \Phi_0 | \hat{H} |
\Phi_0 \rangle - \langle \Phi_0 | \hat{F} | \Phi_0 \rangle\)</span></li>
<li>由于 <span class="math inline">\(\langle \Phi_0 | \hat{H} | \Phi_0
\rangle\)</span> 正是 Hartree-Fock 能量 <span
class="math inline">\(E_{HF}\)</span>，而 <span
class="math inline">\(\langle \Phi_0 | \hat{F} | \Phi_0 \rangle =
E_{MP0} = \sum \varepsilon_i\)</span>。</li>
<li>因此：<strong><span class="math inline">\(E_{MP1} = E_{HF} -
\sum_{i=1}^N \varepsilon_i\)</span></strong></li>
</ul></li>
</ol>
<blockquote>
<p><strong>重要结论：</strong> 零阶能量 (<span
class="math inline">\(E_{MP0}\)</span>) 和一阶能量 (<span
class="math inline">\(E_{MP1}\)</span>) 的总和恰好等于 Hartree-Fock
能量： <span class="math inline">\(E_{MP0} + E_{MP1} = (\sum
\varepsilon_i) + (E_{HF} - \sum \varepsilon_i) = E_{HF}\)</span>
这意味着 <strong>MP1 理论得到的总能量就是 HF 能量</strong>。要获得对 HF
能量的第一个修正（即电子相关能），我们必须计算到二阶，即
<strong>MP2</strong>。</p>
</blockquote>
<h2 id="mp2-能量推导">2. MP2 能量推导</h2>
<p>这部分是 MP 理论的核心，推导了 <strong>MP2
能量（二阶能量修正）</strong>：</p>
<ol type="1">
<li><strong>MP2 通用公式:</strong>
<ul>
<li><span class="math inline">\(E_{MP2} = \sum_{k \neq 0} \frac{|\langle
k | \hat{H}&#39; | 0 \rangle|^2}{E_0 - E_k}\)</span></li>
<li>这是将左侧的 <span class="math inline">\(E_n^{(2)}\)</span>
公式应用于基态 (<span class="math inline">\(n=0\)</span>)。</li>
</ul></li>
<li><strong>关键简化 (Brillouin 定理):</strong>
<ul>
<li>根据 Brillouin 定理，对于所有<strong>单激发</strong>行列式 <span
class="math inline">\(|\Phi_i^a\rangle\)</span> (即一个电子从占据轨道
<span class="math inline">\(i\)</span> 跃迁到空轨道 <span
class="math inline">\(a\)</span>)，矩阵元 <span
class="math inline">\(\langle \Phi_i^a | \hat{H}&#39; | \Phi_0 \rangle =
0\)</span>。</li>
<li>这意味着在求和 <span class="math inline">\(\sum_{k \neq 0}\)</span>
时，所有单激发的项都为零。</li>
<li>因此，对 MP2
能量有贡献的<strong>第一类非零项</strong>来自<strong>双激发</strong>行列式
(Doubly-excited determinants)，记为 <span
class="math inline">\(|\Phi_{ij}^{ab}\rangle\)</span>（即电子 <span
class="math inline">\(i, j\)</span> 激发到空轨道 <span
class="math inline">\(a, b\)</span>）。</li>
<li>白板中间的图示 <code>|| -&gt; #</code> (两条线变到两条更高阶的线)
正是形象地表示了这种双激发。</li>
</ul></li>
<li><strong>MP2 最终公式:</strong>
<ul>
<li><span class="math inline">\(E_{MP2} = \sum_{i&lt;j}^{occ}
\sum_{a&lt;b}^{vir} \frac{|\langle \Phi_{ij}^{ab} | \hat{H}&#39; |
\Phi_0 \rangle|^2}{\varepsilon_i + \varepsilon_j - \varepsilon_a -
\varepsilon_b}\)</span></li>
<li><strong>求和:</strong> 遍历所有占据轨道对 (<span
class="math inline">\(i, j\)</span>) 和所有空轨道（虚拟轨道）对 (<span
class="math inline">\(a, b\)</span>)。</li>
<li><strong>分母:</strong> <span class="math inline">\(E_0 - E_k = (\sum
\varepsilon) - (\sum \varepsilon - \varepsilon_i - \varepsilon_j +
\varepsilon_a + \varepsilon_b) = \varepsilon_i + \varepsilon_j -
\varepsilon_a -
\varepsilon_b\)</span>。这是零阶能量差，即激发所消耗的轨道能量。</li>
<li><strong>分子:</strong> <span class="math inline">\(\langle
\Phi_{ij}^{ab} | \hat{H}&#39; | \Phi_0 \rangle\)</span>
可以被简化为一个关于分子轨道的双电子积分 <span
class="math inline">\(\langle ij || ab \rangle\)</span>。</li>
</ul></li>
</ol>
<p><strong>总结：</strong> Møller-Plesset
微扰理论的标准推导，核心思想是将 HF
解作为零阶近似，并将电子相关的“剩余部分”作为微扰。推导表明，MP1
能量只是重现了 HF 能量，而 <strong>MP2
能量是第一个真正包含电子相关效应的修正项</strong>，它通过计算所有可能的双激发对总能量的贡献来实现。</p>
<p>从 Møller-Plesset (MP)
理论过渡到了另一种更高级的量子化学方法：<strong>耦合簇理论 (Coupled
Cluster Theory)</strong>。</p>
<h3 id="mp2-能量的深入探讨">MP2 能量的深入探讨</h3>
<p>这部分继续讨论 MP2 能量（二阶能量修正）：</p>
<ol type="1">
<li><strong>MP2 能量公式 (重复):</strong>
<ul>
<li><span class="math inline">\(E_{MP2} = \sum_{k \neq 0} \frac{|\langle
\Phi_0 | \hat{H}&#39; | \Phi_k \rangle|^2}{E_{MP0} -
E_k^{(0)}}\)</span></li>
<li>这是 MP2 能量的通用形式。</li>
</ul></li>
<li><strong>微扰的来源:</strong>
<ul>
<li><strong><span class="math inline">\(\frac{e^2}{|r_i -
r_j|}\)</span></strong> (被圈出)</li>
<li>这是电子-电子间的库仑排斥算符。这个项是 <span
class="math inline">\(\hat{H}\)</span>（真实哈密顿算符）和 <span
class="math inline">\(\hat{H}_0\)</span>（Fock
算符，仅包含平均场排斥）之间差异的核心。正是这个“瞬时相关”的相互作用导致了
<span class="math inline">\(\hat{H}&#39;\)</span>（微扰）的存在，也是 MP
理论试图修正的能量来源。</li>
</ul></li>
<li><strong>Brillouin 定理的应用:</strong>
<ul>
<li><span class="math inline">\(\langle \Phi_0 | \hat{H}&#39; | \Phi_i^a
\rangle = 0\)</span></li>
<li>这再次强调了上一张白板的结论：微扰算符 <span
class="math inline">\(\hat{H}&#39;\)</span> 在基态 <span
class="math inline">\(\Phi_0\)</span>
和任何<strong>单激发</strong>行列式 <span
class="math inline">\(\Phi_i^a\)</span> 之间的矩阵元为零。</li>
<li><span class="math inline">\(\langle \Phi_0 | \hat{H} - \hat{F} |
\Phi_i^a \rangle = 0\)</span> (这是 <span
class="math inline">\(\hat{H}&#39;\)</span> 的展开)</li>
<li><span class="math inline">\(\langle \Phi_a | \hat{f} | \Phi_i
\rangle = 0\)</span> (注：<span class="math inline">\(\Phi_i\)</span> 和
<span class="math inline">\(\Phi_a\)</span> 应为轨道 <span
class="math inline">\(\phi_i\)</span> 和 <span
class="math inline">\(\phi_a\)</span>)</li>
<li>这一行解释了为什么 Brillouin 定理在 MP 理论中成立：因为在标准的
Hartree-Fock (HF) 方法中，占据轨道 (<span
class="math inline">\(\phi_i\)</span>) 和空轨道 (<span
class="math inline">\(\phi_a\)</span>) 之间的 Fock 矩阵元（即 <span
class="math inline">\(\langle \phi_a | \hat{f} | \phi_i
\rangle\)</span>）被设为零。</li>
</ul></li>
<li><strong>MP2 最终实用公式:</strong>
<ul>
<li><span class="math inline">\(E_{MP2} = \sum_{i&lt;j}^{occ}
\sum_{a&lt;b}^{vir} \frac{|\langle \Phi_0 | \hat{H}&#39; |
\Phi_{ij}^{ab} \rangle|^2}{E_{MP0} - E_{ij}^{ab}}\)</span></li>
<li>由于单激发项为零，求和 <span class="math inline">\(k \neq 0\)</span>
中幸存下来的第一类项就是<strong>双激发</strong>项 <span
class="math inline">\(\Phi_{ij}^{ab}\)</span>（电子从 <span
class="math inline">\(i,j\)</span> 轨道激发到 <span
class="math inline">\(a,b\)</span> 轨道）。</li>
<li><span class="math inline">\(E_{ij}^{ab}\)</span>
是双激发组态的零阶能量。</li>
</ul></li>
<li><strong>MP2 能量的性质:</strong>
<ul>
<li><strong><span class="math inline">\(&lt; 0\)</span></strong>: MP2
修正能量<strong>总是负值</strong>。这意味着 MP2 能量总是在 HF
能量（<span class="math inline">\(E_{MP0} +
E_{MP1}\)</span>）的基础上进一步降低总能量。这是符合物理直觉的，因为电子相关效应允许电子更好地相互“躲避”，从而降低体系的总能量。</li>
<li><strong>“80%” (大约)</strong>:
这是一个常见的经验之谈，即对于许多小分子，MP2 方法大约能“恢复”80% 到 90%
的电子相关能。</li>
</ul></li>
</ol>
<h2 id="耦合簇理论-coupled-cluster-theory">3. 耦合簇理论 (Coupled
Cluster Theory)</h2>
<p>这部分介绍了一种更强大、更准确（也更昂贵）的后-HF方法。</p>
<ol type="1">
<li><p><strong>标题:</strong>
<code>Coupled Cluster Theory</code></p></li>
<li><p><strong>CC 波函数拟设 (Ansatz):</strong></p>
<ul>
<li><span class="math inline">\(\Psi_{CC} = e^{\hat{T}}
\Phi_{HF}\)</span> (这里 <span class="math inline">\(\Phi_{HF}\)</span>
即 <span class="math inline">\(\Phi_0\)</span>)</li>
<li>这是 CC 理论的核心！它假设真实的波函数 <span
class="math inline">\(\Psi_{CC}\)</span> 可以通过一个“指数算符” <span
class="math inline">\(e^{\hat{T}}\)</span> 作用在 HF 参考波函数 <span
class="math inline">\(\Phi_0\)</span> 上得到。</li>
</ul></li>
<li><p><strong>指数算符 <span
class="math inline">\(e^{\hat{T}}\)</span>:</strong></p>
<ul>
<li><span class="math inline">\(e^{\hat{T}} = \hat{1} + \hat{T} +
\frac{1}{2!} \hat{T}^2 + \frac{1}{3!} \hat{T}^3 + \dots\)</span></li>
<li>指数算符通过泰勒级数展开。这种指数形式具有非常重要的特性，即它能自动包含“非关联”的高阶激发（例如，两次独立的双激发
<span class="math inline">\(\hat{T}_2^2\)</span> 会产生四激发），这使得
CC 方法具有<strong>大小一致性 (size-consistency)</strong>，这是 MP
理论（在某些阶数上）所缺乏的重要特性。</li>
</ul></li>
<li><p><strong>簇算符 (Cluster Operator) <span
class="math inline">\(\hat{T}\)</span>:</strong></p>
<ul>
<li><span class="math inline">\(\hat{T} = \hat{T}_1 + \hat{T}_2 +
\hat{T}_3 + \dots\)</span></li>
<li>簇算符 <span class="math inline">\(\hat{T}\)</span>
本身是所有可能的激发算符的总和。</li>
</ul></li>
<li><p><strong>激发算符的定义:</strong></p>
<ul>
<li><strong><span class="math inline">\(\hat{T}_1\)</span>
(单激发):</strong>
<ul>
<li><span class="math inline">\(\hat{T}_1 \Phi_0 = \sum_{i}^{occ}
\sum_{a}^{vir} t_i^a \Phi_i^a\)</span></li>
<li><span class="math inline">\(\hat{T}_1\)</span>
产生所有可能的单激发态的线性组合。<span
class="math inline">\(t_i^a\)</span> 是未知的“振幅
(amplitudes)”，即激发的重要性权重，需要通过求解 CC 方程得到。</li>
</ul></li>
<li><strong><span class="math inline">\(\hat{T}_2\)</span>
(双激发):</strong>
<ul>
<li><span class="math inline">\(\hat{T}_2 \Phi_0 = \sum_{i&lt;j}^{occ}
\sum_{a&lt;b}^{vir} t_{ij}^{ab} \Phi_{ij}^{ab}\)</span></li>
<li><span class="math inline">\(\hat{T}_2\)</span>
产生所有可能的双激发态的线性组合。<span
class="math inline">\(t_{ij}^{ab}\)</span> 是双激发的振幅。</li>
</ul></li>
</ul></li>
</ol>
<p><strong>总结：</strong> 这张白板从 MP2
理论（一种基于微扰的方法）过渡到了耦合簇理论（一种基于指数拟设的非微扰方法）。</p>
<ul>
<li><strong>MP2</strong>
通过二阶微扰，只显式地考虑了<strong>双激发</strong>对能量的贡献。</li>
<li><strong>Coupled Cluster</strong> (例如 CCSD，即 <span
class="math inline">\(\hat{T} = \hat{T}_1 + \hat{T}_2\)</span>)
则系统地包含了 <span class="math inline">\(\hat{T}_1\)</span> (单激发)
和 <span class="math inline">\(\hat{T}_2\)</span> (双激发)
及其所有乘积（如 <span class="math inline">\(\hat{T}_1^2, \hat{T}_1
\hat{T}_2, \hat{T}_2^2\)</span>
等），因此它隐式地包含了某些更高阶的激发（如四激发），使其成为比 MP2
更准确、更鲁棒的方法。</li>
</ul>
<p>当然可以。这是一个非常核心的量子化学概念。</p>
<p>简单来说：<strong>双激发 (Double Excitations)
是描述电子“躲避”彼此这一行为的<em>最主要</em>、<em>最基本</em>的数学方式。</strong></p>
<p>MP2 和 CCSD 都高度关注双激发，因为它们是捕获“电子相关能” (Electron
Correlation Energy) 的关键。</p>
<h3 id="问题的根源hf-理论错过了什么">问题的根源：HF
理论错过了什么？</h3>
<p>在白板的 MP 理论推导中，我们从 <strong>Hartree-Fock (HF)
波函数</strong> (<span class="math inline">\(\Phi_0\)</span>) 开始。</p>
<ul>
<li><strong>HF 的问题：</strong> HF
是一种“平均场”理论。它假设一个电子（例如电子 <span
class="math inline">\(i\)</span>）只感受到所有其他电子（例如电子 <span
class="math inline">\(j\)</span>）的<em>平均</em>电荷分布，而不是它们在某一<em>瞬时</em>的真实位置。</li>
<li><strong>物理现实：</strong>
电子是带负电的粒子，它们会<em>瞬时</em>地相互排斥。如果电子 <span
class="math inline">\(i\)</span> 在分子的 A 点，电子 <span
class="math inline">\(j\)</span> 会倾向于<em>避开</em> A 点，跑到 B
点去。这种为了“躲避”对方而调整自己行为的现象，就叫做 <strong>电子相关
(Electron Correlation)</strong>。</li>
</ul>
<p>HF 理论忽略了这种瞬时躲避，因此 HF
能量总是高于真实的基态能量。我们所说的“电子相关能”，就是这个能量差。</p>
<h3 id="解决方案双激发phi_ijab">解决方案：双激发（<span
class="math inline">\(\Phi_{ij}^{ab}\)</span>）</h3>
<p>我们如何用数学来描述“电子 <span class="math inline">\(i\)</span> 和
<span class="math inline">\(j\)</span> 相互躲避”？</p>
<p>想象一下，在 HF 基态 (<span class="math inline">\(\Phi_0\)</span>)
中，电子 <span class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span> 在它们各自的轨道里。</p>
<ul>
<li>为了描述它们“躲开”彼此，我们需要在波函数中“混入”一个新的组态
(configuration)。</li>
<li>在这个新组态中，电子 <span class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span> <em>同时</em>从它们原来的轨道（占据轨道
<span class="math inline">\(i,
j\)</span>）“跳”到了两个新的、能量更高的空轨道（虚拟轨道 <span
class="math inline">\(a, b\)</span>）。</li>
<li>这个“双重跳跃”的态，就是白板上的 <strong>双激发态 <span
class="math inline">\(\Phi_{ij}^{ab}\)</span></strong>。</li>
</ul>
<p>通过将这个双激发态 <span
class="math inline">\(\Phi_{ij}^{ab}\)</span> 线性叠加到基态 <span
class="math inline">\(\Phi_0\)</span>
中，我们的总波函数就能描述这样一种情形：电子 <span
class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span>
有一定概率<em>不在</em>它们“应该”在的地方，而是跑到了别处——这就是它们相互躲避的数学表达。</p>
<p><strong>为什么单激发 (<span class="math inline">\(\Phi_i^a\)</span>)
不行？</strong> 如白板所示，Brillouin 定理（<span
class="math inline">\(\langle \Phi_0 | \hat{H}&#39; | \Phi_i^a \rangle =
0\)</span>）告诉我们，在 MP 理论框架下，HF
基态和单激发态之间没有直接的相互作用。单激发主要描述的是轨道本身的形状调整（轨道弛豫），而不是电子<em>之间</em>的相关。</p>
<h3 id="mp2-和-ccsd-的联系与区别">MP2 和 CCSD 的联系与区别</h3>
<p>MP2 和 CCSD
都是基于这个核心思想，但实现方式的复杂程度和准确性完全不同。</p>
<h4 id="mp2-møller-plesset-2nd-order">🔹 MP2 (Møller-Plesset 2nd
Order)</h4>
<ul>
<li><strong>它做了什么：</strong> MP2
是一种<strong>微扰</strong>方法。它把双激发当作一种“微小的扰动”。</li>
<li><strong>如何工作：</strong> 它使用二阶微扰公式（白板上的 <span
class="math inline">\(E_{MP2}\)</span>）来计算：<em>“如果我允许体系中的每一对电子
(i, j) 发生一次双激发 (到 a, b)，这会使体系的总能量降低多少？”</em></li>
<li><strong>局限性：</strong>
<ol type="1">
<li>它只计算<em>一次</em>激发的影响。</li>
<li>它只考虑了双激发。</li>
<li>它假设这种扰动很“小”。</li>
</ol></li>
</ul>
<p>MP2
是最简单、计算最便宜的包含电子相关的方法，它抓住了相关能的“大头”（
<code>~80%</code>）。</p>
<h4 id="ccsd-coupled-cluster-singles-and-doubles">🔹 CCSD (Coupled
Cluster Singles and Doubles)</h4>
<ul>
<li><strong>它做了什么：</strong> CCSD
是一种<strong>非微扰</strong>方法。它不认为相关是“小扰动”，而是从根本上重新构建波函数。</li>
<li><strong>如何工作：</strong> 它使用指数拟设 <span
class="math inline">\(\Psi_{CC} = e^{\hat{T}_1 + \hat{T}_2}
\Phi_0\)</span>。
<ul>
<li><span class="math inline">\(\hat{T}_2\)</span>
算符（白板上有写）就是用来产生所有双激发的。</li>
<li><span class="math inline">\(\hat{T}_1\)</span>
算符（白板上有写）用来产生所有单激发（用于轨道弛豫）。</li>
</ul></li>
<li><strong>关键区别（指数的魔力）：</strong>
<ul>
<li>当指数 <span class="math inline">\(e^{\hat{T}}\)</span> 展开时
(<span class="math inline">\(e^{\hat{T}} = 1 + \hat{T} +
\frac{1}{2}\hat{T}^2 + \dots\)</span>)，你会得到像 <span
class="math inline">\(\frac{1}{2}(\hat{T}_2)^2\)</span> 这样的项。</li>
<li><span class="math inline">\(\frac{1}{2}(\hat{T}_2)^2\)</span>
意味着<strong>两次独立的双激发</strong>。这在物理上代表了<strong>四激发</strong>（例如，分子中两对互不相干的电子同时在各自“躲避”）。</li>
<li><strong>这就是 CCSD 远比 MP2 准确的原因</strong>：它不仅包含了 <span
class="math inline">\(\hat{T}_2\)</span>（双激发），还通过指数形式<em>自动包含</em>了由
<span class="math inline">\(\hat{T}_2\)</span>
组合产生的高阶激发（如四激发、六激发等）。它正确地描述了多个电子对同时发生相关行为的情况。</li>
</ul></li>
</ul>
<h3 id="总结对比">总结对比</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">特性</th>
<th style="text-align: left;">MP2</th>
<th style="text-align: left;">CCSD ( <span class="math inline">\(\hat{T}
= \hat{T}_1 + \hat{T}_2\)</span> )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>方法类型</strong></td>
<td style="text-align: left;">微扰理论 (Perturbative)</td>
<td style="text-align: left;">非微扰 (Non-perturbative)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>核心思想</strong></td>
<td
style="text-align: left;">计算<strong>双激发</strong>对能量的<em>二阶</em>修正。</td>
<td
style="text-align: left;">用指数算符包含所有<strong>单、双激发</strong>。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>包含的激发</strong></td>
<td style="text-align: left;">仅显式包含<strong>双激发</strong>。</td>
<td style="text-align: left;">显式包含<strong>单、双激发</strong>。<br>
<em>隐式</em>包含高阶激发（四、六等）。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>准确性</strong></td>
<td style="text-align: left;">良好（约 80-90% 相关能）</td>
<td style="text-align: left;">优秀（“黄金标准”之一）</td>
</tr>
<tr>
<td style="text-align: left;"><strong>数学联系</strong></td>
<td style="text-align: left;">MP2 的能量公式可以被证明是 CCSD
方程的<em>最低阶近似</em>。</td>
<td style="text-align: left;">更完整、更高级的理论。</td>
</tr>
</tbody>
</table>
<p><strong>总结：</strong></p>
<p><strong>双激发是描述电子相关（相互躲避）的物理核心。MP2
用最简单的方式估算了它的能量贡献，而 CCSD
则用一种更完备、更强大的数学（指数）形式将其及组合效应系统地包含了进来。</strong></p>
<h2 id="量子蒙特卡洛-quantum-monte-carlo">4. 量子蒙特卡洛 (Quantum Monte
Carlo)</h2>
<h3 id="耦合簇理论-coupled-cluster">耦合簇理论 (Coupled Cluster)</h3>
<h4 id="ccsdtt-的含义">1. CCSD(T)：“(T)” 的含义</h4>
<ul>
<li><strong>回顾 <code>CCSD</code>：</strong> 正如标题
<code>Coupled Cluster Singles and Doubles (CCSD)</code> 所示，CCSD
方法只完整地包含了单激发算符 (<span
class="math inline">\(\hat{T}_1\)</span>) 和双激发算符 (<span
class="math inline">\(\hat{T}_2\)</span>)。它通过指数形式 <span
class="math inline">\(e^{(\hat{T}_1 +
\hat{T}_2)}\)</span>，已经能间接地包含一些高阶激发（如四激发 <span
class="math inline">\(\hat{T}_2^2\)</span>）。</li>
<li><strong>缺失的项：</strong> CCSD <strong>没有</strong> 显式地包含
<strong>“连通”(connected) 三激发算符 <span
class="math inline">\(\hat{T}_3\)</span></strong>。<span
class="math inline">\(\hat{T}_3\)</span>
描述的是三个电子<em>同时</em>相互关联并激发到三个空轨道。</li>
<li><strong><code>(T)</code> 的含义：</strong> <code>(T)</code> 代表
<strong>“微扰三激发” (perturbative Triples)</strong>。
<ul>
<li><strong>为什么不直接用 <code>CCSDT</code>？</strong> 完整地求解包含
<span class="math inline">\(\hat{T}_3\)</span> 的方程（即 CCSDT
方法）在计算上极其昂贵（计算量随体系大小的 <span
class="math inline">\(N^8\)</span> 增长）。</li>
<li><strong><code>(T)</code> 的解决之道：</strong> <code>CCSD(T)</code>
是一种巧妙的折中。它首先执行一个完整的、较便宜的 CCSD 计算（求解 <span
class="math inline">\(\hat{T}_1\)</span> 和 <span
class="math inline">\(\hat{T}_2\)</span>）。然后，它使用这些已知的 <span
class="math inline">\(\hat{T}_1\)</span> 和 <span
class="math inline">\(\hat{T}_2\)</span>
振幅，通过<strong>微扰理论</strong>（就像第一张白板上的 MP
理论一样！）来<em>估算</em> <span
class="math inline">\(\hat{T}_3\)</span>
会对总能量产生的<em>修正</em>。</li>
</ul></li>
<li><strong>“黄金标准” (Gold Standard)：</strong> <code>CCSD(T)</code>
方法被广泛认为是量子化学的“黄金标准”。它以可承受的计算成本（<span
class="math inline">\(N^7\)</span>
增长）提供了接近“完美”的能量，是绝大多数高精度计算的基准。</li>
<li><strong>“95%” 注释：</strong> 白板上的 <code>95%</code>
注释很可能是讲师的一个经验之谈，即 CCSD 大约能恢复 95%
的电子相关能（相比 MP2 的 ~80%），而 <code>(T)</code>
修正则能将这个数字推向 99% 甚至更高。</li>
</ul>
<h4 id="耦合簇方程-cc-equations">2. 耦合簇方程 (CC Equations)</h4>
<p>白板的中间部分展示了如何<em>求解</em> CCSD 方程以获得能量 <span
class="math inline">\(E\)</span> 和振幅 <span
class="math inline">\(t_i^a, t_{ij}^{ab}\)</span>。</p>
<ul>
<li><p><strong>总薛定谔方程：</strong></p>
<ul>
<li><span class="math inline">\(H e^{\hat{T}} | \Phi_0 \rangle = E
e^{\hat{T}} | \Phi_0 \rangle\)</span></li>
<li>这是 CC 理论要解的薛定谔方程 (<span class="math inline">\(\hat{H}
|\Psi_{CC}\rangle = E |\Psi_{CC}\rangle\)</span>)。</li>
</ul></li>
<li><p><strong>求解方法（投影法）：</strong> 为了解出未知的 <span
class="math inline">\(E\)</span>, <span
class="math inline">\(\hat{T}_1\)</span>, <span
class="math inline">\(\hat{T}_2\)</span>，我们将这个总方程“投影”到不同的激发空间上：</p>
<ol type="1">
<li><strong>能量 <span class="math inline">\(E\)</span>：</strong>
<ul>
<li><span class="math inline">\(\langle \Phi_0 |
e^{-(\hat{T}_1+\hat{T}_2)} \hat{H} e^{(\hat{T}_1+\hat{T}_2)} | \Phi_0
\rangle = E\)</span></li>
<li>将总方程左乘 <span class="math inline">\(\langle \Phi_0
|\)</span>（HF 基态）并积分，可以直接得到总能量 <span
class="math inline">\(E\)</span>。</li>
</ul></li>
<li><strong>求解 <span class="math inline">\(\hat{T}_1\)</span>（方程
①）：</strong>
<ul>
<li><span class="math inline">\(\langle \Phi_i^a |
e^{-(\hat{T}_1+\hat{T}_2)} \hat{H} e^{(\hat{T}_1+\hat{T}_2)} | \Phi_0
\rangle = 0\)</span></li>
<li>将总方程左乘 <span class="math inline">\(\langle \Phi_i^a
|\)</span>（所有<strong>单激发</strong>态）。这会产生一系列方程，求解它们可以得到所有
<span class="math inline">\(\hat{T}_1\)</span> 的振幅 <span
class="math inline">\(t_i^a\)</span>。</li>
</ul></li>
<li><strong>求解 <span class="math inline">\(\hat{T}_2\)</span>（方程
②）：</strong>
<ul>
<li><span class="math inline">\(\langle \Phi_{ij}^{ab} |
e^{-(\hat{T}_1+\hat{T}_2)} \hat{H} e^{(\hat{T}_1+\hat{T}_2)} | \Phi_0
\rangle = 0\)</span></li>
<li>将总方程左乘 <span class="math inline">\(\langle \Phi_{ij}^{ab}
|\)</span>（所有<strong>双激发</strong>态）。这会产生另一系列方程，求解它们可以得到所有
<span class="math inline">\(\hat{T}_2\)</span> 的振幅 <span
class="math inline">\(t_{ij}^{ab}\)</span>。</li>
</ul></li>
</ol></li>
</ul>
<p><strong>总结：</strong> CCSD
是一个复杂的非线性方程组。我们通过求解方程 ① 和 ②
得到所有激发振幅，然后将这些振幅代入能量方程，得到最终的 CCSD 能量。</p>
<h3 id="量子蒙特卡洛-quantum-monte-carlo-qmc">量子蒙特卡洛 (Quantum
Monte Carlo, QMC)</h3>
<p>这部分介绍了一种<strong>完全不同</strong>的、不依赖于轨道和激发的方法。</p>
<ul>
<li><strong>标题：</strong> <code>Quantum Monte Carlo</code></li>
<li><strong>子标题：</strong> <code>Variational / Diffusion MC</code>
(变分蒙特卡洛 / 扩散蒙特卡洛)</li>
</ul>
<h4 id="变分蒙特卡洛-vmc">变分蒙特卡洛 (VMC)</h4>
<p>VMC 的核心思想，基于<strong>变分原理</strong>：</p>
<ol type="1">
<li><strong>能量期望值：</strong>
<ul>
<li><span class="math inline">\(E(\theta) = \frac{\langle \Psi(\theta) |
\hat{H} | \Psi(\theta) \rangle}{\langle \Psi(\theta) | \Psi(\theta)
\rangle}\)</span></li>
<li>任何一个“试验波函数” <span
class="math inline">\(\Psi(\theta)\)</span>（<span
class="math inline">\(\theta\)</span> 是函数中的可调参数）所计算出的能量
<span class="math inline">\(E(\theta)\)</span>
永远<strong>大于或等于</strong>真实的基态能量 <span
class="math inline">\(E_{\text{ground}}\)</span>。</li>
<li><strong>目标：</strong>
<code>min $E(\theta)$ = $E_&#123;\text&#123;ground&#125;&#125;$</code> (更准确地说是 <span
class="math inline">\(E_{\text{approx}}\)</span>)</li>
<li>通过调整参数 <span class="math inline">\(\theta\)</span> 来最小化
<span
class="math inline">\(E(\theta)\)</span>，我们可以获得对基态能量的最佳近似。</li>
</ul></li>
<li><strong>蒙特卡洛方法如何计算？</strong>
<ul>
<li>上述能量公式是一个极其复杂的高维积分（积分维度 = 3 <span
class="math inline">\(\times\)</span> 电子数）。</li>
<li>QMC
它<strong>不直接计算这个积分</strong>，而是使用<strong>随机抽样</strong>（“蒙特卡洛”方法）来估算它。</li>
</ul></li>
<li><strong>VMC 计算步骤（如白板所示）：</strong>
<ul>
<li><strong>a. 重写积分：</strong> <span class="math inline">\(E =
\frac{\int |\Psi(\vec{x}, \theta)|^2 \left[ \frac{\hat{H} \Psi(\vec{x},
\theta)}{\Psi(\vec{x}, \theta)} \right] d\vec{x}}{\int |\Psi(\vec{x},
\theta)|^2 d\vec{x}}\)</span>
<ul>
<li><span class="math inline">\(\vec{x}\)</span> 代表所有电子的坐标
(<span class="math inline">\(r_1, r_2, \dots\)</span>)。</li>
<li><span class="math inline">\(\frac{|\Psi(\vec{x})|^2}{\int
|\Psi(\vec{x})|^2 d\vec{x}}\)</span> 是电子在 <span
class="math inline">\(\vec{x}\)</span>
处被发现的<strong>概率密度</strong> <span
class="math inline">\(P(\vec{x})\)</span>。</li>
<li><span class="math inline">\(E_L(\vec{x}) = \frac{\hat{H}
\Psi(\vec{x}, \theta)}{\Psi(\vec{x}, \theta)}\)</span> 被称为
<strong>“局域能量” (Local Energy)</strong>。</li>
</ul></li>
<li><strong>b. 抽样：</strong>
<ul>
<li>整个积分就变成了在 <span class="math inline">\(P(\vec{x})\)</span>
概率分布下，对 <span class="math inline">\(E_L(\vec{x})\)</span>
求平均值。</li>
<li>VMC 方法通过一种算法（如 Metropolis 算法）产生大量的、符合 <span
class="math inline">\(|\Psi|^2\)</span>
分布的随机电子构型（“walkers”）。</li>
</ul></li>
<li><strong>c. 求平均：</strong>
<ul>
<li><span class="math inline">\(E \approx \frac{1}{N}
\sum_{\text{samples } \vec{x}} E_L(\vec{x})\)</span></li>
<li>最终的能量就是所有采样点上“局域能量”的简单平均值。</li>
</ul></li>
</ul></li>
</ol>
<p><strong>总结：</strong> VMC
通过在“真实空间”中随机移动电子来直接估算能量，完全绕过了 MP 或 CC
理论中复杂的轨道和激发概念。<code>Diffusion MC</code> (DMC)
是其更高级的变体，原则上可以找到精确的基态能量。</p>
<p>这张白板标志着一个<strong>重大的理论转变</strong>：从前面讨论的“波函数方法”（Wavefunction
Methods, 如 MP2,
CCSD）转向了一种完全不同的、在计算化学和物理中占主导地位的方法——<strong>密度泛函理论
(Density Functional Theory, DFT)</strong>。</p>
<h2 id="波函数方法的维度灾难">5. 波函数方法的“维度灾难”</h2>
<p>一个生动的例子，说明了为什么基于波函数的方法（如
CCSD）在计算上极其昂贵，甚至是不可能的。</p>
<ol type="1">
<li><strong>“wavefunction method” (波函数方法):</strong>
<ul>
<li>这类方法（如 HF, MP2, CCSD）的中心目标是求解体系的 <span
class="math inline">\(N\)</span> 电子波函数 <span
class="math inline">\(\Psi(\vec{r}_1, \vec{r}_2, \dots,
\vec{r}_N)\)</span>。</li>
<li>这个波函数 <span class="math inline">\(\Psi\)</span>
是一个极其复杂的对象，它是一个 <span class="math inline">\(3N\)</span>
维度的函数（每个电子有 3 个空间坐标 <span class="math inline">\(x, y,
z\)</span>）。</li>
</ul></li>
<li><strong>计算成本的“立方体”比喻：</strong>
<ul>
<li>假设我们想在一个 3D 空间中存储<em>一个</em>电子的波函数。</li>
<li>如果我们在每个维度（x, y, z）上只使用 <strong>10</strong>
个网格点，我们就需要 <span class="math inline">\(10 \times 10 \times 10
= 10^3\)</span> 个点来描述这<em>一个</em>电子。</li>
<li>现在，考虑一个有 <strong>30</strong>
个电子的体系（一个中等大小的分子）。</li>
<li>由于总波函数 <span class="math inline">\(\Psi\)</span> 是 <span
class="math inline">\(3 \times 30 = 90\)</span> 维的，我们需要 <span
class="math inline">\((10^3)^{30} = 10^{90}\)</span>
个网格点来存储这个波函数。</li>
<li><strong><span class="math inline">\(10^{90}\)</span>
是一个天文数字！</strong> 白板上的 <span
class="math inline">\(10^{77}\)</span> 或 <span
class="math inline">\(10^{80}\)</span>
可能是用来比较的数字（例如，可观测宇宙中的原子总数约 <span
class="math inline">\(10^{80}\)</span>）。这个数字 (<span
class="math inline">\(10^{90}\)</span>) 意味着直接存储或计算 N
电子波函数在计算上是<strong>绝对不可能</strong>的。</li>
<li>这就是所谓的“<strong>维度灾难 (Curse of
Dimensionality)</strong>”。</li>
</ul></li>
<li><strong>DFT：解决方案登场</strong>
<ul>
<li>在指出了波函数方法的根本困难后，白板上写下了
<strong>DFT</strong>，预示着它是一种解决方案。</li>
</ul></li>
<li><strong>体系的哈密顿算符 (<span
class="math inline">\(\hat{H}\)</span>):</strong>
<ul>
<li><span class="math inline">\(\hat{H} = \underbrace{-\sum_i
\frac{\hbar^2}{2m} \nabla_i^2}_{\text{电子动能}} + \underbrace{\sum_i
V_{\text{ext}}(\vec{r}_i)}_{\text{电子-原子核吸引}} +
\underbrace{\frac{1}{2} \sum_{i \neq j} \frac{e^2}{|\vec{r}_i -
\vec{r}_j|}}_{\text{电子-电子排斥}}\)</span></li>
<li>这是任何分子体系的完整（非相对论）哈密顿算符。</li>
</ul></li>
<li><strong>关键的简化：</strong>
<ul>
<li><span class="math inline">\(\langle \Psi | \sum_i
V_{\text{ext}}(\vec{r}_i) | \Psi \rangle = \int V_{\text{ext}}(\vec{r})
\rho(\vec{r}) d\vec{r}\)</span></li>
<li>这一行展示了一个至关重要的简化：<span
class="math inline">\(N\)</span> 电子的“电子-原子核吸引能”的期望值（一个
<span class="math inline">\(3N\)</span>
维积分），可以被<em>精确地</em>重写为一个只涉及<strong>电子密度 <span
class="math inline">\(\rho(\vec{r})\)</span></strong> 的** 3
维积分**。</li>
<li>这就引出了一个问题：我们是否能用这个简单的 <span
class="math inline">\(\rho(\vec{r})\)</span> 来代替 <span
class="math inline">\(\Psi\)</span> 呢？</li>
</ul></li>
</ol>
<h3 id="dft-的理论基石">DFT 的理论基石</h3>
<p>这部分介绍了 DFT 的核心定理</p>
<ol type="1">
<li><strong>电子密度 (Electron Density, <span
class="math inline">\(\rho(\vec{r})\)</span>):</strong>
<ul>
<li><span class="math inline">\(\int \rho(\vec{r}) d\vec{r} =
N\)</span></li>
<li>电子密度 <span class="math inline">\(\rho(\vec{r})\)</span>
是一个<strong>在 3D 空间中的函数</strong>。它描述了在任意空间点 <span
class="math inline">\(\vec{r}\)</span> 处找到一个电子的概率。</li>
<li>无论体系有多少电子（<span class="math inline">\(N=30\)</span> 或
<span class="math inline">\(N=1000\)</span>），<span
class="math inline">\(\rho(\vec{r})\)</span>
<strong>始终</strong>是一个简单的 3 维函数。这与 <span
class="math inline">\(3N\)</span> 维的波函数 <span
class="math inline">\(\Psi\)</span> 形成了鲜明对比。</li>
</ul></li>
<li><strong>Hohenberg-Kohn (H-K) 定理：</strong>
<ul>
<li>这是 DFT 的全部理论基础。白板上的图示完美地总结了第一个 H-K
定理：</li>
<li><strong>标准路径（上 <span
class="math inline">\(\rightarrow\)</span> 下）：</strong> <span
class="math inline">\(V_{\text{ext}}(\vec{r})\)</span>（外势，即原子核的位置和电荷）<span
class="math inline">\(\implies \Psi(\vec{r})\)</span>（基态波函数）<span
class="math inline">\(\implies \rho(\vec{r})\)</span>（基态密度）。
<ul>
<li><em>解释：</em> 原子核的位置决定了 <span
class="math inline">\(\hat{H}\)</span>，求解 <span
class="math inline">\(\hat{H}\)</span> 得到 <span
class="math inline">\(\Psi\)</span>，由 <span
class="math inline">\(\Psi\)</span> 可以计算出 <span
class="math inline">\(\rho\)</span>。</li>
</ul></li>
<li><strong>H-K 革命性路径（下 <span
class="math inline">\(\leftrightarrow\)</span> 上）：</strong> <span
class="math inline">\(V_{\text{ext}}(\vec{r}) \Longleftrightarrow
\rho(\vec{r})\)</span>
<ul>
<li><strong>第一 H-K 定理</strong>证明：体系的<strong>基态电子密度 <span
class="math inline">\(\rho(\vec{r})\)</span></strong>
<em>唯一地</em>决定了<strong>外势 <span
class="math inline">\(V_{\text{ext}}(\vec{r})\)</span></strong>（因此也唯一决定了
<span class="math inline">\(\hat{H}\)</span>、<span
class="math inline">\(\Psi\)</span> 和总能量 <span
class="math inline">\(E\)</span>）。</li>
</ul></li>
</ul></li>
<li><strong>H-K 定理的重大意义：</strong>
<ul>
<li><strong>所有信息都在 <span
class="math inline">\(\rho(\vec{r})\)</span> 中！</strong>
体系的基态总能量 <span class="math inline">\(E\)</span> 是基态密度 <span
class="math inline">\(\rho\)</span> 的一个<strong>泛函
(Functional)</strong>，记为 <span
class="math inline">\(E[\rho]\)</span>。</li>
<li><strong>目标转变：</strong> 我们不再需要去求解那个 <span
class="math inline">\(10^{90}\)</span> 维的波函数 <span
class="math inline">\(\Psi\)</span>！我们只需要找到那个能使总能量 <span
class="math inline">\(E[\rho]\)</span> 最小的 3 维密度 <span
class="math inline">\(\rho(\vec{r})\)</span>。</li>
</ul></li>
</ol>
<p><strong>总结：</strong>
首先论证了“波函数方法”在计算上的不可能性（维度灾难），然后引入了
<strong>DFT</strong> 作为解决方案，其理论依据是 <strong>H-K
定理</strong>——即体系的所有信息都包含在简单的 3 维电子密度 <span
class="math inline">\(\rho(\vec{r})\)</span> 中。</p>
<h2
id="证明-第一-hohenberg-kohn-h-k-定理的经典证明这个定理是密度泛函理论-dft-的基石">6：证明
<strong>第一 Hohenberg-Kohn (H-K)
定理的经典证明</strong>。这个定理是密度泛函理论 (DFT) 的基石。</h2>
<p>这个证明使用的是 <strong>“Proof by contradiction”
(反证法)</strong>。</p>
<p><strong>定理内容：</strong> 体系的基态电子密度 <span
class="math inline">\(\rho_0(\vec{r})\)</span> 唯一地决定了其外势 <span
class="math inline">\(V_{ext}(\vec{r})\)</span>（即原子核的位置和电荷），并因此唯一地决定了体系的哈密顿算符
<span class="math inline">\(\hat{H}\)</span> 和波函数 <span
class="math inline">\(\Psi\)</span>。</p>
<h3 id="证明步骤详解">证明步骤详解</h3>
<h4 id="假设-为反证法设置">1. 假设 (为反证法设置)</h4>
<p>我们假设 H-K 定理是<strong>错</strong>的。</p>
<p>这意味着我们假设存在<strong>两个不同 (different)</strong> 的外势
<span class="math inline">\(V_{ext}^{(1)}\)</span> 和 <span
class="math inline">\(V_{ext}^{(2)}\)</span>，它们分别对应各自的哈密顿算符
<span class="math inline">\(\hat{H}^{(1)}\)</span>、<span
class="math inline">\(\hat{H}^{(2)}\)</span> 和基态波函数 <span
class="math inline">\(\Psi^{(1)}\)</span>、<span
class="math inline">\(\Psi^{(2)}\)</span>。</p>
<p>但我们<strong>假设</strong>，这两个完全不同的体系却碰巧产生了<strong>完全相同
(same)</strong> 的基态电子密度 <span
class="math inline">\(\rho_0(\vec{r})\)</span>。</p>
<blockquote>
<p><strong>假设总结：</strong> * <span
class="math inline">\(V_{ext}^{(1)} \neq V_{ext}^{(2)}\)</span> （因此
<span class="math inline">\(\hat{H}^{(1)} \neq \hat{H}^{(2)}\)</span> 且
<span class="math inline">\(\Psi^{(1)} \neq \Psi^{(2)}\)</span>） * 但是
<span class="math inline">\(\rho^{(1)}(\vec{r}) = \rho^{(2)}(\vec{r}) =
\rho_0(\vec{r})\)</span> * <span class="math inline">\(E^{(1)}\)</span>
是体系1的基态能量。 * <span class="math inline">\(E^{(2)}\)</span>
是体系2的基态能量。</p>
</blockquote>
<h4 id="应用变分原理-第-1-步">2. 应用变分原理 (第 1 步)</h4>
<p>变分原理指出：任何一个“试验波函数” <span
class="math inline">\(\Psi_{\text{trial}}\)</span> 对某个哈密顿算符
<span class="math inline">\(\hat{H}\)</span>
的能量期望值，永远高于或等于该 <span
class="math inline">\(\hat{H}\)</span> 的真实基态能量 <span
class="math inline">\(E_{GS}\)</span>。</p>
<ul>
<li>我们将 <span class="math inline">\(\Psi^{(2)}\)</span>
(体系2的基态波函数) 作为体系1的<strong>试验波函数</strong>。</li>
<li>根据变分原理，<span class="math inline">\(\Psi^{(2)}\)</span>
计算出的 <span class="math inline">\(\hat{H}^{(1)}\)</span>
的能量必定高于 <span class="math inline">\(\hat{H}^{(1)}\)</span>
的真实基态能量 <span
class="math inline">\(E^{(1)}\)</span>（因为我们假设了 <span
class="math inline">\(\Psi^{(1)} \neq \Psi^{(2)}\)</span>）。</li>
<li><strong><span class="math inline">\(E^{(1)} &lt; \langle \Psi^{(2)}
| \hat{H}^{(1)} | \Psi^{(2)} \rangle\)</span></strong>
(白板上的第3行)</li>
</ul>
<h4 id="展开能量-第-2-步">3. 展开能量 (第 2 步)</h4>
<p>我们来展开 <span class="math inline">\(\langle \Psi^{(2)} |
\hat{H}^{(1)} | \Psi^{(2)} \rangle\)</span> 这一项。</p>
<ul>
<li><p>我们知道 <span class="math inline">\(\hat{H}^{(1)} =
\hat{H}^{(2)} + (V_{ext}^{(1)} - V_{ext}^{(2)})\)</span>。</p></li>
<li><p>代入上式： <span class="math inline">\(\langle \Psi^{(2)} |
\hat{H}^{(1)} | \Psi^{(2)} \rangle = \langle \Psi^{(2)} | \hat{H}^{(2)}
+ V_{ext}^{(1)} - V_{ext}^{(2)} | \Psi^{(2)} \rangle\)</span></p></li>
<li><p>拆分它： <span class="math inline">\(= \langle \Psi^{(2)} |
\hat{H}^{(2)} | \Psi^{(2)} \rangle + \langle \Psi^{(2)} | V_{ext}^{(1)}
- V_{ext}^{(2)} | \Psi^{(2)} \rangle\)</span></p></li>
<li><p>第一项 <span class="math inline">\(\langle \Psi^{(2)} |
\hat{H}^{(2)} | \Psi^{(2)} \rangle\)</span> 正是体系2的基态能量 <span
class="math inline">\(E^{(2)}\)</span>。</p></li>
<li><p>第二项（如上一张白板所示）可以写成关于密度的积分： <span
class="math inline">\(\int (V_{ext}^{(1)} - V_{ext}^{(2)})
\rho^{(2)}(\vec{r}) d\vec{r}\)</span></p></li>
<li><p>根据我们的初始假设，<span
class="math inline">\(\rho^{(2)}(\vec{r}) =
\rho_0(\vec{r})\)</span>。</p></li>
<li><p>将这些组合起来，第1步的变分不等式 <span
class="math inline">\(E^{(1)} &lt; \dots\)</span> 就变成了：
<strong><span class="math inline">\(E^{(1)} &lt; E^{(2)} + \int
(V_{ext}^{(1)} - V_{ext}^{(2)}) \rho_0(\vec{r})
d\vec{r}\)</span></strong> (白板上的第5行)</p></li>
</ul>
<h4 id="对称地应用变分原理-第-3-步">4. 对称地应用变分原理 (第 3 步)</h4>
<p>现在我们反过来，将 <span class="math inline">\(\Psi^{(1)}\)</span>
作为体系2的<strong>试验波函数</strong>。</p>
<ul>
<li><p>根据变分原理： <strong><span class="math inline">\(E^{(2)} &lt;
\langle \Psi^{(1)} | \hat{H}^{(2)} | \Psi^{(1)}
\rangle\)</span></strong></p></li>
<li><p>我们用同样的方法展开 <span class="math inline">\(\hat{H}^{(2)} =
\hat{H}^{(1)} + (V_{ext}^{(2)} - V_{ext}^{(1)})\)</span>： <span
class="math inline">\(\langle \Psi^{(1)} | \hat{H}^{(2)} | \Psi^{(1)}
\rangle = \langle \Psi^{(1)} | \hat{H}^{(1)} | \Psi^{(1)} \rangle +
\langle \Psi^{(1)} | V_{ext}^{(2)} - V_{ext}^{(1)} | \Psi^{(1)}
\rangle\)</span></p></li>
<li><p>这等于： <span class="math inline">\(= E^{(1)} + \int
(V_{ext}^{(2)} - V_{ext}^{(1)}) \rho^{(1)}(\vec{r})
d\vec{r}\)</span></p></li>
<li><p>再次使用我们的假设 <span
class="math inline">\(\rho^{(1)}(\vec{r}) =
\rho_0(\vec{r})\)</span>。</p></li>
<li><p>因此，我们得到了第二个不等式： <strong><span
class="math inline">\(E^{(2)} &lt; E^{(1)} + \int (V_{ext}^{(2)} -
V_{ext}^{(1)}) \rho_0(\vec{r}) d\vec{r}\)</span></strong>
(白板上的第6行)</p></li>
</ul>
<h4 id="导出矛盾-第-4-步">5. 导出矛盾 (第 4 步)</h4>
<p>现在我们把两个不等式（第5行和第6行）相加：</p>
<p><span class="math inline">\(E^{(1)} + E^{(2)} &lt; \left[ E^{(2)} +
\int (V_{ext}^{(1)} - V_{ext}^{(2)}) \rho_0 d\vec{r} \right] + \left[
E^{(1)} + \int (V_{ext}^{(2)} - V_{ext}^{(1)}) \rho_0 d\vec{r}
\right]\)</span></p>
<p>我们来合并右侧的项：</p>
<p><span class="math inline">\(E^{(1)} + E^{(2)} &lt; (E^{(1)} +
E^{(2)}) + \int \underbrace{[(V_{ext}^{(1)} - V_{ext}^{(2)}) +
(V_{ext}^{(2)} - V_{ext}^{(1)})]}_{= 0} \rho_0 d\vec{r}\)</span></p>
<p>右侧的两个积分项<strong>完全抵消</strong>，变成了 0。</p>
<ul>
<li>于是我们得到了最终的荒谬结论： <strong><span
class="math inline">\(E^{(1)} + E^{(2)} &lt; E^{(1)} +
E^{(2)}\)</span></strong> (白板上的最后一行)</li>
</ul>
<h4 id="结论">6. 结论</h4>
<p>“一个数严格小于它自己” (<span class="math inline">\(A &lt;
A\)</span>) 是一个数学上<strong>不可能</strong>的悖论。</p>
<p>这个悖论证明了我们的<strong>初始假设一定是错误的</strong>。</p>
<p>因此，两个不同的外势 <span
class="math inline">\(V_{ext}^{(1)}\)</span> 和 <span
class="math inline">\(V_{ext}^{(2)}\)</span> <strong>不可能</strong>
产生相同的基态密度 <span class="math inline">\(\rho_0\)</span>。</p>
<p><strong>证明完毕：</strong> 体系的基态电子密度 <span
class="math inline">\(\rho_0(\vec{r})\)</span>
<strong>唯一地</strong>决定了其外势 <span
class="math inline">\(V_{ext}(\vec{r})\)</span>，并因此决定了体系的所有基态性质。</p>
<h2 id="更多">7. 更多</h2>
<p>Hohenberg-Kohn (H-K) 定理证明了 <span
class="math inline">\(E[\rho]\)</span>（能量是密度的泛函）的<em>存在性</em>，但它没有告诉我们这个泛函到底长什么样。</p>
<p><strong>问题在于：</strong> 我们不知道电子动能 <span
class="math inline">\(T[\rho]\)</span> 和电子-电子排斥能 <span
class="math inline">\(V_{ee}[\rho]\)</span> 的精确泛函形式。</p>
<p><strong>Kohn-Sham (KS) 理论</strong>
就是为了解决这个问题而提出的。</p>
<p>这个理论的核心思想是：<strong>“我们假装在解一个简单问题，然后把所有的复杂性都藏在一个我们去近似的项里。”</strong></p>
<p>以下是这个实践过程的分解：</p>
<h3 id="引入一个虚拟的无相互作用体系">1.
引入一个“虚拟”的无相互作用体系</h3>
<p>Kohn 和 Sham
假设存在一个<em>虚拟的</em>、<strong>无相互作用</strong>（non-interacting）的电子体系。</p>
<p>这个虚拟体系有一个关键的约束：它被设计为与<em>真实的</em>、有相互作用的体系具有<strong>完全相同的基态电子密度
<span class="math inline">\(\rho(\vec{r})\)</span></strong>。</p>
<p><strong>这为什么有帮助？</strong>
因为对于一个<strong>无相互作用</strong>的体系，我们<strong>精确地知道</strong>它的动能泛函
<span class="math inline">\(T_s[\rho]\)</span>！它就是所有单电子轨道
<span class="math inline">\(\phi_i\)</span> 的动能之和。（<span
class="math inline">\(s\)</span> 代表 “single-particle” 或
“non-interacting”）。</p>
<h3 id="重写总能量泛函-erho">2. 重写总能量泛函 <span
class="math inline">\(E[\rho]\)</span></h3>
<p>现在，Kohn-Sham 将<em>真实</em>体系的总能量 <span
class="math inline">\(E[\rho]\)</span> 重新组织为以下四项：</p>
<p><span class="math inline">\(E[\rho] = T_s[\rho] + \int
V_{ext}(\vec{r}) \rho(\vec{r}) d\vec{r} + E_H[\rho] +
E_{xc}[\rho]\)</span></p>
<p>我们来逐项分析：</p>
<ol type="1">
<li><strong><span
class="math inline">\(T_s[\rho]\)</span>：无相互作用动能</strong>
<ul>
<li>这是我们刚刚引入的<em>虚拟</em>体系的动能。我们<strong>可以精确计算</strong>它（通过求解轨道
<span class="math inline">\(\phi_i\)</span>）。</li>
<li><em>（注：这</em>不是<em>真实体系的动能 <span
class="math inline">\(T[\rho]\)</span>，但它通常是 <span
class="math inline">\(T[\rho]\)</span> 的一个很好的近似。）</em></li>
</ul></li>
<li><strong><span class="math inline">\(\int V_{ext}(\vec{r})
\rho(\vec{r}) d\vec{r}\)</span>：外势能</strong>
<ul>
<li>这是电子-原子核的吸引能。这个泛函是<strong>精确已知</strong>的（就像上一张白板展示的）。</li>
</ul></li>
<li><strong><span class="math inline">\(E_H[\rho]\)</span>：哈特里
(Hartree) 能量</strong>
<ul>
<li><span class="math inline">\(E_H[\rho] = \frac{1}{2} \iint
\frac{\rho(\vec{r})\rho(\vec{r}&#39;)}{|\vec{r}-\vec{r}&#39;|}
d\vec{r}d\vec{r}&#39;\)</span></li>
<li>这是电子密度与其自身相互作用的经典库仑排斥能。这个泛函也是<strong>精确已知</strong>的。</li>
</ul></li>
<li><strong><span class="math inline">\(E_{xc}[\rho]\)</span>：交换-相关
(Exchange-Correlation) 泛函</strong>
<ul>
<li><strong>这是 DFT 实践的核心！</strong></li>
<li><span class="math inline">\(E_{xc}[\rho]\)</span>
被定义为一个“垃圾桶”，它包含了所有我们不知道的、以及我们故意用近似替换掉的<em>所有</em>复杂物理：
<ul>
<li>(<span class="math inline">\(T[\rho] -
T_s[\rho]\)</span>)：真实动能与无相互作用动能之间的差值（即动能的相关部分）。</li>
<li>(<span class="math inline">\(V_{ee}[\rho] -
E_H[\rho]\)</span>)：总电子排斥能与经典库仑排斥能之间的差值（即所有非经典的交换效应和相关效应）。</li>
</ul></li>
</ul></li>
</ol>
<h3 id="kohn-sham-方程实践的工具">3. Kohn-Sham 方程：实践的工具</h3>
<p>现在我们有了能量表达式。根据变分原理（Hohenberg-Kohn
第二定理），我们通过最小化 <span class="math inline">\(E[\rho]\)</span>
来寻找基态密度 <span class="math inline">\(\rho(\vec{r})\)</span>。</p>
<p>对这个能量泛函 <span class="math inline">\(E[\rho]\)</span>
应用变分法，最终会得到一组<strong>类似于薛定谔方程</strong>的单电子方程，这就是著名的
<strong>Kohn-Sham (KS) 方程</strong>：</p>
<p><span class="math inline">\(\left( -\frac{\hbar^2}{2m} \nabla^2 +
V_{eff}(\vec{r}) \right) \phi_i(\vec{r}) = \varepsilon_i
\phi_i(\vec{r})\)</span></p>
<ul>
<li><span class="math inline">\(\phi_i(\vec{r})\)</span> 就是“Kohn-Sham
轨道”，电子密度由它们构成：<span class="math inline">\(\rho(\vec{r}) =
\sum_i |\phi_i(\vec{r})|^2\)</span>。</li>
<li><span class="math inline">\(V_{eff}(\vec{r})\)</span>
是一个“有效势”，无相互作用的电子在这个势场中运动： <span
class="math inline">\(V_{eff}(\vec{r}) = V_{ext}(\vec{r}) + V_H(\vec{r})
+ V_{xc}(\vec{r})\)</span>
<ul>
<li><span class="math inline">\(V_{ext}\)</span>：原子核的势。</li>
<li><span class="math inline">\(V_H\)</span>：电子间的经典库仑势（来自
<span class="math inline">\(E_H\)</span>）。</li>
<li><span
class="math inline">\(V_{xc}\)</span>：<strong>交换-相关势</strong>（来自
<span class="math inline">\(E_{xc}\)</span>）。</li>
</ul></li>
</ul>
<h3 id="唯一的近似泛函动物园">4. 唯一的近似：“泛函动物园”</h3>
<p><strong>Kohn-Sham 理论在形式上是精确的。</strong>
如果我们知道了<em>精确的</em> <span
class="math inline">\(E_{xc}[\rho]\)</span>
泛函，我们将得到体系的精确基态能量和密度。</p>
<p><strong>但在实践中，我们不知道精确的 <span
class="math inline">\(E_{xc}[\rho]\)</span>。</strong></p>
<p>因此，<strong>所有实用的 DFT 计算都变成了对 <span
class="math inline">\(E_{xc}[\rho]\)</span> 的近似。</strong></p>
<p>这就是您可能听说过的所有“泛函”（functionals）的来源，它们是对 <span
class="math inline">\(E_{xc}[\rho]\)</span>
的不同近似，构成了所谓的“泛函动物园”(Functional Zoo)：</p>
<ul>
<li><strong>LDA (局域密度近似):</strong> 最简单的近似，只依赖于 <span
class="math inline">\(\rho\)</span>。</li>
<li><strong>GGA (广义梯度近似):</strong> 依赖于 <span
class="math inline">\(\rho\)</span> 和它的梯度 <span
class="math inline">\(\nabla\rho\)</span> (例如 PBE, BLYP)。</li>
<li><strong>Hybrid (杂化) 泛函:</strong> 混合了一部分 Hartree-Fock
的精确交换（例如 B3LYP, PBE0）。</li>
</ul>
<h3 id="总结实践中的-dft">总结：实践中的 DFT</h3>
<ol type="1">
<li><strong>选择一个近似的 <span
class="math inline">\(E_{xc}[\rho]\)</span> 泛函</strong>（例如
B3LYP）。</li>
<li><strong>猜测</strong>一个初始的电子密度 <span
class="math inline">\(\rho_{guess}\)</span>。</li>
<li>根据 <span class="math inline">\(\rho_{guess}\)</span> 计算出有效势
<span class="math inline">\(V_{eff}\)</span>。</li>
<li>求解 Kohn-Sham 方程，得到一组新的轨道 <span
class="math inline">\(\phi_i\)</span>。</li>
<li>根据新的 <span class="math inline">\(\phi_i\)</span>
计算出一个新的电子密度 <span
class="math inline">\(\rho_{new}\)</span>。</li>
<li>比较 <span class="math inline">\(\rho_{new}\)</span> 和 <span
class="math inline">\(\rho_{guess}\)</span>。如果它们足够接近，计算完成（“自洽”）。</li>
<li>如果不同，则混合新旧密度，返回第 3
步，<strong>循环迭代</strong>直到收敛。</li>
</ol>
<p>Kohn-Sham 理论的伟大之处在于，它将一个极其复杂的 <span
class="math inline">\(N\)</span> 电子问题（如 <span
class="math inline">\(10^{90}\)</span>
维度），<strong>在数学上等价地</strong>转化为了一个（原则上精确的）求解
<span class="math inline">\(N\)</span> 个电子在有效势场中运动的 3
维问题。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/13/5054C6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/13/5054C6/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L6</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-10-13 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-13T21:00:00+08:00">2025-10-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-10-20 03:45:06" itemprop="dateModified" datetime="2025-10-20T03:45:06+08:00">2025-10-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>统计机器学习Lecture-6</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1
id="linear-model-selection-and-regularization-线性模型选择与正则化">1.
Linear Model Selection and Regularization 线性模型选择与正则化</h1>
<h2 id="summary-of-core-concepts">Summary of Core Concepts</h2>
<p><strong>Chapter 6: Linear Model Selection and
Regularization</strong>, focusing specifically on <strong>Section 6.1:
Subset Selection</strong>.
<strong>第六章：线性模型选择与正则化</strong>，<strong>6.1节：子集选择</strong></p>
<ul>
<li><p><strong>The Problem:</strong> You have a dataset with many
potential predictor variables (features). If you include all of them
(like <strong>Model 1</strong> with <span
class="math inline">\(p\)</span> predictors in slide
<code>...221320.png</code>), you risk including “noise” variables. These
irrelevant features can decrease model accuracy (overfitting) and make
the model difficult to interpret.
数据集包含许多潜在的预测变量（特征）。如果包含所有这些变量（例如幻灯片“…221320.png”中带有<span
class="math inline">\(p\)</span>个预测变量的<strong>模型1</strong>），则可能会包含“噪声”变量。这些不相关的特征会降低模型的准确率（过拟合），并使模型难以解释。</p></li>
<li><p><strong>The Goal:</strong> Identify a smaller subset of variables
that are truly related to the response. This creates a simpler, more
interpretable, and often more accurate model (like <strong>Model
2</strong> with <span class="math inline">\(q\)</span> predictors).
找出一个与响应真正相关的较小变量子集。这将创建一个更简单、更易于解释且通常更准确的模型（例如带有<span
class="math inline">\(q\)</span>个预测变量的<strong>模型2</strong>）。</p></li>
<li><p><strong>The Main Method Discussed: Best Subset
Selection</strong></p></li>
<li><p><strong>主要讨论的方法：最佳子集选择</strong> This is an
<em>exhaustive search</em> algorithm. It checks <em>every possible
combination</em> of predictors to find the “best” model. With <span
class="math inline">\(p\)</span> variables, this means checking <span
class="math inline">\(2^p\)</span> total models.
这是一种<em>穷举搜索</em>算法。它检查<em>所有可能的预测变量组合</em>，以找到“最佳”模型。对于
<span class="math inline">\(p\)</span> 个变量，这意味着需要检查总共
<span class="math inline">\(2^p\)</span> 个模型。</p>
<p>The algorithm (from slide <code>...221333.png</code>) works in three
steps:</p>
<ol type="1">
<li><p><strong>Step 1:</strong> Fit the “null model” <span
class="math inline">\(M_0\)</span>, which has no predictors (it just
predicts the average of the response). 拟合“空模型”<span
class="math inline">\(M_0\)</span>，它没有预测变量（它只预测响应的平均值）。</p></li>
<li><p><strong>Step 2:</strong> For each <span
class="math inline">\(k\)</span> (from 1 to <span
class="math inline">\(p\)</span>):</p>
<ul>
<li><p>Fit all <span class="math inline">\(\binom{p}{k}\)</span> models
that contain exactly <span class="math inline">\(k\)</span> predictors.
(e.g., fit all models with 1 predictor, then all models with 2
predictors, etc.).</p></li>
<li><p>拟合所有包含 <span class="math inline">\(k\)</span> 个预测变量的
<span class="math inline">\(\binom{p}{k}\)</span>
个模型。（例如，先拟合所有包含 1 个预测变量的模型，然后拟合所有包含 2
个预测变量的模型，等等）。</p></li>
<li><p>From this group, select the single best model <em>for that size
<span class="math inline">\(k\)</span></em>. This “best” model is the
one with the highest <strong><span
class="math inline">\(R^2\)</span></strong> (or lowest
<strong>RSS</strong> - Residual Sum of Squares) on the <em>training
data</em>. Call this model <span
class="math inline">\(M_k\)</span>.</p></li>
<li><p>从这组中，选择 <em>对于该规模 <span
class="math inline">\(k\)</span></em> 的最佳模型。这个“最佳”模型是在
<em>训练数据</em> 上具有最高 <strong><span
class="math inline">\(R^2\)</span></strong>（或最低 <strong>RSS</strong>
- 残差平方和）的模型。将此模型称为 <span
class="math inline">\(M_k\)</span>。</p></li>
</ul></li>
<li><p><strong>Step 3:</strong> You now have <span
class="math inline">\(p+1\)</span> models: <span
class="math inline">\(M_0, M_1, \dots, M_p\)</span>. You must select the
single best one from this list. To do this, you <strong>cannot</strong>
use training <span class="math inline">\(R^2\)</span> (as it will always
pick the biggest model <span class="math inline">\(M_p\)</span>).
Instead, you must use a metric that estimates <em>test error</em>, such
as: <strong>现在你有 <span class="math inline">\(p+1\)</span>
个模型：<span class="math inline">\(M_0, M_1, \dots,
M_p\)</span>。你必须从列表中选择一个最佳模型。为此，你</strong>不能**使用训练
<span class="math inline">\(R^2\)</span>（因为它总是会选择最大的模型
<span
class="math inline">\(M_p\)</span>）。相反，你必须使用一个能够估计<em>测试误差</em>的指标，例如：</p>
<ul>
<li><strong>Cross-Validation (CV) 交叉验证 (CV)</strong> (This is what
the Python code uses)</li>
<li><strong>AIC</strong> (Akaike Information Criterion
赤池信息准则)</li>
<li><strong>BIC</strong> (Bayesian Information Criterion
贝叶斯信息准则)</li>
<li><strong>Adjusted <span class="math inline">\(R^2\)</span> 调整后的
<span class="math inline">\(R^2\)</span></strong></li>
</ul></li>
</ol></li>
<li><p><strong>Key Takeaway:</strong> The slides show this “subset
selection” concept can be applied <em>beyond</em> linear models. The
Python code demonstrates this by applying best subset selection to a
<strong>K-Nearest Neighbors (KNN) Regressor</strong>, a non-linear
model.“子集选择”的概念可以应用于线性模型<em>之外</em>。</p></li>
</ul>
<h2
id="mathematical-understanding-key-questions-数学理解与关键问题">Mathematical
Understanding &amp; Key Questions 数学理解与关键问题</h2>
<p>This section directly answers the questions posed on your slides.</p>
<h3 id="how-to-compare-which-model-is-better">How to compare which model
is better?</h3>
<p>(From slides <code>...221320.png</code> and
<code>...221326.png</code>)</p>
<p>You cannot use <strong>training error</strong> (like <span
class="math inline">\(R^2\)</span> or RSS) to compare models with
<em>different numbers of predictors</em>. A model with more predictors
will almost always have a better <em>training</em> score, even if those
extra predictors are just noise. This is called
<strong>overfitting</strong>. 不能使用<strong>训练误差</strong>（例如
<span class="math inline">\(R^2\)</span> 或
RSS）来比较具有<em>不同数量预测变量</em>的模型。具有更多预测变量的模型几乎总是具有更好的<em>训练</em>分数，即使这些额外的预测变量只是噪声。这被称为<strong>过拟合</strong>。</p>
<p>To compare models of different sizes (like Model 1 vs. Model 2, or
<span class="math inline">\(M_2\)</span> vs. <span
class="math inline">\(M_5\)</span>), you <strong>must</strong> use a
method that estimates <strong>test error</strong> (how the model
performs on new, unseen data). The slides mention:
要比较不同大小的模型（例如模型 1 与模型 2，或 <span
class="math inline">\(M_2\)</span> 与 <span
class="math inline">\(M_5\)</span>），您<strong>必须</strong>使用一种估算<strong>测试误差</strong>（模型在新的、未见过的数据上的表现）的方法。</p>
<ul>
<li><p><strong>Cross-Validation (CV):</strong> This is the gold
standard. You split your data into “folds,” train the model on some
folds, and test it on the remaining fold. You repeat this and average
the test scores. The model with the best (e.g., lowest) average CV error
is chosen.
将数据分成“折叠”，在一些折叠上训练模型，然后在剩余的折叠上测试模型。重复此操作并取测试分数的平均值。选择平均
CV 误差最小（例如，最小）的模型。</p></li>
<li><p><strong>AIC &amp; BIC:</strong> These are mathematical
adjustments to the training error (like RSS) that add a <em>penalty</em>
for having more predictors. They balance model <em>fit</em> with model
<em>complexity</em>. 这些是对训练误差（如
RSS）的数学调整，会因预测变量较多而增加<em>惩罚</em>。它们平衡了模型<em>拟合度</em>和模型<em>复杂度</em>。</p></li>
</ul>
<h3 id="why-use-r2-in-step-2">Why use <span
class="math inline">\(R^2\)</span> in Step 2?</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p>In Step 2, you are only comparing models <strong>of the same
size</strong> (i.e., all models that have exactly <span
class="math inline">\(k\)</span> predictors). For models with the same
number of parameters, a higher <span class="math inline">\(R^2\)</span>
(or lower RSS) on the training data directly corresponds to a better
fit. You don’t need to penalize for complexity because all models being
compared <em>have the same complexity</em>.
只比较<strong>大小相同</strong>的模型（即所有恰好具有 <span
class="math inline">\(k\)</span>
个预测变量的模型）。对于参数数量相同的模型，训练数据上更高的 <span
class="math inline">\(R^2\)</span>（或更低的
RSS）直接对应着更好的拟合度。您不需要对复杂度进行惩罚，因为所有被比较的模型<em>都具有相同的复杂度</em>。</p>
<h3 id="why-cant-we-use-training-error-in-step-3">Why can’t we use
training error in Step 3?</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p>In Step 3, you are comparing models <strong>of different
sizes</strong> (<span class="math inline">\(M_0\)</span> vs. <span
class="math inline">\(M_1\)</span> vs. <span
class="math inline">\(M_2\)</span>, etc.). As you add predictors, the
training <span class="math inline">\(R^2\)</span> will <em>always</em>
go up (or stay the same), and the training RSS will <em>always</em> go
down (or stay the same). If you used <span
class="math inline">\(R^2\)</span> to pick the best model in Step 3, you
would <em>always</em> pick the most complex model <span
class="math inline">\(M_p\)</span>, which is almost certainly overfit.
将比较<strong>不同大小</strong>的模型（例如 <span
class="math inline">\(M_0\)</span> vs. <span
class="math inline">\(M_1\)</span> vs. <span
class="math inline">\(M_2\)</span> 等）。随着您添加预测变量，训练 <span
class="math inline">\(R^2\)</span>
将<em>始终</em>上升（或保持不变），而训练 RSS
将<em>始终</em>下降（或保持不变）。如果您在步骤 3 中使用 <span
class="math inline">\(R^2\)</span>
来选择最佳模型，那么您<em>始终</em>会选择最复杂的模型 <span
class="math inline">\(M_p\)</span>，而该模型几乎肯定会过拟合。</p>
<p>Therefore, you <em>must</em> use a metric that estimates test error
(like CV) or penalizes for complexity (like AIC, BIC, or Adjusted <span
class="math inline">\(R^2\)</span>) to find the right balance between
fit and simplicity. 因此，您<em>必须</em>使用一个可以估算测试误差（例如
CV）或惩罚复杂度（例如 AIC、BIC 或调整后的 <span
class="math inline">\(R^2\)</span>）的指标来找到拟合度和简单性之间的平衡。</p>
<h2 id="code-analysis">Code Analysis</h2>
<p>The Python code (slides <code>...221249.jpg</code> and
<code>...221303.jpg</code>) implements the <strong>Best Subset
Selection</strong> algorithm using <strong>KNN Regression</strong>.</p>
<h3 id="key-functions">Key Functions</h3>
<ul>
<li><code>main()</code>:
<ol type="1">
<li><strong>Loads Data:</strong> Reads the <code>Credit.csv</code>
file.</li>
<li><strong>Preprocesses Data:</strong>
<ul>
<li>Converts categorical features (‘Gender’, ‘Student’, ‘Married’,
‘Ethnicity’) into numerical ones (dummy variables).
将分类特征（“性别”、“学生”、“已婚”、“种族”）转换为数值特征（虚拟变量）。</li>
<li>Creates the feature matrix <code>X</code> and target variable
<code>y</code> (‘Balance’). 创建特征矩阵 <code>X</code> 和目标变量
<code>y</code>（“余额”）。</li>
<li><strong>Scales</strong> the features using
<code>StandardScaler</code>. This is crucial for KNN, which is sensitive
to the scale of features. 用 <code>StandardScaler</code>
对特征进行<strong>缩放</strong>。这对于 KNN
至关重要，因为它对特征的缩放非常敏感。</li>
</ul></li>
<li><strong>Adds Noise (in the second example):</strong> Slide
<code>...221303.jpg</code> shows code that <em>adds 20 new “noisy”
columns</em> to the data. This is to test if the selection algorithm is
smart enough to ignore them. 向数据中添加 20
个新的“噪声”列的代码。这是为了测试选择算法是否足够智能，能够忽略它们。</li>
<li><strong>Runs Selection:</strong> Calls
<code>best_subset_selection_parallel</code> to do the main work.</li>
<li><strong>Prints Results:</strong> Finds the best subset (lowest
error) and prints the top 20 best-performing subsets.
找到最佳子集（误差最小），并打印出表现最佳的前 20 个子集。</li>
<li><strong>Final Evaluation:</strong> It re-trains a KNN model on
<em>only</em> the best subset and calculates the final cross-validated
RMSE. 仅基于最佳子集重新训练 KNN 模型，并计算最终的交叉验证 RMSE。</li>
</ol></li>
<li><code>evaluate_subset(subset, ...)</code>:
<ul>
<li>This is the “worker” function. It’s called for <em>every single</em>
possible subset.</li>
<li>It takes a <code>subset</code> (a list of feature names, e.g.,
<code>['Income', 'Limit']</code>).</li>
<li>It creates a new <code>X_subset</code> containing <em>only</em>
those columns.</li>
<li>It runs 5-fold cross-validation (<code>cross_val_score</code>) on a
KNN model using this <code>X_subset</code>.</li>
<li>It uses <code>'neg_mean_squared_error'</code> as the metric. This is
negative MSE; a <em>higher</em> score (closer to 0) is better.
它会创建一个新的“X_subset”<em>，仅包含这些列。 它会使用此“X_subset”在
KNN 模型上运行 5 倍交叉验证（“cross_val_score”）。
它使用“neg_mean_squared_error”作为度量标准。这是负
MSE；</em>更高*的分数（越接近 0）越好。</li>
<li>It returns the subset and its average CV score.</li>
</ul></li>
<li><code>best_subset_selection_parallel(model, ...)</code>:
<ul>
<li>This is the “manager” function.这是“管理器”函数。</li>
<li>It iterates from <code>k=1</code> up to the total number of
features.它从“k=1”迭代到特征总数。</li>
<li>For each <code>k</code>, it generates <em>all combinations</em> of
features of that size (this is the <span
class="math inline">\(\binom{p}{k}\)</span> part).
对于每个“k”，它会生成该大小的特征的<em>所有组合</em>（这是 <span
class="math inline">\(\binom{p}{k}\)</span> 部分）。</li>
<li>It uses <code>Parallel</code> and <code>delayed</code> (from
<code>joblib</code>) to run <code>evaluate_subset</code> for all these
combinations <em>in parallel</em>, speeding up the process
significantly. 它使用 <code>Parallel</code> 和
<code>delayed</code>（来自
<code>joblib</code>）对所有这些组合<em>并行</em>运行
<code>evaluate_subset</code>，从而显著加快了处理速度。</li>
<li>It collects all the results and returns
them.它收集所有结果并返回。</li>
</ul></li>
</ul>
<h3 id="analysis-of-the-output">Analysis of the Output</h3>
<ul>
<li><strong>Slide <code>...221255.png</code> (Original Data):</strong>
<ul>
<li>The code runs subset selection on the original dataset.</li>
<li>The “Top 20 Best Feature Subsets” are shown. The CV scores are
negative (they are <code>neg_mean_squared_error</code>), so the scores
<em>closest to zero</em> (smallest magnitude) are best.</li>
<li>The <strong>Best feature subset</strong> is found to be
<code>('Income', 'Limit', 'Rating', 'Student')</code>.</li>
<li>The final cross-validated RMSE for this model is
<strong>105.41</strong>.</li>
</ul></li>
<li><strong>Slide <code>...221309.png</code> (Data with 20 Noisy
Variables):</strong>
<ul>
<li>The code is re-run after adding 20 useless “Noisy” features.</li>
<li>The algorithm <em>still</em> works. It correctly identifies that the
“Noisy” variables are useless.</li>
<li>The <strong>Best feature subset</strong> is now
<code>('Income', 'Limit', 'Student')</code>. (Note: ‘Rating’ was
dropped, likely because it’s highly correlated with ‘Limit’, and the
noisy data made the simpler model perform slightly better in CV).</li>
<li>The final RMSE is <strong>114.94</strong>. This is <em>higher</em>
than the original 105.41, which is expected—the presence of so many
noise variables makes the selection problem harder, but the final model
is still good and, most importantly, <em>it successfully excluded all 20
noisy features</em>. 最终的 RMSE 为 <strong>114.94</strong>。这比最初的
105.41<em>更高</em>，这是预期的——如此多的噪声变量的存在使得选择问题更加困难，但最终模型仍然很好，最重要的是，<em>它成功地排除了所有
20 个噪声特征</em>。</li>
</ul></li>
</ul>
<h2 id="conceptual-overview-the-why">Conceptual Overview: The “Why”</h2>
<p>Slides cover <strong>Chapter 6: Linear Model Selection and
Regularization</strong>, which is all about a fundamental trade-off in
machine learning: the <strong>bias-variance trade-off</strong>.
该部分主要讨论机器学习中的一个基本权衡：<strong>偏差-方差权衡</strong>。</p>
<ul>
<li><p><strong>The Problem (Slide <code>...221320.png</code>):</strong>
Imagine you have a dataset with 50 predictors (<span
class="math inline">\(p=50\)</span>). You want to predict a response
<span class="math inline">\(y\)</span>. 假设你有一个包含 50
个预测变量（p=50）的数据集。你想要预测响应 <span
class="math inline">\(y\)</span>。</p>
<ul>
<li><strong>Model 1 (Full Model):</strong> You use all 50 predictors.
This model is very <strong>flexible</strong>. It will fit the
<em>training data</em> extremely well, resulting in a low
<strong>bias</strong>. However, it’s highly likely that many of those 50
predictors are just “noise” (random, unrelated variables). By fitting to
this noise, the model will be <strong>overfit</strong>. When you show it
new, unseen data (the <em>test data</em>), it will perform poorly. This
is called <strong>high variance</strong>. 你使用了所有 50
个预测变量。这个模型非常<strong>灵活</strong>。它能很好地拟合<em>训练数据</em>，从而产生较低的<strong>偏差</strong>。然而，这
50
个预测变量中很可能有很多只是“噪声”（随机的、不相关的变量）。由于拟合这些噪声，模型会<strong>过拟合</strong>。当你向它展示新的、未见过的数据（<em>测试数据</em>）时，它的表现会很差。这被称为<strong>高方差</strong>。</li>
<li><strong>Model 2 (Subset Model):</strong> You intelligently select
only the 3 predictors (<span class="math inline">\(q=3\)</span>) that
are <em>actually</em> related to <span class="math inline">\(y\)</span>.
This model is less flexible. It won’t fit the <em>training data</em> as
perfectly as Model 1 (it has higher <strong>bias</strong>). But, because
it’s <em>not</em> fitting the noise, it will generalize much better to
new data. It will have a much lower <strong>variance</strong>, and thus
a lower overall <em>test error</em>. 你智能地只选择与 <span
class="math inline">\(y\)</span> <em>真正</em>相关的 3 个预测变量 (<span
class="math inline">\(q=3\)</span>)。这个模型的灵活性较差。它对
<em>训练数据</em> 的拟合度不如模型 1
完美（它的<strong>偏差</strong>更高）。但是，由于它对噪声的拟合度更高，因此对新数据的泛化能力会更好。它的<strong>方差</strong>会更低，因此总体的<em>测试误差</em>也会更低。</li>
</ul></li>
<li><p><strong>The Goal:</strong> The goal is to find the model that has
the <strong>lowest test error</strong>. We need a formal method to
<em>find</em> the best subset (like Model 2) without just guessing.
<strong>目标是找到</strong>测试误差**最低的模型。我们需要一个正式的方法来<em>找到</em>最佳子集（例如模型
2），而不是仅仅靠猜测。</p></li>
<li><p><strong>Two Main Strategies (Slide
<code>...221314.png</code>):</strong></p>
<ol type="1">
<li><p><strong>Subset Selection (Section 6.1):</strong> This is what
we’re focused on. It’s an “all-or-nothing” approach. You either
<em>keep</em> a variable in the model or you <em>discard</em> it
completely. The “Best Subset Selection” algorithm is the most extreme,
“brute-force” way to do this.
是我们关注的重点。这是一种“全有或全无”的方法。你要么在模型中“保留”一个变量，要么“彻底丢弃”它。“最佳子集选择”算法是最极端、最“暴力”的做法。</p></li>
<li><p><strong>Shrinkage/Regularization (Section 6.2):</strong> This is
a more subtle approach (e.g., Ridge Regression, LASSO). Instead of
discarding variables, you <em>keep all <span
class="math inline">\(p\)</span> variables</em> but add a penalty to the
model that “shrinks” the coefficients (<span
class="math inline">\(\beta\)</span>) of the useless variables towards
zero.
这是一种更巧妙的方法（例如，岭回归、LASSO）。你不是丢弃变量，而是<em>保留所有
<span class="math inline">\(p\)</span>
个变量</em>，但会给模型添加一个惩罚项，将无用变量的系数（<span
class="math inline">\(\beta\)</span>）“收缩”到零。</p></li>
</ol></li>
</ul>
<h2 id="questions">Questions 🎯</h2>
<h3 id="q1-how-to-compare-which-model-is-better">Q1: “How to compare
which model is better?”</h3>
<p>(From slides <code>...221320.png</code> and
<code>...221326.png</code>)</p>
<p>This is the most important question. You <strong>cannot</strong> use
metrics based on <em>training data</em> (like <span
class="math inline">\(R^2\)</span> or RSS - Residual Sum of Squares) to
compare models with <em>different numbers of predictors</em>.
这是最重要的问题。您<strong>不能</strong>使用基于<em>训练数据</em>的指标（例如
R^2 或 RSS - 残差平方和）来比较具有<em>不同数量预测变量</em>的模型。</p>
<ul>
<li><p><strong>The Trap:</strong> A model with more predictors will
<em>always</em> have a higher <span class="math inline">\(R^2\)</span>
(or lower RSS) on the data it was trained on. <span
class="math inline">\(R^2\)</span> will <em>always</em> increase as you
add variables, even if they are pure noise. If you used <span
class="math inline">\(R^2\)</span> to compare a 3-predictor model to a
10-predictor model, the 10-predictor model would <em>always</em> look
better on paper, even if it’s terribly overfit.
具有更多预测变量的模型在其训练数据上<em>总是</em>具有更高的
R^2（或更低的 RSS）。随着变量的增加，R^2
会<em>总是</em>增加，即使这些变量是纯噪声。如果您使用 R^2 来比较 3
个预测变量的模型和 10 个预测变量的模型，那么 10
个预测变量的模型在纸面上<em>总是</em>看起来更好，即使它严重过拟合。</p></li>
<li><p><strong>The Correct Way:</strong> You must use a metric that
estimates the <strong>test error</strong>. The slides and code show two
ways:您必须使用一个能够估计<strong>测试误差</strong>的指标。</p>
<ol type="1">
<li><strong>Cross-Validation (CV):</strong> This is the method used in
your Python code. It works by:
<ul>
<li>Splitting your training data into <span
class="math inline">\(k\)</span> “folds” (e.g., 5 folds).
将训练数据拆分成 <span class="math inline">\(k\)</span> 个“折叠”（例如 5
个折叠）。</li>
<li>Training the model on 4 folds and testing it on the 5th fold.
使用其中 4 个折叠训练模型，并使用第 5 个折叠进行测试。</li>
<li>Repeating this 5 times, so each fold gets to be the test set once.
重复此操作 5 次，使每个折叠都作为测试集一次。</li>
<li>Averaging the 5 test errors. 对 5 个测试误差求平均值。 This gives
you a robust estimate of how your model will perform on <em>unseen
data</em>. You then choose the model with the best (lowest) average CV
error.
这可以让你对模型在<em>未见数据</em>上的表现有一个稳健的估计。然后，你可以选择平均
CV 误差最小（最佳）的模型。</li>
</ul></li>
<li><strong>Mathematical Adjustments (AIC, BIC, Adjusted <span
class="math inline">\(R^2\)</span>):</strong> These are formulas that
take the training error (like RSS) and add a <em>penalty</em> for each
predictor (<span class="math inline">\(k\)</span>) you add.
<ul>
<li><span class="math inline">\(AIC \approx RSS +
2k\sigma^2\)</span></li>
<li><span class="math inline">\(BIC \approx RSS +
\log(n)k\sigma^2\)</span> A model with more predictors (larger <span
class="math inline">\(k\)</span>) gets a bigger penalty. To be chosen, a
more complex model must <em>significantly</em> improve the RSS to
overcome this penalty. 预测变量越多（k
越大）的模型，惩罚越大。要被选中，更复杂的模型必须<em>显著</em>提升 RSS
以克服此惩罚。</li>
</ul></li>
</ol></li>
</ul>
<h3 id="q2-why-using-r2-for-step-2">Q2: “Why using <span
class="math inline">\(R^2\)</span> for step 2?”</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p><strong>Step 2</strong> of the “Best Subset Selection” algorithm
says: “For <span class="math inline">\(k = 1, \dots, p\)</span>: Fit all
<span class="math inline">\(\binom{p}{k}\)</span> models… Pick the best
model, that with the largest <span class="math inline">\(R^2\)</span>, …
and call it <span class="math inline">\(M_k\)</span>.” “对于 <span
class="math inline">\(k = 1, \dots, p\)</span>：拟合所有 <span
class="math inline">\(\binom{p}{k}\)</span> 个模型……选择具有最大 <span
class="math inline">\(R^2\)</span> 的最佳模型……并将其命名为 <span
class="math inline">\(M_k\)</span>。”</p>
<ul>
<li><strong>The Reason:</strong> In Step 2, you are <em>only</em>
comparing models <strong>of the same size</strong>. For example, when
<span class="math inline">\(k=3\)</span>, you are comparing all possible
3-predictor models: 步骤 2
中，您<em>仅</em>比较**相同大小的模型。例如，当 <span
class="math inline">\(k=3\)</span> 时，您将比较所有可能的 3
预测变量模型：
<ul>
<li>Model A: (<span class="math inline">\(X_1, X_2, X_3\)</span>)</li>
<li>Model B: (<span class="math inline">\(X_1, X_2, X_4\)</span>)</li>
<li>Model C: (<span class="math inline">\(X_1, X_3, X_5\)</span>)</li>
<li>…and so on.</li>
</ul>
Since all these models have the <em>exact same complexity</em> (they all
have <span class="math inline">\(k=3\)</span> predictors), there is no
risk of unfairly favoring a more complex model. Therefore, you are free
to use a training metric like <span class="math inline">\(R^2\)</span>
(or RSS). The model with the highest <span
class="math inline">\(R^2\)</span> is, by definition, the one that
<em>best fits the training data</em> for that specific size <span
class="math inline">\(k\)</span>.
由于所有这些模型都具有<em>完全相同的复杂度</em>（它们都具有 <span
class="math inline">\(k=3\)</span>
个预测变量），因此不存在不公平地偏向更复杂模型的风险。因此，您可以自由使用像
<span class="math inline">\(R^2\)</span>（或
RSS）这样的训练指标。根据定义，具有最高 <span
class="math inline">\(R^2\)</span> 的模型就是在特定大小 <span
class="math inline">\(k\)</span>
下<em>与训练数据拟合度</em>最高的模型。</li>
</ul>
<h3
id="q3-cannot-use-training-error-in-step-3.-why-not-步骤-3-中不能使用训练误差-为什么">Q3:
“Cannot use training error in Step 3.” Why not? “步骤 3
中不能使用训练误差。” 为什么？</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p><strong>Step 3</strong> says: “Select a single best model from <span
class="math inline">\(M_0, M_1, \dots, M_p\)</span> by cross validation,
AIC, or BIC.”“通过交叉验证、AIC 或 BIC，从 <span
class="math inline">\(M_0、M_1、\dots、M_p\)</span>
中选择一个最佳模型。”</p>
<ul>
<li><p><strong>The Reason:</strong> In Step 3, you are now comparing
models <strong>of different sizes</strong>. You are comparing the best
1-predictor model (<span class="math inline">\(M_1\)</span>) vs. the
best 2-predictor model (<span class="math inline">\(M_2\)</span>)
vs. the best 3-predictor model (<span
class="math inline">\(M_3\)</span>), and so on, all the way up to <span
class="math inline">\(M_p\)</span>. 在步骤 3
中，您正在比较<strong>不同大小</strong>的模型。您正在比较最佳的单预测模型
(<span class="math inline">\(M_1\)</span>)、最佳的双预测模型 (<span
class="math inline">\(M_2\)</span>) 和最佳的三预测模型 (<span
class="math inline">\(M_3\)</span>)，依此类推，直到 <span
class="math inline">\(M_p\)</span>。</p>
<p>As explained in Q1, if you used a training error metric like <span
class="math inline">\(R^2\)</span> here, the <span
class="math inline">\(R^2\)</span> would just keep going up, and you
would <em>always</em> select the largest, most complex model, <span
class="math inline">\(M_p\)</span>. This completely defeats the purpose
of model selection. 如问题 1 所述，如果您在此处使用像 <span
class="math inline">\(R^2\)</span> 这样的训练误差指标，那么 <span
class="math inline">\(R^2\)</span>
会持续上升，并且您<em>总是</em>会选择最大、最复杂的模型 <span
class="math inline">\(M_p\)</span>。这完全违背了模型选择的目的。</p>
<p>Therefore, in Step 3, you <em>must</em> use a method that estimates
<strong>test error</strong> (like Cross-Validation) or one that
<strong>penalizes for complexity</strong> (like AIC or BIC) to find the
“sweet spot” model that balances fit and simplicity. 因此，在步骤 3
中，您<em>必须</em>使用一种估算<strong>测试误差</strong>的方法（例如交叉验证）或<strong>惩罚复杂性</strong>的方法（例如
AIC 或
BIC），以找到在拟合度和简单性之间取得平衡的“最佳点”模型。</p></li>
</ul>
<h2 id="mathematical-deep-dive">Mathematical Deep Dive 🧮</h2>
<ul>
<li><strong><span class="math inline">\(Y = \beta_0 + \beta_1X_1 + \dots
+ \beta_pX_p + \epsilon\)</span>:</strong> The full linear model. The
goal of subset selection is to find a subset of <span
class="math inline">\(X_j\)</span>’s where <span
class="math inline">\(\beta_j \neq 0\)</span> and set all other <span
class="math inline">\(\beta\)</span>’s to 0.
完整的线性模型。子集选择的目标是找到 <span
class="math inline">\(X_j\)</span> 的一个子集，其中 $_j 等于
0，并将所有其他 <span class="math inline">\(\beta\)</span> 设置为
0。</li>
<li><strong><span class="math inline">\(2^p\)</span>
combinations:</strong> (Slide <code>...221333.png</code>) This is the
total number of models you have to check. For each of the <span
class="math inline">\(p\)</span> variables, you have two choices: either
it is <strong>IN</strong> the model or it is
<strong>OUT</strong>.这是你需要检查的模型总数。对于每个 <span
class="math inline">\(p\)</span>
个变量，你有两个选择：要么它在模型<strong>内部</strong>，要么它在模型<strong>外部</strong>。
<ul>
<li>Example: <span class="math inline">\(p=3\)</span> (variables <span
class="math inline">\(X_1, X_2, X_3\)</span>)</li>
<li>The <span class="math inline">\(2^3 = 8\)</span> possible models
are:
<ol type="1">
<li>{} (The null model, <span class="math inline">\(M_0\)</span>)</li>
<li>{ <span class="math inline">\(X_1\)</span> }</li>
<li>{ <span class="math inline">\(X_2\)</span> }</li>
<li>{ <span class="math inline">\(X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_2\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_2, X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_2, X_3\)</span> } (The full
model, <span class="math inline">\(M_3\)</span>)</li>
</ol></li>
<li>This is why this method is called an <strong>“exhaustive
search”</strong>. It literally checks every single one. For <span
class="math inline">\(p=20\)</span>, <span
class="math inline">\(2^{20}\)</span> is over a million
models!这就是该方法被称为<strong>“穷举搜索”</strong>的原因。它实际上会检查每一个模型。对于
<span class="math inline">\(p=20\)</span>，<span
class="math inline">\(2^{20}\)</span> 就超过一百万个模型！</li>
</ul></li>
<li><strong><span class="math inline">\(\binom{p}{k} =
\frac{p!}{k!(p-k)!}\)</span>:</strong> (Slide
<code>...221333.png</code>) This is the “combinations” formula. It tells
you <em>how many</em> models you fit <em>in Step 2</em> for a specific
<span
class="math inline">\(k\)</span>.这是“组合”公式。它告诉你，对于特定的
<span class="math inline">\(k\)</span>，<em>在步骤 2</em>中，你拟合了
<em>多少</em> 个模型。
<ul>
<li>Example: <span class="math inline">\(p=10\)</span> total
predictors.</li>
<li>For <span class="math inline">\(k=1\)</span>: You fit <span
class="math inline">\(\binom{10}{1} = 10\)</span> models.</li>
<li>For <span class="math inline">\(k=2\)</span>: You fit <span
class="math inline">\(\binom{10}{2} = \frac{10 \times 9}{2 \times 1} =
45\)</span> models.</li>
<li>For <span class="math inline">\(k=3\)</span>: You fit <span
class="math inline">\(\binom{10}{3} = \frac{10 \times 9 \times 8}{3
\times 2 \times 1} = 120\)</span> models.</li>
<li>…and so on. The sum of all these <span
class="math inline">\(\binom{p}{k}\)</span> from <span
class="math inline">\(k=0\)</span> to <span
class="math inline">\(k=p\)</span> equals <span
class="math inline">\(2^p\)</span>.</li>
</ul></li>
</ul>
<h2 id="detailed-code-analysis">Detailed Code Analysis 💻</h2>
<p>Your slides show Python code that applies the <strong>Best Subset
Selection algorithm</strong> to a <strong>KNN Regressor</strong>. This
is a great example of how the <em>selection algorithm</em> is
independent of the <em>model type</em> (as mentioned in slide
<code>...221314.png</code>).</p>
<h3 id="key-functions-1">Key Functions</h3>
<ul>
<li><strong><code>main()</code></strong>
<ol type="1">
<li><strong>Load &amp; Preprocess:</strong> Reads
<code>Credit.csv</code>. The most important step here is converting
categorical text (like ‘Male’/‘Female’) into numbers (1/0).</li>
<li><strong>Scale Data:</strong> <code>scaler = StandardScaler()</code>
and <code>X_scaled = scaler.fit_transform(X)</code>.
<ul>
<li><strong>WHY?</strong> This is <strong>CRITICAL</strong> for KNN. KNN
works by measuring distance. If ‘Income’ (e.g., 50,000) is on a vastly
different scale than ‘Cards’ (e.g., 3), the ‘Income’ feature will
completely dominate the distance calculation, making ‘Cards’ irrelevant.
Scaling resizes all features to have a mean of 0 and standard deviation
of 1, so they all contribute fairly.</li>
</ul></li>
<li><strong>Handle Noisy Data (Slide
<code>...221303.jpg</code>):</strong> This version of the code
<em>intentionally</em> adds 20 columns of useless, random numbers. This
is a test to see if the algorithm is smart enough to ignore them.</li>
<li><strong>Run Selection:</strong>
<code>results_df = best_subset_selection_parallel(...)</code>. This
function does all the heavy lifting (explained next).</li>
<li><strong>Find Best Model:</strong>
<code>results_df.sort_values(by='CV_Score', ascending=False)</code>.
<ul>
<li><strong>WHY <code>ascending=False</code>?</strong> The code uses the
metric <code>'neg_mean_squared_error'</code>. This is MSE, but negative
(e.g., -15000). A <em>better</em> model has an error closer to 0 (e.g.,
-10000). Since -10000 is <em>greater than</em> -15000, you sort in
descending (high-to-low) order to put the best models at the top.</li>
</ul></li>
<li><strong>Final Evaluation (Step 3):</strong>
<code>final_scores = cross_val_score(knn, X_best, y, ...)</code>
<ul>
<li>This is the implementation of Step 3. It takes <em>only</em> the
single best subset (<code>X_best</code>) and runs a <em>new</em>
cross-validation on it. This gives a final, unbiased estimate of how
good that one model is.</li>
</ul></li>
<li><strong>Print RMSE:</strong>
<code>final_rmse = np.sqrt(-final_scores)</code>. It converts the
negative MSE back into a positive RMSE (Root Mean Squared Error), which
is in the same units as the target <span
class="math inline">\(y\)</span> (in this case, ‘Balance’ in
dollars).</li>
</ol></li>
<li><strong><code>best_subset_selection_parallel(model, ...)</code></strong>
<ol type="1">
<li>This is the “manager” function. It implements the loop from Step
2.</li>
<li><code>for k in range(1, n_features + 1):</code> This is the loop
“For <span class="math inline">\(k = 1, \dots, p\)</span>”.</li>
<li><code>subsets = list(combinations(feature_names, k))</code>: This
generates the <span class="math inline">\(\binom{p}{k}\)</span>
combinations for the current <span
class="math inline">\(k\)</span>.</li>
<li><code>results = Parallel(n_jobs=n_jobs)(...)</code>: This is a
non-core, “speed-up” command. It uses the <code>joblib</code> library to
run the evaluations on all your computer’s CPU cores at once (in
parallel). Without this, checking millions of models would take
days.</li>
<li><code>subset_scores = ... [delayed(evaluate_subset)(...) ...]</code>
This line farms out the <em>actual work</em> to the
<code>evaluate_subset</code> function for every single subset.</li>
</ol></li>
<li><strong><code>evaluate_subset(subset, ...)</code></strong>
<ol type="1">
<li>This is the “worker” function. It gets called thousands or millions
of times.</li>
<li>Its job is to evaluate <em>one single subset</em> (e.g.,
<code>('Income', 'Limit', 'Student')</code>).</li>
<li><code>X_subset = X[list(subset)]</code>: It slices the data to get
<em>only</em> these columns.</li>
<li><code>scores = cross_val_score(model, X_subset, ...)</code>:
<strong>This is the most important line.</strong> It takes the subset
and performs a full 5-fold cross-validation on it.</li>
<li><code>return (subset, np.mean(scores))</code>: It returns the subset
and its average CV score.</li>
</ol></li>
</ul>
<h3 id="summary-of-outputs-slides-...221255.png-...221309.png">Summary
of Outputs (Slides <code>...221255.png</code> &amp;
<code>...221309.png</code>)</h3>
<ul>
<li><strong>Original Data (Slide <code>...221255.png</code>):</strong>
<ul>
<li><strong>Best Subset:</strong>
<code>('Income', 'Limit', 'Rating', 'Student')</code></li>
<li><strong>Final RMSE:</strong> ~105.4</li>
</ul></li>
<li><strong>Data with 20 “Noisy” Variables (Slide
<code>...221309.png</code>):</strong>
<ul>
<li><strong>Best Subset:</strong>
<code>('Income', 'Limit', 'Student')</code></li>
<li><strong>Result:</strong> The algorithm <em>successfully</em>
identified that all 20 “Noisy” variables were useless and
<strong>excluded every single one of them</strong> from the best
models.</li>
<li><strong>Final RMSE:</strong> ~114.9</li>
<li><strong>Key Takeaway:</strong> The RMSE is slightly higher, which
makes sense because the selection problem was much harder. But the
<em>method worked perfectly</em>. It filtered all the “noise” and found
a simple, powerful model, just as the theory on slide
<code>...221320.png</code> predicted.</li>
</ul></li>
</ul>
<h1
id="the-core-problem-training-error-vs.-test-error-核心问题训练误差-vs.-测试误差">2.
The Core Problem: Training Error vs. Test Error 核心问题：训练误差
vs. 测试误差</h1>
<p>The central theme of these slides is finding the “best” model. The
problem is that a model with more predictors (more complex) will
<em>always</em> fit the data it was trained on better. This is a trap.
寻找“最佳”模型。问题在于，预测因子越多（越复杂）的模型<em>总是</em>能更好地拟合训练数据。这是一个陷阱。</p>
<ul>
<li><strong>Training Error:</strong> How well the model fits the data we
used to build it. <strong><span class="math inline">\(R^2\)</span> and
<span class="math inline">\(RSS\)</span> measure this.</strong>
模型与我们构建模型时所用数据的拟合程度。<strong><span
class="math inline">\(R^2\)</span> 和 <span
class="math inline">\(RSS\)</span> 衡量了这一点。</strong></li>
<li><strong>Test Error:</strong> How well the model predicts new, unseen
data. This is what we <em>actually</em> care about. A model that is too
complex (e.g., has 10 predictors when only 3 are useful) will have low
training error but very high test error. This is called
<strong>overfitting</strong>.
模型预测新的、未见过的数据的准确程度。这才是我们<em>真正</em>关心的。过于复杂的模型（例如，有
10 个预测因子，但只有 3
个有用）的训练误差会很低，但测试误差会很高。这被称为<strong>过拟合</strong>。</li>
</ul>
<p>The goal is to choose a model that has the lowest <em>test
error</em>. The metrics below (Adjusted <span
class="math inline">\(R^2\)</span>, AIC, BIC) are all attempts to
<em>estimate</em> this test error without having to actually collect new
data. They do this by adding a <strong>penalty</strong> for complexity.
目标是选择一个具有最低<em>测试误差</em>的模型。以下指标（调整后的 <span
class="math inline">\(R^2\)</span>、AIC、BIC）都是在无需实际收集新数据的情况下尝试<em>估计</em>此测试误差。他们通过增加<strong>复杂度惩罚</strong>来实现这一点。</p>
<h2 id="basic-metrics-measures-of-fit">Basic Metrics (Measures of
Fit)</h2>
<p>These formulas from slide 13 describe how well a model fits the
<em>training data</em>.</p>
<h3 id="residue-error-残差误差">Residue (Error) 残差（误差）</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\epsilon}_i = y_i - \hat{y}_i = y_i -
\hat{\beta}_0 - \sum_{j=1}^{p} \hat{\beta}_j x_{ij}\)</span></li>
<li><strong>Concept:</strong> This is the most basic building block.
It’s the difference between the <em>actual</em> observed value (<span
class="math inline">\(y_i\)</span>) and the value your model
<em>predicted</em> (<span class="math inline">\(\hat{y}_i\)</span>). It
is the “error” for a single data point.
这是最基本的构建块。它是<em>实际</em>观测值 (<span
class="math inline">\(y_i\)</span>) 与模型*预测值 (<span
class="math inline">\(\hat{y}_i\)</span>)
之间的差值。它是单个数据点的“误差”。</li>
</ul>
<h3 id="residual-sum-of-squares-rss-残差平方和-rss">Residual Sum of
Squares (RSS) 残差平方和 (RSS)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(RSS =
\sum_{i=1}^{n} \hat{\epsilon}_i^2\)</span></li>
<li><strong>Concept:</strong> This is the overall measure of model
error. You square all the individual errors (residues) to make them
positive and then add them all up.
这是模型误差的总体度量。将所有单个误差（残差）平方，使其为正，然后将它们全部相加。</li>
<li><strong>Goal:</strong> The entire process of linear regression
(called “Ordinary Least Squares”) is designed to find the <span
class="math inline">\(\hat{\beta}\)</span> coefficients that make this
<strong>RSS value as small as possible</strong>.
整个线性回归过程（称为“普通最小二乘法”）旨在找到使<strong>RSS
值尽可能小</strong>的 <span class="math inline">\(\hat{\beta}\)</span>
个系数。</li>
<li><strong>The Flaw 缺陷:</strong> <span
class="math inline">\(RSS\)</span> will <em>always</em> decrease (or
stay the same) as you add more predictors (<span
class="math inline">\(p\)</span>). A model with all 10 predictors will
have a lower <span class="math inline">\(RSS\)</span> than a model with
9, even if that 10th predictor is useless. Therefore, <span
class="math inline">\(RSS\)</span> is useless for choosing
<em>between</em> models of different sizes. 随着预测变量 (<span
class="math inline">\(p\)</span>) 的增加，<span
class="math inline">\(RSS\)</span>
总是会减小（或保持不变）。一个包含所有 10 个预测变量的模型的 <span
class="math inline">\(RSS\)</span> 会低于一个包含 9
个预测变量的模型，即使第 10 个预测变量毫无用处。因此，<span
class="math inline">\(RSS\)</span>
对于在不同规模的模型之间进行选择毫无用处。</li>
</ul>
<h3 id="r-squared-r2">R-squared (<span
class="math inline">\(R^2\)</span>)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(R^2 = 1 -
\frac{SS_{error}}{SS_{total}} = 1 - \frac{RSS}{\sum_{i=1}^{n} (y_i -
\bar{y})^2}\)</span></li>
<li><strong>Concept:</strong> This metric reframes <span
class="math inline">\(RSS\)</span> into a more interpretable
percentage.此指标将 <span class="math inline">\(RSS\)</span>
重新定义为更易于解释的百分比。
<ul>
<li><span class="math inline">\(SS_{total}\)</span> (the denominator)
represents the <em>total variance</em> of the data. It’s the error you
would get if your “model” was just guessing the average value (<span
class="math inline">\(\bar{y}\)</span>) for every single observation.
（分母）表示数据的<em>总方差</em>。如果你的“模型”只是猜测每个观测值的平均值
(<span
class="math inline">\(\bar{y}\)</span>)，那么你就会得到这个误差。</li>
<li><span class="math inline">\(SS_{error}\)</span> (the <span
class="math inline">\(RSS\)</span>) is the error <em>after</em> using
your model. 是“模型解释的总方差的比例”。 <span
class="math inline">\(R^2\)</span> 为 0.75
意味着你的模型可以解释响应变量 75% 的变异。</li>
<li><span class="math inline">\(R^2\)</span> is the “proportion of total
variance explained by the model.” An <span
class="math inline">\(R^2\)</span> of 0.75 means your model can explain
75% of the variation in the response variable.</li>
</ul></li>
<li><strong>The Flaw 缺陷:</strong> Just like <span
class="math inline">\(RSS\)</span>, <span
class="math inline">\(R^2\)</span> will <em>always</em> increase (or
stay the same) as you add more predictors. This is visually confirmed in
Figure 6.1, where the red line for <span
class="math inline">\(R^2\)</span> only goes up. It will always pick the
most complex model. 与 <span class="math inline">\(RSS\)</span>
一样，随着预测变量的增加，<span class="math inline">\(R^2\)</span>
会<em>始终</em>增加（或保持不变）。图 6.1 直观地证实了这一点，其中 <span
class="math inline">\(R^2\)</span>
的红线只会上升。它总是会选择最复杂的模型。</li>
</ul>
<h2
id="advanced-metrics-for-model-selection-高级指标用于模型选择">Advanced
Metrics (For Model Selection) 高级指标（用于模型选择）</h2>
<p>These metrics “fix” the flaw of <span
class="math inline">\(R^2\)</span> by including a penalty for the number
of predictors.</p>
<h3 id="adjusted-r2">Adjusted <span
class="math inline">\(R^2\)</span></h3>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
  \text{Adjusted } R^2 = 1 - \frac{RSS / (n - p - 1)}{SS_{total} / (n -
1)}
  \]</span></li>
<li><strong>Mathematical Concept:</strong> This formula replaces the
“Sum of Squares” (<span class="math inline">\(SS\)</span>) with “Mean
Squares” (<span class="math inline">\(MS\)</span>).
<ul>
<li><span class="math inline">\(MS_{error} =
\frac{RSS}{n-p-1}\)</span></li>
<li><span class="math inline">\(MS_{total} =
\frac{SS_{total}}{n-1}\)</span></li>
</ul></li>
<li><strong>The “Penalty” Explained:</strong> The penalty is
<strong>degrees of freedom</strong>.
<ul>
<li><span class="math inline">\(n\)</span> = number of data points.</li>
<li><span class="math inline">\(p\)</span> = number of predictors.</li>
<li>The term <span class="math inline">\(n-p-1\)</span> is the degrees
of freedom for the residuals. You start with <span
class="math inline">\(n\)</span> data points, but you “use up” one
degree of freedom to estimate the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) and <span
class="math inline">\(p\)</span> more to estimate the <span
class="math inline">\(p\)</span> slopes.</li>
</ul></li>
<li><strong>How it Works:</strong>
<ol type="1">
<li>When you add a new predictor (increase <span
class="math inline">\(p\)</span>), <span
class="math inline">\(RSS\)</span> goes down, which makes the numerator
(<span class="math inline">\(MS_{error}\)</span>) smaller.</li>
<li>…But, increasing <span class="math inline">\(p\)</span>
<em>also</em> decreases the denominator (<span
class="math inline">\(n-p-1\)</span>), which makes the numerator (<span
class="math inline">\(MS_{error}\)</span>) <em>larger</em>.</li>
</ol>
<ul>
<li>This creates a “tug-of-war.” If the new predictor is
<strong>useful</strong>, it will drop <span
class="math inline">\(RSS\)</span> a lot, and Adjusted <span
class="math inline">\(R^2\)</span> will <strong>increase</strong>. If
the new predictor is <strong>useless</strong>, <span
class="math inline">\(RSS\)</span> will barely change, and the penalty
from decreasing the denominator will win, causing Adjusted <span
class="math inline">\(R^2\)</span> to <strong>decrease</strong>.</li>
</ul></li>
<li><strong>Goal:</strong> You select the model with the
<strong>highest</strong> Adjusted <span
class="math inline">\(R^2\)</span>.</li>
</ul>
<h3 id="akaike-information-criterion-aic">Akaike Information Criterion
(AIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Concept Breakdown:</strong>
<ul>
<li><span class="math inline">\(\ell(\hat{\theta})\)</span>: This is the
<strong>Maximized Likelihood Function</strong>.
<ul>
<li>The <strong>Likelihood Function</strong> <span
class="math inline">\(\ell(\theta)\)</span> asks: “Given a set of model
parameters <span class="math inline">\(\theta\)</span>, how probable is
the data we observed?”</li>
<li>The <strong>Maximum Likelihood Estimate (MLE)</strong> <span
class="math inline">\(\hat{\theta}\)</span> is the specific set of
parameters (the <span class="math inline">\(\hat{\beta}\)</span>’s) that
<em>maximizes</em> this probability.</li>
</ul></li>
<li><span class="math inline">\(\log \ell(\hat{\theta})\)</span>: The
<strong>log-likelihood</strong>. This is just a number that represents
the <em>best possible fit</em> the model can achieve for the data. A
higher number is a better fit.</li>
<li><span class="math inline">\(-2 \log \ell(\hat{\theta})\)</span>:
This is the <strong>Deviance</strong>. Since a higher log-likelihood is
better, a <em>lower</em> deviance is better. This term measures
<strong>poorness-of-fit</strong>.</li>
<li><span class="math inline">\(d\)</span>: The number of parameters
estimated by the model. (e.g., <span class="math inline">\(p\)</span>
predictors + 1 intercept).</li>
<li><span class="math inline">\(2d\)</span>: This is the <strong>Penalty
Term</strong>.</li>
</ul></li>
<li><strong>How it Works:</strong> <span class="math inline">\(AIC =
(\text{Poorness-of-Fit}) + (\text{Complexity Penalty})\)</span>. As you
add predictors, the fit gets better (the deviance term goes down), but
the penalty term (<span class="math inline">\(2d\)</span>) goes up.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> AIC.</li>
</ul>
<h3 id="bayesian-information-criterion-bic">Bayesian Information
Criterion (BIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(BIC =
-2 \log \ell(\hat{\theta}) + \log(n)d\)</span></li>
<li><strong>Concept:</strong> This is mathematically identical to AIC,
but the penalty term is different.
<ul>
<li><strong>AIC Penalty:</strong> <span
class="math inline">\(2d\)</span></li>
<li><strong>BIC Penalty:</strong> <span
class="math inline">\(\log(n)d\)</span></li>
</ul></li>
<li><strong>Comparison:</strong>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations
in your dataset.</li>
<li>As long as your dataset has 8 or more observations (<span
class="math inline">\(n \ge 8\)</span>), <span
class="math inline">\(\log(n)\)</span> will be greater than 2.</li>
<li>This means <strong>BIC applies a much harsher penalty for
complexity</strong> than AIC.</li>
</ul></li>
<li><strong>Consequence:</strong> BIC will tend to choose
<em>simpler</em> models (fewer predictors) than AIC.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> BIC.</li>
</ul>
<h2 id="the-deeper-theory-why-aic-works">The Deeper Theory: Why AIC
Works</h2>
<p>Slide 27 (“Understanding AIC”) gives the deep mathematical
justification.</p>
<ul>
<li><strong>Goal:</strong> We have a <em>true</em>, unknown process
<span class="math inline">\(p\)</span> that generates our data. We are
creating a model <span class="math inline">\(\hat{p}_j\)</span>. We want
our model to be as “close” to the truth as possible.</li>
<li><strong>Kullback-Leibler (K-L) Distance:</strong> This is a function
<span class="math inline">\(K(p, \hat{p}_j)\)</span> that measures the
“information lost” when you use your model <span
class="math inline">\(\hat{p}_j\)</span> to approximate the truth <span
class="math inline">\(p\)</span>. You want to <em>minimize</em> this
distance.</li>
<li><strong>The Math:</strong>
<ol type="1">
<li><span class="math inline">\(K(p, \hat{p}_j) = \int p(y) \log \left(
\frac{p(y)}{\hat{p}_j(y)} \right) dy\)</span></li>
<li>This splits into: <span class="math inline">\(K(p, \hat{p}_j) =
\underbrace{\int p(y) \log(p(y)) dy}_{\text{Constant}} -
\underbrace{\int p(y) \log(\hat{p}_j(y)) dy}_{\text{This is what we need
to maximize}}\)</span></li>
</ol></li>
<li><strong>The Problem:</strong> We can’t calculate that second term
because it requires knowing the <em>true</em> function <span
class="math inline">\(p\)</span>.</li>
<li><strong>Akaike’s Insight:</strong> Akaike proved that the
log-likelihood we <em>can</em> calculate, <span
class="math inline">\(\log \ell(\hat{\theta})\)</span>, is a
<em>biased</em> estimator of that target. He also proved that the bias
is approximately <span class="math inline">\(-d\)</span>.</li>
<li><strong>The Solution:</strong> An <em>unbiased</em> estimate of the
target is <span class="math inline">\(\log \ell(\hat{\theta}) -
d\)</span>.</li>
<li><strong>Final Step:</strong> For historical and statistical reasons,
he multiplied this by <span class="math inline">\(-2\)</span> to create
the final AIC formula.</li>
<li><strong>Conclusion:</strong> AIC is not just a random formula. It is
a carefully derived estimate of how much information your model loses
compared to the “truth” (i.e., its expected performance on new
data).</li>
</ul>
<h2 id="aicbic-for-linear-regression">AIC/BIC for Linear Regression</h2>
<p>Slide 26 shows how these general formulas simplify for linear
regression (assuming normal, Gaussian errors).</p>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Linear Regression Formula:</strong> <span
class="math inline">\(AIC = \frac{1}{n\hat{\sigma}^2}(RSS +
2d\hat{\sigma}^2)\)</span></li>
</ul>
<p><strong>Key Insight:</strong> For linear regression, the
“poorness-of-fit” term (<span class="math inline">\(-2 \log
\ell(\hat{\theta})\)</span>) is <em>directly proportional to</em> the
<span class="math inline">\(RSS\)</span>.</p>
<p>This makes it much easier to understand. You can just think of the
formulas as: * <strong>AIC <span class="math inline">\(\approx\)</span>
<span class="math inline">\(RSS + 2d\hat{\sigma}^2\)</span></strong> *
<strong>BIC <span class="math inline">\(\approx\)</span> <span
class="math inline">\(RSS + \log(n)d\hat{\sigma}^2\)</span></strong></p>
<p>(Here <span class="math inline">\(\hat{\sigma}^2\)</span> is an
estimate of the error variance, which can often be treated as a
constant).</p>
<p>This clearly shows the trade-off: We want a model with a low
<strong><span class="math inline">\(RSS\)</span></strong> (good fit) and
a low <strong><span class="math inline">\(d\)</span></strong> (low
complexity). These two goals are in direct competition.</p>
<p><strong>Mallow’s <span class="math inline">\(C_p\)</span>:</strong>
The slide notes that <span class="math inline">\(C_p\)</span> is
equivalent to AIC for linear regression. The <span
class="math inline">\(C_p\)</span> formula is <span
class="math inline">\(C_p = \frac{1}{n}(RSS +
2d\hat{\sigma}^2_{full})\)</span>, where <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> is the error
variance estimated from the <em>full</em> model. Since <span
class="math inline">\(n\)</span> and <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> are constants,
minimizing <span class="math inline">\(C_p\)</span> is mathematically
identical to minimizing <span class="math inline">\(RSS +
2d\hat{\sigma}^2_{full}\)</span>, which is the same logic as AIC.</p>
<p>Here is a detailed breakdown of the mathematical formulas and
concepts from your slides.</p>
<h2 id="the-core-problem-training-error-vs.-test-error">The Core
Problem: Training Error vs. Test Error</h2>
<p>The central theme of these slides is finding the “best” model. The
problem is that a model with more predictors (more complex) will
<em>always</em> fit the data it was trained on better. This is a
trap.</p>
<ul>
<li><strong>Training Error:</strong> How well the model fits the data we
used to build it. <strong><span class="math inline">\(R^2\)</span> and
<span class="math inline">\(RSS\)</span> measure this.</strong></li>
<li><strong>Test Error:</strong> How well the model predicts new, unseen
data. This is what we <em>actually</em> care about. A model that is too
complex (e.g., has 10 predictors when only 3 are useful) will have low
training error but very high test error. This is called
<strong>overfitting</strong>.</li>
</ul>
<p>The goal is to choose a model that has the lowest <em>test
error</em>. The metrics below (Adjusted <span
class="math inline">\(R^2\)</span>, AIC, BIC) are all attempts to
<em>estimate</em> this test error without having to actually collect new
data. They do this by adding a <strong>penalty</strong> for
complexity.</p>
<h2 id="basic-metrics-measures-of-fit-1">Basic Metrics (Measures of
Fit)</h2>
<p>These formulas from slide 13 describe how well a model fits the
<em>training data</em>.</p>
<h3 id="residue-error">Residue (Error)</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\epsilon}_i = y_i - \hat{y}_i = y_i -
\hat{\beta}_0 - \sum_{j=1}^{p} \hat{\beta}_j x_{ij}\)</span></li>
<li><strong>Concept:</strong> This is the most basic building block.
It’s the difference between the <em>actual</em> observed value (<span
class="math inline">\(y_i\)</span>) and the value your model
<em>predicted</em> (<span class="math inline">\(\hat{y}_i\)</span>). It
is the “error” for a single data point.</li>
</ul>
<h3 id="residual-sum-of-squares-rss">Residual Sum of Squares (RSS)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(RSS =
\sum_{i=1}^{n} \hat{\epsilon}_i^2\)</span></li>
<li><strong>Concept:</strong> This is the overall measure of model
error. You square all the individual errors (residues) to make them
positive and then add them all up.</li>
<li><strong>Goal:</strong> The entire process of linear regression
(called “Ordinary Least Squares”) is designed to find the <span
class="math inline">\(\hat{\beta}\)</span> coefficients that make this
<strong>RSS value as small as possible</strong>.</li>
<li><strong>The Flaw:</strong> <span class="math inline">\(RSS\)</span>
will <em>always</em> decrease (or stay the same) as you add more
predictors (<span class="math inline">\(p\)</span>). A model with all 10
predictors will have a lower <span class="math inline">\(RSS\)</span>
than a model with 9, even if that 10th predictor is useless. Therefore,
<span class="math inline">\(RSS\)</span> is useless for choosing
<em>between</em> models of different sizes.</li>
</ul>
<h3 id="r-squared-r2-1">R-squared (<span
class="math inline">\(R^2\)</span>)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(R^2 = 1 -
\frac{SS_{error}}{SS_{total}} = 1 - \frac{RSS}{\sum_{i=1}^{n} (y_i -
\bar{y})^2}\)</span></li>
<li><strong>Concept:</strong> This metric reframes <span
class="math inline">\(RSS\)</span> into a more interpretable percentage.
<ul>
<li><span class="math inline">\(SS_{total}\)</span> (the denominator)
represents the <em>total variance</em> of the data. It’s the error you
would get if your “model” was just guessing the average value (<span
class="math inline">\(\bar{y}\)</span>) for every single
observation.</li>
<li><span class="math inline">\(SS_{error}\)</span> (the <span
class="math inline">\(RSS\)</span>) is the error <em>after</em> using
your model.</li>
<li><span class="math inline">\(R^2\)</span> is the “proportion of total
variance explained by the model.” An <span
class="math inline">\(R^2\)</span> of 0.75 means your model can explain
75% of the variation in the response variable.</li>
</ul></li>
<li><strong>The Flaw:</strong> Just like <span
class="math inline">\(RSS\)</span>, <span
class="math inline">\(R^2\)</span> will <em>always</em> increase (or
stay the same) as you add more predictors. This is visually confirmed in
Figure 6.1, where the red line for <span
class="math inline">\(R^2\)</span> only goes up. It will always pick the
most complex model.</li>
</ul>
<h2 id="advanced-metrics-for-model-selection">Advanced Metrics (For
Model Selection)</h2>
<p>These metrics “fix” the flaw of <span
class="math inline">\(R^2\)</span> by including a penalty for the number
of predictors.</p>
<h3 id="adjusted-r2-1">Adjusted <span
class="math inline">\(R^2\)</span></h3>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
  \text{Adjusted } R^2 = 1 - \frac{RSS / (n - p - 1)}{SS_{total} / (n -
1)}
  \]</span></li>
<li><strong>Mathematical Concept:</strong> This formula replaces the
“Sum of Squares” (<span class="math inline">\(SS\)</span>) with “Mean
Squares” (<span class="math inline">\(MS\)</span>).
<ul>
<li><span class="math inline">\(MS_{error} =
\frac{RSS}{n-p-1}\)</span></li>
<li><span class="math inline">\(MS_{total} =
\frac{SS_{total}}{n-1}\)</span></li>
</ul></li>
<li><strong>The “Penalty” Explained:</strong> The penalty is
<strong>degrees of freedom</strong>.
<ul>
<li><span class="math inline">\(n\)</span> = number of data points.</li>
<li><span class="math inline">\(p\)</span> = number of predictors.</li>
<li>The term <span class="math inline">\(n-p-1\)</span> is the degrees
of freedom for the residuals. You start with <span
class="math inline">\(n\)</span> data points, but you “use up” one
degree of freedom to estimate the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) and <span
class="math inline">\(p\)</span> more to estimate the <span
class="math inline">\(p\)</span> slopes.</li>
</ul></li>
<li><strong>How it Works:</strong>
<ol type="1">
<li>When you add a new predictor (increase <span
class="math inline">\(p\)</span>), <span
class="math inline">\(RSS\)</span> goes down, which makes the numerator
(<span class="math inline">\(MS_{error}\)</span>) smaller.</li>
<li>…But, increasing <span class="math inline">\(p\)</span>
<em>also</em> decreases the denominator (<span
class="math inline">\(n-p-1\)</span>), which makes the numerator (<span
class="math inline">\(MS_{error}\)</span>) <em>larger</em>.</li>
</ol>
<ul>
<li>This creates a “tug-of-war.” If the new predictor is
<strong>useful</strong>, it will drop <span
class="math inline">\(RSS\)</span> a lot, and Adjusted <span
class="math inline">\(R^2\)</span> will <strong>increase</strong>. If
the new predictor is <strong>useless</strong>, <span
class="math inline">\(RSS\)</span> will barely change, and the penalty
from decreasing the denominator will win, causing Adjusted <span
class="math inline">\(R^2\)</span> to <strong>decrease</strong>.</li>
</ul></li>
<li><strong>Goal:</strong> You select the model with the
<strong>highest</strong> Adjusted <span
class="math inline">\(R^2\)</span>.</li>
</ul>
<h3 id="akaike-information-criterion-aic-1">Akaike Information Criterion
(AIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Concept Breakdown:</strong>
<ul>
<li><span class="math inline">\(\ell(\hat{\theta})\)</span>: This is the
<strong>Maximized Likelihood Function</strong>.
<ul>
<li>The <strong>Likelihood Function</strong> <span
class="math inline">\(\ell(\theta)\)</span> asks: “Given a set of model
parameters <span class="math inline">\(\theta\)</span>, how probable is
the data we observed?”</li>
<li>The <strong>Maximum Likelihood Estimate (MLE)</strong> <span
class="math inline">\(\hat{\theta}\)</span> is the specific set of
parameters (the <span class="math inline">\(\hat{\beta}\)</span>’s) that
<em>maximizes</em> this probability.</li>
</ul></li>
<li><span class="math inline">\(\log \ell(\hat{\theta})\)</span>: The
<strong>log-likelihood</strong>. This is just a number that represents
the <em>best possible fit</em> the model can achieve for the data. A
higher number is a better fit.</li>
<li><span class="math inline">\(-2 \log \ell(\hat{\theta})\)</span>:
This is the <strong>Deviance</strong>. Since a higher log-likelihood is
better, a <em>lower</em> deviance is better. This term measures
<strong>poorness-of-fit</strong>.</li>
<li><span class="math inline">\(d\)</span>: The number of parameters
estimated by the model. (e.g., <span class="math inline">\(p\)</span>
predictors + 1 intercept).</li>
<li><span class="math inline">\(2d\)</span>: This is the <strong>Penalty
Term</strong>.</li>
</ul></li>
<li><strong>How it Works:</strong> <span class="math inline">\(AIC =
(\text{Poorness-of-Fit}) + (\text{Complexity Penalty})\)</span>. As you
add predictors, the fit gets better (the deviance term goes down), but
the penalty term (<span class="math inline">\(2d\)</span>) goes up.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> AIC.</li>
</ul>
<h3 id="bayesian-information-criterion-bic-1">Bayesian Information
Criterion (BIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(BIC =
-2 \log \ell(\hat{\theta}) + \log(n)d\)</span></li>
<li><strong>Concept:</strong> This is mathematically identical to AIC,
but the penalty term is different.
<ul>
<li><strong>AIC Penalty:</strong> <span
class="math inline">\(2d\)</span></li>
<li><strong>BIC Penalty:</strong> <span
class="math inline">\(\log(n)d\)</span></li>
</ul></li>
<li><strong>Comparison:</strong>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations
in your dataset.</li>
<li>As long as your dataset has 8 or more observations (<span
class="math inline">\(n \ge 8\)</span>), <span
class="math inline">\(\log(n)\)</span> will be greater than 2.</li>
<li>This means <strong>BIC applies a much harsher penalty for
complexity</strong> than AIC.</li>
</ul></li>
<li><strong>Consequence:</strong> BIC will tend to choose
<em>simpler</em> models (fewer predictors) than AIC.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> BIC.</li>
</ul>
<h2 id="the-deeper-theory-why-aic-works-1">The Deeper Theory: Why AIC
Works</h2>
<p>Slide 27 (“Understanding AIC”) gives the deep mathematical
justification.</p>
<ul>
<li><strong>Goal:</strong> We have a <em>true</em>, unknown process
<span class="math inline">\(p\)</span> that generates our data. We are
creating a model <span class="math inline">\(\hat{p}_j\)</span>. We want
our model to be as “close” to the truth as possible.</li>
<li><strong>Kullback-Leibler (K-L) Distance:</strong> This is a function
<span class="math inline">\(K(p, \hat{p}_j)\)</span> that measures the
“information lost” when you use your model <span
class="math inline">\(\hat{p}_j\)</span> to approximate the truth <span
class="math inline">\(p\)</span>. You want to <em>minimize</em> this
distance.</li>
<li><strong>The Math:</strong>
<ol type="1">
<li><span class="math inline">\(K(p, \hat{p}_j) = \int p(y) \log \left(
\frac{p(y)}{\hat{p}_j(y)} \right) dy\)</span></li>
<li>This splits into: <span class="math inline">\(K(p, \hat{p}_j) =
\underbrace{\int p(y) \log(p(y)) dy}_{\text{Constant}} -
\underbrace{\int p(y) \log(\hat{p}_j(y)) dy}_{\text{This is what we need
to maximize}}\)</span></li>
</ol></li>
<li><strong>The Problem:</strong> We can’t calculate that second term
because it requires knowing the <em>true</em> function <span
class="math inline">\(p\)</span>.</li>
<li><strong>Akaike’s Insight:</strong> Akaike proved that the
log-likelihood we <em>can</em> calculate, <span
class="math inline">\(\log \ell(\hat{\theta})\)</span>, is a
<em>biased</em> estimator of that target. He also proved that the bias
is approximately <span class="math inline">\(-d\)</span>.</li>
<li><strong>The Solution:</strong> An <em>unbiased</em> estimate of the
target is <span class="math inline">\(\log \ell(\hat{\theta}) -
d\)</span>.</li>
<li><strong>Final Step:</strong> For historical and statistical reasons,
he multiplied this by <span class="math inline">\(-2\)</span> to create
the final AIC formula.</li>
<li><strong>Conclusion:</strong> AIC is not just a random formula. It is
a carefully derived estimate of how much information your model loses
compared to the “truth” (i.e., its expected performance on new
data).</li>
</ul>
<h2 id="aicbic-for-linear-regression-1">AIC/BIC for Linear
Regression</h2>
<p>Slide 26 shows how these general formulas simplify for linear
regression (assuming normal, Gaussian errors).</p>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Linear Regression Formula:</strong> <span
class="math inline">\(AIC = \frac{1}{n\hat{\sigma}^2}(RSS +
2d\hat{\sigma}^2)\)</span></li>
</ul>
<p><strong>Key Insight:</strong> For linear regression, the
“poorness-of-fit” term (<span class="math inline">\(-2 \log
\ell(\hat{\theta})\)</span>) is <em>directly proportional to</em> the
<span class="math inline">\(RSS\)</span>.</p>
<p>This makes it much easier to understand. You can just think of the
formulas as: * <strong>AIC <span class="math inline">\(\approx\)</span>
<span class="math inline">\(RSS + 2d\hat{\sigma}^2\)</span></strong> *
<strong>BIC <span class="math inline">\(\approx\)</span> <span
class="math inline">\(RSS + \log(n)d\hat{\sigma}^2\)</span></strong></p>
<p>(Here <span class="math inline">\(\hat{\sigma}^2\)</span> is an
estimate of the error variance, which can often be treated as a
constant).</p>
<p>This clearly shows the trade-off: We want a model with a low
<strong><span class="math inline">\(RSS\)</span></strong> (good fit) and
a low <strong><span class="math inline">\(d\)</span></strong> (low
complexity). These two goals are in direct competition.</p>
<p><strong>Mallow’s <span class="math inline">\(C_p\)</span>:</strong>
The slide notes that <span class="math inline">\(C_p\)</span> is
equivalent to AIC for linear regression. The <span
class="math inline">\(C_p\)</span> formula is <span
class="math inline">\(C_p = \frac{1}{n}(RSS +
2d\hat{\sigma}^2_{full})\)</span>, where <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> is the error
variance estimated from the <em>full</em> model. Since <span
class="math inline">\(n\)</span> and <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> are constants,
minimizing <span class="math inline">\(C_p\)</span> is mathematically
identical to minimizing <span class="math inline">\(RSS +
2d\hat{\sigma}^2_{full}\)</span>, which is the same logic as AIC.</p>
<h1 id="variable-selection">3. Variable Selection</h1>
<h2 id="core-concept-the-problem-of-variable-selection">Core Concept:
The Problem of Variable Selection</h2>
<p>In regression, we want to model a response variable <span
class="math inline">\(Y\)</span> using a set of <span
class="math inline">\(p\)</span> predictor variables <span
class="math inline">\(X_1, X_2, ..., X_p\)</span>.</p>
<ul>
<li><p><strong>The “Kitchen Sink” Problem:</strong> A common temptation
is to include all available predictors in the model: <span
class="math display">\[Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... +
\beta_pX_p + \epsilon\]</span> This often leads to
<strong>overfitting</strong>. The model may fit the training data well
but will perform poorly on new, unseen data. It’s also hard to interpret
a model with dozens of predictors.</p></li>
<li><p><strong>The Solution: Subset Selection.</strong> The goal is to
find a smaller subset of the predictors that builds a model that is:</p>
<ol type="1">
<li><strong>Accurate:</strong> Has low prediction error.</li>
<li><strong>Parsimonious:</strong> Uses the fewest predictors
necessary.</li>
<li><strong>Interpretable:</strong> Is simple enough for a human to
understand.</li>
</ol></li>
</ul>
<p>Your slides present two main methods to achieve this: <strong>Best
Subset Selection</strong> and <strong>Forward Stepwise
Selection</strong>.</p>
<h2 id="method-1-best-subset-selection-bss">Method 1: Best Subset
Selection (BSS)</h2>
<p>This is the “brute force” approach. It considers <em>every single
possible model</em>.</p>
<h3 id="conceptual-algorithm">Conceptual Algorithm</h3>
<ol type="1">
<li>Fit all models with <span class="math inline">\(k=1\)</span>
predictor (there are <span class="math inline">\(p\)</span> of these).
Find the best one (lowest RSS) and call it <span
class="math inline">\(M_1\)</span>.</li>
<li>Fit all models with <span class="math inline">\(k=2\)</span>
predictors (there are <span class="math inline">\(\binom{p}{2}\)</span>
of these). Find the best one and call it <span
class="math inline">\(M_2\)</span>.</li>
<li>…</li>
<li>Fit the one model with <span class="math inline">\(k=p\)</span>
predictors (the full model), <span
class="math inline">\(M_p\)</span>.</li>
<li>You now have a list of <span class="math inline">\(p\)</span> “best”
models: <span class="math inline">\(M_1, M_2, ..., M_p\)</span>.</li>
<li>Use a selection criterion (like <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>BIC</strong>,
<strong>AIC</strong>, or <strong><span
class="math inline">\(C_p\)</span></strong>) to choose the single best
model from this list.</li>
</ol>
<h3
id="mathematical-computational-cost-from-slide-225641.png">Mathematical
&amp; Computational Cost (from slide <code>225641.png</code>)</h3>
<ul>
<li>For each predictor, there are two possibilities: it’s either
<strong>IN</strong> the model or <strong>OUT</strong>.</li>
<li>With <span class="math inline">\(p\)</span> predictors, the total
number of models to test is <span class="math inline">\(2 \times 2
\times ... \times 2\)</span> (<span class="math inline">\(p\)</span>
times).</li>
<li><strong>Total Models = <span
class="math inline">\(2^p\)</span></strong></li>
<li>This is a “combinatorial explosion.” As the slide notes, if <span
class="math inline">\(p=20\)</span>, <span class="math inline">\(2^{20}
= 1,048,576\)</span> models. This is computationally infeasible for
large <span class="math inline">\(p\)</span>.</li>
</ul>
<h2 id="method-2-forward-stepwise-selection-fss">Method 2: Forward
Stepwise Selection (FSS)</h2>
<p>This is a “greedy” algorithm. It’s an efficient alternative to BSS
that does <em>not</em> test every model.</p>
<h3
id="conceptual-algorithm-from-slides-225645.png-225648.png">Conceptual
Algorithm (from slides <code>225645.png</code> &amp;
<code>225648.png</code>)</h3>
<ul>
<li><p><strong>Step 1:</strong> Start with the <strong>null
model</strong>, <span class="math inline">\(M_0\)</span>, which has no
predictors. <span class="math display">\[M_0: Y = \beta_0 +
\epsilon\]</span> The prediction is just the sample mean of <span
class="math inline">\(Y\)</span>.</p></li>
<li><p><strong>Step 2 (Iterative):</strong></p>
<ul>
<li><strong>For <span class="math inline">\(k=0\)</span> (to get <span
class="math inline">\(M_1\)</span>):</strong> Fit all <span
class="math inline">\(p\)</span> models that add <em>one</em> predictor
to <span class="math inline">\(M_0\)</span>. Choose the best one (lowest
<strong>RSS</strong> or highest <strong><span
class="math inline">\(R^2\)</span></strong>). This is <span
class="math inline">\(M_1\)</span>. Let’s say it contains <span
class="math inline">\(X_1\)</span>.</li>
<li><strong>For <span class="math inline">\(k=1\)</span> (to get <span
class="math inline">\(M_2\)</span>):</strong> <em>Keep</em> <span
class="math inline">\(X_1\)</span> in the model. Fit all <span
class="math inline">\(p-1\)</span> models that add <em>one more</em>
predictor to <span class="math inline">\(M_1\)</span> (e.g., <span
class="math inline">\(M_1+X_2\)</span>, <span
class="math inline">\(M_1+X_3\)</span>, …). Choose the best of these.
This is <span class="math inline">\(M_2\)</span>.</li>
<li><strong>Repeat:</strong> Continue this process, adding one variable
at a time, until all <span class="math inline">\(p\)</span> predictors
are in the model <span class="math inline">\(M_p\)</span>.</li>
</ul></li>
<li><p><strong>Step 3:</strong> You now have a sequence of <span
class="math inline">\(p+1\)</span> models: <span
class="math inline">\(M_0, M_1, ..., M_p\)</span>. Choose the single
best model from this sequence using <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>AIC</strong>,
<strong>BIC</strong>, or <strong><span
class="math inline">\(C_p\)</span></strong>.</p></li>
</ul>
<h3
id="mathematical-computational-cost-from-slide-225651.png">Mathematical
&amp; Computational Cost (from slide <code>225651.png</code>)</h3>
<ul>
<li>To find <span class="math inline">\(M_1\)</span>, you fit <span
class="math inline">\(p\)</span> models.</li>
<li>To find <span class="math inline">\(M_2\)</span>, you fit <span
class="math inline">\(p-1\)</span> models.</li>
<li>To find <span class="math inline">\(M_p\)</span>, you fit <span
class="math inline">\(1\)</span> model.</li>
<li>The null model <span class="math inline">\(M_0\)</span> is 1
model.</li>
<li><strong>Total Models = <span class="math inline">\(1 +
\sum_{k=0}^{p-1} (p-k) = 1 + p + (p-1) + ... + 1 = 1 +
\frac{p(p+1)}{2}\)</span></strong></li>
<li>As the slide notes, if <span class="math inline">\(p=20\)</span>,
this is only <span class="math inline">\(1 + 20(21)/2 = 211\)</span>
models. This is vastly more efficient than BSS.</li>
<li><strong>Key weakness:</strong> The method is “greedy.” If it adds
<span class="math inline">\(X_1\)</span> in Step 1, it can
<em>never</em> be removed. It’s possible the true best 2-variable model
is <span class="math inline">\((X_2, X_3)\)</span>, but if FSS chose
<span class="math inline">\(X_1\)</span> as the best 1-variable model,
it will never find <span class="math inline">\((X_2, X_3)\)</span>.</li>
</ul>
<h2 id="how-to-choose-the-best-model-the-criteria">4. How to Choose the
“Best” Model: The Criteria</h2>
<p>You can’t use <strong>RSS</strong> or <strong><span
class="math inline">\(R^2\)</span></strong> to compare models with
<em>different numbers of predictors</em> (<span
class="math inline">\(k\)</span>). This is because RSS always decreases
(and <span class="math inline">\(R^2\)</span> always increases) as you
add more variables. You <em>must</em> use a criterion that penalizes
complexity.</p>
<ul>
<li><p><strong>RSS (Residual Sum of Squares):</strong> Goal is to
<strong>minimize</strong>. <span class="math display">\[RSS =
\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span> Good for comparing models
<em>of the same size <span
class="math inline">\(k\)</span></em>.</p></li>
<li><p><strong>Adjusted R-squared (<span class="math inline">\(Adj.
R^2\)</span>):</strong> Goal is to <strong>maximize</strong>. <span
class="math display">\[Adj. R^2 = 1 -
\frac{(1-R^2)(n-1)}{n-p-1}\]</span> This “adjusts” <span
class="math inline">\(R^2\)</span> by adding a penalty for having more
predictors (<span class="math inline">\(p\)</span>). Adding a useless
predictor will make <span class="math inline">\(Adj. R^2\)</span> go
down.</p></li>
<li><p><strong>Mallow’s <span
class="math inline">\(C_p\)</span>:</strong> Goal is to
<strong>minimize</strong>. <span class="math display">\[C_p \approx
\frac{1}{n}(RSS + 2p\hat{\sigma}^2)\]</span> Here, <span
class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the
error variance from the <em>full model</em> (with all <span
class="math inline">\(p\)</span> predictors). A good model will have
<span class="math inline">\(C_p \approx p\)</span>.</p></li>
<li><p><strong>AIC (Akaike Information Criterion) &amp; BIC (Bayesian
Information Criterion):</strong> Goal is to <strong>minimize</strong>.
<span class="math display">\[AIC = 2p - 2\ln(\hat{L})\]</span> <span
class="math display">\[BIC = p\ln(n) - 2\ln(\hat{L})\]</span> Here,
<span class="math inline">\(\hat{L}\)</span> is the maximized likelihood
of the model. You don’t need to calculate this by hand; software
provides it.</p>
<ul>
<li><strong>Key difference:</strong> BIC’s penalty for <span
class="math inline">\(p\)</span> is <span
class="math inline">\(p\ln(n)\)</span>, while AIC’s is <span
class="math inline">\(2p\)</span>. Since <span
class="math inline">\(\ln(n)\)</span> is almost always <span
class="math inline">\(&gt; 2\)</span> (for <span
class="math inline">\(n&gt;7\)</span>), <strong>BIC applies a much
heavier penalty for complexity</strong>.</li>
<li>This means <strong>BIC tends to choose smaller, more parsimonious
models</strong> than AIC or <span class="math inline">\(Adj.
R^2\)</span>.</li>
</ul></li>
</ul>
<h2 id="python-code-analysis-slide-225546.jpg">5. Python Code Analysis
(Slide <code>225546.jpg</code>)</h2>
<p>This slide shows the Python code for <strong>Best Subset
Selection</strong> (BSS).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations <span class="comment"># &lt;-- This is the BSS engine</span></span><br></pre></td></tr></table></figure>
<h3 id="block-1-load-the-credit-dataset">Block 1: Load the Credit
dataset</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Load the Credit dataset</span></span><br><span class="line">Credit = pd.read_csv(<span class="string">&#x27;Credit.csv&#x27;</span>)</span><br><span class="line">Credit[<span class="string">&#x27;ID&#x27;</span>] = Credit[<span class="string">&#x27;ID&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">(num_samples, num_predictors) = Credit.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert categorical text data to numerical (dummy variables)</span></span><br><span class="line">Credit[<span class="string">&#x27;Gender&#x27;</span>] = Credit[<span class="string">&#x27;Gender&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Male&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Female&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Student&#x27;</span>] = Credit[<span class="string">&#x27;Student&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Yes&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;No&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Married&#x27;</span>] = Credit[<span class="string">&#x27;Married&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Yes&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;No&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Ethnicity&#x27;</span>] = Credit[<span class="string">&#x27;Ethnicity&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Asian&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Caucasian&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;African American&#x27;</span>: <span class="number">0</span>&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>pd.read_csv</code></strong>: Reads the data into a
<code>pandas</code> DataFrame.</li>
<li><strong><code>.map()</code></strong>: This is a crucial
preprocessing step. Regression models require numbers, not text like
‘Yes’ or ‘Male’. This line converts those strings into <code>1</code>s
and <code>0</code>s.</li>
</ul>
<h3 id="block-2-plot-scatterplot-matrix">Block 2: Plot scatterplot
matrix</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. Plot scatterplot matrix</span></span><br><span class="line">selected_columns = [<span class="string">&#x27;Balance&#x27;</span>, <span class="string">&#x27;Education&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Cards&#x27;</span>, <span class="string">&#x27;Rating&#x27;</span>, <span class="string">&#x27;Limit&#x27;</span>, <span class="string">&#x27;Income&#x27;</span>]</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&quot;ticks&quot;</span>)</span><br><span class="line">sns.pairplot(Credit[selected_columns], diag_kind=<span class="string">&#x27;kde&#x27;</span>)</span><br><span class="line">plt.suptitle(<span class="string">&#x27;Scatterplot Matrix&#x27;</span>, y=<span class="number">1.02</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>sns.pairplot</code></strong>: A powerful visualization
from the <code>seaborn</code> library. The resulting plot (right side of
the slide) is a grid.
<ul>
<li><strong>Diagonal plots (kde)</strong>: Show the distribution (Kernel
Density Estimate) of a single variable (e.g., ‘Balance’ is skewed
right).</li>
<li><strong>Off-diagonal plots (scatter)</strong>: Show the relationship
between two variables (e.g., ‘Limit’ and ‘Rating’ are almost perfectly
linear). This helps you visually spot potentially strong
predictors.</li>
</ul></li>
</ul>
<h3 id="block-3-best-subset-selection">Block 3: Best Subset
Selection</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Best Subset Selection</span></span><br><span class="line"><span class="comment"># (This code is incomplete on the slide, I&#x27;ll fill in the logic)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define target and predictors</span></span><br><span class="line">target = <span class="string">&#x27;Balance&#x27;</span></span><br><span class="line">predictors = [col <span class="keyword">for</span> col <span class="keyword">in</span> Credit.columns <span class="keyword">if</span> col != target] </span><br><span class="line">nvmax = <span class="number">10</span> <span class="comment"># Max number of predictors to test (up to 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize lists to store model statistics</span></span><br><span class="line">model_stats = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over number of predictors from 1 to nvmax</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, nvmax + <span class="number">1</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Generate all possible combinations of predictors of size k</span></span><br><span class="line">    <span class="comment"># This is the core of BSS</span></span><br><span class="line">    <span class="keyword">for</span> subset <span class="keyword">in</span> <span class="built_in">list</span>(combinations(predictors, k)):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the design matrix (X)</span></span><br><span class="line">        X_subset = Credit[<span class="built_in">list</span>(subset)]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Add a constant (intercept) term to the model</span></span><br><span class="line">        <span class="comment"># Y = B0 + B1*X1 -&gt; statsmodels needs B0 to be added manually</span></span><br><span class="line">        X_subset_const = sm.add_constant(X_subset)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the target variable (y)</span></span><br><span class="line">        y_target = Credit[target]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Fit the Ordinary Least Squares (OLS) model</span></span><br><span class="line">        model = sm.OLS(y_target, X_subset_const).fit()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate RSS</span></span><br><span class="line">        RSS = ((model.resid) ** <span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (The full code would also calculate R-squared, Adj. R-sq, BIC, etc. here)</span></span><br><span class="line">        <span class="comment"># model_stats.append(&#123;&#x27;k&#x27;: k, &#x27;subset&#x27;: subset, &#x27;RSS&#x27;: RSS, ...&#125;)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>for k in range(1, nvmax + 1)</code></strong>: This is
the <em>outer</em> loop that iterates from <span
class="math inline">\(k=1\)</span> (1 predictor) to <span
class="math inline">\(k=10\)</span> (10 predictors).</li>
<li><strong><code>list(combinations(predictors, k))</code></strong>:
This is the <em>inner</em> loop and the <strong>most important
line</strong>. The <code>itertools.combinations</code> function is a
highly efficient way to generate all unique subsets.
<ul>
<li>When <span class="math inline">\(k=1\)</span>, it returns
<code>[('Income',), ('Limit',), ('Rating',), ...]</code>.</li>
<li>When <span class="math inline">\(k=2\)</span>, it returns
<code>[('Income', 'Limit'), ('Income', 'Rating'), ('Limit', 'Rating'), ...]</code>.</li>
<li>This is what generates the <span class="math inline">\(2^p\)</span>
(or in this case, <span class="math inline">\(\sum_{k=1}^{10}
\binom{p}{k}\)</span>) models to test.</li>
</ul></li>
<li><strong><code>sm.add_constant(X_subset)</code></strong>: Your
regression equation is <span class="math inline">\(Y = \beta_0 +
\beta_1X_1\)</span>. The <span class="math inline">\(X_1\)</span> is
your <code>X_subset</code>. The <code>sm.add_constant</code> function
adds a column of <code>1</code>s to your data, which allows the
<code>statsmodels</code> library to estimate the <span
class="math inline">\(\beta_0\)</span> (intercept) term.</li>
<li><strong><code>sm.OLS(y_target, X_subset_const).fit()</code></strong>:
This fits the Ordinary Least Squares (OLS) model, which finds the <span
class="math inline">\(\beta\)</span> coefficients that <strong>minimize
the RSS</strong>.</li>
<li><strong><code>model.resid</code></strong>: This attribute of the
fitted model contains the residuals (<span class="math inline">\(e_i =
y_i - \hat{y}_i\)</span>) for each data point.</li>
<li><strong><code>((model.resid) ** 2).sum()</code></strong>: This line
is the direct code implementation of the formula <span
class="math inline">\(RSS = \sum e_i^2\)</span>.</li>
</ul>
<h2 id="synthesizing-the-results-the-plots">Synthesizing the Results
(The Plots)</h2>
<p>After running the BSS code, you get the data used in the plots and
the table.</p>
<ul>
<li><p><strong>Image <code>225550.png</code> (Adjusted
R-squared)</strong></p>
<ul>
<li><strong>Goal:</strong> Maximize.</li>
<li><strong>What it shows:</strong> The gray dots are <em>all</em> the
models tested for each <span class="math inline">\(k\)</span>. The red
line connects the single <em>best</em> model for each <span
class="math inline">\(k\)</span>.</li>
<li><strong>Conclusion:</strong> The plot shows a sharp “elbow.” The
<span class="math inline">\(Adj. R^2\)</span> increases dramatically up
to <span class="math inline">\(k=4\)</span>, then increases very slowly.
The maximum is around <span class="math inline">\(k=6\)</span> or <span
class="math inline">\(k=7\)</span>, but the gain after <span
class="math inline">\(k=4\)</span> is minimal.</li>
</ul></li>
<li><p><strong>Image <code>225554.png</code> (BIC)</strong></p>
<ul>
<li><strong>Goal:</strong> Minimize.</li>
<li><strong>What it shows:</strong> BIC heavily penalizes
complexity.</li>
<li><strong>Conclusion:</strong> The plot shows a very clear minimum.
The BIC value plummets from <span class="math inline">\(k=2\)</span> to
<span class="math inline">\(k=3\)</span> and hits its lowest point at
<strong><span class="math inline">\(k=4\)</span></strong>. After <span
class="math inline">\(k=4\)</span>, the penalty for adding more
variables is <em>larger</em> than the benefit in model fit, so the BIC
score starts to rise. This is a very strong vote for the 4-predictor
model.</li>
</ul></li>
<li><p><strong>Image <code>225635.png</code> (Mallow’s <span
class="math inline">\(C_p\)</span>)</strong></p>
<ul>
<li><strong>Goal:</strong> Minimize.</li>
<li><strong>What it shows:</strong> A very similar story to BIC.</li>
<li><strong>Conclusion:</strong> The <span
class="math inline">\(C_p\)</span> value drops significantly and hits
its minimum at <strong><span
class="math inline">\(k=4\)</span></strong>.</li>
</ul></li>
<li><p><strong>Image <code>225638.png</code> (Summary
Table)</strong></p>
<ul>
<li>This is the <strong>most important image</strong> for the final
conclusion. It summarizes the red line from all the plots.</li>
<li>Look at the row for <code>Num_Predictors = 4</code>. The predictors
are <strong>(Income, Limit, Cards, Student)</strong>.</li>
<li>Now look at the columns for <code>BIC</code> and <code>Cp</code>.
<ul>
<li><strong>BIC:</strong> <code>4841.615607</code>. This is the lowest
value in the entire <code>BIC</code> column (the value at <span
class="math inline">\(k=3\)</span> is <code>4865.352851</code>).</li>
<li><strong>Cp:</strong> <code>7.122228</code>. This is also the lowest
value in the <code>Cp</code> column.</li>
</ul></li>
<li>The <code>Adj_R_squared</code> at <span
class="math inline">\(k=4\)</span> is <code>0.953580</code>, which is
very close to its maximum of <code>~0.954</code> at <span
class="math inline">\(k=7-10\)</span>.</li>
</ul></li>
</ul>
<p><strong>Final Conclusion:</strong> All three “penalized” criteria
(Adjusted <span class="math inline">\(R^2\)</span>, BIC, and <span
class="math inline">\(C_p\)</span>) point to the same conclusion. While
<span class="math inline">\(Adj. R^2\)</span> is a bit ambiguous,
<strong>BIC and <span class="math inline">\(C_p\)</span> provide a clear
signal that the best, most parsimonious model is the 4-predictor model
using <code>Income</code>, <code>Limit</code>, <code>Cards</code>, and
<code>Student</code></strong>.</p>
<h1 id="subset-selection">4. Subset Selection</h1>
<h2 id="summary-of-subset-selection">Summary of Subset Selection</h2>
<p>These slides introduce <strong>subset selection</strong>, a process
in statistical learning used to identify the best subset of predictors
(variables) for a regression model. The goal is to find a model that has
low prediction error and avoids overfitting by excluding irrelevant
variables.</p>
<p>The slides cover two main “greedy” (stepwise) algorithms and the
criteria used to select the final best model.</p>
<h2 id="stepwise-selection-algorithms">Stepwise Selection
Algorithms</h2>
<p>Instead of testing all <span class="math inline">\(2^p\)</span>
possible models (which is “best subset selection” and computationally
unfeasible), stepwise methods build a single path of models.</p>
<h3 id="forward-stepwise-selection">Forward Stepwise Selection</h3>
<p>This is an <strong>additive</strong> (bottom-up) approach:</p>
<ol type="1">
<li><strong>Start</strong> with the null model (no predictors).</li>
<li><strong>Find</strong> the best 1-variable model (the one that gives
the lowest Residual Sum of Squares, or RSS).</li>
<li><strong>Add</strong> the single variable that, when added to the
current model, results in the <em>new</em> best model (lowest RSS).</li>
<li><strong>Repeat</strong> this process until all <span
class="math inline">\(p\)</span> predictors are in the model.</li>
<li>This generates a sequence of <span
class="math inline">\(p+1\)</span> models, from <span
class="math inline">\(\mathcal{M}_0\)</span> to <span
class="math inline">\(\mathcal{M}_p\)</span>.</li>
</ol>
<h3 id="backward-stepwise-selection">Backward Stepwise Selection</h3>
<p>This is a <strong>subtractive</strong> (top-down) approach:</p>
<ol type="1">
<li><strong>Start</strong> with the full model containing all <span
class="math inline">\(p\)</span> predictors.</li>
<li><strong>Find</strong> the best <span
class="math inline">\((p-1)\)</span>-variable model by <em>removing</em>
the single variable that results in the <em>lowest RSS</em> (or highest
<span class="math inline">\(R^2\)</span>). This variable is considered
the least significant.</li>
<li><strong>Remove</strong> the next variable that, when removed from
the current best model, gives the new best model.</li>
<li><strong>Repeat</strong> until only the null model remains.</li>
<li>This also generates a sequence of <span
class="math inline">\(p+1\)</span> models.</li>
</ol>
<h4 id="pros-and-cons-backward-selection">Pros and Cons (Backward
Selection)</h4>
<ul>
<li><strong>Pro:</strong> Computationally efficient compared to best
subset. It fits <span class="math inline">\(1 + \sum_{k=0}^{p-1}(p-k) =
\mathbf{1 + p(p+1)/2}\)</span> models, which is much less than <span
class="math inline">\(2^p\)</span>. (e.g., for <span
class="math inline">\(p=20\)</span>, it’s 211 models vs. &gt;1
million).</li>
<li><strong>Con:</strong> <strong>Cannot be used if <span
class="math inline">\(p &gt; n\)</span></strong> (more predictors than
observations), because the initial full model cannot be fit.</li>
<li><strong>Con (for both):</strong> These methods are
<strong>greedy</strong>. A variable added in forward selection is
<em>never removed</em>, and a variable removed in backward selection is
<em>never added back</em>. This means they are not guaranteed to find
the true best model.</li>
</ul>
<h2 id="choosing-the-final-best-model">Choosing the Final Best
Model</h2>
<p>Both forward and backward selection give you a set of candidate
models (e.g., the best 1-variable model, best 2-variable model, etc.).
You must then choose the <em>single best</em> one. The slides show two
main approaches:</p>
<h3 id="a.-direct-error-estimation">A. Direct Error Estimation</h3>
<p>Use a validation set or cross-validation (CV) to estimate the test
error for each model (e.g., the 1-variable, 2-variable… models).
<strong>Choose the model with the lowest estimated test
error.</strong></p>
<h3 id="b.-adjusted-metrics-penalizing-for-complexity">B. Adjusted
Metrics (Penalizing for Complexity)</h3>
<p>Standard RSS and <span class="math inline">\(R^2\)</span> will always
improve as you add variables, leading to overfitting. Instead, use
metrics that <em>penalize</em> the model for having too many
predictors.</p>
<ul>
<li><p><strong>Mallows’ <span
class="math inline">\(C_p\)</span>:</strong> An estimate of test Mean
Squared Error (MSE). <span class="math display">\[C_p = \frac{1}{n} (RSS
+ 2d\hat{\sigma}^2)\]</span> (where <span
class="math inline">\(d\)</span> is the number of predictors, and <span
class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the
error variance). <strong>You want to find the model with the
<em>minimum</em> <span
class="math inline">\(C_p\)</span>.</strong></p></li>
<li><p><strong>BIC (Bayesian Information Criterion):</strong> <span
class="math display">\[BIC = \frac{1}{n} (RSS +
\log(n)d\hat{\sigma}^2)\]</span> BIC’s penalty <span
class="math inline">\(\log(n)\)</span> is stronger than <span
class="math inline">\(C_p\)</span>’s (or AIC’s) penalty of <span
class="math inline">\(2\)</span>, so it tends to select <em>smaller</em>
(more parsimonious) models. <strong>You want to find the model with the
<em>minimum</em> BIC.</strong></p></li>
<li><p><strong>Adjusted <span
class="math inline">\(R^2\)</span>:</strong> <span
class="math display">\[R^2_{adj} = 1 -
\frac{RSS/(n-d-1)}{TSS/(n-1)}\]</span> (where <span
class="math inline">\(TSS\)</span> is the Total Sum of Squares). Unlike
<span class="math inline">\(R^2\)</span>, this metric can decrease if
adding a variable doesn’t help enough. <strong>You want to find the
model with the <em>maximum</em> Adjusted <span
class="math inline">\(R^2\)</span>.</strong></p></li>
</ul>
<h2 id="python-code-understanding">Python Code Understanding</h2>
<p>The slides use the <code>regsubsets()</code> function from the
<code>leaps</code> package in <strong>R</strong>.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># R Code from slides</span></span><br><span class="line">library<span class="punctuation">(</span>leaps<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Forward Selection</span></span><br><span class="line">regfit.fwd <span class="operator">&lt;-</span> regsubsets<span class="punctuation">(</span>Balance<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Credit<span class="punctuation">,</span> method<span class="operator">=</span><span class="string">&quot;forward&quot;</span><span class="punctuation">,</span> nvmax<span class="operator">=</span><span class="number">11</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Backward Selection</span></span><br><span class="line">regfit.bwd <span class="operator">&lt;-</span> regsubsets<span class="punctuation">(</span>Balance<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Credit<span class="punctuation">,</span> method<span class="operator">=</span><span class="string">&quot;backward&quot;</span><span class="punctuation">,</span> nvmax<span class="operator">=</span><span class="number">11</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>In <strong>Python</strong>, the standard tool for this is
<code>SequentialFeatureSelector</code> from
<strong><code>scikit-learn</code></strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SequentialFeatureSelector</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Credit&#x27; is a pandas DataFrame with &#x27;Balance&#x27; as the target</span></span><br><span class="line">X = Credit.drop(<span class="string">&#x27;Balance&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = Credit[<span class="string">&#x27;Balance&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the linear regression estimator</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Forward Selection ---</span></span><br><span class="line"><span class="comment"># direction=&#x27;forward&#x27; starts with 0 features and adds them</span></span><br><span class="line"><span class="comment"># To get the best 4-variable model, for example:</span></span><br><span class="line">sfs_forward = SequentialFeatureSelector(</span><br><span class="line">    model,</span><br><span class="line">    n_features_to_select=<span class="number">4</span>,</span><br><span class="line">    direction=<span class="string">&#x27;forward&#x27;</span>,</span><br><span class="line">    cv=<span class="literal">None</span> <span class="comment"># Or use cross-validation, e.g., cv=10</span></span><br><span class="line">)</span><br><span class="line">sfs_forward.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Forward selection best 4 features:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(sfs_forward.get_feature_names_out())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Backward Selection ---</span></span><br><span class="line"><span class="comment"># direction=&#x27;backward&#x27; starts with all features and removes them</span></span><br><span class="line">sfs_backward = SequentialFeatureSelector(</span><br><span class="line">    model,</span><br><span class="line">    n_features_to_select=<span class="number">4</span>,</span><br><span class="line">    direction=<span class="string">&#x27;backward&#x27;</span>,</span><br><span class="line">    cv=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line">sfs_backward.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nBackward selection best 4 features:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(sfs_backward.get_feature_names_out())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: To replicate the plots, you would loop this process,</span></span><br><span class="line"><span class="comment"># changing &#x27;n_features_to_select&#x27; from 1 to p,</span></span><br><span class="line"><span class="comment"># record the model scores (e.g., RSS, AIC, BIC) at each step,</span></span><br><span class="line"><span class="comment"># and then plot the results.</span></span><br></pre></td></tr></table></figure>
<h2 id="important-images">Important Images</h2>
<ol type="1">
<li><p><strong>Slide <code>...230014.png</code> (Forward Selection
Plots) &amp; <code>...230036.png</code> (Backward Selection
Plots):</strong></p>
<ul>
<li><strong>What they are:</strong> These <span class="math inline">\(2
\times 2\)</span> plot grids are the most important visuals. They show
<strong>Residual Sum of Squares (RSS)</strong>, <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>BIC</strong>, and
<strong>Mallows’ <span class="math inline">\(C_p\)</span></strong>
plotted against the <em>Number of Variables</em>.</li>
<li><strong>Why they’re important:</strong> They are the
<strong>decision-making tool</strong>. You use these plots to choose the
best model.
<ul>
<li>You look for the “elbow” or <strong>minimum</strong> value for BIC
and <span class="math inline">\(C_p\)</span>.</li>
<li>You look for the “peak” or <strong>maximum</strong> value for
Adjusted <span class="math inline">\(R^2\)</span>.</li>
<li>(RSS is not used for selection as it always decreases).</li>
</ul></li>
</ul></li>
<li><p><strong>Slide <code>...230040.png</code> (Find the best
model):</strong></p>
<ul>
<li><strong>What it is:</strong> This slide shows a close-up of the
<span class="math inline">\(C_p\)</span>, BIC, and Adjusted <span
class="math inline">\(R^2\)</span> plots, with the “best” model (the
min/max) marked with a blue ‘x’.</li>
<li><strong>Why it’s important:</strong> It explicitly states the
selection criteria. The text highlights that BIC suggests a 4-variable
model, while the other two are “rather flat” after 4, making the choice
less obvious but pointing to a simple model.</li>
</ul></li>
<li><p><strong>Slide <code>...230045.png</code> (BIC vs. Validation
vs. CV):</strong></p>
<ul>
<li><strong>What it is:</strong> This shows three plots for selecting
the best model using different criteria: BIC, Validation Set Error, and
Cross-Validation Error.</li>
<li><strong>Why it’s important:</strong> It shows that <strong>different
selection criteria can lead to different “best” models</strong>. Here,
BIC (a mathematical adjustment) picks a 4-variable model, while
validation and CV (direct error estimation) both pick a 6-variable
model.</li>
</ul></li>
</ol>
<p>The slides use the <code>Credit</code> dataset to demonstrate two key
tasks: 1. <strong>Running</strong> different subset selection algorithms
(forward, backward, best). 2. <strong>Using</strong> various statistical
metrics (BIC, <span class="math inline">\(C_p\)</span>, CV error) to
choose the single best model.</p>
<h2 id="comparing-selection-algorithms-the-path">Comparing Selection
Algorithms (The Path)</h2>
<p>This part of the example compares the <em>sequence</em> of models
selected by “Forward Stepwise” selection versus “Best Subset”
selection.</p>
<p><strong>Key Result (from Table 6.1):</strong></p>
<p>This table is the most important result for comparing the
algorithms.</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Variables</th>
<th style="text-align: left;">Best Subset</th>
<th style="text-align: left;">Forward Stepwise</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>one</strong></td>
<td style="text-align: left;"><code>rating</code></td>
<td style="text-align: left;"><code>rating</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>two</strong></td>
<td style="text-align: left;"><code>rating</code>,
<code>income</code></td>
<td style="text-align: left;"><code>rating</code>,
<code>income</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>three</strong></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>four</strong></td>
<td style="text-align: left;"><code>cards</code>, <code>income</code>,
<code>student</code>, <code>limit</code></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code>, <code>limit</code></td>
</tr>
</tbody>
</table>
<p><strong>Summary of this result:</strong></p>
<ul>
<li><strong>Identical for 1, 2, and 3 variables:</strong> Both methods
agree on the best one-variable model (<code>rating</code>), the best
two-variable model (<code>rating</code>, <code>income</code>), and the
best three-variable model (<code>rating</code>, <code>income</code>,
<code>student</code>).</li>
<li><strong>They Diverge at 4 variables:</strong>
<ul>
<li><strong>Forward selection</strong> is <em>greedy</em>. It started
with <code>rating</code>, <code>income</code>, <code>student</code> and
was “stuck” with them. It then added <code>limit</code>, as that was the
best variable to <em>add</em> to its existing 3-variable model.</li>
<li><strong>Best subset selection</strong> is <em>not</em> greedy. It
tests all possible 4-variable combinations. It discovered that the model
<code>cards</code>, <code>income</code>, <code>student</code>,
<code>limit</code> has a slightly lower RSS than the model forward
selection found.</li>
</ul></li>
<li><strong>Main Takeaway:</strong> This demonstrates the limitation of
a greedy algorithm. Forward selection missed the “true” best 4-variable
model because it was locked into its previous choices and couldn’t “swap
out” <code>rating</code> for <code>cards</code>.</li>
</ul>
<h2 id="choosing-the-single-best-model-the-destination">Choosing the
Single Best Model (The Destination)</h2>
<p>This is the most critical part of the analysis. After running a
selection algorithm (like forward, backward, or best subset), you get a
list of the “best” models for each size (best 1-variable, best
2-variable, etc.). Now you must decide: <strong>is the best model the
4-variable one, the 6-variable one, or another?</strong></p>
<p>The slides show several plots to help make this decision, all plotted
against the “Number of Predictors.”</p>
<p><strong>Summary of Plot Results:</strong></p>
<p>Here’s what each plot tells you:</p>
<ul>
<li><strong>Residual Sum of Squares (RSS)</strong> (e.g., in slide
<code>...230014.png</code>, top-left)
<ul>
<li><strong>What it shows:</strong> RSS <em>always</em> decreases as you
add more variables. It drops sharply until 4 variables, then flattens
out.</li>
<li><strong>Conclusion:</strong> This plot is <strong>not useful for
picking the best model</strong> because it will always pick the full
model, which is overfit. It’s only used to see the diminishing returns
of adding new variables.</li>
</ul></li>
<li><strong>Adjusted <span class="math inline">\(R^2\)</span></strong>
(e.g., in slide <code>...230040.png</code>, right)
<ul>
<li><strong>What it shows:</strong> This metric penalizes adding useless
variables. The plot rises quickly, then flattens, peaking at its
<strong>maximum value around 6 or 7 variables</strong>.</li>
<li><strong>Conclusion:</strong> This metric suggests a <strong>6 or
7-variable model</strong>.</li>
</ul></li>
<li><strong>Mallows’ <span class="math inline">\(C_p\)</span></strong>
(e.g., in slide <code>...230040.png</code>, left)
<ul>
<li><strong>What it shows:</strong> This is an estimate of test error.
We want the model with the <strong>minimum <span
class="math inline">\(C_p\)</span></strong>. The plot drops to a low
value at 4 variables and stays low, with its absolute minimum around
<strong>6 or 7 variables</strong>.</li>
<li><strong>Conclusion:</strong> This metric also suggests a <strong>6
or 7-variable model</strong>.</li>
</ul></li>
<li><strong>BIC (Bayesian Information Criterion)</strong> (e.g., in
slide <code>...230040.png</code>, center)
<ul>
<li><strong>What it shows:</strong> This is another estimate of test
error, but it has a <em>stronger penalty</em> for model complexity. The
plot shows a clear “U” shape, reaching its <strong>minimum value at 4
variables</strong> and then <em>increasing</em> afterward.</li>
<li><strong>Conclusion:</strong> This metric strongly suggests a
<strong>4-variable model</strong>.</li>
</ul></li>
<li><strong>Validation Set &amp; Cross-Validation (CV) Error</strong>
(Slide <code>...230045.png</code>)
<ul>
<li><strong>What it shows:</strong> These plots show the <em>direct</em>
estimate of test error (not a mathematical adjustment like BIC or <span
class="math inline">\(C_p\)</span>). Both the validation set error and
the 10-fold CV error show a “U” shape.</li>
<li><strong>Conclusion:</strong> Both methods reach their
<strong>minimum error at 6 variables</strong>. This is considered a very
reliable result.</li>
</ul></li>
</ul>
<h2 id="final-summary-of-results">Final Summary of Results</h2>
<p>The analysis of the <code>Credit</code> dataset reveals two strong
candidates for the “best” model, depending on your goal:</p>
<ol type="1">
<li><p><strong>The 6-Variable Model:</strong> This model is supported by
the <strong>Adjusted <span class="math inline">\(R^2\)</span></strong>,
<strong>Mallows’ <span class="math inline">\(C_p\)</span></strong>, and
(most importantly) the <strong>Validation Set</strong> and
<strong>10-fold Cross-Validation</strong> results. These metrics all
indicate that the 6-variable model has the <strong>lowest prediction
error</strong> on new data.</p></li>
<li><p><strong>The 4-Variable Model:</strong> This model is supported by
<strong>BIC</strong>. Because BIC penalizes complexity more heavily, it
selects a simpler (more <em>parsimonious</em>) model.</p></li>
</ol>
<p><strong>Overall Conclusion:</strong> If your primary goal is
<strong>maximum predictive accuracy</strong>, you should choose the
<strong>6-variable model</strong>. If your goal is a <strong>simpler,
more interpretable model</strong> that is still very good (and avoids
any risk of overfitting), the <strong>4-variable model</strong> is an
excellent choice.</p>
<h1
id="two-main-strategies-for-controlling-model-complexity-in-linear-regression">5.
Two main strategies for controlling model complexity in linear
regression</h1>
<p>This presentation covers two main strategies for controlling model
complexity in linear regression: <strong>Subset Selection</strong>
(choosing <em>which</em> variables to include) and <strong>Shrinkage
Methods</strong> (keeping all variables but <em>reducing the impact</em>
of their coefficients).</p>
<h2 id="subset-selection-1">Subset Selection</h2>
<p>This method involves selecting a subset of the <span
class="math inline">\(p\)</span> total predictors to use in the
model.</p>
<h3 id="key-concepts-formulas">Key Concepts &amp; Formulas</h3>
<ul>
<li><p><strong>The Model:</strong> The standard linear regression model
is represented in matrix form: <span class="math display">\[\mathbf{y} =
\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\]</span> The goal
of subset selection is to find a coefficient vector <span
class="math inline">\(\boldsymbol{\beta}\)</span> that is
<strong>sparse</strong>, meaning it has many zero entries.</p></li>
<li><p><strong>Forward Selection:</strong> This is a <em>greedy
algorithm</em> that starts with an empty model and iteratively adds the
single predictor that most improves the fit.</p></li>
<li><p><strong>Theoretical Guarantee:</strong> Can forward selection
find the <em>true</em> sparse set of variables?</p>
<ul>
<li>Yes, <em>if</em> the predictors are not strongly correlated.</li>
<li>This is quantified by the <strong>Mutual Coherence
Condition</strong>. Assuming the predictors <span
class="math inline">\(\mathbf{x}_i\)</span> are normalized, the method
is guaranteed to work if: <span class="math display">\[\mu = \max_{i
\neq j} |\langle \mathbf{x}_i, \mathbf{x}_j \rangle| &lt; \frac{1}{2s -
1}\]</span> where <span class="math inline">\(s\)</span> is the number
of true non-zero coefficients and <span class="math inline">\(\langle
\mathbf{x}_i, \mathbf{x}_j \rangle\)</span> represents the correlation
between predictors.</li>
</ul></li>
</ul>
<h3 id="practical-application-finding-the-best-model-size">Practical
Application: Finding the Best Model Size</h3>
<p>How do you know whether to choose a model with 3, 4, or 5 variables?
You use <strong>Cross-Validation (CV)</strong>.</p>
<ul>
<li><p><strong>Important Image:</strong> The plot titled “10-fold CV”
(from the first slide) is the most important visual. It plots the
estimated test error (CV Error) on the y-axis against the number of
variables in the model on the x-axis.</p></li>
<li><p><strong>The “One Standard Deviation Rule”:</strong> Looking at
the plot, the error drops sharply and then flattens. The absolute
minimum error might be at 6 variables, but it’s only slightly better
than the 3-variable model.</p>
<ol type="1">
<li>Find the model with the <em>lowest</em> CV error.</li>
<li>Calculate the standard error for that error estimate.</li>
<li>Select the <strong>simplest model</strong> (fewest variables) whose
error is <em>within one standard deviation</em> of the minimum.</li>
<li>This follows <strong>Occam’s razor</strong>: choose the simplest
explanation (model) that fits the data well enough. In the example
given, this rule selects the 3-variable model.</li>
</ol></li>
</ul>
<h3 id="code-interpretation-r-vs.-python">Code Interpretation (R
vs. Python)</h3>
<p>The R code in the first slide performs this 10-fold CV manually for
forward selection:</p>
<ol type="1">
<li>It loops from <code>p = 1</code> to <code>10</code> (model
sizes).</li>
<li>Inside the loop, it identifies the <code>p</code> variables chosen
by a pre-computed forward selection model
(<code>regfit.fwd</code>).</li>
<li>It fits a new model (<code>glm.fit</code>) using <em>only</em> those
<code>p</code> variables.</li>
<li>It runs 10-fold CV (<code>cv.glm</code>) on <em>that specific
model</em> to get its test error.</li>
<li>It stores the error in <code>CV10.err[p]</code>.</li>
<li>Finally, it plots the results.</li>
</ol>
<p><strong>In Python (with <code>scikit-learn</code>):</strong> This
entire process is often automated.</p>
<ul>
<li>You would use <code>sklearn.feature_selection.RFECV</code>
(Recursive Feature Elimination with Cross-Validation).</li>
<li><code>RFECV</code> automatically performs cross-validation to find
the optimal number of features, effectively producing the same plot and
result as the R code.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Conceptual Python equivalent for finding the best model size</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFECV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"></span><br><span class="line"><span class="comment"># X, y = load_your_data()</span></span><br><span class="line">X, y = make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">10</span>, n_informative=<span class="number">3</span>, noise=<span class="number">10</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">estimator = LinearRegression()</span><br><span class="line"><span class="comment"># RFECV will test models with 1 feature, 2 features, etc.,</span></span><br><span class="line"><span class="comment"># and use cross-validation (cv=10) to find the best number.</span></span><br><span class="line">selector = RFECV(estimator, step=<span class="number">1</span>, cv=<span class="number">10</span>, scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>)</span><br><span class="line">selector = selector.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal number of features: <span class="subst">&#123;selector.n_features_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># You can plot selector.cv_results_[&#x27;mean_test_score&#x27;] to get the CV curve</span></span><br></pre></td></tr></table></figure>
<h2 id="shrinkage-methods-regularization">Shrinkage Methods
(Regularization)</h2>
<p>Instead of explicitly removing variables, shrinkage methods keep all
<span class="math inline">\(p\)</span> variables but <em>shrink</em>
their coefficients <span class="math inline">\(\beta_j\)</span> towards
zero.</p>
<h3 id="ridge-regression">Ridge Regression</h3>
<p>Ridge regression is a prime example of a shrinkage method.</p>
<ul>
<li><p><strong>Objective Function:</strong> It finds the coefficients
<span class="math inline">\(\boldsymbol{\beta}\)</span> that minimize a
new quantity: <span class="math display">\[\underbrace{\sum_{i=1}^{n}
(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2}_{\text{RSS (Goodness
of Fit)}} + \underbrace{\lambda \sum_{j=1}^{p}
\beta_j^2}_{\text{$\ell_2$ Penalty (Shrinkage)}}\]</span></p></li>
<li><p><strong>The <span class="math inline">\(\lambda\)</span> Tuning
Parameter:</strong> This parameter controls the strength of the
penalty:</p>
<ul>
<li><strong>If <span class="math inline">\(\lambda =
0\)</span>:</strong> The penalty term disappears. Ridge regression is
identical to standard Ordinary Least Squares (OLS).</li>
<li><strong>If <span class="math inline">\(\lambda \to
\infty\)</span>:</strong> The penalty is “infinitely” strong. To
minimize the function, all coefficients <span
class="math inline">\(\beta_j\)</span> (for <span
class="math inline">\(j=1...p\)</span>) are forced to be zero. The model
becomes an intercept-only model.</li>
<li><strong>Note:</strong> The intercept <span
class="math inline">\(\beta_0\)</span> is <em>not penalized</em>.</li>
</ul></li>
<li><p><strong>The Bias-Variance Trade-off:</strong> This is the core
concept of regularization.</p>
<ul>
<li>Standard OLS has low bias but can have high variance (it
overfits).</li>
<li>Ridge regression adds a <em>small amount of bias</em> (the
coefficients are “wrong” on purpose) to <strong>significantly reduce the
model’s variance</strong>.</li>
<li>This trade-off often leads to a model with a lower overall test
error.</li>
</ul></li>
<li><p><strong>Matrix Solution:</strong> The discussion slide asks “What
is the solution?”. While OLS has the solution <span
class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>, the Ridge
solution is: <span class="math display">\[\hat{\boldsymbol{\beta}}^R =
(\mathbf{X}^T\mathbf{X} + \lambda
\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</span> where <span
class="math inline">\(\mathbf{I}\)</span> is the identity matrix. The
<span class="math inline">\(\lambda \mathbf{I}\)</span> term adds a
“ridge” to the diagonal, making the matrix invertible even if <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is singular (which
happens if <span class="math inline">\(p &gt; n\)</span> or predictors
are collinear).</p></li>
</ul>
<h3 id="an-essential-step-standardization">An Essential Step:
Standardization</h3>
<ul>
<li><strong>Problem:</strong> The <span
class="math inline">\(\ell_2\)</span> penalty <span
class="math inline">\(\lambda \sum \beta_j^2\)</span> is applied equally
to all coefficients. If predictor <span
class="math inline">\(x_1\)</span> (e.g., house size in sq-ft) is on a
much larger scale than <span class="math inline">\(x_2\)</span> (e.g.,
number of rooms), its coefficient <span
class="math inline">\(\beta_1\)</span> will naturally be much smaller
than <span class="math inline">\(\beta_2\)</span>. The penalty will
unfairly punish <span class="math inline">\(\beta_2\)</span> more.</li>
<li><strong>Solution:</strong> You <strong>must standardize</strong>
your inputs <em>before</em> fitting a Ridge model.</li>
<li><strong>Formula:</strong> For each predictor <span
class="math inline">\(X_j\)</span>, all its observations <span
class="math inline">\(x_{ij}\)</span> are rescaled: <span
class="math display">\[\tilde{x}_{ij} = \frac{x_{ij} -
\bar{x}_j}{\sigma_j}\]</span> (where <span
class="math inline">\(\bar{x}_j\)</span> is the mean of the predictor
and <span class="math inline">\(\sigma_j\)</span> is its standard
deviation). This puts all predictors on a common scale (mean=0,
std=1).</li>
</ul>
<p><strong>In Python (with <code>scikit-learn</code>):</strong></p>
<ul>
<li>You use <code>sklearn.preprocessing.StandardScaler</code> to
standardize your data.</li>
<li>You use <code>sklearn.linear_model.Ridge</code> to fit the
model.</li>
<li>You use <code>sklearn.linear_model.RidgeCV</code> to automatically
find the best value for <span class="math inline">\(\lambda\)</span>
(called <code>alpha</code> in scikit-learn) using cross-validation.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Conceptual Python code for Ridge Regression</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># X, y = load_your_data()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a pipeline that first standardizes the data,</span></span><br><span class="line"><span class="comment"># then fits a Ridge model.</span></span><br><span class="line"><span class="comment"># RidgeCV tests a range of alphas (lambdas) automatically.</span></span><br><span class="line">model = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    RidgeCV(alphas=[<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">100.0</span>], scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best alpha (lambda): <span class="subst">&#123;model.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].alpha_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model coefficients: <span class="subst">&#123;model.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].coef_&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="subset-selection-2">Subset Selection</h2>
<p>This section is about choosing <em>which</em> predictors (variables)
to include in your linear model. The main idea is to find a “sparse”
model (one with few variables) that performs well.</p>
<h3 id="the-model-and-the-goal">The Model and The Goal</h3>
<ul>
<li><strong>Slide: “Forward selection in Linear
Regression”</strong></li>
<li><strong>Formula:</strong> The standard linear regression model is
<span class="math inline">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} +
\boldsymbol{\epsilon}\)</span>
<ul>
<li><span class="math inline">\(\mathbf{y}\)</span> is the <span
class="math inline">\(n \times 1\)</span> vector of outcomes.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is the <span
class="math inline">\(n \times (p+1)\)</span> matrix of predictors (with
a leading column of 1s for the intercept).</li>
<li><span class="math inline">\(\boldsymbol{\beta}\)</span> is the <span
class="math inline">\((p+1) \times 1\)</span> vector of coefficients
(<span class="math inline">\(\beta_0, \beta_1, ...,
\beta_p\)</span>).</li>
<li><span class="math inline">\(\boldsymbol{\epsilon}\)</span> is the
<span class="math inline">\(n \times 1\)</span> vector of irreducible
error.</li>
</ul></li>
<li><strong>Key Question:</strong> “If <span
class="math inline">\(\boldsymbol{\beta}\)</span> is sparse with at most
<span class="math inline">\(s\)</span> non-zero entries, can forward
selection find those variables?”
<ul>
<li><strong>Sparse</strong> means most coefficients are zero.</li>
<li><strong>Forward Selection</strong> is a <em>greedy algorithm</em>:
<ol type="1">
<li>Start with no variables.</li>
<li>Add the one variable that gives the best fit.</li>
<li>Add the <em>next</em> best variable to the existing model.</li>
<li>Repeat until you have a model with <span
class="math inline">\(s\)</span> variables.</li>
</ol></li>
<li>The slide suggests the answer is <strong>yes</strong>, but only
under certain conditions.</li>
</ul></li>
</ul>
<h3 id="the-condition-for-success">The Condition for Success</h3>
<ul>
<li><strong>Slide: “Orthogonal Matching Pursuit”</strong></li>
<li><strong>Key Concept:</strong> Forward selection can provably find
the correct variables if those variables are not strongly
correlated.</li>
<li><strong>Formula:</strong> This is formalized by the <strong>Mutual
Coherence Condition</strong>: <span class="math display">\[\mu = \max_{i
\neq j} |\langle \mathbf{x}_i, \mathbf{x}_j \rangle| &lt; \frac{1}{2s -
1}\]</span>
<ul>
<li><strong>What it means:</strong>
<ul>
<li><code>assuming $\mathbf&#123;x&#125;_i$'s are normalized</code> means we’ve
scaled them to have a length of 1.</li>
<li><span class="math inline">\(\langle \mathbf{x}_i, \mathbf{x}_j
\rangle\)</span> is the dot product, which is just their
<strong>correlation</strong> since they are normalized.</li>
<li><span class="math inline">\(\mu\)</span> (mu) is the <strong>largest
absolute correlation</strong> you can find between any two
<em>different</em> predictors.</li>
<li><span class="math inline">\(s\)</span> is the true number of
important variables.</li>
</ul></li>
<li><strong>In English:</strong> If the maximum correlation between any
of your predictors is less than this threshold, the greedy forward
selection algorithm is guaranteed to find the true, sparse set of
variables.</li>
</ul></li>
</ul>
<h3 id="how-to-choose-the-model-size-practice">How to Choose the Model
Size (Practice)</h3>
<p>The theory is nice, but in practice, you don’t know <span
class="math inline">\(s\)</span>. How many variables should you
pick?</p>
<ul>
<li><p><strong>Slide: “10-fold CV Errors”</strong></p></li>
<li><p><strong>This is the most important practical slide for this
section.</strong></p></li>
<li><p><strong>What the plot shows:</strong></p>
<ul>
<li><strong>X-axis:</strong> “Number of Variables” (from 1 to 10).</li>
<li><strong>Y-axis:</strong> “CV Error” (the 10-fold cross-validated
Mean Squared Error).</li>
<li><strong>The Curve:</strong> The error drops very fast as we add the
first 2-3 variables. Then, it flattens out. Adding more than 3 variables
doesn’t really help much.</li>
</ul></li>
<li><p><strong>Slide: “The one standard deviation
rule”</strong></p></li>
<li><p>This rule helps you pick the “best” model from the CV plot.</p>
<ol type="1">
<li>Find the model with the absolute <em>minimum</em> CV error (in the
plot, this looks to be around 6 or 7 variables).</li>
<li>Calculate the standard error of that minimum CV error.</li>
<li>Draw a “tolerance” line at
<code>(minimum error) + (one standard error)</code>.</li>
<li>Choose the <strong>simplest model</strong> (fewest variables) whose
CV error is <em>below</em> this tolerance line.</li>
</ol>
<!-- end list -->
<ul>
<li>The slide states this rule “gives the model with 3 variable” for
this example. This is because the 3-variable model is much simpler than
the 6-variable one, and its error is “good enough” (within one standard
deviation of the minimum). This is an application of <strong>Occam’s
razor</strong>.</li>
</ul></li>
</ul>
<h3 id="code-r-vs.-python">Code: R vs. Python</h3>
<p>The R code on the “10-fold CV Errors” slide generates that exact
plot.</p>
<ul>
<li><p><strong>R Code Explained:</strong></p>
<ul>
<li><code>library(boot)</code>: Loads the cross-validation library.</li>
<li><code>CV10.err=rep(0,10)</code>: Creates an empty vector to store
the 10 error scores.</li>
<li><code>for(p in 1:10)</code>: A loop that will test model sizes from
1 to 10.</li>
<li><code>x&lt;-which(summary(regfit.fwd)$which[p,])</code>: Gets the
<em>names</em> of the <span class="math inline">\(p\)</span> variables
chosen by a pre-run forward selection (<code>regfit.fwd</code>).</li>
<li><code>glm.fit=glm(Balance~.,data=newCred)</code>: Fits a model using
<em>only</em> those <span class="math inline">\(p\)</span>
variables.</li>
<li><code>cv.err=cv.glm(newCred,glm.fit,K=10)</code>: Performs 10-fold
CV on <em>that specific <span class="math inline">\(p\)</span>-variable
model</em>.</li>
<li><code>CV10.err[p]&lt;-cv.err$delta[1]</code>: Stores the CV
error.</li>
<li><code>plot(...)</code>: Plots the 10 errors against the 10 model
sizes.</li>
</ul></li>
<li><p><strong>Python Equivalent (Conceptual):</strong></p>
<ul>
<li>In <code>scikit-learn</code>, this process is often automated. You
wouldn’t write the CV loop yourself.</li>
<li>You would use <code>sklearn.feature_selection.RFECV</code>
(Recursive Feature Elimination with Cross-Validation). This tool
automatically wraps a model (like <code>LinearRegression</code>),
performs cross-validation, and finds the optimal number of features,
effectively producing the same plot and result.</li>
</ul></li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- Python equivalent for 6.1 ---</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFECV</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="comment"># Assume X and y are your data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create a pipeline</span></span><br><span class="line"><span class="comment"># (Note: It&#x27;s good practice to scale, even for OLS, if you&#x27;re comparing)</span></span><br><span class="line">pipeline = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    LinearRegression()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create the RFECV (Recursive Feature Elimination w/ CV) object</span></span><br><span class="line"><span class="comment"># This is an *alternative* to forward selection, but serves the same purpose</span></span><br><span class="line"><span class="comment"># It will test models with 1, 2, 3... features using 10-fold CV</span></span><br><span class="line">feature_selector = RFECV(</span><br><span class="line">    estimator=pipeline, </span><br><span class="line">    min_features_to_select=<span class="number">1</span>, </span><br><span class="line">    step=<span class="number">1</span>, </span><br><span class="line">    cv=<span class="number">10</span>, </span><br><span class="line">    scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span> <span class="comment"># We want to minimize error</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit it</span></span><br><span class="line">feature_selector.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal number of features found: <span class="subst">&#123;feature_selector.n_features_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You could then plot feature_selector.cv_results_[&#x27;mean_test_score&#x27;]</span></span><br><span class="line"><span class="comment"># to replicate the R plot.</span></span><br></pre></td></tr></table></figure>
<h2 id="shrinkage-methods-by-regularization">Shrinkage Methods by
Regularization</h2>
<p>This is a different approach. Instead of <em>removing</em> variables,
we keep all <span class="math inline">\(p\)</span> variables but
<em>shrink</em> their coefficients <span
class="math inline">\(\beta_j\)</span> towards 0.</p>
<h3 id="ridge-regression-the-core-idea">Ridge Regression: The Core
Idea</h3>
<ul>
<li><strong>Slide: “Ridge regression”</strong></li>
<li><strong>Formula:</strong> Ridge regression minimizes a new objective
function: <span class="math display">\[\min_{\boldsymbol{\beta}} \left(
\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 +
\lambda \sum_{j=1}^{p} \beta_j^2 \right)\]</span>
<ul>
<li><strong>Term 1: <span class="math inline">\(\text{RSS}\)</span>
(Residual Sum of Squares).</strong> This is the original OLS “goodness
of fit” term. We want this to be small.</li>
<li><strong>Term 2: <span class="math inline">\(\lambda \sum
\beta_j^2\)</span>.</strong> This is the <strong><span
class="math inline">\(\ell_2\)</span> penalty</strong> or “shrinkage
penalty”. It adds a “cost” for having large coefficients.</li>
</ul></li>
<li><strong>The <span class="math inline">\(\lambda\)</span> (lambda)
Parameter:</strong>
<ul>
<li>This is the <strong>tuning parameter</strong> that controls the
trade-off between fit and simplicity.</li>
<li><code>$\lambda = 0$</code>: No penalty. The objective is just to
minimize RSS. The solution <span
class="math inline">\(\hat{\boldsymbol{\beta}}^R\)</span> is identical
to the OLS solution <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</li>
<li><code>$\lambda = \infty$</code>: Infinite penalty. The only way to
minimize the cost is to make all <span class="math inline">\(\beta_j =
0\)</span> (for <span class="math inline">\(j \ge 1\)</span>). The model
becomes an intercept-only model.</li>
<li><code>Large $\lambda$</code>: Heavy penalty, more shrinkage.</li>
<li><strong>Crucial Note:</strong> The intercept <span
class="math inline">\(\beta_0\)</span> is <strong>not
penalized</strong>. This is because <span
class="math inline">\(\beta_0\)</span> just represents the mean of <span
class="math inline">\(y\)</span> when all <span
class="math inline">\(x\)</span>’s are 0; shrinking it makes no
sense.</li>
</ul></li>
</ul>
<h3 id="the-need-for-standardization">The Need for Standardization</h3>
<ul>
<li><strong>Slide: “Standardize the inputs”</strong></li>
<li><strong>Problem:</strong> The penalty <span
class="math inline">\(\lambda \sum \beta_j^2\)</span> is applied to all
coefficients. But what if <span class="math inline">\(x_1\)</span> is
“house size in sq-ft” (values 1000-5000) and <span
class="math inline">\(x_2\)</span> is “number of bedrooms” (values 1-5)?
<ul>
<li>The coefficient <span class="math inline">\(\beta_1\)</span> for
house size will naturally be <em>tiny</em>, while the coefficient <span
class="math inline">\(\beta_2\)</span> for bedrooms will be
<em>large</em>, even if they are equally important.</li>
<li>Ridge regression would unfairly and heavily penalize <span
class="math inline">\(\beta_2\)</span> while barely touching <span
class="math inline">\(\beta_1\)</span>.</li>
</ul></li>
<li><strong>Solution:</strong> You <strong>must</strong> standardize all
predictors <em>before</em> fitting a Ridge model.</li>
<li><strong>Formula:</strong> For each observation <span
class="math inline">\(i\)</span> of each predictor <span
class="math inline">\(j\)</span>: <span
class="math display">\[\tilde{x}_{ij} = \frac{x_{ij} -
\bar{x}_j}{\sqrt{(1/n) \sum_{i=1}^{n} (x_{ij} - \bar{x}_j)^2}}\]</span>
<ul>
<li>This formula rescales every predictor to have a mean of 0 and a
standard deviation of 1.</li>
<li>Now, all coefficients <span class="math inline">\(\beta_j\)</span>
are on a “level playing field” and can be penalized fairly.</li>
</ul></li>
</ul>
<h3 id="answering-the-discussion-questions">Answering the Discussion
Questions</h3>
<ul>
<li><strong>Slide: “DISCUSSION”</strong>
<ul>
<li><code>What is the solution of Ridge regression?</code></li>
<li><code>What is the bias and the variance?</code></li>
</ul></li>
</ul>
<h4 id="what-is-the-solution-of-ridge-regression">1. What is the
solution of Ridge regression?</h4>
<p>The solution can be written in matrix form, which is very
elegant.</p>
<ul>
<li><p><strong>Standard OLS Solution:</strong> The coefficients <span
class="math inline">\(\hat{\boldsymbol{\beta}}^{\text{OLS}}\)</span>
that minimize RSS are found by: <span
class="math display">\[\hat{\boldsymbol{\beta}}^{\text{OLS}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p></li>
<li><p><strong>Ridge Regression Solution:</strong> The coefficients
<span class="math inline">\(\hat{\boldsymbol{\beta}}^{R}\)</span> that
minimize the Ridge objective are: <span
class="math display">\[\hat{\boldsymbol{\beta}}^{R} =
(\mathbf{X}^T\mathbf{X} + \lambda
\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><strong>Explanation:</strong>
<ul>
<li><span class="math inline">\(\mathbf{I}\)</span> is the
<strong>identity matrix</strong> (a matrix of 1s on the diagonal, 0s
everywhere else).</li>
<li>By adding <span class="math inline">\(\lambda\mathbf{I}\)</span>, we
are adding a positive value <span class="math inline">\(\lambda\)</span>
to the <em>diagonal</em> of the <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> matrix.</li>
<li>This addition <strong>stabilizes</strong> the matrix. <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> might not be
invertible (if <span class="math inline">\(p &gt; n\)</span> or if
predictors are perfectly collinear), but <span
class="math inline">\((\mathbf{X}^T\mathbf{X} + \lambda
\mathbf{I})\)</span> is <em>always</em> invertible for <span
class="math inline">\(\lambda &gt; 0\)</span>.</li>
<li>This addition is what mathematically “shrinks” the coefficients
toward zero.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="what-is-the-bias-and-the-variance">2. What is the bias and the
variance?</h4>
<p>This is the <strong>most important concept</strong> in
regularization. It’s the <strong>bias-variance trade-off</strong>.</p>
<ul>
<li><p><strong>Standard OLS (where <span
class="math inline">\(\lambda=0\)</span>):</strong></p>
<ul>
<li><strong>Bias: Low.</strong> The OLS estimator is
<strong>unbiased</strong>, meaning that if you took many samples and fit
many OLS models, their average <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> would be the
<em>true</em> <span
class="math inline">\(\boldsymbol{\beta}\)</span>.</li>
<li><strong>Variance: High.</strong> The OLS solution can be
<em>highly</em> sensitive to the training data. If you change a few data
points, the coefficients can swing wildly. This is especially true if
<span class="math inline">\(p\)</span> is large or predictors are
correlated. This “sensitivity” is high variance, which leads to
<strong>overfitting</strong>.</li>
</ul></li>
<li><p><strong>Ridge Regression (where <span
class="math inline">\(\lambda &gt; 0\)</span>):</strong></p>
<ul>
<li><strong>Bias: High(er).</strong> Ridge regression is a
<strong>biased</strong> estimator. By adding the penalty, we are
<em>purposefully</em> pulling the coefficients away from the OLS
solution and towards zero. The average <span
class="math inline">\(\hat{\boldsymbol{\beta}}^R\)</span> from many
samples will <em>not</em> equal the true <span
class="math inline">\(\boldsymbol{\beta}\)</span>. We have
<em>introduced</em> bias into our model.</li>
<li><strong>Variance: Low(er).</strong> In exchange for this bias, we
get a massive <em>reduction in variance</em>. The <span
class="math inline">\(\lambda\mathbf{I}\)</span> term stabilizes the
solution. The coefficients won’t change wildly even if the training data
changes. The model is more robust and less sensitive.</li>
</ul></li>
</ul>
<p><strong>The Trade-off:</strong> The total expected test error of a
model is: <span class="math inline">\(\text{Error} = \text{Bias}^2 +
\text{Variance} + \text{Irreducible Error}\)</span></p>
<p>By using Ridge regression, we <em>increase</em> the <span
class="math inline">\(\text{Bias}^2\)</span> term a little, but we
<em>decrease</em> the <span
class="math inline">\(\text{Variance}\)</span> term a lot. The goal is
to find a <span class="math inline">\(\lambda\)</span> where the
<em>total error</em> is minimized. Ridge regression reduces variance
<em>at the cost of</em> increased bias.</p>
<h3 id="python-equivalent-for-6.2">Python Equivalent for 6.2</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- Python equivalent for 6.2 ---</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="comment"># Assume X and y are your data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create a pipeline that AUTOMATICALLY</span></span><br><span class="line"><span class="comment">#    - Standardizes the data</span></span><br><span class="line"><span class="comment">#    - Fits a Ridge Regression model</span></span><br><span class="line"><span class="comment">#    - Uses Cross-Validation to find the BEST lambda (alpha in scikit-learn)</span></span><br><span class="line">alphas_to_test = [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">100.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># RidgeCV handles everything for us</span></span><br><span class="line">pipeline = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    RidgeCV(alphas=alphas_to_test, scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>, cv=<span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit the pipeline</span></span><br><span class="line">pipeline.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Get the results</span></span><br><span class="line">best_lambda = pipeline.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].alpha_</span><br><span class="line">ridge_coefficients = pipeline.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].coef_</span><br><span class="line">intercept = pipeline.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].intercept_</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha) found by CV: <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model intercept (beta_0): <span class="subst">&#123;intercept&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model coefficients (beta_j): <span class="subst">&#123;ridge_coefficients&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="the-why-of-ridge-regression">6. The “Why” of Ridge
Regression</h1>
<h2 id="core-concepts-the-why-of-ridge-regression">Core Concepts: The
“Why” of Ridge Regression</h2>
<p>Your slides explain that ridge regression is a “shrinkage method”
designed to solve a major problem with standard Ordinary Least Squares
(OLS) regression: <strong>high variance</strong>.</p>
<h3 id="the-bias-variance-tradeoff-slide-3">The Bias-Variance Tradeoff
(Slide 3)</h3>
<p>This is the most important theoretical concept. In prediction, the
total error (Mean Squared Error, or MSE) of a model is composed of three
parts: <span class="math inline">\(\text{Error} = \text{Variance} +
\text{Bias}^2 + \text{Irreducible Error}\)</span></p>
<ul>
<li><strong>Ordinary Least Squares (OLS):</strong> Aims to be unbiased
(low bias). However, when you have many predictors (<span
class="math inline">\(p\)</span>), especially if they are correlated, or
if <span class="math inline">\(p\)</span> is large compared to the
number of samples <span class="math inline">\(n\)</span> (<span
class="math inline">\(p \approx n\)</span> or <span
class="math inline">\(p &gt; n\)</span>), the OLS model becomes highly
<em>unstable</em>. A small change in the training data can cause the
coefficients to change wildly. This is <strong>high variance</strong>.
(See Slide 6, “Remarks”).</li>
<li><strong>Ridge Regression:</strong> By adding a penalty, ridge
<em>intentionally</em> introduces a small amount of
<strong>bias</strong> (it pulls coefficients away from their “true” OLS
values). In return, it achieves a <em>massive</em> reduction in
<strong>variance</strong>.</li>
</ul>
<p>As <strong>Slide 3</strong> shows:</p>
<ul>
<li>The <strong>green line (Variance)</strong> starts very high for low
<span class="math inline">\(\lambda\)</span> (left side) and drops
quickly.</li>
<li>The <strong>black line (Squared Bias)</strong> starts at zero (for
OLS at <span class="math inline">\(\lambda=0\)</span>) and slowly
increases as <span class="math inline">\(\lambda\)</span> grows.</li>
<li>The <strong>purple line (Test MSE)</strong> is the sum of the two.
It’s U-shaped. The goal of ridge is to find the <span
class="math inline">\(\lambda\)</span> (marked by the ‘x’) at the
<em>bottom</em> of this “U,” which gives the lowest possible total
error.</li>
</ul>
<h3 id="why-is-it-called-ridge-the-3d-spatial-meaning-slide-5">Why Is It
Called “Ridge”? The 3D Spatial Meaning (Slide 5)</h3>
<p>This slide explains the problem of <strong>collinearity</strong> and
the origin of the name.</p>
<ul>
<li><strong>Left Plot (Least Squares):</strong> Imagine a model with two
correlated predictors, <span class="math inline">\(\beta_1\)</span> and
<span class="math inline">\(\beta_2\)</span>. The y-axis (SS1) is the
error (RSS). Because the predictors are correlated, there isn’t one
single “point” that is the minimum. Instead, there’s a long, flat
<em>valley</em> or <em>trough</em> (marked “unstable”). Many different
combinations of <span class="math inline">\(\beta_1\)</span> and <span
class="math inline">\(\beta_2\)</span> along this valley give a
similarly low error. The OLS solution is unstable because it can pick
<em>any</em> point in this flat-bottomed valley.</li>
<li><strong>Right Plot (Ridge):</strong> The ridge objective function
adds a penalty term: <span class="math inline">\(\lambda(\beta_1^2 +
\beta_2^2)\)</span>. This penalty term, by itself, is a perfect circular
bowl centered at (0,0). When you add this “bowl” to the OLS “valley,” it
<em>stabilizes</em> the function. It pulls the minimum towards (0,0) and
creates a single, stable, well-defined minimum.</li>
<li><strong>The “Ridge” Name:</strong> The penalty <span
class="math inline">\(\lambda\mathbf{I}\)</span> (from the matrix
formula) adds a “ridge” of values to the diagonal of the <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> matrix, which
geometrically turns the unstable flat valley into a stable bowl.</li>
</ul>
<h2 id="mathematical-formulas">Mathematical Formulas</h2>
<p>The key difference between OLS and Ridge is the function they try to
minimize.</p>
<ol type="1">
<li><p><strong>OLS Objective Function:</strong> Minimize the Residual
Sum of Squares (RSS). <span class="math display">\[\text{RSS} =
\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}
\right)^2\]</span></p></li>
<li><p><strong>Ridge Objective Function (Slide 6):</strong> Minimize the
RSS <em>plus</em> an L2 penalty term. <span
class="math display">\[\text{Minimize: } \left[ \sum_{i=1}^{n} \left(
y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 \right] +
\lambda \sum_{j=1}^{p} \beta_j^2\]</span></p>
<ul>
<li><span class="math inline">\(\lambda\)</span> is the <strong>tuning
parameter</strong> controlling the penalty strength.</li>
<li><span class="math inline">\(\sum_{j=1}^{p} \beta_j^2\)</span> is the
<strong>L2-norm</strong> (squared) of the coefficients. It penalizes
large coefficients.</li>
</ul></li>
<li><p><strong>L2 Norm (Slide 1):</strong> The L2 norm of a vector <span
class="math inline">\(\mathbf{a}\)</span> is its standard Euclidean
length. The plot on Slide 1 uses this to show the <em>total
magnitude</em> of the ridge coefficients. <span
class="math display">\[\|\mathbf{a}\|_2 = \sqrt{\sum_{j=1}^p
a_j^2}\]</span></p></li>
<li><p><strong>Matrix Solution (Slide 6):</strong> This is the
“closed-form” solution for the ridge coefficients <span
class="math inline">\(\hat{\beta}^R\)</span>. <span
class="math display">\[\hat{\beta}^R = (\mathbf{X}^T\mathbf{X} +
\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{I}\)</span> is the identity
matrix.</li>
<li>The term <span class="math inline">\(\lambda\mathbf{I}\)</span> is
what stabilizes the <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> matrix, making it
invertible even if it’s singular (due to <span class="math inline">\(p
&gt; n\)</span> or collinearity).</li>
</ul></li>
</ol>
<h2 id="walkthrough-of-the-credit-data-example-all-slides">Walkthrough
of the “Credit Data” Example (All Slides)</h2>
<p>Here is the logical story of the R code, from start to finish.</p>
<h3 id="step-1-data-preparation-slide-8">Step 1: Data Preparation (Slide
8)</h3>
<ul>
<li><code>x=scale(model.matrix(Balance~., Credit)[,-1])</code>
<ul>
<li><code>model.matrix(...)</code> creates the predictor matrix
<code>x</code>.</li>
<li><code>scale(...)</code> is <strong>critically important</strong>. It
standardizes all predictors to have a mean of 0 and a standard deviation
of 1. This is necessary because the ridge penalty <span
class="math inline">\(\lambda \sum \beta_j^2\)</span> is
<em>unit-dependent</em>. If <code>Income</code> (in 10,000s) and
<code>Cards</code> (1-10) were unscaled, the penalty would unfairly
crush the <code>Income</code> coefficient. Scaling puts all predictors
on a level playing field.</li>
</ul></li>
<li><code>y=Credit$Balance</code>
<ul>
<li>This sets the <code>y</code> (target) variable.</li>
</ul></li>
</ul>
<h3 id="step-2-fit-the-ridge-model-slide-8">Step 2: Fit the Ridge Model
(Slide 8)</h3>
<ul>
<li><code>grid=10^seq(4,-2,length=100)</code>
<ul>
<li>This creates a <em>grid</em> of 100 <span
class="math inline">\(\lambda\)</span> values to test, ranging from
<span class="math inline">\(10^4\)</span> (a huge penalty) down to <span
class="math inline">\(10^{-2}\)</span> (a tiny penalty).</li>
</ul></li>
<li><code>ridge.mod=glmnet(x,y,alpha=0,lambda=grid)</code>
<ul>
<li>This is the main command. It fits a <em>separate</em> ridge model
for <em>every single <span class="math inline">\(\lambda\)</span></em>
in the <code>grid</code>.</li>
<li><code>alpha=0</code> is the specific command that tells
<code>glmnet</code> to perform <strong>Ridge Regression</strong>.
(Setting <code>alpha=1</code> would be LASSO).</li>
</ul></li>
<li><code>coef(ridge.mod)[,50]</code>
<ul>
<li>This inspects the model. It pulls out the vector of coefficients for
the 50th <span class="math inline">\(\lambda\)</span> in the grid (which
is <span class="math inline">\(\lambda=10.72\)</span>).</li>
</ul></li>
</ul>
<h3
id="step-3-visualize-the-coefficient-solution-path-slides-1-4-9">Step 3:
Visualize the Coefficient “Solution Path” (Slides 1, 4, 9)</h3>
<p>These plots all show the same thing: how the coefficients change as
<span class="math inline">\(\lambda\)</span> changes.</p>
<ul>
<li><strong>Slide 9 Plot:</strong> This plots the standardized
coefficients for 4 predictors (<code>Income</code>, <code>Limit</code>,
<code>Rating</code>, <code>Student</code>) against the <em>index</em> (1
to 100). Index 1 (left) is the largest <span
class="math inline">\(\lambda\)</span>, and index 100 (right) is the
smallest <span class="math inline">\(\lambda\)</span> (closest to OLS).
You can see the coefficients “grow” from 0 as the penalty (<span
class="math inline">\(\lambda\)</span>) gets smaller.</li>
<li><strong>Slide 1 (Left Plot):</strong> This is the <em>same plot</em>
as Slide 9, but more professional. It plots the coefficients against
<span class="math inline">\(\lambda\)</span> on a log scale. You can
clearly see all coefficients (gray lines) being “shrunk” toward zero as
<span class="math inline">\(\lambda\)</span> increases (moves right).
The key predictors (<code>Income</code>, <code>Rating</code>, etc.) are
highlighted.</li>
<li><strong>Slide 1 (Right Plot):</strong> This is the <em>exact same
data</em> again, but with a different x-axis: <span
class="math inline">\(\|\hat{\beta}_\lambda^R\|_2 /
\|\hat{\beta}\|_2\)</span>.
<ul>
<li><strong>1.0</strong> on the right means <span
class="math inline">\(\lambda=0\)</span>. The ratio of the ridge norm to
the OLS norm is 1 (they are the same).</li>
<li><strong>0.0</strong> on the left means <span
class="math inline">\(\lambda=\infty\)</span>. The ridge coefficients
are all 0, so their norm is 0.</li>
<li>This axis shows the “fraction” of the full OLS coefficient magnitude
that the model is using.</li>
</ul></li>
<li><strong>Slide 4 Plot:</strong> This plots the <em>total L2 norm</em>
of <em>all</em> coefficients (<span
class="math inline">\(\|\hat{\beta}_\lambda^R\|_2\)</span>) against the
index. As the index goes from 1 to 100 (i.e., <span
class="math inline">\(\lambda\)</span> gets smaller), the total
magnitude of the coefficients gets larger, which is exactly what we
expect.</li>
</ul>
<h3
id="step-4-find-the-best-lambda-using-cross-validation-slides-4-7">Step
4: Find the <em>Best</em> <span class="math inline">\(\lambda\)</span>
using Cross-Validation (Slides 4 &amp; 7)</h3>
<p>We have 100 models. Which one is best?</p>
<ul>
<li><p><strong>The “Manual” Way (Slide 4):</strong></p>
<ul>
<li>The code splits the data into a <code>train</code> and
<code>test</code> set.</li>
<li>It fits a model <em>only</em> on the <code>train</code> set.</li>
<li>It tests two <span class="math inline">\(\lambda\)</span> values:
<ul>
<li><code>s=4</code>: Gives a test MSE of <code>10293.33</code>.</li>
<li><code>s=10</code>: Gives a test MSE of <code>168981.1</code> (much
worse!).</li>
</ul></li>
<li>This shows that <span class="math inline">\(\lambda=4\)</span> is
better than <span class="math inline">\(\lambda=10\)</span>, but we
don’t know if it’s the <em>best</em>.</li>
</ul></li>
<li><p><strong>The “Automatic” Way (Slide 7):</strong></p>
<ul>
<li><code>cv.out=cv.glmnet(x[train,], y[train], alpha=0)</code></li>
<li>This runs <strong>10-fold Cross-Validation</strong> on the training
set. It automatically splits the training set into 10 “folds,” trains on
9, tests on 1, and repeats this 10 times for <em>every <span
class="math inline">\(\lambda\)</span></em>.</li>
<li><strong>The Plot:</strong> The plot on this slide is the result. It
shows the average MSE (y-axis) for each <span
class="math inline">\(\log(\lambda)\)</span> (x-axis). This is the
<em>real-data version</em> of the theoretical purple curve from Slide
3.</li>
<li><code>bestlam=cv.out$lambda.min</code></li>
<li>This command finds the <span class="math inline">\(\lambda\)</span>
at the <em>very bottom</em> of the U-shaped curve. The output shows
<code>bestlam</code> is <strong>41.6</strong>.</li>
<li><code>ridge.pred=predict(ridge.mod, s=bestlam, newx=x[test,])</code></li>
<li>Now, we use this <em>one best <span
class="math inline">\(\lambda\)</span></em> to make predictions on our
held-out <code>test</code> set.</li>
<li><code>mean((ridge.pred-y.test)^2)</code></li>
<li>The final, reliable test MSE is <strong>16129.68</strong>. This is
our best estimate of how the model will perform on new, unseen
data.</li>
</ul></li>
</ul>
<h2 id="python-scikit-learn-equivalents">Python
(<code>scikit-learn</code>) Equivalents</h2>
<p>Here is how you would perform the entire R workflow from your slides
in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. Load and Prepare Data (like Slide 8) ---</span></span><br><span class="line"><span class="comment"># Assuming &#x27;Credit&#x27; is a pandas DataFrame</span></span><br><span class="line"><span class="comment"># X = Credit.drop(&#x27;Balance&#x27;, axis=1)</span></span><br><span class="line"><span class="comment"># y = Credit[&#x27;Balance&#x27;]</span></span><br><span class="line"><span class="comment"># ... (need to handle categorical variables first, e.g., with pd.get_dummies) ...</span></span><br><span class="line"><span class="comment"># For this example, let&#x27;s assume X and y are already loaded and numeric.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the predictors (CRITICAL)</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. Train/Test Split (like Slide 4) ---</span></span><br><span class="line"><span class="comment"># test_size=0.5 and random_state=1 mimic the R code</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X_scaled, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 3. Find Best Lambda (alpha) with Cross-Validation (like Slide 7) ---</span></span><br><span class="line"><span class="comment"># Create the same log-spaced grid of lambdas (sklearn calls it &#x27;alpha&#x27;)</span></span><br><span class="line">lambda_grid = np.logspace(<span class="number">4</span>, -<span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># RidgeCV performs cross-validation to find the best alpha</span></span><br><span class="line"><span class="comment"># cv=10 matches the 10-fold CV</span></span><br><span class="line"><span class="comment"># store_cv_values=True is needed to plot the CV error curve</span></span><br><span class="line">cv_model = RidgeCV(alphas=lambda_grid, store_cv_values=<span class="literal">True</span>, cv=<span class="number">10</span>)</span><br><span class="line">cv_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best lambda found</span></span><br><span class="line">best_lambda = cv_model.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha) found by CV: <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the CV error curve (like Slide 7 plot)</span></span><br><span class="line"><span class="comment"># cv_model.cv_values_ has shape (n_samples, n_alphas)</span></span><br><span class="line"><span class="comment"># We need to average over the samples for each alpha</span></span><br><span class="line">mse_path = np.mean(cv_model.cv_values_, axis=<span class="number">0</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(np.log10(cv_model.alphas_), mse_path, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Log(lambda)&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Mean Squared Error&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Cross-Validation Error Path&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 4. Evaluate on Test Set (like Slide 7) ---</span></span><br><span class="line"><span class="comment"># &#x27;cv_model&#x27; is already refit on the full training set using the best_lambda</span></span><br><span class="line">test_pred = cv_model.predict(X_test)</span><br><span class="line">final_test_mse = mean_squared_error(y_test, test_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Final Test MSE with best lambda: <span class="subst">&#123;final_test_mse&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 5. Get Final Coefficients (like Slide 7, bottom) ---</span></span><br><span class="line"><span class="comment"># The coefficients from the CV-trained model:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Intercept: <span class="subst">&#123;cv_model.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Coefficients:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> coef, feature <span class="keyword">in</span> <span class="built_in">zip</span>(cv_model.coef_, X.columns):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;  <span class="subst">&#123;feature&#125;</span>: <span class="subst">&#123;coef&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 6. Plot the Solution Path (like Slide 1) ---</span></span><br><span class="line"><span class="comment"># To do this, we fit a Ridge model for each lambda and store the coefficients</span></span><br><span class="line">coefs = []</span><br><span class="line"><span class="keyword">for</span> lam <span class="keyword">in</span> lambda_grid:</span><br><span class="line">    model = Ridge(alpha=lam)</span><br><span class="line">    model.fit(X_scaled, y)  <span class="comment"># Fit on all data</span></span><br><span class="line">    coefs.append(model.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(np.log10(lambda_grid), coefs)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Log(lambda)&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Standardized Coefficients&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Ridge Solution Path&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="shrinkage-methods-regularization-1">7. Shrinkage Methods
(Regularization)</h1>
<p>These slides cover <strong>Shrinkage Methods</strong>, also known as
<strong>Regularization</strong>, which are techniques used to improve on
the standard least squares model, particularly when dealing with many
variables or multicollinearity. The main focus is on
<strong>LASSO</strong> regression.</p>
<h2 id="key-mathematical-formulas">Key Mathematical Formulas</h2>
<p>The slides present two main, but equivalent, ways to formulate these
methods.</p>
<h3 id="penalized-formulation-slide-1">1. Penalized Formulation (Slide
1)</h3>
<p>This is the most common formulation. The goal is to minimize a
function that is a combination of the <strong>Residual Sum of Squares
(RSS)</strong> and a <strong>penalty term</strong>. The penalty
discourages large coefficients.</p>
<ul>
<li><strong>LASSO (Least Absolute Shrinkage and Selection
Operator):</strong> The goal is to find coefficients (<span
class="math inline">\(\beta_0, \beta_j\)</span>) that minimize: <span
class="math display">\[\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p}
\beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j|\]</span>
<ul>
<li><strong>Penalty:</strong> The <span
class="math inline">\(L_1\)</span> norm (<span
class="math inline">\(\|\beta\|_1\)</span>), which is the sum of the
<em>absolute values</em> of the coefficients.</li>
<li><strong>Key Property:</strong> This penalty can force some
coefficients to be <strong>exactly zero</strong>, effectively performing
automatic variable selection.</li>
</ul></li>
</ul>
<h3 id="constrained-formulation-slide-2">2. Constrained Formulation
(Slide 2)</h3>
<p>This alternative formulation minimizes the RSS <em>subject to a
constraint</em> (a “budget”) on the size of the coefficients.</p>
<ul>
<li><p><strong>For Lasso:</strong> Minimize RSS subject to: <span
class="math display">\[\sum_{j=1}^{p} |\beta_j| \le s\]</span> (The sum
of the absolute values of the coefficients must be less than some budget
<span class="math inline">\(s\)</span>.)</p></li>
<li><p><strong>For Ridge:</strong> Minimize RSS subject to: <span
class="math display">\[\sum_{j=1}^{p} \beta_j^2 \le s\]</span> (The sum
of the <em>squares</em> of the coefficients (<span
class="math inline">\(L_2\)</span> norm) must be less than <span
class="math inline">\(s\)</span>.)</p></li>
</ul>
<p><strong>Equivalence (Slide 3):</strong> For any penalty value <span
class="math inline">\(\lambda\)</span> used in the first formulation,
there is a corresponding budget <span class="math inline">\(s\)</span>
in the second formulation that will give the exact same set of
coefficients. <span class="math inline">\(\lambda\)</span> and <span
class="math inline">\(s\)</span> are inversely related: a large <span
class="math inline">\(\lambda\)</span> (high penalty) corresponds to a
small <span class="math inline">\(s\)</span> (small budget).</p>
<h2 id="important-plots-and-interpretation">Important Plots and
Interpretation</h2>
<p>Your slides show the two most important plots for understanding and
using LASSO.</p>
<h3 id="the-cross-validation-cv-plot-slide-5">1. The Cross-Validation
(CV) Plot (Slide 5)</h3>
<p>This plot is crucial for <strong>choosing the best tuning parameter
(<span class="math inline">\(\lambda\)</span>)</strong>.</p>
<ul>
<li><strong>X-axis:</strong> <span
class="math inline">\(\text{Log}(\lambda)\)</span>. This is the penalty
strength.
<ul>
<li><strong>Right side (high <span
class="math inline">\(\lambda\)</span>):</strong> High penalty, simple
model (many coefficients are 0), high bias, high Mean-Squared Error
(MSE).</li>
<li><strong>Left side (low <span
class="math inline">\(\lambda\)</span>):</strong> Low penalty, complex
model (like standard linear regression), high variance, MSE starts to
increase (overfitting).</li>
</ul></li>
<li><strong>Y-axis:</strong> Mean-Squared Error (MSE) from
cross-validation.</li>
<li><strong>Goal:</strong> Find the <span
class="math inline">\(\lambda\)</span> at the <strong>bottom of the “U”
shape</strong>, which gives the <em>lowest</em> MSE. This is the optimal
trade-off between bias and variance. The top axis shows how many
variables are included in the model at each <span
class="math inline">\(\lambda\)</span>.</li>
</ul>
<h3 id="the-coefficient-path-plot-slide-6">2. The Coefficient Path Plot
(Slide 6)</h3>
<p>This plot is the best visualization for <strong>understanding what
LASSO does</strong>.</p>
<ul>
<li><strong>Left Plot (vs. <span
class="math inline">\(\lambda\)</span>):</strong>
<ul>
<li><strong>X-axis:</strong> The penalty strength <span
class="math inline">\(\lambda\)</span>.</li>
<li><strong>Y-axis:</strong> The standardized value of each
coefficient.</li>
<li><strong>How to read it:</strong> Start from the
<strong>right</strong> (high <span
class="math inline">\(\lambda\)</span>). All coefficients are 0. As you
move <strong>left</strong>, <span class="math inline">\(\lambda\)</span>
<em>decreases</em>, and the penalty is relaxed. Variables “enter” the
model one by one (their coefficients become non-zero). You can see that
‘Rating’, ‘Income’, and ‘Student’ are the most important variables, as
they are the first to become non-zero.</li>
</ul></li>
<li><strong>Right Plot (vs. <span class="math inline">\(L_1\)</span>
Norm Ratio):</strong>
<ul>
<li>This shows the exact same information as the left plot, but the
x-axis is reversed and rescaled. An axis value of 0.0 means full penalty
(all <span class="math inline">\(\beta=0\)</span>), and 1.0 means no
penalty.</li>
</ul></li>
</ul>
<h2 id="code-understanding-r-to-python">Code Understanding (R to
Python)</h2>
<p>The slides use the <code>glmnet</code> package in R. The equivalent
and most popular library in Python is <strong>scikit-learn</strong>.</p>
<h3 id="finding-the-best-lambda-cv">1. Finding the Best <span
class="math inline">\(\lambda\)</span> (CV)</h3>
<p>The R code <code>cv.out=cv.glmnet(x[train,],y[train],alpha=1)</code>
performs cross-validation to find the best <span
class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><strong>Python Equivalent:</strong> Use <code>LassoCV</code>. It
does the same thing: tests many <span
class="math inline">\(\lambda\)</span> values (called
<code>alphas</code> in scikit-learn) and picks the best one.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the LassoCV object</span></span><br><span class="line"><span class="comment"># cv=5 means 5-fold cross-validation</span></span><br><span class="line">lasso_cv = LassoCV(cv=<span class="number">5</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model to the training data</span></span><br><span class="line">lasso_cv.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best lambda (called alpha_ in sklearn)</span></span><br><span class="line">best_lambda = lasso_cv.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha): <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the MSEs</span></span><br><span class="line"><span class="comment"># This is what&#x27;s plotted in the CV plot</span></span><br><span class="line"><span class="built_in">print</span>(lasso_cv.mse_path_)</span><br></pre></td></tr></table></figure>
<h3 id="fitting-with-the-best-lambda-and-getting-coefficients">2.
Fitting with the Best <span class="math inline">\(\lambda\)</span> and
Getting Coefficients</h3>
<p>The R code
<code>lasso.coef=predict(out,type="coefficients",s=bestlam)</code> gets
the coefficients for the best <span
class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><strong>Python Equivalent:</strong> The <code>LassoCV</code> object
is <em>already</em> refitted on the full training data using the best
<span class="math inline">\(\lambda\)</span>. You can also fit a new
<code>Lasso</code> model with that specific <span
class="math inline">\(\lambda\)</span>.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Option 1: Use the already-fitted LassoCV object ---</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Coefficients from LassoCV:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(lasso_cv.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on the test set</span></span><br><span class="line">y_pred = lasso_cv.predict(X_test)</span><br><span class="line">test_mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test MSE: <span class="subst">&#123;test_mse&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Option 2: Fit a new Lasso model with the best lambda ---</span></span><br><span class="line">final_lasso = Lasso(alpha=best_lambda)</span><br><span class="line">final_lasso.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get coefficients (Slide 7 shows this)</span></span><br><span class="line"><span class="comment"># Note how some are 0!</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nCoefficients from new Lasso model:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(final_lasso.coef_)</span><br></pre></td></tr></table></figure>
<h2 id="the-core-problem-two-equivalent-formulas">The Core Problem: Two
Equivalent Formulas</h2>
<p>The slides show two ways of writing the <em>same problem</em>.
Understanding this equivalence is key.</p>
<h3 id="formulation-1-the-penalized-method-slides-1-4">Formulation 1:
The Penalized Method (Slides 1 &amp; 4)</h3>
<ul>
<li><p><strong>Formula:</strong> <span
class="math display">\[\min_{\beta} \left( \sum_{i=1}^{n} (y_i -
\mathbf{x}_i^T \beta)^2 + \lambda \|\beta\|_1 \right)\]</span></p>
<ul>
<li><strong><span class="math inline">\(\sum (y_i - \mathbf{x}_i^T
\beta)^2\)</span></strong>: This is the normal <strong>Residual Sum of
Squares (RSS)</strong>. We want to make this small (fit the data
well).</li>
<li><strong><span class="math inline">\(\lambda
\|\beta\|_1\)</span></strong>: This is the <strong><span
class="math inline">\(L_1\)</span> penalty</strong>.
<ul>
<li><span class="math inline">\(\|\beta\|_1 = \sum_{j=1}^{p}
|\beta_j|\)</span> is the sum of the absolute values of the
coefficients.</li>
<li><span class="math inline">\(\lambda\)</span> (lambda) is a tuning
parameter. Think of it as a <strong>“penalty knob”</strong>.</li>
</ul></li>
</ul></li>
<li><p><strong>How to think about <span
class="math inline">\(\lambda\)</span></strong>:</p>
<ul>
<li><strong>If <span class="math inline">\(\lambda =
0\)</span>:</strong> There is no penalty. This is just standard Ordinary
Least Squares (OLS) regression. The model will likely overfit.</li>
<li><strong>If <span class="math inline">\(\lambda\)</span> is
<em>small</em>:</strong> There’s a small penalty. Coefficients will
shrink a <em>little</em> bit.</li>
<li><strong>If <span class="math inline">\(\lambda\)</span> is <em>very
large</em>:</strong> The penalty is severe. The <em>only</em> way to
make the penalty term small is to make the coefficients (<span
class="math inline">\(\beta\)</span>) themselves small. The model will
eventually shrink all coefficients to <strong>exactly 0</strong>.</li>
</ul></li>
</ul>
<h3 id="formulation-2-the-constrained-method-slides-2-3">Formulation 2:
The Constrained Method (Slides 2 &amp; 3)</h3>
<ul>
<li><p><strong>Formula:</strong> <span
class="math display">\[\min_{\beta} \sum_{i=1}^{n} (y_i - \mathbf{x}_i^T
\beta)^2 \quad \text{subject to} \quad \|\beta\|_1 \le
s\]</span></p></li>
<li><p><strong>How to think about <span
class="math inline">\(s\)</span></strong>:</p>
<ul>
<li>This says: “Find the best-fitting model (minimize RSS) <em>but</em>
you have a limited <strong>‘budget’ <span
class="math inline">\(s\)</span></strong> for the total size of your
coefficients.”</li>
<li><strong>If <span class="math inline">\(s\)</span> is <em>very
large</em>:</strong> The budget is huge. This constraint does nothing.
You get the standard OLS solution.</li>
<li><strong>If <span class="math inline">\(s\)</span> is
<em>small</em>:</strong> The budget is tight. You <em>must</em> shrink
your coefficients to stay under the budget <span
class="math inline">\(s\)</span>. To get the best fit, the model will be
forced to set unimportant coefficients to 0 and only “spend” its budget
on the most important variables.</li>
</ul></li>
</ul>
<p><strong>The Equivalence:</strong> These two forms are equivalent. For
any <span class="math inline">\(\lambda\)</span> you pick, there’s a
corresponding budget <span class="math inline">\(s\)</span> that gives
the <em>exact same solution</em>.</p>
<ul>
<li>High <span class="math inline">\(\lambda\)</span> (strong penalty)
<span class="math inline">\(\iff\)</span> Small <span
class="math inline">\(s\)</span> (tight budget)</li>
<li>Low <span class="math inline">\(\lambda\)</span> (weak penalty)
<span class="math inline">\(\iff\)</span> Large <span
class="math inline">\(s\)</span> (loose budget)</li>
</ul>
<p>This equivalence is why you see plots with both <span
class="math inline">\(\lambda\)</span> and <span
class="math inline">\(L_1\)</span> Norm on the x-axis. They are just two
different ways of looking at the same “penalty” spectrum.</p>
<h2 id="detailed-plot-code-analysis">Detailed Plot &amp; Code
Analysis</h2>
<p>Let’s look at the plots and code, which answer the practical
questions: <strong>(1)</strong> How do we pick the <em>best</em> <span
class="math inline">\(\lambda\)</span>? and <strong>(2)</strong> What
does LASSO <em>do</em> to the coefficients?</p>
<h3 id="question-1-how-to-pick-the-best-lambda-slide-5">Question 1: How
to pick the best <span class="math inline">\(\lambda\)</span>? (Slide
5)</h3>
<p>This is the <strong>Cross-Validation (CV) Plot</strong>. Its one and
only job is to help you find the optimal <span
class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><strong>R Code:</strong>
<code>cv.out=cv.glmnet(x[train,],y[train],alpha=1)</code>
<ul>
<li><code>cv.glmnet</code>: This R function <em>automatically</em> does
K-fold cross-validation. <code>alpha=1</code> explicitly tells it to use
<strong>LASSO</strong> (alpha=0 would be Ridge).</li>
<li>It tries a whole range of <span
class="math inline">\(\lambda\)</span> values, calculates the
Mean-Squared Error (MSE) for each, and stores the results in
<code>cv.out</code>.</li>
</ul></li>
<li><strong>Plot Analysis:</strong>
<ul>
<li><strong>X-axis:</strong> <span
class="math inline">\(\text{Log}(\lambda)\)</span>. The penalty
strength. <strong>Right = High Penalty</strong> (simple model),
<strong>Left = Low Penalty</strong> (complex model).</li>
<li><strong>Y-axis:</strong> Mean-Squared Error (MSE). <strong>Lower is
better.</strong></li>
<li><strong>Red Dots:</strong> The average MSE for each <span
class="math inline">\(\lambda\)</span>.</li>
<li><strong>Gray Bars:</strong> The error bars (standard error).</li>
<li><strong>The “U” Shape:</strong> This is the classic
<strong>bias-variance trade-off</strong>.
<ul>
<li><strong>Right Side (High <span
class="math inline">\(\lambda\)</span>):</strong> The model is <em>too
simple</em> (too many coefficients are 0). It’s “underfitting.” The
error is high (high bias).</li>
<li><strong>Left Side (High <span
class="math inline">\(\lambda\)</span>):</strong> The model is <em>too
complex</em> (low penalty, like OLS). It’s “overfitting” the training
data. The error on new data is high (high variance).</li>
<li><strong>Bottom of the “U”:</strong> This is the “sweet spot.” The
<span class="math inline">\(\lambda\)</span> at the very bottom (marked
by the left vertical dotted line) gives the <strong>lowest possible
MSE</strong>. This is <code>lambda.min</code>.</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Answer:</strong> You pick the <span
class="math inline">\(\lambda\)</span> that corresponds to the lowest
point on this graph.</p>
<h3 id="question-2-what-does-lasso-do-slides-5-6-7">Question 2: What
does LASSO <em>do</em>? (Slides 5, 6, 7)</h3>
<p>These slides all show the <em>effect</em> of LASSO.</p>
<p><strong>A. The Coefficient Path Plots (Slides 5 &amp; 6)</strong></p>
<p>These plots visualize how coefficients change. They show the <em>same
information</em> just with different x-axes.</p>
<ul>
<li><strong>Left Plot (Slide 6) vs. <span
class="math inline">\(\lambda\)</span>:</strong>
<ul>
<li><strong>How to read:</strong> Read from <strong>RIGHT to
LEFT</strong>.</li>
<li>At the far right (<span class="math inline">\(\lambda\)</span> is
large), all coefficients are 0.</li>
<li>As you move left, <span class="math inline">\(\lambda\)</span> gets
smaller, and the penalty is relaxed. Variables “enter” the model one by
one as their coefficients become non-zero.</li>
<li>You can see ‘Rating’ (red-dashed), ‘Student’ (black-solid), and
‘Income’ (blue-dotted) are the first to enter, suggesting they are the
most important predictors.</li>
</ul></li>
<li><strong>Right Plot (Slide 6) vs. <span
class="math inline">\(L_1\)</span> Norm Ratio:</strong>
<ul>
<li>This is the <em>same plot</em>, just flipped and rescaled. The
x-axis is <span class="math inline">\(\|\hat{\beta}_\lambda\|_1 /
\|\hat{\beta}_{OLS}\|_1\)</span>.</li>
<li><strong>How to read:</strong> Read from <strong>LEFT to
RIGHT</strong>.</li>
<li><strong>At 0.0:</strong> This is a “0% budget” (like <span
class="math inline">\(s=0\)</span> or <span
class="math inline">\(\lambda=\infty\)</span>). All coefficients are
0.</li>
<li><strong>At 1.0:</strong> This is a “100% budget” (like <span
class="math inline">\(s=\infty\)</span> or <span
class="math inline">\(\lambda=0\)</span>). This is the full OLS
model.</li>
<li>This view clearly shows the coefficients “growing” from 0 as their
“budget” (<span class="math inline">\(L_1\)</span> Norm) increases.</li>
</ul></li>
</ul>
<p><strong>B. The Code Output (Slide 7) - This is the most important
“answer”</strong></p>
<p>This slide <em>explicitly demonstrates</em> variable selection by
comparing the coefficients from two different <span
class="math inline">\(\lambda\)</span> values.</p>
<ul>
<li><p><strong>First Block (The “Optimal” Model):</strong></p>
<ul>
<li><code>bestlam.cv &lt;- cv.out$lambda.min</code>: This gets the <span
class="math inline">\(\lambda\)</span> from the bottom of the “U” in the
CV plot.</li>
<li><code>lasso.conf &lt;- predict(out,type="coefficients",s=bestlam.cv)[1:12,]</code>:
This gets the coefficients using that <em>best</em> <span
class="math inline">\(\lambda\)</span>.</li>
<li><code>lasso.conf[lasso.conf!=0]</code>: This R command filters the
list to show <em>only the non-zero coefficients</em>.</li>
<li><strong>Result:</strong> The optimal model <em>still keeps 10
variables</em> (‘Income’, ‘Limit’, ‘Rating’, etc.). It has shrunk them,
but it hasn’t set many to 0.</li>
</ul></li>
<li><p><strong>Second Block (The “High Penalty” Model):</strong></p>
<ul>
<li>The slide text says “if we choose a larger regularization
parameter.” Here, they’ve picked an arbitrary <em>larger</em> value,
<code>s=10</code>. (Note: R’s <code>predict.glmnet</code> can be
confusing; <code>s=10</code> here means <span
class="math inline">\(\lambda=10\)</span>).</li>
<li><code>lasso.conf &lt;- predict(out,type="coefficients",s=10)[1:12,]</code>:
This gets the coefficients using a <em>stronger penalty</em> (<span
class="math inline">\(\lambda=10\)</span>).</li>
<li><code>lasso.conf[lasso.conf!=0]</code>: Again, show only the
non-zero coefficients.</li>
<li><strong>Result:</strong> Look! The list is much shorter. The
coefficients for ‘Age’, ‘Education’, ‘GenderFemale’, ‘MarriedYes’, and
‘Ethnicity’ are <em>all gone</em> (shrunk to 0.000000). The model has
decided these are not important enough to “spend” budget on.</li>
</ul></li>
</ul>
<p><strong>Conclusion:</strong> LASSO performs <strong>automatic
variable selection</strong>. By increasing <span
class="math inline">\(\lambda\)</span>, you create a
<strong>sparser</strong> (simpler) model. Slide 7 is the concrete
proof.</p>
<h2 id="python-equivalents-in-more-detail">Python Equivalents (in more
detail)</h2>
<p>Here is how you would replicate the <em>entire</em> workflow from the
slides in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso, LassoCV, lasso_path</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Assume X_train, y_train, X_test, y_test are loaded ---</span></span><br><span class="line"><span class="comment"># Example: </span></span><br><span class="line"><span class="comment"># data = pd.read_csv(&#x27;Credit.csv&#x27;)</span></span><br><span class="line"><span class="comment"># X = pd.get_dummies(data.drop([&#x27;ID&#x27;, &#x27;Balance&#x27;], axis=1), drop_first=True)</span></span><br><span class="line"><span class="comment"># y = data[&#x27;Balance&#x27;]</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It&#x27;s CRITICAL to scale data before regularization</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train_scaled = scaler.fit_transform(X_train)</span><br><span class="line">X_test_scaled = scaler.transform(X_test)</span><br><span class="line">feature_names = X.columns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Replicate the CV Plot (Slide 5: ...000200.png)</span></span><br><span class="line"><span class="comment"># LassoCV does what cv.glmnet does: finds the best lambda (alpha)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Running LassoCV to find best lambda (alpha)...&quot;</span>)</span><br><span class="line"><span class="comment"># &#x27;alphas&#x27; is the list of lambdas to try. We can let it choose automatically.</span></span><br><span class="line"><span class="comment"># cv=10 means 10-fold cross-validation.</span></span><br><span class="line">lasso_cv = LassoCV(cv=<span class="number">10</span>, random_state=<span class="number">1</span>, max_iter=<span class="number">10000</span>)</span><br><span class="line">lasso_cv.fit(X_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The best lambda found</span></span><br><span class="line">best_lambda = lasso_cv.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha) found: <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Plotting the CV (MSE vs. Log(Lambda)) ---</span></span><br><span class="line"><span class="comment"># This recreates the R plot</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"><span class="comment"># lasso_cv.mse_path_ is a (n_alphas, n_folds) array of MSEs</span></span><br><span class="line"><span class="comment"># We take the mean across the folds (axis=1)</span></span><br><span class="line">mean_mses = np.mean(lasso_cv.mse_path_, axis=<span class="number">1</span>)</span><br><span class="line">log_lambdas = np.log10(lasso_cv.alphas_)</span><br><span class="line"></span><br><span class="line">plt.plot(log_lambdas, mean_mses, <span class="string">&#x27;r.-&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Log(Lambda / Alpha)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Mean-Squared Error&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;LASSO Cross-Validation Path (Replicating R Plot)&#x27;</span>)</span><br><span class="line"><span class="comment"># Plot a vertical line at the best lambda</span></span><br><span class="line">plt.axvline(np.log10(best_lambda), linestyle=<span class="string">&#x27;--&#x27;</span>, color=<span class="string">&#x27;k&#x27;</span>, label=<span class="string">f&#x27;Best Lambda (alpha) = <span class="subst">&#123;best_lambda:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.gca().invert_xaxis() <span class="comment"># High lambda is on the right in R plot</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Replicate the Coefficient Path Plot (Slide 6: ...000206.png)</span></span><br><span class="line"><span class="comment"># We can use the lasso_path function, or just use the CV object</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The lasso_cv object already calculated the paths!</span></span><br><span class="line">coefs = lasso_cv.path(X_train_scaled, y_train, alphas=lasso_cv.alphas_)[<span class="number">1</span>].T</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X_train_scaled.shape[<span class="number">1</span>]):</span><br><span class="line">    plt.plot(log_lambdas, coefs[:, i], label=feature_names[i])</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Log(Lambda / Alpha)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Standardized Coefficients&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;LASSO Coefficient Path (Replicating R Plot)&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">plt.gca().invert_xaxis()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Replicate the Code Output (Slide 7: ...000202.png)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Replicating R Output ---&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- First Block: Coefficients with BEST lambda ---</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Coefficients using best lambda (alpha = <span class="subst">&#123;best_lambda:<span class="number">.4</span>f&#125;</span>):&quot;</span>)</span><br><span class="line"><span class="comment"># The lasso_cv object is already fitted with the best lambda</span></span><br><span class="line">best_coefs = lasso_cv.coef_</span><br><span class="line">coef_series_best = pd.Series(best_coefs, index=feature_names)</span><br><span class="line"><span class="comment"># This is like R&#x27;s `lasso.conf[lasso.conf != 0]`</span></span><br><span class="line"><span class="built_in">print</span>(coef_series_best[coef_series_best != <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Second Block: Coefficients with a LARGER lambda ---</span></span><br><span class="line"><span class="comment"># Let&#x27;s pick a larger lambda, e.g., 10 (like the slide)</span></span><br><span class="line">large_lambda = <span class="number">10</span> </span><br><span class="line">lasso_high_penalty = Lasso(alpha=large_lambda)</span><br><span class="line">lasso_high_penalty.fit(X_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nCoefficients using larger lambda (alpha = <span class="subst">&#123;large_lambda&#125;</span>):&quot;</span>)</span><br><span class="line">high_pen_coefs = lasso_high_penalty.coef_</span><br><span class="line">coef_series_high = pd.Series(high_pen_coefs, index=feature_names)</span><br><span class="line"><span class="comment"># This is the second R command: `lasso.conf[lasso.conf != 0]`</span></span><br><span class="line"><span class="built_in">print</span>(coef_series_high[coef_series_high != <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Final Prediction ---</span></span><br><span class="line"><span class="comment"># This is R&#x27;s `mean((lasso.pred-y.test)^2)`</span></span><br><span class="line">y_pred = lasso_cv.predict(X_test_scaled)</span><br><span class="line">test_mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nTest MSE using best lambda: <span class="subst">&#123;test_mse:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="the-game-of-regularization">The “Game” of Regularization</h3>
<p>First, let’s understand what these plots are showing. This is a “map”
of a constrained optimization problem.</p>
<ul>
<li><strong>The Red Ellipses (RSS Contours):</strong> Think of these as
contour lines on a topographic map.
<ul>
<li><strong>The Center (<span
class="math inline">\(\hat{\beta}\)</span>):</strong> This point is the
“bottom of the valley.” It represents the <em>perfect</em>,
unconstrained solution—the standard Ordinary Least Squares (OLS)
coefficients. This point has the lowest possible Residual Sum of Squares
(RSS), or error.</li>
<li><strong>The Lines:</strong> Every point on a single red ellipse has
the <em>exact same</em> RSS. As the ellipses get bigger (moving away
from the center <span class="math inline">\(\hat{\beta}\)</span>), the
error gets higher.</li>
</ul></li>
<li><strong>The Blue Shaded Area (Constraint Region):</strong> This is
the “rule” of the game.
<ul>
<li>This is our “budget.” We are <em>only allowed</em> to pick a
solution (<span class="math inline">\(\beta_1, \beta_2\)</span>) from
<em>inside or on the boundary</em> of this blue shape.</li>
<li><strong>LASSO:</strong> The constraint is <span
class="math inline">\(|\beta_1| + |\beta_2| \le s\)</span>. This
equation forms a <strong>diamond</strong> (or a rotated square).</li>
<li><strong>Ridge:</strong> The constraint is <span
class="math inline">\(\beta_1^2 + \beta_2^2 \le s\)</span>. This
equation forms a <strong>circle</strong>.</li>
</ul></li>
<li><strong>The Goal:</strong> Find the “best” point that is <em>inside
the blue area</em>.
<ul>
<li>The “best” point is the one with the lowest possible error
(RSS).</li>
<li>Geometrically, this means we start at the center (<span
class="math inline">\(\hat{\beta}\)</span>) and expand our ellipse
outward. The <em>very first point</em> where the ellipse
<strong>touches</strong> the blue constraint region is our
solution.</li>
</ul></li>
</ul>
<h3 id="why-lasso-performs-variable-selection-the-diamond">Why LASSO
Performs Variable Selection (The Diamond) 🎯</h3>
<p>This is the most important concept. Look at the LASSO diagrams.</p>
<ul>
<li><strong>The Shape:</strong> The LASSO constraint is a
<strong>diamond</strong>.</li>
<li><strong>The Key Feature:</strong> This diamond has <strong>sharp
corners</strong> (vertices). And most importantly, these corners lie
<strong>exactly on the axes</strong>.
<ul>
<li>The top corner is at <span class="math inline">\((\beta_1=0,
\beta_2=s)\)</span>.</li>
<li>The right corner is at <span class="math inline">\((\beta_1=s,
\beta_2=0)\)</span>.</li>
</ul></li>
<li><strong>The “Collision”:</strong> Now, imagine the red ellipses
(representing our error) expanding from the OLS solution (<span
class="math inline">\(\hat{\beta}\)</span>). They will almost always
“hit” the blue diamond at one of its <strong>sharp corners</strong>.
<ul>
<li>Look at your textbook diagram (slide <code>...000304.png</code>).
The ellipse clearly makes contact with the diamond at the top corner,
where <span class="math inline">\(\beta_1 = 0\)</span>.</li>
<li>Look at your example (slide <code>...000259.jpg</code>). The center
of the ellipses is at (4, 0.1). The closest point on the diamond that
the expanding ellipses will hit is the corner at (2, 0). At this
solution, <strong><span class="math inline">\(y\)</span> is exactly
0</strong>.</li>
</ul></li>
</ul>
<p><strong>Conclusion:</strong> Because the <span
class="math inline">\(L_1\)</span> “diamond” has corners on the axes,
the optimal solution is very likely to land on one of them. When it
does, the coefficient for the <em>other</em> axis is set to
<strong>exactly zero</strong>. This is the <strong>variable selection
property</strong>.</p>
<h3 id="why-ridge-regression-only-shrinks-the-circle">Why Ridge
Regression Only Shrinks (The Circle) 🤏</h3>
<p>Now, look at the Ridge regression diagram.</p>
<ul>
<li><strong>The Shape:</strong> The Ridge constraint is a
<strong>circle</strong>.</li>
<li><strong>The Key Feature:</strong> A circle is perfectly smooth and
has <strong>no corners</strong>.</li>
<li><strong>The “Collision”:</strong> Imagine the same ellipses
expanding and hitting the blue circle. The contact point will be a
<em>tangent</em> point.
<ul>
<li>Because the circle is round, this tangent point can be
<em>anywhere</em> on its circumference.</li>
<li>It is <em>extremely unlikely</em> that the contact point will be
exactly on an axis (e.g., at <span class="math inline">\((\beta_1=0,
\beta_2=s)\)</span>). This would only happen if the OLS solution <span
class="math inline">\(\hat{\beta}\)</span> was <em>already</em>
perfectly aligned with that axis.</li>
</ul></li>
<li><strong>Conclusion:</strong> The Ridge solution will find a point
where <em>both</em> <span class="math inline">\(\beta_1\)</span> and
<span class="math inline">\(\beta_2\)</span> are non-zero. The
coefficients are “shrunk” (pulled in from <span
class="math inline">\(\hat{\beta}\)</span> towards the origin), but they
<strong>never become zero</strong>. This is why Ridge is called a
“shrinkage” method, but not a “variable selection” method.</li>
</ul>
<h3 id="summary-diamond-vs.-circle">Summary: Diamond vs. Circle</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">LASSO (<span
class="math inline">\(L_1\)</span> Norm)</th>
<th style="text-align: left;">Ridge (<span
class="math inline">\(L_2\)</span> Norm)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Constraint Shape</strong></td>
<td style="text-align: left;"><strong>Diamond</strong> (or
hyper-rhombus)</td>
<td style="text-align: left;"><strong>Circle</strong> (or
hypersphere)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Key Feature</strong></td>
<td style="text-align: left;"><strong>Sharp corners</strong> on the
axes</td>
<td style="text-align: left;"><strong>Smooth curve</strong> with no
corners</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Geometric Solution</strong></td>
<td style="text-align: left;">Ellipses hit the
<strong>corners</strong></td>
<td style="text-align: left;">Ellipses hit a <strong>smooth
part</strong></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Result</strong></td>
<td style="text-align: left;">Forces some coefficients to
<strong>exactly 0</strong></td>
<td style="text-align: left;">Shrinks all coefficients <em>towards</em>
0</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Name</strong></td>
<td style="text-align: left;"><strong>Variable Selection</strong></td>
<td style="text-align: left;"><strong>Shrinkage</strong></td>
</tr>
</tbody>
</table>
<p>The “space meaning” is that the <strong>sharp corners of the <span
class="math inline">\(L_1\)</span> diamond are what make variable
selection possible</strong>. The smooth circle of the <span
class="math inline">\(L_2\)</span> norm does not have these corners and
thus cannot force coefficients to zero.</p>
<h1 id="shrinkage-methods-lasso-vs.-ridge">8. Shrinkage Methods (Lasso
vs. Ridge)</h1>
<h2 id="core-concept-shrinkage-methods">Core Concept: Shrinkage
Methods</h2>
<p>Both <strong>Ridge (L2)</strong> and <strong>Lasso (L1)</strong> are
regularization techniques used to improve upon standard <strong>Ordinary
Least Squares (OLS)</strong> regression.</p>
<p>Their main goal is to manage the <strong>bias-variance
tradeoff</strong>. OLS often has low bias but very high variance,
especially when you have many predictors (<span
class="math inline">\(p\)</span>) or when predictors are correlated.
Ridge and Lasso improve prediction accuracy by <em>shrinking</em> the
regression coefficients towards zero. This adds a small amount of bias
but significantly <em>reduces</em> the variance, leading to a lower
overall Test Mean Squared Error (MSE).</p>
<h2 id="the-key-difference-math-how-they-shrink">The Key Difference:
Math &amp; How They Shrink</h2>
<p>The slides show that the two methods use different penalties, which
leads to very different mathematical forms and practical outcomes.</p>
<ul>
<li><strong>Ridge Regression (L2 Penalty):</strong> Minimizes <span
class="math inline">\(RSS + \lambda \sum_{j=1}^{p}
\beta_j^2\)</span></li>
<li><strong>Lasso Regression (L1 Penalty):</strong> Minimizes <span
class="math inline">\(RSS + \lambda \sum_{j=1}^{p}
|\beta_j|\)</span></li>
</ul>
<p>Slide 80 provides the exact formulas for their coefficient estimates
in a simple, orthogonal case (where predictors are independent):</p>
<h3 id="ridge-regression-proportional-shrinkage">Ridge Regression
(Proportional Shrinkage)</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\beta}_j^R = \hat{\beta}_j^{LSE} / (1 +
\lambda)\)</span></li>
<li><strong>What this means:</strong> Ridge <em>shrinks</em> every least
squares coefficient by a proportional amount. It will make coefficients
<em>smaller</em>, but it will <strong>never set them to exactly
zero</strong> (unless <span class="math inline">\(\lambda\)</span> is
<span class="math inline">\(\infty\)</span>).</li>
</ul>
<h3 id="lasso-regression-soft-thresholding">Lasso Regression
(Soft-Thresholding)</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\beta}_j^L =
\text{sign}(\hat{\beta}_j^{LSE})(|\hat{\beta}_j^{LSE}| -
\lambda/2)_+\)</span></li>
<li><strong>What this means:</strong> This is a “soft-thresholding”
operator.
<ul>
<li>If the original coefficient <span
class="math inline">\(\hat{\beta}_j^{LSE}\)</span> is small (its
absolute value is less than <span
class="math inline">\(\lambda/2\)</span>), Lasso <strong>sets it to
exactly zero</strong>.</li>
<li>If the coefficient is large, Lasso subtracts <span
class="math inline">\(\lambda/2\)</span> from its absolute value,
shrinking it towards zero.</li>
</ul></li>
<li><strong>Key Property:</strong> Because of this, Lasso performs
<strong>automatic feature selection</strong> by eliminating
predictors.</li>
</ul>
<h2 id="important-images-explained">Important Images Explained</h2>
<h3 id="most-important-figure-6.10-slide-82">Most Important: Figure 6.10
(Slide 82)</h3>
<p>This is the best visual for understanding the <em>mathematical
difference</em> from the formulas above.</p>
<ul>
<li><strong>Left (Ridge):</strong> The red line shows the Ridge estimate
vs. the OLS estimate. It’s a straight, diagonal line with a slope less
than 1. It shrinks everything <em>proportionally</em>.</li>
<li><strong>Right (Lasso):</strong> The red line shows the Lasso
estimate. It’s “flat” at zero for a range, showing it <strong>sets small
coefficients to zero</strong>. Then, it slopes up, but it’s shifted (it
shrinks the large coefficients by a fixed amount).</li>
</ul>
<h3 id="scenario-1-figure-6.8-slide-76">Scenario 1: Figure 6.8 (Slide
76)</h3>
<p>This plot shows what happens when <strong>all 45 predictors are truly
related to the response</strong>.</p>
<ul>
<li><strong>Result (Slide 77):</strong> <strong>Ridge performs slightly
better</strong> (has a lower minimum MSE, shown by the dotted purple
line).</li>
<li><strong>Why:</strong> Lasso’s assumption (that some coefficients are
zero) is <em>wrong</em> in this case. By forcing some relevant
predictors to zero, it adds too much bias. Ridge, by just
<em>shrinking</em> all of them, finds a better balance.</li>
</ul>
<h3 id="scenario-2-figure-6.9-slide-78">Scenario 2: Figure 6.9 (Slide
78)</h3>
<p>This plot shows the <em>opposite</em> scenario: <strong>only 2 out of
45 predictors are truly related</strong> (a “sparse” model).</p>
<ul>
<li><strong>Result:</strong> <strong>Lasso performs much better</strong>
(its solid purple line has a much lower minimum MSE).</li>
<li><strong>Why:</strong> Lasso’s assumption is <em>correct</em>. It
successfully sets the 43 “noise” predictors to zero, which dramatically
reduces variance, while correctly keeping the 2 important ones.</li>
</ul>
<h2 id="python-code-understanding-1">Python &amp; Code
Understanding</h2>
<p>The slides don’t contain Python code, but they describe the exact
concepts you would use, primarily in <code>scikit-learn</code>.</p>
<ul>
<li><p><strong>Implementing Ridge &amp; Lasso:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, Lasso, RidgeCV, LassoCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># It&#x27;s crucial to scale data before regularization</span></span><br><span class="line"><span class="comment"># alpha is the same as the λ (lambda) in your slides</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Ridge ---</span></span><br><span class="line"><span class="comment"># The math for Ridge is a &quot;closed-form solution&quot; (Slide 80)</span></span><br><span class="line"><span class="comment"># ridge_model = make_pipeline(StandardScaler(), Ridge(alpha=1.0))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Lasso ---</span></span><br><span class="line"><span class="comment"># Lasso requires a numerical solver (like coordinate descent)</span></span><br><span class="line"><span class="comment"># lasso_model = make_pipeline(StandardScaler(), Lasso(alpha=0.1))</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>The Soft-Thresholding Formula:</strong> The math from
Slide 80, <span class="math inline">\(\text{sign}(y)(|y| -
\lambda/2)_+\)</span>, is the core operation in the “coordinate descent”
algorithm used to solve Lasso. You could write it in Python/Numpy:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">soft_threshold</span>(<span class="params">x, lambda_val</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements the Lasso soft-thresholding formula.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">return</span> np.sign(x) * np.maximum(<span class="number">0</span>, np.<span class="built_in">abs</span>(x) - (lambda_val / <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example:</span></span><br><span class="line"><span class="comment"># ols_coefficient = 1.5</span></span><br><span class="line"><span class="comment"># threshold = 4.0</span></span><br><span class="line"><span class="comment"># lasso_coefficient = soft_threshold(ols_coefficient, threshold) </span></span><br><span class="line"><span class="comment"># print(lasso_coefficient) # Output: 0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ols_coefficient = 3.0</span></span><br><span class="line"><span class="comment"># threshold = 4.0</span></span><br><span class="line"><span class="comment"># lasso_coefficient = soft_threshold(ols_coefficient, threshold) </span></span><br><span class="line"><span class="comment"># print(lasso_coefficient) # Output: 1.0 (it was 3.0, shrunk by 4/2 = 2)</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>Choosing <span class="math inline">\(\lambda\)</span>
(alpha):</strong> Slide 79 says to “Use cross validation to determine
which one has better prediction.” In <code>scikit-learn</code>, this is
done for you with <code>RidgeCV</code> and <code>LassoCV</code>, which
automatically test a range of <code>alpha</code> values.</p></li>
</ul>
<h2 id="summary-lasso-vs.-ridge">Summary: Lasso vs. Ridge</h2>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Ridge (L2)</th>
<th style="text-align: left;">Lasso (L1)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Penalty</strong></td>
<td style="text-align: left;"><span class="math inline">\(L_2\)</span>
norm: <span class="math inline">\(\lambda \sum \beta_j^2\)</span></td>
<td style="text-align: left;"><span class="math inline">\(L_1\)</span>
norm: <span class="math inline">\(\lambda \sum |\beta_j|\)</span></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Coefficient
Shrinkage</strong></td>
<td style="text-align: left;">Proportional; shrinks all coefficients,
but never to <em>exactly</em> zero.</td>
<td style="text-align: left;">Soft-thresholding; can force coefficients
to be <em>exactly</em> zero.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Feature Selection?</strong></td>
<td style="text-align: left;">No</td>
<td style="text-align: left;"><strong>Yes</strong>, this is its main
advantage.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Interpretability</strong></td>
<td style="text-align: left;">Less interpretable (keeps all <span
class="math inline">\(p\)</span> variables).</td>
<td style="text-align: left;">More interpretable (produces a “sparse”
model with fewer variables).</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Best Used When…</strong></td>
<td style="text-align: left;">…most predictors are useful. (e.g., Slide
76: 45/45 relevant).</td>
<td style="text-align: left;">…many predictors are “noise” and only a
few are strong. (e.g., Slide 78: 2/45 relevant).</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Computation</strong></td>
<td style="text-align: left;">Has a simple, closed-form solution.</td>
<td style="text-align: left;">Requires numerical optimization (e.g.,
coordinate descent).</td>
</tr>
</tbody>
</table>
<h1 id="shrinkage-methods-ridge-lasso">9. Shrinkage Methods (Ridge &amp;
LASSO)</h1>
<h2 id="summary-of-shrinkage-methods-ridge-lasso">Summary of Shrinkage
Methods (Ridge &amp; LASSO)</h2>
<p>These slides introduce <strong>shrinkage methods</strong>, also known
as <strong>regularization</strong>, a technique used in regression (like
linear regression) to improve model performance. The main idea is to add
a <em>penalty</em> to the model’s loss function to “shrink” the size of
the coefficients. This helps to reduce model variance and prevent
overfitting, especially when you have many features.</p>
<p>The two main methods discussed are <strong>Ridge Regression</strong>
(<span class="math inline">\(L_2\)</span> penalty) and
<strong>LASSO</strong> (<span class="math inline">\(L_1\)</span>
penalty).</p>
<h2 id="key-mathematical-formulas-1">Key Mathematical Formulas</h2>
<ol type="1">
<li><p><strong>Standard Linear Model:</strong> The problem starts with
the standard linear regression model (from slide 1):</p>
<p><span class="math display">\[
\]</span>$$\mathbf{y} = \mathbf{X}\beta + \epsilon</p>
<p><span class="math display">\[
\]</span>$$ * <span class="math inline">\(\mathbf{y}\)</span> is the
<span class="math inline">\(n \times 1\)</span> vector of observed
outcomes.</p>
<ul>
<li><span class="math inline">\(\mathbf{X}\)</span> is the <span
class="math inline">\(n \times p\)</span> matrix of <span
class="math inline">\(p\)</span> predictor features for <span
class="math inline">\(n\)</span> observations.</li>
<li><span class="math inline">\(\beta\)</span> is the <span
class="math inline">\(p \times 1\)</span> vector of coefficients (what
we want to find).</li>
<li><span class="math inline">\(\epsilon\)</span> is the <span
class="math inline">\(n \times 1\)</span> vector of random errors.</li>
<li>The goal of standard “Ordinary Least Squares” (OLS) regression is to
find the <span class="math inline">\(\beta\)</span> that minimizes the
loss: <span class="math inline">\(\|\mathbf{X}\beta -
\mathbf{y}\|^2_2\)</span>.</li>
</ul></li>
<li><p><strong>LASSO (L1 Regularization):</strong> LASSO (Least Absolute
Shrinkage and Selection Operator) adds a penalty based on the
<em>absolute value</em> of the coefficients (the <span
class="math inline">\(L_1\)</span>-norm). This is the key formula from
slide 1:</p>
<p><span class="math display">\[
\]</span>$$\hat{\beta}(\lambda) \leftarrow \arg \min_{\beta} \left(
|\mathbf{X}\beta - \mathbf{y}|^2_2 + \lambda|\beta|_1 \right)</p>
<p><span class="math display">\[
\]</span>$$ * <span class="math inline">\(\|\beta\|_1 = \sum_{j=1}^{p}
|\beta_j|\)</span></p>
<ul>
<li><span class="math inline">\(\lambda\)</span> (lambda) is the
<strong>tuning parameter</strong> that controls the strength of the
penalty. A larger <span class="math inline">\(\lambda\)</span> means
more shrinkage.</li>
<li><strong>Key Property (Variable Selection):</strong> The <span
class="math inline">\(L_1\)</span> penalty can force some coefficients
(<span class="math inline">\(\beta_j\)</span>) to become <strong>exactly
zero</strong>. This means LASSO simultaneously performs <em>feature
selection</em> by automatically removing irrelevant predictors.</li>
<li><strong>Support (Slide 1):</strong> The question “Can it recover the
support of <span class="math inline">\(\beta\)</span>?” is asking if
LASSO can correctly identify the set of true non-zero coefficients
(defined as <span class="math inline">\(S := \{j : \beta_j \neq
0\}\)</span>).</li>
</ul></li>
<li><p><strong>Ridge Regression (L2 Regularization):</strong> Ridge
regression (mentioned on slide 2, shown on slide 3) adds a penalty based
on the <em>squared value</em> of the coefficients (the <span
class="math inline">\(L_2\)</span>-norm).</p>
<p><span class="math display">\[
\]</span>$$\hat{\beta}(\lambda) \leftarrow \arg \min_{\beta} \left(
|\mathbf{X}\beta - \mathbf{y}|^2_2 + \lambda|\beta|^2_2 \right)</p>
<p><span class="math display">\[
\]</span>$$ * <span class="math inline">\(\|\beta\|^2_2 = \sum_{j=1}^{p}
\beta_j^2\)</span></p>
<ul>
<li><strong>Key Property (Shrinkage):</strong> The <span
class="math inline">\(L_2\)</span> penalty <em>shrinks</em> coefficients
<em>towards</em> zero but <strong>never</strong> sets them to
<em>exactly</em> zero (unless <span class="math inline">\(\lambda =
\infty\)</span>). It is effective at handling multicollinearity.</li>
</ul></li>
</ol>
<h2 id="important-images-concepts">Important Images &amp; Concepts</h2>
<p>The most important images are the plots from slides 3 and 4. They
illustrate the two most critical concepts: <strong>how to choose <span
class="math inline">\(\lambda\)</span></strong> and <strong>what the
penalty does to the coefficients</strong>.</p>
<h3 id="tuning-parameter-selection-slides-3-4-left-plots">Tuning
Parameter Selection (Slides 3 &amp; 4, Left Plots)</h3>
<ul>
<li><strong>Problem:</strong> How do you find the <em>best</em> value
for <span class="math inline">\(\lambda\)</span>?</li>
<li><strong>Solution:</strong> <strong>Cross-Validation (CV)</strong>.
The slides show 10-fold CV.</li>
<li><strong>What the Plots Show:</strong> The left plots on slides 3 and
4 show the <strong>Cross-Validation Error</strong> (like MSE) for
different values of the penalty.
<ul>
<li>The x-axis represents the penalty strength (either <span
class="math inline">\(\lambda\)</span> itself or a related measure like
the shrinkage ratio <span
class="math inline">\(\|\hat{\beta}_\lambda\|_1 /
\|\hat{\beta}\|_1\)</span>).</li>
<li>The y-axis is the prediction error.</li>
<li>The curve is typically <strong>U-shaped</strong>. The vertical
dashed line marks the <strong>minimum</strong> of this curve. This
minimum point corresponds to the <strong>optimal <span
class="math inline">\(\lambda\)</span></strong>, which provides the best
balance between bias and variance, leading to the best-performing model
on unseen data.</li>
</ul></li>
</ul>
<h3 id="coefficient-paths-slides-3-4-right-plots">Coefficient Paths
(Slides 3 &amp; 4, Right Plots)</h3>
<p>These “trace” plots are crucial for understanding the difference
between Ridge and LASSO. They show how the value of each coefficient
(y-axis) changes as the penalty strength (x-axis) changes.</p>
<ul>
<li><strong>Slide 3 (Ridge):</strong> As <span
class="math inline">\(\lambda\)</span> increases (moving right), all
coefficient values are smoothly shrunk <em>towards</em> zero, but none
of them actually hit zero.</li>
<li><strong>Slide 4 (LASSO):</strong> As the penalty increases (moving
from right to left, as the ratio <span class="math inline">\(s\)</span>
goes from 1.0 to 0.0), you can see coefficients “drop off” and become
<strong>exactly zero</strong> one by one. The model with the optimal
<span class="math inline">\(\lambda\)</span> (vertical line) has
selected only a few non-zero coefficients (the pink and teal lines),
while all the grey lines have been set to zero. This is <em>feature
selection</em> in action.</li>
</ul>
<h2 id="key-discussion-points-slide-2">Key Discussion Points (Slide
2)</h2>
<ul>
<li><strong>Non-linear models:</strong> You can apply these methods to
non-linear models by first creating non-linear features (e.g., <span
class="math inline">\(x_1^2\)</span>, <span
class="math inline">\(x_2^2\)</span>, <span class="math inline">\(x_1
\cdot x_2\)</span>) and then feeding them into a LASSO or Ridge model.
The regularization will then select which of these linear <em>or</em>
non-linear terms are important.</li>
<li><strong>Correlated Features (Multicollinearity):</strong> The
question “If <span class="math inline">\(x_j \approx x_k\)</span>, how
does LASSO behave?” is a key weakness of LASSO.
<ul>
<li><strong>LASSO:</strong> Tends to <em>arbitrarily</em> select one of
the correlated features and set the others to zero. This can make the
model unstable.</li>
<li><strong>Ridge:</strong> Tends to shrink the coefficients of
correlated features <em>together</em>, giving them similar (but smaller)
values.</li>
<li><strong>Elastic Net</strong> (not shown) is a hybrid of Ridge and
LASSO that is often used to get the best of both worlds: it can select
groups of correlated variables.</li>
</ul></li>
</ul>
<h2 id="python-code-understanding-using-scikit-learn">Python Code
Understanding (using <code>scikit-learn</code>)</h2>
<p>Here is how you would implement these concepts in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso, Ridge, LassoCV, RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Assume you have your data ---</span></span><br><span class="line"><span class="comment"># X: your feature matrix (e.g., shape 100, 20)</span></span><br><span class="line"><span class="comment"># y: your target vector (e.g., shape 100,)</span></span><br><span class="line"><span class="comment"># X, y = ... load your data ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. It&#x27;s crucial to scale your data before regularization</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Find the optimal lambda (alpha) using Cross-Validation</span></span><br><span class="line"><span class="comment"># scikit-learn uses &#x27;alpha&#x27; instead of &#x27;lambda&#x27; for the tuning parameter.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- For LASSO ---</span></span><br><span class="line"><span class="comment"># LassoCV automatically performs cross-validation (e.g., cv=10)</span></span><br><span class="line"><span class="comment"># to find the best alpha.</span></span><br><span class="line">lasso_cv_model = LassoCV(cv=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">lasso_cv_model.fit(X_scaled, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best alpha (lambda)</span></span><br><span class="line">best_alpha_lasso = lasso_cv_model.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal alpha (lambda) for LASSO: <span class="subst">&#123;best_alpha_lasso&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the final coefficients</span></span><br><span class="line">lasso_coeffs = lasso_cv_model.coef_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;LASSO coefficients: <span class="subst">&#123;lasso_coeffs&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># You will see that many of these are exactly 0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- For Ridge ---</span></span><br><span class="line"><span class="comment"># RidgeCV works similarly. It&#x27;s often good to test alphas on a log scale.</span></span><br><span class="line">ridge_alphas = np.logspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>) <span class="comment"># 100 values from 0.001 to 1000</span></span><br><span class="line">ridge_cv_model = RidgeCV(alphas=ridge_alphas, store_cv_values=<span class="literal">True</span>)</span><br><span class="line">ridge_cv_model.fit(X_scaled, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best alpha (lambda)</span></span><br><span class="line">best_alpha_ridge = ridge_cv_model.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal alpha (lambda) for Ridge: <span class="subst">&#123;best_alpha_ridge&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the final coefficients</span></span><br><span class="line">ridge_coeffs = ridge_cv_model.coef_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ridge coefficients: <span class="subst">&#123;ridge_coeffs&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># You will see these are small, but not exactly zero.</span></span><br></pre></td></tr></table></figure>
<h2 id="bias-variance-tradeoff">Bias-variance tradeoff</h2>
<h2 id="key-mathematical-formulas-concepts">Key Mathematical Formulas
&amp; Concepts</h2>
<h3 id="lasso-sign-consistency">LASSO: Sign Consistency</h3>
<p>This is the “ideal” scenario for LASSO. Sign consistency means that,
with enough data, the LASSO model not only selects the <em>correct</em>
set of features (it recovers the “support” <span
class="math inline">\(S\)</span>) but also correctly identifies the
<em>sign</em> (positive or negative) of their coefficients.</p>
<ul>
<li><p><strong>The Goal (Slide 1):</strong></p>
<p><span class="math display">\[
\]</span>$$\text{sign}(\hat{\beta}(\lambda)) = \text{sign}(\beta)</p>
<p><span class="math display">\[
\]</span>$$This means the signs of our <em>estimated</em> coefficients
<span class="math inline">\(\hat{\beta}(\lambda)\)</span> match the
signs of the <em>true</em> underlying coefficients <span
class="math inline">\(\beta\)</span>.</p></li>
<li><p><strong>The “Irrepresentable Condition” (Slide 1):</strong> This
is the mathematical guarantee required for LASSO to achieve sign
consistency.</p>
<p><span class="math display">\[
\]</span>$$|\mathbf{X}_{S<sup>c}</sup>\top \mathbf{X}_S
(\mathbf{X}_S^\top \mathbf{X}<em>S)^{-1}
\text{sign}(\beta_S)|</em>\infty &lt; 1</p>
<p><span class="math display">\[
\]</span>$$ * <strong>Plain English:</strong> This formula is a complex
way of saying: <strong>The irrelevant features (<span
class="math inline">\(\mathbf{X}_{S^c}\)</span>) cannot be too strongly
correlated with the true, relevant features (<span
class="math inline">\(\mathbf{X}_S\)</span>).</strong></p>
<ul>
<li>If an irrelevant feature is very similar (highly correlated) to a
true feature, LASSO can get “confused” and might pick the wrong one, or
its estimate will be unstable. This condition fails.</li>
</ul></li>
</ul>
<h3 id="ridge-regression-the-bias-variance-tradeoff">Ridge Regression:
The Bias-Variance Tradeoff</h3>
<ul>
<li><p><strong>The Formula (Slide 3):</strong></p>
<p><span class="math display">\[
\]</span>$$\hat{\beta}<em>{\text{ridge}}(\lambda) \leftarrow \arg
\min</em>{\beta} \left( |\mathbf{y} - \mathbf{X}\beta|^2 +
\lambda|\beta|^2 \right)</p>
<p><span class="math display">\[
\]</span>$$<em>(Note: This is the <span
class="math inline">\(L_2\)</span> penalty, so <span
class="math inline">\(\|\beta\|^2 = \sum
\beta_j^2\)</span>)</em></p></li>
<li><p><strong>The Problem it Solves: Collinearity (Slide 2)</strong>
When features are strongly correlated (e.g., <span
class="math inline">\(x_i \approx x_j\)</span>), regular methods
fail:</p>
<ul>
<li><strong>LSE (OLS):</strong> Fails because the matrix <span
class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> is
“non-invertible” (or singular), so the math for the solution <span
class="math inline">\(\hat{\beta} = (\mathbf{X}^\top
\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\)</span> breaks down.</li>
<li><strong>LASSO:</strong> Fails because the <strong>Irrepresentable
Condition</strong> is violated. LASSO will tend to <em>arbitrarily</em>
pick one of the correlated features and set the others to zero.</li>
</ul></li>
<li><p><strong>The Ridge Solution (Slide 3):</strong></p>
<ol type="1">
<li><strong>Always has a solution:</strong> Adding the <span
class="math inline">\(\lambda\)</span> penalty makes the matrix math
work, even if <span class="math inline">\(\mathbf{X}^\top
\mathbf{X}\)</span> is non-invertible.</li>
<li><strong>Groups variables:</strong> This is the key takeaway. Instead
of arbitrarily picking one feature, <strong>Ridge tends to shrink the
coefficients of collinear variables <em>together</em></strong>.</li>
<li><strong>Bias-Variance Tradeoff:</strong> Ridge <em>introduces
bias</em> into the estimates (they are “wrong” on purpose) to
<em>massively reduce variance</em> (they are more stable and less
sensitive to the specific training data). This trade-off usually leads
to a much lower overall error (Mean Squared Error).</li>
</ol></li>
</ul>
<h2 id="important-images-key-takeaways">Important Images &amp; Key
Takeaways</h2>
<ol type="1">
<li><p><strong>Slide 2 (Collinearity Failures):</strong> This is the
most important “problem” slide. It clearly explains <em>why</em> you
can’t always use standard LSE or LASSO. The fact that all three methods
(LSE, LASSO, Forward Selection) fail with strong collinearity motivates
the need for Ridge.</p></li>
<li><p><strong>Slide 3 (Ridge Properties):</strong> This is the most
important “solution” slide. The two most critical points are:</p>
<ul>
<li><code>Always unique solution for λ &gt; 0</code></li>
<li><code>Collinear variables tend to be grouped!</code> (This is the
“fix” for the problem on Slide 2).</li>
</ul></li>
</ol>
<h2 id="python-code-understanding-2">Python Code Understanding</h2>
<p>Let’s demonstrate the <strong>key difference</strong> (Slide 3) in
how LASSO and Ridge handle collinear features.</p>
<p>We will create two features, <code>x1</code> and <code>x2</code>,
that are nearly identical.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso, Ridge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create a dataset with 2 strongly correlated features</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line"><span class="comment"># x1: a standard feature</span></span><br><span class="line">x1 = np.random.randn(n_samples)</span><br><span class="line"><span class="comment"># x2: almost identical to x1</span></span><br><span class="line">x2 = x1 + <span class="number">0.01</span> * np.random.randn(n_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine into our feature matrix X</span></span><br><span class="line">X = np.c_[x1, x2]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y: The target variable (let&#x27;s say y = 2*x1 + 2*x2)</span></span><br><span class="line">y = <span class="number">2</span> * x1 + <span class="number">2</span> * x2 + np.random.randn(n_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit LASSO (alpha is the same as lambda)</span></span><br><span class="line"><span class="comment"># We use a moderate alpha</span></span><br><span class="line">lasso_model = Lasso(alpha=<span class="number">1.0</span>)</span><br><span class="line">lasso_model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit Ridge (alpha is the same as lambda)</span></span><br><span class="line">ridge_model = Ridge(alpha=<span class="number">1.0</span>)</span><br><span class="line">ridge_model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Compare the coefficients</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- Results for Correlated Features ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;True Coefficients: [2.0, 2.0]&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;LASSO Coefficients: <span class="subst">&#123;np.<span class="built_in">round</span>(lasso_model.coef_, <span class="number">2</span>)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ridge Coefficients: <span class="subst">&#123;np.<span class="built_in">round</span>(ridge_model.coef_, <span class="number">2</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="example-output">Example Output:</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--- Results for Correlated Features ---</span><br><span class="line">True Coefficients: [2.0, 2.0]</span><br><span class="line">LASSO Coefficients: [3.89 0.  ]</span><br><span class="line">Ridge Coefficients: [1.95 1.94]</span><br></pre></td></tr></table></figure>
<h3 id="code-explanation">Code Explanation:</h3>
<ul>
<li><strong>LASSO:</strong> As predicted by the slides, LASSO failed to
find the true model. It <em>arbitrarily</em> picked <code>x1</code>,
gave it a large coefficient, and <strong>set <code>x2</code> to
zero</strong>. This is unstable and not what we wanted.</li>
<li><strong>Ridge:</strong> As predicted by Slide 3, Ridge handled the
collinearity perfectly. It identified that both <code>x1</code> and
<code>x2</code> were important and <strong>“grouped” them</strong> by
assigning them nearly identical, stable coefficients (1.95 and 1.94),
which are very close to the true values of 2.0.</li>
</ul>
<h1 id="elastic-net">10. Elastic Net</h1>
<h2 id="overall-summary">Overall Summary</h2>
<p>These slides introduce <strong>Elastic Net</strong>, a modern
regression method that solves the major weaknesses of its two
predecessors, <strong>Ridge</strong> and <strong>LASSO</strong>
regression.</p>
<ul>
<li><strong>Ridge</strong> is good for <strong>collinearity</strong>
(correlated features) but can’t do <strong>variable selection</strong>
(it can’t set any feature’s coefficient to <em>exactly</em> zero).</li>
<li><strong>LASSO</strong> is good for <strong>variable
selection</strong> (it creates <em>sparse</em> models by setting
coefficients to zero) but behaves <strong>unstably</strong> when
features are correlated (it tends to randomly pick one and discard the
others).</li>
</ul>
<p><strong>Elastic Net</strong> combines the L1 penalty of LASSO and the
L2 penalty of Ridge. The result is a single, flexible model that:</p>
<ol type="1">
<li>Performs <strong>variable selection</strong> (like LASSO).</li>
<li>Handles <strong>correlated features</strong> stably by grouping them
together (like Ridge).</li>
<li>Can select more features than samples (<span class="math inline">\(p
&gt; n\)</span>), which LASSO cannot do.</li>
</ol>
<h3 id="slide-1-the-definition-and-formula-file-...020245.png">Slide 1:
The Definition and Formula (File: <code>...020245.png</code>)</h3>
<p>This slide explains <em>why</em> Elastic Net was created and defines
it <em>mathematically</em>.</p>
<ul>
<li><strong>The Problem:</strong> It states the exact trade-off:
<ul>
<li>“Ridge regression can handle collinearity, but cannot perform
variable selection;”</li>
<li>“LASSO can perform variable selection, but performs poorly when
collinearity;”</li>
</ul></li>
<li><strong>The Solution (The Formula):</strong> The core of the method
is this optimization formula: <span
class="math display">\[\hat{\beta}_{eNet}(\lambda, \alpha) \leftarrow
\arg \min_{\beta} \left( \underbrace{\|\mathbf{y} -
\mathbf{X}\beta\|^2}_{\text{Loss}} + \lambda \left(
\underbrace{\alpha\|\beta\|_1}_{\text{L1 Penalty}} +
\underbrace{\frac{1-\alpha}{2}\|\beta\|_2^2}_{\text{L2 Penalty}} \right)
\right)\]</span></li>
<li><strong>Breaking Down the Formula:</strong>
<ul>
<li><strong><span class="math inline">\(\|\mathbf{y} -
\mathbf{X}\beta\|^2\)</span></strong>: This is the standard “Residual
Sum of Squares” (RSS). We want to find coefficients (<span
class="math inline">\(\beta\)</span>) that make the model’s predictions
(<span class="math inline">\(X\beta\)</span>) as close as possible to
the true values (<span class="math inline">\(y\)</span>).</li>
<li><strong><span class="math inline">\(\lambda\)</span>
(Lambda)</strong>: This is the <strong>master knob</strong> for
<em>total regularization strength</em>. A larger <span
class="math inline">\(\lambda\)</span> means a bigger penalty, which
“shrinks” all coefficients more.</li>
<li><strong><span class="math inline">\(\alpha\)</span>
(Alpha)</strong>: This is the <strong>mixing parameter</strong> that
balances L1 and L2. This is the key innovation.
<ul>
<li><strong><span
class="math inline">\(\alpha\|\beta\|_1\)</span></strong>: This is the
<strong>L1 (LASSO)</strong> part. It forces weak coefficients to become
exactly zero, thus selecting variables.</li>
<li><strong><span
class="math inline">\(\frac{1-\alpha}{2}\|\beta\|_2^2\)</span></strong>:
This is the <strong>L2 (Ridge)</strong> part. It shrinks all
coefficients and, crucially, encourages correlated features to have
similar coefficients (the grouping effect).</li>
</ul></li>
</ul></li>
<li><strong>The Special Cases:</strong>
<ul>
<li>If <strong><span class="math inline">\(\alpha = 0\)</span></strong>,
the L1 term vanishes, and the model becomes pure <strong>Ridge
Regression</strong>.</li>
<li>If <strong><span class="math inline">\(\alpha = 1\)</span></strong>,
the L2 term vanishes, and the model becomes pure <strong>LASSO
Regression</strong>.</li>
<li>If <strong><span class="math inline">\(0 &lt; \alpha &lt;
1\)</span></strong>, you get <strong>Elastic Net</strong>, which
“encourages grouping of correlated variables” <em>and</em> “can perform
variable selection.”</li>
</ul></li>
</ul>
<h3
id="slide-2-the-intuition-and-the-grouping-effect-file-...020249.jpg">Slide
2: The Intuition and The Grouping Effect (File:
<code>...020249.jpg</code>)</h3>
<p>This slide gives you the <em>visual intuition</em> and the
<em>practical proof</em> of why Elastic Net works. It has two parts.</p>
<h4 id="part-1-the-three-graphs-geometric-intuition">Part 1: The Three
Graphs (Geometric Intuition)</h4>
<p>These graphs show the <em>constraint region</em> (the shaded shape)
for each penalty. The model tries to find the best coefficients (<span
class="math inline">\(\theta_{opt}\)</span>), and the final solution
(the green dot) is the first point where the cost function (the blue
ellipses) “touches” the constraint region.</p>
<ul>
<li><strong>L1 Norm (LASSO):</strong> The region is a
<strong>diamond</strong>. Because of its <strong>sharp corners</strong>,
the ellipses are very likely to hit a corner first. At a corner, one of
the coefficients (e.g., <span class="math inline">\(\theta_1\)</span>)
is zero. This is a visual explanation of how LASSO creates
<strong>sparsity</strong> (variable selection).</li>
<li><strong>L2 Norm (Ridge):</strong> The region is a
<strong>circle</strong>. It has <strong>no corners</strong>. The
ellipses will hit a “smooth” point on the circle, shrinking both
coefficients (<span class="math inline">\(\theta_1\)</span> and <span
class="math inline">\(\theta_2\)</span>) but not setting either to zero.
This is <strong>weight sharing</strong>.</li>
<li><strong>L1 + L2 (Elastic Net):</strong> The region is a
<strong>“rounded square”</strong>. It’s the perfect compromise.
<ul>
<li>It has “corners” (like LASSO) so it can still set coefficients to
zero.</li>
<li>It has “curved edges” (like Ridge) so it’s more stable and handles
correlated variables by finding a solution on an edge rather than a
single sharp corner.</li>
</ul></li>
</ul>
<h4 id="part-2-the-formula-the-grouping-effect">Part 2: The Formula (The
Grouping Effect)</h4>
<p>The text at the bottom explains Elastic Net’s “grouping effect.”</p>
<ul>
<li><strong>The Implication:</strong> “If <span
class="math inline">\(x_j \approx x_k\)</span>, then <span
class="math inline">\(\hat{\beta}_j \approx
\hat{\beta}_k\)</span>.”</li>
<li><strong>Meaning:</strong> If two features (<span
class="math inline">\(x_j\)</span> and <span
class="math inline">\(x_k\)</span>) are highly correlated (their values
are very similar), Elastic Net will force their <em>coefficients</em>
(<span class="math inline">\(\hat{\beta}_j\)</span> and <span
class="math inline">\(\hat{\beta}_k\)</span>) to also be very
similar.</li>
<li><strong>Why this is good:</strong> This is the <em>opposite</em> of
LASSO. LASSO would be unstable and might arbitrarily set <span
class="math inline">\(\hat{\beta}_j\)</span> to a large value and <span
class="math inline">\(\hat{\beta}_k\)</span> to zero. Elastic Net
“groups” them: it will either keep <em>both</em> in the model with
similar importance, or it will shrink <em>both</em> of them out of the
model together. This is a much more stable and realistic result.</li>
<li><strong>The Warning:</strong> “LASSO may be unstable in this case!”
This directly highlights the problem that Elastic Net solves.</li>
</ul>
<h3 id="slide-3-the-feature-comparison-table-file-...020255.png">Slide
3: The Feature Comparison Table (File: <code>...020255.png</code>)</h3>
<p>This table is your “cheat sheet” for choosing the right model. It
compares Ridge, LASSO, and Elastic Net on all their key properties.</p>
<ul>
<li><strong>Penalty:</strong> Shows the L2, L1, and combined
penalties.</li>
<li><strong>Sparsity:</strong> Can the model set coefficients to 0?
<ul>
<li>Ridge: <strong>No ❌</strong></li>
<li>LASSO: <strong>Yes ✅</strong></li>
<li>Elastic Net: <strong>Yes ✅</strong></li>
</ul></li>
<li><strong>Variable Selection:</strong> This is a <em>crucial</em> row.
<ul>
<li>LASSO: <strong>Yes ✅</strong>, BUT it has a major limitation: if
you have more features than samples (<span class="math inline">\(p &gt;
n\)</span>), LASSO can select <em>at most</em> <span
class="math inline">\(n\)</span> features.</li>
<li>Elastic Net: <strong>Yes ✅</strong>, and it <strong>can select more
than <span class="math inline">\(n\)</span> variables</strong>. This
makes it the clear choice for “wide” data problems (e.g., in genomics,
where <span class="math inline">\(p=20,000\)</span> features and <span
class="math inline">\(n=100\)</span> samples).</li>
</ul></li>
<li><strong>Grouping Effect:</strong> How does it handle correlated
features?
<ul>
<li>Ridge: <strong>Strong ✅</strong></li>
<li>LASSO: <strong>Weak ❌</strong> (it “picks one”)</li>
<li>Elastic Net: <strong>Strong ✅</strong></li>
</ul></li>
<li><strong>Solution Uniqueness:</strong> Is the answer stable?
<ul>
<li>Ridge: <strong>Always ✅</strong></li>
<li>LASSO: <strong>No ❌</strong> (not if <span
class="math inline">\(X\)</span> is “rank-deficient,” e.g., <span
class="math inline">\(p &gt; n\)</span> or correlated features)</li>
<li>Elastic Net: <strong>Always ✅</strong> (as long as <span
class="math inline">\(\alpha &lt; 1\)</span>, the Ridge component
guarantees a unique, stable solution).</li>
</ul></li>
<li><strong>Use Case:</strong> When should you use each?
<ul>
<li><strong>Ridge:</strong> For prediction, especially with
<strong>multicollinearity</strong>.</li>
<li><strong>LASSO:</strong> For <strong>interpretability</strong> and
creating <strong>sparse models</strong> (when you think only a few
features matter).</li>
<li><strong>Elastic Net:</strong> The best all-arounder. Use it for
<strong>correlated predictors</strong>, when <strong><span
class="math inline">\(p \gg n\)</span></strong>, or when you need both
<strong>sparsity + stability</strong>.</li>
</ul></li>
</ul>
<h3 id="code-understanding-python-scikit-learn">Code Understanding
(Python <code>scikit-learn</code>)</h3>
<p>When you use this in Python, be aware of a common confusion in the
parameter names:</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Concept (from your slides)</th>
<th style="text-align: left;"><code>scikit-learn</code> Parameter</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong><span
class="math inline">\(\lambda\)</span></strong> (Lambda)</td>
<td style="text-align: left;"><code>alpha</code></td>
<td style="text-align: left;">The <strong>overall strength</strong> of
regularization.</td>
</tr>
<tr>
<td style="text-align: left;"><strong><span
class="math inline">\(\alpha\)</span></strong> (Alpha)</td>
<td style="text-align: left;"><code>l1_ratio</code></td>
<td style="text-align: left;">The <strong>mixing parameter</strong>
between L1 and L2.</td>
</tr>
</tbody>
</table>
<p><strong>Example:</strong> An <code>l1_ratio</code> of <code>0</code>
is Ridge. An <code>l1_ratio</code> of <code>1</code> is LASSO. An
<code>l1_ratio</code> of <code>0.5</code> is a 50/50 mix.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet, ElasticNetCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Initialize a specific model</span></span><br><span class="line"><span class="comment"># This uses 0.5 for lambda (slide&#x27;s alpha) and 0.1 for lambda (slide&#x27;s lambda)</span></span><br><span class="line">model = ElasticNet(alpha=<span class="number">0.1</span>, l1_ratio=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. A much better way: Find the best parameters automatically</span></span><br><span class="line"><span class="comment"># This will test l1_ratios of 0.1, 0.5, and 0.9</span></span><br><span class="line"><span class="comment"># and automatically find the best &#x27;alpha&#x27; (strength) for each.</span></span><br><span class="line">cv_model = ElasticNetCV(</span><br><span class="line">    l1_ratio=[<span class="number">.1</span>, <span class="number">.5</span>, <span class="number">.9</span>],</span><br><span class="line">    cv=<span class="number">5</span>  <span class="comment"># 5-fold cross-validation</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit the model to your data (X_train, y_train)</span></span><br><span class="line"><span class="comment"># cv_model.fit(X_train, y_train)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. See the best parameters it found</span></span><br><span class="line"><span class="comment"># print(f&quot;Best l1_ratio (slide&#x27;s alpha): &#123;cv_model.l1_ratio_&#125;&quot;)</span></span><br><span class="line"><span class="comment"># print(f&quot;Best alpha (slide&#x27;s lambda): &#123;cv_model.alpha_&#125;&quot;)</span></span><br></pre></td></tr></table></figure>
<h1 id="high-dimensional-data-analysis">11. High-Dimensional Data
Analysis</h1>
<h2 id="the-core-problem-large-p-small-n">The Core Problem: Large <span
class="math inline">\(p\)</span>, Small <span
class="math inline">\(n\)</span></h2>
<p>The slides introduce the challenge of high-dimensional data, which is
defined by having <strong>many more features (predictors) <span
class="math inline">\(p\)</span> than observations (samples) <span
class="math inline">\(n\)</span></strong>. This is often written as
<strong><span class="math inline">\(p \gg n\)</span></strong>.</p>
<ul>
<li><strong>Example:</strong> Predicting blood pressure (the response
<span class="math inline">\(y\)</span>) using millions of genetic
markers (SNPs) as features <span class="math inline">\(X\)</span>, but
only having data from a few hundred patients.</li>
<li><strong>Troubles:</strong>
<ul>
<li><strong>Overfitting:</strong> Models become “too flexible” and learn
the noise in the training data, rather than the true underlying
pattern.</li>
<li><strong>Non-Unique Solution:</strong> When <span
class="math inline">\(p &gt; n\)</span>, the standard least squares
linear regression model doesn’t even have a unique solution.</li>
<li><strong>Misleading Metrics:</strong> This leads to a common symptom:
a very small <strong>training error</strong> (or high <span
class="math inline">\(R^2\)</span>) but a very large <strong>test
error</strong>.</li>
</ul></li>
</ul>
<h2 id="most-important-image-the-overfitting-trap-figure-6.23">Most
Important Image: The Overfitting Trap (Figure 6.23)</h2>
<p>Figure 6.23 (from the first uploaded image) is the most critical
visual for understanding the <em>problem</em>. It shows what happens
when you add features (variables) that are <em>completely unrelated</em>
to the outcome.</p>
<ul>
<li><strong>Left Plot (R²):</strong> The <span
class="math inline">\(R^2\)</span> on the training data increases
towards 1. This <em>looks</em> like a perfect fit.</li>
<li><strong>Center Plot (Training MSE):</strong> The Mean Squared Error
on the <em>training</em> data decreases to 0. This also <em>looks</em>
perfect.</li>
<li><strong>Right Plot (Test MSE):</strong> The Mean Squared Error on
the <em>test</em> data (new, unseen data) explodes. This reveals the
model is garbage and has just memorized the training set.</li>
</ul>
<p>⚠️ <strong>This is the key takeaway:</strong> In high dimensions,
<span class="math inline">\(R^2\)</span> and training MSE are
<strong>useless</strong> and <strong>misleading</strong> metrics for
model quality.</p>
<h2 id="the-solution-regularization-model-selection">The Solution:
Regularization &amp; Model Selection</h2>
<p>To combat overfitting, we must use <strong>less flexible
models</strong>. The main strategy is <strong>regularization</strong>
(also called shrinkage), which involves adding a penalty term to the
cost function to “shrink” the model coefficients (<span
class="math inline">\(\beta\)</span>).</p>
<h3 id="mathematical-formulas-python-code">Mathematical Formulas &amp;
Python Code 🐍</h3>
<p>The standard <strong>Least Squares</strong> cost function you try to
minimize is: <span class="math display">\[\text{RSS} = \sum_{i=1}^n
\left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j\right)^2 \quad
\text{or} \quad \|y - X\beta\|^2_2\]</span> This fails when <span
class="math inline">\(p &gt; n\)</span>. The solutions modify this:</p>
<h4 id="a.-ridge-regression-l_2-penalty">A. Ridge Regression (<span
class="math inline">\(L_2\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> Shrinks all coefficients towards zero, but
never <em>to</em> zero. It’s good when many features are related to the
outcome.</li>
<li><strong>Math Formula:</strong> <span
class="math display">\[\text{Minimize: } \left( \|y - X\beta\|^2_2 +
\lambda \sum_{j=1}^p \beta_j^2 \right)\]</span>
<ul>
<li>The <span class="math inline">\(\lambda \sum_{j=1}^p
\beta_j^2\)</span> is the <strong><span
class="math inline">\(L_2\)</span> penalty</strong>.</li>
<li><span class="math inline">\(\lambda\)</span> (lambda) is a
<em>tuning parameter</em> that controls the penalty strength. A larger
<span class="math inline">\(\lambda\)</span> means more shrinkage.</li>
</ul></li>
<li><strong>Python (Scikit-learn):</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># alpha is the lambda (λ) tuning parameter</span></span><br><span class="line"><span class="comment"># We find the best alpha using cross-validation</span></span><br><span class="line">ridge_model = Ridge(alpha=<span class="number">1.0</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">ridge_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate using test error (e.g., MSE on test set)</span></span><br><span class="line"><span class="comment"># NOT with training R-squared</span></span><br><span class="line">test_score = ridge_model.score(X_test, y_test) </span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="b.-the-lasso-l_1-penalty">B. The Lasso (<span
class="math inline">\(L_1\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> This is a very important method. The <span
class="math inline">\(L_1\)</span> penalty can force coefficients to be
<strong>exactly zero</strong>. This means Lasso performs
<strong>automatic feature selection</strong>, creating a <em>sparse</em>
model.</li>
<li><strong>Math Formula:</strong> <span
class="math display">\[\text{Minimize: } \left( \|y - X\beta\|^2_2 +
\lambda \sum_{j=1}^p |\beta_j| \right)\]</span>
<ul>
<li>The <span class="math inline">\(\lambda \sum_{j=1}^p
|\beta_j|\)</span> is the <strong><span
class="math inline">\(L_1\)</span> penalty</strong>.</li>
<li>Again, <span class="math inline">\(\lambda\)</span> is the tuning
parameter.</li>
</ul></li>
<li><strong>Python (Scikit-learn):</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"></span><br><span class="line"><span class="comment"># alpha is the lambda (λ) tuning parameter</span></span><br><span class="line">lasso_model = Lasso(alpha=<span class="number">0.1</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">lasso_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The model automatically selects features</span></span><br><span class="line"><span class="comment"># Coefficients that are zero were &#x27;dropped&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(lasso_model.coef_) </span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="c.-other-methods">C. Other Methods</h4>
<p>The slides also mention:</p>
<ul>
<li><strong>Forward Stepwise Selection:</strong> A different approach
where you start with no features and add them one by one, picking the
one that improves the model most (based on a criterion like
cross-validation error).</li>
<li><strong>Principal Components Regression (PCR):</strong> A
dimensionality reduction technique.</li>
</ul>
<h2 id="the-curse-of-dimensionality-figure-6.24">The Curse of
Dimensionality (Figure 6.24)</h2>
<p>This example (Figures 6.24 and its description) shows a more subtle
problem.</p>
<ul>
<li><strong>Setup:</strong> A model with <span
class="math inline">\(n=100\)</span> observations and 20 <em>true</em>
features.</li>
<li><strong>Plots:</strong> They test Lasso by adding more and more
<em>irrelevant</em> features:
<ul>
<li><strong><span class="math inline">\(p=20\)</span> (Left):</strong>
Lasso performs well. The lowest test MSE is found with minimal
regularization.</li>
<li><strong><span class="math inline">\(p=50\)</span> (Center):</strong>
Lasso still works well, but it needs more regularization (a smaller
“Degrees of Freedom”) to filter out the 30 junk features.</li>
<li><strong><span class="math inline">\(p=2000\)</span>
(Right):</strong> This is the <strong>curse of dimensionality</strong>.
Even with a good method like Lasso, the 1,980 irrelevant features add so
much noise that the model <strong>performs poorly regardless</strong> of
the tuning parameter. The true signal is “lost in the noise.”</li>
</ul></li>
</ul>
<h2 id="summary-cautions-for-p-n">Summary: Cautions for <span
class="math inline">\(p &gt; n\)</span></h2>
<p>The final slide gives the most important rules to follow:</p>
<ol type="1">
<li><strong>Beware Extreme Multicollinearity:</strong> When <span
class="math inline">\(p &gt; n\)</span>, your features are
mathematically guaranteed to be linearly related, which breaks standard
regression.</li>
<li><strong>Don’t Overstate Results:</strong> A model you find (e.g.,
with Lasso) is just <em>one</em> of many potentially good models.</li>
<li><strong>🚫 DO NOT USE</strong> training <span
class="math inline">\(R^2\)</span>, <span
class="math inline">\(p\)</span>-values, or training MSE to justify your
model. As Figure 6.23 showed, they are misleading.</li>
<li><strong>✅ DO USE</strong> <strong>test error</strong> and
<strong>cross-validation error</strong> to choose your model and assess
its performance.</li>
</ol>
<h2 id="the-core-problem-p-gg-n-the-troubles-slide">The Core Problem:
<span class="math inline">\(p \gg n\)</span> (The “Troubles” Slide)</h2>
<p>This slide (filename: <code>...020259.png</code>) sets up the entire
problem. The issue isn’t just “overfitting”; it’s a fundamental
mathematical breakdown of standard methods.</p>
<ul>
<li><strong>“Large <span class="math inline">\(p\)</span> makes our
linear regression model too flexible”</strong>: This is an
understatement. It leads to a problem called an <strong>underdetermined
system</strong>.</li>
<li><strong>“If <span class="math inline">\(p &gt; n\)</span>, the LSE
is not even uniquely determined”</strong>: This is the most important
technical point.
<ul>
<li><strong>Mathematical Reason:</strong> The standard solution for
Ordinary Least Squares (OLS) is <span class="math inline">\(\hat{\beta}
= (X^T X)^{-1} X^T y\)</span>.</li>
<li><span class="math inline">\(X\)</span> is the data matrix with <span
class="math inline">\(n\)</span> rows (observations) and <span
class="math inline">\(p\)</span> columns (features).</li>
<li>The matrix <span class="math inline">\(X^T X\)</span> has dimensions
<span class="math inline">\(p \times p\)</span>.</li>
<li>When <span class="math inline">\(p &gt; n\)</span>, the <span
class="math inline">\(X^T X\)</span> matrix is
<strong>singular</strong>, which means its determinant is zero and it
<strong>cannot be inverted</strong>. The <span
class="math inline">\((X^T X)^{-1}\)</span> term does not exist.</li>
<li><strong>“Extreme multicollinearity”</strong> (from slide
<code>...020744.png</code>) is the direct cause. When <span
class="math inline">\(p &gt; n\)</span>, the columns of <span
class="math inline">\(X\)</span> (the features) are <em>guaranteed</em>
to be linearly dependent. There are infinite combinations of the
features that can explain the data.</li>
</ul></li>
</ul>
<h2 id="the-simplest-example-n2-figure-6.22">The Simplest Example: <span
class="math inline">\(n=2\)</span> (Figure 6.22)</h2>
<p>This slide (filename: <code>...020728.png</code>) is the
<em>perfect</em> illustration of the “not uniquely determined”
problem.</p>
<ul>
<li><strong>Left Plot (Low-D):</strong> Many points (<span
class="math inline">\(n\)</span>), only two parameters (<span
class="math inline">\(p=2\)</span>: intercept <span
class="math inline">\(\beta_0\)</span> and slope <span
class="math inline">\(\beta_1\)</span>). The line is a “best fit” that
balances the errors. The training error (RSS) is non-zero.</li>
<li><strong>Right Plot (High-D):</strong> We have <span
class="math inline">\(n=2\)</span> observations and <span
class="math inline">\(p=2\)</span> parameters.
<ul>
<li>You have two equations (one for each point) and two unknowns (<span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>).</li>
<li>The model has <em>exactly</em> enough flexibility to pass
<em>perfectly</em> through both points.</li>
<li>The result is <strong>zero training error</strong>.</li>
<li>This “perfect” fit is an illusion. If you got a <em>new</em> data
point, this line would almost certainly be a terrible predictor. This is
the essence of overfitting.</li>
</ul></li>
</ul>
<h2 id="the-consequence-misleading-metrics-figure-6.23">The Consequence:
Misleading Metrics (Figure 6.23)</h2>
<p>This slide (filename: <code>...020730.png</code>) scales up the
problem from <span class="math inline">\(n=2\)</span> to <span
class="math inline">\(n=20\)</span> and shows <em>why</em> you must be
cautious.</p>
<ul>
<li><strong>The Setup:</strong> <span
class="math inline">\(n=20\)</span> observations. We start with 1
feature and add more and more <em>irrelevant, junk</em> features.</li>
<li><strong>Left Plot (<span
class="math inline">\(R^2\)</span>):</strong> The <span
class="math inline">\(R^2\)</span> on the training data steadily
increases towards 1 as we add features. This is because, by pure chance,
each new junk feature can explain a tiny bit more of the noise in the
training set.</li>
<li><strong>Center Plot (Training MSE):</strong> The training error
drops to 0. This is the same as the <span
class="math inline">\(n=2\)</span> plot. Once the number of features
(<span class="math inline">\(p\)</span>) gets close to the number of
observations (<span class="math inline">\(n=20\)</span>), the model can
perfectly fit the 20 data points, even if the features are random
noise.</li>
<li><strong>Right Plot (Test MSE):</strong> This is the “truth.” The
<em>actual</em> error on new, unseen data gets worse and worse. By
adding noise features, we are just “memorizing” the training set, and
our model’s ability to generalize is destroyed.</li>
<li><strong>Key Lesson:</strong> (from slide <code>...020744.png</code>)
This is why you must <strong>“Avoid using… <span
class="math inline">\(p\)</span>-values, <span
class="math inline">\(R^2\)</span>, or other traditional measures of
model on training as evidence of good fit.”</strong> They are guaranteed
to lie to you when <span class="math inline">\(p &gt; n\)</span>.</li>
</ul>
<h2 id="the-solutions-the-deal-with-slide">The Solutions (The “Deal
with…” Slide)</h2>
<p>This slide (filename: <code>...020734.png</code>) lists the
strategies to fix this. The core idea is <strong>regularization</strong>
(or shrinkage). We add a “penalty” to the cost function to stop the
<span class="math inline">\(\beta\)</span> coefficients from getting too
large or too numerous.</p>
<h4 id="a.-ridge-regression-l_2-penalty-1">A. Ridge Regression (<span
class="math inline">\(L_2\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> Keeps all <span
class="math inline">\(p\)</span> features, but shrinks their
coefficients. It’s excellent for handling multicollinearity.</li>
<li><strong>Math:</strong> <span class="math inline">\(\text{Minimize: }
\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j\right)^2 +
\lambda \sum_{j=1}^p \beta_j^2\)</span>
<ul>
<li>The first part is the standard RSS.</li>
<li>The <span class="math inline">\(\lambda \sum \beta_j^2\)</span> is
the <strong><span class="math inline">\(L_2\)</span> penalty</strong>.
It punishes large coefficient values.</li>
</ul></li>
<li><strong><span class="math inline">\(\lambda\)</span>
(Lambda):</strong> This is the <strong>tuning parameter</strong>.
<ul>
<li>If <span class="math inline">\(\lambda=0\)</span>, it’s just OLS
(which fails).</li>
<li>If <span class="math inline">\(\lambda \to \infty\)</span>, all
<span class="math inline">\(\beta\)</span>’s are shrunk to 0.</li>
<li>The right <span class="math inline">\(\lambda\)</span> is chosen via
<strong>cross-validation</strong>.</li>
</ul></li>
</ul>
<h4 id="b.-the-lasso-l_1-penalty-1">B. The Lasso (<span
class="math inline">\(L_1\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> This is often preferred because it
performs <strong>automatic feature selection</strong>. It shrinks many
coefficients to be <strong>exactly zero</strong>.</li>
<li><strong>Math:</strong> <span class="math inline">\(\text{Minimize: }
\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j\right)^2 +
\lambda \sum_{j=1}^p |\beta_j|\)</span>
<ul>
<li>The <span class="math inline">\(\lambda \sum |\beta_j|\)</span> is
the <strong><span class="math inline">\(L_1\)</span> penalty</strong>.
This absolute value penalty is what allows coefficients to become
exactly 0.</li>
</ul></li>
<li><strong>Benefit:</strong> The final model is <em>sparse</em> (e.g.,
it might say “out of 2,000 features, only these 15 matter”).</li>
</ul>
<h4 id="c.-tuning-parameter-choice-the-real-work">C. Tuning Parameter
Choice (The <em>Real</em> Work)</h4>
<p>How do you pick the best <span
class="math inline">\(\lambda\)</span>? You must use the data you have.
The slides mention this and “cross validation error” (from
<code>...020744.png</code>).</p>
<ul>
<li><strong>Python Code (Scikit-learn):</strong> You don’t just guess
<code>alpha</code> (which is <span
class="math inline">\(\lambda\)</span> in scikit-learn). You use a tool
like <code>LassoCV</code> or <code>GridSearchCV</code> to find the best
one. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a high-dimensional dataset</span></span><br><span class="line">X, y = make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">500</span>, n_informative=<span class="number">10</span>, noise=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># LassoCV automatically performs cross-validation to find the best alpha (lambda)</span></span><br><span class="line"><span class="comment"># cv=10 means 10-fold cross-validation</span></span><br><span class="line">lasso_cv_model = LassoCV(cv=<span class="number">10</span>, random_state=<span class="number">0</span>, max_iter=<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">lasso_cv_model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is the best lambda (alpha) it found:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best alpha (lambda): <span class="subst">&#123;lasso_cv_model.alpha_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can now see the coefficients</span></span><br><span class="line"><span class="comment"># Most of the 500 coefficients will be 0.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of non-zero features: <span class="subst">&#123;np.<span class="built_in">sum</span>(lasso_cv_model.coef_ != <span class="number">0</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="a-final-warning-the-curse-of-dimensionality-figure-6.24">A Final
Warning: The Curse of Dimensionality (Figure 6.24)</h2>
<p>This final set of slides (filenames: <code>...020738.png</code> and
<code>...020741.jpg</code>) provides a crucial, subtle warning:
<strong>Regularization is not magic.</strong></p>
<ul>
<li><strong>The Setup:</strong> <span
class="math inline">\(n=100\)</span> observations. There are <strong>20
real features</strong> that truly affect the response.</li>
<li><strong>The Experiment:</strong> They run Lasso three times, adding
more and more <em>noise</em> features:
<ul>
<li><strong>Left Plot (<span
class="math inline">\(p=20\)</span>):</strong> All 20 features are real.
The lowest test MSE is found with minimal regularization (high “Degrees
of Freedom,” meaning many non-zero coefficients). This makes sense; you
want to keep all 20 real features.</li>
<li><strong>Center Plot (<span
class="math inline">\(p=50\)</span>):</strong> Now we have 20 real
features + 30 noise features. Lasso still works! The best model is found
with more regularization (fewer “Degrees of Freedom”). Lasso
successfully “zeroed out” many of the 30 noise features.</li>
<li><strong>Right Plot (<span
class="math inline">\(p=2000\)</span>):</strong> This is the
<strong>curse of dimensionality</strong>. We have 20 real features +
1980 noise features. The <em>noise</em> has completely overwhelmed the
<em>signal</em>. <strong>Lasso fails.</strong> The test MSE is high
<em>no matter what</em> tuning parameter you choose. The model cannot
distinguish the 20 real features from the 1980 junk ones.</li>
</ul></li>
</ul>
<p><strong>Final Takeaway:</strong> Even with advanced methods like
Lasso, if your <span class="math inline">\(p \gg n\)</span> problem is
<em>too</em> extreme (i.S. the signal-to-noise ratio is too low), it may
be impossible to build a good predictive model.</p>
<h2 id="the-goal-collaborative-filtering">The Goal: “Collaborative
Filtering”</h2>
<p>The first slide (<code>...021218.png</code>) uses the term
<strong>Collaborative Filtering</strong>. This is the key concept. The
model “collaborates” by using the ratings of <em>all</em> users to fill
in the blanks for a <em>single</em> user.</p>
<ul>
<li><strong>How it works:</strong> The model assumes your “taste”
(vector <span class="math inline">\(\mathbf{u}_i\)</span>) can be
described as a combination of <span class="math inline">\(r\)</span>
“latent features” (e.g., <span class="math inline">\(r=3\)</span>: %
action, % comedy, % drama). It <em>also</em> assumes each movie (vector
<span class="math inline">\(\mathbf{v}_j\)</span>) has a profile on
these same features.</li>
<li>Your predicted rating for a movie is the dot product of your taste
vector and the movie’s feature vector.</li>
<li>The model finds the best “taste” vectors <span
class="math inline">\(\mathbf{U}\)</span> and “movie” vectors <span
class="math inline">\(\mathbf{V}\)</span> that explain all the known
ratings <em>simultaneously</em>. It’s collaborative because Lee’s
ratings help define the features of “Bullet Train” (<span
class="math inline">\(\mathbf{v}_2\)</span>), which in turn helps
predict Yang’s rating for that same movie.</li>
</ul>
<h2 id="the-hard-problem-and-its-2-flavors">The Hard Problem (and its 2
Flavors)</h2>
<p>The second slide (<code>...021222.png</code>) presents the intuitive,
but computationally <em>very</em> hard, way to frame the problem.</p>
<h4 id="detail-1-noise-vs.-no-noise">Detail 1: Noise vs. No Noise</h4>
<p>The slide shows <span class="math inline">\(\mathbf{Y} = \mathbf{M} +
\mathbf{E}\)</span>. This is critical. * <span
class="math inline">\(\mathbf{M}\)</span> is the “true,” “clean,”
underlying low-rank matrix of everyone’s “true” preferences. * <span
class="math inline">\(\mathbf{E}\)</span> is a matrix of random noise.
(e.g., your true rating is 4.3, but you entered a 4; or you were in a
bad mood and rated a 3). * <span
class="math inline">\(\mathbf{Y}\)</span> is the <em>noisy data</em> we
actually observe.</p>
<p>Because of this noise, we don’t expect to find a matrix <span
class="math inline">\(\mathbf{N}\)</span> that <em>perfectly</em>
matches our data. Instead, we try to find a low-rank <span
class="math inline">\(\mathbf{N}\)</span> that is <em>as close as
possible</em>. This leads to the formula: <span
class="math display">\[\underset{\text{rank}(\mathbf{N}) \le
r}{\text{minimize}} \quad \left\| \mathcal{P}_{\mathcal{O}}(\mathbf{Y} -
\mathbf{N}) \right\|_{\text{F}}^2\]</span> This says: “Find a matrix
<span class="math inline">\(\mathbf{N}\)</span> (of rank <span
class="math inline">\(r\)</span> or less) that minimizes the sum of
squared errors <em>only on the ratings we observed</em> (<span
class="math inline">\(\mathcal{O}\)</span>).”</p>
<h4
id="detail-2-why-is-textrankmathbfn-le-r-a-non-convex-constraint">Detail
2: Why is <span class="math inline">\(\text{rank}(\mathbf{N}) \le
r\)</span> a “Non-convex constraint”?</h4>
<p>This is the “difficult to optimize” part. A convex problem is
(simplistically) one with a single valley, making it easy to find the
single lowest point. A non-convex problem has many local valleys, and an
algorithm can get stuck in a “pretty good” valley instead of the “best”
one.</p>
<p>The rank constraint is non-convex. For example, the average of two
rank-1 matrices is <em>not</em> necessarily a rank-1 matrix (it could be
rank-2). This lack of a “smooth valley” property makes the problem
NP-hard.</p>
<h4 id="detail-3-the-number-of-parameters-rd_1-d_2">Detail 3: The Number
of Parameters: <span class="math inline">\(r(d_1 + d_2)\)</span></h4>
<p>The slide asks, “how many entries are needed?” The answer is based on
the number of unknown parameters. * A rank-<span
class="math inline">\(r\)</span> matrix <span
class="math inline">\(\mathbf{M}\)</span> can be factored into <span
class="math inline">\(\mathbf{U}\)</span> (which is <span
class="math inline">\(d_1 \times r\)</span>) and <span
class="math inline">\(\mathbf{V}^T\)</span> (which is <span
class="math inline">\(r \times d_2\)</span>). * The number of entries in
<span class="math inline">\(\mathbf{U}\)</span> is <span
class="math inline">\(d_1 \times r\)</span>. * The number of entries in
<span class="math inline">\(\mathbf{V}\)</span> is <span
class="math inline">\(d_2 \times r\)</span>. * Total “unknowns” to solve
for: <span class="math inline">\(d_1 r + d_2 r = r(d_1 + d_2)\)</span>.
* This means we must have <em>at least</em> <span
class="math inline">\(r(d_1 + d_2)\)</span> observed ratings to have any
hope of uniquely solving for <span
class="math inline">\(\mathbf{U}\)</span> and <span
class="math inline">\(\mathbf{V}\)</span>. If our number of observations
<span class="math inline">\(|\mathcal{O}|\)</span> is less than this,
the problem is hopelessly underdetermined.</p>
<h2 id="the-magic-solution-convex-relaxation">The “Magic” Solution:
Convex Relaxation</h2>
<p>The final slide (<code>...021225.png</code>) presents the
groundbreaking solution from Candès and Recht. This solution cleverly
<em>changes the problem</em> to one that is convex and solvable.</p>
<h4
id="detail-1-the-l1-norm-analogy-this-is-the-most-important-concept">Detail
1: The L1-Norm Analogy (This is the most important concept)</h4>
<p>This is the key to understanding <em>why</em> this works.</p>
<ul>
<li><strong>In Vectors (Lasso):</strong>
<ul>
<li><strong>Hard Problem:</strong> Find the <em>sparsest</em> vector
<span class="math inline">\(\beta\)</span> (fewest non-zeros). This is
<span class="math inline">\(L_0\)</span> norm, <span
class="math inline">\(\text{minimize } \|\beta\|_0\)</span>. This is
non-convex.</li>
<li><strong>Easy Problem:</strong> Minimize the <span
class="math inline">\(L_1\)</span> norm, <span
class="math inline">\(\text{minimize } \|\beta\|_1 = \sum
|\beta_j|\)</span>. This is convex, and it’s a “relaxation” that
<em>also</em> produces sparse solutions.</li>
</ul></li>
<li><strong>In Matrices (Matrix Completion):</strong>
<ul>
<li><strong>Hard Problem:</strong> Find the <em>lowest-rank</em> matrix
<span class="math inline">\(\mathbf{X}\)</span>. Rank is the number of
non-zero singular values. This is <span
class="math inline">\(\text{minimize } \text{rank}(\mathbf{X})\)</span>.
This is non-convex.</li>
<li><strong>Easy Problem:</strong> Minimize the <strong>Nuclear
Norm</strong>, <span class="math inline">\(\text{minimize }
\|\mathbf{X}\|_* = \sum \sigma_i(\mathbf{X})\)</span> (where <span
class="math inline">\(\sigma_i\)</span> are the singular values). This
is convex, and it’s the “matrix equivalent” of the <span
class="math inline">\(L_1\)</span> norm. It’s a relaxation that
<em>also</em> produces low-rank solutions.</li>
</ul></li>
</ul>
<h4 id="detail-2-noiseless-vs.-noisy-again">Detail 2: Noiseless
vs. Noisy (Again)</h4>
<p>Notice the <em>constraint</em> in this new problem: <span
class="math display">\[\text{Minimize } \quad \|\mathbf{X}\|_*\]</span>
<span class="math display">\[\text{Subject to } \quad X_{ij} = M_{ij},
\quad (i, j) \in \mathcal{O}\]</span></p>
<p>This formulation is for the <strong>noiseless</strong> case. It
assumes the <span class="math inline">\(M_{ij}\)</span> we observed are
<em>perfectly accurate</em>. It demands that our solution <span
class="math inline">\(\mathbf{X}\)</span> <em>exactly matches</em> the
known ratings. This is different from the optimization problem on the
previous slide, which just tried to get <em>close</em> to the noisy data
<span class="math inline">\(\mathbf{Y}\)</span>.</p>
<p>(In practice, you solve a noisy-aware version that combines both
ideas, but the slide shows the original, “exact completion”
problem.)</p>
<h4 id="detail-3-the-guarantee-what-the-math-at-the-bottom-means">Detail
3: The Guarantee (What the math at the bottom means)</h4>
<p><span class="math display">\[\text{If } \mathcal{O} \text{ is
randomly sampled and } |\mathcal{O}| \gg r(d_1+d_2)\log(d_1+d_2),
\text{... then the solution is unique and } \mathbf{M}
\text{...}\]</span></p>
<p>This is the punchline. The Candès paper <em>proved</em> that if you
have <em>enough</em> (but still very few) <em>randomly</em> sampled
ratings, solving this easy convex problem (minimizing the nuclear norm)
will <em>magically give you the exact, true, low-rank matrix <span
class="math inline">\(\mathbf{M}\)</span></em>.</p>
<ul>
<li><strong><span class="math inline">\(|\mathcal{O}| \gg
r(d_1+d_2)\)</span></strong>: This part makes sense. We need <em>at
least</em> as many observations as our <span
class="math inline">\(r(d_1+d_2)\)</span> degrees of freedom.</li>
<li><strong><span class="math inline">\(\log(d_1+d_2)\)</span></strong>:
This “log” factor is the “price” we pay for not knowing <em>where</em>
the information is. It’s an astonishingly small price.</li>
<li><strong>Example:</strong> For a 1,000,000 user x 10,000 movie matrix
(like Netflix) with <span class="math inline">\(r=10\)</span>, you don’t
need <span class="math inline">\(\approx 10^{10}\)</span> ratings. You
need a number closer to <span class="math inline">\(10 \times (10^6 +
10^4) \times \log(\dots)\)</span>, which is <em>dramatically</em>
smaller. This is why this method is practical.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/06/5054C5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/06/5054C5/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-10-06 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-06T21:00:00+08:00">2025-10-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-10-19 22:11:02" itemprop="dateModified" datetime="2025-10-19T22:11:02+08:00">2025-10-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>统计机器学习Lecture-5</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="resampling">1. Resampling</h1>
<p><strong>Resampling</strong> as a statistical tool to assess the
accuracy of models whose main goal is to estimate the <em>test
error</em> (a model’s performance on new, unseen data) because the
<em>training error</em> is overly optimistic due to overfitting.</p>
<p><strong>重采样</strong>是一种统计工具，用于评估模型的准确性，其主要目标是估计<em>测试误差</em>（模型在新的、未见过的数据上的表现），因为由于过拟合导致<em>训练误差</em>过于乐观。</p>
<h2 id="key-concepts">Key Concepts</h2>
<ul>
<li><strong>Resampling:</strong> The process of repeatedly drawing
samples from a dataset. The two main types mentioned are
<strong>Cross-validation</strong> (to estimate model test error) and
<strong>Bootstrap</strong> (to quantify the uncertainty of estimates).
从数据集中反复抽取样本的过程。主要提到的两种类型是<strong>交叉验证</strong>（用于估计模型测试误差）和<strong>自举</strong>（用于量化估计的不确定性）。</li>
<li><strong>Data Splitting (Ideal Scenario):</strong> In a “data-rich”
situation, you split your data into three parts:
**在“数据丰富”的情况下，您可以将数据拆分为三部分：
<ol type="1">
<li><strong>Training Data:</strong> Used to fit and train the parameters
of various models.用于拟合和训练各种模型的参数。</li>
<li><strong>Validation Data:</strong> Used to assess the trained models,
tune hyperparameters (e.g., choose the polynomial degree), and select
the <em>best</em> model. This helps prevent
overfitting.用于评估已训练的模型、调整超参数（例如，选择多项式的次数）并选择<em>最佳</em>模型。这有助于防止过度拟合。</li>
<li><strong>Test Data:</strong> Used <em>only once</em> on the final,
selected model to get an unbiased estimate of its real-world
performance.
在最终选定的模型上仅使用一次，以获得其实际性能的无偏估计。</li>
</ol></li>
<li><strong>Validation vs. Test Data:</strong> The slides emphasize this
difference (Slide 7). The <strong>validation set</strong> is part of the
model-building and selection process. The <strong>test set</strong> is
kept separate and is only used for the final report card after all
decisions are
made.<strong>验证集</strong>是模型构建和选择过程的一部分。<strong>测试集</strong>是独立的，仅在所有决策完成后用于最终报告。</li>
</ul>
<h2 id="the-validation-set-approach">The Validation Set Approach</h2>
<p>This is the simplest cross-validation
method.这是最简单的交叉验证方法。</p>
<ol type="1">
<li><strong>Split:</strong> The total dataset is randomly divided into
two parts: a <strong>training set</strong> and a <strong>validation
set</strong> (often a 50/50 or 70/30
split).将整个数据集随机分成两部分：<strong>训练集</strong>和<strong>验证集</strong>（通常为
50/50 或 70/30 的比例）。</li>
<li><strong>Train:</strong> Various models are fit <em>only</em> on the
<strong>training
set</strong>.各种模型<em>仅</em>在<strong>训练集</strong>上进行拟合。</li>
<li><strong>Validate:</strong> The performance of each trained model is
evaluated using the <strong>validation set</strong>.
使用<strong>验证集</strong>评估每个训练模型的性能。</li>
<li><strong>Select:</strong> The model with the best performance (e.g.,
the lowest error) on the validation set is chosen as the final model.
选择在验证集上性能最佳（例如，误差最小）的模型作为最终模型。</li>
</ol>
<h3 id="important-image-schematic-slide-10">Important Image: Schematic
(Slide 10)</h3>
<p>This diagram clearly shows a set of <span
class="math inline">\(n\)</span> observations being randomly split into
a training set (blue, with observations 7, 22, 13) and a validation set
(beige, with observation 91). The model learns from the blue set and is
tested on the beige set. 此图清晰地展示了一组 <span
class="math inline">\(n\)</span>
个观测值被随机分成训练集（蓝色，观测值编号为
7、22、13）和验证集（米色，观测值编号为
91）。模型从蓝色数据集进行学习，并在米色数据集上进行测试。</p>
<h2 id="example-auto-data-formulas-code">Example: Auto Data (Formulas
&amp; Code)</h2>
<p>The slides use the <code>Auto</code> dataset to decide the best
polynomial degree to predict <code>mpg</code> from
<code>horsepower</code>.</p>
<h3 id="mathematical-models">Mathematical Models</h3>
<p>The models being compared are polynomials of different degrees. For
example:</p>
<ul>
<li><p><strong>Linear:</strong> <span class="math inline">\(mpg =
\beta_0 + \beta_1(horsepower)\)</span></p></li>
<li><p><strong>Quadratic:</strong> <span class="math inline">\(mpg =
\beta_0 + \beta_1(horsepower) + \beta_2(horsepower)^2\)</span></p></li>
<li><p><strong>Cubic:</strong> <span class="math inline">\(mpg = \beta_0
+ \beta_1(horsepower) + \beta_2(horsepower)^2 +
\beta_3(horsepower)^3\)</span></p></li>
<li><p><strong>线性</strong>：<span class="math inline">\(mpg = \beta_0
+ \beta_1(马力)\)</span></p></li>
<li><p><strong>二次</strong>：<span class="math inline">\(mpg = \beta_0
+ \beta_1(马力) + \beta_2(马力)^2\)</span></p></li>
<li><p><strong>三次</strong>：<span class="math inline">\(mpg = \beta_0
+ \beta_1(马力) + \beta_2(马力)^2 + \beta_3(马力)^3\)</span></p></li>
</ul>
<p>The performance metric used is the <strong>Mean Squared Error
(MSE)</strong> on the validation set:
使用的性能指标是验证集上的<strong>均方误差 (MSE)</strong>： <span
class="math display">\[MSE_{val} = \frac{1}{n_{val}} \sum_{i \in val}
(y_i - \hat{f}(x_i))^2\]</span> where <span
class="math inline">\(n_{val}\)</span> is the number of observations in
the validation set, <span class="math inline">\(y_i\)</span> is the true
<code>mpg</code> value, and <span
class="math inline">\(\hat{f}(x_i)\)</span> is the model’s prediction
for the <span class="math inline">\(i\)</span>-th observation in the
validation set. 其中 <span class="math inline">\(n_{val}\)</span>
是验证集中的观测值数量， <span class="math inline">\(y_i\)</span>
是真实的 <code>mpg</code> 值，<span
class="math inline">\(\hat{f}(x_i)\)</span> 是模型对验证集中第 <span
class="math inline">\(i\)</span> 个观测值的预测。 ### Important Image:
Polynomial Fits (Slide 8) 多项式拟合（幻灯片 8）</p>
<p>This plot is crucial. It shows the <code>Auto</code> data with linear
(red), quadratic (green), and cubic (blue) regression lines. * The
<strong>linear fit</strong> is clearly poor. * The <strong>quadratic and
cubic fits</strong> follow the data’s curve much better. * The inset box
shows the MSE calculated on the <em>full dataset</em> (this is training
MSE): * Linear MSE: ~26.42 * Quadratic MSE: ~21.60 * Cubic MSE: ~21.51
This suggests a non-linear fit is necessary, but it doesn’t tell us
which one will generalize better.</p>
<p>这张图至关重要。它用线性（红色）、二次（绿色）和三次（蓝色）回归线展示了
<code>Auto</code> 数据。 * <strong>线性拟合</strong> 明显较差。 *
<strong>二次和三次拟合</strong> 更能贴合数据曲线。 * 插图显示了基于
<em>完整数据集</em> 计算的均方误差（这是训练均方误差）： *
线性均方误差：~26.42 * 二次均方误差：~21.60 * 三次均方误差：~21.51
这表明非线性拟合是必要的，但它并没有告诉我们哪种拟合方式的泛化效果更好。
### Code Analysis</p>
<p>The slides show two different approaches in code:</p>
<p><strong>1. Python Code (Slide 9): Model Selection
Criteria</strong></p>
<ul>
<li><strong>What it does:</strong> This Python code (using
<code>pandas</code> and <code>statsmodels</code>) does <em>not</em>
implement the validation set approach. Instead, it fits polynomial
models (degrees 1 through 5) to the <em>entire</em> dataset.</li>
<li><strong>How it works:</strong> It calculates statistical criteria
like <strong>BIC</strong>, <strong>Mallow’s <span
class="math inline">\(C_p\)</span></strong>, and <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>. These are mathematical
adjustments to the training error that <em>estimate</em> the test error
without needing a validation set.
<strong>它计算统计标准，例如</strong>BIC<strong>、</strong>Mallow 的
<span class="math inline">\(C_p\)</span>** 和<strong>调整后的 <span
class="math inline">\(R^2\)</span></strong>。这些是对训练误差的数学调整，无需验证集即可<em>估算</em>测试误差。</li>
<li><strong>Key line (logic):</strong> <code>sm.OLS(y, X).fit()</code>
is used to fit the model, and then metrics like <code>model.bic</code>
and <code>model.rsquared_adj</code> are extracted.</li>
<li><strong>Result:</strong> The table shows that the model with
<code>[horsepower, horsepower2]</code> (quadratic) has the lowest BIC
and <span class="math inline">\(C_p\)</span> values, suggesting it’s the
best model according to these criteria.</li>
<li><strong>结果：</strong>表格显示，带有
<code>[马力, 马力2]</code>（二次函数）的模型具有最低的 BIC 和 <span
class="math inline">\(C_p\)</span>
值，这表明根据这些标准，它是最佳模型。</li>
</ul>
<p><strong>2. R Code (Slides 14 &amp; 15): The Validation Set
Approach</strong></p>
<ul>
<li><strong>What it does:</strong> This R code <em>directly
implements</em> the validation set approach described on Slide 13.</li>
<li><strong>How it works:</strong>
<ol type="1">
<li><code>set.seed(...)</code>: Sets a random seed to make the split
reproducible.</li>
<li><code>train=sample(392, 196)</code>: Randomly selects 196 indices
(out of 392) to be the <strong>training set</strong>.</li>
<li><code>lm.fit=lm(mpg~poly(horsepower, 2), ..., subset=train)</code>:
Fits a quadratic model <em>only</em> using the <code>train</code>
data.</li>
<li><code>mean((mpg-predict(lm.fit,Auto))[-train]^2)</code>: This is the
key calculation.
<ul>
<li><code>predict(lm.fit, Auto)</code>: Predicts <code>mpg</code> for
<em>all</em> data.</li>
<li><code>[-train]</code>: Selects only the predictions for the
<strong>validation set</strong> (the data <em>not</em> in
<code>train</code>).</li>
<li><code>mean(...)</code>: Calculates the <strong>MSE on the validation
set</strong>.</li>
</ul></li>
</ol></li>
<li><strong>Result:</strong> The code is run three times with different
seeds (1, 2022, 1997).
<ul>
<li><strong>Seed 1:</strong> Quadratic MSE (18.71) is lowest.</li>
<li><strong>Seed 2022:</strong> Quadratic MSE (19.70) is lowest.</li>
<li><strong>Seed 1997:</strong> Quadratic MSE (19.08) is lowest.</li>
</ul></li>
<li><strong>Main Takeaway:</strong> In all random splits, the
<strong>quadratic model gives the lowest validation set MSE</strong>.
This provides evidence that the quadratic model is the best choice for
generalizing to new data. The fact that the MSE values change with each
seed also highlights a key <em>disadvantage</em> of this simple method:
the results can be variable depending on the random split.
<strong>主要结论</strong>：在所有随机拆分中，**二次模型的验证集 MSE
最低。这证明了二次模型是推广到新数据的最佳选择。MSE
值随每个种子变化的事实也凸显了这种简单方法的一个关键<em>缺点</em>：结果可能会因随机拆分而变化。</li>
</ul>
<h1 id="the-validation-set-approach-验证集方法">2. The Validation Set
Approach 验证集方法</h1>
<p>This method is a simple way to estimate a model’s performance on new,
unseen data (the “test error”).
这种方法是一种简单的方法，用于评估模型在新的、未见过的数据（“测试误差”）上的性能。
The core idea is to <strong>randomly split</strong> your available data
into two parts:
其核心思想是将可用数据<strong>随机拆分</strong>为两部分： 1.
<strong>Training Set:</strong> Used to fit (or “train”) your model.
用于拟合（或“训练”）模型。 2. <strong>Validation Set (or Test
Set):</strong> Used to evaluate the trained model’s performance. You
calculate the error (like Mean Squared Error) on this set.
用于评估训练后的模型性能。计算此集合的误差（例如均方误差）。</p>
<h3 id="python-code-explained-slide-1">Python Code Explained (Slide
1)</h3>
<p>The first slide shows a Python example using the <code>Auto</code>
dataset to predict <code>mpg</code> from <code>horsepower</code>.</p>
<ol type="1">
<li><strong>Setup &amp; Data Loading:</strong>
<ul>
<li><code>import</code> statements load libraries like
<code>pandas</code> (for data),
<code>sklearn.model_selection.train_test_split</code> (the key function
for this method), and
<code>sklearn.linear_model.LinearRegression</code>.</li>
<li><code>Auto = pd.read_csv(...)</code> loads the data.</li>
<li><code>X = Auto['horsepower'].values</code> and
<code>y = Auto['mpg'].values</code> select the variables of
interest.</li>
</ul></li>
<li><strong>The Split:</strong>
<ul>
<li><code>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=007)</code></li>
<li>This is the <strong>most important line</strong> for this method. It
splits the data <code>X</code> and <code>y</code> into training and
testing (validation) sets.</li>
<li><code>train_size=0.5</code> means 50% of the data is for training
and 50% is for validation.</li>
<li><code>random_state=007</code> ensures the split is “random” but
“reproducible” (using the same seed <code>007</code> will always produce
the same split).</li>
</ul></li>
<li><strong>Model Fitting &amp; Evaluation:</strong>
<ul>
<li>The code fits three different polynomial models, but it <strong>only
uses the training data</strong> (<code>X_train</code>,
<code>y_train</code>) to do so.</li>
<li><strong>Linear (Degree 1):</strong> A simple
<code>LinearRegression</code>.</li>
<li><strong>Quadratic (Degree 2):</strong> Uses
<code>PolynomialFeatures(2)</code> to create <span
class="math inline">\(x\)</span> and <span
class="math inline">\(x^2\)</span> terms, then fits a linear model to
them.</li>
<li><strong>Cubic (Degree 3):</strong> Uses
<code>PolynomialFeatures(3)</code> to create <span
class="math inline">\(x\)</span>, <span
class="math inline">\(x^2\)</span>, and <span
class="math inline">\(x^3\)</span> terms.</li>
<li>It then calculates the <strong>Mean Squared Error (MSE)</strong> for
all three models using the <strong>test data</strong>
(<code>X_test</code>, <code>y_test</code>).</li>
</ul></li>
<li><strong>Results (from the text on the slide):</strong>
<ul>
<li><strong>Linear MSE:</strong> <span class="math inline">\(\approx
23.3\)</span></li>
<li><strong>Quadratic MSE:</strong> <span class="math inline">\(\approx
19.4\)</span></li>
<li><strong>Cubic MSE:</strong> <span class="math inline">\(\approx
19.4\)</span></li>
<li><strong>Conclusion:</strong> The quadratic model gives a
significantly lower error than the linear model. The cubic model does
not offer any real improvement over the quadratic one.</li>
</ul>
<strong>结果（来自幻灯片上的文字）：</strong>
<ul>
<li><strong>线性均方误差</strong>：约 23.3</li>
<li><strong>二次均方误差</strong>：约 19.4</li>
<li><strong>三次均方误差</strong>：约 19.4</li>
<li><strong>结论：</strong>二次模型的误差显著低于线性模型。三次模型与二次模型相比并没有任何实质性的改进。</li>
</ul></li>
</ol>
<h3 id="key-images-the-problem-with-a-single-split">Key Images: The
Problem with a Single Split</h3>
<p>The most important images are on <strong>slide 9</strong> (labeled
“Figure” and “Page 20”).</p>
<ul>
<li><strong>Plot on the Left (Single Split):</strong> This graph shows
the validation MSE for polynomial degrees 1 through 10, based on the
<em>single random split</em> from the R code (slide 2). Just like the
Python example, it shows that the MSE drops sharply from degree 1 to 2,
and then stays relatively low. Based on this <em>one</em> chart, you
might pick degree 2 (quadratic) as the best model.</li>
</ul>
<p>**此图显示了多项式次数为 1 至 10 的验证均方误差，基于 R 代码（幻灯片
2）中的<em>单次随机分割</em>。与 Python 示例一样，它显示 MSE 从 1 阶到 2
阶急剧下降，然后保持在相对较低的水平。基于这张<em>一</em>图，您可能会选择
2 阶（二次）作为最佳模型。</p>
<ul>
<li><strong>Plot on the Right (Ten Splits):</strong> This is the
<strong>most critical plot</strong>. It shows the results of
<em>repeating the entire process 10 times</em>, each with a new random
split (from R code on slide 3).
<ul>
<li>You can see 10 different error curves.</li>
<li>While they all agree that degree 1 (linear) is bad, they <strong>do
not agree on the best model</strong>. Some curves suggest degree 2 is
best, others suggest 3, 4, or even 6.</li>
</ul>
<strong>这是</strong>最关键的图表**。它显示了<em>重复整个过程 10
次</em>的结果，每次都使用新的随机分割（来自幻灯片 3 上的 R 代码）。
<ul>
<li>您可以看到 10 条不同的误差曲线。</li>
<li>虽然他们都认为 1
阶（线性）模型不好，但他们<strong>对最佳模型的看法并不一致</strong>。有些曲线表明
2 阶最佳，而另一些则表明 3 阶、4 阶甚至 6 阶最佳。</li>
</ul></li>
</ul>
<h3 id="summary-of-drawbacks-slides-7-8-9-23-25">Summary of Drawbacks
(Slides 7, 8, 9, 23, 25)</h3>
<p>The slides repeatedly emphasize the two main drawbacks of this simple
validation set approach:</p>
<ol type="1">
<li><p><strong>High Variability 高变异性:</strong> The estimated test
MSE can be <strong>highly variable</strong>, depending on which
observations happen to land in the training set versus the validation
set. The plot with 10 curves (slide 9, right) proves this perfectly.
估计的测试 MSE
可能<strong>高度变异</strong>，具体取决于哪些观测值恰好落在训练集和验证集中。包含
10 条曲线的图表（幻灯片 9，右侧）完美地证明了这一点。</p></li>
<li><p><strong>Overestimation of Test Error 高估测试误差:</strong></p>
<ul>
<li>The model is <strong>only trained on a subset</strong> (e.g., 50%)
of the available data. The validation data is “wasted” and not used for
model building.</li>
<li>Statistical methods tend to perform worse when trained on fewer
observations.</li>
<li>Therefore, the model trained on just the training set is likely
<em>worse</em> than a model trained on the <em>entire</em> dataset.</li>
<li>This “worse” model will have a <em>higher</em> error rate on the
validation set. This means the validation set MSE <strong>tends to
overestimate</strong> the true test error you would get from a model
trained on all your data.</li>
<li>该模型<strong>仅基于可用数据的子集</strong>（例如
50%）进行训练。验证数据被“浪费”了，并未用于模型构建。</li>
<li>统计方法在较少的观测值上进行训练时往往表现较差。</li>
<li>因此，仅基于训练集训练的模型可能比基于<em>整个</em>数据集训练的模型<em>更差</em>。</li>
<li>这个“更差”的模型在验证集上的错误率会更高。这意味着验证集的 MSE
<strong>倾向于高估</strong>基于所有数据训练的模型的真实测试误差。</li>
</ul></li>
</ol>
<h2 id="cross-validation-the-solution-交叉验证解决方案">3.
Cross-Validation: The Solution 交叉验证：解决方案</h2>
<p>The slides introduce <strong>Cross-Validation (CV)</strong> as the
method to overcome these drawbacks. The core idea is to use <em>all</em>
data points for both training and validation, just at different times.
<strong>交叉验证
(CV)</strong>，以此来克服这些缺点。其核心思想是将<em>所有</em>数据点用于训练和验证，只是使用的时间不同。</p>
<h3
id="leave-one-out-cross-validation-loocv-留一法交叉验证-loocv">Leave-One-Out
Cross-Validation (LOOCV) 留一法交叉验证 (LOOCV)</h3>
<p>This is the first type of CV introduced (slide 10, page 26). For a
dataset with <span class="math inline">\(n\)</span> data points:</p>
<ol type="1">
<li><strong>Hold out</strong> the 1st data point (this is your
validation set).
<strong>保留</strong>第一个数据点（这是你的验证集）。</li>
<li><strong>Train</strong> the model on the <em>other <span
class="math inline">\(n-1\)</span> data points</em>. 使用<em>其他 <span
class="math inline">\(n-1\)</span>
个数据点</em><strong>训练</strong>模型。</li>
<li><strong>Calculate</strong> the error (e.g., <span
class="math inline">\(\text{MSE}_1\)</span>) using only that 1st
held-out point. 仅使用第一个保留点<strong>计算</strong>误差（例如，<span
class="math inline">\(\text{MSE}_1\)</span>）。</li>
<li><strong>Repeat</strong> this <span class="math inline">\(n\)</span>
times, holding out the 2nd point, then the 3rd, and so on, until every
point has been used as the validation set exactly once.
<strong>重复</strong>此操作 <span class="math inline">\(n\)</span>
次，保留第二个点，然后是第三个点，依此类推，直到每个点都作为验证集使用一次。</li>
<li>Your final test error estimate is the <strong>average of all <span
class="math inline">\(n\)</span> errors</strong>.
最终的测试误差估计是<strong>所有 <span class="math inline">\(n\)</span>
个误差的平均值</strong>。</li>
</ol>
<h3 id="key-formula-from-slide-10">Key Formula (from Slide 10)</h3>
<p>The formula for the <span class="math inline">\(n\)</span>-fold LOOCV
error estimate is: <span class="math inline">\(n\)</span> 倍 LOOCV
误差估计公式为： <span class="math display">\[\text{CV}_{(n)} =
\frac{1}{n} \sum_{i=1}^{n} \text{MSE}_i\]</span></p>
<p>Where: * <span class="math inline">\(n\)</span> is the total number
of data points. 是数据点的总数。 * <span
class="math inline">\(\text{MSE}_i\)</span> is the Mean Squared Error
calculated on the <span class="math inline">\(i\)</span>-th data point
when it was held out. 是保留第 <span class="math inline">\(i\)</span>
个数据点时计算的均方误差。</p>
<h1 id="what-is-loocv-leave-one-out-cross-validation">3.What is LOOCV
(Leave-One-Out Cross Validation)</h1>
<p>Leave-One-Out Cross Validation (LOOCV) is a method for estimating the
test error of a model. For a dataset with <span
class="math inline">\(n\)</span> observations, you: 留一交叉验证 (LOOCV)
是一种估算模型测试误差的方法。对于包含 <span
class="math inline">\(n\)</span> 个观测值的数据集，您需要：</p>
<ol type="1">
<li><strong>Fit the model <span class="math inline">\(n\)</span> times.
对模型进行 <span class="math inline">\(n\)</span> 次拟合</strong></li>
<li>For each fit <span class="math inline">\(i\)</span> (from <span
class="math inline">\(1\)</span> to <span
class="math inline">\(n\)</span>), you train the model on all data
points <em>except</em> for observation <span
class="math inline">\(i\)</span>. 对于每个拟合 <span
class="math inline">\(i\)</span> 个样本（从 <span
class="math inline">\(1\)</span> 到 <span
class="math inline">\(n\)</span>），您需要在除观测值 <span
class="math inline">\(i\)</span> 之外的所有数据点上训练模型。</li>
<li>You then use this trained model to make a prediction for the single
observation <span class="math inline">\(i\)</span> that was left out.
然后，您需要使用这个训练好的模型对被遗漏的单个观测值 <span
class="math inline">\(i\)</span> 进行预测。</li>
<li>The final LOOCV error is the average of the <span
class="math inline">\(n\)</span> prediction errors (typically the Mean
Squared Error, or MSE). 最终的 LOOCV 误差是 <span
class="math inline">\(n\)</span>
个预测误差的平均值（通常为均方误差，简称 MSE）。</li>
</ol>
<p>This process is shown visually in the slide titled “LOOCV” (slide
27), which is a key image for understanding the concept. <strong>Pros
&amp; Cons (from slide 28):</strong> * <strong>Pro:</strong> It has low
bias because the training set (<span class="math inline">\(n-1\)</span>
samples) is almost identical to the full dataset.由于训练集（<span
class="math inline">\(n-1\)</span>
个样本）与完整数据集几乎完全相同，因此偏差较低。 * <strong>Pro:</strong>
It produces a stable, non-random error estimate (unlike <span
class="math inline">\(k\)</span>-fold CV, which depends on the random
fold assignments). 它能产生稳定的非随机误差估计（不同于 k
倍交叉验证，后者依赖于随机折叠分配）。 * <strong>Con:</strong> It can be
extremely <strong>computationally expensive</strong>, as the model must
be refit <span class="math inline">\(n\)</span> times.
由于模型必须重新拟合 <span class="math inline">\(n\)</span>
次，计算成本极其高昂。 * <strong>Con:</strong> The <span
class="math inline">\(n\)</span> error estimates can be highly
correlated, which can sometimes lead to high variance in the final <span
class="math inline">\(CV\)</span> estimate. 这 <span
class="math inline">\(n\)</span> 个误差估计可能高度相关，有时会导致最终
<span class="math inline">\(CV\)</span> 估计值出现较大方差。</p>
<h2 id="key-mathematical-formulas">Key Mathematical Formulas</h2>
<p>The main challenge of LOOCV (being computationally expensive) has a
very efficient solution for linear models. LOOCV
的主要挑战（计算成本高昂）对于线性模型来说，有一个非常有效的解决方案。</p>
<h3 id="the-standard-slow-formula">1. The Standard (Slow) Formula</h3>
<p>As defined on slide 33, the LOOCV estimate of the MSE is:</p>
<p><span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n}
(y_i - \hat{y}_i^{(i)})^2\]</span></p>
<ul>
<li><span class="math inline">\(y_i\)</span> is the true value of the
<span class="math inline">\(i\)</span>-th observation. 是第 <span
class="math inline">\(i\)</span> 个观测值的真实值。</li>
<li><span class="math inline">\(\hat{y}_i^{(i)}\)</span> is the
predicted value for <span class="math inline">\(y_i\)</span> from a
model trained on all data <em>except</em> observation <span
class="math inline">\(i\)</span>. 是使用除观测值 <span
class="math inline">\(i\)</span> 之外的所有数据训练的模型对 <span
class="math inline">\(y_i\)</span> 的预测值。</li>
</ul>
<p>Calculating <span class="math inline">\(\hat{y}_i^{(i)}\)</span>
requires refitting the model <span class="math inline">\(n\)</span>
times. 计算 <span class="math inline">\(\hat{y}_i^{(i)}\)</span>
需要重新拟合模型 <span class="math inline">\(n\)</span> 次。</p>
<h3 id="the-shortcut-fast-formula">2. The Shortcut (Fast) Formula</h3>
<p>Slide 34 provides a much simpler formula that <strong>only requires
fitting the model once</strong> on the <em>entire</em> dataset:
<em>只需对</em>整个*数据集进行一次模型拟合**：</p>
<p><span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n}
\left( \frac{y_i - \hat{y}_i}{1 - h_i} \right)^2\]</span></p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> is the prediction for
<span class="math inline">\(y_i\)</span> from the model trained on
<strong>all <span class="math inline">\(n\)</span> data points</strong>.
是使用<strong>所有 <span class="math inline">\(n\)</span>
个数据点</strong>训练的模型对 <span class="math inline">\(y_i\)</span>
的预测值。</li>
<li><span class="math inline">\(h_i\)</span> is the
<strong>leverage</strong> of the <span
class="math inline">\(i\)</span>-th observation. 是第 <span
class="math inline">\(i\)</span>
个观测值的<strong>杠杆率</strong>。</li>
</ul>
<h3 id="what-is-leverage-h_i">3. What is Leverage (<span
class="math inline">\(h_i\)</span>)?</h3>
<p>Slide 35 defines leverage:</p>
<ul>
<li><p><strong>Hat Matrix (<span
class="math inline">\(\mathbf{H}\)</span>):</strong> In a linear model,
the fitted values <span class="math inline">\(\hat{\mathbf{y}}\)</span>
are related to the true values <span
class="math inline">\(\mathbf{y}\)</span> by the hat matrix: <span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{H}\mathbf{y}\)</span>.</p></li>
<li><p><strong>Formula:</strong> The hat matrix is defined as <span
class="math inline">\(\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span>.</p></li>
<li><p><strong>Leverage (<span
class="math inline">\(h_i\)</span>):</strong> The leverage for the <span
class="math inline">\(i\)</span>-th observation is simply the <span
class="math inline">\(i\)</span>-th diagonal element of the hat matrix,
<span class="math inline">\(h_{ii}\)</span> (often just written as <span
class="math inline">\(h_i\)</span>).</p>
<ul>
<li><span class="math inline">\(h_i = \mathbf{x}_i^T
(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{x}_i\)</span></li>
</ul></li>
<li><p><strong>Meaning:</strong> Leverage measures how “influential” an
observation’s <span class="math inline">\(x_i\)</span> value is in
determining its own predicted value <span
class="math inline">\(\hat{y}_i\)</span>. A high leverage score means
that point has a lot of influence on the model’s fit.</p></li>
<li><p><strong>帽子矩阵 (<span
class="math inline">\(\mathbf{H}\)</span>)：</strong>在线性模型中，拟合值
<span class="math inline">\(\hat{\mathbf{y}}\)</span> 与真实值 <span
class="math inline">\(\mathbf{y}\)</span> 之间存在帽子矩阵关系：<span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{H}\mathbf{y}\)</span>。</p></li>
<li><p><strong>公式：</strong>帽子矩阵定义为 <span
class="math inline">\(\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span>。</p></li>
<li><p><strong>杠杆率 (<span
class="math inline">\(h_i\)</span>)：</strong>第 <span
class="math inline">\(i\)</span> 个观测值的杠杆率就是帽子矩阵的第 <span
class="math inline">\(i\)</span> 个对角线元素 <span
class="math inline">\(h_{ii}\)</span>（通常写为 <span
class="math inline">\(h_i\)</span>）。</p></li>
<li><p><span class="math inline">\(h_i = \mathbf{x}_i^T
(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{x}_i\)</span></p></li>
<li><p><strong>含义：</strong>杠杆率衡量观测值的 <span
class="math inline">\(x_i\)</span> 值对其自身预测值 <span
class="math inline">\(\hat{y}_i\)</span>
的“影响力”。杠杆率得分高意味着该点对模型拟合有很大影响。</p></li>
</ul>
<p>This shortcut formula is extremely important because it makes LOOCV
as fast to compute as a single model
fit.这个快捷公式非常重要，因为它使得 LOOCV
的计算速度与单个模型拟合一样快。</p>
<h2 id="python-code-explained-slide-29">Python Code Explained (Slide
29)</h2>
<p>This slide shows how to use LOOCV to select the best polynomial
degree for predicting <code>mpg</code> from <code>horsepower</code>.</p>
<ol type="1">
<li><strong>Imports:</strong> It imports standard libraries
(<code>pandas</code>, <code>matplotlib</code>) and key modules from
<code>sklearn</code>:
<ul>
<li><code>LinearRegression</code>: The model to be fit.</li>
<li><code>PolynomialFeatures</code>: A tool to create polynomial terms
(e.g., <span class="math inline">\(x, x^2, x^3\)</span>).</li>
<li><code>LeaveOneOut</code>: The LOOCV cross-validation strategy
object.</li>
<li><code>cross_val_score</code>: A function that automatically runs a
cross-validation test.</li>
</ul></li>
<li><strong>Setup:</strong>
<ul>
<li>It loads the <code>Auto.csv</code> data.</li>
<li>It defines <span class="math inline">\(X\)</span>
(<code>horsepower</code>) and <span class="math inline">\(y\)</span>
(<code>mpg</code>).</li>
<li>It creates a <code>LeaveOneOut</code> object:
<code>loo = LeaveOneOut()</code>.</li>
</ul></li>
<li><strong>Looping through Degrees:</strong>
<ul>
<li>The code loops <code>degree</code> from 1 to 10.</li>
<li><strong><code>make_pipeline</code>:</strong> For each degree, it
creates a <code>model</code> using <code>make_pipeline</code>. This
pipeline is a crucial concept:
<ul>
<li>It first runs <code>PolynomialFeatures(degree)</code> to transform
<span class="math inline">\(X\)</span> into <span
class="math inline">\([X, X^2, ..., X^{\text{degree}}]\)</span>.</li>
<li>It then feeds those features into <code>LinearRegression()</code> to
fit the model.</li>
</ul></li>
<li><strong><code>cross_val_score</code>:</strong> This is the most
important line.
<ul>
<li><code>scores = cross_val_score(model, X, y, cv=loo, scoring='neg_mean_squared_error')</code></li>
<li>This function automatically does the <em>entire</em> LOOCV process.
It takes the <code>model</code> (the pipeline), the data <span
class="math inline">\(X\)</span> and <span
class="math inline">\(y\)</span>, and the CV strategy
(<code>cv=loo</code>).</li>
<li><code>sklearn</code>’s <code>cross_val_score</code> uses the “fast”
leverage method internally for linear models, so it doesn’t actually fit
the model <span class="math inline">\(n\)</span> times.</li>
<li>It uses <code>scoring='neg_mean_squared_error'</code> because the
<code>scoring</code> function assumes “higher is better.” By calculating
the <em>negative</em> MSE, the best model will have the highest score
(i.e., closest to 0).</li>
</ul></li>
<li><strong>Storing Results:</strong> It calculates the mean of the
scores (which is the <span class="math inline">\(CV_{(n)}\)</span>) and
stores it.</li>
</ul></li>
<li><strong>Visualization:</strong>
<ul>
<li>The code then plots the final <code>cv_errors</code> (after flipping
the sign back to positive) against the <code>degree</code>.</li>
<li>The resulting plot (also on slide 32) shows the test MSE, allowing
you to visually pick the best degree (where the error is
minimized).</li>
<li>生成的图（也在幻灯片 32 上）显示了测试 MSE，让您可以直观地选择最佳
degree（误差最小化的 degree）。</li>
</ul></li>
</ol>
<hr />
<h2 id="important-images">Important Images</h2>
<ul>
<li><p><strong>Slide 27 (<code>.../103628.png</code>):</strong> This is
the <strong>best conceptual image</strong>. It visually demonstrates how
LOOCV splits the data <span class="math inline">\(n\)</span> times, with
each observation getting one turn as the validation set.
<strong>这是</strong>最佳概念图**。它直观地展示了 LOOCV 如何将数据拆分
<span class="math inline">\(n\)</span>
次，每个观察值都会被旋转一次作为验证集。</p></li>
<li><p><strong>Slide 34 (<code>.../103711.png</code>):</strong> This
slide presents the <strong>most important formula</strong>: the “Easy
formula” or shortcut, <span class="math inline">\(CV_{(n)} = \frac{1}{n}
\sum (\frac{y_i - \hat{y}_i}{1 - h_i})^2\)</span>. This is the key
takeaway for <em>computing</em> LOOCV efficiently in linear models.
<strong>这张幻灯片展示了</strong>最重要的公式**：“简单公式”或简称，<span
class="math inline">\(CV_{(n)} = \frac{1}{n} \sum (\frac{y_i -
\hat{y}_i}{1 - h_i})^2\)</span>。这是在线性模型中高效<em>计算</em> LOOCV
的关键要点。</p></li>
<li><p><strong>Slide 32 (<code>.../103701.jpg</code>):</strong> This is
the <strong>key results image</strong>. It contrasts the LOOCV error
curve (left) with the 10-fold CV error curves (right). It clearly shows
that LOOCV produces a single, stable error curve, while 10-fold CV
results vary slightly each time it’s run due to the random data splits.
<strong>这是</strong>关键结果图**。它将 LOOCV 误差曲线（左）与 10 倍 CV
误差曲线（右）进行了对比。它清楚地表明，LOOCV
产生了单一、稳定的误差曲线，而由于数据分割的随机性，10 倍 CV
的结果每次运行时都会略有不同。</p></li>
</ul>
<h1 id="cross-validation-overview">4. Cross-Validation Overview</h1>
<p>These slides explain <strong>Cross-Validation (CV)</strong>, a method
used to estimate the test error of a model, helping to select the best
level of flexibility (e.g., the best polynomial degree). It’s an
improvement over a single validation set because it uses all the data
for both training and validation at different times.
这是一种用于估算模型测试误差的方法，有助于选择最佳的灵活性（例如，最佳多项式次数）。它比单个验证集有所改进，因为它在不同时间使用所有数据进行训练和验证。</p>
<p>The two main types discussed are <strong>K-fold CV</strong> and
<strong>Leave-One-Out CV (LOOCV)</strong>. 主要讨论的两种类型是<strong>K
折交叉验证</strong>和<strong>留一法交叉验证 (LOOCV)</strong>。</p>
<h2 id="k-fold-cross-validation-k-折交叉验证">K-Fold Cross-Validation K
折交叉验证</h2>
<p>This is the most common method.</p>
<h3 id="the-process">The Process</h3>
<p>As shown in the slides, the K-fold CV process is: 1.
<strong>Divide</strong> the dataset randomly into <span
class="math inline">\(K\)</span> non-overlapping groups (or “folds”),
usually of equal size. Common choices are <span
class="math inline">\(K=5\)</span> or <span
class="math inline">\(K=10\)</span>. 将数据集随机<strong>划分</strong>为
<span class="math inline">\(K\)</span>
个不重叠的组（或“折”），通常大小相等。常见的选择是 <span
class="math inline">\(K=5\)</span> 或 <span
class="math inline">\(K=10\)</span>。 2. <strong>Iterate <span
class="math inline">\(K\)</span> times</strong>: In each iteration <span
class="math inline">\(i\)</span>, use the <span
class="math inline">\(i\)</span>-th fold as the <strong>validation
set</strong> and all other <span class="math inline">\(K-1\)</span>
folds combined as the <strong>training set</strong>. <strong>迭代 <span
class="math inline">\(K\)</span> 次</strong>：在每次迭代 <span
class="math inline">\(i\)</span> 中，使用第 <span
class="math inline">\(i\)</span>
个样本集作为<strong>验证集</strong>，并将所有其他 <span
class="math inline">\(K-1\)</span>
个样本集合并作为<strong>训练集</strong>。 3. <strong>Calculate</strong>
the Mean Squared Error (<span class="math inline">\(MSE_i\)</span>) on
the validation fold. <strong>计算</strong>验证集的均方误差 (<span
class="math inline">\(MSE_i\)</span>)。 4. <strong>Average</strong> all
<span class="math inline">\(K\)</span> error estimates to get the final
CV score. <strong>平均</strong>所有 <span
class="math inline">\(K\)</span> 个误差估计值，得到最终的 CV 分数。 ###
Key Formula The final K-fold CV error estimate is the average of the
errors from each fold: 最终的 K 折 CV
误差估计值是每个样本集误差的平均值： <span
class="math display">\[CV_{(K)} = \frac{1}{K} \sum_{i=1}^{K}
MSE_i\]</span></p>
<h3 id="important-image-the-concept">Important Image: The Concept</h3>
<p>The diagram in slide <code>104145.png</code> is the most important
for understanding the <em>concept</em> of K-fold CV. It shows a dataset
split into 5 folds (<span class="math inline">\(K=5\)</span>). The
process is repeated 5 times, with a different fold (in beige) held out
as the validation set in each run, while the rest (in blue) is used for
training. 它展示了一个被分成 5 个样本集 (<span
class="math inline">\(K=5\)</span>) 的数据集。该过程重复 5
次，每次运行都会保留一个不同的折叠（米色）作为验证集，其余折叠（蓝色）用于训练。</p>
<h2 id="leave-one-out-cross-validation-loocv">Leave-One-Out
Cross-Validation (LOOCV)</h2>
<p>LOOCV is just a special case of K-fold CV where <strong><span
class="math inline">\(K = n\)</span></strong> (the total number of
observations). LOOCV 只是 K 折交叉验证的一个特例，其中 <strong><span
class="math inline">\(K = n\)</span></strong>（观测值总数）。 * You
create <span class="math inline">\(n\)</span> “folds,” each containing
just one data point. 创建 <span class="math inline">\(n\)</span>
个“折叠”，每个折叠仅包含一个数据点。 * You train the model <span
class="math inline">\(n\)</span> times, each time leaving out a
<em>single</em> different observation and then calculating the error for
that one point. 对模型进行 <span class="math inline">\(n\)</span>
次训练，每次都省略一个不同的观测值，然后计算该点的误差。</p>
<h3 id="key-formulas">Key Formulas</h3>
<ol type="1">
<li><p><strong>Standard Definition:</strong> The LOOCV error is the
average of the <span class="math inline">\(n\)</span> squared errors:
<span class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N}
e_{[i]}^2\]</span> where <span class="math inline">\(e_{[i]} = y_i -
\hat{y}_{[i]}\)</span> is the prediction error for the <span
class="math inline">\(i\)</span>-th observation, calculated from a model
that was trained on <em>all data except</em> the <span
class="math inline">\(i\)</span>-th observation. This looks
computationally expensive. LOOCV 误差是 <span
class="math inline">\(n\)</span> 个平方误差的平均值： <span
class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N}
e_{[i]}^2\]</span> 其中 <span class="math inline">\(e_{[i]} = y_i -
\hat{y}_{[i]}\)</span> 是第 <span class="math inline">\(i\)</span>
个观测值的预测误差，该误差由一个使用除第 <span
class="math inline">\(i\)</span>
个观测值以外的所有数据训练的模型计算得出。这看起来计算成本很高。</p></li>
<li><p><strong>Fast Computation (for Linear Regression):</strong> A key
point from the slides is that for linear regression, you don’t need to
re-fit the model <span class="math inline">\(N\)</span> times. You can
fit the model <em>once</em> on all <span
class="math inline">\(N\)</span> data points and use the following
shortcut: <span class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N}
\left( \frac{e_i}{1 - h_i} \right)^2\]</span></p>
<ul>
<li><span class="math inline">\(e_i = y_i - \hat{y}_i\)</span> is the
standard residual (from the model fit on <em>all</em> data).</li>
<li><span class="math inline">\(h_i\)</span> is the <em>leverage
statistic</em> for the <span class="math inline">\(i\)</span>-th
observation (the <span class="math inline">\(i\)</span>-th diagonal
entry of the “hat matrix” <span class="math inline">\(H\)</span>). This
makes LOOCV as fast to compute as a single model fit.
对于线性回归，您无需重新拟合模型 <span class="math inline">\(N\)</span>
次。您可以对所有 <span class="math inline">\(N\)</span>
个数据点<em>一次性</em>地拟合模型，并使用以下快捷方式： <span
class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N} \left(
\frac{e_i}{1 - h_i} \right)^2\]</span></li>
<li><span class="math inline">\(e_i = y_i - \hat{y}_i\)</span>
是标准残差（来自对<em>所有</em>数据的模型拟合）。</li>
<li><span class="math inline">\(h_i\)</span> 是第 <span
class="math inline">\(i\)</span> 个观测值（“帽子矩阵”<span
class="math inline">\(H\)</span> 的第 <span
class="math inline">\(i\)</span> 个对角线元素）的<em>杠杆统计量</em>。
这使得 LOOCV 的计算速度与单次模型拟合一样快。</li>
</ul></li>
</ol>
<h2 id="python-code-results">Python Code &amp; Results</h2>
<p>The Python code in slide <code>104156.jpg</code> shows how to use
10-fold CV to find the best polynomial degree for a model.</p>
<h3 id="code-understanding-slide-104156.jpg">Code Understanding (Slide
<code>104156.jpg</code>)</h3>
<p>Here’s a breakdown of the key <code>sklearn</code> parts:</p>
<ol type="1">
<li><strong><code>from sklearn.pipeline import make_pipeline</code></strong>:
This is used to chain steps. The pipeline
<code>make_pipeline(PolynomialFeatures(degree), LinearRegression())</code>
first creates polynomial features (like <span
class="math inline">\(x\)</span>, <span
class="math inline">\(x^2\)</span>, <span
class="math inline">\(x^3\)</span>) and then fits a linear model to
them.</li>
<li><strong><code>from sklearn.model_selection import KFold</code></strong>:
This object is used to define the <span
class="math inline">\(K\)</span>-fold split strategy.
<code>kf = KFold(n_splits=10, shuffle=True, random_state=1)</code>
creates a 10-fold splitter that shuffles the data first.</li>
<li><strong><code>from sklearn.model_selection import cross_val_score</code></strong>:
This is the most important function.
<ul>
<li><code>scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')</code></li>
<li>This one function does all the work: it takes the <code>model</code>
(the pipeline), the data <code>X</code> and <code>y</code>, and the CV
splitter <code>kf</code>. It automatically trains and evaluates the
model 10 times and returns an array of 10 scores (one for each
fold).</li>
<li><code>scoring='neg_mean_squared_error'</code> is used because
<code>cross_val_score</code> expects a <em>higher</em> score to be
<em>better</em>. Since we want to <em>minimize</em> MSE, we use
<em>negative</em> MSE.</li>
</ul></li>
<li><strong><code>avg_mse = -scores.mean()</code></strong>: The code
averages the 10 scores and flips the sign back to positive to get the
final CV (MSE) estimate for that polynomial degree.</li>
</ol>
<h3 id="important-image-the-results">Important Image: The Results</h3>
<p>The plots in slides <code>104156.jpg</code> (Python) and
<code>104224.png</code> (R) show the key result.</p>
<ul>
<li><strong>X-axis:</strong> Degree of Polynomial (model
complexity).多项式的次数（模型复杂度）。</li>
<li><strong>Y-axis:</strong> Estimated Test Error (CV Error /
MSE).估计测试误差（CV 误差 / MSE）。</li>
<li><strong>Interpretation:</strong> The plot shows a clear “U” shape.
The error is high for degree 1 (a simple line), drops to its minimum at
<strong>degree 2</strong> (a quadratic <span class="math inline">\(ax^2
+ bx + c\)</span>), and then starts to rise again for higher degrees.
This rise indicates <strong>overfitting</strong>—the more complex models
are fitting the training data’s noise, leading to worse performance on
unseen validation data. 该图呈现出清晰的“U”形。1
次（一条简单的直线）时误差较大，在<strong>2 次</strong>（二次 <span
class="math inline">\(ax^2 + bx +
c\)</span>）时降至最小，然后随着次数的增加，误差再次上升。这种上升表明<strong>过拟合</strong>——更复杂的模型会拟合训练数据的噪声，导致在未见过的验证数据上的性能下降。</li>
<li><strong>Conclusion:</strong> The 10-fold CV analysis suggests that a
<strong>quadratic model (degree 2)</strong> is the best choice, as it
provides the lowest estimated test error. 10 倍 CV
分析表明<strong>二次模型（2
次）</strong>是最佳选择，因为它提供了最低的估计测试误差。</li>
</ul>
<p>Let’s dive into the details of that proof.</p>
<h2 id="detailed-summary-the-fast-computation-of-loocv-proof">Detailed
Summary: The “Fast Computation of LOOCV” Proof</h2>
<p>The most mathematically dense and important part of your slides is
the proof (spanning slides <code>104126.jpg</code>,
<code>104132.png</code>, and <code>104136.png</code>) that LOOCV, which
seems computationally very expensive, can be calculated quickly for
linear regression. LOOCV
虽然计算成本看似非常高，但对于线性回归来说，它可以快速计算。 ### The
Goal</p>
<p>The goal is to prove that the LOOCV statistic, which is defined as:
<span class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N} e_{[i]}^2
\quad \text{where } e_{[i]} = y_i - \hat{y}_{[i]}\]</span> (Here, <span
class="math inline">\(\hat{y}_{[i]}\)</span> is the prediction for <span
class="math inline">\(y_i\)</span> from a model trained on all data
<em>except</em> point <span
class="math inline">\(i\)</span>).（其中，<span
class="math inline">\(\hat{y}_{[i]}\)</span> 表示基于除点 <span
class="math inline">\(i\)</span> 之外的所有数据训练的模型对 <span
class="math inline">\(y_i\)</span> 的预测）。</p>
<p>…can be computed <em>without</em> re-fitting the model <span
class="math inline">\(N\)</span> times, using this “fast” formula:
无需重新拟合模型 <span class="math inline">\(N\)</span>
次即可计算，使用以下“快速”公式： <span class="math display">\[CV =
\frac{1}{N} \sum_{i=1}^{N} \left( \frac{e_i}{1 - h_i} \right)^2\]</span>
(Here, <span class="math inline">\(e_i\)</span> is the <em>standard</em>
residual and <span class="math inline">\(h_i\)</span> is the
<em>leverage</em>, both from a single model fit on <em>all</em>
data).</p>
<p>The entire proof boils down to showing one identity: <strong><span
class="math inline">\(e_{[i]} = e_i / (1 - h_i)\)</span></strong>.</p>
<h3 id="key-definitions-the-matrix-algebra-setup-矩阵代数设置">Key
Definitions (The Matrix Algebra Setup) （矩阵代数设置）</h3>
<ul>
<li><strong>Model 模型:</strong> <span class="math inline">\(\mathbf{Y}
= \mathbf{X}\beta + \mathbf{e}\)</span></li>
<li><strong>Full Data Estimate 完整数据估计 (<span
class="math inline">\(\hat{\beta}\)</span>):</strong> <span
class="math inline">\(\hat{\beta} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\)</span></li>
<li><strong>Hat Matrix 帽子矩阵 (<span
class="math inline">\(\mathbf{H}\)</span>):</strong> <span
class="math inline">\(\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span></li>
<li><strong>Full Data Residual 完整数据残差 (<span
class="math inline">\(e_i\)</span>):</strong> <span
class="math inline">\(e_i = y_i - \hat{y}_i = y_i -
\mathbf{x}_i^T\hat{\beta}\)</span></li>
<li><strong>Leverage (<span class="math inline">\(h_i\)</span>) 杠杆
(<span class="math inline">\(h_i\)</span>):</strong> The <span
class="math inline">\(i\)</span>-th diagonal element of <span
class="math inline">\(\mathbf{H}\)</span>. <span
class="math inline">\(h_i =
\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i\)</span></li>
<li><strong>Leave-One-Out Estimate (<span
class="math inline">\(\hat{\beta}_{[i]}\)</span>):</strong> <span
class="math inline">\(\hat{\beta}_{[i]} =
(\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1}\mathbf{X}_{[i]}^T\mathbf{Y}_{[i]}\)</span>
<ul>
<li><span class="math inline">\(\mathbf{X}_{[i]}\)</span> and <span
class="math inline">\(\mathbf{Y}_{[i]}\)</span> are the data with the
<span class="math inline">\(i\)</span>-th row removed.</li>
</ul></li>
<li><strong>LOOCV Residual LOOCV 残差 (<span
class="math inline">\(e_{[i]}\)</span>):</strong> <span
class="math inline">\(e_{[i]} = y_i -
\mathbf{x}_i^T\hat{\beta}_{[i]}\)</span></li>
</ul>
<h3 id="the-proof-step-by-step">The Proof Step-by-Step</h3>
<p>Here is the logic from your slides, broken down:</p>
<h4 id="step-1-relating-the-matrices-slide-104132.png">Step 1: Relating
the Matrices (Slide <code>104132.png</code>)</h4>
<p>The proof’s “trick” is to relate the “full data” matrix <span
class="math inline">\((\mathbf{X}^T\mathbf{X})\)</span> to the
“leave-one-out” matrix <span
class="math inline">\((\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})\)</span>.
证明的“技巧”是将“全数据”矩阵 <span
class="math inline">\((\mathbf{X}^T\mathbf{X})\)</span> 与“留一法”矩阵
<span
class="math inline">\((\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})\)</span>
关联起来。</p>
<ul>
<li><p>The full sum-of-squares matrix is just the leave-one-out matrix
<em>plus</em> the one observation’s contribution:
完整的平方和矩阵就是留一法矩阵<em>加上</em>一个观测值的贡献：</p>
<p><span class="math display">\[\mathbf{X}^T\mathbf{X} =
\mathbf{X}_{[i]}^T\mathbf{X}_{[i]} +
\mathbf{x}_i\mathbf{x}_i^T\]</span></p></li>
<li><p>This means: <span
class="math inline">\(\mathbf{X}_{[i]}^T\mathbf{X}_{[i]} =
\mathbf{X}^T\mathbf{X} - \mathbf{x}_i\mathbf{x}_i^T\)</span></p></li>
</ul>
<h4 id="step-2-the-key-matrix-trick-slide-104132.png">Step 2: The Key
Matrix Trick (Slide <code>104132.png</code>)</h4>
<p>We need the inverse <span
class="math inline">\((\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1}\)</span>
to calculate <span class="math inline">\(\hat{\beta}_{[i]}\)</span>.
Finding this inverse directly is hard. Instead, we use the
<strong>Sherman-Morrison-Woodbury formula</strong>, which tells us how
to find the inverse of a matrix that’s been “updated” (in this case, by
subtracting <span
class="math inline">\(\mathbf{x}_i\mathbf{x}_i^T\)</span>).</p>
<p>我们需要逆<span
class="math inline">\((\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1}\)</span>
来计算 <span
class="math inline">\(\hat{\beta}_{[i]}\)</span>。直接求这个逆矩阵很困难。因此，我们使用
<strong>Sherman-Morrison-Woodbury
公式</strong>，它告诉我们如何求一个“更新”后的矩阵的逆矩阵（在本例中，是通过减去
<span class="math inline">\(\mathbf{x}_i\mathbf{x}_i^T\)</span>
来实现的）。</p>
<p>The slide applies this formula to get: <span
class="math display">\[(\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1} =
(\mathbf{X}^T\mathbf{X})^{-1} +
\frac{(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}}{1
- h_i}\]</span> * This is the most complex step, but it’s a standard
matrix identity. It’s crucial because it expresses the “leave-one-out”
inverse in terms of the “full data” inverse <span
class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span>, which we
already have.</p>
<h4 id="step-3-finding-hatbeta_i-slide-104136.png">Step 3: Finding <span
class="math inline">\(\hat{\beta}_{[i]}\)</span> (Slide
<code>104136.png</code>)</h4>
<p>Now we can write a new formula for <span
class="math inline">\(\hat{\beta}_{[i]}\)</span> by substituting the
result from Step 2. We also note that <span
class="math inline">\(\mathbf{X}_{[i]}^T\mathbf{Y}_{[i]} =
\mathbf{X}^T\mathbf{Y} - \mathbf{x}_i y_i\)</span>.</p>
<p><span class="math display">\[\hat{\beta}_{[i]} =
(\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1}
(\mathbf{X}_{[i]}^T\mathbf{Y}_{[i]})\]</span> <span
class="math display">\[\hat{\beta}_{[i]} = \left[
(\mathbf{X}^T\mathbf{X})^{-1} +
\frac{(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}}{1
- h_i} \right] (\mathbf{X}^T\mathbf{Y} - \mathbf{x}_i y_i)\]</span></p>
<p>The slide then shows the algebra to simplify this big expression.
When you expand and simplify everything, you get a much cleaner
result:</p>
<p><span class="math display">\[\hat{\beta}_{[i]} = \hat{\beta} -
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i \frac{e_i}{1 - h_i}\]</span> *
This is a beautiful result! It says the LOOCV coefficient vector is just
the <em>full</em> coefficient vector minus a small adjustment term
related to the <span class="math inline">\(i\)</span>-th observation’s
residual (<span class="math inline">\(e_i\)</span>) and leverage (<span
class="math inline">\(h_i\)</span>). * 这是一个非常棒的结果！它表明
LOOCV 系数向量就是<em>完整</em>的系数向量减去一个与第 <span
class="math inline">\(i\)</span> 个观测值的残差 (<span
class="math inline">\(e_i\)</span>) 和杠杆率 (<span
class="math inline">\(h_i\)</span>) 相关的小调整项。</p>
<h4 id="step-4-finding-e_i-slide-104136.png">Step 4: Finding <span
class="math inline">\(e_{[i]}\)</span> (Slide
<code>104136.png</code>)</h4>
<p>This is the final step. We use the definition of <span
class="math inline">\(e_{[i]}\)</span> and the result from Step 3.
这是最后一步。我们使用 <span class="math inline">\(e_{[i]}\)</span>
的定义和步骤 3 的结果。</p>
<ul>
<li><strong>Start with the definition:</strong> <span
class="math inline">\(e_{[i]} = y_i -
\mathbf{x}_i^T\hat{\beta}_{[i]}\)</span></li>
<li><strong>Substitute <span
class="math inline">\(\hat{\beta}_{[i]}\)</span>:</strong> <span
class="math inline">\(e_{[i]} = y_i - \mathbf{x}_i^T \left[ \hat{\beta}
- (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i \frac{e_i}{1 - h_i}
\right]\)</span></li>
<li><strong>Distribute <span
class="math inline">\(\mathbf{x}_i^T\)</span>:</strong> <span
class="math inline">\(e_{[i]} = (y_i - \mathbf{x}_i^T\hat{\beta}) +
\left( \mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i \right)
\frac{e_i}{1 - h_i}\)</span></li>
<li><strong>Recognize the terms!</strong>
<ul>
<li>The first term is just the standard residual: <span
class="math inline">\((y_i - \mathbf{x}_i^T\hat{\beta}) =
e_i\)</span></li>
<li>The second term in parentheses is the definition of leverage: <span
class="math inline">\((\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i)
= h_i\)</span></li>
</ul></li>
<li><strong>Substitute back:</strong> <span
class="math inline">\(e_{[i]} = e_i + h_i \left( \frac{e_i}{1 - h_i}
\right)\)</span></li>
<li><strong>Get a common denominator:</strong> <span
class="math inline">\(e_{[i]} = \frac{e_i(1 - h_i) + h_i e_i}{1 -
h_i}\)</span></li>
<li><strong>Simplify the numerator:</strong> <span
class="math inline">\(e_{[i]} = \frac{e_i - e_ih_i + e_ih_i}{1 -
h_i}\)</span></li>
</ul>
<p>This gives the final, simple relationship: <span
class="math display">\[e_{[i]} = \frac{e_i}{1 - h_i}\]</span></p>
<h3 id="conclusion">Conclusion</h3>
<p>By proving this identity, the slides show that to get all <span
class="math inline">\(N\)</span> of the “leave-one-out” errors, you only
need to: 1. Fit <strong>one</strong> linear regression model on
<strong>all</strong> the data. 2. Calculate the standard residuals <span
class="math inline">\(e_i\)</span> and the leverage values <span
class="math inline">\(h_i\)</span> for all <span
class="math inline">\(N\)</span> points. 3. Apply the formula <span
class="math inline">\(e_i / (1 - h_i)\)</span> for each point.</p>
<p>This turns a procedure that looked like it would take <span
class="math inline">\(N\)</span> times the work into a procedure that
takes only <strong>1</strong> model fit. This is why LOOCV is a
practical and efficient method for linear regression.</p>
<p>通过证明这个恒等式，幻灯片显示，要获得所有 <span
class="math inline">\(N\)</span> 个“留一法”误差，您只需： 1.
对<strong>所有</strong>数据拟合<strong>一个</strong>线性回归模型。 2.
计算所有 <span class="math inline">\(N\)</span> 个点的标准残差 <span
class="math inline">\(e_i\)</span> 和杠杆值 <span
class="math inline">\(h_i\)</span>。 3. 对每个点应用公式 <span
class="math inline">\(e_i / (1 - h_i)\)</span>。</p>
<p>这将一个看似需要 <span class="math inline">\(N\)</span>
倍工作量的过程变成了只需 <strong>1</strong>
次模型拟合的过程。这就是为什么 LOOCV
是一种实用且高效的线性回归方法。</p>
<h1 id="main-goal-of-cross-validation-交叉验证的主要目标">5. Main Goal
of Cross-Validation 交叉验证的主要目标</h1>
<p>The central purpose of cross-validation is to <strong>estimate the
true test error</strong> of a machine learning model. This is crucial
for:</p>
<ol type="1">
<li><strong>Model Assessment:</strong> Evaluating how well a model will
perform on new, unseen data. 评估模型在新的、未见过的数据上的表现。</li>
<li><strong>Model Selection:</strong> Choosing the best level of model
flexibility (e.g., the degree of a polynomial or the value of <span
class="math inline">\(K\)</span> in KNN) to avoid
<strong>overfitting</strong>.
选择最佳的模型灵活性水平（例如，多项式的次数或 KNN 中的 <span
class="math inline">\(K\)</span>
值），以避免<strong>过拟合</strong>。</li>
</ol>
<p>As the slides show, <strong>training error</strong> (the error on the
data the model was trained on) consistently decreases as model
complexity increases. However, the <strong>test error</strong> follows a
U-shape: it first decreases (as the model learns the true signal) and
then increases (as the model starts fitting the noise, or
“overfitting”). CV helps find the minimum point of this U-shaped test
error curve.
<strong>训练误差</strong>（模型训练数据的误差）随着模型复杂度的增加而持续下降。然而，<strong>测试误差</strong>呈现
U
形：它先下降（当模型学习真实信号时），然后上升（当模型开始拟合噪声，即“过拟合”时）。交叉验证有助于找到这条
U 形测试误差曲线的最小值。</p>
<h2 id="important-images-1">Important Images 🖼️</h2>
<p>The most important image is on <strong>Slide 61</strong>.</p>
<p>These two plots perfectly illustrate the concept:</p>
<ul>
<li><strong>Blue Line (Training Error):</strong> Always goes down.</li>
<li><strong>Brown Line (True Test Error):</strong> Forms a “U” shape.
This is what we <em>want</em> to find the minimum of, but it’s unknown
in practice.</li>
<li><strong>Black Line (10-fold CV Error):</strong> This is our
<em>estimate</em> of the test error. Notice how closely it tracks the
brown line. The minimum of the CV curve (marked with an ‘x’) is very
close to the minimum of the true test error.</li>
</ul>
<p>This shows <em>why</em> CV works: it provides a reliable estimate to
guide our choice of model (e.g., polynomial degree 3-4 for logistic
regression, or <span class="math inline">\(K \approx 10\)</span> for
KNN).</p>
<ul>
<li><strong>蓝线（训练误差）：</strong>始终向下。</li>
<li><strong>棕线（真实测试误差）：</strong>呈“U”形。这正是我们<em>想要</em>找到的最小值，但在实际应用中无法确定。</li>
<li><strong>黑线（10 倍 CV
误差）：</strong>这是我们对测试误差的<em>估计</em>。注意它与棕线的吻合程度。CV
曲线的最小值（标有“x”）非常接近真实测试误差的最小值。</li>
</ul>
<p>这说明了 CV
的<em>原因</em>：它提供了可靠的估计值来指导我们选择模型（例如，逻辑回归的多项式次数为
3-4，KNN 的 <span class="math inline">\(K \approx 10\)</span>）。</p>
<h2 id="key-formulas-for-classification">Key Formulas for
Classification</h2>
<p>For regression, we often use Mean Squared Error (MSE). For
classification, the slides introduce the <strong>classification error
rate</strong>.</p>
<p>For Leave-One-Out Cross-Validation (LOOCV), the error for a single
observation <span class="math inline">\(i\)</span> is: <span
class="math display">\[Err_i = I(y_i \neq \hat{y}_i^{(i)})\]</span> *
<span class="math inline">\(y_i\)</span> is the true label for
observation <span class="math inline">\(i\)</span>. * <span
class="math inline">\(\hat{y}_i^{(i)}\)</span> is the model’s prediction
for observation <span class="math inline">\(i\)</span> when the model
was trained on all <em>other</em> observations <em>except</em> <span
class="math inline">\(i\)</span>. * <span
class="math inline">\(I(\dots)\)</span> is an <strong>indicator
function</strong>: it’s <span class="math inline">\(1\)</span> if the
condition is true (prediction is wrong) and <span
class="math inline">\(0\)</span> if false (prediction is correct).</p>
<p>The total <strong>CV error</strong> is simply the average of these
individual errors, which is the overall fraction of incorrect
classifications: <span class="math display">\[CV_{(n)} = \frac{1}{n}
\sum_{i=1}^{n} Err_i\]</span> The slides also show examples using
<strong>Log Loss</strong> (Slide 64), which is another common and
sensitive metric for classification. The logistic regression model
itself is defined by: <span class="math display">\[P(Y=1 | X) =
\frac{1}{1 + \exp(-\beta_0 - \beta_1 X_1 - \beta_2 X_2 -
\dots)}\]</span></p>
<p>对于回归，我们通常使用均方误差
(MSE)。对于分类，幻灯片介绍了<strong>分类错误率</strong>。</p>
<p>对于留一交叉验证 (LOOCV)，单个观测值 <span
class="math inline">\(i\)</span> 的误差为： <span
class="math display">\[Err_i = I(y_i \neq \hat{y}_i^{(i)})\]</span> *
<span class="math inline">\(y_i\)</span> 是观测值 <span
class="math inline">\(i\)</span> 的真实标签。 * <span
class="math inline">\(\hat{y}_i^{(i)}\)</span> 是模型在除 <span
class="math inline">\(i\)</span>
之外的所有其他观测值上进行训练后，对观测值 <span
class="math inline">\(i\)</span> 的预测。 * <span
class="math inline">\(I(\dots)\)</span>
是一个<strong>指示函数</strong>：如果条件为真（预测错误），则为 <span
class="math inline">\(1\)</span>；如果条件为假（预测正确），则为 <span
class="math inline">\(0\)</span>。</p>
<p>总<strong>CV误差</strong>只是这些单个误差的平均值，也就是错误分类的总体比例：
<span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n}
Err_i\]</span>
幻灯片还展示了使用<strong>对数损失</strong>（幻灯片64）的示例，这是另一个常见且敏感的分类指标。逻辑回归模型本身的定义如下：
<span class="math display">\[P(Y=1 | X) = \frac{1}{1 + \exp(-\beta_0 -
\beta_1 X_1 - \beta_2 X_2 - \dots)}\]</span></p>
<h2 id="python-code-explained">Python Code Explained 🐍</h2>
<p>The slides provide two key Python examples. Both manually implement
K-fold cross-validation to show how it works.</p>
<h3 id="knn-regression-slide-52-knn-回归">1. KNN Regression (Slide 52)
KNN 回归</h3>
<ul>
<li><strong>Goal:</strong> Find the best <code>n_neighbors</code> (K)
for a <code>KNeighborsRegressor</code>. 为
<code>KNeighborsRegressor</code> 找到最佳的 <code>n_neighbors</code>
(K)。</li>
<li><strong>Logic:</strong>
<ol type="1">
<li>It creates a <code>KFold</code> object to split the data into 10
folds (<code>n_splits=10</code>). 创建一个 <code>KFold</code>
对象，将数据拆分成 10 个折叠（<code>n_splits=10</code>）。</li>
<li>It has an <strong>outer loop</strong> that iterates through
different values of <span class="math inline">\(K\)</span> (from 1 to
10). 它有一个 <strong>外循环</strong>，迭代不同的 <span
class="math inline">\(K\)</span> 值（从 1 到 10）。</li>
<li>It has an <strong>inner loop</strong> that iterates through the 10
folds (<code>for train_index, test_index in kfold.split(X)</code>).
它有一个 <strong>内循环</strong>，迭代这 10
个折叠（<code>for train_index, test_index in kfold.split(X)</code>）。</li>
<li><strong>Inside the inner loop:</strong>
<ul>
<li>It trains a <code>KNeighborsRegressor</code> on the 9 training folds
(<code>X_train</code>, <code>y_train</code>).</li>
<li>It makes predictions on the 1 held-out test fold
(<code>X_test</code>).</li>
<li>It calculates the mean squared error for that fold and stores
it.</li>
<li>在 9 个训练折叠（<code>X_train</code>, <code>y_train</code>）上训练
<code>KNeighborsRegressor</code>。</li>
<li>它对第一个保留的测试集 (<code>X_test</code>) 进行预测。</li>
<li>它计算该集的均方误差并存储。</li>
</ul></li>
<li><strong>After the inner loop:</strong> It averages the 10 error
scores (one from each fold) to get the final CV error for that specific
<span class="math inline">\(K\)</span>. 对 10
个误差分数（每个集一个）求平均值，得到该特定 <span
class="math inline">\(K\)</span> 的最终 CV 误差。</li>
<li>The final plot shows this CV error vs. <span
class="math inline">\(K\)</span>, allowing us to pick the <span
class="math inline">\(K\)</span> with the lowest error. 最终图表显示了
CV 误差与 <span class="math inline">\(K\)</span>
的关系，使我们能够选择误差最小的 <span
class="math inline">\(K\)</span>。</li>
</ol></li>
</ul>
<h3
id="logistic-regression-with-polynomials-slide-64-使用多项式的逻辑回归">2.
Logistic Regression with Polynomials (Slide 64)
使用多项式的逻辑回归</h3>
<ul>
<li><strong>Goal:</strong> Find the best <code>degree</code> for
<code>PolynomialFeatures</code> used with
<code>LogisticRegression</code>.</li>
<li><strong>Logic:</strong> This is very similar to the KNN example but
uses a different model and error metric.
<ol type="1">
<li>It sets up a 10-fold split (<code>kf = KFold(...)</code>).</li>
<li>An <strong>outer loop</strong> iterates through the
<code>degree</code> <span class="math inline">\(d\)</span> (from 1 to
10).</li>
<li>An <strong>inner loop</strong> iterates through the 10 folds.</li>
<li><strong>Inside the inner loop:</strong>
<ul>
<li>It creates <code>PolynomialFeatures</code> of degree <span
class="math inline">\(d\)</span>.</li>
<li>It transforms the 9 training folds (<code>X_train</code>) into
polynomial features (<code>X_train_poly</code>).</li>
<li>It trains a <code>LogisticRegression</code> model on
<code>X_train_poly</code>.</li>
<li>It transforms the 1 held-out test fold (<code>X_test</code>) using
the <em>same</em> polynomial transformer.</li>
<li>It calculates the <code>log_loss</code> on the test fold.</li>
</ul></li>
<li><strong>After the inner loop:</strong> It averages the 10
<code>log_loss</code> scores to get the final CV error for that
<code>degree</code>.</li>
<li>The plot shows CV error vs. degree, and the minimum is clearly at
<code>degree=3</code>.</li>
</ol></li>
</ul>
<h2 id="the-bias-variance-trade-off-in-cv-cv-中的偏差-方差权衡">The
Bias-Variance Trade-off in CV CV 中的偏差-方差权衡</h2>
<p>This is a key theoretical point from <strong>Slide 54</strong> that
answers the questions on Slide 65. It compares LOOCV (<span
class="math inline">\(K=n\)</span>) with K-fold CV (<span
class="math inline">\(K=5\)</span> or <span
class="math inline">\(10\)</span>). 这是<strong>幻灯片
54</strong>中的一个关键理论点，它回答了幻灯片 65 中的问题。它比较了
LOOCV（K=n）和 K 倍 CV（K=5 或 10）。</p>
<ul>
<li><strong>LOOCV (K=n):</strong>
<ul>
<li><strong>Bias:</strong> Very <strong>low</strong>. The model is
trained on <span class="math inline">\(n-1\)</span> samples, which is
almost the full dataset. The resulting error estimate is nearly unbiased
for the true test error. 该模型基于 <span
class="math inline">\(n-1\)</span>
个样本进行训练，这几乎是整个数据集。得到的误差估计对于真实测试误差几乎没有偏差。</li>
<li><strong>Variance:</strong> Very <strong>high</strong>. You are
training <span class="math inline">\(n\)</span> models that are
<em>almost identical</em> to each other (they only differ by one data
point). Averaging these highly correlated error estimates doesn’t reduce
the variance much, making the CV estimate unstable.
非常<strong>高</strong>。您正在训练 <span
class="math inline">\(n\)</span>
个彼此<em>几乎相同</em>的模型（它们仅相差一个数据点）。对这些高度相关的误差估计求平均值并不能显著降低方差，从而导致
CV 估计不稳定。</li>
</ul></li>
<li><strong>K-Fold CV (K=5 or 10):</strong>
<ul>
<li><strong>Bias:</strong> Slightly <strong>higher</strong> than LOOCV.
The models are trained on, for example, 90% of the data. Since they are
trained on less data, they <em>might</em> perform slightly worse. This
means K-fold CV <strong>tends to slightly overestimate the true test
error</strong> (Slide 66).</li>
<li><strong>Variance:</strong> Much <strong>lower</strong> than LOOCV.
The 10 models are trained on more different “chunks” of data (they
overlap less), so their error estimates are less correlated. Averaging
less-correlated estimates significantly reduces the overall
variance.</li>
</ul></li>
</ul>
<p><strong>Conclusion:</strong> We generally prefer <strong>10-fold
CV</strong> over LOOCV. It gives a much more stable (low-variance)
estimate of the test error, even if it’s slightly more biased
(overestimating the error, which is a safe/conservative estimate).
我们通常更喜欢<strong>10 倍交叉验证</strong>而不是
LOOCV。它能给出更稳定（低方差）的测试误差估计值，即使它的偏差略大（高估了误差，这是一个安全/保守的估计值）。</p>
<h2 id="the-core-problem-scenarios-slides-47-51">The Core Problem &amp;
Scenarios (Slides 47-51)</h2>
<p>These slides use three scenarios to show <em>why</em> we need
cross-validation (CV). The goal is to pick the right level of
<strong>model flexibility</strong> (e.g., the degree of a polynomial or
the complexity of a spline) to minimize the <strong>Test MSE</strong>
(Mean Squared Error), which we can’t see in real life.
这些幻灯片使用了三种场景来说明为什么我们需要交叉验证
(CV)。目标是选择合适的<strong>模型灵活性</strong>（例如，多项式的次数或样条函数的复杂度），以最小化<strong>测试均方误差</strong>（Mean
Squared Error），而这在现实生活中是无法观察到的。</p>
<ul>
<li><p><strong>The Curves (Slide 47):</strong> This slide is
central.</p>
<ul>
<li><p><strong>True Test MSE (Blue) 真实测试均方误差（蓝色）:</strong>
This is the <em>real</em> error on new data. It has a
<strong>U-shape</strong>. Error is high for simple models (high bias),
drops as the model fits the data, and rises again for overly complex
models (high variance, or overfitting).
<strong>这是新数据的<em>真实</em>误差。它呈</strong>U
形**。对于简单模型（高偏差），误差较高；随着模型拟合数据的深入，误差会下降；对于过于复杂的模型（高方差或过拟合），误差会再次上升。</p></li>
<li><p><strong>LOOCV (Black Dashed) &amp; 10-Fold CV (Orange)
LOOCV（黑色虚线）和 10 倍 CV（橙色）:</strong> These are our
<em>estimates</em> of the true test MSE. Notice how closely they track
the blue curve. The ‘x’ marks the minimum of the CV curve, which is our
<em>best guess</em> for the model with the minimum test MSE.
这些是我们对真实测试 MSE
的<em>估计</em>。请注意它们与蓝色曲线的吻合程度。“x”标记 CV
曲线的最小值，这是我们对具有最小测试 MSE
的模型的<em>最佳猜测</em>。</p></li>
</ul></li>
<li><p><strong>Scenario 1 (Slide 48):</strong> The true relationship is
non-linear. The right-hand plot shows that the test MSE (red curve) is
high for the simple linear model (blue square), but lower for the more
flexible smoothing splines (teal squares). CV helps us find the “sweet
spot.”
真实的关系是非线性的。右侧图表显示，对于简单的线性模型（蓝色方块），测试
MSE（红色曲线）较高，而对于更灵活的平滑样条函数（蓝绿色方块），测试 MSE
较低。CV 帮助我们找到“最佳点”。</p></li>
<li><p><strong>Scenario 2 (Slide 49):</strong> The true relationship is
<strong>linear</strong>. Here, the test MSE (red curve) is
<em>lowest</em> for the simplest model (the linear one, blue square). CV
correctly identifies this, and its error estimate (blue square) is
lowest for that model.
真实的关系是<strong>线性</strong>的。在这里，对于最简单的模型（线性模型，蓝色方块），测试
MSE（红色曲线）<em>最低</em>。CV
正确地识别了这一点，并且其误差估计（蓝色方块）是该模型中最低的。</p></li>
<li><p><strong>Scenario 3 (Slide 50):</strong> The true relationship is
<strong>highly non-linear</strong>. The linear model (orange) is a very
poor fit. The test MSE (red curve) is minimized by the most flexible
model (teal square). CV again finds this.
真实的关系是<strong>高度非线性</strong>的。线性模型（橙色）拟合度很差。测试
MSE（红色曲线）被最灵活的模型（蓝绿色方块）最小化。CV
再次发现了这一点。</p></li>
<li><p><strong>Key Takeaway (Slide 51):</strong> We use CV to find the
<strong>tuning parameter</strong> (like polynomial degree) that
minimizes the test error. We care less about the <em>actual value</em>
of the CV error and more about <em>where its minimum is</em>. 我们使用
CV
来找到最小化测试误差的<strong>调整参数</strong>（例如多项式次数）。我们不太关心
CV 误差的<em>实际值</em>，而更关心<em>它的最小值</em>。</p></li>
</ul>
<h2 id="cv-for-classification-slides-55-61">CV for Classification
(Slides 55-61)</h2>
<p>This section shifts from regression (predicting a number, using MSE)
to classification (predicting a category, like “blue” or “orange”).
本节从回归（使用 MSE
预测数字）转向分类（预测类别，例如“蓝色”或“橙色”）。</p>
<ul>
<li><strong>New Error Metric (Slide 55):</strong> We can’t use MSE. A
natural choice is the <strong>classification error rate</strong>.
我们不能使用 MSE。一个自然的选择是<strong>分类错误率</strong>。
<ul>
<li><span class="math inline">\(Err_i = I(y_i \neq
\hat{y}_i^{(i)})\)</span></li>
<li>This is an <strong>indicator function</strong>: it is
<strong>1</strong> if the prediction for the <span
class="math inline">\(i\)</span>-th data point (when trained
<em>without</em> it) is wrong, and <strong>0</strong> if it’s correct.
如果对第 <span class="math inline">\(i\)</span>
个数据点的预测（在没有它的情况下训练时）错误，则为
<strong>1</strong>；如果正确，则为 <strong>0</strong>。</li>
<li>The final CV error is just the average of these 0s and 1s, giving
the total fraction of misclassified points: <span
class="math inline">\(CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n}
Err_i\)</span> 最终的 CV 误差就是这些 0 和 1
的平均值，即错误分类点的总比例：<span class="math inline">\(CV_{(n)} =
\frac{1}{n} \sum_{i=1}^{n} Err_i\)</span></li>
</ul></li>
<li><strong>The Example (Slides 56-61):</strong>
<ul>
<li><strong>Slides 56-58:</strong> We are shown a “true” (but unknown)
non-linear boundary (purple dashed line) separating two classes. We then
try to <em>estimate</em> this boundary using logistic regression with
different polynomial degrees (degree 1, 2, 3, 4).
我们看到了一条“真实”（但未知）的非线性边界（紫色虚线），它将两个类别分开。然后，我们尝试使用不同次数（1、2、3、4
次）的逻辑回归来<em>估计</em>这条边界。</li>
<li><strong>Slides 59-60:</strong> This is a crucial point. In this
<em>simulated</em> example, we <em>do</em> know the true test error
rates. The true errors are [0.201, 0.197, <strong>0.160</strong>,
0.162]. The lowest error is for the 3rd-degree polynomial. But in a
real-world problem, <strong>we can never know these true
errors</strong>.
这一点至关重要。在这个<em>模拟</em>示例中，我们<em>确实</em>知道真实的测试错误率。真实误差为
[0.201, 0.197, <strong>0.160</strong>,
0.162]。最小误差出现在三次多项式中。但在实际问题中，<strong>我们永远无法知道这些真实误差</strong>。</li>
<li><strong>Slide 61 (The Solution):</strong> This is the most important
image. It shows how CV <em>solves</em> the problem from slide 60.展示了
CV 如何<em>解决</em>幻灯片 60 中的问题。
<ul>
<li><p><strong>Brown Curve (Test Error):</strong> This is the
<em>true</em> test error (from slide 59). We can’t see this in practice.
Its minimum is at degree 3. 这是<em>真实</em>的测试误差（来自幻灯片
59）。我们在实践中看不到它。它的最小值在 3 次方处。</p></li>
<li><p><strong>Black Curve (10-fold CV Error):</strong> This is what we
<em>can</em> calculate. It’s our estimate of the test error.
<strong>Crucially, its minimum is also at degree 3.</strong></p></li>
<li><p><strong>黑色曲线（10 倍 CV
误差）：</strong>这是我们<em>可以</em>计算出来的。这是我们对测试误差的估计。<strong>至关重要的是，它的最小值也在
3 次方处。</strong></p></li>
<li><p>This proves that CV successfully found the best model (degree 3)
without ever seeing the <em>true</em> test error. The same logic is
shown for the KNN classifier on the right.</p></li>
<li><p>这证明 CV 成功地找到了最佳模型（3
次方），而从未看到<em>真实</em>的测试误差。右侧的 KNN
分类器也显示了相同的逻辑。</p></li>
</ul></li>
</ul></li>
</ul>
<h2 id="python-code-explained-slides-52-63-64">Python Code Explained
(Slides 52, 63, 64)</h2>
<p>The slides show how to <em>manually</em> implement K-fold CV. This is
great for understanding, even though libraries like
<code>GridSearchCV</code> can do this automatically.</p>
<ul>
<li><strong>KNN Regression (Slide 52):</strong>
<ol type="1">
<li><code>kfold = KFold(n_splits=10, ...)</code>: Creates an object that
knows how to split the data into 10 folds.</li>
<li><code>for n_k in neighbors:</code>: This is the <strong>outer
loop</strong> to test different <span class="math inline">\(K\)</span>
values (e.g., <span class="math inline">\(K\)</span>=1, 2, 3…).</li>
<li><code>for train_index, test_index in kfold.split(X):</code>: This is
the <strong>inner loop</strong>. For a <em>single</em> <span
class="math inline">\(K\)</span>, it loops 10 times.</li>
<li>Inside the inner loop:
<ul>
<li>It splits the data into a 9-fold training set (<code>X_train</code>)
and a 1-fold test set (<code>X_test</code>).</li>
<li>It trains a <code>KNeighborsRegressor</code> on
<code>X_train</code>.</li>
<li>It makes predictions on <code>X_test</code> and calculates the error
(<code>mean_squared_error</code>).</li>
</ul></li>
<li><code>cv_errors.append(np.mean(mse_errors_k))</code>: After the
inner loop finishes 10 runs, it averages the 10 error scores for that
<span class="math inline">\(K\)</span> and stores it.</li>
<li>The final plot shows <code>cv_errors</code>
vs. <code>neighbors</code>, letting you pick the <span
class="math inline">\(K\)</span> with the lowest average error.</li>
</ol></li>
<li><strong>Logistic Regression Classification (Slides 63-64):</strong>
<ul>
<li>This code is almost identical, but with three key differences:
<ol type="1">
<li>The model is <code>LogisticRegression</code>.</li>
<li>It uses <code>PolynomialFeatures</code> to create new features
(<span class="math inline">\(X^2, X^3,\)</span> etc.) <em>inside</em>
the loop.</li>
<li>The error metric is <code>log_loss</code> (a common, more sensitive
metric than the simple 0/1 error rate).</li>
</ol></li>
<li>The plot on slide 64 shows the 10-fold CV error (using Log Loss)
vs. the Degree of the Polynomial. The minimum is clearly at
<strong>Degree = 3</strong>, matching the finding from slide 61.</li>
</ul></li>
</ul>
<h2 id="answering-the-key-questions-slides-54-65">Answering the Key
Questions (Slides 54 &amp; 65)</h2>
<p>Slide 65 asks two critical questions, which are answered directly by
the concepts on <strong>Slide 54 (Bias and variance
trade-off)</strong>.</p>
<h3 id="q1-how-does-k-affect-the-bias-and-variance-of-the-cv-error">Q1:
How does K affect the bias and variance of the CV error?</h3>
<p>This refers to <span class="math inline">\(K\)</span> in K-fold CV
(not to be confused with <span class="math inline">\(K\)</span> in KNN).
K 如何影响 CV 误差的偏差和方差？</p>
<ul>
<li><strong>Bias:</strong>
<ul>
<li><p><strong>LOOCV (K = n):</strong> This has <strong>very low
bias</strong>. The model is trained on <span
class="math inline">\(n-1\)</span> samples, which is <em>almost</em> the
full dataset. So, the error estimate <span
class="math inline">\(CV_{(n)}\)</span> is an almost-unbiased estimate
of the true test error. ** 它的<strong>偏差非常低</strong>。该模型基于
<span class="math inline">\(n-1\)</span>
个样本进行训练，这几乎是整个数据集。因此，误差估计 <span
class="math inline">\(CV_{(n)}\)</span>
是对真实测试误差的几乎无偏估计。</p></li>
<li><p><strong>K-Fold (K &lt; n, e.g., K=10):</strong> This has
<strong>slightly higher bias</strong>. The models are trained on, for
example, 90% of the data. Because they are trained on less data, they
<em>might</em> perform slightly worse than a model trained on 100% of
the data. This “pessimism” is the source of the bias.
<strong>偏差略高</strong>。例如，这些模型是基于 90%
的数据进行训练的。由于它们基于较少的数据进行训练，因此它们的性能<em>可能</em>会比基于
100% 数据进行训练的模型略差。这种“悲观”正是偏差的根源。</p></li>
</ul></li>
<li><strong>Variance:</strong>
<ul>
<li><p><strong>LOOCV (K = n):</strong> This has <strong>very high
variance</strong>. You are training <span
class="math inline">\(n\)</span> models that are <em>almost
identical</em> (they only differ by one data point). Averaging <span
class="math inline">\(n\)</span> highly-correlated error estimates
doesn’t reduce the variance much. This makes the final <span
class="math inline">\(CV_{(n)}\)</span> estimate unstable.
<strong>这种模型的方差</strong>非常高**。您正在训练 <span
class="math inline">\(n\)</span>
个<em>几乎相同</em>的模型（它们只有一个数据点不同）。对 <span
class="math inline">\(n\)</span>
个高度相关的误差估计取平均值并不能显著降低方差。这使得最终的 <span
class="math inline">\(CV_{(n)}\)</span> 估计值不稳定。</p></li>
<li><p><strong>K-Fold (K &lt; n, e.g., K=10):</strong> This has
<strong>much lower variance</strong>. The 10 models are trained on more
different “chunks” of data (they overlap less). Their error estimates
are less correlated, and averaging 10 less-correlated numbers gives a
much more stable (low-variance) final estimate.
<strong>这种模型的方差</strong>非常低**。这 10
个模型基于更多不同的数据“块”进行训练（它们重叠较少）。它们的误差估计值相关性较低，对
10
个相关性较低的数取平均值可以得到更稳定（低方差）的最终估计值。</p></li>
</ul></li>
</ul>
<p><strong>Conclusion (The Trade-off):</strong> We prefer <strong>K-fold
CV (K=5 or 10)</strong> over LOOCV. It gives a much more stable
(low-variance) estimate, and we are willing to accept a tiny increase in
bias to get it. 我们更喜欢<strong>K 倍交叉验证（K=5 或
10）</strong>，而不是单倍交叉验证。它能给出更稳定（低方差）的估计值，并且我们愿意接受偏差的轻微增加来获得它。</p>
<h3
id="q2-does-cross-validation-over-estimate-or-under-estimate-the-true-test-error">Q2:
Does Cross Validation over-estimate or under-estimate the true test
error?</h3>
<p>交叉验证会高估还是低估真实测试误差？</p>
<p>Based on the bias discussion above:</p>
<p>Cross-validation (especially K-fold) generally <strong>over-estimates
the true test error</strong>. 交叉验证（尤其是 K
倍交叉验证）通常会<strong>高估真实测试误差</strong>。</p>
<p><strong>Reasoning:</strong> 1. The “true test error” is the error of
a model trained on the <em>entire dataset</em> (<span
class="math inline">\(n\)</span> samples). 2. K-fold CV trains its
models on <em>subsets</em> of the data (e.g., <span
class="math inline">\(n \times (K-1)/K\)</span> samples). 3. Since these
models are trained on <em>less</em> data, they are (on average) slightly
worse than the final model trained on all the data. 4. Because the CV
models are slightly worse, their error rates will be slightly
<em>higher</em>. 5. Therefore, the final CV error score is a slightly
“pessimistic” or high estimate. This is considered a good thing, as it’s
a <em>conservative</em> estimate of how our model will perform.
<strong>理由：</strong> 1.
“真实测试误差”是指在<em>整个数据集</em>（<span
class="math inline">\(n\)</span> 个样本）上训练的模型的误差。 2. K
折交叉验证 (K-fold CV) 在数据<em>子集</em>上训练其模型（例如，<span
class="math inline">\(n \times (K-1)/K\)</span> 个样本）。 3.
由于这些模型基于<em>较少</em>的数据进行训练，因此它们（平均而言）比基于所有数据训练的最终模型略差。
4. 由于 CV 模型略差，其错误率会略高<em>。 5. 因此，最终的 CV
错误率是一个略微“悲观”或偏高的估计。这被认为是一件好事，因为它是对模型性能的</em>保守*估计。</p>
<h1 id="summary-of-bootstrap">6. Summary of Bootstrap</h1>
<p>Bootstrap is a <strong>resampling technique</strong> used to estimate
the <strong>uncertainty</strong> (like standard error or confidence
intervals) of a statistic. Its key idea is to <strong>treat your
original data sample as a proxy for the true population</strong>. It
then simulates the process of drawing new samples by instead
<strong>sampling <em>with replacement</em></strong> from your original
sample. Bootstrap
是一种<strong>重采样技术</strong>，用于估计统计数据的<strong>不确定性</strong>（例如标准误差或置信区间）。其核心思想是<strong>将原始数据样本视为真实总体的替代样本</strong>。然后，它通过从原始样本中进行<strong>有放回的</strong>抽样来模拟抽取新样本的过程。</p>
<h3 id="the-problem">The Problem</h3>
<p>You have a single data sample (e.g., <span
class="math inline">\(n=100\)</span> people) and you calculate a
statistic, like the sample mean (<span
class="math inline">\(\bar{x}\)</span>) or a regression coefficient
(<span class="math inline">\(\hat{\beta}\)</span>). You want to know how
<em>accurate</em> this statistic is. How much would it vary if you could
repeat your experiment many times? This variation is measured by the
<strong>standard error (SE)</strong>. 您有一个数据样本（例如，<span
class="math inline">\(n=100\)</span>
人），并计算一个统计数据，例如样本均值 (<span
class="math inline">\(\bar{x}\)</span>) 或回归系数 (<span
class="math inline">\(\hat{\beta}\)</span>)。您想知道这个统计数据的<em>准确度</em>。如果可以多次重复实验，它会有多少变化？这种变化可以用<strong>标准误差
(SE)</strong> 来衡量。</p>
<h3 id="the-bootstrap-solution">The Bootstrap Solution</h3>
<p>Since you can’t re-run the whole experiment, you <em>simulate</em> it
using the one sample you have.
由于您无法重新运行整个实验，因此您可以使用现有的一个样本进行“模拟”。</p>
<p><strong>The Process:</strong> 1. <strong>Original Sample (<span
class="math inline">\(Z\)</span>) 原始样本 (<span
class="math inline">\(Z\)</span>):</strong> You have your one dataset
with <span class="math inline">\(n\)</span> observations. 2.
<strong>Bootstrap Sample (<span class="math inline">\(Z^{*1}\)</span>)
Bootstrap 样本 (<span class="math inline">\(Z^{*1}\)</span>):</strong>
Create a <em>new</em> dataset of size <span
class="math inline">\(n\)</span> by randomly pulling observations from
your original sample <em>with replacement</em>. (This means some
original observations will be picked multiple times, and some not at
all). 3. <strong>Calculate Statistic (<span
class="math inline">\(\hat{\theta}^{*1}\)</span>) 计算统计量 (<span
class="math inline">\(\hat{\theta}^{*1}\)</span>):</strong> Calculate
your statistic of interest (e.g., the mean, <span
class="math inline">\(\hat{\alpha}\)</span>, regression coefficients) on
this new bootstrap sample. 4. <strong>Repeat 重复:</strong> Repeat steps
2 and 3 a large number of times (<span class="math inline">\(B\)</span>,
e.g., <span class="math inline">\(B=1000\)</span>). This gives you <span
class="math inline">\(B\)</span> bootstrap statistics: <span
class="math inline">\(\hat{\theta}^{*1}, \hat{\theta}^{*2}, ...,
\hat{\theta}^{*B}\)</span>. 5. <strong>Analyze the Bootstrap
Distribution 分析自举分布:</strong> This collection of <span
class="math inline">\(B\)</span> statistics is your “bootstrap
distribution.” * <strong>Standard Error 标准误差:</strong> The
<strong>standard deviation</strong> of this bootstrap distribution is
your estimate of the <strong>standard error</strong> of your original
statistic. * <strong>Confidence Interval 置信区间:</strong> A 95%
confidence interval can be found by taking the <strong>2.5th and 97.5th
percentiles</strong> of this bootstrap distribution.</p>
<p><strong>Why use it?</strong> It’s powerful because it doesn’t rely on
strong theoretical assumptions (like data being normally distributed).
It can be applied to almost <em>any</em> statistic, even very complex
ones (like the prediction from a KNN model), for which a simple
mathematical formula for standard error doesn’t exist.
它非常强大，因为它不依赖于严格的理论假设（例如数据服从正态分布）。它几乎可以应用于<em>任何</em>统计数据，即使是非常复杂的统计数据（例如
KNN 模型的预测），因为这些统计数据没有简单的标准误差数学公式。</p>
<h2 id="mathematical-understanding">Mathematical Understanding</h2>
<p>The core idea is to use the <strong>empirical distribution</strong>
(your sample) as an estimate for the true <strong>population
distribution</strong>.
其核心思想是使用<strong>经验分布</strong>（你的样本）来估计真实的<strong>总体分布</strong>。</p>
<h3 id="example-estimating-alpha">Example: Estimating <span
class="math inline">\(\alpha\)</span></h3>
<p>Your slides provide an example of finding the <span
class="math inline">\(\alpha\)</span> that minimizes the variance of a
portfolio, <span class="math inline">\(var(\alpha X +
(1-\alpha)Y)\)</span>. 用于计算使投资组合方差最小化的 <span
class="math inline">\(\alpha\)</span>，即 <span
class="math inline">\(var(\alpha X + (1-\alpha)Y)\)</span>。</p>
<ol type="1">
<li><p><strong>True Population Parameter (<span
class="math inline">\(\alpha\)</span>) 真实总体参数 (<span
class="math inline">\(\alpha\)</span>):</strong> The <em>true</em> <span
class="math inline">\(\alpha\)</span> is a function of the
<em>population</em> variances and covariance: <em>真实</em> <span
class="math inline">\(\alpha\)</span>
是<em>总体</em>方差和协方差的函数： <span class="math display">\[\alpha
= \frac{\sigma_Y^2 - \sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 -
2\sigma_{XY}}\]</span> We can never know this value exactly unless we
know the entire population.
除非我们了解整个总体，否则我们永远无法准确知道这个值。</p></li>
<li><p><strong>Sample Statistic (<span
class="math inline">\(\hat{\alpha}\)</span>) 样本统计量 (<span
class="math inline">\(\hat{\alpha}\)</span>):</strong> We
<em>estimate</em> <span class="math inline">\(\alpha\)</span> using our
sample, creating the statistic <span
class="math inline">\(\hat{\alpha}\)</span> by plugging in our
<em>sample</em> variances and covariance: 我们使用样本<em>估计</em>
<span
class="math inline">\(\alpha\)</span>，通过代入<em>样本</em>方差和协方差来创建统计量
<span class="math inline">\(\hat{\alpha}\)</span>： <span
class="math display">\[\hat{\alpha} = \frac{\hat{\sigma}_Y^2 -
\hat{\sigma}_{XY}}{\hat{\sigma}_X^2 + \hat{\sigma}_Y^2 -
2\hat{\sigma}_{XY}}\]</span> This <span
class="math inline">\(\hat{\alpha}\)</span> is just <em>one number</em>
from our single sample. How confident are we in it? We need its standard
error, <span class="math inline">\(SE(\hat{\alpha})\)</span>. 这个 <span
class="math inline">\(\hat{\alpha}\)</span>
只是我们单个样本中的一个数字。我们对它的置信度有多高？我们需要它的标准误差，<span
class="math inline">\(SE(\hat{\alpha})\)</span>。</p></li>
<li><p><strong>Bootstrap Statistic (<span
class="math inline">\(\hat{\alpha}^*\)</span>) 自举统计量 (<span
class="math inline">\(\hat{\alpha}^*\)</span>):</strong> We apply the
bootstrap process:</p>
<ul>
<li>Create a bootstrap sample (by resampling with replacement).
创建一个自举样本（通过放回重采样）。</li>
<li>Calculate <span class="math inline">\(\hat{\alpha}^*\)</span> using
the sample (co)variances of this <em>new bootstrap sample</em>.
使用这个<em>新自举样本</em>的样本（协）方差计算 <span
class="math inline">\(\hat{\alpha}^*\)</span>。</li>
<li>Repeat <span class="math inline">\(B\)</span> times to get <span
class="math inline">\(B\)</span> values: <span
class="math inline">\(\hat{\alpha}^{*1}, \hat{\alpha}^{*2}, ...,
\hat{\alpha}^{*B}\)</span>. 重复 <span class="math inline">\(B\)</span>
次，得到 <span class="math inline">\(B\)</span> 个值：<span
class="math inline">\(\hat{\alpha}^{*1}, \hat{\alpha}^{*2}, ...,
\hat{\alpha}^{*B}\)</span>。</li>
</ul></li>
<li><p><strong>Estimating the Standard Error 估算标准误差:</strong> The
standard error of our original estimate <span
class="math inline">\(\hat{\alpha}\)</span> is <em>estimated</em> by the
standard deviation of all our bootstrap estimates: 我们原始估计值 <span
class="math inline">\(\hat{\alpha}\)</span>
的标准误差是通过所有自举估计值的标准差来“估算”的： <span
class="math display">\[SE_{boot}(\hat{\alpha}) = \sqrt{\frac{1}{B-1}
\sum_{j=1}^{B} (\hat{\alpha}^{*j} - \bar{\alpha}^*)^2}\]</span> where
<span class="math inline">\(\bar{\alpha}^*\)</span> is the average of
all <span class="math inline">\(B\)</span> bootstrap estimates. <span
class="math inline">\(\bar{\alpha}^*\)</span> 是所有 <span
class="math inline">\(B\)</span> 个自举估计值的平均值。</p></li>
</ol>
<p>The slides (p. 73, 77-78) show this visually. The “sampling from
population” histogram (left) is the <em>true</em> sampling distribution,
which we can only create in a simulation. The “Bootstrap” histogram
(right) is the bootstrap distribution created from <em>one</em> sample.
They look very similar, which shows the method works.
“从总体抽样”直方图（左图）是<em>真实</em>的抽样分布，我们只能在模拟中创建它。“自举”直方图（右图）是从<em>一个</em>样本创建的自举分布。它们看起来非常相似，这表明该方法有效。</p>
<h2 id="code-analysis">Code Analysis</h2>
<h3 id="r-alpha-example-slides-75-77">R: <span
class="math inline">\(\alpha\)</span> Example (Slides 75 &amp; 77)</h3>
<ul>
<li><strong>Slide 75 (<code>The R code</code>): This is a SIMULATION,
not Bootstrap.</strong>
<ul>
<li><code>for(i in 1:m)&#123;...&#125;</code>: This loop runs <code>m=1000</code>
times.</li>
<li><code>returns &lt;- rmvnorm(...)</code>: <strong>Inside the
loop,</strong> it draws a <em>brand new sample</em> from the <em>true
population</em> every time.</li>
<li><code>alpha[i] &lt;- ...</code>: It calculates <span
class="math inline">\(\hat{\alpha}\)</span> for each new sample.</li>
<li><strong>Purpose:</strong> This code shows the <em>true sampling
distribution</em> of <span class="math inline">\(\hat{\alpha}\)</span>
(the “Histogram of alpha”). You can only do this if you know the true
population, as in a simulation.</li>
</ul></li>
<li><strong>Slide 77 (<code>The R code</code>): This IS
Bootstrap.</strong>
<ul>
<li><code>returns &lt;- rmvnorm(...)</code>: <strong>Outside the
loop,</strong> this is done <em>only once</em> to get <em>one</em>
original sample.</li>
<li><code>for(i in 1:B)&#123;...&#125;</code>: This is the bootstrap loop.</li>
<li><code>sample(1:nrow(returns), n, replace = T)</code>: <strong>This
is the key line.</strong> It randomly selects row numbers <em>with
replacement</em> from the <em>single</em> <code>returns</code>
dataset.</li>
<li><code>returns_boot &lt;- returns[sample(...), ]</code>: This creates
the bootstrap sample.</li>
<li><code>alpha_bootstrap[i] &lt;- ...</code>: It calculates <span
class="math inline">\(\hat{\alpha}^*\)</span> on the
<code>returns_boot</code> sample.</li>
<li><strong>Purpose:</strong> This code generates the <em>bootstrap
distribution</em> (the “Bootstrap” histogram on slide 78) to
<em>estimate</em> the true sampling distribution.</li>
</ul></li>
</ul>
<h3 id="r-linear-regression-example-slides-79-81">R: Linear Regression
Example (Slides 79 &amp; 81)</h3>
<ul>
<li><strong>Slide 79:</strong>
<ul>
<li><code>boot.fn &lt;- function(data, index)&#123; ... &#125;</code>: Defines a
function that the <code>boot</code> package needs. It takes data and an
<code>index</code> vector.</li>
<li><code>lm(mpg~horsepower, data=data, subset=index)</code>: This is
the core. It fits a linear model <em>only</em> on the data points
specified by the <code>index</code>. The <code>boot</code> function will
automatically supply this <code>index</code> as a
resampled-with-replacement vector.</li>
<li><code>boot(Auto, boot.fn, R=1000)</code>: This runs the bootstrap.
It calls <code>boot.fn</code> 1000 times, each time with a new resampled
<code>index</code>, and collects the coefficients.</li>
</ul></li>
<li><strong>Slide 81:</strong>
<ul>
<li><code>summary(lm(...))</code>: Shows the standard output. The “Std.
Error” column (e.g., 0.860, 0.006) is calculated using <em>mathematical
theory</em>.</li>
<li><code>boot.res</code>: Shows the bootstrap output. The “std. error”
column (e.g., 0.841, 0.007) is the <strong>standard deviation of the
1000 bootstrap estimates</strong>.</li>
<li><strong>Main Point:</strong> The standard errors from the bootstrap
are very close to the theoretical ones. This confirms the uncertainty.
If the model assumptions were violated, the bootstrap SE would be more
trustworthy.</li>
<li>The histograms show the bootstrap distributions for the intercept
(<code>t1*</code>) and the slope (<code>t2*</code>). The arrows show the
95% percentile confidence interval.</li>
</ul></li>
</ul>
<h3 id="python-knn-regression-example-slide-80">Python: KNN Regression
Example (Slide 80)</h3>
<p>This shows how to get a confidence interval for a <em>single
prediction</em>.</p>
<ul>
<li><code>for i in range(n_bootstraps):</code>: The bootstrap loop.</li>
<li><code>indices = np.random.choice(train_samples.shape[0], train_samples.shape[0], replace=True)</code>:
<strong>This is the key line</strong> in Python (like
<code>sample</code> in R). It gets a new set of indices with
replacement.</li>
<li><code>X_boot, y_boot = ...</code>: Creates the bootstrap
sample.</li>
<li><code>model.fit(X_boot, y_boot)</code>: A <em>new</em> KNN model is
trained on this bootstrap sample.</li>
<li><code>bootstrap_preds.append(model.predict(predict_point))</code>:
The model (trained on <span class="math inline">\(Z^{*i}\)</span>) makes
a prediction for the <em>same</em> fixed point. This is repeated 1000
times.</li>
<li><strong>Result:</strong> You get a <em>distribution of
predictions</em> for that one point. The 2.5th and 97.5th percentiles of
this distribution give you a 95% confidence interval <em>for that
specific prediction</em>. 你会得到该点的<em>预测分布</em>。该分布的 2.5
和 97.5 百分位数为该特定预测提供了 95% 的置信区间。</li>
</ul>
<h3 id="python-knn-on-auto-data-slide-82">Python: KNN on Auto data
(Slide 82)</h3>
<ul>
<li><strong>BE CAREFUL:</strong> This slide <strong>does NOT show
Bootstrap</strong>. It shows <strong>K-Fold Cross-Validation
(CV)</strong>.</li>
<li><strong>Purpose:</strong> The goal here is <em>not</em> to find
uncertainty. The goal is to find the <strong>best
hyperparameter</strong> (the best value for <span
class="math inline">\(k\)</span>, the number of neighbors).</li>
<li><strong>Method:</strong>
<ul>
<li><code>kf = KFold(n_splits=10)</code>: Splits the data into 10 chunks
(“folds”).</li>
<li><code>for train_index, test_index in kf.split(X):</code>: It loops
10 times. Each time, it trains on 9 chunks and tests on 1 chunk.</li>
</ul></li>
<li><strong>Key Difference for Exam:</strong>
<ul>
<li><strong>Bootstrap:</strong> Samples <em>with replacement</em> to
estimate <strong>uncertainty/standard error</strong>.</li>
<li><strong>Cross-Validation:</strong> Splits data <em>without
replacement</em> into <span class="math inline">\(K\)</span> folds to
estimate model <strong>performance/prediction error</strong> and tune
hyperparameters.</li>
<li><strong>自举法</strong>：使用<em>有放回</em>的样本来估计<strong>不确定性/标准误差</strong>。</li>
<li><strong>交叉验证</strong>：将数据<em>无放回</em>地分成 <span
class="math inline">\(K\)</span>
份，以估计模型<strong>性能/预测误差</strong>并调整超参数。</li>
</ul></li>
</ul>
<h1
id="the-mathematical-theory-of-bootstrap-and-the-extension-to-cross-validation-cv.">7.
The mathematical theory of Bootstrap and the extension to
Cross-Validation (CV).</h1>
<h2 id="code-analysis-bootstrap-for-a-knn-prediction-slide-85">1. Code
Analysis: Bootstrap for a KNN Prediction (Slide 85)</h2>
<p>This Python code shows a different use of bootstrap: <strong>finding
the confidence interval for a single prediction</strong>, not for a
model coefficient.</p>
<ul>
<li><strong>Goal:</strong> To estimate the uncertainty of a KNN model’s
prediction for a <em>specific</em> new data point
(<code>predict_point</code>).</li>
<li><strong>Process:</strong>
<ol type="1">
<li><strong>Train Full Model:</strong> A KNN model (<code>knn</code>) is
first trained on the <em>entire</em> dataset. It makes one prediction
(<code>knpred</code>) for <code>predict_point</code>. This is our <span
class="math inline">\(\hat{f}(x_0)\)</span>.</li>
<li><strong>Bootstrap Loop
(<code>for i in range(n_bootstraps)</code>):</strong>
<ul>
<li><code>indices = np.random.choice(...)</code>: <strong>This is the
core bootstrap step.</strong> It creates a new list of indices by
sampling <em>with replacement</em> from the original data.</li>
<li><code>X_boot, y_boot = ...</code>: This creates the new bootstrap
dataset (<span class="math inline">\(Z^{*i}\)</span>).</li>
<li><code>km.fit(X_boot, y_boot)</code>: A <em>new</em> KNN model
(<code>km</code>) is trained <em>only</em> on this bootstrap
sample.</li>
<li><code>bootstrap_preds.append(km.predict(predict_point))</code>: This
newly trained model makes a prediction for the <em>same</em>
<code>predict_point</code>. This value is <span
class="math inline">\(\hat{f}^{*i}(x_0)\)</span>.</li>
</ul></li>
<li><strong>Analyze Distribution:</strong> After 1000 loops,
<code>bootstrap_preds</code> contains 1000 different predictions for the
same point.</li>
<li><strong>Confidence Interval:</strong>
<ul>
<li><code>np.percentile(bootstrap_preds, [2.5, 97.5])</code>: This finds
the 2.5th and 97.5th percentiles of the 1000 bootstrap predictions.</li>
<li>The resulting <code>[lower_bound, upper_bound]</code> (e.g.,
<code>[13.70, 15.70]</code>) forms the 95% confidence interval for the
prediction.</li>
</ul></li>
</ol></li>
<li><strong>Histogram Plot:</strong> The plot on the right visually
confirms this. It shows the distribution of the 1000 bootstrap
predictions, with the 95% confidence interval marked by the red dashed
lines.</li>
</ul>
<h2
id="mathematical-understanding-why-does-bootstrap-work-slides-87-88">2.
Mathematical Understanding: Why Does Bootstrap Work? (Slides 87-88)</h2>
<p>This is the theoretical justification for the entire method. It’s
based on an analogy. 这是整个方法的理论依据。它基于一个类比。</p>
<h3 id="the-true-world-slide-87-top">The “True” World (Slide 87,
Top)</h3>
<ul>
<li><p><strong>Population:</strong> There is a true, unknown population
distribution <span class="math inline">\(F\)</span>.
存在一个真实的、未知的总体分布 <span
class="math inline">\(F\)</span>。</p></li>
<li><p><strong>Parameter:</strong> We want to know a true parameter,
<span class="math inline">\(\theta\)</span>, which is a function of
<span class="math inline">\(F\)</span> (e.g., the true population mean).
我们想知道一个真实的参数 <span
class="math inline">\(\theta\)</span>，它是 <span
class="math inline">\(F\)</span>
的函数（例如，真实的总体均值）。</p></li>
<li><p><strong>Sample:</strong> We get <em>one</em> sample <span
class="math inline">\(X_1, ..., X_n\)</span> from <span
class="math inline">\(F\)</span>. 我们从 <span
class="math inline">\(F\)</span> 中获取<em>一个</em>样本 <span
class="math inline">\(X_1, ..., X_n\)</span>。</p></li>
<li><p><strong>Statistic:</strong> We calculate our best estimate <span
class="math inline">\(\hat{\theta}\)</span> from our sample. (e.g., the
sample mean <span class="math inline">\(\bar{x}\)</span>). <span
class="math inline">\(\hat{\theta}\)</span> is our proxy for <span
class="math inline">\(\theta\)</span>. 我们从样本中计算出最佳估计值
<span class="math inline">\(\hat{\theta}\)</span>。（例如，样本均值
<span class="math inline">\(\bar{x}\)</span>）。<span
class="math inline">\(\hat{\theta}\)</span> 是 <span
class="math inline">\(\theta\)</span> 的替代值。</p></li>
<li><p><strong>The Problem:</strong> We want to know the accuracy of
<span class="math inline">\(\hat{\theta}\)</span>. How much would <span
class="math inline">\(\hat{\theta}\)</span> vary if we could draw many
samples? We want the <em>sampling distribution</em> of <span
class="math inline">\(\hat{\theta}\)</span> around <span
class="math inline">\(\theta\)</span>, specifically the distribution of
the error: <span class="math inline">\((\hat{\theta} - \theta)\)</span>.
我们想知道 <span class="math inline">\(\hat{\theta}\)</span>
的准确率。如果我们可以抽取多个样本，<span
class="math inline">\(\hat{\theta}\)</span> 会有多少变化？我们想要 <span
class="math inline">\(\hat{\theta}\)</span> 围绕 <span
class="math inline">\(\theta\)</span> 的
<em>抽样分布</em>，具体来说是误差的分布：<span
class="math inline">\((\hat{\theta} - \theta)\)</span>。</p></li>
<li><p><strong>CLT:</strong> The Central Limit Theorem states that <span
class="math inline">\(\sqrt{n}(\hat{\theta} - \theta)
\xrightarrow{\text{dist}} N(0, Var_F(\theta))\)</span>.</p></li>
<li><p><strong>中心极限定理</strong>：<span
class="math inline">\(\sqrt{n}(\hat{\theta} - \theta)
\xrightarrow{\text{dist}} N(0, Var_F(\theta))\)</span>。</p></li>
<li><p><strong>The Catch:</strong> This is <strong>UNKNOWN</strong>
because we don’t know <span
class="math inline">\(F\)</span>.这是<strong>未知</strong>的，因为我们不知道
<span class="math inline">\(F\)</span>。</p></li>
</ul>
<h3 id="the-bootstrap-world-slide-87-bottom">The “Bootstrap” World
(Slide 87, Bottom)</h3>
<ul>
<li><strong>Population:</strong> We <em>pretend</em> our original sample
<em>is</em> the population. We call its distribution the “empirical
distribution,” <span class="math inline">\(\hat{F}_n\)</span>.
我们<em>假设</em>原始样本<em>就是</em>总体。我们称其分布为“经验分布”，即
<span class="math inline">\(\hat{F}_n\)</span>。</li>
<li><strong>Parameter:</strong> In this new world, the “true” parameter
is our original statistic, <span
class="math inline">\(\hat{\theta}\)</span> (which is a function of
<span class="math inline">\(\hat{F}_n\)</span>).
在这个新世界中，“真实”参数是我们原始的统计量 <span
class="math inline">\(\hat{\theta}\)</span>（它是 <span
class="math inline">\(\hat{F}_n\)</span> 的函数）。</li>
<li><strong>Sample:</strong> We draw <em>many</em> bootstrap samples
<span class="math inline">\(X_1^*, ..., X_n^*\)</span> <em>from <span
class="math inline">\(\hat{F}_n\)</span></em> (i.e., sampling <em>with
replacement</em> from our original sample). 我们从 <span
class="math inline">\(\hat{F}_n\)</span>* 中抽取 <em>许多</em> 自举样本
<span class="math inline">\(X_1^*, ...,
X_n^*\)</span>（即从原始样本中进行 <em>有放回</em> 抽样）。</li>
<li><strong>Statistic:</strong> From each bootstrap sample, we calculate
a <em>bootstrap statistic</em>, <span
class="math inline">\(\hat{\theta}^*\)</span>.
从每个自举样本中，我们计算一个 <em>自举统计量</em>，即 <span
class="math inline">\(\hat{\theta}^*\)</span>。</li>
<li><strong>The Solution:</strong> We can now <em>empirically</em> find
the distribution of <span class="math inline">\(\hat{\theta}^*\)</span>
around <span class="math inline">\(\hat{\theta}\)</span>. We look at the
distribution of the bootstrap error: <span
class="math inline">\((\hat{\theta}^* - \hat{\theta})\)</span>.
我们现在可以 <em>凭经验</em> 找到 <span
class="math inline">\(\hat{\theta}^*\)</span> 围绕 <span
class="math inline">\(\hat{\theta}\)</span>
的分布。我们来看看自举误差的分布：<span
class="math inline">\((\hat{\theta}^* - \hat{\theta})\)</span>。</li>
<li><strong>CLT:</strong> The CLT also states that <span
class="math inline">\(\sqrt{n}(\hat{\theta}^* - \hat{\theta})
\xrightarrow{\text{dist}} N(0, Var_{\hat{F}_n}(\theta))\)</span>.</li>
<li><strong>The Power:</strong> This distribution is
<strong>ESTIMABLE!</strong> We just run the bootstrap <span
class="math inline">\(B\)</span> times and we get <span
class="math inline">\(B\)</span> values of <span
class="math inline">\(\hat{\theta}^*\)</span>. We can then calculate
their variance, standard deviation, and percentiles directly.
这个分布是<strong>可估计的！</strong>我们只需运行 <span
class="math inline">\(B\)</span> 次自举程序，就能得到 <span
class="math inline">\(B\)</span> 个 <span
class="math inline">\(\hat{\theta}^*\)</span>
值。然后我们可以直接计算它们的方差、标准差和百分位数。</li>
</ul>
<h3 id="the-core-approximation-slide-88">The Core Approximation (Slide
88)</h3>
<p>The entire method relies on the assumption that <strong>the
(knowable) bootstrap distribution is a good approximation of the
(unknown) true sampling distribution.</strong>
整个方法依赖于以下假设：<strong>（已知的）自举分布能够很好地近似（未知的）真实抽样分布</strong>。</p>
<p>The distribution of the <em>bootstrap error</em> approximates the
distribution of the <em>true error</em>.
<em>自举误差</em>的分布近似于<em>真实误差</em>的分布。</p>
<p><span class="math display">\[\text{distribution of }
\sqrt{n}(\hat{\theta}^* - \hat{\theta}) \approx \text{distribution of }
\sqrt{n}(\hat{\theta} - \theta)\]</span></p>
<p>This is why: * The <strong>standard deviation</strong> of the <span
class="math inline">\(\hat{\theta}^*\)</span> values is our estimate for
the <strong>standard error</strong> of <span
class="math inline">\(\hat{\theta}\)</span>.
值的<strong>标准差</strong>是我们对 <span
class="math inline">\(\hat{\theta}\)</span>
的<strong>标准误差</strong>的估计值。 * The <strong>percentiles</strong>
of the <span class="math inline">\(\hat{\theta}^*\)</span> distribution
(e.g., 2.5th and 97.5th) can be used to build a <strong>confidence
interval</strong> for the true parameter <span
class="math inline">\(\theta\)</span>.
分布的<strong>百分位数</strong>（例如，第 2.5 个和第 97.5
个）可用于为真实参数 <span class="math inline">\(\theta\)</span>
建立<strong>置信区间</strong>。</p>
<h2 id="extension-cross-validation-cv-analysis">3. Extension:
Cross-Validation (CV) Analysis</h2>
<h3 id="cv-for-hyperparameter-tuning-slide-84-超参数调优的-cv">CV for
Hyperparameter Tuning (Slide 84) 超参数调优的 CV</h3>
<p>This plot is the <em>result</em> of the 10-fold CV code shown in the
previous set of slides (slide 82). * <strong>Purpose:</strong> To find
the optimal hyperparameter <span class="math inline">\(k\)</span>
(number of neighbors) for the KNN model. * <strong>X-axis:</strong>
Number of Neighbors (<span class="math inline">\(k\)</span>). *
<strong>Y-axis:</strong> CV Error (Mean Squared Error). *
<strong>Analysis:</strong> * <strong>Low <span
class="math inline">\(k\)</span> (e.g., <span class="math inline">\(k=1,
2\)</span>):</strong> High error. The model is too complex and
<strong>overfitting</strong> to the training data. * <strong>High <span
class="math inline">\(k\)</span> (e.g., <span
class="math inline">\(k&gt;40\)</span>):</strong> Error slowly
increases. The model is too simple and <strong>underfitting</strong>
(e.g., averaging too many neighbors). * <strong>Optimal <span
class="math inline">\(k\)</span>:</strong> The “sweet spot” is at the
bottom of the “U” shape, around <strong><span class="math inline">\(k
\approx 20-30\)</span></strong>, which gives the lowest CV error.</p>
<ul>
<li><strong>目的</strong>：为 KNN 模型找到最优超参数 <span
class="math inline">\(k\)</span>（邻居数）。</li>
<li><strong>X 轴：</strong>邻居数 (<span
class="math inline">\(k\)</span>)。</li>
<li><strong>Y 轴：</strong>CV 误差（均方误差）。</li>
<li><strong>分析</strong>：**</li>
<li><strong>低 <span class="math inline">\(k\)</span>（例如，<span
class="math inline">\(k=1,
2\)</span>）：</strong>误差较大。模型过于复杂，并且与训练数据<strong>过拟合</strong>。</li>
<li><strong>高 <span class="math inline">\(k\)</span>（例如，<span
class="math inline">\(k&gt;40\)</span>）：</strong>误差缓慢增加。模型过于简单且<strong>欠拟合</strong>（例如，对太多邻居进行平均）。</li>
<li><strong>最优 <span
class="math inline">\(k\)</span>：</strong>“最佳点”位于“U”形的底部，大约为<strong><span
class="math inline">\(k \approx 20-30\)</span></strong>，此时 CV
误差最低。</li>
</ul>
<h3 id="why-cv-over-estimates-test-error-slide-89">Why CV Over-Estimates
Test Error (Slide 89)</h3>
<p>This is a subtle but important theoretical point. * <strong>Our
Goal:</strong> We want to know the test error of our <em>final
model</em> (<span class="math inline">\(\hat{f}^{\text{full}}\)</span>),
which we will train on the <strong>full dataset</strong> (all <span
class="math inline">\(n\)</span> observations).
我们想知道<em>最终模型</em> (<span
class="math inline">\(\hat{f}^{\text{full}}\)</span>)
的测试误差，我们将在<strong>完整数据集</strong>（所有 <span
class="math inline">\(n\)</span> 个观测值）上训练该模型。 * <strong>What
CV Measures:</strong> <span class="math inline">\(k\)</span>-fold CV
does <em>not</em> test the final model. It tests <span
class="math inline">\(k\)</span> different models (<span
class="math inline">\(\hat{f}^{(k)}\)</span>), each trained on a
<em>smaller</em> dataset (of size <span
class="math inline">\(\frac{k-1}{k} \times n\)</span>). <span
class="math inline">\(k\)</span> 倍 CV <em>不</em>测试最终模型。它测试了
<span class="math inline">\(k\)</span> 个不同的模型 (<span
class="math inline">\(\hat{f}^{(k)}\)</span>)，每个模型都基于一个<em>较小</em>的数据集（大小为
<span class="math inline">\(\frac{k-1}{k} \times
n\)</span>）进行训练。</p>
<ul>
<li><strong>The Logic:</strong>
<ol type="1">
<li>Models trained on <em>less data</em> generally perform
<em>worse</em> than models trained on <em>more data</em>.
基于<em>较少数据</em>训练的模型通常比基于<em>较多数据</em>训练的模型表现<em>更差</em>。</li>
<li>The CV error is the average error of models trained on <span
class="math inline">\(\frac{k-1}{k} n\)</span> observations. CV
误差是使用 <span class="math inline">\(\frac{k-1}{k} n\)</span>
个观测值训练的模型的平均误差。</li>
<li>The “true test error” is the error of the model trained on <span
class="math inline">\(n\)</span> observations. “真实测试误差”是使用
<span class="math inline">\(n\)</span> 个观测值训练的模型的误差。</li>
</ol></li>
<li><strong>Conclusion:</strong> Since the CV models are trained on
smaller datasets, they will, on average, have a slightly higher error
than the final model. Therefore, <strong>the CV error score is a
slightly <em>pessimistic</em> estimate (it over-estimates) the true test
error of the final model.</strong> 由于 CV
模型是在较小的数据集上训练的，因此它们的平均误差会略高于最终模型。因此，<strong>CV
误差分数是一个略微<em>悲观</em>的估计（它高估了）最终模型的真实测试误差。</strong></li>
</ul>
<h3 id="correction-of-cv-error-slides-90-91">Correction of CV Error
(Slides 90-91)</h3>
<ul>
<li><p><strong>Theory (Slide 91):</strong> Advanced theory suggests the
expected test error <span class="math inline">\(R(n)\)</span> behaves
like <span class="math inline">\(R(n) = R^* + c/n\)</span>, where <span
class="math inline">\(R^*\)</span> is the irreducible error and <span
class="math inline">\(n\)</span> is the sample size. This formula
mathematically confirms that error <em>decreases</em> as sample size
<span class="math inline">\(n\)</span> <em>increases</em>.
高级理论表明，预期测试误差 <span class="math inline">\(R(n)\)</span>
的行为类似于 <span class="math inline">\(R(n) = R^* + c/n\)</span>，其中
<span class="math inline">\(R^*\)</span> 是不可约误差，<span
class="math inline">\(n\)</span>
是样本量。该公式从数学上证实了误差会随着样本量 <span
class="math inline">\(n\)</span> 的增加而<em>减小</em>。</p></li>
<li><p><strong>R Code (Slide 90):</strong> The <code>cv.glm</code>
function from the <code>boot</code> library automatically provides
this.</p>
<ul>
<li><code>cv.err$delta</code>: This output vector contains two
values.</li>
<li><code>[1] 24.23151</code> (Raw CV Error): This is the standard
Leave-One-Out CV (LOOCV) error.</li>
<li><code>[2] 24.23114</code> (Adjusted CV Error): This is a
bias-corrected estimate that accounts for the overestimation problem.
It’s slightly lower, representing a more accurate guess for the error of
the <em>final model</em> trained on all <span
class="math inline">\(n\)</span> data points.</li>
</ul></li>
</ul>
<p># The “Correction of CV Error” extension.</p>
<h3 id="summary">Summary</h3>
<p>This section provides a deeper mathematical look at <em>why</em>
k-fold cross-validation (CV) slightly <strong>over-estimates</strong>
the true test error. 本节从数学角度更深入地阐述了 <em>为什么</em> k
折交叉验证 (CV) 会略微<strong>高估</strong>真实测试误差。</p>
<ol type="1">
<li><p><strong>The Overestimation 高估:</strong> CV trains on <span
class="math inline">\(\frac{k-1}{k}\)</span> of the data, which is
<em>less</em> than the full dataset (size <span
class="math inline">\(n\)</span>). Models trained on less data are
generally <em>worse</em>. Therefore, the average error from CV (<span
class="math inline">\(CV_k\)</span>) is slightly <em>higher</em> (more
pessimistic) than the true error of the final model trained on all <span
class="math inline">\(n\)</span> data (<span
class="math inline">\(R(n)\)</span>). CV 训练的数据为 <span
class="math inline">\(\frac{k-1}{k}\)</span>，小于完整数据集（大小为
<span
class="math inline">\(n\)</span>）。使用较少数据训练的模型通常<em>更差</em>。因此，CV
的平均误差 (<span class="math inline">\(CV_k\)</span>)
略高于（更悲观地）基于所有 <span class="math inline">\(n\)</span>
个数据训练的最终模型的真实误差 (<span
class="math inline">\(R(n)\)</span>)。</p></li>
<li><p><strong>A Simple Correction 简单修正:</strong> A mathematical
formula, <span class="math inline">\(\tilde{CV_k} = \frac{k-1}{k} \cdot
CV_k\)</span>, is proposed to “correct” this overestimation.</p></li>
<li><p><strong>The Critical Flaw 关键缺陷:</strong> This correction is
derived <em>assuming</em> the <strong>irreducible error (<span
class="math inline">\(R^*\)</span>) is
zero</strong>.此修正是在<em>假设</em><strong>不可约误差 (<span
class="math inline">\(R^*\)</span>)
为零</strong>的情况下得出的。</p></li>
<li><p><strong>The Takeaway 要点 (Code Analysis):</strong> The Python
code demonstrates a real-world scenario where there is noise
(<code>noise_std = 0.5</code>), meaning <span class="math inline">\(R^*
&gt; 0\)</span>. In this case, the <strong>simple correction
fails</strong>—it produces an error (0.217) that is <em>less
accurate</em> and further from the true error (0.272) than the
<strong>original raw CV error</strong> (0.271).</p></li>
</ol>
<p>Python
代码演示了一个存在噪声（<code>noise_std = 0.5</code>）的真实场景，即
<span class="math inline">\(R^* &gt;
0\)</span>。在这种情况下，<strong>简单修正失败</strong>——它产生的误差
(0.217) <em>精度较低</em>，并且与真实误差 (0.272) 的距离比<strong>原始
CV 误差</strong> (0.271) 更远。</p>
<p><strong>Exam Conclusion:</strong> For most real-world problems (which
have noise), the <strong>raw <span class="math inline">\(k\)</span>-fold
CV error is a better and more reliable estimate</strong> of the true
test error than the simple (and flawed) correction.
对于大多数实际问题（包含噪声），<strong>原始 <span
class="math inline">\(k\)</span> 倍 CV
误差比简单（且有缺陷的）修正方法更能准确、可靠地估计真实测试误差</strong>。</p>
<h3 id="mathematical-understanding-1">Mathematical Understanding</h3>
<p>This section explains the theory of <em>why</em> <span
class="math inline">\(CV_k &gt; R(n)\)</span> and derives the simple
correction. 本节解释了为什么 <span class="math inline">\(CV_k &gt;
R(n)\)</span>，并推导出简单的修正方法。</p>
<ol type="1">
<li><p><strong>Assumed Error Behavior 假设误差行为:</strong> We assume
the test error <span class="math inline">\(R(n)\)</span> for a model
trained on <span class="math inline">\(n\)</span> data points behaves
like: 我们假设基于 <span class="math inline">\(n\)</span>
个数据点训练的模型的测试误差 <span class="math inline">\(R(n)\)</span>
的行为如下： <span class="math display">\[R(n) = R^* +
\frac{c}{n}\]</span></p>
<ul>
<li><span class="math inline">\(R^*\)</span>: The <strong>irreducible
error</strong> (the “noise floor” you can never beat).
<strong>不可约误差</strong>（即你永远无法克服的“本底噪声”）。</li>
<li><span class="math inline">\(c/n\)</span>: The model variance, which
<em>decreases</em> as sample size <span class="math inline">\(n\)</span>
<em>increases</em>. 模型方差，随着样本量 <span
class="math inline">\(n\)</span> 的增加而减小。</li>
</ul></li>
<li><p><strong>Test Error vs. CV Error 测试误差 vs. CV
误差:</strong></p>
<ul>
<li><strong>Test Error of Interest:</strong> This is the error of our
<em>final model</em> trained on all <span
class="math inline">\(n\)</span> points: <span
class="math display">\[R(n) = R^* + \frac{c}{n}\]</span></li>
<li><strong>感兴趣的测试误差</strong>：这是我们在所有 <span
class="math inline">\(n\)</span>
个点上训练的<em>最终模型</em>的误差：</li>
<li><strong>k-fold CV Error:</strong> This is the average error of <span
class="math inline">\(k\)</span> models, each trained on a smaller
sample of size <span class="math inline">\(n&#39; =
(\frac{k-1}{k})n\)</span>.</li>
<li><strong>k 倍 CV 误差</strong>：这是 <span
class="math inline">\(k\)</span>
个模型的平均误差，每个模型都使用一个较小的样本（大小为 <span
class="math inline">\(n&#39; = (\frac{k-1}{k})n\)</span>）进行训练。
<span class="math display">\[CV_k \approx R(n&#39;) =
R\left(\frac{k-1}{k}n\right) = R^* +
\frac{c}{\left(\frac{k-1}{k}\right)n} = R^* +
\frac{ck}{(k-1)n}\]</span></li>
</ul></li>
<li><p><strong>The Overestimation 高估:</strong> Let’s compare <span
class="math inline">\(CV_k\)</span> and <span
class="math inline">\(R(n)\)</span>: <span class="math display">\[CV_k
\approx R^* + \left(\frac{k}{k-1}\right) \frac{c}{n}\]</span> <span
class="math display">\[R(n) = R^* + \left(\frac{k-1}{k-1}\right)
\frac{c}{n}\]</span> Since <span class="math inline">\(k &gt;
(k-1)\)</span>, the factor <span
class="math inline">\(\left(\frac{k}{k-1}\right)\)</span> is
<strong>greater than 1</strong>. This means the <span
class="math inline">\(CV_k\)</span> error term is larger than the <span
class="math inline">\(R(n)\)</span> error term. Thus: <strong><span
class="math inline">\(CV_k &gt; \text{Test error of interest }
R(n)\)</span></strong> 由于 <span class="math inline">\(k &gt;
(k-1)\)</span>，因子 <span
class="math inline">\(\left(\frac{k}{k-1}\right)\)</span> <strong>大于
1</strong>。这意味着 <span class="math inline">\(CV_k\)</span>
误差项大于 <span class="math inline">\(R(n)\)</span> 误差项。因此：
<strong><span class="math inline">\(CV_k &gt; \text{目标测试误差 }
R(n)\)</span></strong></p></li>
<li><p><strong>Deriving the (Flawed) Correction
推导（有缺陷的）修正:</strong> This correction makes a <strong>strong
assumption: <span class="math inline">\(R^* \approx 0\)</span></strong>
(the model is perfectly specified, and there is no noise).
此修正基于一个<strong>强假设：<span class="math inline">\(R^* \approx
0\)</span></strong>（模型完全正确，且无噪声）。</p>
<ul>
<li>If <span class="math inline">\(R^* = 0\)</span>, then <span
class="math inline">\(R(n) \approx \frac{c}{n}\)</span></li>
<li>If <span class="math inline">\(R^* = 0\)</span>, then <span
class="math inline">\(CV_k \approx \frac{ck}{(k-1)n}\)</span></li>
</ul>
<p>Now, look at the ratio between them: <span
class="math display">\[\frac{R(n)}{CV_k} \approx \frac{c/n}{ck/((k-1)n)}
= \frac{c}{n} \cdot \frac{(k-1)n}{ck} = \frac{k-1}{k}\]</span></p>
<p>This gives us the correction formula by isolating <span
class="math inline">\(R(n)\)</span>: 通过分离 <span
class="math inline">\(R(n)\)</span>，我们得到了校正公式： <span
class="math display">\[R(n) \approx \left(\frac{k-1}{k}\right) \cdot
CV_k\]</span> This corrected version is denoted <span
class="math inline">\(\tilde{CV_k}\)</span>.这个校正版本表示为 <span
class="math inline">\(\tilde{CV_k}\)</span>。</p></li>
</ol>
<h3 id="code-analysis-slides-92-93">Code Analysis (Slides 92-93)</h3>
<p>The Python code is an experiment designed to <strong>test the
correction formula</strong>.</p>
<ul>
<li><p><strong>Goal:</strong> Compare the “Raw CV Error” (<span
class="math inline">\(CV_k\)</span>), the “Corrected CV Error” (<span
class="math inline">\(\tilde{CV_k}\)</span>), and the “True Test Error”
(<span class="math inline">\(R(n)\)</span>) in a realistic
setting.</p></li>
<li><p><strong>Key Setup:</strong></p>
<ol type="1">
<li><code>def f(x)</code>: Defines the true, underlying function <span
class="math inline">\(y = x^2 + 15\sin(x)\)</span>.</li>
<li><code>noise_std = 0.5</code>: <strong>This is the most important
line.</strong> It adds significant random noise to the data. This
ensures that the <strong>irreducible error <span
class="math inline">\(R^*\)</span> is large and <span
class="math inline">\(R^* &gt; 0\)</span></strong>.</li>
<li><code>y = f(...) + np.random.normal(...)</code>: Creates the noisy
training data (the blue dots).</li>
</ol></li>
<li><p><strong>CV Calculation (Standard K-Fold):</strong></p>
<ul>
<li><code>kf = KFold(...)</code>: Sets up 5-fold CV (<span
class="math inline">\(k=5\)</span>).</li>
<li><code>for train_index, val_index in kf.split(x):</code>: This is the
standard loop. It trains on 4 folds and validates on 1 fold.</li>
<li><code>cv_error = np.mean(cv_mse_list)</code>: Calculates the
<strong>raw <span class="math inline">\(CV_5\)</span> error</strong>.
This is the first result (e.g., <strong>0.2715</strong>).</li>
</ul></li>
<li><p><strong>Correction Calculation:</strong></p>
<ul>
<li><code>correction_factor = (k_splits - 1) / k_splits</code>: This is
<span class="math inline">\(\frac{k-1}{k}\)</span>, which is <span
class="math inline">\(4/5 = 0.8\)</span>.</li>
<li><code>corrected_cv_error = correction_factor * cv_error</code>: This
applies the flawed formula from the math section (<span
class="math inline">\(0.2715 \times 0.8\)</span>). This is the second
result (e.g., <strong>0.2172</strong>).</li>
</ul></li>
<li><p><strong>“True” Test Error Calculation:</strong></p>
<ul>
<li><code>knn.fit(x, y)</code>: Trains the <em>final model</em> on the
<em>entire</em> noisy dataset.</li>
<li><code>n_test = 1000</code>: Creates a <em>new, large</em> test set
to estimate the true error.</li>
<li><code>true_test_error = mean_squared_error(...)</code>: Calculates
the error of the final model on this new test set. This is our best
estimate of <span class="math inline">\(R(n)\)</span> (e.g.,
<strong>0.2725</strong>).</li>
</ul></li>
<li><p><strong>Analysis of Results (Slide 93):</strong></p>
<ul>
<li><strong>Raw 5-Fold CV MSE:</strong> 0.2715</li>
<li><strong>True test error:</strong> 0.2725</li>
<li><strong>Corrected 5-Fold CV MSE:</strong> 0.2172</li>
</ul>
<p>The <strong>Raw CV Error (0.2715) is an excellent estimate</strong>
of the True Test Error (0.2725). The <strong>Corrected Error (0.2172) is
much worse</strong>. This experiment <em>proves</em> that when noise
(<span class="math inline">\(R^*\)</span>) is present, the simple
correction formula should not be used.</p></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/01/5054C4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/01/5054C4/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-10-01 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-01T21:00:00+08:00">2025-10-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-10-18 23:00:24" itemprop="dateModified" datetime="2025-10-18T23:00:24+08:00">2025-10-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>统计机器学习Lecture-4</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="what-is-classification">1. What is Classification?</h1>
<p>Classification is a type of <strong>supervised machine
learning</strong> where the goal is to predict a
<strong>categorical</strong> or qualitative response. Unlike regression
where you predict a continuous numerical value (like a price or
temperature), classification assigns an input to a specific category or
class.
分类是一种<strong>监督式机器学习</strong>，其目标是预测<strong>分类</strong>或定性响应。与预测连续数值（例如价格或温度）的回归不同，分类将输入分配到特定的类别或类别。</p>
<p><strong>Key characteristics:</strong></p>
<ul>
<li><p><strong>Goal:</strong> Predict the class of a subject based on
input features.</p></li>
<li><p><strong>Output (Response):</strong> The output is a category,
such as ‘Yes’/‘No’, ‘Spam’/‘Not Spam’, or
‘High’/‘Medium’/‘Low’.</p></li>
<li><p><strong>Applications:</strong> Common examples include email spam
detectors, medical diagnosis (e.g., virus carrier vs. non-carrier), and
fraud detection.</p>
<ul>
<li><strong>目标</strong>：根据输入特征预测主题的类别。</li>
<li><strong>输出（响应）：</strong>输出是一个类别，例如“是”/“否”、“垃圾邮件”/“非垃圾邮件”或“高”/“中”/“低”。</li>
<li><strong>应用</strong>：常见示例包括垃圾邮件检测器、医学诊断（例如，病毒携带者与非病毒携带者）和欺诈检测。
The example used in the slides is a credit card <strong>Default
dataset</strong>. The goal is to predict whether a customer will
<strong>default</strong> (‘Yes’ or ‘No’) on their payments based on
their monthly <strong>income</strong> and account
<strong>balance</strong>.</li>
</ul></li>
</ul>
<p>## Why Not Use Linear Regression?为什么不使用线性回归？</p>
<p>At first, it might seem possible to use linear regression for
classification. For a binary (two-class) problem like the default
dataset, you could code the outcomes as numbers, for example:</p>
<ul>
<li>Default = ‘No’ =&gt; <span class="math inline">\(y = 0\)</span></li>
<li>Default = ‘Yes’ =&gt; <span class="math inline">\(y =
1\)</span></li>
</ul>
<p>You could then fit a standard linear regression model: <span
class="math inline">\(Y \approx \beta_0 + \beta_1 X\)</span>. In this
context, we would interpret the prediction <span
class="math inline">\(\hat{y}\)</span> as the <em>probability</em> of
default, so we’d be modeling <span class="math inline">\(P(Y=1|X) =
\beta_0 + \beta_1 X\)</span>.</p>
<p>However, this approach has two major problems:
然而，这种方法有两个主要问题： <strong>1. The Output Is Not a
Probability</strong> A linear model can produce outputs that are less
than 0 or greater than 1. This doesn’t make sense for a probability,
which must always be between 0 and 1.</p>
<p>The image below is the most important one for understanding this
issue. The left plot shows a linear regression line fit to the 0/1
default data. You can see the line goes below 0 and would eventually go
above 1 for higher balances. The right plot shows a logistic regression
curve, which always stays between 0 and 1.</p>
<ul>
<li><strong>Left (Linear Regression):</strong> The straight blue line
predicts probabilities &lt; 0 for low balances.</li>
<li><strong>Right (Logistic Regression):</strong> The S-shaped blue
curve correctly constrains the probability output between 0 and 1.</li>
</ul>
<p><strong>2. It Doesn’t Work for Multi-Class Problems</strong> If you
have more than two categories (e.g., ‘mild’, ‘moderate’, ‘severe’), you
might code them as 0, 1, and 2. A linear regression model would
incorrectly assume that the “distance” between ‘mild’ and ‘moderate’ is
the same as the distance between ‘moderate’ and ‘severe’, which is
usually not a valid assumption.</p>
<p><strong>1. 输出不是概率</strong> 线性模型可以产生小于 0 或大于 1
的输出。这对于概率来说毫无意义，因为概率必须始终介于 0 和 1 之间。</p>
<p>下图是理解这个问题最重要的图。左图显示了与 0/1
默认数据拟合的线性回归线。您可以看到，该线低于
0，并且最终会随着余额的增加而高于
1。右图显示了逻辑回归曲线，它始终保持在 0 和 1 之间。</p>
<ul>
<li><strong>左图（线性回归）：</strong>蓝色直线预测低余额的概率小于
0。</li>
<li><strong>右图（逻辑回归）：</strong>S
形蓝色曲线正确地将概率输出限制在 0 和 1 之间。</li>
</ul>
<p><strong>2.它不适用于多类别问题</strong>
如果您有两个以上的类别（例如，“轻度”、“中度”、“重度”），您可能会将它们编码为
0、1 和
2。线性回归模型会错误地假设“轻度”和“中度”之间的“距离”与“中度”和“重度”之间的距离相同，这通常不是一个有效的假设。</p>
<p>## The Solution: Logistic Regression</p>
<p>Instead of modeling the response <span
class="math inline">\(y\)</span> directly, logistic regression models
the <strong>probability</strong> that <span
class="math inline">\(y\)</span> belongs to a particular class. To solve
the issue of the output not being a probability, it uses the
<strong>logistic function</strong> (also known as the sigmoid
function).</p>
<p>This function takes any real-valued input and squeezes it into an
output between 0 and 1.</p>
<p>The formula for the probability in a logistic regression model is:
<span class="math display">\[P(Y=1|X) = \frac{e^{\beta_0 + \beta_1 X}}{1
+ e^{\beta_0 + \beta_1 X}}\]</span> This S-shaped function, shown in the
right-hand plot above, ensures that the output is always a valid
probability. We can then set a threshold (e.g., 0.5) to make the final
class prediction. If <span class="math inline">\(P(Y=1|X) &gt;
0.5\)</span>, we predict ‘Yes’; otherwise, we predict ‘No’.</p>
<p>## 解决方案：逻辑回归</p>
<p>逻辑回归不是直接对响应 <span class="math inline">\(y\)</span>
进行建模，而是对 <span class="math inline">\(y\)</span>
属于特定类别的<strong>概率</strong>进行建模。为了解决输出不是概率的问题，它使用了<strong>逻辑函数</strong>（也称为
S 型函数）。</p>
<p>此函数接受任何实值输入，并将其压缩为介于 0 和 1 之间的输出。</p>
<p>逻辑回归模型中的概率公式为： <span class="math display">\[P(Y=1|X) =
\frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}\]</span>
如上图右侧所示，这个 S
形函数确保输出始终是有效概率。然后，我们可以设置一个阈值（例如
0.5）来进行最终的类别预测。如果 <span class="math inline">\(P(Y=1|X)
&gt; 0.5\)</span>，则预测“是”；否则，预测“否”。</p>
<p>## Data Visualization &amp; Code in Python</p>
<p>The slides use R to visualize the data. The boxplots are particularly
important because they show which variable is a better predictor.</p>
<ul>
<li><p><strong>Balance vs. Default:</strong> The boxplots for balance
show a clear difference. The median balance for those who default
(‘Yes’) is much higher than for those who do not (‘No’). This suggests
<strong>balance is a strong predictor</strong>.</p></li>
<li><p><strong>Income vs. Default:</strong> The boxplots for income show
a lot of overlap. The median incomes for both groups are very similar.
This suggests <strong>income is a weak predictor</strong>.</p></li>
<li><p><strong>余额
vs. 违约</strong>：余额的箱线图显示出明显的差异。违约者（“是”）的余额中位数远高于未违约者（“否”）。这表明<strong>余额是一个强有力的预测指标</strong>。</p></li>
<li><p><strong>收入
vs. 违约</strong>：收入的箱线图显示出很大的重叠。两组的收入中位数非常相似。这表明<strong>收入是一个弱的预测指标</strong>。</p></li>
</ul>
<p>Here’s how you could perform similar analysis and modeling in Python
using <code>seaborn</code> and <code>scikit-learn</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;default_data.csv&#x27; has columns: &#x27;default&#x27; (Yes/No), &#x27;balance&#x27;, &#x27;income&#x27;</span></span><br><span class="line"><span class="comment"># You would load your data like this:</span></span><br><span class="line"><span class="comment"># df = pd.read_csv(&#x27;default_data.csv&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># For demonstration, let&#x27;s create some sample data</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;balance&#x27;</span>: [<span class="number">1200</span>, <span class="number">2100</span>, <span class="number">800</span>, <span class="number">1800</span>, <span class="number">500</span>, <span class="number">1600</span>, <span class="number">2200</span>, <span class="number">1900</span>],</span><br><span class="line">    <span class="string">&#x27;income&#x27;</span>: [<span class="number">45000</span>, <span class="number">60000</span>, <span class="number">30000</span>, <span class="number">55000</span>, <span class="number">25000</span>, <span class="number">48000</span>, <span class="number">70000</span>, <span class="number">65000</span>],</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>: [<span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. Data Visualization (like the slides) ---</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">14</span>, <span class="number">5</span>))</span><br><span class="line">fig.suptitle(<span class="string">&#x27;Predictor Analysis for Default&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boxplot for Balance</span></span><br><span class="line">sns.boxplot(ax=axes[<span class="number">0</span>], x=<span class="string">&#x27;default&#x27;</span>, y=<span class="string">&#x27;balance&#x27;</span>, data=df)</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&#x27;Balance vs. Default Status&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boxplot for Income</span></span><br><span class="line">sns.boxplot(ax=axes[<span class="number">1</span>], x=<span class="string">&#x27;default&#x27;</span>, y=<span class="string">&#x27;income&#x27;</span>, data=df)</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">&#x27;Income vs. Default Status&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. Logistic Regression Modeling ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert categorical &#x27;default&#x27; column to 0s and 1s</span></span><br><span class="line">df[<span class="string">&#x27;default_encoded&#x27;</span>] = df[<span class="string">&#x27;default&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x == <span class="string">&#x27;Yes&#x27;</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define features (X) and target (y)</span></span><br><span class="line">X = df[[<span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line">y = df[<span class="string">&#x27;default_encoded&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create and train the logistic regression model</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on new data</span></span><br><span class="line"><span class="comment"># For example, a person with a $2000 balance and $50,000 income</span></span><br><span class="line">new_customer = [[<span class="number">2000</span>, <span class="number">50000</span>]]</span><br><span class="line">predicted_prob = model.predict_proba(new_customer)</span><br><span class="line">prediction = model.predict(new_customer)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Customer data: Balance=2000, Income=50000&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Probability of No Default vs. Default: <span class="subst">&#123;predicted_prob&#125;</span>&quot;</span>) <span class="comment"># [[P(No), P(Yes)]]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Final Prediction (0=No, 1=Yes): <span class="subst">&#123;prediction&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="the-mathematical-foundation-of-logistic-regression">2. the
mathematical foundation of logistic regression</h1>
<p>This set of slides explains the mathematical foundation of logistic
regression, how its parameters are estimated using Maximum Likelihood
Estimation (MLE), and how an iterative algorithm called Newton-Raphson
is used to perform this estimation.</p>
<p>逻辑回归的数学基础、如何使用最大似然估计 (MLE)
估计其参数，以及如何使用名为 Newton-Raphson 的迭代算法进行估计。</p>
<h2
id="the-logistic-regression-model-from-probabilities-to-log-odds逻辑回归模型从概率到对数几率">2.1
The Logistic Regression Model: From Probabilities to
Log-Odds逻辑回归模型：从概率到对数几率</h2>
<p>The core of logistic regression is transforming a linear model into a
valid probability. This is done using the <strong>logistic
function</strong>, also known as the sigmoid function.
逻辑回归的核心是将线性模型转换为有效的概率。这可以通过<strong>逻辑函数</strong>（也称为
S 型函数）来实现。 #### <strong>Key Mathematical Formulas</strong></p>
<ol type="1">
<li><p><strong>Probability of Class 1:</strong> The model assumes the
probability of an observation <span
class="math inline">\(\mathbf{x}\)</span> belonging to class 1 is given
by the sigmoid function: <span class="math display">\[
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-\beta^T \mathbf{x})} =
\frac{\exp(\beta^T \mathbf{x})}{1 + \exp(\beta^T \mathbf{x})}
\]</span> This function always outputs a value between 0 and 1, making
it perfect for modeling probabilities.</p></li>
<li><p><strong>Odds:</strong> The odds are the ratio of the probability
of an event happening to the probability of it not happening. <span
class="math display">\[
\text{Odds} = \frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})} = \exp(\beta^T
\mathbf{x})
\]</span></p></li>
<li><p><strong>Log-Odds (Logit):</strong> By taking the natural
logarithm of the odds, we get a linear relationship with the predictors.
This is called the <strong>logit transformation</strong>. <span
class="math display">\[
\text{logit}(P(y=1|\mathbf{x})) =
\log\left(\frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})}\right) = \beta^T
\mathbf{x}
\]</span> This final equation is the heart of the model. It states that
the log-odds of the outcome are a linear function of the predictors.
This provides a great interpretation: a one-unit increase in a predictor
<span class="math inline">\(x_j\)</span> changes the log-odds by <span
class="math inline">\(\beta_j\)</span>.</p></li>
<li><p><strong>类别 1 的概率</strong>：该模型假设观测值 <span
class="math inline">\(\mathbf{x}\)</span> 属于类别 1 的概率由 S
型函数给出： <span class="math display">\[
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-\beta^T \mathbf{x})} =
\frac{\exp(\beta^T \mathbf{x})}{1 + \exp(\beta^T \mathbf{x})}
\]</span> 此函数的输出值始终介于 0 和 1
之间，非常适合用于概率建模。</p></li>
<li><p><strong>几率</strong>：**几率是事件发生的概率与不发生的概率之比。
<span class="math display">\[
\text{Odds} = \frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})} = \exp(\beta^T
\mathbf{x})
\]</span></p></li>
<li><p><strong>对数概率
(Logit)</strong>：通过对概率取自然对数，我们可以得到概率与预测变量之间的线性关系。这被称为<strong>logit
变换</strong>。 <span class="math display">\[
\text{logit}(P(y=1|\mathbf{x})) =
\log\left(\frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})}\right) = \beta^T
\mathbf{x}
\]</span>
最后一个方程是模型的核心。它指出结果的对数概率是预测变量的线性函数。这提供了一个很好的解释：预测变量
<span class="math inline">\(x_j\)</span>
每增加一个单位，对数概率就会改变 <span
class="math inline">\(\beta_j\)</span>。</p></li>
</ol>
<h2
id="fitting-the-model-maximum-likelihood-estimation-mle-拟合模型最大似然估计-mle">2.2
Fitting the Model: Maximum Likelihood Estimation (MLE)
拟合模型：最大似然估计 (MLE)</h2>
<p>Unlike linear regression, which uses least squares to find the
best-fit line, logistic regression uses <strong>Maximum Likelihood
Estimation (MLE)</strong>. The goal of MLE is to find the parameter
values (the <span class="math inline">\(\beta\)</span> coefficients)
that maximize the probability of observing the actual data that we have.
与使用最小二乘法寻找最佳拟合线的线性回归不同，逻辑回归使用<strong>最大似然估计
(MLE)</strong>。MLE
的目标是找到使观测到实际数据的概率最大化的参数值（<span
class="math inline">\(\beta\)</span> 系数）。</p>
<ol type="1">
<li><p><strong>Likelihood Function:</strong> This is the joint
probability of observing all the data points in our sample. Assuming
each observation is independent, it’s the product of the individual
probabilities:
1.<strong>似然函数</strong>：这是观测到样本中所有数据点的联合概率。假设每个观测值都是独立的，它是各个概率的乘积：
<span class="math display">\[
L(\beta) = \prod_{i=1}^{n} P(y_i|\mathbf{x}_i)
\]</span> A clever way to write this for a binary (0/1) outcome is:
<span class="math display">\[
L(\beta) = \prod_{i=1}^{n} \frac{\exp(y_i \beta^T \mathbf{x}_i)}{1 +
\exp(\beta^T \mathbf{x}_i)}
\]</span></p></li>
<li><p><strong>Log-Likelihood Function:</strong> Products are difficult
to work with mathematically, so we work with the logarithm of the
likelihood, which turns the product into a sum. Maximizing the
log-likelihood is the same as maximizing the likelihood.</p></li>
<li><p><strong>对数似然函数</strong>：乘积在数学上很难处理，所以我们使用似然的对数，将乘积转化为和。最大化对数似然与最大化似然相同。
<span class="math display">\[
\ell(\beta) = \log(L(\beta)) = \sum_{i=1}^{n} \left[ y_i \beta^T
\mathbf{x}_i - \log(1 + \exp(\beta^T \mathbf{x}_i)) \right]
\]</span> <strong>Key Takeaway:</strong> The slides correctly state that
there is <strong>no explicit formula</strong> to solve for the <span
class="math inline">\(\hat{\beta}\)</span> that maximizes this function.
We must find it using a numerical optimization algorithm.
没有<strong>明确的公式</strong>来求解最大化该函数的<span
class="math inline">\(\hat{\beta}\)</span>。我们必须使用数值优化算法来找到它。</p></li>
</ol>
<h2 id="the-algorithm-newton-raphson-算法牛顿-拉夫森算法">2.3 The
Algorithm: Newton-Raphson 算法：牛顿-拉夫森算法</h2>
<p>The slides introduce the <strong>Newton-Raphson algorithm</strong> as
the method to find the optimal <span
class="math inline">\(\hat{\beta}\)</span>. It’s an efficient iterative
algorithm for finding the roots of a function (i.e., where <span
class="math inline">\(f(x)=0\)</span>).</p>
<p><strong>How does this apply to logistic regression?</strong> To
maximize the log-likelihood function <span
class="math inline">\(\ell(\beta)\)</span>, we need to find the point
where its derivative (gradient) is equal to zero. So, Newton-Raphson is
used to solve <span class="math inline">\(\frac{d\ell(\beta)}{d\beta} =
0\)</span>.</p>
<p>它是一种高效的迭代算法，用于求函数的根（即，当<span
class="math inline">\(f(x)=0\)</span>时）。</p>
<p><strong>这如何应用于逻辑回归？</strong> 为了最大化对数似然函数 <span
class="math inline">\(\ell(\beta)\)</span>，我们需要找到其导数（梯度）等于零的点。因此，牛顿-拉夫森法用于求解
<span class="math inline">\(\frac{d\ell(\beta)}{d\beta} =
0\)</span>。</p>
<h4 id="the-general-newton-raphson-method"><strong>The General
Newton-Raphson Method</strong></h4>
<p>The algorithm starts with an initial guess, <span
class="math inline">\(x^{old}\)</span>, and iteratively refines it using
the following update rule, which is based on a Taylor series
approximation: <span class="math display">\[
x^{new} = x^{old} - \frac{f(x^{old})}{f&#39;(x^{old})}
\]</span> where <span class="math inline">\(f&#39;(x)\)</span> is the
derivative of <span class="math inline">\(f(x)\)</span>. You repeat this
step until the value of <span class="math inline">\(x\)</span>
converges.</p>
<p>该算法从初始估计 <span class="math inline">\(x^{old}\)</span>
开始，并使用以下基于泰勒级数近似的更新规则迭代地对其进行优化： <span
class="math display">\[
x^{new} = x^{old} - \frac{f(x^{old})}{f&#39;(x^{old})}
\]</span> 其中 <span class="math inline">\(f&#39;(x)\)</span> 是 <span
class="math inline">\(f(x)\)</span> 的导数。重复此步骤，直到 <span
class="math inline">\(x\)</span> 的值收敛。</p>
<h4
id="important-image-newton-raphson-example-x3---4-0"><strong>Important
Image: Newton-Raphson Example (<span class="math inline">\(x^3 - 4 =
0\)</span>)</strong></h4>
<p>[Image showing iterations of Newton-Raphson]</p>
<p>This slide is a great illustration of the algorithm’s power. *
<strong>Goal:</strong> Find <span class="math inline">\(x\)</span> such
that <span class="math inline">\(f(x) = x^3 - 4 = 0\)</span>. *
<strong>Function:</strong> <span class="math inline">\(f(x) = x^3 -
4\)</span> * <strong>Derivative:</strong> <span
class="math inline">\(f&#39;(x) = 3x^2\)</span> * <strong>Update
Rule:</strong> <span class="math inline">\(x^{new} = x^{old} -
\frac{(x^{old})^3 - 4}{3(x^{old})^2}\)</span> Starting with a guess of
<span class="math inline">\(x^{old} = 2\)</span>, the algorithm
converges to the true answer (<span class="math inline">\(4^{1/3}
\approx 1.5874\)</span>) in just 4 steps.</p>
<ul>
<li><strong>目标</strong>：找到 <span
class="math inline">\(x\)</span>，使得 <span class="math inline">\(f(x)
= x^3 - 4 = 0\)</span>。</li>
<li><strong>函数</strong>：<span class="math inline">\(f(x) = x^3 -
4\)</span></li>
<li><strong>导数</strong>：<span class="math inline">\(f&#39;(x) =
3x^2\)</span></li>
<li><strong>更新规则</strong>：<span class="math inline">\(x^{new} =
x^{old} - \frac{(x^{old})^3 - 4}{3(x^{old})^2}\)</span> 从 <span
class="math inline">\(x^{old} = 2\)</span> 的猜测开始，该算法仅用 4
步就收敛到真实答案 (<span class="math inline">\(4^{1/3} \approx
1.5874\)</span>)。</li>
</ul>
<h4 id="code-understanding-python"><strong>Code Understanding
(Python)</strong></h4>
<p>The slides show Python code implementing Newton-Raphson. Let’s break
down the key function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the function we want to find the root of</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) - x*x + <span class="number">3</span> * np.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define its derivative</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_prime</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) - <span class="number">2</span>*x + <span class="number">3</span> * np.cos(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Newton-Raphson method</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">newton_raphson</span>(<span class="params">x0, tol=<span class="number">1e-10</span>, max_iter=<span class="number">100</span></span>):</span><br><span class="line">    x = x0 <span class="comment"># Start with the initial guess</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        fx = f(x)      <span class="comment"># Calculate f(x_old)</span></span><br><span class="line">        fpx = f_prime(x) <span class="comment"># Calculate f&#x27;(x_old)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> fpx == <span class="number">0</span>: <span class="comment"># Cannot divide by zero</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Zero derivative. No solution found.&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is the core update rule</span></span><br><span class="line">        x_new = x - fx / fpx</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check if the change is small enough to stop</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(x_new - x) &lt; tol:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Converged to <span class="subst">&#123;x_new&#125;</span> after <span class="subst">&#123;i+<span class="number">1</span>&#125;</span> iterations.&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> x_new</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update x for the next iteration</span></span><br><span class="line">        x = x_new</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Exceeded maximum iterations. No solution found.&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial guess and execution</span></span><br><span class="line">x0 = <span class="number">0.5</span></span><br><span class="line">root = newton_raphson(x0)</span><br></pre></td></tr></table></figure>
<p>The slides show that with a good initial guess
(<code>x0 = 0.5</code>), the algorithm converges quickly. With a bad one
(<code>x0 = 50</code>), it still converges but takes many more steps.
This highlights the importance of the starting point. The slides also
show an implementation of <strong>Gradient Descent</strong>, another
popular optimization algorithm which uses the update rule
<code>x_new = x - learning_rate * gradient</code>.</p>
<h1
id="provide-a-great-case-study-on-logistic-regression-particularly-on-the-important-concept-of-confounding-variables.-heres-a-summary-covering-the-math-code-and-key-insights.">Provide
a great case study on logistic regression, particularly on the important
concept of confounding variables. Here’s a summary covering the math,
code, and key insights.</h1>
<ol start="3" type="1">
<li><h1 id="core-concept-logistic-regression-核心概念逻辑回归">Core
Concept: Logistic Regression 📈 # 核心概念：逻辑回归 📈</h1></li>
</ol>
<p>Logistic regression is a statistical method used for <strong>binary
classification</strong>, which means predicting an outcome that can only
be one of two things (e.g., Yes/No, True/False, 1/0).</p>
<p>In this example, the goal is to predict the probability that a
customer will <strong>default</strong> on a loan (Yes or No) based on
factors like their account <code>balance</code>, <code>income</code>,
and whether they are a <code>student</code>.</p>
<p>The core of logistic regression is the <strong>sigmoid (or logistic)
function</strong>, which takes any real-valued number and squishes it to
a value between 0 and 1, representing a probability.</p>
<p><span class="math display">\[
\hat{P}(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_p
X_p)}}
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{P}(Y=1|X)\)</span> is the predicted
probability of the outcome being “Yes” (e.g., default).</li>
<li><span class="math inline">\(\beta_0\)</span> is the intercept.</li>
<li><span class="math inline">\(\beta_1, ..., \beta_p\)</span> are the
coefficients for each input variable (<span class="math inline">\(X_1,
..., X_p\)</span>). The model’s job is to find the best values for these
<span class="math inline">\(\beta\)</span> coefficients.</li>
</ul>
<hr />
<p>逻辑回归是一种用于<strong>二元分类</strong>的统计方法，这意味着预测结果只能是两种情况之一（例如，是/否、真/假、1/0）。</p>
<p>在本例中，目标是根据客户账户“余额”、“收入”以及是否为“学生”等因素，预测客户<strong>拖欠</strong>贷款（是或否）的概率。</p>
<p>逻辑回归的核心是<strong>Sigmoid（或逻辑）函数</strong>，它将任何实数压缩为介于
0 和 1 之间的值，以表示概率。</p>
<p><span class="math display">\[
\hat{P}(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_p
X_p)}}
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{P}(Y=1|X)\)</span>
是结果为“是”（例如，默认）的预测概率。</li>
<li><span class="math inline">\(\beta_0\)</span> 是截距。</li>
<li><span class="math inline">\(\beta_1, ..., \beta_p\)</span>
是每个输入变量 (<span class="math inline">\(X_1, ..., X_p\)</span>)
的系数。模型的任务是找到这些 <span class="math inline">\(\beta\)</span>
系数的最佳值。</li>
</ul>
<h2 id="how-the-model-learns-mathematical-foundation">3.1 How the Model
“Learns” (Mathematical Foundation)</h2>
<p>The slides show that the model’s coefficients (<span
class="math inline">\(\beta\)</span>) are found using an algorithm like
<strong>Newton-Raphson</strong>. This is an iterative process to find
the values that <strong>maximize the log-likelihood function</strong>.
Think of this as finding the coefficient values that make the observed
data most
probable.这是一个迭代过程，用于查找<strong>最大化对数似然函数</strong>的值。可以将其视为查找使观测数据概率最大的系数值。</p>
<p>The key slide for this is the one titled “Newton-Raphson Iterative
Algorithm”. It shows the formulas for: * The <strong>Gradient</strong>
(<span class="math inline">\(\nabla\ell\)</span>): The direction of the
steepest ascent of the log-likelihood function. * The
<strong>Hessian</strong> (<span class="math inline">\(H\)</span>): The
curvature of the log-likelihood function.</p>
<ul>
<li><strong>梯度</strong> (<span
class="math inline">\(\nabla\ell\)</span>)：对数似然函数最陡上升的方向。</li>
<li><strong>黑森矩阵</strong> (<span
class="math inline">\(H\)</span>)：对数似然函数的曲率。</li>
</ul>
<p>The updating rule is given by: <span class="math display">\[
\beta^{new} = \beta^{old} - H^{-1}\nabla\ell
\]</span> This formula is used repeatedly until the coefficient values
stop changing significantly, meaning the algorithm has converged to the
best fit. This process is also referred to as <strong>Iteratively
Reweighted Least Squares (IRLS)</strong>.
此公式反复使用，直到系数值不再发生显著变化，这意味着算法已收敛到最佳拟合值。此过程也称为<strong>迭代重加权最小二乘法
(IRLS)</strong>。</p>
<hr />
<h2 id="the-puzzle-a-tale-of-two-models">3.2 The Puzzle: A Tale of Two
Models 🕵️‍♂️</h2>
<p>The most important story in these slides is how the effect of being a
student changes depending on the model. This is a classic example of a
<strong>confounding variable</strong>.</p>
<h4 id="model-1-simple-logistic-regression-default-vs.-student">Model 1:
Simple Logistic Regression (Default vs. Student)</h4>
<p>When predicting default using <em>only</em> student status, the model
is: <code>default ~ student</code></p>
<p>From the slides, the coefficients are: * Intercept (<span
class="math inline">\(\beta_0\)</span>): -3.5041 * student[Yes] (<span
class="math inline">\(\beta_1\)</span>): <strong>0.4049</strong>
(positive)</p>
<p>The equation for the log-odds is: <span class="math display">\[
\log\left(\frac{P(\text{default})}{1-P(\text{default})}\right) = -3.5041
+ 0.4049 \times (\text{is\_student})
\]</span></p>
<p><strong>Conclusion:</strong> The positive coefficient (0.4049)
suggests that <strong>students are more likely to default</strong> than
non-students. The slides calculate the probabilities: * <strong>Student
Default Probability:</strong> 4.31% * <strong>Non-Student Default
Probability:</strong> 2.92%</p>
<p>学生身份的影响如何根据模型而变化。这是一个典型的<strong>混杂变量</strong>的例子。</p>
<h4 id="模型-1简单逻辑回归违约-vs.-学生">模型 1：简单逻辑回归（违约
vs. 学生）</h4>
<p>仅使用学生身份预测违约时，模型为： <code>default ~ student</code></p>
<p>幻灯片中显示的系数为： * 截距 (<span
class="math inline">\(\beta_0\)</span>): -3.5041 * 学生[是] (<span
class="math inline">\(\beta_1\)</span>):
<strong>0.4049</strong>（正）</p>
<p>对数概率公式为： <span class="math display">\[
\log\left(\frac{P(\text{default})}{1-P(\text{default})}\right) = -3.5041
+ 0.4049 \times (\text{is\_student})
\]</span></p>
<p><strong>结论</strong>：正系数 (0.4049)
表明<strong>学生比非学生更有可能违约</strong>。幻灯片计算了以下概率： *
<strong>学生违约概率</strong>：4.31% *
<strong>非学生违约概率</strong>：2.92%</p>
<h2
id="model-2-multiple-logistic-regression-default-vs.-all-variables-模型-2多元逻辑回归违约-vs.-所有变量">3.3
Model 2: Multiple Logistic Regression (Default vs. All Variables) 模型
2：多元逻辑回归（违约 vs. 所有变量）</h2>
<p>When we add <code>balance</code> and <code>income</code> to the
model, it becomes: <code>default ~ student + balance + income</code></p>
<p>From the slides, the new coefficients are: * Intercept (<span
class="math inline">\(\beta_0\)</span>): -10.8690 * balance (<span
class="math inline">\(\beta_1\)</span>): 0.0057 * income (<span
class="math inline">\(\beta_2\)</span>): 0.0030 * student[Yes] (<span
class="math inline">\(\beta_3\)</span>): <strong>-0.6468</strong>
(negative)</p>
<p><strong>The Shocking Twist!</strong> The coefficient for
<code>student[Yes]</code> is now <strong>negative</strong>.</p>
<p><strong>Conclusion:</strong> When we control for balance and income,
<strong>students are actually <em>less</em> likely to default</strong>
than non-students with the same balance and income.</p>
<h4 id="why-the-change-the-confounding-variable-explained">Why the
Change? The Confounding Variable Explained</h4>
<p>The key insight, explained on the slide with multi-colored text
bubbles, is that <strong>students, on average, have higher credit card
balances</strong>.</p>
<ul>
<li>In the simple model, the <code>student</code> variable was
inadvertently capturing the risk associated with having a high
<code>balance</code>. The model mistakenly concluded “being a student
causes default.”</li>
<li>In the multiple model, the <code>balance</code> variable properly
accounts for the risk from a high balance. With that effect isolated,
the <code>student</code> variable can show its true, underlying
relationship with default, which is negative.</li>
</ul>
<p>This demonstrates why it’s crucial to consider multiple relevant
variables to avoid drawing incorrect conclusions. <strong>The most
important slides are the ones that present this paradox and its
explanation.</strong></p>
<p><strong>令人震惊的转折！</strong> <code>student[Yes]</code>
的系数现在为<strong>负</strong>。</p>
<p><strong>结论：</strong>当我们控制余额和收入时，<strong>学生实际上比具有相同余额和收入的非学生更<em>低</em>于违约</strong>。</p>
<h4 id="为什么会有变化混杂变量解释">为什么会有变化？混杂变量解释</h4>
<p>幻灯片上用彩色文字气泡解释了关键的见解，即<strong>学生平均拥有更高的信用卡余额</strong>。</p>
<ul>
<li>在简单模型中，“学生”变量无意中捕捉到了高余额带来的风险。该模型错误地得出了“学生身份导致违约”的结论。</li>
<li>在多元模型中，“余额”变量正确地解释了高余额带来的风险。在分离出这一影响后，“学生”变量可以显示其与违约之间真实的潜在关系，即负相关关系。</li>
</ul>
<p>这说明了为什么考虑多个相关变量以避免得出错误结论至关重要。</p>
<hr />
<h3 id="code-implementation-r-vs.-python">Code Implementation: R
vs. Python</h3>
<p>The slides use R’s <code>glm()</code> (Generalized Linear Model)
function. Here’s how you would replicate this in Python.</p>
<h4 id="r-code-from-slides">R Code (from slides)</h4>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Simple Model</span></span><br><span class="line">glmod2 <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>default <span class="operator">~</span> student<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> family<span class="operator">=</span>binomial<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>glmod2<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model</span></span><br><span class="line">glmod3 <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>default <span class="operator">~</span> .<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> family<span class="operator">=</span>binomial<span class="punctuation">)</span> <span class="comment"># &#x27;.&#x27; means all other variables</span></span><br><span class="line">summary<span class="punctuation">(</span>glmod3<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<h4 id="python-equivalent">Python Equivalent</h4>
<p>We can use two popular libraries: <code>statsmodels</code> (which
gives R-style summaries) and <code>scikit-learn</code> (the standard for
machine learning).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Default&#x27; is a pandas DataFrame with columns:</span></span><br><span class="line"><span class="comment"># &#x27;default&#x27; (0/1), &#x27;student&#x27; (0/1), &#x27;balance&#x27;, &#x27;income&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Using statsmodels (recommended for interpretation) ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data</span></span><br><span class="line"><span class="comment"># For statsmodels, we need to manually add the intercept</span></span><br><span class="line">X_simple = Default[[<span class="string">&#x27;student&#x27;</span>]]</span><br><span class="line">X_simple = sm.add_constant(X_simple)</span><br><span class="line">y = Default[<span class="string">&#x27;default&#x27;</span>]</span><br><span class="line"></span><br><span class="line">X_multiple = Default[[<span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line">X_multiple = sm.add_constant(X_multiple)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simple Model: default ~ student</span></span><br><span class="line">model_simple = sm.Logit(y, X_simple).fit()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- Simple Model ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_simple.summary())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model: default ~ student + balance + income</span></span><br><span class="line">model_multiple = sm.Logit(y, X_multiple).fit()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Multiple Model ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_multiple.summary())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Using scikit-learn (recommended for prediction tasks) ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data (scikit-learn adds intercept by default)</span></span><br><span class="line">X_simple_sk = Default[[<span class="string">&#x27;student&#x27;</span>]]</span><br><span class="line">y_sk = Default[<span class="string">&#x27;default&#x27;</span>]</span><br><span class="line"></span><br><span class="line">X_multiple_sk = Default[[<span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simple Model</span></span><br><span class="line">clf_simple = LogisticRegression().fit(X_simple_sk, y_sk)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nSimple Model Intercept (scikit-learn): <span class="subst">&#123;clf_simple.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Simple Model Coefficient (scikit-learn): <span class="subst">&#123;clf_simple.coef_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model</span></span><br><span class="line">clf_multiple = LogisticRegression().fit(X_multiple_sk, y_sk)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nMultiple Model Intercept (scikit-learn): <span class="subst">&#123;clf_multiple.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Multiple Model Coefficients (scikit-learn): <span class="subst">&#123;clf_multiple.coef_&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1
id="making-predictions-and-the-decision-boundary-进行预测和决策边界">4
Making Predictions and the Decision Boundary 🎯进行预测和决策边界</h1>
<p>Once the model is trained (i.e., we have the coefficients <span
class="math inline">\(\hat{\beta}\)</span>), we can make predictions.
一旦模型训练完成（即，我们有了系数 <span
class="math inline">\(\hat{\beta}\)</span>），我们就可以进行预测了。 ##
Math Behind Predictions</p>
<p>The model outputs the <strong>log-odds</strong>, which can be
converted into a probability. A key concept is the <strong>decision
boundary</strong>, which is the threshold where the model is uncertain
(probability = 50%).
模型输出<strong>对数概率</strong>，它可以转换为概率。一个关键概念是<strong>决策边界</strong>，它是模型不确定的阈值（概率
= 50%）。</p>
<ol type="1">
<li><p><strong>The Estimated Odds</strong>: The core output of the
linear part of the model is the exponential of the linear equation,
which gives the odds of the outcome being ‘Yes’ (or 1).
<strong>估计概率</strong>：模型线性部分的核心输出是线性方程的指数，它给出了结果为“是”（或
1）的概率。</p>
<p><span class="math display">\[
\]</span>$$\frac{\hat{P}(y=1|\mathbf{x}_0)}{\hat{P}(y=0|\mathbf{x}_0)} =
\exp(\hat{\beta}^\top \mathbf{x}_0)</p>
<p><span class="math display">\[
\]</span><span class="math display">\[
\]</span></p></li>
<li><p><strong>The Decision Rule</strong>: We classify a new observation
<span class="math inline">\(\mathbf{x}_0\)</span> by comparing its
predicted odds to a threshold <span
class="math inline">\(\delta\)</span>.
<strong>决策规则</strong>：我们通过比较新观测值 <span
class="math inline">\(\mathbf{x}_0\)</span> 的预测概率与阈值 <span
class="math inline">\(\delta\)</span> 来对其进行分类。</p>
<ul>
<li>Predict <span class="math inline">\(y=1\)</span> if <span
class="math inline">\(\exp(\hat{\beta}^\top \mathbf{x}_0) &gt;
\delta\)</span></li>
<li>Predict <span class="math inline">\(y=0\)</span> if <span
class="math inline">\(\exp(\hat{\beta}^\top \mathbf{x}_0) &lt;
\delta\)</span> A common default is <span
class="math inline">\(\delta=1\)</span>, which means we predict ‘Yes’ if
the probability is greater than 0.5.</li>
</ul></li>
<li><p><strong>The Linear Boundary</strong>: The decision boundary
itself is where the odds are exactly equal to the threshold. By taking
the logarithm, we see that this boundary is a <strong>linear
equation</strong>. This is why logistic regression is called a
<strong>linear classifier</strong>.
<strong>线性边界</strong>：决策边界本身就是概率恰好等于阈值的地方。取对数后，我们发现这个边界是一个<strong>线性方程</strong>。这就是逻辑回归被称为<strong>线性分类器</strong>的原因。
<span class="math display">\[
\]</span>$$\hat{\beta}^\top \mathbf{x} = \log(\delta)</p>
<p><span class="math display">\[
\]</span>$$For <span class="math inline">\(\delta=1\)</span>, the
boundary is simply <span class="math inline">\(\hat{\beta}^\top
\mathbf{x} = 0\)</span>.</p></li>
</ol>
<p>This concept is visualized perfectly in the slide titled “Linear
Classifier,” which shows a straight line neatly separating two classes
of data points.
题为“线性分类器”的幻灯片完美地展示了这一概念，它展示了一条直线，将两类数据点巧妙地分隔开来。</p>
<h2 id="visualizing-the-confounding-effect">Visualizing the Confounding
Effect</h2>
<p>The most important image in this set is <strong>Figure 4.3</strong>,
as it visually explains the confounding puzzle from the first set of
slides.</p>
<ul>
<li><strong>Right Panel (Boxplots)</strong>: This shows that
<strong>students (Yes) tend to have higher credit card balances</strong>
than non-students (No). This is the source of the confounding.</li>
<li><strong>Left Panel (Default Rates)</strong>:
<ul>
<li>The <strong>dashed lines</strong> show the <em>overall</em> default
rates. The orange line (students) is higher than the blue line
(non-students). This matches our simple model
(<code>default ~ student</code>).</li>
<li>The <strong>solid S-shaped curves</strong> show the probability of
default as a function of balance. For any <em>given</em> balance, the
blue curve (non-students) is slightly higher than the orange curve
(students). This means that <strong>at the same level of debt, students
are <em>less</em> likely to default</strong>. This matches our multiple
regression model
(<code>default ~ student + balance + income</code>).</li>
</ul></li>
</ul>
<p>This single figure brilliantly illustrates how a variable can appear
to have one effect in isolation but the opposite effect when controlling
for a confounding factor. *
<strong>右侧面板（箱线图）</strong>：这表明<strong>学生（是）的信用卡余额往往高于非学生（否）。这就是混杂效应的根源。
* </strong>左图（违约率）<strong>： *
</strong>虚线<strong>显示<em>总体</em>违约率。橙色线（学生）高于蓝色线（非学生）。这与我们的简单模型（“违约
~ 学生”）相符。 * </strong>S
形实线<strong>显示违约概率与余额的关系。对于任何<em>给定</em>的余额，蓝色曲线（非学生）略高于橙色曲线（学生）。这意味着</strong>在相同的债务水平下，学生违约的可能性<em>较小</em>。这与我们的多元回归模型（“违约
~ 学生 + 余额 + 收入”）相符。</p>
<p>这张图巧妙地说明了为什么一个变量在单独使用时似乎会产生一种影响，但在控制混杂因素后却会产生相反的影响。</p>
<h2 id="an-important-edge-case-perfect-separation">An Important Edge
Case: Perfect Separation ⚠️</h2>
<p>What happens if the data can be perfectly separated by a straight
line? 如果数据可以用一条直线完美分离，会发生什么？</p>
<p>One might think this is the ideal scenario, but it causes a problem
for the logistic regression algorithm. The model will try to find
coefficients that make the probabilities for each class as close to 1
and 0 as possible. To do this, the magnitude of the coefficients (<span
class="math inline">\(\hat{\beta}\)</span>) must grow infinitely large.
人们可能认为这是理想情况，但它会给逻辑回归算法带来问题。模型会尝试找到使每个类别的概率尽可能接近
1 和 0 的系数。为此，系数 (<span
class="math inline">\(\hat{\beta}\)</span>) 的大小必须无限大。</p>
<p>The slide “Non-convergence for perfectly separated case” demonstrates
this:</p>
<ul>
<li><p><strong>The Code</strong>: It generates two distinct,
non-overlapping clusters of data points using Python’s
<code>scikit-learn</code>.</p></li>
<li><p><strong>Parameter Estimates Graph</strong>: It shows the
<code>Intercept</code>, <code>Coefficient 1</code>, and
<code>Coefficient 2</code> values increasing or decreasing without limit
as the algorithm runs through more iterations. They never converge to a
stable value.</p></li>
<li><p><strong>Decision Boundary Graph</strong>: The decision boundary
itself might look reasonable, but the underlying coefficients are
unstable.</p></li>
<li><p><strong>代码</strong>：它使用 Python 的 <code>scikit-learn</code>
生成两个不同的、不重叠的数据点聚类。</p></li>
<li><p><strong>参数估计图</strong>：它显示“截距”、“系数 1”和“系数
2”的值随着算法迭代次数的增加或减少而无限增大或减小。它们永远不会收敛到一个稳定的值。</p></li>
<li><p><strong>决策边界图</strong>：决策边界本身可能看起来合理，但底层系数是不稳定的。</p></li>
</ul>
<p><strong>Key Takeaway</strong>: If your logistic regression model
fails to converge, the first thing you should check for is perfect
separation in your training data.
<strong>关键要点</strong>：如果您的逻辑回归模型未能收敛，您应该检查的第一件事就是训练数据是否完美分离。</p>
<h2 id="code-understanding">Code Understanding</h2>
<p>The slides provide useful code snippets in both R and Python.</p>
<h2 id="r-code-plotting-predictions">R Code (Plotting Predictions)</h2>
<p>This code generates the plot with the two S-shaped curves (one for
students, one for non-students) showing the probability of default as
balance increases.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Create a data frame for prediction with a range of balances</span></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># One version for students, one for non-students</span></span><br><span class="line">Default.st <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>balance<span class="operator">=</span>seq<span class="punctuation">(</span><span class="number">500</span><span class="punctuation">,</span> <span class="number">2500</span><span class="punctuation">,</span> by<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> student<span class="operator">=</span><span class="string">&quot;Yes&quot;</span><span class="punctuation">)</span></span><br><span class="line">Default.nonst <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>balance<span class="operator">=</span>seq<span class="punctuation">(</span><span class="number">500</span><span class="punctuation">,</span> <span class="number">2500</span><span class="punctuation">,</span> by<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> student<span class="operator">=</span><span class="string">&quot;No&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Use the trained multiple regression model (glmod3) to predict probabilities</span></span><br><span class="line">pred.st <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>glmod3<span class="punctuation">,</span> Default.st<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line">pred.nonst <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>glmod3<span class="punctuation">,</span> Default.nonst<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Plot the results</span></span><br><span class="line">plot<span class="punctuation">(</span>Default.st<span class="operator">$</span>balance<span class="punctuation">,</span> pred.st<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;l&quot;</span><span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;red&quot;</span><span class="punctuation">,</span> ...<span class="punctuation">)</span> <span class="operator">/</span><span class="operator">/</span> Students</span><br><span class="line">lines<span class="punctuation">(</span>Default.nonst<span class="operator">$</span>balance<span class="punctuation">,</span> pred.nonst<span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;blue&quot;</span><span class="punctuation">,</span> ...<span class="punctuation">)</span> <span class="operator">/</span><span class="operator">/</span> Non<span class="operator">-</span>students</span><br></pre></td></tr></table></figure>
<h4 id="python-code-visualizing-the-decision-boundary">Python Code
(Visualizing the Decision Boundary)</h4>
<p>This Python code uses <code>scikit-learn</code> and
<code>matplotlib</code> to create the plot showing the linear decision
boundary.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Generate synthetic data with two classes</span></span><br><span class="line">X, y = make_classification(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Initialize and fit the logistic regression model</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create a mesh grid of points to make predictions over the entire plot area</span></span><br><span class="line">xx, yy = np.meshgrid(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Predict the probability for each point on the grid</span></span><br><span class="line">probs = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Plot the decision boundary where the probability is 0.5</span></span><br><span class="line">plt.contour(xx, yy, probs.reshape(xx.shape), levels=[<span class="number">0.5</span>], ...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. Scatter plot the actual data points</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, ...)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="other-important-remarks">Other Important Remarks</h3>
<p>The “Remarks” slide briefly mentions some key extensions:</p>
<ul>
<li><p><strong>Probit Model</strong>: An alternative to logistic
regression that uses the cumulative distribution function (CDF) of the
standard normal distribution instead of the sigmoid function. The
results are often very similar.</p></li>
<li><p><strong>Softmax Regression</strong>: An extension of logistic
regression used for multi-class classification (when there are more than
two possible outcomes).</p></li>
<li><p><strong>Probit
模型</strong>：逻辑回归的替代方法，它使用标准正态分布的累积分布函数
(CDF) 代替 S 型函数。结果通常非常相似。</p></li>
<li><p><strong>Softmax
回归</strong>：逻辑回归的扩展，用于多类分类（当存在两个以上可能结果时）。</p></li>
</ul>
<h1
id="here-is-a-summary-of-the-slides-on-linear-discriminant-analysis-lda-including-the-key-mathematical-formulas-visual-explanations-and-how-to-implement-it-in-python.">5.
Here is a summary of the slides on Linear Discriminant Analysis (LDA),
including the key mathematical formulas, visual explanations, and how to
implement it in Python.</h1>
<h2
id="the-main-idea-classification-using-probabilities-使用概率进行分类">The
Main Idea: Classification Using Probabilities 使用概率进行分类</h2>
<p>Linear Discriminant Analysis (LDA) is a classification method. For a
given input <strong>x</strong>, it calculates the probability that
<strong>x</strong> belongs to each class and then assigns
<strong>x</strong> to the class with the <strong>highest
probability</strong>.</p>
<p>It does this using <strong>Bayes’ Theorem</strong>, which provides a
formula for the posterior probability <span class="math inline">\(P(Y=k
| X=x)\)</span>, or the probability that the class is <span
class="math inline">\(k\)</span> given the input <span
class="math inline">\(x\)</span>. 线性判别分析 (LDA)
是一种分类方法。对于给定的输入 <strong>x</strong>，它计算
<strong>x</strong> 属于每个类别的概率，然后将 <strong>x</strong>
分配给<strong>概率最高</strong>的类别。</p>
<p>它使用<strong>贝叶斯定理</strong>来实现这一点，该定理提供了后验概率
<span class="math inline">\(P(Y=k | X=x)\)</span> 的公式，即给定输入
<span class="math inline">\(x\)</span>，该类别属于 <span
class="math inline">\(k\)</span> 的概率。 <span class="math display">\[
p_k(x) = P(Y=k|X=x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)}
\]</span></p>
<ul>
<li><span class="math inline">\(p_k(x)\)</span> is the <strong>posterior
probability</strong> we want to maximize.</li>
<li><span class="math inline">\(\pi_k = P(Y=k)\)</span> is the
<strong>prior probability</strong> of class <span
class="math inline">\(k\)</span> (how common the class is overall).</li>
<li><span class="math inline">\(f_k(x) = f(x|Y=k)\)</span> is the
<strong>class-conditional probability density function</strong> of
observing input <span class="math inline">\(x\)</span> if it belongs to
class <span class="math inline">\(k\)</span>.</li>
</ul>
<p>To classify a new observation <span class="math inline">\(x\)</span>,
we simply find the class <span class="math inline">\(k\)</span> that
makes <span class="math inline">\(p_k(x)\)</span> the largest.
为了对新的观察值 <span class="math inline">\(x\)</span>
进行分类，我们只需找到使 <span class="math inline">\(p_k(x)\)</span>
最大的类别 <span class="math inline">\(k\)</span> 即可。</p>
<hr />
<h2 id="key-assumptions-of-lda">Key Assumptions of LDA</h2>
<p>LDA’s power comes from a specific, simplifying assumption about the
data’s distribution. LDA
的强大之处在于它对数据分布进行了特定的简化假设。</p>
<ol type="1">
<li><p><strong>Gaussian Distribution:</strong> LDA assumes that the data
within each class <span class="math inline">\(k\)</span> follows a
p-dimensional multivariate normal (or Gaussian) distribution, denoted as
<span class="math inline">\(X|Y=k \sim \mathcal{N}(\mu_k,
\Sigma)\)</span>.</p></li>
<li><p><strong>Common Covariance:</strong> A crucial assumption is that
all classes share the <strong>same covariance matrix</strong> <span
class="math inline">\(\Sigma\)</span>. This means that while the classes
may have different centers (means, <span
class="math inline">\(\mu_k\)</span>), their shape and orientation
(covariance, <span class="math inline">\(\Sigma\)</span>) are
identical.</p></li>
<li><p><strong>高斯分布</strong>：LDA 假设每个类 <span
class="math inline">\(k\)</span> 中的数据服从 p
维多元正态（或高斯）分布，表示为 <span class="math inline">\(X|Y=k \sim
\mathcal{N}(\mu_k, \Sigma)\)</span>。</p></li>
<li><p><strong>共同协方差</strong>：一个关键假设是所有类共享<strong>相同的协方差矩阵</strong>
<span
class="math inline">\(\Sigma\)</span>。这意味着虽然类可能具有不同的中心（均值，<span
class="math inline">\(\mu_k\)</span>），但它们的形状和方向（协方差，<span
class="math inline">\(\Sigma\)</span>）是相同的。</p></li>
</ol>
<p>The probability density function for a class <span
class="math inline">\(k\)</span> is: <span class="math display">\[
f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp \left( -\frac{1}{2}(x
- \mu_k)^T \Sigma^{-1} (x - \mu_k) \right)
\]</span></p>
<p>The image above (from your slide “Knowing normal distribution”)
illustrates this. The two “bells” have different centers (different
<span class="math inline">\(\mu_k\)</span>) but similar shapes. The one
on the right is “tilted,” indicating correlation between variables,
which is captured in the shared covariance matrix <span
class="math inline">\(\Sigma\)</span>.
上图（摘自幻灯片“了解正态分布”）说明了这一点。两个“钟”形的中心不同（<span
class="math inline">\(\mu_k\)</span>
不同），但形状相似。右边的钟形“倾斜”，表示变量之间存在相关性，这体现在共享协方差矩阵
<span class="math inline">\(\Sigma\)</span> 中。</p>
<hr />
<h2 id="the-math-behind-lda-the-discriminant-function-判别函数">The Math
Behind LDA: The Discriminant Function 判别函数</h2>
<p>Since we only need to find the class <span
class="math inline">\(k\)</span> that maximizes the posterior
probability <span class="math inline">\(p_k(x)\)</span>, we can simplify
the math. The denominator in Bayes’ theorem is the same for all classes,
so we only need to maximize the numerator: <span
class="math inline">\(\pi_k f_k(x)\)</span>.
由于我们只需要找到使后验概率 <span class="math inline">\(p_k(x)\)</span>
最大化的类别 <span
class="math inline">\(k\)</span>，因此可以简化数学计算。贝叶斯定理中的分母对于所有类别都是相同的，因此我们只需要最大化分子：<span
class="math inline">\(\pi_k f_k(x)\)</span>。 Taking the logarithm
(which doesn’t change which class is maximal) and removing constant
terms gives us the <strong>linear discriminant function</strong>, <span
class="math inline">\(\delta_k(x)\)</span>:
取对数（这不会改变哪个类别是最大值）并移除常数项，得到<strong>线性判别函数</strong>，<span
class="math inline">\(\delta_k(x)\)</span>：</p>
<p><span class="math display">\[
\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1}
\mu_k + \log(\pi_k)
\]</span></p>
<p>This function is <strong>linear</strong> in <span
class="math inline">\(x\)</span>, which is why the method is called
<em>Linear</em> Discriminant Analysis. The decision boundary between any
two classes, say class <span class="math inline">\(k\)</span> and class
<span class="math inline">\(l\)</span>, is the set of points where <span
class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>, which defines
a linear hyperplane. 该函数关于 <span class="math inline">\(x\)</span>
是<strong>线性</strong>的，因此该方法被称为<em>线性</em>判别分析。任意两个类别（例如类别
<span class="math inline">\(k\)</span> 和类别 <span
class="math inline">\(l\)</span>）之间的决策边界是满足 <span
class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>
的点的集合，这定义了一个线性超平面。</p>
<p>The image above (from your “Graph of LDA” slide) is very important. *
<strong>Left:</strong> The ellipses show the true 95% probability
contours for three Gaussian classes. The dashed lines are the ideal
Bayes decision boundaries, which are perfectly linear because the
assumption of common covariance holds. * <strong>Right:</strong> This
shows a sample of data points drawn from those distributions. The solid
lines are the LDA decision boundaries calculated from the sample. They
are a very good estimate of the ideal boundaries. 上图（来自您的“LDA
图”幻灯片）非常重要。 *
<strong>左图：</strong>椭圆显示了三个高斯类别的真实 95%
概率轮廓。虚线是理想的贝叶斯决策边界，由于共同协方差假设成立，因此它们是完美的线性。
*
<strong>右图：</strong>这显示了从这些分布中抽取的数据点样本。实线是根据样本计算出的
LDA 决策边界。它们是对理想边界的非常好的估计。 ***</p>
<h2
id="practical-implementation-estimating-the-parameters-实际应用估计参数">Practical
Implementation: Estimating the Parameters 实际应用：估计参数</h2>
<p>In a real-world scenario, we don’t know the true parameters (<span
class="math inline">\(\mu_k\)</span>, <span
class="math inline">\(\Sigma\)</span>, <span
class="math inline">\(\pi_k\)</span>). Instead, we
<strong>estimate</strong> them from our training data (<span
class="math inline">\(n\)</span> total samples, with <span
class="math inline">\(n_k\)</span> samples in class <span
class="math inline">\(k\)</span>).
在实际场景中，我们不知道真正的参数（<span
class="math inline">\(\mu_k\)</span>、<span
class="math inline">\(\Sigma\)</span>、<span
class="math inline">\(\pi_k\)</span>）。相反，我们根据训练数据（<span
class="math inline">\(n\)</span> 个样本，<span
class="math inline">\(n_k\)</span> 个样本属于 <span
class="math inline">\(k\)</span> 类）来<strong>估计</strong>它们。</p>
<ul>
<li><strong>Prior Probability (<span
class="math inline">\(\hat{\pi}_k\)</span>):</strong> The proportion of
training samples in class <span class="math inline">\(k\)</span>. <span
class="math display">\[\hat{\pi}_k = \frac{n_k}{n}\]</span></li>
<li><strong>Class Mean (<span
class="math inline">\(\hat{\mu}_k\)</span>):</strong> The average of the
training samples in class <span class="math inline">\(k\)</span>. <span
class="math display">\[\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i=k}
x_i\]</span></li>
<li><strong>Common Covariance (<span
class="math inline">\(\hat{\Sigma}\)</span>):</strong> A weighted
average of the sample covariance matrices for each class. This is often
called the “pooled” covariance. <span
class="math display">\[\hat{\Sigma} = \frac{1}{n-K} \sum_{k=1}^{K}
\sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T\]</span></li>
<li><strong>先验概率 (<span
class="math inline">\(\hat{\pi}_k\)</span>)：</strong>训练样本在 <span
class="math inline">\(k\)</span> 类中的比例。 <span
class="math display">\[\hat{\pi}_k = \frac{n_k}{n}\]</span></li>
<li><strong>类别均值 (<span
class="math inline">\(\hat{\mu}_k\)</span>)：</strong>训练样本在 <span
class="math inline">\(k\)</span> 类中的平均值。 <span
class="math display">\[\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i=k}
x_i\]</span></li>
<li><strong>公共协方差 (<span
class="math inline">\(\hat{\Sigma}\)</span>)：</strong>每个类的样本协方差矩阵的加权平均值。这通常被称为“合并”协方差。
<span class="math display">\[\hat{\Sigma} = \frac{1}{n-K} \sum_{k=1}^{K}
\sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T\]</span></li>
</ul>
<p>We then plug these estimates into the discriminant function to get
<span class="math inline">\(\hat{\delta}_k(x)\)</span> and classify a
new observation <span class="math inline">\(x\)</span> to the class with
the largest score. 然后，我们将这些估计值代入判别函数，得到 <span
class="math inline">\(\hat{\delta}_k(x)\)</span>，并将新的观测值 <span
class="math inline">\(x\)</span> 归类到得分最高的类别。 ***</p>
<h2 id="evaluating-performance">Evaluating Performance</h2>
<p>After training the model, we evaluate its performance using a
<strong>confusion matrix</strong>.
训练模型后，我们使用<strong>混淆矩阵</strong>来评估其性能。</p>
<p>This matrix shows the true classes versus the predicted classes. *
<strong>Diagonal elements</strong> (9644, 81) are correct predictions. *
<strong>Off-diagonal elements</strong> (23, 252) are errors.
该矩阵显示了真实类别与预测类别的对比。 * <strong>对角线元素</strong>
(9644, 81) 表示正确预测。 * <strong>非对角线元素</strong> (23, 252)
表示错误预测。</p>
<p>From this matrix, we can calculate key metrics: * <strong>Overall
Error Rate:</strong> Total incorrect predictions / Total predictions. *
Example: <span class="math inline">\((252 + 23) / 10000 =
2.75\%\)</span> * <strong>Sensitivity (True Positive Rate):</strong>
Correctly predicted positives / Total actual positives. It answers: “Of
all the people who actually defaulted, what fraction did we catch?” *
Example: <span class="math inline">\(81 / 333 = 24.3\%\)</span>. The
sensitivity is <span class="math inline">\(1 - 75.7\% = 24.3\%\)</span>.
* <strong>Specificity (True Negative Rate):</strong> Correctly predicted
negatives / Total actual negatives. It answers: “Of all the people who
did not default, what fraction did we correctly identify?” * Example:
<span class="math inline">\(9644 / 9667 = 99.8\%\)</span>. The
specificity is <span class="math inline">\(1 - 0.24\% =
99.8\%\)</span>.</p>
<p>The example in your slides shows a high error rate for “default”
people (75.7%) because the classes are <strong>unbalanced</strong>—there
are far fewer defaulters. This highlights the importance of looking at
class-specific metrics, not just the overall error rate.</p>
<hr />
<h2 id="python-code-understanding">Python Code Understanding</h2>
<p>In Python, you can easily implement LDA using the
<code>scikit-learn</code> library. The code conceptually mirrors the
steps we discussed.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume you have your data X (features) and y (labels)</span></span><br><span class="line"><span class="comment"># X = features (e.g., balance, income)</span></span><br><span class="line"><span class="comment"># y = labels (e.g., 0 for &#x27;no-default&#x27;, 1 for &#x27;default&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create an instance of the LDA model</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit the model to the training data</span></span><br><span class="line"><span class="comment"># This is where the model calculates the estimates:</span></span><br><span class="line"><span class="comment">#  - Prior probabilities (pi_k)</span></span><br><span class="line"><span class="comment">#  - Class means (mu_k)</span></span><br><span class="line"><span class="comment">#  - Pooled covariance matrix (Sigma)</span></span><br><span class="line">lda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Make predictions on new, unseen data</span></span><br><span class="line">predictions = lda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Evaluate the model&#x27;s performance</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, predictions))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, predictions))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>LinearDiscriminantAnalysis()</code> creates the classifier
object.</li>
<li><code>lda.fit(X_train, y_train)</code> is the core training step
where the model learns the <span
class="math inline">\(\hat{\pi}_k\)</span>, <span
class="math inline">\(\hat{\mu}_k\)</span>, and <span
class="math inline">\(\hat{\Sigma}\)</span> parameters from the
data.</li>
<li><code>lda.predict(X_test)</code> uses the learned discriminant
function <span class="math inline">\(\hat{\delta}_k(x)\)</span> to
classify each sample in the test set.</li>
<li><code>confusion_matrix</code> and <code>classification_report</code>
are tools to evaluate the results, just like in the slides.</li>
</ul>
<h1
id="here-is-a-summary-of-the-provided-slides-on-linear-discriminant-analysis-lda-focusing-on-mathematical-concepts-python-code-interpretation-and-key-visuals.">6.
Here is a summary of the provided slides on Linear Discriminant Analysis
(LDA), focusing on mathematical concepts, Python code interpretation,
and key visuals.</h1>
<h2 id="core-concept-lda-for-classification">Core Concept: LDA for
Classification</h2>
<p>Linear Discriminant Analysis (LDA) is a classification method that
models the probability that an observation belongs to a certain class.
It works by finding a linear combination of features that best separates
two or more classes.</p>
<p>The decision is based on <strong>Bayes’ theorem</strong>. For a given
observation with features <span class="math inline">\(X=x\)</span>, LDA
calculates the <strong>posterior probability</strong>, <span
class="math inline">\(p_k(x) = Pr(Y=k|X=x)\)</span>, for each class
<span class="math inline">\(k\)</span>. This is the probability that the
observation belongs to class <span class="math inline">\(k\)</span>
given its features. 线性判别分析 (LDA)
是一种分类方法，它对观测值属于某个类别的概率进行建模。它的工作原理是找到能够最好地区分两个或多个类别的特征的线性组合。</p>
<p>该决策基于<strong>贝叶斯定理</strong>。对于特征为 <span
class="math inline">\(X=x\)</span> 的给定观测值，LDA 会计算每个类别
<span class="math inline">\(k\)</span>
的<strong>后验概率</strong>，<span class="math inline">\(p_k(x) =
Pr(Y=k|X=x)\)</span>。这是给定观测值的特征后，该观测值属于类别 <span
class="math inline">\(k\)</span> 的概率。</p>
<p>By default, the Bayes classifier assigns an observation to the class
with the highest posterior probability. For a binary (two-class) problem
like ‘Yes’ vs. ‘No’, this means:
默认情况下，贝叶斯分类器将观测值分配给后验概率最高的类别。对于像“是”与“否”这样的二分类问题，这意味着：</p>
<ul>
<li>Assign to ‘Yes’ if <span class="math inline">\(Pr(Y=\text{Yes}|X=x)
&gt; 0.5\)</span></li>
<li>Assign to ‘No’ otherwise</li>
</ul>
<h2 id="modifying-the-decision-threshold">Modifying the Decision
Threshold</h2>
<p>The default 0.5 threshold isn’t always optimal. In many real-world
scenarios, the cost of one type of error is much higher than another.
For example, in credit card default prediction: 默认的 0.5
阈值并非总是最优的。在许多实际场景中，一种错误的代价远高于另一种。例如，在信用卡违约预测中：</p>
<ul>
<li><strong>False Negative:</strong> Incorrectly classifying a person
who will default as someone who won’t. (The bank loses money).</li>
<li><strong>False Positive:</strong> Incorrectly classifying a person
who won’t default as someone who will. (The bank loses a potential
customer).</li>
</ul>
<p>A bank might decide that missing a defaulter is much worse than
denying a good customer. To catch more potential defaulters, they can
<strong>lower the probability threshold</strong>.
银行可能会认为错过一个违约者比拒绝一个优质客户更糟糕。为了捕捉更多潜在的违约者，他们可以<strong>降低概率阈值</strong>。</p>
<p>A modified rule could be: <span class="math display">\[
Pr(\text{default}=\text{Yes}|X=x) &gt; 0.2
\]</span> This makes the model more “sensitive” to flagging potential
defaulters, even at the cost of misclassifying more non-defaulters.
降低阈值<strong>会提高敏感度</strong>，但<strong>会降低特异性</strong>。</p>
<p>This decision leads to a <strong>trade-off</strong> between two key
performance metrics: * <strong>Sensitivity (True Positive
Rate):</strong> The ability to correctly identify positive cases. (e.g.,
<code>Correctly identified defaulters / Total actual defaulters</code>).
* <strong>Specificity (True Negative Rate):</strong> The ability to
correctly identify negative cases. (e.g.,
<code>Correctly identified non-defaulters / Total actual non-defaulters</code>).</p>
<p>这一决策会导致两个关键绩效指标之间的<strong>权衡</strong>： *
<strong>敏感度（真阳性率）：</strong>正确识别阳性案例的能力。（例如，“正确识别的违约者/实际违约者总数”）。
*
<strong>特异性（真阴性率）：</strong>正确识别阴性案例的能力。（例如，“正确识别的非违约者/实际非违约者总数”）。</p>
<p>Lowering the threshold <strong>increases sensitivity</strong> but
<strong>decreases specificity</strong>. ## Python Code Explained</p>
<p>The slides show how to implement and adjust LDA using Python’s
<code>scikit-learn</code> library.</p>
<h2 id="basic-lda-implementation">Basic LDA Implementation</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the necessary library</span></span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize and train the LDA model</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line">lda_train = lda.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions using the default 0.5 threshold</span></span><br><span class="line">y_pred = lda.predict(X)</span><br></pre></td></tr></table></figure>
<p>This code trains an LDA model and makes predictions using the
standard 50% probability boundary.</p>
<h2 id="adjusting-the-prediction-threshold">Adjusting the Prediction
Threshold</h2>
<p>To use a custom threshold (e.g., 0.2), you don’t use the
<code>.predict()</code> method. Instead, you get the class probabilities
with <code>.predict_proba()</code> and apply the threshold manually.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Get the probabilities for each class</span></span><br><span class="line"><span class="comment"># lda.predict_proba(X) returns an array like [[P(No), P(Yes)], ...]</span></span><br><span class="line"><span class="comment"># We select the second column [:, 1] for the &#x27;Yes&#x27; class probability</span></span><br><span class="line">lda_probs = lda.predict_proba(X)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Define a custom threshold</span></span><br><span class="line">threshold = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Apply the threshold to get new predictions</span></span><br><span class="line"><span class="comment"># This creates a boolean array (True where prob &gt; 0.2, else False)</span></span><br><span class="line"><span class="comment"># We then convert True/False to &#x27;Yes&#x27;/&#x27;No&#x27; labels</span></span><br><span class="line">lda_pred1 = np.where(lda_probs &gt; threshold, <span class="string">&quot;Yes&quot;</span>, <span class="string">&quot;No&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>This is the core technique for tuning the classifier’s behavior to
meet specific business needs, as demonstrated on slides 55 and 56 for
both LDA and Logistic Regression.</p>
<h2 id="important-images-to-understand">Important Images to
Understand</h2>
<ol type="1">
<li><strong>Confusion Matrix (Slide 49):</strong> This table is crucial.
It breaks down the model’s predictions into True Positives, True
Negatives, False Positives, and False Negatives. All key metrics like
error rate, sensitivity, and specificity are calculated from this
matrix. <strong>混淆矩阵（幻灯片
49）：</strong>这张表至关重要。它将模型的预测分解为真阳性、真阴性、假阳性和假阴性。所有关键指标，例如错误率、灵敏度和特异性，都基于此矩阵计算得出。</li>
<li><strong>LDA Decision Boundaries (Slide 51):</strong> This plot
provides a powerful visual intuition. It shows the data points for two
classes and the decision boundary line. The different parallel lines
show how changing the threshold from 0.5 to 0.1 or 0.9 shifts the
boundary, making the model classify more or fewer points into the
minority class. <strong>LDA 决策边界（幻灯片
51）：</strong>这张图提供了强大的视觉直观性。它展示了两个类别的数据点和决策边界线。不同的平行线显示了将阈值从
0.5 更改为 0.1 或 0.9
时边界如何移动，从而使模型将更多或更少的点归入少数类</li>
<li><strong>Error Rate Tradeoff Curve (Slide 53):</strong> This graph is
the most important for understanding the business implication of
changing the threshold. It clearly shows that as the threshold changes,
the error rate for one class goes down while the error rate for the
other goes up. The overall error is minimized at a certain point, but
that may not be the optimal point from a business perspective.
<strong>错误率权衡曲线（幻灯片
53）：</strong>这张图对于理解更改阈值的业务含义至关重要。它清楚地表明，随着阈值的变化，一个类别的错误率下降，而另一个类别的错误率上升。总体误差在某个点达到最小，但从业务角度来看，这可能并非最佳点。</li>
<li><strong>ROC Curve (Slides 54 &amp; 55):</strong> The Receiver
Operating Characteristic (ROC) curve plots Sensitivity vs. (1 -
Specificity) for <em>all possible thresholds</em>. An ideal classifier
has a curve that “hugs” the top-left corner, indicating high sensitivity
and high specificity. It’s a standard way to visualize and compare the
overall performance of different classifiers. <strong>ROC 曲线（幻灯片
54 和 55）：</strong> 接收者操作特性 (ROC)
曲线绘制了<em>所有可能阈值</em>的灵敏度与（1 -
特异性）的关系。理想的分类器曲线“紧贴”左上角，表示高灵敏度和高特异性。这是可视化和比较不同分类器整体性能的标准方法。</li>
</ol>
<h1
id="here-is-a-summary-of-the-provided-slides-on-linear-and-quadratic-discriminant-analysis-including-the-key-formulas-python-code-equivalents-and-explanations-of-the-important-concepts.">7.
Here is a summary of the provided slides on Linear and Quadratic
Discriminant Analysis, including the key formulas, Python code
equivalents, and explanations of the important concepts.</h1>
<h2 id="key-goal-classification"><strong>Key Goal:
Classification</strong></h2>
<p>Both <strong>Linear Discriminant Analysis (LDA)</strong> and
<strong>Quadratic Discriminant Analysis (QDA)</strong> are
classification algorithms. Their main goal is to find a decision
boundary to separate different classes (e.g., “default” vs. “not
default”) in the data. <strong>线性判别分析 (LDA)</strong> 和
<strong>二次判别分析 (QDA)</strong>
都是分类算法。它们的主要目标是找到一个决策边界来区分数据中的不同类别（例如，“默认”与“非默认”）。</p>
<h3 id="linear-discriminant-analysis-lda">## Linear Discriminant
Analysis (LDA)</h3>
<p>LDA creates a <strong>linear</strong> decision boundary between
classes. LDA 在类别之间创建<strong>线性</strong>决策边界。</p>
<h4 id="core-idea-fishers-interpretation"><strong>Core Idea (Fisher’s
Interpretation)</strong></h4>
<p>Imagine you have data points for different classes in a 3D space.
Fisher’s idea is to find the best angle to shine a “flashlight” on the
data to project its shadow onto a 2D wall (or a 1D line). The “best”
projection is the one where the shadows of the different classes are
<strong>as far apart from each other as possible</strong>, while the
shadows within each class are <strong>as tightly packed as
possible</strong>. 想象一下，你在三维空间中拥有不同类别的数据点。Fisher
的思想是找到最佳角度，用“手电筒”照射数据，将其阴影投射到二维墙壁（或一维线上）。
“最佳”投影是不同类别的阴影<strong>彼此之间尽可能远</strong>，而每个类别内的阴影<strong>尽可能紧密</strong>的投影。</p>
<ul>
<li><strong>Maximize:</strong> The distance between the means of the
projected classes (Between-Class Variance).
投影类别均值之间的距离（类间方差）。</li>
<li><strong>Minimize:</strong> The spread or variance within each
projected class (Within-Class Variance).
每个投影类别内的扩散或方差（类内方差）。 This is the most important
image for understanding the intuition behind LDA. It shows how
projecting the data onto a specific line (defined by vector
<code>w</code>) can make the two classes clearly separable.
这是理解LDA背后直觉的最重要图像。它展示了如何将数据投影到特定直线（由向量“w”定义）上，从而使两个类别清晰可分。</li>
</ul>
<h4 id="key-mathematical-formulas"><strong>Key Mathematical
Formulas</strong></h4>
<p>To achieve this, LDA maximizes a ratio called the <strong>Rayleigh
quotient</strong>. LDA最大化一个称为<strong>瑞利商</strong>的比率。</p>
<ol type="1">
<li><strong>Within-Class Covariance (<span
class="math inline">\(\hat{\Sigma}_W\)</span>)</strong>: Measures the
spread of data <em>inside</em> each class. <strong>类内协方差 (<span
class="math inline">\(\hat{\Sigma}_W\)</span>)</strong>：衡量每个类别<em>内部</em>数据的扩散程度。
<span class="math display">\[\hat{\Sigma}_W = \frac{1}{n-K}
\sum_{k=1}^{K} \sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i -
\hat{\mu}_k)^\top\]</span></li>
<li><strong>Between-Class Covariance (<span
class="math inline">\(\hat{\Sigma}_B\)</span>)</strong>: Measures the
spread <em>between</em> the means of different classes.
<strong>类间协方差 (<span
class="math inline">\(\hat{\Sigma}_B\)</span>)</strong>：衡量不同类别均值<em>之间</em>的差异。
<span class="math display">\[\hat{\Sigma}_B = \sum_{k=1}^{K} n_k
(\hat{\mu}_k - \hat{\mu})(\hat{\mu}_k - \hat{\mu})^\top\]</span></li>
<li><strong>Objective Function</strong>: Find the projection vector
<span class="math inline">\(w\)</span> that maximizes the ratio of
between-class variance to within-class variance.
<strong>目标函数</strong>：找到投影向量 <span
class="math inline">\(w\)</span>，使类间方差与类内方差之比最大化。 <span
class="math display">\[\max_w \frac{w^\top \hat{\Sigma}_B w}{w^\top
\hat{\Sigma}_W w}\]</span></li>
</ol>
<h4 id="ldas-main-assumption"><strong>LDA’s Main
Assumption</strong></h4>
<p>The key assumption of LDA is that all classes share the <strong>same
covariance matrix (<span
class="math inline">\(\Sigma\)</span>)</strong>. They can have different
means (<span class="math inline">\(\mu_k\)</span>), but their spread and
orientation must be identical. This assumption is what results in a
linear decision boundary. LDA
的关键假设是所有类别共享<strong>相同的协方差矩阵 (<span
class="math inline">\(\Sigma\)</span>)</strong>。它们可以具有不同的均值
(<span
class="math inline">\(\mu_k\)</span>)，但它们的散度和方向必须相同。正是这一假设导致了线性决策边界。</p>
<h3 id="quadratic-discriminant-analysis-qda">## Quadratic Discriminant
Analysis (QDA)</h3>
<p>QDA is a more flexible extension of LDA that creates a
<strong>quadratic</strong> (curved) decision boundary. QDA 是 LDA
的更灵活的扩展，它创建了<strong>二次</strong>（曲线）决策边界。 ####
<strong>Core Idea &amp; Key Assumption</strong></p>
<p>QDA starts with the same principles as LDA but drops the key
assumption. QDA assumes that <strong>each class has its own unique
covariance matrix (<span
class="math inline">\(\Sigma_k\)</span>)</strong>. QDA 的原理与 LDA
相同，但放弃了关键假设。QDA 假设<strong>每个类别都有自己独特的协方差矩阵
(<span class="math inline">\(\Sigma_k\)</span>)</strong>。</p>
<p>This means each class can have its own spread, shape, and
orientation. This additional flexibility allows for a more complex,
curved decision boundary.
这意味着每个类别可以拥有自己的散度、形状和方向。这种额外的灵活性使得决策边界更加复杂、曲线化。</p>
<h4 id="key-mathematical-formula"><strong>Key Mathematical
Formula</strong></h4>
<p>The classification is made using a discrimination function, <span
class="math inline">\(\delta_k(x)\)</span>. We assign a data point <span
class="math inline">\(x\)</span> to the class <span
class="math inline">\(k\)</span> for which <span
class="math inline">\(\delta_k(x)\)</span> is largest. The function for
QDA is: <span class="math display">\[\delta_k(x) = -\frac{1}{2}(x -
\mu_k)^\top \Sigma_k^{-1}(x - \mu_k) - \frac{1}{2}\log(|\Sigma_k|) +
\log \pi_k\]</span> The term containing <span
class="math inline">\(x^\top \Sigma_k^{-1} x\)</span> makes this
function a <strong>quadratic</strong> function of <span
class="math inline">\(x\)</span>.</p>
<h3 id="lda-vs.-qda-the-trade-off">## LDA vs. QDA: The Trade-Off</h3>
<p>The choice between LDA and QDA is a classic <strong>bias-variance
trade-off</strong>. 在 LDA 和 QDA
之间进行选择是典型的<strong>偏差-方差权衡</strong>。</p>
<ul>
<li><p><strong>Use LDA when:</strong></p>
<ul>
<li>The assumption of a common covariance matrix is reasonable (the
classes have similar shapes).</li>
<li>You have a small amount of training data, as LDA is less prone to
overfitting.</li>
<li>Simplicity is preferred. LDA is less flexible (high bias) but has
lower variance.</li>
<li>假设共同协方差矩阵是合理的（类别具有相似的形状）。</li>
<li>训练数据量较少，因为 LDA 不易过拟合。</li>
<li>简洁是首选。LDA 灵活性较差（偏差较大），但方差较小。</li>
</ul></li>
<li><p><strong>Use QDA when:</strong></p>
<ul>
<li>The classes have clearly different shapes and spreads (different
covariance matrices).</li>
<li>You have a large amount of training data to properly estimate the
separate covariance matrices for each class.</li>
<li>QDA is more flexible (low bias) but can have high variance, meaning
it might overfit on smaller datasets.</li>
<li>类别具有明显不同的形状和分布（不同的协方差矩阵）。</li>
<li>拥有大量训练数据，可以正确估计每个类别的独立协方差矩阵。</li>
<li>QDA
更灵活（偏差较小），但方差较大，这意味着它可能在较小的数据集上过拟合。
<strong>Rule of Thumb:</strong> If the class variances are equal or
close, LDA is better. Otherwise, QDA is better.
<strong>经验法则：</strong> 如果类别方差相等或接近，则 LDA
更佳。否则，QDA 更好。</li>
</ul></li>
</ul>
<h3 id="code-understanding-python-equivalent">## Code Understanding
(Python Equivalent)</h3>
<p>The slides show code in R. Here’s how you would perform LDA and
evaluate it in Python using the popular <code>scikit-learn</code>
library.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, accuracy_score, roc_curve, auc</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;df&#x27; is your DataFrame with features and a &#x27;target&#x27; column</span></span><br><span class="line"><span class="comment"># X = df.drop(&#x27;target&#x27;, axis=1)</span></span><br><span class="line"><span class="comment"># y = df[&#x27;target&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit an LDA model (equivalent to lda() in R)</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line">lda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Make predictions (equivalent to predict() in R)</span></span><br><span class="line">y_pred_lda = lda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># To fit a QDA model, the process is identical:</span></span><br><span class="line"><span class="comment"># qda = QuadraticDiscriminantAnalysis()</span></span><br><span class="line"><span class="comment"># qda.fit(X_train, y_train)</span></span><br><span class="line"><span class="comment"># y_pred_qda = qda.predict(X_test)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Create a confusion matrix (equivalent to table())</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LDA Confusion Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, y_pred_lda))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Plot the ROC Curve (equivalent to the R code for ROC)</span></span><br><span class="line"><span class="comment"># Get prediction probabilities for the positive class</span></span><br><span class="line">y_pred_proba = lda.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate ROC curve points</span></span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate Area Under the Curve (AUC)</span></span><br><span class="line">roc_auc = auc(fpr, tpr)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(fpr, tpr, color=<span class="string">&#x27;blue&#x27;</span>, lw=<span class="number">2</span>, label=<span class="string">f&#x27;LDA ROC curve (area = <span class="subst">&#123;roc_auc:<span class="number">.2</span>f&#125;</span>)&#x27;</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">&#x27;gray&#x27;</span>, lw=<span class="number">2</span>, linestyle=<span class="string">&#x27;--&#x27;</span>) <span class="comment"># Random guess line</span></span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate (1 - Specificity)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate (Sensitivity)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Receiver Operating Characteristic (ROC) Curve&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="understanding-the-roc-curve"><strong>Understanding the ROC
Curve</strong></h4>
<p>The <strong>ROC Curve</strong> is another important image. It helps
you visualize a classifier’s performance across all possible
classification thresholds. <strong>ROC 曲线</strong>
是另一个重要的图像。它可以帮助您直观地了解分类器在所有可能的分类阈值下的性能。</p>
<ul>
<li>The <strong>Y-axis</strong> is the <strong>True Positive
Rate</strong> (Sensitivity): “Of all the actual positives, how many did
we correctly identify?”</li>
<li>The <strong>X-axis</strong> is the <strong>False Positive
Rate</strong>: “Of all the actual negatives, how many did we incorrectly
label as positive?”</li>
<li>A perfect classifier would have a curve that goes straight up to the
top-left corner (100% TPR, 0% FPR). The diagonal line represents a
random guess. The <strong>Area Under the Curve (AUC)</strong> summarizes
the model’s performance; a value closer to 1.0 is better.</li>
<li><strong>Y 轴</strong>
表示<strong>真阳性率</strong>（敏感度）：“在所有实际的阳性样本中，我们正确识别了多少个？”</li>
<li><strong>X 轴</strong>
表示<strong>假阳性率</strong>：“在所有实际的阴性样本中，我们错误地将多少个标记为阳性？”</li>
<li>一个完美的分类器应该有一条直线上升到左上角的曲线（真阳性率
100%，假阳性率 0%）。对角线表示随机猜测。<strong>曲线下面积
(AUC)</strong> 概括了模型的性能；该值越接近 1.0 越好。</li>
</ul>
<h1
id="here-is-a-summary-of-the-provided-slides-on-quadratic-discriminant-analysis-qda-including-the-key-formulas-code-explanations-with-python-equivalents-and-a-guide-to-the-most-important-images.">8.
Here is a summary of the provided slides on Quadratic Discriminant
Analysis (QDA), including the key formulas, code explanations with
Python equivalents, and a guide to the most important images.</h1>
<h3 id="core-concept-qda-vs.-lda">## Core Concept: QDA vs. LDA</h3>
<p>The main difference between <strong>Linear Discriminant Analysis
(LDA)</strong> and <strong>Quadratic Discriminant Analysis
(QDA)</strong> lies in their assumptions about the data.
<strong>线性判别分析 (LDA)</strong> 和 <strong>二次判别分析
(QDA)</strong> 的主要区别在于它们对数据的假设。 * <strong>LDA</strong>
assumes that all classes share the <strong>same covariance
matrix</strong> (<span class="math inline">\(\Sigma\)</span>). It models
each class as a normal distribution with a different mean (<span
class="math inline">\(\mu_k\)</span>) but the same shape and
orientation. This results in a <em>linear</em> decision boundary between
classes. 假设所有类别共享<strong>相同的协方差矩阵</strong> (<span
class="math inline">\(\Sigma\)</span>)。它将每个类别建模为均值不同
(<span class="math inline">\(\mu_k\)</span>)
但形状和方向相同的正态分布。这会导致类别之间出现 <em>线性</em>
决策边界。 * <strong>QDA</strong> is more flexible. It assumes that each
class <span class="math inline">\(k\)</span> has its <strong>own,
separate covariance matrix</strong> (<span
class="math inline">\(\Sigma_k\)</span>). This allows each class’s
distribution to have a unique shape, size, and orientation. This
flexibility results in a <em>quadratic</em> decision boundary (like a
parabola, hyperbola, or ellipse). 更灵活。它假设每个类别 <span
class="math inline">\(k\)</span> 都有其<strong>独立的协方差矩阵</strong>
(<span
class="math inline">\(\Sigma_k\)</span>)。这使得每个类别的分布具有独特的形状、大小和方向。这种灵活性导致了<em>二次</em>决策边界（类似于抛物线、双曲线或椭圆）。
<strong>Analogy</strong> 💡: Imagine you’re drawing boundaries around
different clusters of stars. LDA gives you only straight lines to
separate the clusters. QDA gives you curved lines (circles, ellipses),
which can create a much better fit if the clusters themselves are
elliptical and point in different directions.
想象一下，你正在围绕不同的星团绘制边界。LDA 只提供直线来分隔星团。QDA
提供曲线（圆形、椭圆形），如果星团本身是椭圆形且指向不同的方向，则可以产生更好的拟合效果。</p>
<h3 id="the-math-behind-qda">## The Math Behind QDA</h3>
<p>QDA classifies a new observation <span
class="math inline">\(x\)</span> to the class <span
class="math inline">\(k\)</span> that has the highest discriminant
score, <span class="math inline">\(\delta_k(x)\)</span>. The formula for
this score is what makes the boundary quadratic. QDA 将新的观测值 <span
class="math inline">\(x\)</span> 归类到具有最高判别分数 <span
class="math inline">\(\delta_k(x)\)</span> 的类 <span
class="math inline">\(k\)</span>
中。该分数的公式使得边界具有二次项。</p>
<p>The discriminant function for class <span
class="math inline">\(k\)</span> is: <span
class="math display">\[\delta_k(x) = -\frac{1}{2}(x - \mu_k)^T
\Sigma_k^{-1}(x - \mu_k) - \frac{1}{2}\log(|\Sigma_k|) +
\log(\pi_k)\]</span></p>
<p>Let’s break it down:</p>
<ul>
<li><span class="math inline">\((x - \mu_k)^T \Sigma_k^{-1}(x -
\mu_k)\)</span>: This is a quadratic term (since it involves <span
class="math inline">\(x^T \Sigma_k^{-1} x\)</span>). It measures the
squared Mahalanobis distance from <span class="math inline">\(x\)</span>
to the class mean <span class="math inline">\(\mu_k\)</span>, scaled by
that class’s specific covariance <span
class="math inline">\(\Sigma_k\)</span>.</li>
<li><span class="math inline">\(\log(|\Sigma_k|)\)</span>: A term that
penalizes classes with larger variance.</li>
<li><span class="math inline">\(\log(\pi_k)\)</span>: The prior
probability of class <span class="math inline">\(k\)</span>. This is our
initial belief about how likely class <span
class="math inline">\(k\)</span> is, before seeing the data.
<ul>
<li><span class="math inline">\((x - \mu_k)^T \Sigma_k^{-1}(x -
\mu_k)\)</span>：这是一个二次项（因为它涉及 <span
class="math inline">\(x^T \Sigma_k^{-1} x\)</span>）。它测量从 <span
class="math inline">\(x\)</span> 到类均值 <span
class="math inline">\(\mu_k\)</span>
的平方马氏距离，并根据该类的特定协方差 <span
class="math inline">\(\Sigma_k\)</span> 进行缩放。</li>
<li><span
class="math inline">\(\log(|\Sigma_k|)\)</span>：用于惩罚方差较大的类的项。</li>
<li><span class="math inline">\(\log(\pi_k)\)</span>：类 <span
class="math inline">\(k\)</span> 的先验概率。这是我们在看到数据之前对类
<span class="math inline">\(k\)</span> 可能性的初始信念。 Because each
class <span class="math inline">\(k\)</span> has its own <span
class="math inline">\(\Sigma_k\)</span>, the quadratic term doesn’t
cancel out when comparing scores between classes, leading to a quadratic
boundary. 由于每个类 <span class="math inline">\(k\)</span> 都有其自己的
<span
class="math inline">\(\Sigma_k\)</span>，因此在比较类之间的分数时，二次项不会抵消，从而导致二次边界。
<strong>Key Trade-off</strong>:</li>
</ul></li>
<li>If the class variances (<span
class="math inline">\(\Sigma_k\)</span>) are truly different,
<strong>QDA is better</strong>.</li>
<li>If the class variances are similar, <strong>LDA is often
better</strong> because it’s less flexible and less likely to overfit,
especially with a small number of training samples.</li>
<li>如果类方差 (<span class="math inline">\(\Sigma_k\)</span>)
确实不同，<strong>QDA 更好</strong>。</li>
<li>如果类方差相似，<strong>LDA
通常更好</strong>，因为它的灵活性较差，并且不太可能过拟合，尤其是在训练样本数量较少的情况下。</li>
</ul>
<h3 id="code-implementation-r-and-python">## Code Implementation: R and
Python</h3>
<p>The slides provide R code for fitting a QDA model and evaluating it.
Below is an explanation of the R code and its equivalent in Python using
the popular <code>scikit-learn</code> library.</p>
<h4 id="r-code-from-the-slides">R Code (from the slides)</h4>
<p>The code uses the <code>MASS</code> library for QDA and the
<code>ROCR</code> library for evaluation.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ######## QDA ##########</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Fit the model on the training data</span></span><br><span class="line"><span class="comment"># This formula `Default~.` means &quot;predict &#x27;Default&#x27; using all other variables&quot;.</span></span><br><span class="line">qda.fit.mod2 <span class="operator">&lt;-</span> qda<span class="punctuation">(</span>Default<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> subset<span class="operator">=</span>train.ids<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Make predictions on the test data</span></span><br><span class="line"><span class="comment"># We are interested in the posterior probabilities for the ROC curve</span></span><br><span class="line">qda.fit.pred3 <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>qda.fit.mod2<span class="punctuation">,</span> Default_test<span class="punctuation">)</span><span class="operator">$</span>posterior<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Evaluate using ROC and AUC</span></span><br><span class="line"><span class="comment"># &#x27;prediction&#x27; and &#x27;performance&#x27; are functions from the ROCR library</span></span><br><span class="line">perf <span class="operator">&lt;-</span> performance<span class="punctuation">(</span>prediction<span class="punctuation">(</span>qda.fit.pred3<span class="punctuation">,</span> Default_test<span class="operator">$</span>Default<span class="punctuation">)</span><span class="punctuation">,</span> <span class="string">&quot;auc&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Get the AUC value</span></span><br><span class="line">auc_value <span class="operator">&lt;-</span> perf<span class="operator">@</span>y.values<span class="punctuation">[[</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">]</span></span><br><span class="line"><span class="comment"># Result from slide: 0.9638683</span></span><br></pre></td></tr></table></figure>
<h4 id="python-equivalent-scikit-learn">Python Equivalent
(<code>scikit-learn</code>)</h4>
<p>Here’s how you would perform the same steps in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> QuadraticDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score, roc_curve</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Default&#x27; is your DataFrame and &#x27;default&#x27; is the target column</span></span><br><span class="line"><span class="comment"># (preprocessing &#x27;student&#x27; and &#x27;default&#x27; columns to numbers)</span></span><br><span class="line"><span class="comment"># Default[&#x27;default_num&#x27;] = Default[&#x27;default&#x27;].apply(lambda x: 1 if x == &#x27;Yes&#x27; else 0)</span></span><br><span class="line"><span class="comment"># X = Default[[&#x27;balance&#x27;, &#x27;income&#x27;, ...]]</span></span><br><span class="line"><span class="comment"># y = Default[&#x27;default_num&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Initialize and fit the QDA model</span></span><br><span class="line">qda = QuadraticDiscriminantAnalysis()</span><br><span class="line">qda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Predict probabilities on the test set</span></span><br><span class="line"><span class="comment"># We need the probability of the positive class (&#x27;Yes&#x27;) for the AUC calculation</span></span><br><span class="line">y_pred_proba = qda.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Calculate the AUC score</span></span><br><span class="line">auc_score = roc_auc_score(y_test, y_pred_proba)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;AUC Score for QDA: <span class="subst">&#123;auc_score:<span class="number">.7</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also plot the ROC curve</span></span><br><span class="line"><span class="comment"># fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)</span></span><br><span class="line"><span class="comment"># plt.plot(fpr, tpr)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure>
<h3 id="model-evaluation-roc-and-auc">## Model Evaluation: ROC and
AUC</h3>
<p>The slides correctly emphasize using the <strong>ROC curve</strong>
and the <strong>Area Under the Curve (AUC)</strong> to compare model
performance.</p>
<ul>
<li><p><strong>ROC Curve (Receiver Operating Characteristic)</strong>:
This plot shows how well a model can distinguish between two classes. It
plots the <strong>True Positive Rate</strong> (y-axis) against the
<strong>False Positive Rate</strong> (x-axis) at all possible
classification thresholds. A better model has a curve that is closer to
the top-left corner.</p></li>
<li><p><strong>AUC (Area Under the Curve)</strong>: This is a single
number that summarizes the entire ROC curve.</p>
<ul>
<li><strong>AUC = 1</strong>: Perfect classifier.</li>
<li><strong>AUC = 0.5</strong>: A useless classifier (equivalent to
random guessing).</li>
<li><strong>AUC &gt; 0.7</strong>: Generally considered an acceptable
model.</li>
</ul></li>
<li><p><strong>ROC
曲线（接收者操作特征）</strong>：此图显示了模型区分两个类别的能力。它绘制了所有可能的分类阈值下的
<strong>真阳性率</strong>（y 轴）与 <strong>假阳性率</strong>（x
轴）的对比图。更好的模型的曲线越靠近左上角，效果就越好。</p>
<ul>
<li><p><strong>AUC（曲线下面积）</strong>：这是一个概括整个 ROC
曲线的数值。</p></li>
<li><p><strong>AUC = 1</strong>：完美的分类器。</p></li>
<li><p><strong>AUC =
0.5</strong>：无用的分类器（相当于随机猜测）。</p></li>
<li><p><strong>AUC &gt;
0.7</strong>：通常被认为是可接受的模型。</p></li>
</ul></li>
</ul>
<p>The slides show that for the <code>Default</code> dataset,
<strong>LDA’s AUC (0.9647) was slightly higher than QDA’s
(0.9639)</strong>. This suggests that the assumption of a common
covariance matrix (LDA) was a slightly better fit for this particular
test set, possibly because QDA’s extra flexibility wasn’t needed and it
may have slightly overfit the training data.
这表明，对于这个特定的测试集，公共协方差矩阵 (LDA)
的假设拟合度略高，可能是因为 QDA
的额外灵活性并非必需，并且可能对训练数据略微过拟合。</p>
<h3 id="key-takeaways-and-important-images">## Key Takeaways and
Important Images</h3>
<h3
id="heres-a-ranking-of-the-most-important-visual-aids-in-your-slides">Here’s
a ranking of the most important visual aids in your slides:</h3>
<ol type="1">
<li><p><strong>Slide 68/69 (Model Assumption &amp; Formula)</strong>:
These are the <strong>most critical</strong> slides. They present the
core theoretical difference between LDA and QDA and provide the
mathematical foundation (the discriminant function formula).
Understanding these is key to understanding QDA.</p></li>
<li><p><strong>Slide 73 (ROC Comparison)</strong>: This is the most
important image for <strong>practical evaluation</strong>. It visually
compares the performance of LDA and QDA side-by-side, making it easy to
see which one performs better on this specific dataset. The concept of
AUC is introduced here as the method for comparison.</p></li>
<li><p><strong>Slide 71 (Decision Boundaries with Different
Thresholds)</strong>: This is an excellent conceptual image. It shows
how the quadratic decision boundary (the curved lines) separates the
data points. It also illustrates how changing the probability threshold
(from 0.1 to 0.5 to 0.9) shifts the boundary, trading off between
precision and recall.</p></li>
</ol>
<p>Of course. Here is a summary of the remaining slides, which compare
QDA to other popular classification models like Logistic Regression and
K-Nearest Neighbors (KNN).</p>
<hr />
<h3 id="visualizing-the-core-trade-off-lda-vs.-qda">Visualizing the Core
Trade-off: LDA vs. QDA</h3>
<p>This is the most important concept in these slides. The choice
between LDA and QDA depends entirely on the underlying structure of your
data.</p>
<p>The slide shows two scenarios: 1. <strong>Left Plot (<span
class="math inline">\(\Sigma_1 = \Sigma_2\)</span>):</strong> When the
true covariance matrices of the classes are the same, the optimal
decision boundary (the Bayes classifier) is a straight line. LDA, which
assumes equal covariances, creates a linear boundary that approximates
this optimal boundary very well. QDA’s flexible, curved boundary is
unnecessarily complex and might overfit the training data. <strong>In
this case, LDA is better.</strong> 2. <strong>Right Plot (<span
class="math inline">\(\Sigma_1 \neq \Sigma_2\)</span>):</strong> When
the true covariance matrices are different, the optimal decision
boundary is a curve. QDA’s quadratic model can capture this
non-linearity much better than LDA’s rigid linear model. <strong>In this
case, QDA is better.</strong></p>
<p>This perfectly illustrates the <strong>bias-variance
tradeoff</strong>. LDA has higher bias (it’s less flexible) but lower
variance. QDA has lower bias (it’s more flexible) but higher
variance.</p>
<hr />
<h3 id="comparing-performance-on-the-default-dataset">Comparing
Performance on the “Default” Dataset</h3>
<p>The slides compare four different models on the same classification
task. Let’s look at their performance using the <strong>Area Under the
Curve (AUC)</strong>, where a higher score is better.</p>
<ul>
<li><strong>LDA AUC:</strong> 0.9647</li>
<li><strong>QDA AUC:</strong> 0.9639</li>
<li><strong>Logistic Regression AUC:</strong> 0.9645</li>
<li><strong>K-Nearest Neighbors (KNN):</strong> The plot shows test
error vs. K. The error is lowest around K=4, but it’s not directly
converted to an AUC score in the slides.</li>
</ul>
<p>Interestingly, for this particular dataset, LDA, QDA, and Logistic
Regression perform almost identically. This suggests that the decision
boundary for this problem is likely very close to linear, meaning the
extra flexibility of QDA isn’t providing much benefit.</p>
<hr />
<h3 id="pros-and-cons-which-model-to-choose">Pros and Cons: Which Model
to Choose?</h3>
<p>The final slide asks for a comparison of the models. Here’s a summary
of their key characteristics:</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Decision Boundary</th>
<th style="text-align: left;">Key Pro</th>
<th style="text-align: left;">Key Con</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Logistic Regression</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">Highly interpretable, no strong
assumptions about data distribution.</td>
<td style="text-align: left;">Inflexible; cannot capture non-linear
relationships.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Linear Discriminant Analysis
(LDA)</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">More stable than Logistic Regression when
classes are well-separated.</td>
<td style="text-align: left;">Assumes data is normally distributed with
equal covariance matrices for all classes.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Quadratic Discriminant Analysis
(QDA)</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Quadratic (Curved)</td>
<td style="text-align: left;">More flexible than LDA; can model
non-linear boundaries.</td>
<td style="text-align: left;">Requires more data to estimate parameters
and is more prone to overfitting. Assumes normality.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>K-Nearest Neighbors
(KNN)</strong></td>
<td style="text-align: left;">Non-Parametric</td>
<td style="text-align: left;">Highly Non-linear</td>
<td style="text-align: left;">Extremely flexible; makes no assumptions
about the data’s distribution.</td>
<td style="text-align: left;">Can be slow on large datasets and suffers
from the “curse of dimensionality.” Less interpretable.</td>
</tr>
</tbody>
</table>
<h4 id="summary-of-the-comparison">Summary of the Comparison:</h4>
<ul>
<li><strong>Linear Models (Logistic Regression &amp; LDA):</strong>
Choose these for simplicity, interpretability, and when you believe the
relationship between predictors and the class is linear. LDA often
outperforms Logistic Regression if its normality assumptions are
met.</li>
<li><strong>Non-Linear Models (QDA &amp; KNN):</strong> Choose these
when the decision boundary is likely more complex. QDA is a good middle
ground, offering more flexibility than LDA without being as completely
data-driven as KNN. KNN is the most flexible but requires careful tuning
of the parameter K to avoid overfitting or underfitting.</li>
</ul>
<h1
id="here-is-a-more-detailed-slide-by-slide-analysis-of-the-presentation.">9.
Here is a more detailed, slide-by-slide analysis of the
presentation.</h1>
<h2 id="four-classification-methods-comparison-by-simulation">4.6 Four
Classification Methods: Comparison by Simulation</h2>
<p>This section (slides 81-87) introduces four classification methods
and systematically compares their performance on six different simulated
datasets. The goal is to see which method works best under different
conditions (e.g., linear vs. non-linear boundaries, normal
vs. non-normal data).</p>
<p>The four methods being compared are: * <strong>Logistic
Regression:</strong> A linear method that models the log-odds as a
linear function of the predictors. * <strong>Linear Discriminant
Analysis (LDA):</strong> Another linear method. It also assumes a linear
decision boundary but makes stronger assumptions than logistic
regression (e.g., that data within each class is normally distributed
with a common covariance matrix). * <strong>Quadratic Discriminant
Analysis (QDA):</strong> A non-linear method. It assumes the log-odds
are a <em>quadratic</em> function, which creates a more flexible, curved
decision boundary. It assumes data within each class is normally
distributed, but <em>without</em> a common covariance matrix. *
<strong>K-Nearest Neighbors (KNN):</strong> A non-parametric, highly
flexible method. Two versions are tested: * <strong>KNN-1 (<span
class="math inline">\(K=1\)</span>):</strong> A very flexible (high
variance) model. * <strong>KNN-CV:</strong> A tuned model where the best
<span class="math inline">\(K\)</span> is chosen via
cross-validation.</p>
<p>比较的四种方法是： *
<strong>逻辑回归</strong>：一种将对数概率建模为预测变量线性函数的线性方法。
* <strong>线性判别分析
(LDA)</strong>：另一种线性方法。它也假设线性决策边界，但比逻辑回归做出更强的假设（例如，每个类中的数据呈正态分布，且具有共同的协方差矩阵）。
* <strong>二次判别分析
(QDA)</strong>：一种非线性方法。它假设对数概率为<em>二次</em>函数，从而创建一个更灵活、更弯曲的决策边界。它假设每个类中的数据服从正态分布，但<em>没有</em>共同的协方差矩阵。
* <strong>K最近邻
(KNN)</strong>：一种非参数化、高度灵活的方法。测试了两个版本： *
<strong>KNN-1 (<span
class="math inline">\(K=1\)</span>)</strong>：一个非常灵活（高方差）的模型。
*
<strong>KNN-CV</strong>：一个经过调整的模型，通过交叉验证选择最佳的<span
class="math inline">\(K\)</span>。</p>
<h3 id="analysis-of-simulation-scenarios">Analysis of Simulation
Scenarios</h3>
<p>The performance is measured by the <strong>test error rate</strong>
(lower is better), shown in the boxplots for each scenario.
性能通过<strong>测试错误率</strong>（越低越好）来衡量，每个场景的箱线图都显示了该错误率。</p>
<ul>
<li><strong>Scenario 1 (Slide 82):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>linear</strong> decision boundary.
Data is <strong>normally distributed</strong> with <em>uncorrelated</em>
predictors.</li>
<li><strong>Result:</strong> <strong>LDA and Logistic Regression perform
best</strong>. Their test error rates are low and similar. This is
expected, as the setup perfectly matches their core assumption (linear
boundary). QDA is slightly worse because its extra flexibility (being
quadratic) is unnecessary. KNN-1 is the worst, as its high flexibility
leads to high variance (overfitting).</li>
<li><strong>结果：</strong> <strong>LDA
和逻辑回归表现最佳</strong>。它们的测试错误率较低且相似。这是意料之中的，因为设置完全符合它们的核心假设（线性边界）。QDA
略差，因为其额外的灵活性（二次方）是不必要的。KNN-1
最差，因为其高灵活性导致方差较大（过拟合）。</li>
</ul></li>
<li><strong>Scenario 2 (Slide 83):</strong>
<ul>
<li><strong>Setup:</strong> Same as Scenario 1 (<strong>linear</strong>
boundary, <strong>normal</strong> data), but now the two predictors have
a <strong>correlation of 0.5</strong>.</li>
<li><strong>Result:</strong> <strong>Almost no change</strong> from
Scenario 1. <strong>LDA and Logistic Regression are still the
best</strong>. This shows that these linear methods are robust to
correlation between predictors.</li>
<li><strong>结果：</strong>与场景 1
相比<strong>几乎没有变化</strong>。<strong>LDA
和逻辑回归仍然是最佳</strong>。这表明这些线性方法对预测因子之间的相关性具有鲁棒性。</li>
</ul></li>
<li><strong>Scenario 3 (Slide 84):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>linear</strong> decision boundary,
but the data is drawn from a <strong>t-distribution</strong> (which is
non-normal and has “heavy tails,” or more extreme outliers).</li>
<li><strong>Result:</strong> <strong>Logistic Regression is the clear
winner</strong>. LDA’s performance gets worse because its assumption of
<em>normality</em> is violated by the t-distribution. QDA’s performance
deteriorates significantly due to the non-normality. This highlights a
key difference: logistic regression is more robust to violations of the
normality assumption.</li>
<li><strong>结果：</strong>逻辑回归明显胜出**。LDA 的性能会变差，因为 t
分布违反了其正态性假设。QDA
的性能由于非正态性而显著下降。这凸显了一个关键区别：逻辑回归对违反正态性假设的情况更稳健。</li>
</ul></li>
<li><strong>Scenario 4 (Slide 85):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>quadratic</strong> decision
boundary. Data is <strong>normally distributed</strong> with different
correlations in each class.</li>
<li><strong>Result:</strong> <strong>QDA is the clear winner</strong> by
a large margin. This setup perfectly matches QDA’s assumption (quadratic
boundary from normal data with different covariance structures). All
other methods (LDA, Logistic, KNN) are linear or not flexible enough, so
they perform poorly.</li>
<li><strong>结果：</strong>QDA 明显胜出**，且遥遥领先。此设置完全符合
QDA
的假设（来自具有不同协方差结构的正态数据的二次边界）。所有其他方法（LDA、Logistic、KNN）都是线性的或不够灵活，因此性能不佳。</li>
</ul></li>
<li><strong>Scenario 5 (Slide 86):</strong>
<ul>
<li><strong>Setup:</strong> Another <strong>quadratic</strong> boundary,
but generated in a different way (using a logistic function of quadratic
terms).</li>
<li><strong>Result:</strong> <strong>QDA performs best again</strong>,
closely followed by the flexible <strong>KNN-CV</strong>. The linear
methods (LDA, Logistic) have poor performance because they cannot
capture the curve.</li>
<li><strong>结果：QDA
再次表现最佳</strong>，紧随其后的是灵活的<strong>KNN-CV</strong>。线性方法（LDA、Logistic）性能较差，因为它们无法捕捉曲线。</li>
</ul></li>
<li><strong>Scenario 6 (Slide 87):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>complex, non-linear</strong>
decision boundary (more complex than a simple quadratic curve).</li>
<li><strong>Result:</strong> The <strong>flexible KNN-CV method is the
winner</strong>. Its non-parametric nature allows it to approximate the
complex shape. QDA is not flexible <em>enough</em> and performs worse.
This slide highlights the bias-variance trade-off: the overly simple
KNN-1 is the worst, but the <em>tuned</em> KNN-CV is the best.</li>
<li><strong>结果：</strong>灵活的 KNN-CV
方法胜出**。其非参数特性使其能够近似复杂的形状。 QDA
不够灵活，性能较差。这张幻灯片重点介绍了偏差-方差权衡：过于简单的 KNN-1
最差，而 <em>调整后的</em> KNN-CV 最好。</li>
</ul></li>
</ul>
<h2 id="r-example-on-smarket-data">4.7 R Example on Smarket Data</h2>
<p>This section (slides 88-93) applies Logistic Regression and LDA to
the <code>Smarket</code> dataset from the <code>ISLR</code> package to
predict the stock market’s <code>Direction</code> (Up or Down).
本节（幻灯片 88-93）将逻辑回归和 LDA
应用于“ISLR”包中的“Smarket”数据集，以预测股市的“方向”（上涨或下跌）。
### Data Preparation (Slides 88, 89, 90)</p>
<ol type="1">
<li><strong>Load Data:</strong> The <code>ISLR</code> library is loaded,
and the <code>Smarket</code> dataset is explored. It contains daily
percentage returns (<code>Lag1</code>…<code>Lag5</code> for the previous
5 days, <code>Today</code>), <code>Volume</code>, and the
<code>Year</code>.</li>
<li><strong>Explore Data:</strong> A correlation matrix
(<code>cor(Smarket[,-9])</code>) is computed, and a plot of
<code>Volume</code> over time is generated.</li>
<li><strong>Split Data:</strong> The data is split into a training set
(Years 2001-2004) and a test set (Year 2005).
<ul>
<li><code>train &lt;- (Year&lt;2005)</code></li>
<li><code>Smarket.2005 &lt;- Smarket[!train,]</code></li>
<li><code>Direction.2005 &lt;- Direction[!train]</code></li>
<li>The test set has 252 observations.</li>
</ul></li>
<li><strong>加载数据</strong>：加载“ISLR”库，并探索“Smarket”数据集。该数据集包含每日百分比收益率（前
5 天的“Lag1”…“Lag5”，“今日”）、“成交量”和“年份”。</li>
<li><strong>探索数据</strong>：计算相关矩阵
(<code>cor(Smarket[,-9])</code>)，并生成“成交量”随时间变化的图表。</li>
<li><strong>拆分数据</strong>：将数据拆分为训练集（年份
2001-2004）和测试集（年份 2005）。
<ul>
<li><code>train &lt;- (Year&lt;2005)</code></li>
<li><code>Smarket.2005 &lt;- Smarket[!train,]</code></li>
<li><code>Direction.2005 &lt;- Direction[!train]</code></li>
<li>测试集包含 252 个观测值。</li>
</ul></li>
</ol>
<h3 id="model-1-logistic-regression-all-predictors-slide-90">Model 1:
Logistic Regression (All Predictors) (Slide 90)</h3>
<ul>
<li><strong>Model:</strong> A logistic regression model is fit on the
training data using <em>all</em> predictors.
<ul>
<li><code>glm.fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data=Smarket, family=binomial, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> The model is used to predict the
direction for the 2005 test data.
<ul>
<li><code>glm.probs &lt;- predict(glm.fit, Smarket.2005, type="response")</code></li>
<li>A threshold of 0.5 is used to classify: if <span
class="math inline">\(P(\text{Up}) &gt; 0.5\)</span>, predict “Up”.</li>
</ul></li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.5198 (or <strong>48.0%
accuracy</strong>).</li>
<li><strong>Conclusion:</strong> This is “not good!”—it’s worse than
flipping a coin. This suggests the model is either too complex or the
predictors are not useful.</li>
</ul></li>
</ul>
<h3 id="model-2-logistic-regression-lag1-lag2-slide-91">Model 2:
Logistic Regression (Lag1 &amp; Lag2) (Slide 91)</h3>
<ul>
<li><strong>Model:</strong> Based on the poor results, a simpler model
is tried, using only <code>Lag1</code> and <code>Lag2</code>.
<ul>
<li><code>glm.fit &lt;- glm(Direction ~ Lag1 + Lag2, data=Smarket, family=binomial, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> Predictions are made on the 2005 test
set.</li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.4404 (or <strong>55.95%
accuracy</strong>). This is an improvement.</li>
<li><strong>Confusion Matrix:</strong> | | True Down | True Up | | :— |
:— | :— | | <strong>Pred Down</strong> | 77 | 69 | | <strong>Pred
Up</strong> | 35 | 71 |</li>
<li><strong>ROC and AUC:</strong> The ROC (Receiver Operating
Characteristic) curve is plotted, and the AUC (Area Under the Curve) is
calculated.</li>
<li><strong>AUC Value:</strong> <strong>0.5584</strong>. This is very
close to 0.5 (which represents a random-chance model), indicating that
the model has very weak predictive power, even though its accuracy is
above 50%.</li>
</ul></li>
</ul>
<h3 id="model-3-lda-lag1-lag2-slide-92">Model 3: LDA (Lag1 &amp; Lag2)
(Slide 92)</h3>
<ul>
<li><strong>Model:</strong> LDA is now performed using the same setup:
<code>Lag1</code> and <code>Lag2</code> as predictors, trained on the
2001-2004 data.
<ul>
<li><code>library(MASS)</code></li>
<li><code>lda.fit &lt;- lda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> Predictions are made on the 2005 test
set.
<ul>
<li><code>lda.pred &lt;- predict(lda.fit, Smarket.2005)</code></li>
</ul></li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.4404 (or <strong>55.95%
accuracy</strong>).</li>
<li><strong>Confusion Matrix:</strong> | | True Down | True Up | | :— |
:— | :— | | <strong>Pred Down</strong> | 77 | 69 | | <strong>Pred
Up</strong> | 35 | 71 |</li>
<li><strong>Observation:</strong> The confusion matrix and accuracy are
<em>identical</em> to the logistic regression model.</li>
</ul></li>
</ul>
<h3 id="final-comparison-slide-93">Final Comparison (Slide 93)</h3>
<ul>
<li><strong>ROC and AUC for LDA:</strong> The ROC curve for the LDA
model is plotted.</li>
<li><strong>AUC Value:</strong> <strong>0.5584</strong>.</li>
<li><strong>Main Conclusion:</strong> As highlighted in the green box,
<strong>“LDA has identical performance as Logistic regression!”</strong>
In this specific practical example, using these two predictors, both
linear methods produce the exact same confusion matrix, the same
accuracy (56%), and the same AUC (0.558). This reinforces the
theoretical idea that both are fitting a linear boundary.</li>
</ul>
<h3 id="最终比较幻灯片-93">最终比较（幻灯片 93）</h3>
<ul>
<li><strong>LDA 的 ROC 和 AUC：</strong>绘制了 LDA 模型的 ROC
曲线。</li>
<li><strong>AUC 值：</strong>0.5584**。</li>
<li><strong>主要结论：</strong>如绿色方框所示，“LDA 的性能与 Logistic
回归相同！”**
在这个具体的实际示例中，使用这两个预测变量，两种线性方法都产生了完全相同的混淆矩阵、相同的准确率（56%）和相同的
AUC（0.558）。这强化了两者均拟合线性边界的理论观点。</li>
</ul>
<h2 id="r-example-on-smarket-data-continued">4.7 R Example on Smarket
Data (Continued)</h2>
<p>The previous slides showed that Logistic Regression and Linear
Discriminant Analysis (LDA) had <strong>identical performance</strong>
on the Smarket dataset (using <code>Lag1</code> and <code>Lag2</code>),
both achieving 56% test accuracy and an AUC of 0.558. The analysis now
tests a more flexible method, QDA.</p>
<h3 id="model-3-qda-lag1-lag2-slides-94-95">Model 3: QDA (Lag1 &amp;
Lag2) (Slides 94-95)</h3>
<ul>
<li><strong>Model:</strong> A Quadratic Discriminant Analysis (QDA)
model is fit on the same training data (2001-2004) using only the
<code>Lag1</code> and <code>Lag2</code> predictors.
<ul>
<li><code>qda.fit &lt;- qda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> The model is used to predict the market
direction for the 2005 test set.</li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Accuracy:</strong> The model achieves a test accuracy
of <strong>0.5992 (or 60%)</strong>.</li>
<li><strong>AUC:</strong> The Area Under the Curve (AUC) for the QDA
model is <strong>0.562</strong>.</li>
</ul></li>
<li><strong>Conclusion:</strong> As the slide highlights, <strong>“QDA
has better test performance than LDA and Logistic
regression!”</strong></li>
</ul>
<h3 id="smarket-example-summary">Smarket Example Summary</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Model Type</th>
<th style="text-align: left;">Test Accuracy</th>
<th style="text-align: left;">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Logistic Regression</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">~56%</td>
<td style="text-align: left;">0.558</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LDA</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">~56%</td>
<td style="text-align: left;">0.558</td>
</tr>
<tr>
<td style="text-align: left;"><strong>QDA</strong></td>
<td style="text-align: left;">Quadratic</td>
<td style="text-align: left;"><strong>~60%</strong></td>
<td style="text-align: left;"><strong>0.562</strong></td>
</tr>
</tbody>
</table>
<p>This practical example reinforces the lessons from the simulations
(Section 4.6). The two linear methods (LDA, Logistic) had identical
performance. The more flexible, non-linear QDA model performed better,
suggesting that the true decision boundary between “Up” and “Down”
(based on <code>Lag1</code> and <code>Lag2</code>) is not perfectly
linear.</p>
<h2 id="kernel-lda">4.8 Kernel LDA</h2>
<p>This new section introduces an even more advanced non-linear method,
Kernel LDA.</p>
<h3 id="the-problem-linear-inseparability-slide-97">The Problem: Linear
Inseparability (Slide 97)</h3>
<p>The section starts with a clear visual example. A dataset of two
concentric circles (a “donut” shape) is <strong>linearly
inseparable</strong>. It is impossible to draw a single straight line to
separate the inner (purple) class from the outer (yellow) class.</p>
<h3 id="the-solution-the-kernel-trick-slides-97-99">The Solution: The
Kernel Trick (Slides 97, 99)</h3>
<ol type="1">
<li><strong>Nonlinear Transformation:</strong> The data is “lifted” into
a higher-dimensional <em>feature space</em> using a <strong>nonlinear
transformation</strong>, <span class="math inline">\(x \mapsto
\phi(x)\)</span>. In the example on the slide, the 2D data is
transformed, and in this new space, the two classes <em>become</em>
<strong>linearly separable</strong>.</li>
<li><strong>The “Kernel Trick”:</strong> The main idea (from slide 99)
is that we don’t need to explicitly compute this complex transformation
<span class="math inline">\(\phi(x)\)</span>. LDA (based on Fisher’s
approach) only requires inner products of the data points. The “kernel
trick” allows us to replace the inner product in the high-dimensional
feature space (<span class="math inline">\(x_i^T x_j\)</span>) with a
simple <strong>kernel function</strong>, <span
class="math inline">\(k(x_i, x_j)\)</span>, computed in the original,
low-dimensional space.
<ul>
<li>An example of such a kernel is the <strong>Gaussian (RBF)
kernel</strong>: <span class="math inline">\(k(x_i, x_j) \propto
e^{-\|x_i - x_j\|^2 / \sigma^2}\)</span>.</li>
</ul></li>
</ol>
<h3 id="academic-foundations-slide-98">Academic Foundations (Slide
98)</h3>
<p>This method is based on foundational academic papers that generalized
linear methods using kernels: * <strong>Fisher discriminant analysis
with kernels</strong> (Mika, 1999) * <strong>Generalized Discriminant
Analysis Using a Kernel Approach</strong> (Baudat, 2000) *
<strong>Kernel principal component analysis</strong> (Schölkopf,
1997)</p>
<p>In short, Kernel LDA is an extension of LDA that uses the kernel
trick to find a linear boundary in a high-dimensional feature space,
which corresponds to a highly non-linear boundary in the original
space.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
