<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianyaoblogs.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="TianyaoBlogs">
<meta property="og:url" content="https://tianyaoblogs.github.io/index.html">
<meta property="og:site_name" content="TianyaoBlogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://tianyaoblogs.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>TianyaoBlogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="ÂàáÊç¢ÂØºËà™Ê†è">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TianyaoBlogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>È¶ñÈ°µ</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>ÂΩíÊ°£</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/06/5054C5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/06/5054C5/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">ÂèëË°®‰∫é</span>

              <time title="ÂàõÂª∫Êó∂Èó¥Ôºö2025-10-06 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-06T21:00:00+08:00">2025-10-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Êõ¥Êñ∞‰∫é</span>
                <time title="‰øÆÊîπÊó∂Èó¥Ôºö2025-10-19 20:37:53" itemprop="dateModified" datetime="2025-10-19T20:37:53+08:00">2025-10-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">ÂàÜÁ±ª‰∫é</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>ÁªüËÆ°Êú∫Âô®Â≠¶‰π†Lecture-5</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="resampling">1. Resampling</h1>
<p><strong>Resampling</strong> as a statistical tool to assess the
accuracy of models whose main goal is to estimate the <em>test
error</em> (a model‚Äôs performance on new, unseen data) because the
<em>training error</em> is overly optimistic due to overfitting.</p>
<h2 id="key-concepts">Key Concepts</h2>
<ul>
<li><strong>Resampling:</strong> The process of repeatedly drawing
samples from a dataset. The two main types mentioned are
<strong>Cross-validation</strong> (to estimate model test error) and
<strong>Bootstrap</strong> (to quantify the uncertainty of
estimates).</li>
<li><strong>Data Splitting (Ideal Scenario):</strong> In a ‚Äúdata-rich‚Äù
situation, you split your data into three parts:
<ol type="1">
<li><strong>Training Data:</strong> Used to fit and train the parameters
of various models.</li>
<li><strong>Validation Data:</strong> Used to assess the trained models,
tune hyperparameters (e.g., choose the polynomial degree), and select
the <em>best</em> model. This helps prevent overfitting.</li>
<li><strong>Test Data:</strong> Used <em>only once</em> on the final,
selected model to get an unbiased estimate of its real-world
performance.</li>
</ol></li>
<li><strong>Validation vs.¬†Test Data:</strong> The slides emphasize this
difference (Slide 7). The <strong>validation set</strong> is part of the
model-building and selection process. The <strong>test set</strong> is
kept separate and is only used for the final report card after all
decisions are made.</li>
</ul>
<h2 id="the-validation-set-approach">The Validation Set Approach</h2>
<p>This is the simplest cross-validation method.</p>
<ol type="1">
<li><strong>Split:</strong> The total dataset is randomly divided into
two parts: a <strong>training set</strong> and a <strong>validation
set</strong> (often a 50/50 or 70/30 split).</li>
<li><strong>Train:</strong> Various models are fit <em>only</em> on the
<strong>training set</strong>.</li>
<li><strong>Validate:</strong> The performance of each trained model is
evaluated using the <strong>validation set</strong>.</li>
<li><strong>Select:</strong> The model with the best performance (e.g.,
the lowest error) on the validation set is chosen as the final
model.</li>
</ol>
<h3 id="important-image-schematic-slide-10">Important Image: Schematic
(Slide 10)</h3>
<p>This diagram clearly shows a set of <span
class="math inline">\(n\)</span> observations being randomly split into
a training set (blue, with observations 7, 22, 13) and a validation set
(beige, with observation 91). The model learns from the blue set and is
tested on the beige set.</p>
<h2 id="example-auto-data-formulas-code">Example: Auto Data (Formulas
&amp; Code)</h2>
<p>The slides use the <code>Auto</code> dataset to decide the best
polynomial degree to predict <code>mpg</code> from
<code>horsepower</code>.</p>
<h3 id="mathematical-models">Mathematical Models</h3>
<p>The models being compared are polynomials of different degrees. For
example:</p>
<ul>
<li><strong>Linear:</strong> <span class="math inline">\(mpg = \beta_0 +
\beta_1(horsepower)\)</span></li>
<li><strong>Quadratic:</strong> <span class="math inline">\(mpg =
\beta_0 + \beta_1(horsepower) + \beta_2(horsepower)^2\)</span></li>
<li><strong>Cubic:</strong> <span class="math inline">\(mpg = \beta_0 +
\beta_1(horsepower) + \beta_2(horsepower)^2 +
\beta_3(horsepower)^3\)</span></li>
</ul>
<p>The performance metric used is the <strong>Mean Squared Error
(MSE)</strong> on the validation set: <span
class="math display">\[MSE_{val} = \frac{1}{n_{val}} \sum_{i \in val}
(y_i - \hat{f}(x_i))^2\]</span> where <span
class="math inline">\(n_{val}\)</span> is the number of observations in
the validation set, <span class="math inline">\(y_i\)</span> is the true
<code>mpg</code> value, and <span
class="math inline">\(\hat{f}(x_i)\)</span> is the model‚Äôs prediction
for the <span class="math inline">\(i\)</span>-th observation in the
validation set.</p>
<h3 id="important-image-polynomial-fits-slide-8">Important Image:
Polynomial Fits (Slide 8)</h3>
<p>This plot is crucial. It shows the <code>Auto</code> data with linear
(red), quadratic (green), and cubic (blue) regression lines. * The
<strong>linear fit</strong> is clearly poor. * The <strong>quadratic and
cubic fits</strong> follow the data‚Äôs curve much better. * The inset box
shows the MSE calculated on the <em>full dataset</em> (this is training
MSE): * Linear MSE: ~26.42 * Quadratic MSE: ~21.60 * Cubic MSE: ~21.51
This suggests a non-linear fit is necessary, but it doesn‚Äôt tell us
which one will generalize better.</p>
<h3 id="code-analysis">Code Analysis</h3>
<p>The slides show two different approaches in code:</p>
<p><strong>1. Python Code (Slide 9): Model Selection
Criteria</strong></p>
<ul>
<li><strong>What it does:</strong> This Python code (using
<code>pandas</code> and <code>statsmodels</code>) does <em>not</em>
implement the validation set approach. Instead, it fits polynomial
models (degrees 1 through 5) to the <em>entire</em> dataset.</li>
<li><strong>How it works:</strong> It calculates statistical criteria
like <strong>BIC</strong>, <strong>Mallow‚Äôs <span
class="math inline">\(C_p\)</span></strong>, and <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>. These are mathematical
adjustments to the training error that <em>estimate</em> the test error
without needing a validation set.</li>
<li><strong>Key line (logic):</strong> <code>sm.OLS(y, X).fit()</code>
is used to fit the model, and then metrics like <code>model.bic</code>
and <code>model.rsquared_adj</code> are extracted.</li>
<li><strong>Result:</strong> The table shows that the model with
<code>[horsepower, horsepower2]</code> (quadratic) has the lowest BIC
and <span class="math inline">\(C_p\)</span> values, suggesting it‚Äôs the
best model according to these criteria.</li>
</ul>
<p><strong>2. R Code (Slides 14 &amp; 15): The Validation Set
Approach</strong></p>
<ul>
<li><strong>What it does:</strong> This R code <em>directly
implements</em> the validation set approach described on Slide 13.</li>
<li><strong>How it works:</strong>
<ol type="1">
<li><code>set.seed(...)</code>: Sets a random seed to make the split
reproducible.</li>
<li><code>train=sample(392, 196)</code>: Randomly selects 196 indices
(out of 392) to be the <strong>training set</strong>.</li>
<li><code>lm.fit=lm(mpg~poly(horsepower, 2), ..., subset=train)</code>:
Fits a quadratic model <em>only</em> using the <code>train</code>
data.</li>
<li><code>mean((mpg-predict(lm.fit,Auto))[-train]^2)</code>: This is the
key calculation.
<ul>
<li><code>predict(lm.fit, Auto)</code>: Predicts <code>mpg</code> for
<em>all</em> data.</li>
<li><code>[-train]</code>: Selects only the predictions for the
<strong>validation set</strong> (the data <em>not</em> in
<code>train</code>).</li>
<li><code>mean(...)</code>: Calculates the <strong>MSE on the validation
set</strong>.</li>
</ul></li>
</ol></li>
<li><strong>Result:</strong> The code is run three times with different
seeds (1, 2022, 1997).
<ul>
<li><strong>Seed 1:</strong> Quadratic MSE (18.71) is lowest.</li>
<li><strong>Seed 2022:</strong> Quadratic MSE (19.70) is lowest.</li>
<li><strong>Seed 1997:</strong> Quadratic MSE (19.08) is lowest.</li>
</ul></li>
<li><strong>Main Takeaway:</strong> In all random splits, the
<strong>quadratic model gives the lowest validation set MSE</strong>.
This provides evidence that the quadratic model is the best choice for
generalizing to new data. The fact that the MSE values change with each
seed also highlights a key <em>disadvantage</em> of this simple method:
the results can be variable depending on the random split.</li>
</ul>
<h1 id="the-validation-set-approach-1">2. The Validation Set
Approach</h1>
<p>This method is a simple way to estimate a model‚Äôs performance on new,
unseen data (the ‚Äútest error‚Äù).</p>
<p>The core idea is to <strong>randomly split</strong> your available
data into two parts: 1. <strong>Training Set:</strong> Used to fit (or
‚Äútrain‚Äù) your model. 2. <strong>Validation Set (or Test Set):</strong>
Used to evaluate the trained model‚Äôs performance. You calculate the
error (like Mean Squared Error) on this set.</p>
<h3 id="python-code-explained-slide-1">Python Code Explained (Slide
1)</h3>
<p>The first slide shows a Python example using the <code>Auto</code>
dataset to predict <code>mpg</code> from <code>horsepower</code>.</p>
<ol type="1">
<li><strong>Setup &amp; Data Loading:</strong>
<ul>
<li><code>import</code> statements load libraries like
<code>pandas</code> (for data),
<code>sklearn.model_selection.train_test_split</code> (the key function
for this method), and
<code>sklearn.linear_model.LinearRegression</code>.</li>
<li><code>Auto = pd.read_csv(...)</code> loads the data.</li>
<li><code>X = Auto['horsepower'].values</code> and
<code>y = Auto['mpg'].values</code> select the variables of
interest.</li>
</ul></li>
<li><strong>The Split:</strong>
<ul>
<li><code>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=007)</code></li>
<li>This is the <strong>most important line</strong> for this method. It
splits the data <code>X</code> and <code>y</code> into training and
testing (validation) sets.</li>
<li><code>train_size=0.5</code> means 50% of the data is for training
and 50% is for validation.</li>
<li><code>random_state=007</code> ensures the split is ‚Äúrandom‚Äù but
‚Äúreproducible‚Äù (using the same seed <code>007</code> will always produce
the same split).</li>
</ul></li>
<li><strong>Model Fitting &amp; Evaluation:</strong>
<ul>
<li>The code fits three different polynomial models, but it <strong>only
uses the training data</strong> (<code>X_train</code>,
<code>y_train</code>) to do so.</li>
<li><strong>Linear (Degree 1):</strong> A simple
<code>LinearRegression</code>.</li>
<li><strong>Quadratic (Degree 2):</strong> Uses
<code>PolynomialFeatures(2)</code> to create <span
class="math inline">\(x\)</span> and <span
class="math inline">\(x^2\)</span> terms, then fits a linear model to
them.</li>
<li><strong>Cubic (Degree 3):</strong> Uses
<code>PolynomialFeatures(3)</code> to create <span
class="math inline">\(x\)</span>, <span
class="math inline">\(x^2\)</span>, and <span
class="math inline">\(x^3\)</span> terms.</li>
<li>It then calculates the <strong>Mean Squared Error (MSE)</strong> for
all three models using the <strong>test data</strong>
(<code>X_test</code>, <code>y_test</code>).</li>
</ul></li>
<li><strong>Results (from the text on the slide):</strong>
<ul>
<li><strong>Linear MSE:</strong> <span class="math inline">\(\approx
23.3\)</span></li>
<li><strong>Quadratic MSE:</strong> <span class="math inline">\(\approx
19.4\)</span></li>
<li><strong>Cubic MSE:</strong> <span class="math inline">\(\approx
19.4\)</span></li>
<li><strong>Conclusion:</strong> The quadratic model gives a
significantly lower error than the linear model. The cubic model does
not offer any real improvement over the quadratic one.</li>
</ul></li>
</ol>
<h3 id="key-images-the-problem-with-a-single-split">Key Images: The
Problem with a Single Split</h3>
<p>The most important images are on <strong>slide 9</strong> (labeled
‚ÄúFigure‚Äù and ‚ÄúPage 20‚Äù).</p>
<ul>
<li><p><strong>Plot on the Left (Single Split):</strong> This graph
shows the validation MSE for polynomial degrees 1 through 10, based on
the <em>single random split</em> from the R code (slide 2). Just like
the Python example, it shows that the MSE drops sharply from degree 1 to
2, and then stays relatively low. Based on this <em>one</em> chart, you
might pick degree 2 (quadratic) as the best model.</p></li>
<li><p><strong>Plot on the Right (Ten Splits):</strong> This is the
<strong>most critical plot</strong>. It shows the results of
<em>repeating the entire process 10 times</em>, each with a new random
split (from R code on slide 3).</p>
<ul>
<li>You can see 10 different error curves.</li>
<li>While they all agree that degree 1 (linear) is bad, they <strong>do
not agree on the best model</strong>. Some curves suggest degree 2 is
best, others suggest 3, 4, or even 6.</li>
</ul></li>
</ul>
<h3 id="summary-of-drawbacks-slides-7-8-9-23-25">Summary of Drawbacks
(Slides 7, 8, 9, 23, 25)</h3>
<p>The slides repeatedly emphasize the two main drawbacks of this simple
validation set approach:</p>
<ol type="1">
<li><strong>High Variability:</strong> The estimated test MSE can be
<strong>highly variable</strong>, depending on which observations happen
to land in the training set versus the validation set. The plot with 10
curves (slide 9, right) proves this perfectly.</li>
<li><strong>Overestimation of Test Error:</strong>
<ul>
<li>The model is <strong>only trained on a subset</strong> (e.g., 50%)
of the available data. The validation data is ‚Äúwasted‚Äù and not used for
model building.</li>
<li>Statistical methods tend to perform worse when trained on fewer
observations.</li>
<li>Therefore, the model trained on just the training set is likely
<em>worse</em> than a model trained on the <em>entire</em> dataset.</li>
<li>This ‚Äúworse‚Äù model will have a <em>higher</em> error rate on the
validation set. This means the validation set MSE <strong>tends to
overestimate</strong> the true test error you would get from a model
trained on all your data.</li>
</ul></li>
</ol>
<h2 id="cross-validation-the-solution">5.3 Cross-Validation: The
Solution</h2>
<p>The slides introduce <strong>Cross-Validation (CV)</strong> as the
method to overcome these drawbacks. The core idea is to use <em>all</em>
data points for both training and validation, just at different
times.</p>
<h3 id="leave-one-out-cross-validation-loocv">Leave-One-Out
Cross-Validation (LOOCV)</h3>
<p>This is the first type of CV introduced (slide 10, page 26). For a
dataset with <span class="math inline">\(n\)</span> data points:</p>
<ol type="1">
<li><strong>Hold out</strong> the 1st data point (this is your
validation set).</li>
<li><strong>Train</strong> the model on the <em>other <span
class="math inline">\(n-1\)</span> data points</em>.</li>
<li><strong>Calculate</strong> the error (e.g., <span
class="math inline">\(\text{MSE}_1\)</span>) using only that 1st
held-out point.</li>
<li><strong>Repeat</strong> this <span class="math inline">\(n\)</span>
times, holding out the 2nd point, then the 3rd, and so on, until every
point has been used as the validation set exactly once.</li>
<li>Your final test error estimate is the <strong>average of all <span
class="math inline">\(n\)</span> errors</strong>.</li>
</ol>
<h3 id="key-formula-from-slide-10">Key Formula (from Slide 10)</h3>
<p>The formula for the <span class="math inline">\(n\)</span>-fold LOOCV
error estimate is:</p>
<p><span class="math display">\[\text{CV}_{(n)} = \frac{1}{n}
\sum_{i=1}^{n} \text{MSE}_i\]</span></p>
<p>Where: * <span class="math inline">\(n\)</span> is the total number
of data points. * <span class="math inline">\(\text{MSE}_i\)</span> is
the Mean Squared Error calculated on the <span
class="math inline">\(i\)</span>-th data point when it was held out.</p>
<h1 id="what-is-loocv-leave-one-out-cross-validation">3.What is LOOCV
(Leave-One-Out Cross Validation)</h1>
<p>Leave-One-Out Cross Validation (LOOCV) is a method for estimating the
test error of a model. For a dataset with <span
class="math inline">\(n\)</span> observations, you:</p>
<ol type="1">
<li><strong>Fit the model <span class="math inline">\(n\)</span>
times.</strong></li>
<li>For each fit <span class="math inline">\(i\)</span> (from <span
class="math inline">\(1\)</span> to <span
class="math inline">\(n\)</span>), you train the model on all data
points <em>except</em> for observation <span
class="math inline">\(i\)</span>.</li>
<li>You then use this trained model to make a prediction for the single
observation <span class="math inline">\(i\)</span> that was left
out.</li>
<li>The final LOOCV error is the average of the <span
class="math inline">\(n\)</span> prediction errors (typically the Mean
Squared Error, or MSE).</li>
</ol>
<p>This process is shown visually in the slide titled ‚ÄúLOOCV‚Äù (slide
27), which is a key image for understanding the concept. <strong>Pros
&amp; Cons (from slide 28):</strong> * <strong>Pro:</strong> It has low
bias because the training set (<span class="math inline">\(n-1\)</span>
samples) is almost identical to the full dataset. *
<strong>Pro:</strong> It produces a stable, non-random error estimate
(unlike <span class="math inline">\(k\)</span>-fold CV, which depends on
the random fold assignments). * <strong>Con:</strong> It can be
extremely <strong>computationally expensive</strong>, as the model must
be refit <span class="math inline">\(n\)</span> times. *
<strong>Con:</strong> The <span class="math inline">\(n\)</span> error
estimates can be highly correlated, which can sometimes lead to high
variance in the final <span class="math inline">\(CV\)</span>
estimate.</p>
<h2 id="key-mathematical-formulas">Key Mathematical Formulas</h2>
<p>The main challenge of LOOCV (being computationally expensive) has a
very efficient solution for linear models.</p>
<h3 id="the-standard-slow-formula">1. The Standard (Slow) Formula</h3>
<p>As defined on slide 33, the LOOCV estimate of the MSE is:</p>
<p><span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n}
(y_i - \hat{y}_i^{(i)})^2\]</span></p>
<ul>
<li><span class="math inline">\(y_i\)</span> is the true value of the
<span class="math inline">\(i\)</span>-th observation.</li>
<li><span class="math inline">\(\hat{y}_i^{(i)}\)</span> is the
predicted value for <span class="math inline">\(y_i\)</span> from a
model trained on all data <em>except</em> observation <span
class="math inline">\(i\)</span>.</li>
</ul>
<p>Calculating <span class="math inline">\(\hat{y}_i^{(i)}\)</span>
requires refitting the model <span class="math inline">\(n\)</span>
times.</p>
<h3 id="the-shortcut-fast-formula">2. The Shortcut (Fast) Formula</h3>
<p>Slide 34 provides a much simpler formula that <strong>only requires
fitting the model once</strong> on the <em>entire</em> dataset:</p>
<p><span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n}
\left( \frac{y_i - \hat{y}_i}{1 - h_i} \right)^2\]</span></p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> is the prediction for
<span class="math inline">\(y_i\)</span> from the model trained on
<strong>all <span class="math inline">\(n\)</span> data
points</strong>.</li>
<li><span class="math inline">\(h_i\)</span> is the
<strong>leverage</strong> of the <span
class="math inline">\(i\)</span>-th observation.</li>
</ul>
<h3 id="what-is-leverage-h_i">3. What is Leverage (<span
class="math inline">\(h_i\)</span>)?</h3>
<p>Slide 35 defines leverage:</p>
<ul>
<li><strong>Hat Matrix (<span
class="math inline">\(\mathbf{H}\)</span>):</strong> In a linear model,
the fitted values <span class="math inline">\(\hat{\mathbf{y}}\)</span>
are related to the true values <span
class="math inline">\(\mathbf{y}\)</span> by the hat matrix: <span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{H}\mathbf{y}\)</span>.</li>
<li><strong>Formula:</strong> The hat matrix is defined as <span
class="math inline">\(\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span>.</li>
<li><strong>Leverage (<span
class="math inline">\(h_i\)</span>):</strong> The leverage for the <span
class="math inline">\(i\)</span>-th observation is simply the <span
class="math inline">\(i\)</span>-th diagonal element of the hat matrix,
<span class="math inline">\(h_{ii}\)</span> (often just written as <span
class="math inline">\(h_i\)</span>).
<ul>
<li><span class="math inline">\(h_i = \mathbf{x}_i^T
(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{x}_i\)</span></li>
</ul></li>
<li><strong>Meaning:</strong> Leverage measures how ‚Äúinfluential‚Äù an
observation‚Äôs <span class="math inline">\(x_i\)</span> value is in
determining its own predicted value <span
class="math inline">\(\hat{y}_i\)</span>. A high leverage score means
that point has a lot of influence on the model‚Äôs fit.</li>
</ul>
<p>This shortcut formula is extremely important because it makes LOOCV
as fast to compute as a single model fit.</p>
<h2 id="python-code-explained-slide-29">Python Code Explained (Slide
29)</h2>
<p>This slide shows how to use LOOCV to select the best polynomial
degree for predicting <code>mpg</code> from <code>horsepower</code>.</p>
<ol type="1">
<li><strong>Imports:</strong> It imports standard libraries
(<code>pandas</code>, <code>matplotlib</code>) and key modules from
<code>sklearn</code>:
<ul>
<li><code>LinearRegression</code>: The model to be fit.</li>
<li><code>PolynomialFeatures</code>: A tool to create polynomial terms
(e.g., <span class="math inline">\(x, x^2, x^3\)</span>).</li>
<li><code>LeaveOneOut</code>: The LOOCV cross-validation strategy
object.</li>
<li><code>cross_val_score</code>: A function that automatically runs a
cross-validation test.</li>
</ul></li>
<li><strong>Setup:</strong>
<ul>
<li>It loads the <code>Auto.csv</code> data.</li>
<li>It defines <span class="math inline">\(X\)</span>
(<code>horsepower</code>) and <span class="math inline">\(y\)</span>
(<code>mpg</code>).</li>
<li>It creates a <code>LeaveOneOut</code> object:
<code>loo = LeaveOneOut()</code>.</li>
</ul></li>
<li><strong>Looping through Degrees:</strong>
<ul>
<li>The code loops <code>degree</code> from 1 to 10.</li>
<li><strong><code>make_pipeline</code>:</strong> For each degree, it
creates a <code>model</code> using <code>make_pipeline</code>. This
pipeline is a crucial concept:
<ul>
<li>It first runs <code>PolynomialFeatures(degree)</code> to transform
<span class="math inline">\(X\)</span> into <span
class="math inline">\([X, X^2, ..., X^{\text{degree}}]\)</span>.</li>
<li>It then feeds those features into <code>LinearRegression()</code> to
fit the model.</li>
</ul></li>
<li><strong><code>cross_val_score</code>:</strong> This is the most
important line.
<ul>
<li><code>scores = cross_val_score(model, X, y, cv=loo, scoring='neg_mean_squared_error')</code></li>
<li>This function automatically does the <em>entire</em> LOOCV process.
It takes the <code>model</code> (the pipeline), the data <span
class="math inline">\(X\)</span> and <span
class="math inline">\(y\)</span>, and the CV strategy
(<code>cv=loo</code>).</li>
<li><code>sklearn</code>‚Äôs <code>cross_val_score</code> uses the ‚Äúfast‚Äù
leverage method internally for linear models, so it doesn‚Äôt actually fit
the model <span class="math inline">\(n\)</span> times.</li>
<li>It uses <code>scoring='neg_mean_squared_error'</code> because the
<code>scoring</code> function assumes ‚Äúhigher is better.‚Äù By calculating
the <em>negative</em> MSE, the best model will have the highest score
(i.e., closest to 0).</li>
</ul></li>
<li><strong>Storing Results:</strong> It calculates the mean of the
scores (which is the <span class="math inline">\(CV_{(n)}\)</span>) and
stores it.</li>
</ul></li>
<li><strong>Visualization:</strong>
<ul>
<li>The code then plots the final <code>cv_errors</code> (after flipping
the sign back to positive) against the <code>degree</code>.</li>
<li>The resulting plot (also on slide 32) shows the test MSE, allowing
you to visually pick the best degree (where the error is
minimized).</li>
</ul></li>
</ol>
<hr />
<h2 id="important-images">Important Images</h2>
<ul>
<li><strong>Slide 27 (<code>.../103628.png</code>):</strong> This is the
<strong>best conceptual image</strong>. It visually demonstrates how
LOOCV splits the data <span class="math inline">\(n\)</span> times, with
each observation getting one turn as the validation set.</li>
<li><strong>Slide 34 (<code>.../103711.png</code>):</strong> This slide
presents the <strong>most important formula</strong>: the ‚ÄúEasy formula‚Äù
or shortcut, <span class="math inline">\(CV_{(n)} = \frac{1}{n} \sum
(\frac{y_i - \hat{y}_i}{1 - h_i})^2\)</span>. This is the key takeaway
for <em>computing</em> LOOCV efficiently in linear models.</li>
<li><strong>Slide 32 (<code>.../103701.jpg</code>):</strong> This is the
<strong>key results image</strong>. It contrasts the LOOCV error curve
(left) with the 10-fold CV error curves (right). It clearly shows that
LOOCV produces a single, stable error curve, while 10-fold CV results
vary slightly each time it‚Äôs run due to the random data splits.</li>
</ul>
<h1 id="cross-validation-overview">4. Cross-Validation Overview</h1>
<p>These slides explain <strong>Cross-Validation (CV)</strong>, a method
used to estimate the test error of a model, helping to select the best
level of flexibility (e.g., the best polynomial degree). It‚Äôs an
improvement over a single validation set because it uses all the data
for both training and validation at different times.</p>
<p>The two main types discussed are <strong>K-fold CV</strong> and
<strong>Leave-One-Out CV (LOOCV)</strong>.</p>
<h2 id="k-fold-cross-validation">K-Fold Cross-Validation</h2>
<p>This is the most common method.</p>
<h3 id="the-process">The Process</h3>
<p>As shown in the slides, the K-fold CV process is: 1.
<strong>Divide</strong> the dataset randomly into <span
class="math inline">\(K\)</span> non-overlapping groups (or ‚Äúfolds‚Äù),
usually of equal size. Common choices are <span
class="math inline">\(K=5\)</span> or <span
class="math inline">\(K=10\)</span>. 2. <strong>Iterate <span
class="math inline">\(K\)</span> times</strong>: In each iteration <span
class="math inline">\(i\)</span>, use the <span
class="math inline">\(i\)</span>-th fold as the <strong>validation
set</strong> and all other <span class="math inline">\(K-1\)</span>
folds combined as the <strong>training set</strong>. 3.
<strong>Calculate</strong> the Mean Squared Error (<span
class="math inline">\(MSE_i\)</span>) on the validation fold. 4.
<strong>Average</strong> all <span class="math inline">\(K\)</span>
error estimates to get the final CV score.</p>
<h3 id="key-formula">Key Formula</h3>
<p>The final K-fold CV error estimate is the average of the errors from
each fold: <span class="math display">\[CV_{(K)} = \frac{1}{K}
\sum_{i=1}^{K} MSE_i\]</span></p>
<h3 id="important-image-the-concept">Important Image: The Concept</h3>
<p>The diagram in slide <code>104145.png</code> is the most important
for understanding the <em>concept</em> of K-fold CV. It shows a dataset
split into 5 folds (<span class="math inline">\(K=5\)</span>). The
process is repeated 5 times, with a different fold (in beige) held out
as the validation set in each run, while the rest (in blue) is used for
training.</p>
<h2 id="leave-one-out-cross-validation-loocv-1">Leave-One-Out
Cross-Validation (LOOCV)</h2>
<p>LOOCV is just a special case of K-fold CV where <strong><span
class="math inline">\(K = n\)</span></strong> (the total number of
observations). * You create <span class="math inline">\(n\)</span>
‚Äúfolds,‚Äù each containing just one data point. * You train the model
<span class="math inline">\(n\)</span> times, each time leaving out a
<em>single</em> different observation and then calculating the error for
that one point.</p>
<h3 id="key-formulas">Key Formulas</h3>
<ol type="1">
<li><p><strong>Standard Definition:</strong> The LOOCV error is the
average of the <span class="math inline">\(n\)</span> squared errors:
<span class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N}
e_{[i]}^2\]</span> where <span class="math inline">\(e_{[i]} = y_i -
\hat{y}_{[i]}\)</span> is the prediction error for the <span
class="math inline">\(i\)</span>-th observation, calculated from a model
that was trained on <em>all data except</em> the <span
class="math inline">\(i\)</span>-th observation. This looks
computationally expensive.</p></li>
<li><p><strong>Fast Computation (for Linear Regression):</strong> A key
point from the slides is that for linear regression, you don‚Äôt need to
re-fit the model <span class="math inline">\(N\)</span> times. You can
fit the model <em>once</em> on all <span
class="math inline">\(N\)</span> data points and use the following
shortcut: <span class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N}
\left( \frac{e_i}{1 - h_i} \right)^2\]</span></p>
<ul>
<li><span class="math inline">\(e_i = y_i - \hat{y}_i\)</span> is the
standard residual (from the model fit on <em>all</em> data).</li>
<li><span class="math inline">\(h_i\)</span> is the <em>leverage
statistic</em> for the <span class="math inline">\(i\)</span>-th
observation (the <span class="math inline">\(i\)</span>-th diagonal
entry of the ‚Äúhat matrix‚Äù <span class="math inline">\(H\)</span>). This
makes LOOCV as fast to compute as a single model fit.</li>
</ul></li>
</ol>
<h2 id="python-code-results">Python Code &amp; Results</h2>
<p>The Python code in slide <code>104156.jpg</code> shows how to use
10-fold CV to find the best polynomial degree for a model.</p>
<h3 id="code-understanding-slide-104156.jpg">Code Understanding (Slide
<code>104156.jpg</code>)</h3>
<p>Here‚Äôs a breakdown of the key <code>sklearn</code> parts:</p>
<ol type="1">
<li><strong><code>from sklearn.pipeline import make_pipeline</code></strong>:
This is used to chain steps. The pipeline
<code>make_pipeline(PolynomialFeatures(degree), LinearRegression())</code>
first creates polynomial features (like <span
class="math inline">\(x\)</span>, <span
class="math inline">\(x^2\)</span>, <span
class="math inline">\(x^3\)</span>) and then fits a linear model to
them.</li>
<li><strong><code>from sklearn.model_selection import KFold</code></strong>:
This object is used to define the <span
class="math inline">\(K\)</span>-fold split strategy.
<code>kf = KFold(n_splits=10, shuffle=True, random_state=1)</code>
creates a 10-fold splitter that shuffles the data first.</li>
<li><strong><code>from sklearn.model_selection import cross_val_score</code></strong>:
This is the most important function.
<ul>
<li><code>scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')</code></li>
<li>This one function does all the work: it takes the <code>model</code>
(the pipeline), the data <code>X</code> and <code>y</code>, and the CV
splitter <code>kf</code>. It automatically trains and evaluates the
model 10 times and returns an array of 10 scores (one for each
fold).</li>
<li><code>scoring='neg_mean_squared_error'</code> is used because
<code>cross_val_score</code> expects a <em>higher</em> score to be
<em>better</em>. Since we want to <em>minimize</em> MSE, we use
<em>negative</em> MSE.</li>
</ul></li>
<li><strong><code>avg_mse = -scores.mean()</code></strong>: The code
averages the 10 scores and flips the sign back to positive to get the
final CV (MSE) estimate for that polynomial degree.</li>
</ol>
<h3 id="important-image-the-results">Important Image: The Results</h3>
<p>The plots in slides <code>104156.jpg</code> (Python) and
<code>104224.png</code> (R) show the key result.</p>
<ul>
<li><strong>X-axis:</strong> Degree of Polynomial (model
complexity).</li>
<li><strong>Y-axis:</strong> Estimated Test Error (CV Error / MSE).</li>
<li><strong>Interpretation:</strong> The plot shows a clear ‚ÄúU‚Äù shape.
The error is high for degree 1 (a simple line), drops to its minimum at
<strong>degree 2</strong> (a quadratic <span class="math inline">\(ax^2
+ bx + c\)</span>), and then starts to rise again for higher degrees.
This rise indicates <strong>overfitting</strong>‚Äîthe more complex models
are fitting the training data‚Äôs noise, leading to worse performance on
unseen validation data.</li>
<li><strong>Conclusion:</strong> The 10-fold CV analysis suggests that a
<strong>quadratic model (degree 2)</strong> is the best choice, as it
provides the lowest estimated test error.</li>
</ul>
<p>Let‚Äôs dive into the details of that proof.</p>
<h2 id="detailed-summary-the-fast-computation-of-loocv-proof">Detailed
Summary: The ‚ÄúFast Computation of LOOCV‚Äù Proof</h2>
<p>The most mathematically dense and important part of your slides is
the proof (spanning slides <code>104126.jpg</code>,
<code>104132.png</code>, and <code>104136.png</code>) that LOOCV, which
seems computationally very expensive, can be calculated quickly for
linear regression.</p>
<h3 id="the-goal">The Goal</h3>
<p>The goal is to prove that the LOOCV statistic, which is defined as:
<span class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N} e_{[i]}^2
\quad \text{where } e_{[i]} = y_i - \hat{y}_{[i]}\]</span> (Here, <span
class="math inline">\(\hat{y}_{[i]}\)</span> is the prediction for <span
class="math inline">\(y_i\)</span> from a model trained on all data
<em>except</em> point <span class="math inline">\(i\)</span>).</p>
<p>‚Ä¶can be computed <em>without</em> re-fitting the model <span
class="math inline">\(N\)</span> times, using this ‚Äúfast‚Äù formula: <span
class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N} \left(
\frac{e_i}{1 - h_i} \right)^2\]</span> (Here, <span
class="math inline">\(e_i\)</span> is the <em>standard</em> residual and
<span class="math inline">\(h_i\)</span> is the <em>leverage</em>, both
from a single model fit on <em>all</em> data).</p>
<p>The entire proof boils down to showing one identity: <strong><span
class="math inline">\(e_{[i]} = e_i / (1 - h_i)\)</span></strong>.</p>
<h3 id="key-definitions-the-matrix-algebra-setup">Key Definitions (The
Matrix Algebra Setup)</h3>
<ul>
<li><strong>Model:</strong> <span class="math inline">\(\mathbf{Y} =
\mathbf{X}\beta + \mathbf{e}\)</span></li>
<li><strong>Full Data Estimate (<span
class="math inline">\(\hat{\beta}\)</span>):</strong> <span
class="math inline">\(\hat{\beta} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\)</span></li>
<li><strong>Hat Matrix (<span
class="math inline">\(\mathbf{H}\)</span>):</strong> <span
class="math inline">\(\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span></li>
<li><strong>Full Data Residual (<span
class="math inline">\(e_i\)</span>):</strong> <span
class="math inline">\(e_i = y_i - \hat{y}_i = y_i -
\mathbf{x}_i^T\hat{\beta}\)</span></li>
<li><strong>Leverage (<span
class="math inline">\(h_i\)</span>):</strong> The <span
class="math inline">\(i\)</span>-th diagonal element of <span
class="math inline">\(\mathbf{H}\)</span>. <span
class="math inline">\(h_i =
\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i\)</span></li>
<li><strong>Leave-One-Out Estimate (<span
class="math inline">\(\hat{\beta}_{[i]}\)</span>):</strong> <span
class="math inline">\(\hat{\beta}_{[i]} =
(\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1}\mathbf{X}_{[i]}^T\mathbf{Y}_{[i]}\)</span>
<ul>
<li><span class="math inline">\(\mathbf{X}_{[i]}\)</span> and <span
class="math inline">\(\mathbf{Y}_{[i]}\)</span> are the data with the
<span class="math inline">\(i\)</span>-th row removed.</li>
</ul></li>
<li><strong>LOOCV Residual (<span
class="math inline">\(e_{[i]}\)</span>):</strong> <span
class="math inline">\(e_{[i]} = y_i -
\mathbf{x}_i^T\hat{\beta}_{[i]}\)</span></li>
</ul>
<h3 id="the-proof-step-by-step">The Proof Step-by-Step</h3>
<p>Here is the logic from your slides, broken down:</p>
<h4 id="step-1-relating-the-matrices-slide-104132.png">Step 1: Relating
the Matrices (Slide <code>104132.png</code>)</h4>
<p>The proof‚Äôs ‚Äútrick‚Äù is to relate the ‚Äúfull data‚Äù matrix <span
class="math inline">\((\mathbf{X}^T\mathbf{X})\)</span> to the
‚Äúleave-one-out‚Äù matrix <span
class="math inline">\((\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})\)</span>.</p>
<ul>
<li>The full sum-of-squares matrix is just the leave-one-out matrix
<em>plus</em> the one observation‚Äôs contribution: <span
class="math display">\[\mathbf{X}^T\mathbf{X} =
\mathbf{X}_{[i]}^T\mathbf{X}_{[i]} +
\mathbf{x}_i\mathbf{x}_i^T\]</span></li>
<li>This means: <span
class="math inline">\(\mathbf{X}_{[i]}^T\mathbf{X}_{[i]} =
\mathbf{X}^T\mathbf{X} - \mathbf{x}_i\mathbf{x}_i^T\)</span></li>
</ul>
<h4 id="step-2-the-key-matrix-trick-slide-104132.png">Step 2: The Key
Matrix Trick (Slide <code>104132.png</code>)</h4>
<p>We need the inverse <span
class="math inline">\((\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1}\)</span>
to calculate <span class="math inline">\(\hat{\beta}_{[i]}\)</span>.
Finding this inverse directly is hard. Instead, we use the
<strong>Sherman-Morrison-Woodbury formula</strong>, which tells us how
to find the inverse of a matrix that‚Äôs been ‚Äúupdated‚Äù (in this case, by
subtracting <span
class="math inline">\(\mathbf{x}_i\mathbf{x}_i^T\)</span>).</p>
<p>The slide applies this formula to get: <span
class="math display">\[(\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1} =
(\mathbf{X}^T\mathbf{X})^{-1} +
\frac{(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}}{1
- h_i}\]</span> * This is the most complex step, but it‚Äôs a standard
matrix identity. It‚Äôs crucial because it expresses the ‚Äúleave-one-out‚Äù
inverse in terms of the ‚Äúfull data‚Äù inverse <span
class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span>, which we
already have.</p>
<h4 id="step-3-finding-hatbeta_i-slide-104136.png">Step 3: Finding <span
class="math inline">\(\hat{\beta}_{[i]}\)</span> (Slide
<code>104136.png</code>)</h4>
<p>Now we can write a new formula for <span
class="math inline">\(\hat{\beta}_{[i]}\)</span> by substituting the
result from Step 2. We also note that <span
class="math inline">\(\mathbf{X}_{[i]}^T\mathbf{Y}_{[i]} =
\mathbf{X}^T\mathbf{Y} - \mathbf{x}_i y_i\)</span>.</p>
<p><span class="math display">\[\hat{\beta}_{[i]} =
(\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1}
(\mathbf{X}_{[i]}^T\mathbf{Y}_{[i]})\]</span> <span
class="math display">\[\hat{\beta}_{[i]} = \left[
(\mathbf{X}^T\mathbf{X})^{-1} +
\frac{(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}}{1
- h_i} \right] (\mathbf{X}^T\mathbf{Y} - \mathbf{x}_i y_i)\]</span></p>
<p>The slide then shows the algebra to simplify this big expression.
When you expand and simplify everything, you get a much cleaner
result:</p>
<p><span class="math display">\[\hat{\beta}_{[i]} = \hat{\beta} -
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i \frac{e_i}{1 - h_i}\]</span> *
This is a beautiful result! It says the LOOCV coefficient vector is just
the <em>full</em> coefficient vector minus a small adjustment term
related to the <span class="math inline">\(i\)</span>-th observation‚Äôs
residual (<span class="math inline">\(e_i\)</span>) and leverage (<span
class="math inline">\(h_i\)</span>).</p>
<h4 id="step-4-finding-e_i-slide-104136.png">Step 4: Finding <span
class="math inline">\(e_{[i]}\)</span> (Slide
<code>104136.png</code>)</h4>
<p>This is the final step. We use the definition of <span
class="math inline">\(e_{[i]}\)</span> and the result from Step 3.</p>
<ul>
<li><strong>Start with the definition:</strong> <span
class="math inline">\(e_{[i]} = y_i -
\mathbf{x}_i^T\hat{\beta}_{[i]}\)</span></li>
<li><strong>Substitute <span
class="math inline">\(\hat{\beta}_{[i]}\)</span>:</strong> <span
class="math inline">\(e_{[i]} = y_i - \mathbf{x}_i^T \left[ \hat{\beta}
- (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i \frac{e_i}{1 - h_i}
\right]\)</span></li>
<li><strong>Distribute <span
class="math inline">\(\mathbf{x}_i^T\)</span>:</strong> <span
class="math inline">\(e_{[i]} = (y_i - \mathbf{x}_i^T\hat{\beta}) +
\left( \mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i \right)
\frac{e_i}{1 - h_i}\)</span></li>
<li><strong>Recognize the terms!</strong>
<ul>
<li>The first term is just the standard residual: <span
class="math inline">\((y_i - \mathbf{x}_i^T\hat{\beta}) =
e_i\)</span></li>
<li>The second term in parentheses is the definition of leverage: <span
class="math inline">\((\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i)
= h_i\)</span></li>
</ul></li>
<li><strong>Substitute back:</strong> <span
class="math inline">\(e_{[i]} = e_i + h_i \left( \frac{e_i}{1 - h_i}
\right)\)</span></li>
<li><strong>Get a common denominator:</strong> <span
class="math inline">\(e_{[i]} = \frac{e_i(1 - h_i) + h_i e_i}{1 -
h_i}\)</span></li>
<li><strong>Simplify the numerator:</strong> <span
class="math inline">\(e_{[i]} = \frac{e_i - e_ih_i + e_ih_i}{1 -
h_i}\)</span></li>
</ul>
<p>This gives the final, simple relationship: <span
class="math display">\[e_{[i]} = \frac{e_i}{1 - h_i}\]</span></p>
<h3 id="conclusion">Conclusion</h3>
<p>By proving this identity, the slides show that to get all <span
class="math inline">\(N\)</span> of the ‚Äúleave-one-out‚Äù errors, you only
need to: 1. Fit <strong>one</strong> linear regression model on
<strong>all</strong> the data. 2. Calculate the standard residuals <span
class="math inline">\(e_i\)</span> and the leverage values <span
class="math inline">\(h_i\)</span> for all <span
class="math inline">\(N\)</span> points. 3. Apply the formula <span
class="math inline">\(e_i / (1 - h_i)\)</span> for each point.</p>
<p>This turns a procedure that looked like it would take <span
class="math inline">\(N\)</span> times the work into a procedure that
takes only <strong>1</strong> model fit. This is why LOOCV is a
practical and efficient method for linear regression.</p>
<h1 id="main-goal-of-cross-validation">5. Main Goal of
Cross-Validation</h1>
<p>The central purpose of cross-validation is to <strong>estimate the
true test error</strong> of a machine learning model. This is crucial
for:</p>
<ol type="1">
<li><strong>Model Assessment:</strong> Evaluating how well a model will
perform on new, unseen data.</li>
<li><strong>Model Selection:</strong> Choosing the best level of model
flexibility (e.g., the degree of a polynomial or the value of <span
class="math inline">\(K\)</span> in KNN) to avoid
<strong>overfitting</strong>.</li>
</ol>
<p>As the slides show, <strong>training error</strong> (the error on the
data the model was trained on) consistently decreases as model
complexity increases. However, the <strong>test error</strong> follows a
U-shape: it first decreases (as the model learns the true signal) and
then increases (as the model starts fitting the noise, or
‚Äúoverfitting‚Äù). CV helps find the minimum point of this U-shaped test
error curve.</p>
<h2 id="important-images-1">Important Images üñºÔ∏è</h2>
<p>The most important image is on <strong>Slide 61</strong>.</p>
<p>These two plots perfectly illustrate the concept:</p>
<ul>
<li><strong>Blue Line (Training Error):</strong> Always goes down.</li>
<li><strong>Brown Line (True Test Error):</strong> Forms a ‚ÄúU‚Äù shape.
This is what we <em>want</em> to find the minimum of, but it‚Äôs unknown
in practice.</li>
<li><strong>Black Line (10-fold CV Error):</strong> This is our
<em>estimate</em> of the test error. Notice how closely it tracks the
brown line. The minimum of the CV curve (marked with an ‚Äòx‚Äô) is very
close to the minimum of the true test error.</li>
</ul>
<p>This shows <em>why</em> CV works: it provides a reliable estimate to
guide our choice of model (e.g., polynomial degree 3-4 for logistic
regression, or <span class="math inline">\(K \approx 10\)</span> for
KNN).</p>
<h2 id="key-formulas-for-classification">Key Formulas for
Classification</h2>
<p>For regression, we often use Mean Squared Error (MSE). For
classification, the slides introduce the <strong>classification error
rate</strong>.</p>
<p>For Leave-One-Out Cross-Validation (LOOCV), the error for a single
observation <span class="math inline">\(i\)</span> is: <span
class="math display">\[Err_i = I(y_i \neq \hat{y}_i^{(i)})\]</span> *
<span class="math inline">\(y_i\)</span> is the true label for
observation <span class="math inline">\(i\)</span>. * <span
class="math inline">\(\hat{y}_i^{(i)}\)</span> is the model‚Äôs prediction
for observation <span class="math inline">\(i\)</span> when the model
was trained on all <em>other</em> observations <em>except</em> <span
class="math inline">\(i\)</span>. * <span
class="math inline">\(I(\dots)\)</span> is an <strong>indicator
function</strong>: it‚Äôs <span class="math inline">\(1\)</span> if the
condition is true (prediction is wrong) and <span
class="math inline">\(0\)</span> if false (prediction is correct).</p>
<p>The total <strong>CV error</strong> is simply the average of these
individual errors, which is the overall fraction of incorrect
classifications: <span class="math display">\[CV_{(n)} = \frac{1}{n}
\sum_{i=1}^{n} Err_i\]</span> The slides also show examples using
<strong>Log Loss</strong> (Slide 64), which is another common and
sensitive metric for classification. The logistic regression model
itself is defined by: <span class="math display">\[P(Y=1 | X) =
\frac{1}{1 + \exp(-\beta_0 - \beta_1 X_1 - \beta_2 X_2 -
\dots)}\]</span></p>
<h2 id="python-code-explained">Python Code Explained üêç</h2>
<p>The slides provide two key Python examples. Both manually implement
K-fold cross-validation to show how it works.</p>
<h3 id="knn-regression-slide-52">1. KNN Regression (Slide 52)</h3>
<ul>
<li><strong>Goal:</strong> Find the best <code>n_neighbors</code> (K)
for a <code>KNeighborsRegressor</code>.</li>
<li><strong>Logic:</strong>
<ol type="1">
<li>It creates a <code>KFold</code> object to split the data into 10
folds (<code>n_splits=10</code>).</li>
<li>It has an <strong>outer loop</strong> that iterates through
different values of <span class="math inline">\(K\)</span> (from 1 to
10).</li>
<li>It has an <strong>inner loop</strong> that iterates through the 10
folds (<code>for train_index, test_index in kfold.split(X)</code>).</li>
<li><strong>Inside the inner loop:</strong>
<ul>
<li>It trains a <code>KNeighborsRegressor</code> on the 9 training folds
(<code>X_train</code>, <code>y_train</code>).</li>
<li>It makes predictions on the 1 held-out test fold
(<code>X_test</code>).</li>
<li>It calculates the mean squared error for that fold and stores
it.</li>
</ul></li>
<li><strong>After the inner loop:</strong> It averages the 10 error
scores (one from each fold) to get the final CV error for that specific
<span class="math inline">\(K\)</span>.</li>
<li>The final plot shows this CV error vs.¬†<span
class="math inline">\(K\)</span>, allowing us to pick the <span
class="math inline">\(K\)</span> with the lowest error.</li>
</ol></li>
</ul>
<h3 id="logistic-regression-with-polynomials-slide-64">2. Logistic
Regression with Polynomials (Slide 64)</h3>
<ul>
<li><strong>Goal:</strong> Find the best <code>degree</code> for
<code>PolynomialFeatures</code> used with
<code>LogisticRegression</code>.</li>
<li><strong>Logic:</strong> This is very similar to the KNN example but
uses a different model and error metric.
<ol type="1">
<li>It sets up a 10-fold split (<code>kf = KFold(...)</code>).</li>
<li>An <strong>outer loop</strong> iterates through the
<code>degree</code> <span class="math inline">\(d\)</span> (from 1 to
10).</li>
<li>An <strong>inner loop</strong> iterates through the 10 folds.</li>
<li><strong>Inside the inner loop:</strong>
<ul>
<li>It creates <code>PolynomialFeatures</code> of degree <span
class="math inline">\(d\)</span>.</li>
<li>It transforms the 9 training folds (<code>X_train</code>) into
polynomial features (<code>X_train_poly</code>).</li>
<li>It trains a <code>LogisticRegression</code> model on
<code>X_train_poly</code>.</li>
<li>It transforms the 1 held-out test fold (<code>X_test</code>) using
the <em>same</em> polynomial transformer.</li>
<li>It calculates the <code>log_loss</code> on the test fold.</li>
</ul></li>
<li><strong>After the inner loop:</strong> It averages the 10
<code>log_loss</code> scores to get the final CV error for that
<code>degree</code>.</li>
<li>The plot shows CV error vs.¬†degree, and the minimum is clearly at
<code>degree=3</code>.</li>
</ol></li>
</ul>
<h2 id="the-bias-variance-trade-off-in-cv">The Bias-Variance Trade-off
in CV</h2>
<p>This is a key theoretical point from <strong>Slide 54</strong> that
answers the questions on Slide 65. It compares LOOCV (<span
class="math inline">\(K=n\)</span>) with K-fold CV (<span
class="math inline">\(K=5\)</span> or <span
class="math inline">\(10\)</span>).</p>
<ul>
<li><strong>LOOCV (K=n):</strong>
<ul>
<li><strong>Bias:</strong> Very <strong>low</strong>. The model is
trained on <span class="math inline">\(n-1\)</span> samples, which is
almost the full dataset. The resulting error estimate is nearly unbiased
for the true test error.</li>
<li><strong>Variance:</strong> Very <strong>high</strong>. You are
training <span class="math inline">\(n\)</span> models that are
<em>almost identical</em> to each other (they only differ by one data
point). Averaging these highly correlated error estimates doesn‚Äôt reduce
the variance much, making the CV estimate unstable.</li>
</ul></li>
<li><strong>K-Fold CV (K=5 or 10):</strong>
<ul>
<li><strong>Bias:</strong> Slightly <strong>higher</strong> than LOOCV.
The models are trained on, for example, 90% of the data. Since they are
trained on less data, they <em>might</em> perform slightly worse. This
means K-fold CV <strong>tends to slightly overestimate the true test
error</strong> (Slide 66).</li>
<li><strong>Variance:</strong> Much <strong>lower</strong> than LOOCV.
The 10 models are trained on more different ‚Äúchunks‚Äù of data (they
overlap less), so their error estimates are less correlated. Averaging
less-correlated estimates significantly reduces the overall
variance.</li>
</ul></li>
</ul>
<p><strong>Conclusion:</strong> We generally prefer <strong>10-fold
CV</strong> over LOOCV. It gives a much more stable (low-variance)
estimate of the test error, even if it‚Äôs slightly more biased
(overestimating the error, which is a safe/conservative estimate).</p>
<h2 id="the-core-problem-scenarios-slides-47-51">The Core Problem &amp;
Scenarios (Slides 47-51)</h2>
<p>These slides use three scenarios to show <em>why</em> we need
cross-validation (CV). The goal is to pick the right level of
<strong>model flexibility</strong> (e.g., the degree of a polynomial or
the complexity of a spline) to minimize the <strong>Test MSE</strong>
(Mean Squared Error), which we can‚Äôt see in real life.</p>
<ul>
<li><p><strong>The Curves (Slide 47):</strong> This slide is
central.</p>
<ul>
<li><strong>True Test MSE (Blue):</strong> This is the <em>real</em>
error on new data. It has a <strong>U-shape</strong>. Error is high for
simple models (high bias), drops as the model fits the data, and rises
again for overly complex models (high variance, or overfitting).</li>
<li><strong>LOOCV (Black Dashed) &amp; 10-Fold CV (Orange):</strong>
These are our <em>estimates</em> of the true test MSE. Notice how
closely they track the blue curve. The ‚Äòx‚Äô marks the minimum of the CV
curve, which is our <em>best guess</em> for the model with the minimum
test MSE.</li>
</ul></li>
<li><p><strong>Scenario 1 (Slide 48):</strong> The true relationship is
non-linear. The right-hand plot shows that the test MSE (red curve) is
high for the simple linear model (blue square), but lower for the more
flexible smoothing splines (teal squares). CV helps us find the ‚Äúsweet
spot.‚Äù</p></li>
<li><p><strong>Scenario 2 (Slide 49):</strong> The true relationship is
<strong>linear</strong>. Here, the test MSE (red curve) is
<em>lowest</em> for the simplest model (the linear one, blue square). CV
correctly identifies this, and its error estimate (blue square) is
lowest for that model.</p></li>
<li><p><strong>Scenario 3 (Slide 50):</strong> The true relationship is
<strong>highly non-linear</strong>. The linear model (orange) is a very
poor fit. The test MSE (red curve) is minimized by the most flexible
model (teal square). CV again finds this.</p></li>
<li><p><strong>Key Takeaway (Slide 51):</strong> We use CV to find the
<strong>tuning parameter</strong> (like polynomial degree) that
minimizes the test error. We care less about the <em>actual value</em>
of the CV error and more about <em>where its minimum is</em>.</p></li>
</ul>
<h2 id="cv-for-classification-slides-55-61">CV for Classification
(Slides 55-61)</h2>
<p>This section shifts from regression (predicting a number, using MSE)
to classification (predicting a category, like ‚Äúblue‚Äù or ‚Äúorange‚Äù).</p>
<ul>
<li><strong>New Error Metric (Slide 55):</strong> We can‚Äôt use MSE. A
natural choice is the <strong>classification error rate</strong>.
<ul>
<li><span class="math inline">\(Err_i = I(y_i \neq
\hat{y}_i^{(i)})\)</span></li>
<li>This is an <strong>indicator function</strong>: it is
<strong>1</strong> if the prediction for the <span
class="math inline">\(i\)</span>-th data point (when trained
<em>without</em> it) is wrong, and <strong>0</strong> if it‚Äôs
correct.</li>
<li>The final CV error is just the average of these 0s and 1s, giving
the total fraction of misclassified points: <span
class="math inline">\(CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n}
Err_i\)</span></li>
</ul></li>
<li><strong>The Example (Slides 56-61):</strong>
<ul>
<li><strong>Slides 56-58:</strong> We are shown a ‚Äútrue‚Äù (but unknown)
non-linear boundary (purple dashed line) separating two classes. We then
try to <em>estimate</em> this boundary using logistic regression with
different polynomial degrees (degree 1, 2, 3, 4).</li>
<li><strong>Slides 59-60:</strong> This is a crucial point. In this
<em>simulated</em> example, we <em>do</em> know the true test error
rates. The true errors are [0.201, 0.197, <strong>0.160</strong>,
0.162]. The lowest error is for the 3rd-degree polynomial. But in a
real-world problem, <strong>we can never know these true
errors</strong>.</li>
<li><strong>Slide 61 (The Solution):</strong> This is the most important
image. It shows how CV <em>solves</em> the problem from slide 60.
<ul>
<li><strong>Brown Curve (Test Error):</strong> This is the <em>true</em>
test error (from slide 59). We can‚Äôt see this in practice. Its minimum
is at degree 3.</li>
<li><strong>Black Curve (10-fold CV Error):</strong> This is what we
<em>can</em> calculate. It‚Äôs our estimate of the test error.
<strong>Crucially, its minimum is also at degree 3.</strong></li>
<li>This proves that CV successfully found the best model (degree 3)
without ever seeing the <em>true</em> test error. The same logic is
shown for the KNN classifier on the right.</li>
</ul></li>
</ul></li>
</ul>
<h2 id="python-code-explained-slides-52-63-64">Python Code Explained
(Slides 52, 63, 64)</h2>
<p>The slides show how to <em>manually</em> implement K-fold CV. This is
great for understanding, even though libraries like
<code>GridSearchCV</code> can do this automatically.</p>
<ul>
<li><strong>KNN Regression (Slide 52):</strong>
<ol type="1">
<li><code>kfold = KFold(n_splits=10, ...)</code>: Creates an object that
knows how to split the data into 10 folds.</li>
<li><code>for n_k in neighbors:</code>: This is the <strong>outer
loop</strong> to test different <span class="math inline">\(K\)</span>
values (e.g., <span class="math inline">\(K\)</span>=1, 2, 3‚Ä¶).</li>
<li><code>for train_index, test_index in kfold.split(X):</code>: This is
the <strong>inner loop</strong>. For a <em>single</em> <span
class="math inline">\(K\)</span>, it loops 10 times.</li>
<li>Inside the inner loop:
<ul>
<li>It splits the data into a 9-fold training set (<code>X_train</code>)
and a 1-fold test set (<code>X_test</code>).</li>
<li>It trains a <code>KNeighborsRegressor</code> on
<code>X_train</code>.</li>
<li>It makes predictions on <code>X_test</code> and calculates the error
(<code>mean_squared_error</code>).</li>
</ul></li>
<li><code>cv_errors.append(np.mean(mse_errors_k))</code>: After the
inner loop finishes 10 runs, it averages the 10 error scores for that
<span class="math inline">\(K\)</span> and stores it.</li>
<li>The final plot shows <code>cv_errors</code>
vs.¬†<code>neighbors</code>, letting you pick the <span
class="math inline">\(K\)</span> with the lowest average error.</li>
</ol></li>
<li><strong>Logistic Regression Classification (Slides 63-64):</strong>
<ul>
<li>This code is almost identical, but with three key differences:
<ol type="1">
<li>The model is <code>LogisticRegression</code>.</li>
<li>It uses <code>PolynomialFeatures</code> to create new features
(<span class="math inline">\(X^2, X^3,\)</span> etc.) <em>inside</em>
the loop.</li>
<li>The error metric is <code>log_loss</code> (a common, more sensitive
metric than the simple 0/1 error rate).</li>
</ol></li>
<li>The plot on slide 64 shows the 10-fold CV error (using Log Loss)
vs.¬†the Degree of the Polynomial. The minimum is clearly at
<strong>Degree = 3</strong>, matching the finding from slide 61.</li>
</ul></li>
</ul>
<h2 id="answering-the-key-questions-slides-54-65">Answering the Key
Questions (Slides 54 &amp; 65)</h2>
<p>Slide 65 asks two critical questions, which are answered directly by
the concepts on <strong>Slide 54 (Bias and variance
trade-off)</strong>.</p>
<h3 id="q1-how-does-k-affect-the-bias-and-variance-of-the-cv-error">Q1:
How does K affect the bias and variance of the CV error?</h3>
<p>This refers to <span class="math inline">\(K\)</span> in K-fold CV
(not to be confused with <span class="math inline">\(K\)</span> in
KNN).</p>
<ul>
<li><strong>Bias:</strong>
<ul>
<li><strong>LOOCV (K = n):</strong> This has <strong>very low
bias</strong>. The model is trained on <span
class="math inline">\(n-1\)</span> samples, which is <em>almost</em> the
full dataset. So, the error estimate <span
class="math inline">\(CV_{(n)}\)</span> is an almost-unbiased estimate
of the true test error.</li>
<li><strong>K-Fold (K &lt; n, e.g., K=10):</strong> This has
<strong>slightly higher bias</strong>. The models are trained on, for
example, 90% of the data. Because they are trained on less data, they
<em>might</em> perform slightly worse than a model trained on 100% of
the data. This ‚Äúpessimism‚Äù is the source of the bias.</li>
</ul></li>
<li><strong>Variance:</strong>
<ul>
<li><strong>LOOCV (K = n):</strong> This has <strong>very high
variance</strong>. You are training <span
class="math inline">\(n\)</span> models that are <em>almost
identical</em> (they only differ by one data point). Averaging <span
class="math inline">\(n\)</span> highly-correlated error estimates
doesn‚Äôt reduce the variance much. This makes the final <span
class="math inline">\(CV_{(n)}\)</span> estimate unstable.</li>
<li><strong>K-Fold (K &lt; n, e.g., K=10):</strong> This has
<strong>much lower variance</strong>. The 10 models are trained on more
different ‚Äúchunks‚Äù of data (they overlap less). Their error estimates
are less correlated, and averaging 10 less-correlated numbers gives a
much more stable (low-variance) final estimate.</li>
</ul></li>
</ul>
<p><strong>Conclusion (The Trade-off):</strong> We prefer <strong>K-fold
CV (K=5 or 10)</strong> over LOOCV. It gives a much more stable
(low-variance) estimate, and we are willing to accept a tiny increase in
bias to get it.</p>
<h3
id="q2-does-cross-validation-over-estimate-or-under-estimate-the-true-test-error">Q2:
Does Cross Validation over-estimate or under-estimate the true test
error?</h3>
<p>Based on the bias discussion above:</p>
<p>Cross-validation (especially K-fold) generally <strong>over-estimates
the true test error</strong>.</p>
<p><strong>Reasoning:</strong> 1. The ‚Äútrue test error‚Äù is the error of
a model trained on the <em>entire dataset</em> (<span
class="math inline">\(n\)</span> samples). 2. K-fold CV trains its
models on <em>subsets</em> of the data (e.g., <span
class="math inline">\(n \times (K-1)/K\)</span> samples). 3. Since these
models are trained on <em>less</em> data, they are (on average) slightly
worse than the final model trained on all the data. 4. Because the CV
models are slightly worse, their error rates will be slightly
<em>higher</em>. 5. Therefore, the final CV error score is a slightly
‚Äúpessimistic‚Äù or high estimate. This is considered a good thing, as it‚Äôs
a <em>conservative</em> estimate of how our model will perform.</p>
<h1 id="summary-of-bootstrap">6. Summary of Bootstrap</h1>
<p>Bootstrap is a <strong>resampling technique</strong> used to estimate
the <strong>uncertainty</strong> (like standard error or confidence
intervals) of a statistic. Its key idea is to <strong>treat your
original data sample as a proxy for the true population</strong>. It
then simulates the process of drawing new samples by instead
<strong>sampling <em>with replacement</em></strong> from your original
sample.</p>
<h2 id="summary-of-bootstrap-1">Summary of Bootstrap</h2>
<h3 id="the-problem">The Problem</h3>
<p>You have a single data sample (e.g., <span
class="math inline">\(n=100\)</span> people) and you calculate a
statistic, like the sample mean (<span
class="math inline">\(\bar{x}\)</span>) or a regression coefficient
(<span class="math inline">\(\hat{\beta}\)</span>). You want to know how
<em>accurate</em> this statistic is. How much would it vary if you could
repeat your experiment many times? This variation is measured by the
<strong>standard error (SE)</strong>.</p>
<h3 id="the-bootstrap-solution">The Bootstrap Solution</h3>
<p>Since you can‚Äôt re-run the whole experiment, you <em>simulate</em> it
using the one sample you have.</p>
<p><strong>The Process:</strong> 1. <strong>Original Sample (<span
class="math inline">\(Z\)</span>):</strong> You have your one dataset
with <span class="math inline">\(n\)</span> observations. 2.
<strong>Bootstrap Sample (<span
class="math inline">\(Z^{*1}\)</span>):</strong> Create a <em>new</em>
dataset of size <span class="math inline">\(n\)</span> by randomly
pulling observations from your original sample <em>with
replacement</em>. (This means some original observations will be picked
multiple times, and some not at all). 3. <strong>Calculate Statistic
(<span class="math inline">\(\hat{\theta}^{*1}\)</span>):</strong>
Calculate your statistic of interest (e.g., the mean, <span
class="math inline">\(\hat{\alpha}\)</span>, regression coefficients) on
this new bootstrap sample. 4. <strong>Repeat:</strong> Repeat steps 2
and 3 a large number of times (<span class="math inline">\(B\)</span>,
e.g., <span class="math inline">\(B=1000\)</span>). This gives you <span
class="math inline">\(B\)</span> bootstrap statistics: <span
class="math inline">\(\hat{\theta}^{*1}, \hat{\theta}^{*2}, ...,
\hat{\theta}^{*B}\)</span>. 5. <strong>Analyze the Bootstrap
Distribution:</strong> This collection of <span
class="math inline">\(B\)</span> statistics is your ‚Äúbootstrap
distribution.‚Äù * <strong>Standard Error:</strong> The <strong>standard
deviation</strong> of this bootstrap distribution is your estimate of
the <strong>standard error</strong> of your original statistic. *
<strong>Confidence Interval:</strong> A 95% confidence interval can be
found by taking the <strong>2.5th and 97.5th percentiles</strong> of
this bootstrap distribution.</p>
<p><strong>Why use it?</strong> It‚Äôs powerful because it doesn‚Äôt rely on
strong theoretical assumptions (like data being normally distributed).
It can be applied to almost <em>any</em> statistic, even very complex
ones (like the prediction from a KNN model), for which a simple
mathematical formula for standard error doesn‚Äôt exist.</p>
<h2 id="mathematical-understanding">Mathematical Understanding</h2>
<p>The core idea is to use the <strong>empirical distribution</strong>
(your sample) as an estimate for the true <strong>population
distribution</strong>.</p>
<h3 id="example-estimating-alpha">Example: Estimating <span
class="math inline">\(\alpha\)</span></h3>
<p>Your slides provide an example of finding the <span
class="math inline">\(\alpha\)</span> that minimizes the variance of a
portfolio, <span class="math inline">\(var(\alpha X +
(1-\alpha)Y)\)</span>.</p>
<ol type="1">
<li><p><strong>True Population Parameter (<span
class="math inline">\(\alpha\)</span>):</strong> The <em>true</em> <span
class="math inline">\(\alpha\)</span> is a function of the
<em>population</em> variances and covariance: <span
class="math display">\[\alpha = \frac{\sigma_Y^2 -
\sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 - 2\sigma_{XY}}\]</span> We can
never know this value exactly unless we know the entire
population.</p></li>
<li><p><strong>Sample Statistic (<span
class="math inline">\(\hat{\alpha}\)</span>):</strong> We
<em>estimate</em> <span class="math inline">\(\alpha\)</span> using our
sample, creating the statistic <span
class="math inline">\(\hat{\alpha}\)</span> by plugging in our
<em>sample</em> variances and covariance: <span
class="math display">\[\hat{\alpha} = \frac{\hat{\sigma}_Y^2 -
\hat{\sigma}_{XY}}{\hat{\sigma}_X^2 + \hat{\sigma}_Y^2 -
2\hat{\sigma}_{XY}}\]</span> This <span
class="math inline">\(\hat{\alpha}\)</span> is just <em>one number</em>
from our single sample. How confident are we in it? We need its standard
error, <span class="math inline">\(SE(\hat{\alpha})\)</span>.</p></li>
<li><p><strong>Bootstrap Statistic (<span
class="math inline">\(\hat{\alpha}^*\)</span>):</strong> We apply the
bootstrap process:</p>
<ul>
<li>Create a bootstrap sample (by resampling with replacement).</li>
<li>Calculate <span class="math inline">\(\hat{\alpha}^*\)</span> using
the sample (co)variances of this <em>new bootstrap sample</em>.</li>
<li>Repeat <span class="math inline">\(B\)</span> times to get <span
class="math inline">\(B\)</span> values: <span
class="math inline">\(\hat{\alpha}^{*1}, \hat{\alpha}^{*2}, ...,
\hat{\alpha}^{*B}\)</span>.</li>
</ul></li>
<li><p><strong>Estimating the Standard Error:</strong> The standard
error of our original estimate <span
class="math inline">\(\hat{\alpha}\)</span> is <em>estimated</em> by the
standard deviation of all our bootstrap estimates: <span
class="math display">\[SE_{boot}(\hat{\alpha}) = \sqrt{\frac{1}{B-1}
\sum_{j=1}^{B} (\hat{\alpha}^{*j} - \bar{\alpha}^*)^2}\]</span> where
<span class="math inline">\(\bar{\alpha}^*\)</span> is the average of
all <span class="math inline">\(B\)</span> bootstrap estimates.</p></li>
</ol>
<p>The slides (p.¬†73, 77-78) show this visually. The ‚Äúsampling from
population‚Äù histogram (left) is the <em>true</em> sampling distribution,
which we can only create in a simulation. The ‚ÄúBootstrap‚Äù histogram
(right) is the bootstrap distribution created from <em>one</em> sample.
They look very similar, which shows the method works.</p>
<h2 id="code-analysis-1">Code Analysis</h2>
<h3 id="r-alpha-example-slides-75-77">R: <span
class="math inline">\(\alpha\)</span> Example (Slides 75 &amp; 77)</h3>
<ul>
<li><strong>Slide 75 (<code>The R code</code>): This is a SIMULATION,
not Bootstrap.</strong>
<ul>
<li><code>for(i in 1:m)&#123;...&#125;</code>: This loop runs <code>m=1000</code>
times.</li>
<li><code>returns &lt;- rmvnorm(...)</code>: <strong>Inside the
loop,</strong> it draws a <em>brand new sample</em> from the <em>true
population</em> every time.</li>
<li><code>alpha[i] &lt;- ...</code>: It calculates <span
class="math inline">\(\hat{\alpha}\)</span> for each new sample.</li>
<li><strong>Purpose:</strong> This code shows the <em>true sampling
distribution</em> of <span class="math inline">\(\hat{\alpha}\)</span>
(the ‚ÄúHistogram of alpha‚Äù). You can only do this if you know the true
population, as in a simulation.</li>
</ul></li>
<li><strong>Slide 77 (<code>The R code</code>): This IS
Bootstrap.</strong>
<ul>
<li><code>returns &lt;- rmvnorm(...)</code>: <strong>Outside the
loop,</strong> this is done <em>only once</em> to get <em>one</em>
original sample.</li>
<li><code>for(i in 1:B)&#123;...&#125;</code>: This is the bootstrap loop.</li>
<li><code>sample(1:nrow(returns), n, replace = T)</code>: <strong>This
is the key line.</strong> It randomly selects row numbers <em>with
replacement</em> from the <em>single</em> <code>returns</code>
dataset.</li>
<li><code>returns_boot &lt;- returns[sample(...), ]</code>: This creates
the bootstrap sample.</li>
<li><code>alpha_bootstrap[i] &lt;- ...</code>: It calculates <span
class="math inline">\(\hat{\alpha}^*\)</span> on the
<code>returns_boot</code> sample.</li>
<li><strong>Purpose:</strong> This code generates the <em>bootstrap
distribution</em> (the ‚ÄúBootstrap‚Äù histogram on slide 78) to
<em>estimate</em> the true sampling distribution.</li>
</ul></li>
</ul>
<h3 id="r-linear-regression-example-slides-79-81">R: Linear Regression
Example (Slides 79 &amp; 81)</h3>
<ul>
<li><strong>Slide 79:</strong>
<ul>
<li><code>boot.fn &lt;- function(data, index)&#123; ... &#125;</code>: Defines a
function that the <code>boot</code> package needs. It takes data and an
<code>index</code> vector.</li>
<li><code>lm(mpg~horsepower, data=data, subset=index)</code>: This is
the core. It fits a linear model <em>only</em> on the data points
specified by the <code>index</code>. The <code>boot</code> function will
automatically supply this <code>index</code> as a
resampled-with-replacement vector.</li>
<li><code>boot(Auto, boot.fn, R=1000)</code>: This runs the bootstrap.
It calls <code>boot.fn</code> 1000 times, each time with a new resampled
<code>index</code>, and collects the coefficients.</li>
</ul></li>
<li><strong>Slide 81:</strong>
<ul>
<li><code>summary(lm(...))</code>: Shows the standard output. The ‚ÄúStd.
Error‚Äù column (e.g., 0.860, 0.006) is calculated using <em>mathematical
theory</em>.</li>
<li><code>boot.res</code>: Shows the bootstrap output. The ‚Äústd. error‚Äù
column (e.g., 0.841, 0.007) is the <strong>standard deviation of the
1000 bootstrap estimates</strong>.</li>
<li><strong>Main Point:</strong> The standard errors from the bootstrap
are very close to the theoretical ones. This confirms the uncertainty.
If the model assumptions were violated, the bootstrap SE would be more
trustworthy.</li>
<li>The histograms show the bootstrap distributions for the intercept
(<code>t1*</code>) and the slope (<code>t2*</code>). The arrows show the
95% percentile confidence interval.</li>
</ul></li>
</ul>
<h3 id="python-knn-regression-example-slide-80">Python: KNN Regression
Example (Slide 80)</h3>
<p>This shows how to get a confidence interval for a <em>single
prediction</em>.</p>
<ul>
<li><code>for i in range(n_bootstraps):</code>: The bootstrap loop.</li>
<li><code>indices = np.random.choice(train_samples.shape[0], train_samples.shape[0], replace=True)</code>:
<strong>This is the key line</strong> in Python (like
<code>sample</code> in R). It gets a new set of indices with
replacement.</li>
<li><code>X_boot, y_boot = ...</code>: Creates the bootstrap
sample.</li>
<li><code>model.fit(X_boot, y_boot)</code>: A <em>new</em> KNN model is
trained on this bootstrap sample.</li>
<li><code>bootstrap_preds.append(model.predict(predict_point))</code>:
The model (trained on <span class="math inline">\(Z^{*i}\)</span>) makes
a prediction for the <em>same</em> fixed point. This is repeated 1000
times.</li>
<li><strong>Result:</strong> You get a <em>distribution of
predictions</em> for that one point. The 2.5th and 97.5th percentiles of
this distribution give you a 95% confidence interval <em>for that
specific prediction</em>.</li>
</ul>
<h3 id="python-knn-on-auto-data-slide-82">Python: KNN on Auto data
(Slide 82)</h3>
<ul>
<li><strong>BE CAREFUL:</strong> This slide <strong>does NOT show
Bootstrap</strong>. It shows <strong>K-Fold Cross-Validation
(CV)</strong>.</li>
<li><strong>Purpose:</strong> The goal here is <em>not</em> to find
uncertainty. The goal is to find the <strong>best
hyperparameter</strong> (the best value for <span
class="math inline">\(k\)</span>, the number of neighbors).</li>
<li><strong>Method:</strong>
<ul>
<li><code>kf = KFold(n_splits=10)</code>: Splits the data into 10 chunks
(‚Äúfolds‚Äù).</li>
<li><code>for train_index, test_index in kf.split(X):</code>: It loops
10 times. Each time, it trains on 9 chunks and tests on 1 chunk.</li>
</ul></li>
<li><strong>Key Difference for Exam:</strong>
<ul>
<li><strong>Bootstrap:</strong> Samples <em>with replacement</em> to
estimate <strong>uncertainty/standard error</strong>.</li>
<li><strong>Cross-Validation:</strong> Splits data <em>without
replacement</em> into <span class="math inline">\(K\)</span> folds to
estimate model <strong>performance/prediction error</strong> and tune
hyperparameters.</li>
</ul></li>
</ul>
<h1
id="the-mathematical-theory-of-bootstrap-and-the-extension-to-cross-validation-cv.">7.
The mathematical theory of Bootstrap and the extension to
Cross-Validation (CV).</h1>
<h2 id="code-analysis-bootstrap-for-a-knn-prediction-slide-85">1. Code
Analysis: Bootstrap for a KNN Prediction (Slide 85)</h2>
<p>This Python code shows a different use of bootstrap: <strong>finding
the confidence interval for a single prediction</strong>, not for a
model coefficient.</p>
<ul>
<li><strong>Goal:</strong> To estimate the uncertainty of a KNN model‚Äôs
prediction for a <em>specific</em> new data point
(<code>predict_point</code>).</li>
<li><strong>Process:</strong>
<ol type="1">
<li><strong>Train Full Model:</strong> A KNN model (<code>knn</code>) is
first trained on the <em>entire</em> dataset. It makes one prediction
(<code>knpred</code>) for <code>predict_point</code>. This is our <span
class="math inline">\(\hat{f}(x_0)\)</span>.</li>
<li><strong>Bootstrap Loop
(<code>for i in range(n_bootstraps)</code>):</strong>
<ul>
<li><code>indices = np.random.choice(...)</code>: <strong>This is the
core bootstrap step.</strong> It creates a new list of indices by
sampling <em>with replacement</em> from the original data.</li>
<li><code>X_boot, y_boot = ...</code>: This creates the new bootstrap
dataset (<span class="math inline">\(Z^{*i}\)</span>).</li>
<li><code>km.fit(X_boot, y_boot)</code>: A <em>new</em> KNN model
(<code>km</code>) is trained <em>only</em> on this bootstrap
sample.</li>
<li><code>bootstrap_preds.append(km.predict(predict_point))</code>: This
newly trained model makes a prediction for the <em>same</em>
<code>predict_point</code>. This value is <span
class="math inline">\(\hat{f}^{*i}(x_0)\)</span>.</li>
</ul></li>
<li><strong>Analyze Distribution:</strong> After 1000 loops,
<code>bootstrap_preds</code> contains 1000 different predictions for the
same point.</li>
<li><strong>Confidence Interval:</strong>
<ul>
<li><code>np.percentile(bootstrap_preds, [2.5, 97.5])</code>: This finds
the 2.5th and 97.5th percentiles of the 1000 bootstrap predictions.</li>
<li>The resulting <code>[lower_bound, upper_bound]</code> (e.g.,
<code>[13.70, 15.70]</code>) forms the 95% confidence interval for the
prediction.</li>
</ul></li>
</ol></li>
<li><strong>Histogram Plot:</strong> The plot on the right visually
confirms this. It shows the distribution of the 1000 bootstrap
predictions, with the 95% confidence interval marked by the red dashed
lines.</li>
</ul>
<h2
id="mathematical-understanding-why-does-bootstrap-work-slides-87-88">2.
Mathematical Understanding: Why Does Bootstrap Work? (Slides 87-88)</h2>
<p>This is the theoretical justification for the entire method. It‚Äôs
based on an analogy.</p>
<h3 id="the-true-world-slide-87-top">The ‚ÄúTrue‚Äù World (Slide 87,
Top)</h3>
<ul>
<li><strong>Population:</strong> There is a true, unknown population
distribution <span class="math inline">\(F\)</span>.</li>
<li><strong>Parameter:</strong> We want to know a true parameter, <span
class="math inline">\(\theta\)</span>, which is a function of <span
class="math inline">\(F\)</span> (e.g., the true population mean).</li>
<li><strong>Sample:</strong> We get <em>one</em> sample <span
class="math inline">\(X_1, ..., X_n\)</span> from <span
class="math inline">\(F\)</span>.</li>
<li><strong>Statistic:</strong> We calculate our best estimate <span
class="math inline">\(\hat{\theta}\)</span> from our sample. (e.g., the
sample mean <span class="math inline">\(\bar{x}\)</span>). <span
class="math inline">\(\hat{\theta}\)</span> is our proxy for <span
class="math inline">\(\theta\)</span>.</li>
<li><strong>The Problem:</strong> We want to know the accuracy of <span
class="math inline">\(\hat{\theta}\)</span>. How much would <span
class="math inline">\(\hat{\theta}\)</span> vary if we could draw many
samples? We want the <em>sampling distribution</em> of <span
class="math inline">\(\hat{\theta}\)</span> around <span
class="math inline">\(\theta\)</span>, specifically the distribution of
the error: <span class="math inline">\((\hat{\theta} -
\theta)\)</span>.</li>
<li><strong>CLT:</strong> The Central Limit Theorem states that <span
class="math inline">\(\sqrt{n}(\hat{\theta} - \theta)
\xrightarrow{\text{dist}} N(0, Var_F(\theta))\)</span>.</li>
<li><strong>The Catch:</strong> This is <strong>UNKNOWN</strong> because
we don‚Äôt know <span class="math inline">\(F\)</span>.</li>
</ul>
<h3 id="the-bootstrap-world-slide-87-bottom">The ‚ÄúBootstrap‚Äù World
(Slide 87, Bottom)</h3>
<ul>
<li><strong>Population:</strong> We <em>pretend</em> our original sample
<em>is</em> the population. We call its distribution the ‚Äúempirical
distribution,‚Äù <span class="math inline">\(\hat{F}_n\)</span>.</li>
<li><strong>Parameter:</strong> In this new world, the ‚Äútrue‚Äù parameter
is our original statistic, <span
class="math inline">\(\hat{\theta}\)</span> (which is a function of
<span class="math inline">\(\hat{F}_n\)</span>).</li>
<li><strong>Sample:</strong> We draw <em>many</em> bootstrap samples
<span class="math inline">\(X_1^*, ..., X_n^*\)</span> <em>from <span
class="math inline">\(\hat{F}_n\)</span></em> (i.e., sampling <em>with
replacement</em> from our original sample).</li>
<li><strong>Statistic:</strong> From each bootstrap sample, we calculate
a <em>bootstrap statistic</em>, <span
class="math inline">\(\hat{\theta}^*\)</span>.</li>
<li><strong>The Solution:</strong> We can now <em>empirically</em> find
the distribution of <span class="math inline">\(\hat{\theta}^*\)</span>
around <span class="math inline">\(\hat{\theta}\)</span>. We look at the
distribution of the bootstrap error: <span
class="math inline">\((\hat{\theta}^* - \hat{\theta})\)</span>.</li>
<li><strong>CLT:</strong> The CLT also states that <span
class="math inline">\(\sqrt{n}(\hat{\theta}^* - \hat{\theta})
\xrightarrow{\text{dist}} N(0, Var_{\hat{F}_n}(\theta))\)</span>.</li>
<li><strong>The Power:</strong> This distribution is
<strong>ESTIMABLE!</strong> We just run the bootstrap <span
class="math inline">\(B\)</span> times and we get <span
class="math inline">\(B\)</span> values of <span
class="math inline">\(\hat{\theta}^*\)</span>. We can then calculate
their variance, standard deviation, and percentiles directly.</li>
</ul>
<h3 id="the-core-approximation-slide-88">The Core Approximation (Slide
88)</h3>
<p>The entire method relies on the assumption that <strong>the
(knowable) bootstrap distribution is a good approximation of the
(unknown) true sampling distribution.</strong></p>
<p>The distribution of the <em>bootstrap error</em> approximates the
distribution of the <em>true error</em>.</p>
<p><span class="math display">\[\text{distribution of }
\sqrt{n}(\hat{\theta}^* - \hat{\theta}) \approx \text{distribution of }
\sqrt{n}(\hat{\theta} - \theta)\]</span></p>
<p>This is why: * The <strong>standard deviation</strong> of the <span
class="math inline">\(\hat{\theta}^*\)</span> values is our estimate for
the <strong>standard error</strong> of <span
class="math inline">\(\hat{\theta}\)</span>. * The
<strong>percentiles</strong> of the <span
class="math inline">\(\hat{\theta}^*\)</span> distribution (e.g., 2.5th
and 97.5th) can be used to build a <strong>confidence interval</strong>
for the true parameter <span class="math inline">\(\theta\)</span>.</p>
<h2 id="extension-cross-validation-cv-analysis">3. Extension:
Cross-Validation (CV) Analysis</h2>
<h3 id="cv-for-hyperparameter-tuning-slide-84">CV for Hyperparameter
Tuning (Slide 84)</h3>
<p>This plot is the <em>result</em> of the 10-fold CV code shown in the
previous set of slides (slide 82). * <strong>Purpose:</strong> To find
the optimal hyperparameter <span class="math inline">\(k\)</span>
(number of neighbors) for the KNN model. * <strong>X-axis:</strong>
Number of Neighbors (<span class="math inline">\(k\)</span>). *
<strong>Y-axis:</strong> CV Error (Mean Squared Error). *
<strong>Analysis:</strong> * <strong>Low <span
class="math inline">\(k\)</span> (e.g., <span class="math inline">\(k=1,
2\)</span>):</strong> High error. The model is too complex and
<strong>overfitting</strong> to the training data. * <strong>High <span
class="math inline">\(k\)</span> (e.g., <span
class="math inline">\(k&gt;40\)</span>):</strong> Error slowly
increases. The model is too simple and <strong>underfitting</strong>
(e.g., averaging too many neighbors). * <strong>Optimal <span
class="math inline">\(k\)</span>:</strong> The ‚Äúsweet spot‚Äù is at the
bottom of the ‚ÄúU‚Äù shape, around <strong><span class="math inline">\(k
\approx 20-30\)</span></strong>, which gives the lowest CV error.</p>
<h3 id="why-cv-over-estimates-test-error-slide-89">Why CV Over-Estimates
Test Error (Slide 89)</h3>
<p>This is a subtle but important theoretical point. * <strong>Our
Goal:</strong> We want to know the test error of our <em>final
model</em> (<span class="math inline">\(\hat{f}^{\text{full}}\)</span>),
which we will train on the <strong>full dataset</strong> (all <span
class="math inline">\(n\)</span> observations). * <strong>What CV
Measures:</strong> <span class="math inline">\(k\)</span>-fold CV does
<em>not</em> test the final model. It tests <span
class="math inline">\(k\)</span> different models (<span
class="math inline">\(\hat{f}^{(k)}\)</span>), each trained on a
<em>smaller</em> dataset (of size <span
class="math inline">\(\frac{k-1}{k} \times n\)</span>). * <strong>The
Logic:</strong> 1. Models trained on <em>less data</em> generally
perform <em>worse</em> than models trained on <em>more data</em>. 2. The
CV error is the average error of models trained on <span
class="math inline">\(\frac{k-1}{k} n\)</span> observations. 3. The
‚Äútrue test error‚Äù is the error of the model trained on <span
class="math inline">\(n\)</span> observations. *
<strong>Conclusion:</strong> Since the CV models are trained on smaller
datasets, they will, on average, have a slightly higher error than the
final model. Therefore, <strong>the CV error score is a slightly
<em>pessimistic</em> estimate (it over-estimates) the true test error of
the final model.</strong></p>
<h3 id="correction-of-cv-error-slides-90-91">Correction of CV Error
(Slides 90-91)</h3>
<ul>
<li><strong>Theory (Slide 91):</strong> Advanced theory suggests the
expected test error <span class="math inline">\(R(n)\)</span> behaves
like <span class="math inline">\(R(n) = R^* + c/n\)</span>, where <span
class="math inline">\(R^*\)</span> is the irreducible error and <span
class="math inline">\(n\)</span> is the sample size. This formula
mathematically confirms that error <em>decreases</em> as sample size
<span class="math inline">\(n\)</span> <em>increases</em>.</li>
<li><strong>R Code (Slide 90):</strong> The <code>cv.glm</code> function
from the <code>boot</code> library automatically provides this.
<ul>
<li><code>cv.err$delta</code>: This output vector contains two
values.</li>
<li><code>[1] 24.23151</code> (Raw CV Error): This is the standard
Leave-One-Out CV (LOOCV) error.</li>
<li><code>[2] 24.23114</code> (Adjusted CV Error): This is a
bias-corrected estimate that accounts for the overestimation problem.
It‚Äôs slightly lower, representing a more accurate guess for the error of
the <em>final model</em> trained on all <span
class="math inline">\(n\)</span> data points.</li>
</ul></li>
</ul>
<p># The ‚ÄúCorrection of CV Error‚Äù extension.</p>
<h3 id="summary">Summary</h3>
<p>This section provides a deeper mathematical look at <em>why</em>
k-fold cross-validation (CV) slightly <strong>over-estimates</strong>
the true test error.</p>
<ol type="1">
<li><p><strong>The Overestimation:</strong> CV trains on <span
class="math inline">\(\frac{k-1}{k}\)</span> of the data, which is
<em>less</em> than the full dataset (size <span
class="math inline">\(n\)</span>). Models trained on less data are
generally <em>worse</em>. Therefore, the average error from CV (<span
class="math inline">\(CV_k\)</span>) is slightly <em>higher</em> (more
pessimistic) than the true error of the final model trained on all <span
class="math inline">\(n\)</span> data (<span
class="math inline">\(R(n)\)</span>).</p></li>
<li><p><strong>A Simple Correction:</strong> A mathematical formula,
<span class="math inline">\(\tilde{CV_k} = \frac{k-1}{k} \cdot
CV_k\)</span>, is proposed to ‚Äúcorrect‚Äù this overestimation.</p></li>
<li><p><strong>The Critical Flaw:</strong> This correction is derived
<em>assuming</em> the <strong>irreducible error (<span
class="math inline">\(R^*\)</span>) is zero</strong>.</p></li>
<li><p><strong>The Takeaway (Code Analysis):</strong> The Python code
demonstrates a real-world scenario where there is noise
(<code>noise_std = 0.5</code>), meaning <span class="math inline">\(R^*
&gt; 0\)</span>. In this case, the <strong>simple correction
fails</strong>‚Äîit produces an error (0.217) that is <em>less
accurate</em> and further from the true error (0.272) than the
<strong>original raw CV error</strong> (0.271).</p></li>
</ol>
<p><strong>Exam Conclusion:</strong> For most real-world problems (which
have noise), the <strong>raw <span class="math inline">\(k\)</span>-fold
CV error is a better and more reliable estimate</strong> of the true
test error than the simple (and flawed) correction.</p>
<h3 id="mathematical-understanding-1">Mathematical Understanding</h3>
<p>This section explains the theory of <em>why</em> <span
class="math inline">\(CV_k &gt; R(n)\)</span> and derives the simple
correction.</p>
<ol type="1">
<li><p><strong>Assumed Error Behavior:</strong> We assume the test error
<span class="math inline">\(R(n)\)</span> for a model trained on <span
class="math inline">\(n\)</span> data points behaves like: <span
class="math display">\[R(n) = R^* + \frac{c}{n}\]</span></p>
<ul>
<li><span class="math inline">\(R^*\)</span>: The <strong>irreducible
error</strong> (the ‚Äúnoise floor‚Äù you can never beat).</li>
<li><span class="math inline">\(c/n\)</span>: The model variance, which
<em>decreases</em> as sample size <span class="math inline">\(n\)</span>
<em>increases</em>.</li>
</ul></li>
<li><p><strong>Test Error vs.¬†CV Error:</strong></p>
<ul>
<li><strong>Test Error of Interest:</strong> This is the error of our
<em>final model</em> trained on all <span
class="math inline">\(n\)</span> points: <span
class="math display">\[R(n) = R^* + \frac{c}{n}\]</span></li>
<li><strong>k-fold CV Error:</strong> This is the average error of <span
class="math inline">\(k\)</span> models, each trained on a smaller
sample of size <span class="math inline">\(n&#39; =
(\frac{k-1}{k})n\)</span>. <span class="math display">\[CV_k \approx
R(n&#39;) = R\left(\frac{k-1}{k}n\right) = R^* +
\frac{c}{\left(\frac{k-1}{k}\right)n} = R^* +
\frac{ck}{(k-1)n}\]</span></li>
</ul></li>
<li><p><strong>The Overestimation:</strong> Let‚Äôs compare <span
class="math inline">\(CV_k\)</span> and <span
class="math inline">\(R(n)\)</span>: <span class="math display">\[CV_k
\approx R^* + \left(\frac{k}{k-1}\right) \frac{c}{n}\]</span> <span
class="math display">\[R(n) = R^* + \left(\frac{k-1}{k-1}\right)
\frac{c}{n}\]</span> Since <span class="math inline">\(k &gt;
(k-1)\)</span>, the factor <span
class="math inline">\(\left(\frac{k}{k-1}\right)\)</span> is
<strong>greater than 1</strong>. This means the <span
class="math inline">\(CV_k\)</span> error term is larger than the <span
class="math inline">\(R(n)\)</span> error term. Thus: <strong><span
class="math inline">\(CV_k &gt; \text{Test error of interest }
R(n)\)</span></strong></p></li>
<li><p><strong>Deriving the (Flawed) Correction:</strong> This
correction makes a <strong>strong assumption: <span
class="math inline">\(R^* \approx 0\)</span></strong> (the model is
perfectly specified, and there is no noise).</p>
<ul>
<li>If <span class="math inline">\(R^* = 0\)</span>, then <span
class="math inline">\(R(n) \approx \frac{c}{n}\)</span></li>
<li>If <span class="math inline">\(R^* = 0\)</span>, then <span
class="math inline">\(CV_k \approx \frac{ck}{(k-1)n}\)</span></li>
</ul>
<p>Now, look at the ratio between them: <span
class="math display">\[\frac{R(n)}{CV_k} \approx \frac{c/n}{ck/((k-1)n)}
= \frac{c}{n} \cdot \frac{(k-1)n}{ck} = \frac{k-1}{k}\]</span></p>
<p>This gives us the correction formula by isolating <span
class="math inline">\(R(n)\)</span>: <span class="math display">\[R(n)
\approx \left(\frac{k-1}{k}\right) \cdot CV_k\]</span> This corrected
version is denoted <span
class="math inline">\(\tilde{CV_k}\)</span>.</p></li>
</ol>
<h3 id="code-analysis-slides-92-93">Code Analysis (Slides 92-93)</h3>
<p>The Python code is an experiment designed to <strong>test the
correction formula</strong>.</p>
<ul>
<li><p><strong>Goal:</strong> Compare the ‚ÄúRaw CV Error‚Äù (<span
class="math inline">\(CV_k\)</span>), the ‚ÄúCorrected CV Error‚Äù (<span
class="math inline">\(\tilde{CV_k}\)</span>), and the ‚ÄúTrue Test Error‚Äù
(<span class="math inline">\(R(n)\)</span>) in a realistic
setting.</p></li>
<li><p><strong>Key Setup:</strong></p>
<ol type="1">
<li><code>def f(x)</code>: Defines the true, underlying function <span
class="math inline">\(y = x^2 + 15\sin(x)\)</span>.</li>
<li><code>noise_std = 0.5</code>: <strong>This is the most important
line.</strong> It adds significant random noise to the data. This
ensures that the <strong>irreducible error <span
class="math inline">\(R^*\)</span> is large and <span
class="math inline">\(R^* &gt; 0\)</span></strong>.</li>
<li><code>y = f(...) + np.random.normal(...)</code>: Creates the noisy
training data (the blue dots).</li>
</ol></li>
<li><p><strong>CV Calculation (Standard K-Fold):</strong></p>
<ul>
<li><code>kf = KFold(...)</code>: Sets up 5-fold CV (<span
class="math inline">\(k=5\)</span>).</li>
<li><code>for train_index, val_index in kf.split(x):</code>: This is the
standard loop. It trains on 4 folds and validates on 1 fold.</li>
<li><code>cv_error = np.mean(cv_mse_list)</code>: Calculates the
<strong>raw <span class="math inline">\(CV_5\)</span> error</strong>.
This is the first result (e.g., <strong>0.2715</strong>).</li>
</ul></li>
<li><p><strong>Correction Calculation:</strong></p>
<ul>
<li><code>correction_factor = (k_splits - 1) / k_splits</code>: This is
<span class="math inline">\(\frac{k-1}{k}\)</span>, which is <span
class="math inline">\(4/5 = 0.8\)</span>.</li>
<li><code>corrected_cv_error = correction_factor * cv_error</code>: This
applies the flawed formula from the math section (<span
class="math inline">\(0.2715 \times 0.8\)</span>). This is the second
result (e.g., <strong>0.2172</strong>).</li>
</ul></li>
<li><p><strong>‚ÄúTrue‚Äù Test Error Calculation:</strong></p>
<ul>
<li><code>knn.fit(x, y)</code>: Trains the <em>final model</em> on the
<em>entire</em> noisy dataset.</li>
<li><code>n_test = 1000</code>: Creates a <em>new, large</em> test set
to estimate the true error.</li>
<li><code>true_test_error = mean_squared_error(...)</code>: Calculates
the error of the final model on this new test set. This is our best
estimate of <span class="math inline">\(R(n)\)</span> (e.g.,
<strong>0.2725</strong>).</li>
</ul></li>
<li><p><strong>Analysis of Results (Slide 93):</strong></p>
<ul>
<li><strong>Raw 5-Fold CV MSE:</strong> 0.2715</li>
<li><strong>True test error:</strong> 0.2725</li>
<li><strong>Corrected 5-Fold CV MSE:</strong> 0.2172</li>
</ul>
<p>The <strong>Raw CV Error (0.2715) is an excellent estimate</strong>
of the True Test Error (0.2725). The <strong>Corrected Error (0.2172) is
much worse</strong>. This experiment <em>proves</em> that when noise
(<span class="math inline">\(R^*\)</span>) is present, the simple
correction formula should not be used.</p></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/01/5054C4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/01/5054C4/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">ÂèëË°®‰∫é</span>

              <time title="ÂàõÂª∫Êó∂Èó¥Ôºö2025-10-01 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-01T21:00:00+08:00">2025-10-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Êõ¥Êñ∞‰∫é</span>
                <time title="‰øÆÊîπÊó∂Èó¥Ôºö2025-10-18 23:00:24" itemprop="dateModified" datetime="2025-10-18T23:00:24+08:00">2025-10-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">ÂàÜÁ±ª‰∫é</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>ÁªüËÆ°Êú∫Âô®Â≠¶‰π†Lecture-4</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="what-is-classification">1. What is Classification?</h1>
<p>Classification is a type of <strong>supervised machine
learning</strong> where the goal is to predict a
<strong>categorical</strong> or qualitative response. Unlike regression
where you predict a continuous numerical value (like a price or
temperature), classification assigns an input to a specific category or
class.
ÂàÜÁ±ªÊòØ‰∏ÄÁßç<strong>ÁõëÁù£ÂºèÊú∫Âô®Â≠¶‰π†</strong>ÔºåÂÖ∂ÁõÆÊ†áÊòØÈ¢ÑÊµã<strong>ÂàÜÁ±ª</strong>ÊàñÂÆöÊÄßÂìçÂ∫î„ÄÇ‰∏éÈ¢ÑÊµãËøûÁª≠Êï∞ÂÄºÔºà‰æãÂ¶Ç‰ª∑Ê†ºÊàñÊ∏©Â∫¶ÔºâÁöÑÂõûÂΩí‰∏çÂêåÔºåÂàÜÁ±ªÂ∞ÜËæìÂÖ•ÂàÜÈÖçÂà∞ÁâπÂÆöÁöÑÁ±ªÂà´ÊàñÁ±ªÂà´„ÄÇ</p>
<p><strong>Key characteristics:</strong></p>
<ul>
<li><p><strong>Goal:</strong> Predict the class of a subject based on
input features.</p></li>
<li><p><strong>Output (Response):</strong> The output is a category,
such as ‚ÄòYes‚Äô/‚ÄòNo‚Äô, ‚ÄòSpam‚Äô/‚ÄòNot Spam‚Äô, or
‚ÄòHigh‚Äô/‚ÄòMedium‚Äô/‚ÄòLow‚Äô.</p></li>
<li><p><strong>Applications:</strong> Common examples include email spam
detectors, medical diagnosis (e.g., virus carrier vs.¬†non-carrier), and
fraud detection.</p>
<ul>
<li><strong>ÁõÆÊ†á</strong>ÔºöÊ†πÊçÆËæìÂÖ•ÁâπÂæÅÈ¢ÑÊµã‰∏ªÈ¢òÁöÑÁ±ªÂà´„ÄÇ</li>
<li><strong>ËæìÂá∫ÔºàÂìçÂ∫îÔºâÔºö</strong>ËæìÂá∫ÊòØ‰∏Ä‰∏™Á±ªÂà´Ôºå‰æãÂ¶Ç‚ÄúÊòØ‚Äù/‚ÄúÂê¶‚Äù„ÄÅ‚ÄúÂûÉÂúæÈÇÆ‰ª∂‚Äù/‚ÄúÈùûÂûÉÂúæÈÇÆ‰ª∂‚ÄùÊàñ‚ÄúÈ´ò‚Äù/‚Äú‰∏≠‚Äù/‚Äú‰Ωé‚Äù„ÄÇ</li>
<li><strong>Â∫îÁî®</strong>ÔºöÂ∏∏ËßÅÁ§∫‰æãÂåÖÊã¨ÂûÉÂúæÈÇÆ‰ª∂Ê£ÄÊµãÂô®„ÄÅÂåªÂ≠¶ËØäÊñ≠Ôºà‰æãÂ¶ÇÔºåÁóÖÊØíÊê∫Â∏¶ËÄÖ‰∏éÈùûÁóÖÊØíÊê∫Â∏¶ËÄÖÔºâÂíåÊ¨∫ËØàÊ£ÄÊµã„ÄÇ
The example used in the slides is a credit card <strong>Default
dataset</strong>. The goal is to predict whether a customer will
<strong>default</strong> (‚ÄòYes‚Äô or ‚ÄòNo‚Äô) on their payments based on
their monthly <strong>income</strong> and account
<strong>balance</strong>.</li>
</ul></li>
</ul>
<p>## Why Not Use Linear Regression?‰∏∫‰ªÄ‰πà‰∏ç‰ΩøÁî®Á∫øÊÄßÂõûÂΩíÔºü</p>
<p>At first, it might seem possible to use linear regression for
classification. For a binary (two-class) problem like the default
dataset, you could code the outcomes as numbers, for example:</p>
<ul>
<li>Default = ‚ÄòNo‚Äô =&gt; <span class="math inline">\(y = 0\)</span></li>
<li>Default = ‚ÄòYes‚Äô =&gt; <span class="math inline">\(y =
1\)</span></li>
</ul>
<p>You could then fit a standard linear regression model: <span
class="math inline">\(Y \approx \beta_0 + \beta_1 X\)</span>. In this
context, we would interpret the prediction <span
class="math inline">\(\hat{y}\)</span> as the <em>probability</em> of
default, so we‚Äôd be modeling <span class="math inline">\(P(Y=1|X) =
\beta_0 + \beta_1 X\)</span>.</p>
<p>However, this approach has two major problems:
ÁÑ∂ËÄåÔºåËøôÁßçÊñπÊ≥ïÊúâ‰∏§‰∏™‰∏ªË¶ÅÈóÆÈ¢òÔºö <strong>1. The Output Is Not a
Probability</strong> A linear model can produce outputs that are less
than 0 or greater than 1. This doesn‚Äôt make sense for a probability,
which must always be between 0 and 1.</p>
<p>The image below is the most important one for understanding this
issue. The left plot shows a linear regression line fit to the 0/1
default data. You can see the line goes below 0 and would eventually go
above 1 for higher balances. The right plot shows a logistic regression
curve, which always stays between 0 and 1.</p>
<ul>
<li><strong>Left (Linear Regression):</strong> The straight blue line
predicts probabilities &lt; 0 for low balances.</li>
<li><strong>Right (Logistic Regression):</strong> The S-shaped blue
curve correctly constrains the probability output between 0 and 1.</li>
</ul>
<p><strong>2. It Doesn‚Äôt Work for Multi-Class Problems</strong> If you
have more than two categories (e.g., ‚Äòmild‚Äô, ‚Äòmoderate‚Äô, ‚Äòsevere‚Äô), you
might code them as 0, 1, and 2. A linear regression model would
incorrectly assume that the ‚Äúdistance‚Äù between ‚Äòmild‚Äô and ‚Äòmoderate‚Äô is
the same as the distance between ‚Äòmoderate‚Äô and ‚Äòsevere‚Äô, which is
usually not a valid assumption.</p>
<p><strong>1. ËæìÂá∫‰∏çÊòØÊ¶ÇÁéá</strong> Á∫øÊÄßÊ®°ÂûãÂèØ‰ª•‰∫ßÁîüÂ∞è‰∫é 0 ÊàñÂ§ß‰∫é 1
ÁöÑËæìÂá∫„ÄÇËøôÂØπ‰∫éÊ¶ÇÁéáÊù•ËØ¥ÊØ´Êó†ÊÑè‰πâÔºåÂõ†‰∏∫Ê¶ÇÁéáÂøÖÈ°ªÂßãÁªà‰ªã‰∫é 0 Âíå 1 ‰πãÈó¥„ÄÇ</p>
<p>‰∏ãÂõæÊòØÁêÜËß£Ëøô‰∏™ÈóÆÈ¢òÊúÄÈáçË¶ÅÁöÑÂõæ„ÄÇÂ∑¶ÂõæÊòæÁ§∫‰∫Ü‰∏é 0/1
ÈªòËÆ§Êï∞ÊçÆÊãüÂêàÁöÑÁ∫øÊÄßÂõûÂΩíÁ∫ø„ÄÇÊÇ®ÂèØ‰ª•ÁúãÂà∞ÔºåËØ•Á∫ø‰Ωé‰∫é
0ÔºåÂπ∂‰∏îÊúÄÁªà‰ºöÈöèÁùÄ‰ΩôÈ¢ùÁöÑÂ¢ûÂä†ËÄåÈ´ò‰∫é
1„ÄÇÂè≥ÂõæÊòæÁ§∫‰∫ÜÈÄªËæëÂõûÂΩíÊõ≤Á∫øÔºåÂÆÉÂßãÁªà‰øùÊåÅÂú® 0 Âíå 1 ‰πãÈó¥„ÄÇ</p>
<ul>
<li><strong>Â∑¶ÂõæÔºàÁ∫øÊÄßÂõûÂΩíÔºâÔºö</strong>ËìùËâ≤Áõ¥Á∫øÈ¢ÑÊµã‰Ωé‰ΩôÈ¢ùÁöÑÊ¶ÇÁéáÂ∞è‰∫é
0„ÄÇ</li>
<li><strong>Âè≥ÂõæÔºàÈÄªËæëÂõûÂΩíÔºâÔºö</strong>S
ÂΩ¢ËìùËâ≤Êõ≤Á∫øÊ≠£Á°ÆÂú∞Â∞ÜÊ¶ÇÁéáËæìÂá∫ÈôêÂà∂Âú® 0 Âíå 1 ‰πãÈó¥„ÄÇ</li>
</ul>
<p><strong>2.ÂÆÉ‰∏çÈÄÇÁî®‰∫éÂ§öÁ±ªÂà´ÈóÆÈ¢ò</strong>
Â¶ÇÊûúÊÇ®Êúâ‰∏§‰∏™‰ª•‰∏äÁöÑÁ±ªÂà´Ôºà‰æãÂ¶ÇÔºå‚ÄúËΩªÂ∫¶‚Äù„ÄÅ‚Äú‰∏≠Â∫¶‚Äù„ÄÅ‚ÄúÈáçÂ∫¶‚ÄùÔºâÔºåÊÇ®ÂèØËÉΩ‰ºöÂ∞ÜÂÆÉ‰ª¨ÁºñÁ†Å‰∏∫
0„ÄÅ1 Âíå
2„ÄÇÁ∫øÊÄßÂõûÂΩíÊ®°Âûã‰ºöÈîôËØØÂú∞ÂÅáËÆæ‚ÄúËΩªÂ∫¶‚ÄùÂíå‚Äú‰∏≠Â∫¶‚Äù‰πãÈó¥ÁöÑ‚ÄúË∑ùÁ¶ª‚Äù‰∏é‚Äú‰∏≠Â∫¶‚ÄùÂíå‚ÄúÈáçÂ∫¶‚Äù‰πãÈó¥ÁöÑË∑ùÁ¶ªÁõ∏ÂêåÔºåËøôÈÄöÂ∏∏‰∏çÊòØ‰∏Ä‰∏™ÊúâÊïàÁöÑÂÅáËÆæ„ÄÇ</p>
<p>## The Solution: Logistic Regression</p>
<p>Instead of modeling the response <span
class="math inline">\(y\)</span> directly, logistic regression models
the <strong>probability</strong> that <span
class="math inline">\(y\)</span> belongs to a particular class. To solve
the issue of the output not being a probability, it uses the
<strong>logistic function</strong> (also known as the sigmoid
function).</p>
<p>This function takes any real-valued input and squeezes it into an
output between 0 and 1.</p>
<p>The formula for the probability in a logistic regression model is:
<span class="math display">\[P(Y=1|X) = \frac{e^{\beta_0 + \beta_1 X}}{1
+ e^{\beta_0 + \beta_1 X}}\]</span> This S-shaped function, shown in the
right-hand plot above, ensures that the output is always a valid
probability. We can then set a threshold (e.g., 0.5) to make the final
class prediction. If <span class="math inline">\(P(Y=1|X) &gt;
0.5\)</span>, we predict ‚ÄòYes‚Äô; otherwise, we predict ‚ÄòNo‚Äô.</p>
<p>## Ëß£ÂÜ≥ÊñπÊ°àÔºöÈÄªËæëÂõûÂΩí</p>
<p>ÈÄªËæëÂõûÂΩí‰∏çÊòØÁõ¥Êé•ÂØπÂìçÂ∫î <span class="math inline">\(y\)</span>
ËøõË°åÂª∫Ê®°ÔºåËÄåÊòØÂØπ <span class="math inline">\(y\)</span>
Â±û‰∫éÁâπÂÆöÁ±ªÂà´ÁöÑ<strong>Ê¶ÇÁéá</strong>ËøõË°åÂª∫Ê®°„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥ËæìÂá∫‰∏çÊòØÊ¶ÇÁéáÁöÑÈóÆÈ¢òÔºåÂÆÉ‰ΩøÁî®‰∫Ü<strong>ÈÄªËæëÂáΩÊï∞</strong>Ôºà‰πüÁß∞‰∏∫
S ÂûãÂáΩÊï∞Ôºâ„ÄÇ</p>
<p>Ê≠§ÂáΩÊï∞Êé•Âèó‰ªª‰ΩïÂÆûÂÄºËæìÂÖ•ÔºåÂπ∂Â∞ÜÂÖ∂ÂéãÁº©‰∏∫‰ªã‰∫é 0 Âíå 1 ‰πãÈó¥ÁöÑËæìÂá∫„ÄÇ</p>
<p>ÈÄªËæëÂõûÂΩíÊ®°Âûã‰∏≠ÁöÑÊ¶ÇÁéáÂÖ¨Âºè‰∏∫Ôºö <span class="math display">\[P(Y=1|X) =
\frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}\]</span>
Â¶Ç‰∏äÂõæÂè≥‰æßÊâÄÁ§∫ÔºåËøô‰∏™ S
ÂΩ¢ÂáΩÊï∞Á°Æ‰øùËæìÂá∫ÂßãÁªàÊòØÊúâÊïàÊ¶ÇÁéá„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ËÆæÁΩÆ‰∏Ä‰∏™ÈòàÂÄºÔºà‰æãÂ¶Ç
0.5ÔºâÊù•ËøõË°åÊúÄÁªàÁöÑÁ±ªÂà´È¢ÑÊµã„ÄÇÂ¶ÇÊûú <span class="math inline">\(P(Y=1|X)
&gt; 0.5\)</span>ÔºåÂàôÈ¢ÑÊµã‚ÄúÊòØ‚ÄùÔºõÂê¶ÂàôÔºåÈ¢ÑÊµã‚ÄúÂê¶‚Äù„ÄÇ</p>
<p>## Data Visualization &amp; Code in Python</p>
<p>The slides use R to visualize the data. The boxplots are particularly
important because they show which variable is a better predictor.</p>
<ul>
<li><p><strong>Balance vs.¬†Default:</strong> The boxplots for balance
show a clear difference. The median balance for those who default
(‚ÄòYes‚Äô) is much higher than for those who do not (‚ÄòNo‚Äô). This suggests
<strong>balance is a strong predictor</strong>.</p></li>
<li><p><strong>Income vs.¬†Default:</strong> The boxplots for income show
a lot of overlap. The median incomes for both groups are very similar.
This suggests <strong>income is a weak predictor</strong>.</p></li>
<li><p><strong>‰ΩôÈ¢ù
vs.¬†ËøùÁ∫¶</strong>Ôºö‰ΩôÈ¢ùÁöÑÁÆ±Á∫øÂõæÊòæÁ§∫Âá∫ÊòéÊòæÁöÑÂ∑ÆÂºÇ„ÄÇËøùÁ∫¶ËÄÖÔºà‚ÄúÊòØ‚ÄùÔºâÁöÑ‰ΩôÈ¢ù‰∏≠‰ΩçÊï∞ËøúÈ´ò‰∫éÊú™ËøùÁ∫¶ËÄÖÔºà‚ÄúÂê¶‚ÄùÔºâ„ÄÇËøôË°®Êòé<strong>‰ΩôÈ¢ùÊòØ‰∏Ä‰∏™Âº∫ÊúâÂäõÁöÑÈ¢ÑÊµãÊåáÊ†á</strong>„ÄÇ</p></li>
<li><p><strong>Êî∂ÂÖ•
vs.¬†ËøùÁ∫¶</strong>ÔºöÊî∂ÂÖ•ÁöÑÁÆ±Á∫øÂõæÊòæÁ§∫Âá∫ÂæàÂ§ßÁöÑÈáçÂè†„ÄÇ‰∏§ÁªÑÁöÑÊî∂ÂÖ•‰∏≠‰ΩçÊï∞ÈùûÂ∏∏Áõ∏‰ºº„ÄÇËøôË°®Êòé<strong>Êî∂ÂÖ•ÊòØ‰∏Ä‰∏™Âº±ÁöÑÈ¢ÑÊµãÊåáÊ†á</strong>„ÄÇ</p></li>
</ul>
<p>Here‚Äôs how you could perform similar analysis and modeling in Python
using <code>seaborn</code> and <code>scikit-learn</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;default_data.csv&#x27; has columns: &#x27;default&#x27; (Yes/No), &#x27;balance&#x27;, &#x27;income&#x27;</span></span><br><span class="line"><span class="comment"># You would load your data like this:</span></span><br><span class="line"><span class="comment"># df = pd.read_csv(&#x27;default_data.csv&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># For demonstration, let&#x27;s create some sample data</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;balance&#x27;</span>: [<span class="number">1200</span>, <span class="number">2100</span>, <span class="number">800</span>, <span class="number">1800</span>, <span class="number">500</span>, <span class="number">1600</span>, <span class="number">2200</span>, <span class="number">1900</span>],</span><br><span class="line">    <span class="string">&#x27;income&#x27;</span>: [<span class="number">45000</span>, <span class="number">60000</span>, <span class="number">30000</span>, <span class="number">55000</span>, <span class="number">25000</span>, <span class="number">48000</span>, <span class="number">70000</span>, <span class="number">65000</span>],</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>: [<span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. Data Visualization (like the slides) ---</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">14</span>, <span class="number">5</span>))</span><br><span class="line">fig.suptitle(<span class="string">&#x27;Predictor Analysis for Default&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boxplot for Balance</span></span><br><span class="line">sns.boxplot(ax=axes[<span class="number">0</span>], x=<span class="string">&#x27;default&#x27;</span>, y=<span class="string">&#x27;balance&#x27;</span>, data=df)</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&#x27;Balance vs. Default Status&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boxplot for Income</span></span><br><span class="line">sns.boxplot(ax=axes[<span class="number">1</span>], x=<span class="string">&#x27;default&#x27;</span>, y=<span class="string">&#x27;income&#x27;</span>, data=df)</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">&#x27;Income vs. Default Status&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. Logistic Regression Modeling ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert categorical &#x27;default&#x27; column to 0s and 1s</span></span><br><span class="line">df[<span class="string">&#x27;default_encoded&#x27;</span>] = df[<span class="string">&#x27;default&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x == <span class="string">&#x27;Yes&#x27;</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define features (X) and target (y)</span></span><br><span class="line">X = df[[<span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line">y = df[<span class="string">&#x27;default_encoded&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create and train the logistic regression model</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on new data</span></span><br><span class="line"><span class="comment"># For example, a person with a $2000 balance and $50,000 income</span></span><br><span class="line">new_customer = [[<span class="number">2000</span>, <span class="number">50000</span>]]</span><br><span class="line">predicted_prob = model.predict_proba(new_customer)</span><br><span class="line">prediction = model.predict(new_customer)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Customer data: Balance=2000, Income=50000&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Probability of No Default vs. Default: <span class="subst">&#123;predicted_prob&#125;</span>&quot;</span>) <span class="comment"># [[P(No), P(Yes)]]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Final Prediction (0=No, 1=Yes): <span class="subst">&#123;prediction&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="the-mathematical-foundation-of-logistic-regression">2. the
mathematical foundation of logistic regression</h1>
<p>This set of slides explains the mathematical foundation of logistic
regression, how its parameters are estimated using Maximum Likelihood
Estimation (MLE), and how an iterative algorithm called Newton-Raphson
is used to perform this estimation.</p>
<p>ÈÄªËæëÂõûÂΩíÁöÑÊï∞Â≠¶Âü∫Á°Ä„ÄÅÂ¶Ç‰Ωï‰ΩøÁî®ÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ° (MLE)
‰º∞ËÆ°ÂÖ∂ÂèÇÊï∞Ôºå‰ª•ÂèäÂ¶Ç‰Ωï‰ΩøÁî®Âêç‰∏∫ Newton-Raphson ÁöÑËø≠‰ª£ÁÆóÊ≥ïËøõË°å‰º∞ËÆ°„ÄÇ</p>
<h2
id="the-logistic-regression-model-from-probabilities-to-log-oddsÈÄªËæëÂõûÂΩíÊ®°Âûã‰ªéÊ¶ÇÁéáÂà∞ÂØπÊï∞Âá†Áéá">2.1
The Logistic Regression Model: From Probabilities to
Log-OddsÈÄªËæëÂõûÂΩíÊ®°ÂûãÔºö‰ªéÊ¶ÇÁéáÂà∞ÂØπÊï∞Âá†Áéá</h2>
<p>The core of logistic regression is transforming a linear model into a
valid probability. This is done using the <strong>logistic
function</strong>, also known as the sigmoid function.
ÈÄªËæëÂõûÂΩíÁöÑÊ†∏ÂøÉÊòØÂ∞ÜÁ∫øÊÄßÊ®°ÂûãËΩ¨Êç¢‰∏∫ÊúâÊïàÁöÑÊ¶ÇÁéá„ÄÇËøôÂèØ‰ª•ÈÄöËøá<strong>ÈÄªËæëÂáΩÊï∞</strong>Ôºà‰πüÁß∞‰∏∫
S ÂûãÂáΩÊï∞ÔºâÊù•ÂÆûÁé∞„ÄÇ #### <strong>Key Mathematical Formulas</strong></p>
<ol type="1">
<li><p><strong>Probability of Class 1:</strong> The model assumes the
probability of an observation <span
class="math inline">\(\mathbf{x}\)</span> belonging to class 1 is given
by the sigmoid function: <span class="math display">\[
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-\beta^T \mathbf{x})} =
\frac{\exp(\beta^T \mathbf{x})}{1 + \exp(\beta^T \mathbf{x})}
\]</span> This function always outputs a value between 0 and 1, making
it perfect for modeling probabilities.</p></li>
<li><p><strong>Odds:</strong> The odds are the ratio of the probability
of an event happening to the probability of it not happening. <span
class="math display">\[
\text{Odds} = \frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})} = \exp(\beta^T
\mathbf{x})
\]</span></p></li>
<li><p><strong>Log-Odds (Logit):</strong> By taking the natural
logarithm of the odds, we get a linear relationship with the predictors.
This is called the <strong>logit transformation</strong>. <span
class="math display">\[
\text{logit}(P(y=1|\mathbf{x})) =
\log\left(\frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})}\right) = \beta^T
\mathbf{x}
\]</span> This final equation is the heart of the model. It states that
the log-odds of the outcome are a linear function of the predictors.
This provides a great interpretation: a one-unit increase in a predictor
<span class="math inline">\(x_j\)</span> changes the log-odds by <span
class="math inline">\(\beta_j\)</span>.</p></li>
<li><p><strong>Á±ªÂà´ 1 ÁöÑÊ¶ÇÁéá</strong>ÔºöËØ•Ê®°ÂûãÂÅáËÆæËßÇÊµãÂÄº <span
class="math inline">\(\mathbf{x}\)</span> Â±û‰∫éÁ±ªÂà´ 1 ÁöÑÊ¶ÇÁéáÁî± S
ÂûãÂáΩÊï∞ÁªôÂá∫Ôºö <span class="math display">\[
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-\beta^T \mathbf{x})} =
\frac{\exp(\beta^T \mathbf{x})}{1 + \exp(\beta^T \mathbf{x})}
\]</span> Ê≠§ÂáΩÊï∞ÁöÑËæìÂá∫ÂÄºÂßãÁªà‰ªã‰∫é 0 Âíå 1
‰πãÈó¥ÔºåÈùûÂ∏∏ÈÄÇÂêàÁî®‰∫éÊ¶ÇÁéáÂª∫Ê®°„ÄÇ</p></li>
<li><p><strong>Âá†Áéá</strong>Ôºö**Âá†ÁéáÊòØ‰∫ã‰ª∂ÂèëÁîüÁöÑÊ¶ÇÁéá‰∏é‰∏çÂèëÁîüÁöÑÊ¶ÇÁéá‰πãÊØî„ÄÇ
<span class="math display">\[
\text{Odds} = \frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})} = \exp(\beta^T
\mathbf{x})
\]</span></p></li>
<li><p><strong>ÂØπÊï∞Ê¶ÇÁéá
(Logit)</strong>ÔºöÈÄöËøáÂØπÊ¶ÇÁéáÂèñËá™ÁÑ∂ÂØπÊï∞ÔºåÊàë‰ª¨ÂèØ‰ª•ÂæóÂà∞Ê¶ÇÁéá‰∏éÈ¢ÑÊµãÂèòÈáè‰πãÈó¥ÁöÑÁ∫øÊÄßÂÖ≥Á≥ª„ÄÇËøôË¢´Áß∞‰∏∫<strong>logit
ÂèòÊç¢</strong>„ÄÇ <span class="math display">\[
\text{logit}(P(y=1|\mathbf{x})) =
\log\left(\frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})}\right) = \beta^T
\mathbf{x}
\]</span>
ÊúÄÂêé‰∏Ä‰∏™ÊñπÁ®ãÊòØÊ®°ÂûãÁöÑÊ†∏ÂøÉ„ÄÇÂÆÉÊåáÂá∫ÁªìÊûúÁöÑÂØπÊï∞Ê¶ÇÁéáÊòØÈ¢ÑÊµãÂèòÈáèÁöÑÁ∫øÊÄßÂáΩÊï∞„ÄÇËøôÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂæàÂ•ΩÁöÑËß£ÈáäÔºöÈ¢ÑÊµãÂèòÈáè
<span class="math inline">\(x_j\)</span>
ÊØèÂ¢ûÂä†‰∏Ä‰∏™Âçï‰ΩçÔºåÂØπÊï∞Ê¶ÇÁéáÂ∞±‰ºöÊîπÂèò <span
class="math inline">\(\beta_j\)</span>„ÄÇ</p></li>
</ol>
<h2
id="fitting-the-model-maximum-likelihood-estimation-mle-ÊãüÂêàÊ®°ÂûãÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°-mle">2.2
Fitting the Model: Maximum Likelihood Estimation (MLE)
ÊãüÂêàÊ®°ÂûãÔºöÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ° (MLE)</h2>
<p>Unlike linear regression, which uses least squares to find the
best-fit line, logistic regression uses <strong>Maximum Likelihood
Estimation (MLE)</strong>. The goal of MLE is to find the parameter
values (the <span class="math inline">\(\beta\)</span> coefficients)
that maximize the probability of observing the actual data that we have.
‰∏é‰ΩøÁî®ÊúÄÂ∞è‰∫å‰πòÊ≥ïÂØªÊâæÊúÄ‰Ω≥ÊãüÂêàÁ∫øÁöÑÁ∫øÊÄßÂõûÂΩí‰∏çÂêåÔºåÈÄªËæëÂõûÂΩí‰ΩøÁî®<strong>ÊúÄÂ§ß‰ººÁÑ∂‰º∞ËÆ°
(MLE)</strong>„ÄÇMLE
ÁöÑÁõÆÊ†áÊòØÊâæÂà∞‰ΩøËßÇÊµãÂà∞ÂÆûÈôÖÊï∞ÊçÆÁöÑÊ¶ÇÁéáÊúÄÂ§ßÂåñÁöÑÂèÇÊï∞ÂÄºÔºà<span
class="math inline">\(\beta\)</span> Á≥ªÊï∞Ôºâ„ÄÇ</p>
<ol type="1">
<li><p><strong>Likelihood Function:</strong> This is the joint
probability of observing all the data points in our sample. Assuming
each observation is independent, it‚Äôs the product of the individual
probabilities:
1.<strong>‰ººÁÑ∂ÂáΩÊï∞</strong>ÔºöËøôÊòØËßÇÊµãÂà∞Ê†∑Êú¨‰∏≠ÊâÄÊúâÊï∞ÊçÆÁÇπÁöÑËÅîÂêàÊ¶ÇÁéá„ÄÇÂÅáËÆæÊØè‰∏™ËßÇÊµãÂÄºÈÉΩÊòØÁã¨Á´ãÁöÑÔºåÂÆÉÊòØÂêÑ‰∏™Ê¶ÇÁéáÁöÑ‰πòÁßØÔºö
<span class="math display">\[
L(\beta) = \prod_{i=1}^{n} P(y_i|\mathbf{x}_i)
\]</span> A clever way to write this for a binary (0/1) outcome is:
<span class="math display">\[
L(\beta) = \prod_{i=1}^{n} \frac{\exp(y_i \beta^T \mathbf{x}_i)}{1 +
\exp(\beta^T \mathbf{x}_i)}
\]</span></p></li>
<li><p><strong>Log-Likelihood Function:</strong> Products are difficult
to work with mathematically, so we work with the logarithm of the
likelihood, which turns the product into a sum. Maximizing the
log-likelihood is the same as maximizing the likelihood.</p></li>
<li><p><strong>ÂØπÊï∞‰ººÁÑ∂ÂáΩÊï∞</strong>Ôºö‰πòÁßØÂú®Êï∞Â≠¶‰∏äÂæàÈöæÂ§ÑÁêÜÔºåÊâÄ‰ª•Êàë‰ª¨‰ΩøÁî®‰ººÁÑ∂ÁöÑÂØπÊï∞ÔºåÂ∞Ü‰πòÁßØËΩ¨Âåñ‰∏∫Âíå„ÄÇÊúÄÂ§ßÂåñÂØπÊï∞‰ººÁÑ∂‰∏éÊúÄÂ§ßÂåñ‰ººÁÑ∂Áõ∏Âêå„ÄÇ
<span class="math display">\[
\ell(\beta) = \log(L(\beta)) = \sum_{i=1}^{n} \left[ y_i \beta^T
\mathbf{x}_i - \log(1 + \exp(\beta^T \mathbf{x}_i)) \right]
\]</span> <strong>Key Takeaway:</strong> The slides correctly state that
there is <strong>no explicit formula</strong> to solve for the <span
class="math inline">\(\hat{\beta}\)</span> that maximizes this function.
We must find it using a numerical optimization algorithm.
Ê≤°Êúâ<strong>ÊòéÁ°ÆÁöÑÂÖ¨Âºè</strong>Êù•Ê±ÇËß£ÊúÄÂ§ßÂåñËØ•ÂáΩÊï∞ÁöÑ<span
class="math inline">\(\hat{\beta}\)</span>„ÄÇÊàë‰ª¨ÂøÖÈ°ª‰ΩøÁî®Êï∞ÂÄº‰ºòÂåñÁÆóÊ≥ïÊù•ÊâæÂà∞ÂÆÉ„ÄÇ</p></li>
</ol>
<h2 id="the-algorithm-newton-raphson-ÁÆóÊ≥ïÁâõÈ°ø-ÊãâÂ§´Ê£ÆÁÆóÊ≥ï">2.3 The
Algorithm: Newton-Raphson ÁÆóÊ≥ïÔºöÁâõÈ°ø-ÊãâÂ§´Ê£ÆÁÆóÊ≥ï</h2>
<p>The slides introduce the <strong>Newton-Raphson algorithm</strong> as
the method to find the optimal <span
class="math inline">\(\hat{\beta}\)</span>. It‚Äôs an efficient iterative
algorithm for finding the roots of a function (i.e., where <span
class="math inline">\(f(x)=0\)</span>).</p>
<p><strong>How does this apply to logistic regression?</strong> To
maximize the log-likelihood function <span
class="math inline">\(\ell(\beta)\)</span>, we need to find the point
where its derivative (gradient) is equal to zero. So, Newton-Raphson is
used to solve <span class="math inline">\(\frac{d\ell(\beta)}{d\beta} =
0\)</span>.</p>
<p>ÂÆÉÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑËø≠‰ª£ÁÆóÊ≥ïÔºåÁî®‰∫éÊ±ÇÂáΩÊï∞ÁöÑÊ†πÔºàÂç≥ÔºåÂΩì<span
class="math inline">\(f(x)=0\)</span>Êó∂Ôºâ„ÄÇ</p>
<p><strong>ËøôÂ¶Ç‰ΩïÂ∫îÁî®‰∫éÈÄªËæëÂõûÂΩíÔºü</strong> ‰∏∫‰∫ÜÊúÄÂ§ßÂåñÂØπÊï∞‰ººÁÑ∂ÂáΩÊï∞ <span
class="math inline">\(\ell(\beta)\)</span>ÔºåÊàë‰ª¨ÈúÄË¶ÅÊâæÂà∞ÂÖ∂ÂØºÊï∞ÔºàÊ¢ØÂ∫¶ÔºâÁ≠â‰∫éÈõ∂ÁöÑÁÇπ„ÄÇÂõ†Ê≠§ÔºåÁâõÈ°ø-ÊãâÂ§´Ê£ÆÊ≥ïÁî®‰∫éÊ±ÇËß£
<span class="math inline">\(\frac{d\ell(\beta)}{d\beta} =
0\)</span>„ÄÇ</p>
<h4 id="the-general-newton-raphson-method"><strong>The General
Newton-Raphson Method</strong></h4>
<p>The algorithm starts with an initial guess, <span
class="math inline">\(x^{old}\)</span>, and iteratively refines it using
the following update rule, which is based on a Taylor series
approximation: <span class="math display">\[
x^{new} = x^{old} - \frac{f(x^{old})}{f&#39;(x^{old})}
\]</span> where <span class="math inline">\(f&#39;(x)\)</span> is the
derivative of <span class="math inline">\(f(x)\)</span>. You repeat this
step until the value of <span class="math inline">\(x\)</span>
converges.</p>
<p>ËØ•ÁÆóÊ≥ï‰ªéÂàùÂßã‰º∞ËÆ° <span class="math inline">\(x^{old}\)</span>
ÂºÄÂßãÔºåÂπ∂‰ΩøÁî®‰ª•‰∏ãÂü∫‰∫éÊ≥∞ÂãíÁ∫ßÊï∞Ëøë‰ººÁöÑÊõ¥Êñ∞ËßÑÂàôËø≠‰ª£Âú∞ÂØπÂÖ∂ËøõË°å‰ºòÂåñÔºö <span
class="math display">\[
x^{new} = x^{old} - \frac{f(x^{old})}{f&#39;(x^{old})}
\]</span> ÂÖ∂‰∏≠ <span class="math inline">\(f&#39;(x)\)</span> ÊòØ <span
class="math inline">\(f(x)\)</span> ÁöÑÂØºÊï∞„ÄÇÈáçÂ§çÊ≠§Ê≠•È™§ÔºåÁõ¥Âà∞ <span
class="math inline">\(x\)</span> ÁöÑÂÄºÊî∂Êïõ„ÄÇ</p>
<h4
id="important-image-newton-raphson-example-x3---4-0"><strong>Important
Image: Newton-Raphson Example (<span class="math inline">\(x^3 - 4 =
0\)</span>)</strong></h4>
<p>[Image showing iterations of Newton-Raphson]</p>
<p>This slide is a great illustration of the algorithm‚Äôs power. *
<strong>Goal:</strong> Find <span class="math inline">\(x\)</span> such
that <span class="math inline">\(f(x) = x^3 - 4 = 0\)</span>. *
<strong>Function:</strong> <span class="math inline">\(f(x) = x^3 -
4\)</span> * <strong>Derivative:</strong> <span
class="math inline">\(f&#39;(x) = 3x^2\)</span> * <strong>Update
Rule:</strong> <span class="math inline">\(x^{new} = x^{old} -
\frac{(x^{old})^3 - 4}{3(x^{old})^2}\)</span> Starting with a guess of
<span class="math inline">\(x^{old} = 2\)</span>, the algorithm
converges to the true answer (<span class="math inline">\(4^{1/3}
\approx 1.5874\)</span>) in just 4 steps.</p>
<ul>
<li><strong>ÁõÆÊ†á</strong>ÔºöÊâæÂà∞ <span
class="math inline">\(x\)</span>Ôºå‰ΩøÂæó <span class="math inline">\(f(x)
= x^3 - 4 = 0\)</span>„ÄÇ</li>
<li><strong>ÂáΩÊï∞</strong>Ôºö<span class="math inline">\(f(x) = x^3 -
4\)</span></li>
<li><strong>ÂØºÊï∞</strong>Ôºö<span class="math inline">\(f&#39;(x) =
3x^2\)</span></li>
<li><strong>Êõ¥Êñ∞ËßÑÂàô</strong>Ôºö<span class="math inline">\(x^{new} =
x^{old} - \frac{(x^{old})^3 - 4}{3(x^{old})^2}\)</span> ‰ªé <span
class="math inline">\(x^{old} = 2\)</span> ÁöÑÁåúÊµãÂºÄÂßãÔºåËØ•ÁÆóÊ≥ï‰ªÖÁî® 4
Ê≠•Â∞±Êî∂ÊïõÂà∞ÁúüÂÆûÁ≠îÊ°à (<span class="math inline">\(4^{1/3} \approx
1.5874\)</span>)„ÄÇ</li>
</ul>
<h4 id="code-understanding-python"><strong>Code Understanding
(Python)</strong></h4>
<p>The slides show Python code implementing Newton-Raphson. Let‚Äôs break
down the key function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the function we want to find the root of</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) - x*x + <span class="number">3</span> * np.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define its derivative</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_prime</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) - <span class="number">2</span>*x + <span class="number">3</span> * np.cos(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Newton-Raphson method</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">newton_raphson</span>(<span class="params">x0, tol=<span class="number">1e-10</span>, max_iter=<span class="number">100</span></span>):</span><br><span class="line">    x = x0 <span class="comment"># Start with the initial guess</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        fx = f(x)      <span class="comment"># Calculate f(x_old)</span></span><br><span class="line">        fpx = f_prime(x) <span class="comment"># Calculate f&#x27;(x_old)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> fpx == <span class="number">0</span>: <span class="comment"># Cannot divide by zero</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Zero derivative. No solution found.&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is the core update rule</span></span><br><span class="line">        x_new = x - fx / fpx</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check if the change is small enough to stop</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(x_new - x) &lt; tol:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Converged to <span class="subst">&#123;x_new&#125;</span> after <span class="subst">&#123;i+<span class="number">1</span>&#125;</span> iterations.&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> x_new</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update x for the next iteration</span></span><br><span class="line">        x = x_new</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Exceeded maximum iterations. No solution found.&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial guess and execution</span></span><br><span class="line">x0 = <span class="number">0.5</span></span><br><span class="line">root = newton_raphson(x0)</span><br></pre></td></tr></table></figure>
<p>The slides show that with a good initial guess
(<code>x0 = 0.5</code>), the algorithm converges quickly. With a bad one
(<code>x0 = 50</code>), it still converges but takes many more steps.
This highlights the importance of the starting point. The slides also
show an implementation of <strong>Gradient Descent</strong>, another
popular optimization algorithm which uses the update rule
<code>x_new = x - learning_rate * gradient</code>.</p>
<h1
id="provide-a-great-case-study-on-logistic-regression-particularly-on-the-important-concept-of-confounding-variables.-heres-a-summary-covering-the-math-code-and-key-insights.">Provide
a great case study on logistic regression, particularly on the important
concept of confounding variables. Here‚Äôs a summary covering the math,
code, and key insights.</h1>
<ol start="3" type="1">
<li><h1 id="core-concept-logistic-regression-Ê†∏ÂøÉÊ¶ÇÂøµÈÄªËæëÂõûÂΩí">Core
Concept: Logistic Regression üìà # Ê†∏ÂøÉÊ¶ÇÂøµÔºöÈÄªËæëÂõûÂΩí üìà</h1></li>
</ol>
<p>Logistic regression is a statistical method used for <strong>binary
classification</strong>, which means predicting an outcome that can only
be one of two things (e.g., Yes/No, True/False, 1/0).</p>
<p>In this example, the goal is to predict the probability that a
customer will <strong>default</strong> on a loan (Yes or No) based on
factors like their account <code>balance</code>, <code>income</code>,
and whether they are a <code>student</code>.</p>
<p>The core of logistic regression is the <strong>sigmoid (or logistic)
function</strong>, which takes any real-valued number and squishes it to
a value between 0 and 1, representing a probability.</p>
<p><span class="math display">\[
\hat{P}(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_p
X_p)}}
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{P}(Y=1|X)\)</span> is the predicted
probability of the outcome being ‚ÄúYes‚Äù (e.g., default).</li>
<li><span class="math inline">\(\beta_0\)</span> is the intercept.</li>
<li><span class="math inline">\(\beta_1, ..., \beta_p\)</span> are the
coefficients for each input variable (<span class="math inline">\(X_1,
..., X_p\)</span>). The model‚Äôs job is to find the best values for these
<span class="math inline">\(\beta\)</span> coefficients.</li>
</ul>
<hr />
<p>ÈÄªËæëÂõûÂΩíÊòØ‰∏ÄÁßçÁî®‰∫é<strong>‰∫åÂÖÉÂàÜÁ±ª</strong>ÁöÑÁªüËÆ°ÊñπÊ≥ïÔºåËøôÊÑèÂë≥ÁùÄÈ¢ÑÊµãÁªìÊûúÂè™ËÉΩÊòØ‰∏§ÁßçÊÉÖÂÜµ‰πã‰∏ÄÔºà‰æãÂ¶ÇÔºåÊòØ/Âê¶„ÄÅÁúü/ÂÅá„ÄÅ1/0Ôºâ„ÄÇ</p>
<p>Âú®Êú¨‰æã‰∏≠ÔºåÁõÆÊ†áÊòØÊ†πÊçÆÂÆ¢Êà∑Ë¥¶Êà∑‚Äú‰ΩôÈ¢ù‚Äù„ÄÅ‚ÄúÊî∂ÂÖ•‚Äù‰ª•ÂèäÊòØÂê¶‰∏∫‚ÄúÂ≠¶Áîü‚ÄùÁ≠âÂõ†Á¥†ÔºåÈ¢ÑÊµãÂÆ¢Êà∑<strong>ÊãñÊ¨†</strong>Ë¥∑Ê¨æÔºàÊòØÊàñÂê¶ÔºâÁöÑÊ¶ÇÁéá„ÄÇ</p>
<p>ÈÄªËæëÂõûÂΩíÁöÑÊ†∏ÂøÉÊòØ<strong>SigmoidÔºàÊàñÈÄªËæëÔºâÂáΩÊï∞</strong>ÔºåÂÆÉÂ∞Ü‰ªª‰ΩïÂÆûÊï∞ÂéãÁº©‰∏∫‰ªã‰∫é
0 Âíå 1 ‰πãÈó¥ÁöÑÂÄºÔºå‰ª•Ë°®Á§∫Ê¶ÇÁéá„ÄÇ</p>
<p><span class="math display">\[
\hat{P}(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_p
X_p)}}
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{P}(Y=1|X)\)</span>
ÊòØÁªìÊûú‰∏∫‚ÄúÊòØ‚ÄùÔºà‰æãÂ¶ÇÔºåÈªòËÆ§ÔºâÁöÑÈ¢ÑÊµãÊ¶ÇÁéá„ÄÇ</li>
<li><span class="math inline">\(\beta_0\)</span> ÊòØÊà™Ë∑ù„ÄÇ</li>
<li><span class="math inline">\(\beta_1, ..., \beta_p\)</span>
ÊòØÊØè‰∏™ËæìÂÖ•ÂèòÈáè (<span class="math inline">\(X_1, ..., X_p\)</span>)
ÁöÑÁ≥ªÊï∞„ÄÇÊ®°ÂûãÁöÑ‰ªªÂä°ÊòØÊâæÂà∞Ëøô‰∫õ <span class="math inline">\(\beta\)</span>
Á≥ªÊï∞ÁöÑÊúÄ‰Ω≥ÂÄº„ÄÇ</li>
</ul>
<h2 id="how-the-model-learns-mathematical-foundation">3.1 How the Model
‚ÄúLearns‚Äù (Mathematical Foundation)</h2>
<p>The slides show that the model‚Äôs coefficients (<span
class="math inline">\(\beta\)</span>) are found using an algorithm like
<strong>Newton-Raphson</strong>. This is an iterative process to find
the values that <strong>maximize the log-likelihood function</strong>.
Think of this as finding the coefficient values that make the observed
data most
probable.ËøôÊòØ‰∏Ä‰∏™Ëø≠‰ª£ËøáÁ®ãÔºåÁî®‰∫éÊü•Êâæ<strong>ÊúÄÂ§ßÂåñÂØπÊï∞‰ººÁÑ∂ÂáΩÊï∞</strong>ÁöÑÂÄº„ÄÇÂèØ‰ª•Â∞ÜÂÖ∂ËßÜ‰∏∫Êü•Êâæ‰ΩøËßÇÊµãÊï∞ÊçÆÊ¶ÇÁéáÊúÄÂ§ßÁöÑÁ≥ªÊï∞ÂÄº„ÄÇ</p>
<p>The key slide for this is the one titled ‚ÄúNewton-Raphson Iterative
Algorithm‚Äù. It shows the formulas for: * The <strong>Gradient</strong>
(<span class="math inline">\(\nabla\ell\)</span>): The direction of the
steepest ascent of the log-likelihood function. * The
<strong>Hessian</strong> (<span class="math inline">\(H\)</span>): The
curvature of the log-likelihood function.</p>
<ul>
<li><strong>Ê¢ØÂ∫¶</strong> (<span
class="math inline">\(\nabla\ell\)</span>)ÔºöÂØπÊï∞‰ººÁÑ∂ÂáΩÊï∞ÊúÄÈô°‰∏äÂçáÁöÑÊñπÂêë„ÄÇ</li>
<li><strong>ÈªëÊ£ÆÁü©Èòµ</strong> (<span
class="math inline">\(H\)</span>)ÔºöÂØπÊï∞‰ººÁÑ∂ÂáΩÊï∞ÁöÑÊõ≤Áéá„ÄÇ</li>
</ul>
<p>The updating rule is given by: <span class="math display">\[
\beta^{new} = \beta^{old} - H^{-1}\nabla\ell
\]</span> This formula is used repeatedly until the coefficient values
stop changing significantly, meaning the algorithm has converged to the
best fit. This process is also referred to as <strong>Iteratively
Reweighted Least Squares (IRLS)</strong>.
Ê≠§ÂÖ¨ÂºèÂèçÂ§ç‰ΩøÁî®ÔºåÁõ¥Âà∞Á≥ªÊï∞ÂÄº‰∏çÂÜçÂèëÁîüÊòæËëóÂèòÂåñÔºåËøôÊÑèÂë≥ÁùÄÁÆóÊ≥ïÂ∑≤Êî∂ÊïõÂà∞ÊúÄ‰Ω≥ÊãüÂêàÂÄº„ÄÇÊ≠§ËøáÁ®ã‰πüÁß∞‰∏∫<strong>Ëø≠‰ª£ÈáçÂä†ÊùÉÊúÄÂ∞è‰∫å‰πòÊ≥ï
(IRLS)</strong>„ÄÇ</p>
<hr />
<h2 id="the-puzzle-a-tale-of-two-models">3.2 The Puzzle: A Tale of Two
Models üïµÔ∏è‚Äç‚ôÇÔ∏è</h2>
<p>The most important story in these slides is how the effect of being a
student changes depending on the model. This is a classic example of a
<strong>confounding variable</strong>.</p>
<h4 id="model-1-simple-logistic-regression-default-vs.-student">Model 1:
Simple Logistic Regression (Default vs.¬†Student)</h4>
<p>When predicting default using <em>only</em> student status, the model
is: <code>default ~ student</code></p>
<p>From the slides, the coefficients are: * Intercept (<span
class="math inline">\(\beta_0\)</span>): -3.5041 * student[Yes] (<span
class="math inline">\(\beta_1\)</span>): <strong>0.4049</strong>
(positive)</p>
<p>The equation for the log-odds is: <span class="math display">\[
\log\left(\frac{P(\text{default})}{1-P(\text{default})}\right) = -3.5041
+ 0.4049 \times (\text{is\_student})
\]</span></p>
<p><strong>Conclusion:</strong> The positive coefficient (0.4049)
suggests that <strong>students are more likely to default</strong> than
non-students. The slides calculate the probabilities: * <strong>Student
Default Probability:</strong> 4.31% * <strong>Non-Student Default
Probability:</strong> 2.92%</p>
<p>Â≠¶ÁîüË∫´‰ªΩÁöÑÂΩ±ÂìçÂ¶Ç‰ΩïÊ†πÊçÆÊ®°ÂûãËÄåÂèòÂåñ„ÄÇËøôÊòØ‰∏Ä‰∏™ÂÖ∏ÂûãÁöÑ<strong>Ê∑∑ÊùÇÂèòÈáè</strong>ÁöÑ‰æãÂ≠ê„ÄÇ</p>
<h4 id="Ê®°Âûã-1ÁÆÄÂçïÈÄªËæëÂõûÂΩíËøùÁ∫¶-vs.-Â≠¶Áîü">Ê®°Âûã 1ÔºöÁÆÄÂçïÈÄªËæëÂõûÂΩíÔºàËøùÁ∫¶
vs.¬†Â≠¶ÁîüÔºâ</h4>
<p>‰ªÖ‰ΩøÁî®Â≠¶ÁîüË∫´‰ªΩÈ¢ÑÊµãËøùÁ∫¶Êó∂ÔºåÊ®°Âûã‰∏∫Ôºö <code>default ~ student</code></p>
<p>ÂπªÁÅØÁâá‰∏≠ÊòæÁ§∫ÁöÑÁ≥ªÊï∞‰∏∫Ôºö * Êà™Ë∑ù (<span
class="math inline">\(\beta_0\)</span>): -3.5041 * Â≠¶Áîü[ÊòØ] (<span
class="math inline">\(\beta_1\)</span>):
<strong>0.4049</strong>ÔºàÊ≠£Ôºâ</p>
<p>ÂØπÊï∞Ê¶ÇÁéáÂÖ¨Âºè‰∏∫Ôºö <span class="math display">\[
\log\left(\frac{P(\text{default})}{1-P(\text{default})}\right) = -3.5041
+ 0.4049 \times (\text{is\_student})
\]</span></p>
<p><strong>ÁªìËÆ∫</strong>ÔºöÊ≠£Á≥ªÊï∞ (0.4049)
Ë°®Êòé<strong>Â≠¶ÁîüÊØîÈùûÂ≠¶ÁîüÊõ¥ÊúâÂèØËÉΩËøùÁ∫¶</strong>„ÄÇÂπªÁÅØÁâáËÆ°ÁÆó‰∫Ü‰ª•‰∏ãÊ¶ÇÁéáÔºö *
<strong>Â≠¶ÁîüËøùÁ∫¶Ê¶ÇÁéá</strong>Ôºö4.31% *
<strong>ÈùûÂ≠¶ÁîüËøùÁ∫¶Ê¶ÇÁéá</strong>Ôºö2.92%</p>
<h2
id="model-2-multiple-logistic-regression-default-vs.-all-variables-Ê®°Âûã-2Â§öÂÖÉÈÄªËæëÂõûÂΩíËøùÁ∫¶-vs.-ÊâÄÊúâÂèòÈáè">3.3
Model 2: Multiple Logistic Regression (Default vs.¬†All Variables) Ê®°Âûã
2ÔºöÂ§öÂÖÉÈÄªËæëÂõûÂΩíÔºàËøùÁ∫¶ vs.¬†ÊâÄÊúâÂèòÈáèÔºâ</h2>
<p>When we add <code>balance</code> and <code>income</code> to the
model, it becomes: <code>default ~ student + balance + income</code></p>
<p>From the slides, the new coefficients are: * Intercept (<span
class="math inline">\(\beta_0\)</span>): -10.8690 * balance (<span
class="math inline">\(\beta_1\)</span>): 0.0057 * income (<span
class="math inline">\(\beta_2\)</span>): 0.0030 * student[Yes] (<span
class="math inline">\(\beta_3\)</span>): <strong>-0.6468</strong>
(negative)</p>
<p><strong>The Shocking Twist!</strong> The coefficient for
<code>student[Yes]</code> is now <strong>negative</strong>.</p>
<p><strong>Conclusion:</strong> When we control for balance and income,
<strong>students are actually <em>less</em> likely to default</strong>
than non-students with the same balance and income.</p>
<h4 id="why-the-change-the-confounding-variable-explained">Why the
Change? The Confounding Variable Explained</h4>
<p>The key insight, explained on the slide with multi-colored text
bubbles, is that <strong>students, on average, have higher credit card
balances</strong>.</p>
<ul>
<li>In the simple model, the <code>student</code> variable was
inadvertently capturing the risk associated with having a high
<code>balance</code>. The model mistakenly concluded ‚Äúbeing a student
causes default.‚Äù</li>
<li>In the multiple model, the <code>balance</code> variable properly
accounts for the risk from a high balance. With that effect isolated,
the <code>student</code> variable can show its true, underlying
relationship with default, which is negative.</li>
</ul>
<p>This demonstrates why it‚Äôs crucial to consider multiple relevant
variables to avoid drawing incorrect conclusions. <strong>The most
important slides are the ones that present this paradox and its
explanation.</strong></p>
<p><strong>‰ª§‰∫∫ÈúáÊÉäÁöÑËΩ¨ÊäòÔºÅ</strong> <code>student[Yes]</code>
ÁöÑÁ≥ªÊï∞Áé∞Âú®‰∏∫<strong>Ë¥ü</strong>„ÄÇ</p>
<p><strong>ÁªìËÆ∫Ôºö</strong>ÂΩìÊàë‰ª¨ÊéßÂà∂‰ΩôÈ¢ùÂíåÊî∂ÂÖ•Êó∂Ôºå<strong>Â≠¶ÁîüÂÆûÈôÖ‰∏äÊØîÂÖ∑ÊúâÁõ∏Âêå‰ΩôÈ¢ùÂíåÊî∂ÂÖ•ÁöÑÈùûÂ≠¶ÁîüÊõ¥<em>‰Ωé</em>‰∫éËøùÁ∫¶</strong>„ÄÇ</p>
<h4 id="‰∏∫‰ªÄ‰πà‰ºöÊúâÂèòÂåñÊ∑∑ÊùÇÂèòÈáèËß£Èáä">‰∏∫‰ªÄ‰πà‰ºöÊúâÂèòÂåñÔºüÊ∑∑ÊùÇÂèòÈáèËß£Èáä</h4>
<p>ÂπªÁÅØÁâá‰∏äÁî®ÂΩ©Ëâ≤ÊñáÂ≠óÊ∞îÊ≥°Ëß£Èáä‰∫ÜÂÖ≥ÈîÆÁöÑËßÅËß£ÔºåÂç≥<strong>Â≠¶ÁîüÂπ≥ÂùáÊã•ÊúâÊõ¥È´òÁöÑ‰ø°Áî®Âç°‰ΩôÈ¢ù</strong>„ÄÇ</p>
<ul>
<li>Âú®ÁÆÄÂçïÊ®°Âûã‰∏≠Ôºå‚ÄúÂ≠¶Áîü‚ÄùÂèòÈáèÊó†ÊÑè‰∏≠ÊçïÊçâÂà∞‰∫ÜÈ´ò‰ΩôÈ¢ùÂ∏¶Êù•ÁöÑÈ£éÈô©„ÄÇËØ•Ê®°ÂûãÈîôËØØÂú∞ÂæóÂá∫‰∫Ü‚ÄúÂ≠¶ÁîüË∫´‰ªΩÂØºËá¥ËøùÁ∫¶‚ÄùÁöÑÁªìËÆ∫„ÄÇ</li>
<li>Âú®Â§öÂÖÉÊ®°Âûã‰∏≠Ôºå‚Äú‰ΩôÈ¢ù‚ÄùÂèòÈáèÊ≠£Á°ÆÂú∞Ëß£Èáä‰∫ÜÈ´ò‰ΩôÈ¢ùÂ∏¶Êù•ÁöÑÈ£éÈô©„ÄÇÂú®ÂàÜÁ¶ªÂá∫Ëøô‰∏ÄÂΩ±ÂìçÂêéÔºå‚ÄúÂ≠¶Áîü‚ÄùÂèòÈáèÂèØ‰ª•ÊòæÁ§∫ÂÖ∂‰∏éËøùÁ∫¶‰πãÈó¥ÁúüÂÆûÁöÑÊΩúÂú®ÂÖ≥Á≥ªÔºåÂç≥Ë¥üÁõ∏ÂÖ≥ÂÖ≥Á≥ª„ÄÇ</li>
</ul>
<p>ËøôËØ¥Êòé‰∫Ü‰∏∫‰ªÄ‰πàËÄÉËôëÂ§ö‰∏™Áõ∏ÂÖ≥ÂèòÈáè‰ª•ÈÅøÂÖçÂæóÂá∫ÈîôËØØÁªìËÆ∫Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ</p>
<hr />
<h3 id="code-implementation-r-vs.-python">Code Implementation: R
vs.¬†Python</h3>
<p>The slides use R‚Äôs <code>glm()</code> (Generalized Linear Model)
function. Here‚Äôs how you would replicate this in Python.</p>
<h4 id="r-code-from-slides">R Code (from slides)</h4>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Simple Model</span></span><br><span class="line">glmod2 <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>default <span class="operator">~</span> student<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> family<span class="operator">=</span>binomial<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>glmod2<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model</span></span><br><span class="line">glmod3 <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>default <span class="operator">~</span> .<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> family<span class="operator">=</span>binomial<span class="punctuation">)</span> <span class="comment"># &#x27;.&#x27; means all other variables</span></span><br><span class="line">summary<span class="punctuation">(</span>glmod3<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<h4 id="python-equivalent">Python Equivalent</h4>
<p>We can use two popular libraries: <code>statsmodels</code> (which
gives R-style summaries) and <code>scikit-learn</code> (the standard for
machine learning).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Default&#x27; is a pandas DataFrame with columns:</span></span><br><span class="line"><span class="comment"># &#x27;default&#x27; (0/1), &#x27;student&#x27; (0/1), &#x27;balance&#x27;, &#x27;income&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Using statsmodels (recommended for interpretation) ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data</span></span><br><span class="line"><span class="comment"># For statsmodels, we need to manually add the intercept</span></span><br><span class="line">X_simple = Default[[<span class="string">&#x27;student&#x27;</span>]]</span><br><span class="line">X_simple = sm.add_constant(X_simple)</span><br><span class="line">y = Default[<span class="string">&#x27;default&#x27;</span>]</span><br><span class="line"></span><br><span class="line">X_multiple = Default[[<span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line">X_multiple = sm.add_constant(X_multiple)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simple Model: default ~ student</span></span><br><span class="line">model_simple = sm.Logit(y, X_simple).fit()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- Simple Model ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_simple.summary())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model: default ~ student + balance + income</span></span><br><span class="line">model_multiple = sm.Logit(y, X_multiple).fit()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Multiple Model ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_multiple.summary())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Using scikit-learn (recommended for prediction tasks) ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data (scikit-learn adds intercept by default)</span></span><br><span class="line">X_simple_sk = Default[[<span class="string">&#x27;student&#x27;</span>]]</span><br><span class="line">y_sk = Default[<span class="string">&#x27;default&#x27;</span>]</span><br><span class="line"></span><br><span class="line">X_multiple_sk = Default[[<span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simple Model</span></span><br><span class="line">clf_simple = LogisticRegression().fit(X_simple_sk, y_sk)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nSimple Model Intercept (scikit-learn): <span class="subst">&#123;clf_simple.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Simple Model Coefficient (scikit-learn): <span class="subst">&#123;clf_simple.coef_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model</span></span><br><span class="line">clf_multiple = LogisticRegression().fit(X_multiple_sk, y_sk)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nMultiple Model Intercept (scikit-learn): <span class="subst">&#123;clf_multiple.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Multiple Model Coefficients (scikit-learn): <span class="subst">&#123;clf_multiple.coef_&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1
id="making-predictions-and-the-decision-boundary-ËøõË°åÈ¢ÑÊµãÂíåÂÜ≥Á≠ñËæπÁïå">4
Making Predictions and the Decision Boundary üéØËøõË°åÈ¢ÑÊµãÂíåÂÜ≥Á≠ñËæπÁïå</h1>
<p>Once the model is trained (i.e., we have the coefficients <span
class="math inline">\(\hat{\beta}\)</span>), we can make predictions.
‰∏ÄÊó¶Ê®°ÂûãËÆ≠ÁªÉÂÆåÊàêÔºàÂç≥ÔºåÊàë‰ª¨Êúâ‰∫ÜÁ≥ªÊï∞ <span
class="math inline">\(\hat{\beta}\)</span>ÔºâÔºåÊàë‰ª¨Â∞±ÂèØ‰ª•ËøõË°åÈ¢ÑÊµã‰∫Ü„ÄÇ ##
Math Behind Predictions</p>
<p>The model outputs the <strong>log-odds</strong>, which can be
converted into a probability. A key concept is the <strong>decision
boundary</strong>, which is the threshold where the model is uncertain
(probability = 50%).
Ê®°ÂûãËæìÂá∫<strong>ÂØπÊï∞Ê¶ÇÁéá</strong>ÔºåÂÆÉÂèØ‰ª•ËΩ¨Êç¢‰∏∫Ê¶ÇÁéá„ÄÇ‰∏Ä‰∏™ÂÖ≥ÈîÆÊ¶ÇÂøµÊòØ<strong>ÂÜ≥Á≠ñËæπÁïå</strong>ÔºåÂÆÉÊòØÊ®°Âûã‰∏çÁ°ÆÂÆöÁöÑÈòàÂÄºÔºàÊ¶ÇÁéá
= 50%Ôºâ„ÄÇ</p>
<ol type="1">
<li><p><strong>The Estimated Odds</strong>: The core output of the
linear part of the model is the exponential of the linear equation,
which gives the odds of the outcome being ‚ÄòYes‚Äô (or 1).
<strong>‰º∞ËÆ°Ê¶ÇÁéá</strong>ÔºöÊ®°ÂûãÁ∫øÊÄßÈÉ®ÂàÜÁöÑÊ†∏ÂøÉËæìÂá∫ÊòØÁ∫øÊÄßÊñπÁ®ãÁöÑÊåáÊï∞ÔºåÂÆÉÁªôÂá∫‰∫ÜÁªìÊûú‰∏∫‚ÄúÊòØ‚ÄùÔºàÊàñ
1ÔºâÁöÑÊ¶ÇÁéá„ÄÇ</p>
<p><span class="math display">\[
\]</span>$$\frac{\hat{P}(y=1|\mathbf{x}_0)}{\hat{P}(y=0|\mathbf{x}_0)} =
\exp(\hat{\beta}^\top \mathbf{x}_0)</p>
<p><span class="math display">\[
\]</span><span class="math display">\[
\]</span></p></li>
<li><p><strong>The Decision Rule</strong>: We classify a new observation
<span class="math inline">\(\mathbf{x}_0\)</span> by comparing its
predicted odds to a threshold <span
class="math inline">\(\delta\)</span>.
<strong>ÂÜ≥Á≠ñËßÑÂàô</strong>ÔºöÊàë‰ª¨ÈÄöËøáÊØîËæÉÊñ∞ËßÇÊµãÂÄº <span
class="math inline">\(\mathbf{x}_0\)</span> ÁöÑÈ¢ÑÊµãÊ¶ÇÁéá‰∏éÈòàÂÄº <span
class="math inline">\(\delta\)</span> Êù•ÂØπÂÖ∂ËøõË°åÂàÜÁ±ª„ÄÇ</p>
<ul>
<li>Predict <span class="math inline">\(y=1\)</span> if <span
class="math inline">\(\exp(\hat{\beta}^\top \mathbf{x}_0) &gt;
\delta\)</span></li>
<li>Predict <span class="math inline">\(y=0\)</span> if <span
class="math inline">\(\exp(\hat{\beta}^\top \mathbf{x}_0) &lt;
\delta\)</span> A common default is <span
class="math inline">\(\delta=1\)</span>, which means we predict ‚ÄòYes‚Äô if
the probability is greater than 0.5.</li>
</ul></li>
<li><p><strong>The Linear Boundary</strong>: The decision boundary
itself is where the odds are exactly equal to the threshold. By taking
the logarithm, we see that this boundary is a <strong>linear
equation</strong>. This is why logistic regression is called a
<strong>linear classifier</strong>.
<strong>Á∫øÊÄßËæπÁïå</strong>ÔºöÂÜ≥Á≠ñËæπÁïåÊú¨Ë∫´Â∞±ÊòØÊ¶ÇÁéáÊÅ∞Â•ΩÁ≠â‰∫éÈòàÂÄºÁöÑÂú∞Êñπ„ÄÇÂèñÂØπÊï∞ÂêéÔºåÊàë‰ª¨ÂèëÁé∞Ëøô‰∏™ËæπÁïåÊòØ‰∏Ä‰∏™<strong>Á∫øÊÄßÊñπÁ®ã</strong>„ÄÇËøôÂ∞±ÊòØÈÄªËæëÂõûÂΩíË¢´Áß∞‰∏∫<strong>Á∫øÊÄßÂàÜÁ±ªÂô®</strong>ÁöÑÂéüÂõ†„ÄÇ
<span class="math display">\[
\]</span>$$\hat{\beta}^\top \mathbf{x} = \log(\delta)</p>
<p><span class="math display">\[
\]</span>$$For <span class="math inline">\(\delta=1\)</span>, the
boundary is simply <span class="math inline">\(\hat{\beta}^\top
\mathbf{x} = 0\)</span>.</p></li>
</ol>
<p>This concept is visualized perfectly in the slide titled ‚ÄúLinear
Classifier,‚Äù which shows a straight line neatly separating two classes
of data points.
È¢ò‰∏∫‚ÄúÁ∫øÊÄßÂàÜÁ±ªÂô®‚ÄùÁöÑÂπªÁÅØÁâáÂÆåÁæéÂú∞Â±ïÁ§∫‰∫ÜËøô‰∏ÄÊ¶ÇÂøµÔºåÂÆÉÂ±ïÁ§∫‰∫Ü‰∏ÄÊù°Áõ¥Á∫øÔºåÂ∞Ü‰∏§Á±ªÊï∞ÊçÆÁÇπÂ∑ßÂ¶ôÂú∞ÂàÜÈöîÂºÄÊù•„ÄÇ</p>
<h2 id="visualizing-the-confounding-effect">Visualizing the Confounding
Effect</h2>
<p>The most important image in this set is <strong>Figure 4.3</strong>,
as it visually explains the confounding puzzle from the first set of
slides.</p>
<ul>
<li><strong>Right Panel (Boxplots)</strong>: This shows that
<strong>students (Yes) tend to have higher credit card balances</strong>
than non-students (No). This is the source of the confounding.</li>
<li><strong>Left Panel (Default Rates)</strong>:
<ul>
<li>The <strong>dashed lines</strong> show the <em>overall</em> default
rates. The orange line (students) is higher than the blue line
(non-students). This matches our simple model
(<code>default ~ student</code>).</li>
<li>The <strong>solid S-shaped curves</strong> show the probability of
default as a function of balance. For any <em>given</em> balance, the
blue curve (non-students) is slightly higher than the orange curve
(students). This means that <strong>at the same level of debt, students
are <em>less</em> likely to default</strong>. This matches our multiple
regression model
(<code>default ~ student + balance + income</code>).</li>
</ul></li>
</ul>
<p>This single figure brilliantly illustrates how a variable can appear
to have one effect in isolation but the opposite effect when controlling
for a confounding factor. *
<strong>Âè≥‰æßÈù¢ÊùøÔºàÁÆ±Á∫øÂõæÔºâ</strong>ÔºöËøôË°®Êòé<strong>Â≠¶ÁîüÔºàÊòØÔºâÁöÑ‰ø°Áî®Âç°‰ΩôÈ¢ùÂæÄÂæÄÈ´ò‰∫éÈùûÂ≠¶ÁîüÔºàÂê¶Ôºâ„ÄÇËøôÂ∞±ÊòØÊ∑∑ÊùÇÊïàÂ∫îÁöÑÊ†πÊ∫ê„ÄÇ
* </strong>Â∑¶ÂõæÔºàËøùÁ∫¶ÁéáÔºâ<strong>Ôºö *
</strong>ËôöÁ∫ø<strong>ÊòæÁ§∫<em>ÊÄª‰Ωì</em>ËøùÁ∫¶Áéá„ÄÇÊ©ôËâ≤Á∫øÔºàÂ≠¶ÁîüÔºâÈ´ò‰∫éËìùËâ≤Á∫øÔºàÈùûÂ≠¶ÁîüÔºâ„ÄÇËøô‰∏éÊàë‰ª¨ÁöÑÁÆÄÂçïÊ®°ÂûãÔºà‚ÄúËøùÁ∫¶
~ Â≠¶Áîü‚ÄùÔºâÁõ∏Á¨¶„ÄÇ * </strong>S
ÂΩ¢ÂÆûÁ∫ø<strong>ÊòæÁ§∫ËøùÁ∫¶Ê¶ÇÁéá‰∏é‰ΩôÈ¢ùÁöÑÂÖ≥Á≥ª„ÄÇÂØπ‰∫é‰ªª‰Ωï<em>ÁªôÂÆö</em>ÁöÑ‰ΩôÈ¢ùÔºåËìùËâ≤Êõ≤Á∫øÔºàÈùûÂ≠¶ÁîüÔºâÁï•È´ò‰∫éÊ©ôËâ≤Êõ≤Á∫øÔºàÂ≠¶ÁîüÔºâ„ÄÇËøôÊÑèÂë≥ÁùÄ</strong>Âú®Áõ∏ÂêåÁöÑÂÄ∫Âä°Ê∞¥Âπ≥‰∏ãÔºåÂ≠¶ÁîüËøùÁ∫¶ÁöÑÂèØËÉΩÊÄß<em>ËæÉÂ∞è</em>„ÄÇËøô‰∏éÊàë‰ª¨ÁöÑÂ§öÂÖÉÂõûÂΩíÊ®°ÂûãÔºà‚ÄúËøùÁ∫¶
~ Â≠¶Áîü + ‰ΩôÈ¢ù + Êî∂ÂÖ•‚ÄùÔºâÁõ∏Á¨¶„ÄÇ</p>
<p>ËøôÂº†ÂõæÂ∑ßÂ¶ôÂú∞ËØ¥Êòé‰∫Ü‰∏∫‰ªÄ‰πà‰∏Ä‰∏™ÂèòÈáèÂú®ÂçïÁã¨‰ΩøÁî®Êó∂‰ºº‰πé‰ºö‰∫ßÁîü‰∏ÄÁßçÂΩ±ÂìçÔºå‰ΩÜÂú®ÊéßÂà∂Ê∑∑ÊùÇÂõ†Á¥†ÂêéÂç¥‰ºö‰∫ßÁîüÁõ∏ÂèçÁöÑÂΩ±Âìç„ÄÇ</p>
<h2 id="an-important-edge-case-perfect-separation">An Important Edge
Case: Perfect Separation ‚ö†Ô∏è</h2>
<p>What happens if the data can be perfectly separated by a straight
line? Â¶ÇÊûúÊï∞ÊçÆÂèØ‰ª•Áî®‰∏ÄÊù°Áõ¥Á∫øÂÆåÁæéÂàÜÁ¶ªÔºå‰ºöÂèëÁîü‰ªÄ‰πàÔºü</p>
<p>One might think this is the ideal scenario, but it causes a problem
for the logistic regression algorithm. The model will try to find
coefficients that make the probabilities for each class as close to 1
and 0 as possible. To do this, the magnitude of the coefficients (<span
class="math inline">\(\hat{\beta}\)</span>) must grow infinitely large.
‰∫∫‰ª¨ÂèØËÉΩËÆ§‰∏∫ËøôÊòØÁêÜÊÉ≥ÊÉÖÂÜµÔºå‰ΩÜÂÆÉ‰ºöÁªôÈÄªËæëÂõûÂΩíÁÆóÊ≥ïÂ∏¶Êù•ÈóÆÈ¢ò„ÄÇÊ®°Âûã‰ºöÂ∞ùËØïÊâæÂà∞‰ΩøÊØè‰∏™Á±ªÂà´ÁöÑÊ¶ÇÁéáÂ∞ΩÂèØËÉΩÊé•Ëøë
1 Âíå 0 ÁöÑÁ≥ªÊï∞„ÄÇ‰∏∫Ê≠§ÔºåÁ≥ªÊï∞ (<span
class="math inline">\(\hat{\beta}\)</span>) ÁöÑÂ§ßÂ∞èÂøÖÈ°ªÊó†ÈôêÂ§ß„ÄÇ</p>
<p>The slide ‚ÄúNon-convergence for perfectly separated case‚Äù demonstrates
this:</p>
<ul>
<li><p><strong>The Code</strong>: It generates two distinct,
non-overlapping clusters of data points using Python‚Äôs
<code>scikit-learn</code>.</p></li>
<li><p><strong>Parameter Estimates Graph</strong>: It shows the
<code>Intercept</code>, <code>Coefficient 1</code>, and
<code>Coefficient 2</code> values increasing or decreasing without limit
as the algorithm runs through more iterations. They never converge to a
stable value.</p></li>
<li><p><strong>Decision Boundary Graph</strong>: The decision boundary
itself might look reasonable, but the underlying coefficients are
unstable.</p></li>
<li><p><strong>‰ª£Á†Å</strong>ÔºöÂÆÉ‰ΩøÁî® Python ÁöÑ <code>scikit-learn</code>
ÁîüÊàê‰∏§‰∏™‰∏çÂêåÁöÑ„ÄÅ‰∏çÈáçÂè†ÁöÑÊï∞ÊçÆÁÇπËÅöÁ±ª„ÄÇ</p></li>
<li><p><strong>ÂèÇÊï∞‰º∞ËÆ°Âõæ</strong>ÔºöÂÆÉÊòæÁ§∫‚ÄúÊà™Ë∑ù‚Äù„ÄÅ‚ÄúÁ≥ªÊï∞ 1‚ÄùÂíå‚ÄúÁ≥ªÊï∞
2‚ÄùÁöÑÂÄºÈöèÁùÄÁÆóÊ≥ïËø≠‰ª£Ê¨°Êï∞ÁöÑÂ¢ûÂä†ÊàñÂáèÂ∞ëËÄåÊó†ÈôêÂ¢ûÂ§ßÊàñÂáèÂ∞è„ÄÇÂÆÉ‰ª¨Ê∞∏Ëøú‰∏ç‰ºöÊî∂ÊïõÂà∞‰∏Ä‰∏™Á®≥ÂÆöÁöÑÂÄº„ÄÇ</p></li>
<li><p><strong>ÂÜ≥Á≠ñËæπÁïåÂõæ</strong>ÔºöÂÜ≥Á≠ñËæπÁïåÊú¨Ë∫´ÂèØËÉΩÁúãËµ∑Êù•ÂêàÁêÜÔºå‰ΩÜÂ∫ïÂ±ÇÁ≥ªÊï∞ÊòØ‰∏çÁ®≥ÂÆöÁöÑ„ÄÇ</p></li>
</ul>
<p><strong>Key Takeaway</strong>: If your logistic regression model
fails to converge, the first thing you should check for is perfect
separation in your training data.
<strong>ÂÖ≥ÈîÆË¶ÅÁÇπ</strong>ÔºöÂ¶ÇÊûúÊÇ®ÁöÑÈÄªËæëÂõûÂΩíÊ®°ÂûãÊú™ËÉΩÊî∂ÊïõÔºåÊÇ®Â∫îËØ•Ê£ÄÊü•ÁöÑÁ¨¨‰∏Ä‰ª∂‰∫ãÂ∞±ÊòØËÆ≠ÁªÉÊï∞ÊçÆÊòØÂê¶ÂÆåÁæéÂàÜÁ¶ª„ÄÇ</p>
<h2 id="code-understanding">Code Understanding</h2>
<p>The slides provide useful code snippets in both R and Python.</p>
<h2 id="r-code-plotting-predictions">R Code (Plotting Predictions)</h2>
<p>This code generates the plot with the two S-shaped curves (one for
students, one for non-students) showing the probability of default as
balance increases.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Create a data frame for prediction with a range of balances</span></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># One version for students, one for non-students</span></span><br><span class="line">Default.st <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>balance<span class="operator">=</span>seq<span class="punctuation">(</span><span class="number">500</span><span class="punctuation">,</span> <span class="number">2500</span><span class="punctuation">,</span> by<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> student<span class="operator">=</span><span class="string">&quot;Yes&quot;</span><span class="punctuation">)</span></span><br><span class="line">Default.nonst <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>balance<span class="operator">=</span>seq<span class="punctuation">(</span><span class="number">500</span><span class="punctuation">,</span> <span class="number">2500</span><span class="punctuation">,</span> by<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> student<span class="operator">=</span><span class="string">&quot;No&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Use the trained multiple regression model (glmod3) to predict probabilities</span></span><br><span class="line">pred.st <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>glmod3<span class="punctuation">,</span> Default.st<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line">pred.nonst <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>glmod3<span class="punctuation">,</span> Default.nonst<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Plot the results</span></span><br><span class="line">plot<span class="punctuation">(</span>Default.st<span class="operator">$</span>balance<span class="punctuation">,</span> pred.st<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;l&quot;</span><span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;red&quot;</span><span class="punctuation">,</span> ...<span class="punctuation">)</span> <span class="operator">/</span><span class="operator">/</span> Students</span><br><span class="line">lines<span class="punctuation">(</span>Default.nonst<span class="operator">$</span>balance<span class="punctuation">,</span> pred.nonst<span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;blue&quot;</span><span class="punctuation">,</span> ...<span class="punctuation">)</span> <span class="operator">/</span><span class="operator">/</span> Non<span class="operator">-</span>students</span><br></pre></td></tr></table></figure>
<h4 id="python-code-visualizing-the-decision-boundary">Python Code
(Visualizing the Decision Boundary)</h4>
<p>This Python code uses <code>scikit-learn</code> and
<code>matplotlib</code> to create the plot showing the linear decision
boundary.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Generate synthetic data with two classes</span></span><br><span class="line">X, y = make_classification(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Initialize and fit the logistic regression model</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create a mesh grid of points to make predictions over the entire plot area</span></span><br><span class="line">xx, yy = np.meshgrid(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Predict the probability for each point on the grid</span></span><br><span class="line">probs = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Plot the decision boundary where the probability is 0.5</span></span><br><span class="line">plt.contour(xx, yy, probs.reshape(xx.shape), levels=[<span class="number">0.5</span>], ...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. Scatter plot the actual data points</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, ...)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="other-important-remarks">Other Important Remarks</h3>
<p>The ‚ÄúRemarks‚Äù slide briefly mentions some key extensions:</p>
<ul>
<li><p><strong>Probit Model</strong>: An alternative to logistic
regression that uses the cumulative distribution function (CDF) of the
standard normal distribution instead of the sigmoid function. The
results are often very similar.</p></li>
<li><p><strong>Softmax Regression</strong>: An extension of logistic
regression used for multi-class classification (when there are more than
two possible outcomes).</p></li>
<li><p><strong>Probit
Ê®°Âûã</strong>ÔºöÈÄªËæëÂõûÂΩíÁöÑÊõø‰ª£ÊñπÊ≥ïÔºåÂÆÉ‰ΩøÁî®Ê†áÂáÜÊ≠£ÊÄÅÂàÜÂ∏ÉÁöÑÁ¥ØÁßØÂàÜÂ∏ÉÂáΩÊï∞
(CDF) ‰ª£Êõø S ÂûãÂáΩÊï∞„ÄÇÁªìÊûúÈÄöÂ∏∏ÈùûÂ∏∏Áõ∏‰ºº„ÄÇ</p></li>
<li><p><strong>Softmax
ÂõûÂΩí</strong>ÔºöÈÄªËæëÂõûÂΩíÁöÑÊâ©Â±ïÔºåÁî®‰∫éÂ§öÁ±ªÂàÜÁ±ªÔºàÂΩìÂ≠òÂú®‰∏§‰∏™‰ª•‰∏äÂèØËÉΩÁªìÊûúÊó∂Ôºâ„ÄÇ</p></li>
</ul>
<h1
id="here-is-a-summary-of-the-slides-on-linear-discriminant-analysis-lda-including-the-key-mathematical-formulas-visual-explanations-and-how-to-implement-it-in-python.">5.
Here is a summary of the slides on Linear Discriminant Analysis (LDA),
including the key mathematical formulas, visual explanations, and how to
implement it in Python.</h1>
<h2
id="the-main-idea-classification-using-probabilities-‰ΩøÁî®Ê¶ÇÁéáËøõË°åÂàÜÁ±ª">The
Main Idea: Classification Using Probabilities ‰ΩøÁî®Ê¶ÇÁéáËøõË°åÂàÜÁ±ª</h2>
<p>Linear Discriminant Analysis (LDA) is a classification method. For a
given input <strong>x</strong>, it calculates the probability that
<strong>x</strong> belongs to each class and then assigns
<strong>x</strong> to the class with the <strong>highest
probability</strong>.</p>
<p>It does this using <strong>Bayes‚Äô Theorem</strong>, which provides a
formula for the posterior probability <span class="math inline">\(P(Y=k
| X=x)\)</span>, or the probability that the class is <span
class="math inline">\(k\)</span> given the input <span
class="math inline">\(x\)</span>. Á∫øÊÄßÂà§Âà´ÂàÜÊûê (LDA)
ÊòØ‰∏ÄÁßçÂàÜÁ±ªÊñπÊ≥ï„ÄÇÂØπ‰∫éÁªôÂÆöÁöÑËæìÂÖ• <strong>x</strong>ÔºåÂÆÉËÆ°ÁÆó
<strong>x</strong> Â±û‰∫éÊØè‰∏™Á±ªÂà´ÁöÑÊ¶ÇÁéáÔºåÁÑ∂ÂêéÂ∞Ü <strong>x</strong>
ÂàÜÈÖçÁªô<strong>Ê¶ÇÁéáÊúÄÈ´ò</strong>ÁöÑÁ±ªÂà´„ÄÇ</p>
<p>ÂÆÉ‰ΩøÁî®<strong>Ë¥ùÂè∂ÊñØÂÆöÁêÜ</strong>Êù•ÂÆûÁé∞Ëøô‰∏ÄÁÇπÔºåËØ•ÂÆöÁêÜÊèê‰æõ‰∫ÜÂêéÈ™åÊ¶ÇÁéá
<span class="math inline">\(P(Y=k | X=x)\)</span> ÁöÑÂÖ¨ÂºèÔºåÂç≥ÁªôÂÆöËæìÂÖ•
<span class="math inline">\(x\)</span>ÔºåËØ•Á±ªÂà´Â±û‰∫é <span
class="math inline">\(k\)</span> ÁöÑÊ¶ÇÁéá„ÄÇ <span class="math display">\[
p_k(x) = P(Y=k|X=x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)}
\]</span></p>
<ul>
<li><span class="math inline">\(p_k(x)\)</span> is the <strong>posterior
probability</strong> we want to maximize.</li>
<li><span class="math inline">\(\pi_k = P(Y=k)\)</span> is the
<strong>prior probability</strong> of class <span
class="math inline">\(k\)</span> (how common the class is overall).</li>
<li><span class="math inline">\(f_k(x) = f(x|Y=k)\)</span> is the
<strong>class-conditional probability density function</strong> of
observing input <span class="math inline">\(x\)</span> if it belongs to
class <span class="math inline">\(k\)</span>.</li>
</ul>
<p>To classify a new observation <span class="math inline">\(x\)</span>,
we simply find the class <span class="math inline">\(k\)</span> that
makes <span class="math inline">\(p_k(x)\)</span> the largest.
‰∏∫‰∫ÜÂØπÊñ∞ÁöÑËßÇÂØüÂÄº <span class="math inline">\(x\)</span>
ËøõË°åÂàÜÁ±ªÔºåÊàë‰ª¨Âè™ÈúÄÊâæÂà∞‰Ωø <span class="math inline">\(p_k(x)\)</span>
ÊúÄÂ§ßÁöÑÁ±ªÂà´ <span class="math inline">\(k\)</span> Âç≥ÂèØ„ÄÇ</p>
<hr />
<h2 id="key-assumptions-of-lda">Key Assumptions of LDA</h2>
<p>LDA‚Äôs power comes from a specific, simplifying assumption about the
data‚Äôs distribution. LDA
ÁöÑÂº∫Â§ß‰πãÂ§ÑÂú®‰∫éÂÆÉÂØπÊï∞ÊçÆÂàÜÂ∏ÉËøõË°å‰∫ÜÁâπÂÆöÁöÑÁÆÄÂåñÂÅáËÆæ„ÄÇ</p>
<ol type="1">
<li><p><strong>Gaussian Distribution:</strong> LDA assumes that the data
within each class <span class="math inline">\(k\)</span> follows a
p-dimensional multivariate normal (or Gaussian) distribution, denoted as
<span class="math inline">\(X|Y=k \sim \mathcal{N}(\mu_k,
\Sigma)\)</span>.</p></li>
<li><p><strong>Common Covariance:</strong> A crucial assumption is that
all classes share the <strong>same covariance matrix</strong> <span
class="math inline">\(\Sigma\)</span>. This means that while the classes
may have different centers (means, <span
class="math inline">\(\mu_k\)</span>), their shape and orientation
(covariance, <span class="math inline">\(\Sigma\)</span>) are
identical.</p></li>
<li><p><strong>È´òÊñØÂàÜÂ∏É</strong>ÔºöLDA ÂÅáËÆæÊØè‰∏™Á±ª <span
class="math inline">\(k\)</span> ‰∏≠ÁöÑÊï∞ÊçÆÊúç‰ªé p
Áª¥Â§öÂÖÉÊ≠£ÊÄÅÔºàÊàñÈ´òÊñØÔºâÂàÜÂ∏ÉÔºåË°®Á§∫‰∏∫ <span class="math inline">\(X|Y=k \sim
\mathcal{N}(\mu_k, \Sigma)\)</span>„ÄÇ</p></li>
<li><p><strong>ÂÖ±ÂêåÂçèÊñπÂ∑Æ</strong>Ôºö‰∏Ä‰∏™ÂÖ≥ÈîÆÂÅáËÆæÊòØÊâÄÊúâÁ±ªÂÖ±‰∫´<strong>Áõ∏ÂêåÁöÑÂçèÊñπÂ∑ÆÁü©Èòµ</strong>
<span
class="math inline">\(\Sigma\)</span>„ÄÇËøôÊÑèÂë≥ÁùÄËôΩÁÑ∂Á±ªÂèØËÉΩÂÖ∑Êúâ‰∏çÂêåÁöÑ‰∏≠ÂøÉÔºàÂùáÂÄºÔºå<span
class="math inline">\(\mu_k\)</span>ÔºâÔºå‰ΩÜÂÆÉ‰ª¨ÁöÑÂΩ¢Áä∂ÂíåÊñπÂêëÔºàÂçèÊñπÂ∑ÆÔºå<span
class="math inline">\(\Sigma\)</span>ÔºâÊòØÁõ∏ÂêåÁöÑ„ÄÇ</p></li>
</ol>
<p>The probability density function for a class <span
class="math inline">\(k\)</span> is: <span class="math display">\[
f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp \left( -\frac{1}{2}(x
- \mu_k)^T \Sigma^{-1} (x - \mu_k) \right)
\]</span></p>
<p>The image above (from your slide ‚ÄúKnowing normal distribution‚Äù)
illustrates this. The two ‚Äúbells‚Äù have different centers (different
<span class="math inline">\(\mu_k\)</span>) but similar shapes. The one
on the right is ‚Äútilted,‚Äù indicating correlation between variables,
which is captured in the shared covariance matrix <span
class="math inline">\(\Sigma\)</span>.
‰∏äÂõæÔºàÊëòËá™ÂπªÁÅØÁâá‚Äú‰∫ÜËß£Ê≠£ÊÄÅÂàÜÂ∏É‚ÄùÔºâËØ¥Êòé‰∫ÜËøô‰∏ÄÁÇπ„ÄÇ‰∏§‰∏™‚ÄúÈíü‚ÄùÂΩ¢ÁöÑ‰∏≠ÂøÉ‰∏çÂêåÔºà<span
class="math inline">\(\mu_k\)</span>
‰∏çÂêåÔºâÔºå‰ΩÜÂΩ¢Áä∂Áõ∏‰ºº„ÄÇÂè≥ËæπÁöÑÈíüÂΩ¢‚ÄúÂÄæÊñú‚ÄùÔºåË°®Á§∫ÂèòÈáè‰πãÈó¥Â≠òÂú®Áõ∏ÂÖ≥ÊÄßÔºåËøô‰ΩìÁé∞Âú®ÂÖ±‰∫´ÂçèÊñπÂ∑ÆÁü©Èòµ
<span class="math inline">\(\Sigma\)</span> ‰∏≠„ÄÇ</p>
<hr />
<h2 id="the-math-behind-lda-the-discriminant-function-Âà§Âà´ÂáΩÊï∞">The Math
Behind LDA: The Discriminant Function Âà§Âà´ÂáΩÊï∞</h2>
<p>Since we only need to find the class <span
class="math inline">\(k\)</span> that maximizes the posterior
probability <span class="math inline">\(p_k(x)\)</span>, we can simplify
the math. The denominator in Bayes‚Äô theorem is the same for all classes,
so we only need to maximize the numerator: <span
class="math inline">\(\pi_k f_k(x)\)</span>.
Áî±‰∫éÊàë‰ª¨Âè™ÈúÄË¶ÅÊâæÂà∞‰ΩøÂêéÈ™åÊ¶ÇÁéá <span class="math inline">\(p_k(x)\)</span>
ÊúÄÂ§ßÂåñÁöÑÁ±ªÂà´ <span
class="math inline">\(k\)</span>ÔºåÂõ†Ê≠§ÂèØ‰ª•ÁÆÄÂåñÊï∞Â≠¶ËÆ°ÁÆó„ÄÇË¥ùÂè∂ÊñØÂÆöÁêÜ‰∏≠ÁöÑÂàÜÊØçÂØπ‰∫éÊâÄÊúâÁ±ªÂà´ÈÉΩÊòØÁõ∏ÂêåÁöÑÔºåÂõ†Ê≠§Êàë‰ª¨Âè™ÈúÄË¶ÅÊúÄÂ§ßÂåñÂàÜÂ≠êÔºö<span
class="math inline">\(\pi_k f_k(x)\)</span>„ÄÇ Taking the logarithm
(which doesn‚Äôt change which class is maximal) and removing constant
terms gives us the <strong>linear discriminant function</strong>, <span
class="math inline">\(\delta_k(x)\)</span>:
ÂèñÂØπÊï∞ÔºàËøô‰∏ç‰ºöÊîπÂèòÂì™‰∏™Á±ªÂà´ÊòØÊúÄÂ§ßÂÄºÔºâÂπ∂ÁßªÈô§Â∏∏Êï∞È°πÔºåÂæóÂà∞<strong>Á∫øÊÄßÂà§Âà´ÂáΩÊï∞</strong>Ôºå<span
class="math inline">\(\delta_k(x)\)</span>Ôºö</p>
<p><span class="math display">\[
\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1}
\mu_k + \log(\pi_k)
\]</span></p>
<p>This function is <strong>linear</strong> in <span
class="math inline">\(x\)</span>, which is why the method is called
<em>Linear</em> Discriminant Analysis. The decision boundary between any
two classes, say class <span class="math inline">\(k\)</span> and class
<span class="math inline">\(l\)</span>, is the set of points where <span
class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>, which defines
a linear hyperplane. ËØ•ÂáΩÊï∞ÂÖ≥‰∫é <span class="math inline">\(x\)</span>
ÊòØ<strong>Á∫øÊÄß</strong>ÁöÑÔºåÂõ†Ê≠§ËØ•ÊñπÊ≥ïË¢´Áß∞‰∏∫<em>Á∫øÊÄß</em>Âà§Âà´ÂàÜÊûê„ÄÇ‰ªªÊÑè‰∏§‰∏™Á±ªÂà´Ôºà‰æãÂ¶ÇÁ±ªÂà´
<span class="math inline">\(k\)</span> ÂíåÁ±ªÂà´ <span
class="math inline">\(l\)</span>Ôºâ‰πãÈó¥ÁöÑÂÜ≥Á≠ñËæπÁïåÊòØÊª°Ë∂≥ <span
class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>
ÁöÑÁÇπÁöÑÈõÜÂêàÔºåËøôÂÆö‰πâ‰∫Ü‰∏Ä‰∏™Á∫øÊÄßË∂ÖÂπ≥Èù¢„ÄÇ</p>
<p>The image above (from your ‚ÄúGraph of LDA‚Äù slide) is very important. *
<strong>Left:</strong> The ellipses show the true 95% probability
contours for three Gaussian classes. The dashed lines are the ideal
Bayes decision boundaries, which are perfectly linear because the
assumption of common covariance holds. * <strong>Right:</strong> This
shows a sample of data points drawn from those distributions. The solid
lines are the LDA decision boundaries calculated from the sample. They
are a very good estimate of the ideal boundaries. ‰∏äÂõæÔºàÊù•Ëá™ÊÇ®ÁöÑ‚ÄúLDA
Âõæ‚ÄùÂπªÁÅØÁâáÔºâÈùûÂ∏∏ÈáçË¶Å„ÄÇ *
<strong>Â∑¶ÂõæÔºö</strong>Ê§≠ÂúÜÊòæÁ§∫‰∫Ü‰∏â‰∏™È´òÊñØÁ±ªÂà´ÁöÑÁúüÂÆû 95%
Ê¶ÇÁéáËΩÆÂªì„ÄÇËôöÁ∫øÊòØÁêÜÊÉ≥ÁöÑË¥ùÂè∂ÊñØÂÜ≥Á≠ñËæπÁïåÔºåÁî±‰∫éÂÖ±ÂêåÂçèÊñπÂ∑ÆÂÅáËÆæÊàêÁ´ãÔºåÂõ†Ê≠§ÂÆÉ‰ª¨ÊòØÂÆåÁæéÁöÑÁ∫øÊÄß„ÄÇ
*
<strong>Âè≥ÂõæÔºö</strong>ËøôÊòæÁ§∫‰∫Ü‰ªéËøô‰∫õÂàÜÂ∏É‰∏≠ÊäΩÂèñÁöÑÊï∞ÊçÆÁÇπÊ†∑Êú¨„ÄÇÂÆûÁ∫øÊòØÊ†πÊçÆÊ†∑Êú¨ËÆ°ÁÆóÂá∫ÁöÑ
LDA ÂÜ≥Á≠ñËæπÁïå„ÄÇÂÆÉ‰ª¨ÊòØÂØπÁêÜÊÉ≥ËæπÁïåÁöÑÈùûÂ∏∏Â•ΩÁöÑ‰º∞ËÆ°„ÄÇ ***</p>
<h2
id="practical-implementation-estimating-the-parameters-ÂÆûÈôÖÂ∫îÁî®‰º∞ËÆ°ÂèÇÊï∞">Practical
Implementation: Estimating the Parameters ÂÆûÈôÖÂ∫îÁî®Ôºö‰º∞ËÆ°ÂèÇÊï∞</h2>
<p>In a real-world scenario, we don‚Äôt know the true parameters (<span
class="math inline">\(\mu_k\)</span>, <span
class="math inline">\(\Sigma\)</span>, <span
class="math inline">\(\pi_k\)</span>). Instead, we
<strong>estimate</strong> them from our training data (<span
class="math inline">\(n\)</span> total samples, with <span
class="math inline">\(n_k\)</span> samples in class <span
class="math inline">\(k\)</span>).
Âú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÔºåÊàë‰ª¨‰∏çÁü•ÈÅìÁúüÊ≠£ÁöÑÂèÇÊï∞Ôºà<span
class="math inline">\(\mu_k\)</span>„ÄÅ<span
class="math inline">\(\Sigma\)</span>„ÄÅ<span
class="math inline">\(\pi_k\)</span>Ôºâ„ÄÇÁõ∏ÂèçÔºåÊàë‰ª¨Ê†πÊçÆËÆ≠ÁªÉÊï∞ÊçÆÔºà<span
class="math inline">\(n\)</span> ‰∏™Ê†∑Êú¨Ôºå<span
class="math inline">\(n_k\)</span> ‰∏™Ê†∑Êú¨Â±û‰∫é <span
class="math inline">\(k\)</span> Á±ªÔºâÊù•<strong>‰º∞ËÆ°</strong>ÂÆÉ‰ª¨„ÄÇ</p>
<ul>
<li><strong>Prior Probability (<span
class="math inline">\(\hat{\pi}_k\)</span>):</strong> The proportion of
training samples in class <span class="math inline">\(k\)</span>. <span
class="math display">\[\hat{\pi}_k = \frac{n_k}{n}\]</span></li>
<li><strong>Class Mean (<span
class="math inline">\(\hat{\mu}_k\)</span>):</strong> The average of the
training samples in class <span class="math inline">\(k\)</span>. <span
class="math display">\[\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i=k}
x_i\]</span></li>
<li><strong>Common Covariance (<span
class="math inline">\(\hat{\Sigma}\)</span>):</strong> A weighted
average of the sample covariance matrices for each class. This is often
called the ‚Äúpooled‚Äù covariance. <span
class="math display">\[\hat{\Sigma} = \frac{1}{n-K} \sum_{k=1}^{K}
\sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T\]</span></li>
<li><strong>ÂÖàÈ™åÊ¶ÇÁéá (<span
class="math inline">\(\hat{\pi}_k\)</span>)Ôºö</strong>ËÆ≠ÁªÉÊ†∑Êú¨Âú® <span
class="math inline">\(k\)</span> Á±ª‰∏≠ÁöÑÊØî‰æã„ÄÇ <span
class="math display">\[\hat{\pi}_k = \frac{n_k}{n}\]</span></li>
<li><strong>Á±ªÂà´ÂùáÂÄº (<span
class="math inline">\(\hat{\mu}_k\)</span>)Ôºö</strong>ËÆ≠ÁªÉÊ†∑Êú¨Âú® <span
class="math inline">\(k\)</span> Á±ª‰∏≠ÁöÑÂπ≥ÂùáÂÄº„ÄÇ <span
class="math display">\[\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i=k}
x_i\]</span></li>
<li><strong>ÂÖ¨ÂÖ±ÂçèÊñπÂ∑Æ (<span
class="math inline">\(\hat{\Sigma}\)</span>)Ôºö</strong>ÊØè‰∏™Á±ªÁöÑÊ†∑Êú¨ÂçèÊñπÂ∑ÆÁü©ÈòµÁöÑÂä†ÊùÉÂπ≥ÂùáÂÄº„ÄÇËøôÈÄöÂ∏∏Ë¢´Áß∞‰∏∫‚ÄúÂêàÂπ∂‚ÄùÂçèÊñπÂ∑Æ„ÄÇ
<span class="math display">\[\hat{\Sigma} = \frac{1}{n-K} \sum_{k=1}^{K}
\sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T\]</span></li>
</ul>
<p>We then plug these estimates into the discriminant function to get
<span class="math inline">\(\hat{\delta}_k(x)\)</span> and classify a
new observation <span class="math inline">\(x\)</span> to the class with
the largest score. ÁÑ∂ÂêéÔºåÊàë‰ª¨Â∞ÜËøô‰∫õ‰º∞ËÆ°ÂÄº‰ª£ÂÖ•Âà§Âà´ÂáΩÊï∞ÔºåÂæóÂà∞ <span
class="math inline">\(\hat{\delta}_k(x)\)</span>ÔºåÂπ∂Â∞ÜÊñ∞ÁöÑËßÇÊµãÂÄº <span
class="math inline">\(x\)</span> ÂΩíÁ±ªÂà∞ÂæóÂàÜÊúÄÈ´òÁöÑÁ±ªÂà´„ÄÇ ***</p>
<h2 id="evaluating-performance">Evaluating Performance</h2>
<p>After training the model, we evaluate its performance using a
<strong>confusion matrix</strong>.
ËÆ≠ÁªÉÊ®°ÂûãÂêéÔºåÊàë‰ª¨‰ΩøÁî®<strong>Ê∑∑Ê∑ÜÁü©Èòµ</strong>Êù•ËØÑ‰º∞ÂÖ∂ÊÄßËÉΩ„ÄÇ</p>
<p>This matrix shows the true classes versus the predicted classes. *
<strong>Diagonal elements</strong> (9644, 81) are correct predictions. *
<strong>Off-diagonal elements</strong> (23, 252) are errors.
ËØ•Áü©ÈòµÊòæÁ§∫‰∫ÜÁúüÂÆûÁ±ªÂà´‰∏éÈ¢ÑÊµãÁ±ªÂà´ÁöÑÂØπÊØî„ÄÇ * <strong>ÂØπËßíÁ∫øÂÖÉÁ¥†</strong>
(9644, 81) Ë°®Á§∫Ê≠£Á°ÆÈ¢ÑÊµã„ÄÇ * <strong>ÈùûÂØπËßíÁ∫øÂÖÉÁ¥†</strong> (23, 252)
Ë°®Á§∫ÈîôËØØÈ¢ÑÊµã„ÄÇ</p>
<p>From this matrix, we can calculate key metrics: * <strong>Overall
Error Rate:</strong> Total incorrect predictions / Total predictions. *
Example: <span class="math inline">\((252 + 23) / 10000 =
2.75\%\)</span> * <strong>Sensitivity (True Positive Rate):</strong>
Correctly predicted positives / Total actual positives. It answers: ‚ÄúOf
all the people who actually defaulted, what fraction did we catch?‚Äù *
Example: <span class="math inline">\(81 / 333 = 24.3\%\)</span>. The
sensitivity is <span class="math inline">\(1 - 75.7\% = 24.3\%\)</span>.
* <strong>Specificity (True Negative Rate):</strong> Correctly predicted
negatives / Total actual negatives. It answers: ‚ÄúOf all the people who
did not default, what fraction did we correctly identify?‚Äù * Example:
<span class="math inline">\(9644 / 9667 = 99.8\%\)</span>. The
specificity is <span class="math inline">\(1 - 0.24\% =
99.8\%\)</span>.</p>
<p>The example in your slides shows a high error rate for ‚Äúdefault‚Äù
people (75.7%) because the classes are <strong>unbalanced</strong>‚Äîthere
are far fewer defaulters. This highlights the importance of looking at
class-specific metrics, not just the overall error rate.</p>
<hr />
<h2 id="python-code-understanding">Python Code Understanding</h2>
<p>In Python, you can easily implement LDA using the
<code>scikit-learn</code> library. The code conceptually mirrors the
steps we discussed.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume you have your data X (features) and y (labels)</span></span><br><span class="line"><span class="comment"># X = features (e.g., balance, income)</span></span><br><span class="line"><span class="comment"># y = labels (e.g., 0 for &#x27;no-default&#x27;, 1 for &#x27;default&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create an instance of the LDA model</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit the model to the training data</span></span><br><span class="line"><span class="comment"># This is where the model calculates the estimates:</span></span><br><span class="line"><span class="comment">#  - Prior probabilities (pi_k)</span></span><br><span class="line"><span class="comment">#  - Class means (mu_k)</span></span><br><span class="line"><span class="comment">#  - Pooled covariance matrix (Sigma)</span></span><br><span class="line">lda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Make predictions on new, unseen data</span></span><br><span class="line">predictions = lda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Evaluate the model&#x27;s performance</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, predictions))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, predictions))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>LinearDiscriminantAnalysis()</code> creates the classifier
object.</li>
<li><code>lda.fit(X_train, y_train)</code> is the core training step
where the model learns the <span
class="math inline">\(\hat{\pi}_k\)</span>, <span
class="math inline">\(\hat{\mu}_k\)</span>, and <span
class="math inline">\(\hat{\Sigma}\)</span> parameters from the
data.</li>
<li><code>lda.predict(X_test)</code> uses the learned discriminant
function <span class="math inline">\(\hat{\delta}_k(x)\)</span> to
classify each sample in the test set.</li>
<li><code>confusion_matrix</code> and <code>classification_report</code>
are tools to evaluate the results, just like in the slides.</li>
</ul>
<h1
id="here-is-a-summary-of-the-provided-slides-on-linear-discriminant-analysis-lda-focusing-on-mathematical-concepts-python-code-interpretation-and-key-visuals.">6.
Here is a summary of the provided slides on Linear Discriminant Analysis
(LDA), focusing on mathematical concepts, Python code interpretation,
and key visuals.</h1>
<h2 id="core-concept-lda-for-classification">Core Concept: LDA for
Classification</h2>
<p>Linear Discriminant Analysis (LDA) is a classification method that
models the probability that an observation belongs to a certain class.
It works by finding a linear combination of features that best separates
two or more classes.</p>
<p>The decision is based on <strong>Bayes‚Äô theorem</strong>. For a given
observation with features <span class="math inline">\(X=x\)</span>, LDA
calculates the <strong>posterior probability</strong>, <span
class="math inline">\(p_k(x) = Pr(Y=k|X=x)\)</span>, for each class
<span class="math inline">\(k\)</span>. This is the probability that the
observation belongs to class <span class="math inline">\(k\)</span>
given its features. Á∫øÊÄßÂà§Âà´ÂàÜÊûê (LDA)
ÊòØ‰∏ÄÁßçÂàÜÁ±ªÊñπÊ≥ïÔºåÂÆÉÂØπËßÇÊµãÂÄºÂ±û‰∫éÊüê‰∏™Á±ªÂà´ÁöÑÊ¶ÇÁéáËøõË°åÂª∫Ê®°„ÄÇÂÆÉÁöÑÂ∑•‰ΩúÂéüÁêÜÊòØÊâæÂà∞ËÉΩÂ§üÊúÄÂ•ΩÂú∞Âå∫ÂàÜ‰∏§‰∏™ÊàñÂ§ö‰∏™Á±ªÂà´ÁöÑÁâπÂæÅÁöÑÁ∫øÊÄßÁªÑÂêà„ÄÇ</p>
<p>ËØ•ÂÜ≥Á≠ñÂü∫‰∫é<strong>Ë¥ùÂè∂ÊñØÂÆöÁêÜ</strong>„ÄÇÂØπ‰∫éÁâπÂæÅ‰∏∫ <span
class="math inline">\(X=x\)</span> ÁöÑÁªôÂÆöËßÇÊµãÂÄºÔºåLDA ‰ºöËÆ°ÁÆóÊØè‰∏™Á±ªÂà´
<span class="math inline">\(k\)</span>
ÁöÑ<strong>ÂêéÈ™åÊ¶ÇÁéá</strong>Ôºå<span class="math inline">\(p_k(x) =
Pr(Y=k|X=x)\)</span>„ÄÇËøôÊòØÁªôÂÆöËßÇÊµãÂÄºÁöÑÁâπÂæÅÂêéÔºåËØ•ËßÇÊµãÂÄºÂ±û‰∫éÁ±ªÂà´ <span
class="math inline">\(k\)</span> ÁöÑÊ¶ÇÁéá„ÄÇ</p>
<p>By default, the Bayes classifier assigns an observation to the class
with the highest posterior probability. For a binary (two-class) problem
like ‚ÄòYes‚Äô vs.¬†‚ÄòNo‚Äô, this means:
ÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåË¥ùÂè∂ÊñØÂàÜÁ±ªÂô®Â∞ÜËßÇÊµãÂÄºÂàÜÈÖçÁªôÂêéÈ™åÊ¶ÇÁéáÊúÄÈ´òÁöÑÁ±ªÂà´„ÄÇÂØπ‰∫éÂÉè‚ÄúÊòØ‚Äù‰∏é‚ÄúÂê¶‚ÄùËøôÊ†∑ÁöÑ‰∫åÂàÜÁ±ªÈóÆÈ¢òÔºåËøôÊÑèÂë≥ÁùÄÔºö</p>
<ul>
<li>Assign to ‚ÄòYes‚Äô if <span class="math inline">\(Pr(Y=\text{Yes}|X=x)
&gt; 0.5\)</span></li>
<li>Assign to ‚ÄòNo‚Äô otherwise</li>
</ul>
<h2 id="modifying-the-decision-threshold">Modifying the Decision
Threshold</h2>
<p>The default 0.5 threshold isn‚Äôt always optimal. In many real-world
scenarios, the cost of one type of error is much higher than another.
For example, in credit card default prediction: ÈªòËÆ§ÁöÑ 0.5
ÈòàÂÄºÂπ∂ÈùûÊÄªÊòØÊúÄ‰ºòÁöÑ„ÄÇÂú®ËÆ∏Â§öÂÆûÈôÖÂú∫ÊôØ‰∏≠Ôºå‰∏ÄÁßçÈîôËØØÁöÑ‰ª£‰ª∑ËøúÈ´ò‰∫éÂè¶‰∏ÄÁßç„ÄÇ‰æãÂ¶ÇÔºåÂú®‰ø°Áî®Âç°ËøùÁ∫¶È¢ÑÊµã‰∏≠Ôºö</p>
<ul>
<li><strong>False Negative:</strong> Incorrectly classifying a person
who will default as someone who won‚Äôt. (The bank loses money).</li>
<li><strong>False Positive:</strong> Incorrectly classifying a person
who won‚Äôt default as someone who will. (The bank loses a potential
customer).</li>
</ul>
<p>A bank might decide that missing a defaulter is much worse than
denying a good customer. To catch more potential defaulters, they can
<strong>lower the probability threshold</strong>.
Èì∂Ë°åÂèØËÉΩ‰ºöËÆ§‰∏∫ÈîôËøá‰∏Ä‰∏™ËøùÁ∫¶ËÄÖÊØîÊãíÁªù‰∏Ä‰∏™‰ºòË¥®ÂÆ¢Êà∑Êõ¥Á≥üÁ≥ï„ÄÇ‰∏∫‰∫ÜÊçïÊçâÊõ¥Â§öÊΩúÂú®ÁöÑËøùÁ∫¶ËÄÖÔºå‰ªñ‰ª¨ÂèØ‰ª•<strong>Èôç‰ΩéÊ¶ÇÁéáÈòàÂÄº</strong>„ÄÇ</p>
<p>A modified rule could be: <span class="math display">\[
Pr(\text{default}=\text{Yes}|X=x) &gt; 0.2
\]</span> This makes the model more ‚Äúsensitive‚Äù to flagging potential
defaulters, even at the cost of misclassifying more non-defaulters.
Èôç‰ΩéÈòàÂÄº<strong>‰ºöÊèêÈ´òÊïèÊÑüÂ∫¶</strong>Ôºå‰ΩÜ<strong>‰ºöÈôç‰ΩéÁâπÂºÇÊÄß</strong>„ÄÇ</p>
<p>This decision leads to a <strong>trade-off</strong> between two key
performance metrics: * <strong>Sensitivity (True Positive
Rate):</strong> The ability to correctly identify positive cases. (e.g.,
<code>Correctly identified defaulters / Total actual defaulters</code>).
* <strong>Specificity (True Negative Rate):</strong> The ability to
correctly identify negative cases. (e.g.,
<code>Correctly identified non-defaulters / Total actual non-defaulters</code>).</p>
<p>Ëøô‰∏ÄÂÜ≥Á≠ñ‰ºöÂØºËá¥‰∏§‰∏™ÂÖ≥ÈîÆÁª©ÊïàÊåáÊ†á‰πãÈó¥ÁöÑ<strong>ÊùÉË°°</strong>Ôºö *
<strong>ÊïèÊÑüÂ∫¶ÔºàÁúüÈò≥ÊÄßÁéáÔºâÔºö</strong>Ê≠£Á°ÆËØÜÂà´Èò≥ÊÄßÊ°à‰æãÁöÑËÉΩÂäõ„ÄÇÔºà‰æãÂ¶ÇÔºå‚ÄúÊ≠£Á°ÆËØÜÂà´ÁöÑËøùÁ∫¶ËÄÖ/ÂÆûÈôÖËøùÁ∫¶ËÄÖÊÄªÊï∞‚ÄùÔºâ„ÄÇ
*
<strong>ÁâπÂºÇÊÄßÔºàÁúüÈò¥ÊÄßÁéáÔºâÔºö</strong>Ê≠£Á°ÆËØÜÂà´Èò¥ÊÄßÊ°à‰æãÁöÑËÉΩÂäõ„ÄÇÔºà‰æãÂ¶ÇÔºå‚ÄúÊ≠£Á°ÆËØÜÂà´ÁöÑÈùûËøùÁ∫¶ËÄÖ/ÂÆûÈôÖÈùûËøùÁ∫¶ËÄÖÊÄªÊï∞‚ÄùÔºâ„ÄÇ</p>
<p>Lowering the threshold <strong>increases sensitivity</strong> but
<strong>decreases specificity</strong>. ## Python Code Explained</p>
<p>The slides show how to implement and adjust LDA using Python‚Äôs
<code>scikit-learn</code> library.</p>
<h2 id="basic-lda-implementation">Basic LDA Implementation</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the necessary library</span></span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize and train the LDA model</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line">lda_train = lda.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions using the default 0.5 threshold</span></span><br><span class="line">y_pred = lda.predict(X)</span><br></pre></td></tr></table></figure>
<p>This code trains an LDA model and makes predictions using the
standard 50% probability boundary.</p>
<h2 id="adjusting-the-prediction-threshold">Adjusting the Prediction
Threshold</h2>
<p>To use a custom threshold (e.g., 0.2), you don‚Äôt use the
<code>.predict()</code> method. Instead, you get the class probabilities
with <code>.predict_proba()</code> and apply the threshold manually.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Get the probabilities for each class</span></span><br><span class="line"><span class="comment"># lda.predict_proba(X) returns an array like [[P(No), P(Yes)], ...]</span></span><br><span class="line"><span class="comment"># We select the second column [:, 1] for the &#x27;Yes&#x27; class probability</span></span><br><span class="line">lda_probs = lda.predict_proba(X)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Define a custom threshold</span></span><br><span class="line">threshold = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Apply the threshold to get new predictions</span></span><br><span class="line"><span class="comment"># This creates a boolean array (True where prob &gt; 0.2, else False)</span></span><br><span class="line"><span class="comment"># We then convert True/False to &#x27;Yes&#x27;/&#x27;No&#x27; labels</span></span><br><span class="line">lda_pred1 = np.where(lda_probs &gt; threshold, <span class="string">&quot;Yes&quot;</span>, <span class="string">&quot;No&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>This is the core technique for tuning the classifier‚Äôs behavior to
meet specific business needs, as demonstrated on slides 55 and 56 for
both LDA and Logistic Regression.</p>
<h2 id="important-images-to-understand">Important Images to
Understand</h2>
<ol type="1">
<li><strong>Confusion Matrix (Slide 49):</strong> This table is crucial.
It breaks down the model‚Äôs predictions into True Positives, True
Negatives, False Positives, and False Negatives. All key metrics like
error rate, sensitivity, and specificity are calculated from this
matrix. <strong>Ê∑∑Ê∑ÜÁü©ÈòµÔºàÂπªÁÅØÁâá
49ÔºâÔºö</strong>ËøôÂº†Ë°®Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÂÆÉÂ∞ÜÊ®°ÂûãÁöÑÈ¢ÑÊµãÂàÜËß£‰∏∫ÁúüÈò≥ÊÄß„ÄÅÁúüÈò¥ÊÄß„ÄÅÂÅáÈò≥ÊÄßÂíåÂÅáÈò¥ÊÄß„ÄÇÊâÄÊúâÂÖ≥ÈîÆÊåáÊ†áÔºå‰æãÂ¶ÇÈîôËØØÁéá„ÄÅÁÅµÊïèÂ∫¶ÂíåÁâπÂºÇÊÄßÔºåÈÉΩÂü∫‰∫éÊ≠§Áü©ÈòµËÆ°ÁÆóÂæóÂá∫„ÄÇ</li>
<li><strong>LDA Decision Boundaries (Slide 51):</strong> This plot
provides a powerful visual intuition. It shows the data points for two
classes and the decision boundary line. The different parallel lines
show how changing the threshold from 0.5 to 0.1 or 0.9 shifts the
boundary, making the model classify more or fewer points into the
minority class. <strong>LDA ÂÜ≥Á≠ñËæπÁïåÔºàÂπªÁÅØÁâá
51ÔºâÔºö</strong>ËøôÂº†ÂõæÊèê‰æõ‰∫ÜÂº∫Â§ßÁöÑËßÜËßâÁõ¥ËßÇÊÄß„ÄÇÂÆÉÂ±ïÁ§∫‰∫Ü‰∏§‰∏™Á±ªÂà´ÁöÑÊï∞ÊçÆÁÇπÂíåÂÜ≥Á≠ñËæπÁïåÁ∫ø„ÄÇ‰∏çÂêåÁöÑÂπ≥Ë°åÁ∫øÊòæÁ§∫‰∫ÜÂ∞ÜÈòàÂÄº‰ªé
0.5 Êõ¥Êîπ‰∏∫ 0.1 Êàñ 0.9
Êó∂ËæπÁïåÂ¶Ç‰ΩïÁßªÂä®Ôºå‰ªéËÄå‰ΩøÊ®°ÂûãÂ∞ÜÊõ¥Â§öÊàñÊõ¥Â∞ëÁöÑÁÇπÂΩíÂÖ•Â∞ëÊï∞Á±ª</li>
<li><strong>Error Rate Tradeoff Curve (Slide 53):</strong> This graph is
the most important for understanding the business implication of
changing the threshold. It clearly shows that as the threshold changes,
the error rate for one class goes down while the error rate for the
other goes up. The overall error is minimized at a certain point, but
that may not be the optimal point from a business perspective.
<strong>ÈîôËØØÁéáÊùÉË°°Êõ≤Á∫øÔºàÂπªÁÅØÁâá
53ÔºâÔºö</strong>ËøôÂº†ÂõæÂØπ‰∫éÁêÜËß£Êõ¥ÊîπÈòàÂÄºÁöÑ‰∏öÂä°Âê´‰πâËá≥ÂÖ≥ÈáçË¶Å„ÄÇÂÆÉÊ∏ÖÊ•öÂú∞Ë°®ÊòéÔºåÈöèÁùÄÈòàÂÄºÁöÑÂèòÂåñÔºå‰∏Ä‰∏™Á±ªÂà´ÁöÑÈîôËØØÁéá‰∏ãÈôçÔºåËÄåÂè¶‰∏Ä‰∏™Á±ªÂà´ÁöÑÈîôËØØÁéá‰∏äÂçá„ÄÇÊÄª‰ΩìËØØÂ∑ÆÂú®Êüê‰∏™ÁÇπËææÂà∞ÊúÄÂ∞èÔºå‰ΩÜ‰ªé‰∏öÂä°ËßíÂ∫¶Êù•ÁúãÔºåËøôÂèØËÉΩÂπ∂ÈùûÊúÄ‰Ω≥ÁÇπ„ÄÇ</li>
<li><strong>ROC Curve (Slides 54 &amp; 55):</strong> The Receiver
Operating Characteristic (ROC) curve plots Sensitivity vs.¬†(1 -
Specificity) for <em>all possible thresholds</em>. An ideal classifier
has a curve that ‚Äúhugs‚Äù the top-left corner, indicating high sensitivity
and high specificity. It‚Äôs a standard way to visualize and compare the
overall performance of different classifiers. <strong>ROC Êõ≤Á∫øÔºàÂπªÁÅØÁâá
54 Âíå 55ÔºâÔºö</strong> Êé•Êî∂ËÄÖÊìç‰ΩúÁâπÊÄß (ROC)
Êõ≤Á∫øÁªòÂà∂‰∫Ü<em>ÊâÄÊúâÂèØËÉΩÈòàÂÄº</em>ÁöÑÁÅµÊïèÂ∫¶‰∏éÔºà1 -
ÁâπÂºÇÊÄßÔºâÁöÑÂÖ≥Á≥ª„ÄÇÁêÜÊÉ≥ÁöÑÂàÜÁ±ªÂô®Êõ≤Á∫ø‚ÄúÁ¥ßË¥¥‚ÄùÂ∑¶‰∏äËßíÔºåË°®Á§∫È´òÁÅµÊïèÂ∫¶ÂíåÈ´òÁâπÂºÇÊÄß„ÄÇËøôÊòØÂèØËßÜÂåñÂíåÊØîËæÉ‰∏çÂêåÂàÜÁ±ªÂô®Êï¥‰ΩìÊÄßËÉΩÁöÑÊ†áÂáÜÊñπÊ≥ï„ÄÇ</li>
</ol>
<h1
id="here-is-a-summary-of-the-provided-slides-on-linear-and-quadratic-discriminant-analysis-including-the-key-formulas-python-code-equivalents-and-explanations-of-the-important-concepts.">7.
Here is a summary of the provided slides on Linear and Quadratic
Discriminant Analysis, including the key formulas, Python code
equivalents, and explanations of the important concepts.</h1>
<h2 id="key-goal-classification"><strong>Key Goal:
Classification</strong></h2>
<p>Both <strong>Linear Discriminant Analysis (LDA)</strong> and
<strong>Quadratic Discriminant Analysis (QDA)</strong> are
classification algorithms. Their main goal is to find a decision
boundary to separate different classes (e.g., ‚Äúdefault‚Äù vs.¬†‚Äúnot
default‚Äù) in the data. <strong>Á∫øÊÄßÂà§Âà´ÂàÜÊûê (LDA)</strong> Âíå
<strong>‰∫åÊ¨°Âà§Âà´ÂàÜÊûê (QDA)</strong>
ÈÉΩÊòØÂàÜÁ±ªÁÆóÊ≥ï„ÄÇÂÆÉ‰ª¨ÁöÑ‰∏ªË¶ÅÁõÆÊ†áÊòØÊâæÂà∞‰∏Ä‰∏™ÂÜ≥Á≠ñËæπÁïåÊù•Âå∫ÂàÜÊï∞ÊçÆ‰∏≠ÁöÑ‰∏çÂêåÁ±ªÂà´Ôºà‰æãÂ¶ÇÔºå‚ÄúÈªòËÆ§‚Äù‰∏é‚ÄúÈùûÈªòËÆ§‚ÄùÔºâ„ÄÇ</p>
<h3 id="linear-discriminant-analysis-lda">## Linear Discriminant
Analysis (LDA)</h3>
<p>LDA creates a <strong>linear</strong> decision boundary between
classes. LDA Âú®Á±ªÂà´‰πãÈó¥ÂàõÂª∫<strong>Á∫øÊÄß</strong>ÂÜ≥Á≠ñËæπÁïå„ÄÇ</p>
<h4 id="core-idea-fishers-interpretation"><strong>Core Idea (Fisher‚Äôs
Interpretation)</strong></h4>
<p>Imagine you have data points for different classes in a 3D space.
Fisher‚Äôs idea is to find the best angle to shine a ‚Äúflashlight‚Äù on the
data to project its shadow onto a 2D wall (or a 1D line). The ‚Äúbest‚Äù
projection is the one where the shadows of the different classes are
<strong>as far apart from each other as possible</strong>, while the
shadows within each class are <strong>as tightly packed as
possible</strong>. ÊÉ≥Ë±°‰∏Ä‰∏ãÔºå‰Ω†Âú®‰∏âÁª¥Á©∫Èó¥‰∏≠Êã•Êúâ‰∏çÂêåÁ±ªÂà´ÁöÑÊï∞ÊçÆÁÇπ„ÄÇFisher
ÁöÑÊÄùÊÉ≥ÊòØÊâæÂà∞ÊúÄ‰Ω≥ËßíÂ∫¶ÔºåÁî®‚ÄúÊâãÁîµÁ≠í‚ÄùÁÖßÂ∞ÑÊï∞ÊçÆÔºåÂ∞ÜÂÖ∂Èò¥ÂΩ±ÊäïÂ∞ÑÂà∞‰∫åÁª¥Â¢ôÂ£ÅÔºàÊàñ‰∏ÄÁª¥Á∫ø‰∏äÔºâ„ÄÇ
‚ÄúÊúÄ‰Ω≥‚ÄùÊäïÂΩ±ÊòØ‰∏çÂêåÁ±ªÂà´ÁöÑÈò¥ÂΩ±<strong>ÂΩºÊ≠§‰πãÈó¥Â∞ΩÂèØËÉΩËøú</strong>ÔºåËÄåÊØè‰∏™Á±ªÂà´ÂÜÖÁöÑÈò¥ÂΩ±<strong>Â∞ΩÂèØËÉΩÁ¥ßÂØÜ</strong>ÁöÑÊäïÂΩ±„ÄÇ</p>
<ul>
<li><strong>Maximize:</strong> The distance between the means of the
projected classes (Between-Class Variance).
ÊäïÂΩ±Á±ªÂà´ÂùáÂÄº‰πãÈó¥ÁöÑË∑ùÁ¶ªÔºàÁ±ªÈó¥ÊñπÂ∑ÆÔºâ„ÄÇ</li>
<li><strong>Minimize:</strong> The spread or variance within each
projected class (Within-Class Variance).
ÊØè‰∏™ÊäïÂΩ±Á±ªÂà´ÂÜÖÁöÑÊâ©Êï£ÊàñÊñπÂ∑ÆÔºàÁ±ªÂÜÖÊñπÂ∑ÆÔºâ„ÄÇ This is the most important
image for understanding the intuition behind LDA. It shows how
projecting the data onto a specific line (defined by vector
<code>w</code>) can make the two classes clearly separable.
ËøôÊòØÁêÜËß£LDAËÉåÂêéÁõ¥ËßâÁöÑÊúÄÈáçË¶ÅÂõæÂÉè„ÄÇÂÆÉÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂ∞ÜÊï∞ÊçÆÊäïÂΩ±Âà∞ÁâπÂÆöÁõ¥Á∫øÔºàÁî±ÂêëÈáè‚Äúw‚ÄùÂÆö‰πâÔºâ‰∏äÔºå‰ªéËÄå‰Ωø‰∏§‰∏™Á±ªÂà´Ê∏ÖÊô∞ÂèØÂàÜ„ÄÇ</li>
</ul>
<h4 id="key-mathematical-formulas"><strong>Key Mathematical
Formulas</strong></h4>
<p>To achieve this, LDA maximizes a ratio called the <strong>Rayleigh
quotient</strong>. LDAÊúÄÂ§ßÂåñ‰∏Ä‰∏™Áß∞‰∏∫<strong>ÁëûÂà©ÂïÜ</strong>ÁöÑÊØîÁéá„ÄÇ</p>
<ol type="1">
<li><strong>Within-Class Covariance (<span
class="math inline">\(\hat{\Sigma}_W\)</span>)</strong>: Measures the
spread of data <em>inside</em> each class. <strong>Á±ªÂÜÖÂçèÊñπÂ∑Æ (<span
class="math inline">\(\hat{\Sigma}_W\)</span>)</strong>ÔºöË°°ÈáèÊØè‰∏™Á±ªÂà´<em>ÂÜÖÈÉ®</em>Êï∞ÊçÆÁöÑÊâ©Êï£Á®ãÂ∫¶„ÄÇ
<span class="math display">\[\hat{\Sigma}_W = \frac{1}{n-K}
\sum_{k=1}^{K} \sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i -
\hat{\mu}_k)^\top\]</span></li>
<li><strong>Between-Class Covariance (<span
class="math inline">\(\hat{\Sigma}_B\)</span>)</strong>: Measures the
spread <em>between</em> the means of different classes.
<strong>Á±ªÈó¥ÂçèÊñπÂ∑Æ (<span
class="math inline">\(\hat{\Sigma}_B\)</span>)</strong>ÔºöË°°Èáè‰∏çÂêåÁ±ªÂà´ÂùáÂÄº<em>‰πãÈó¥</em>ÁöÑÂ∑ÆÂºÇ„ÄÇ
<span class="math display">\[\hat{\Sigma}_B = \sum_{k=1}^{K} n_k
(\hat{\mu}_k - \hat{\mu})(\hat{\mu}_k - \hat{\mu})^\top\]</span></li>
<li><strong>Objective Function</strong>: Find the projection vector
<span class="math inline">\(w\)</span> that maximizes the ratio of
between-class variance to within-class variance.
<strong>ÁõÆÊ†áÂáΩÊï∞</strong>ÔºöÊâæÂà∞ÊäïÂΩ±ÂêëÈáè <span
class="math inline">\(w\)</span>Ôºå‰ΩøÁ±ªÈó¥ÊñπÂ∑Æ‰∏éÁ±ªÂÜÖÊñπÂ∑Æ‰πãÊØîÊúÄÂ§ßÂåñ„ÄÇ <span
class="math display">\[\max_w \frac{w^\top \hat{\Sigma}_B w}{w^\top
\hat{\Sigma}_W w}\]</span></li>
</ol>
<h4 id="ldas-main-assumption"><strong>LDA‚Äôs Main
Assumption</strong></h4>
<p>The key assumption of LDA is that all classes share the <strong>same
covariance matrix (<span
class="math inline">\(\Sigma\)</span>)</strong>. They can have different
means (<span class="math inline">\(\mu_k\)</span>), but their spread and
orientation must be identical. This assumption is what results in a
linear decision boundary. LDA
ÁöÑÂÖ≥ÈîÆÂÅáËÆæÊòØÊâÄÊúâÁ±ªÂà´ÂÖ±‰∫´<strong>Áõ∏ÂêåÁöÑÂçèÊñπÂ∑ÆÁü©Èòµ (<span
class="math inline">\(\Sigma\)</span>)</strong>„ÄÇÂÆÉ‰ª¨ÂèØ‰ª•ÂÖ∑Êúâ‰∏çÂêåÁöÑÂùáÂÄº
(<span
class="math inline">\(\mu_k\)</span>)Ôºå‰ΩÜÂÆÉ‰ª¨ÁöÑÊï£Â∫¶ÂíåÊñπÂêëÂøÖÈ°ªÁõ∏Âêå„ÄÇÊ≠£ÊòØËøô‰∏ÄÂÅáËÆæÂØºËá¥‰∫ÜÁ∫øÊÄßÂÜ≥Á≠ñËæπÁïå„ÄÇ</p>
<h3 id="quadratic-discriminant-analysis-qda">## Quadratic Discriminant
Analysis (QDA)</h3>
<p>QDA is a more flexible extension of LDA that creates a
<strong>quadratic</strong> (curved) decision boundary. QDA ÊòØ LDA
ÁöÑÊõ¥ÁÅµÊ¥ªÁöÑÊâ©Â±ïÔºåÂÆÉÂàõÂª∫‰∫Ü<strong>‰∫åÊ¨°</strong>ÔºàÊõ≤Á∫øÔºâÂÜ≥Á≠ñËæπÁïå„ÄÇ ####
<strong>Core Idea &amp; Key Assumption</strong></p>
<p>QDA starts with the same principles as LDA but drops the key
assumption. QDA assumes that <strong>each class has its own unique
covariance matrix (<span
class="math inline">\(\Sigma_k\)</span>)</strong>. QDA ÁöÑÂéüÁêÜ‰∏é LDA
Áõ∏ÂêåÔºå‰ΩÜÊîæÂºÉ‰∫ÜÂÖ≥ÈîÆÂÅáËÆæ„ÄÇQDA ÂÅáËÆæ<strong>ÊØè‰∏™Á±ªÂà´ÈÉΩÊúâËá™Â∑±Áã¨ÁâπÁöÑÂçèÊñπÂ∑ÆÁü©Èòµ
(<span class="math inline">\(\Sigma_k\)</span>)</strong>„ÄÇ</p>
<p>This means each class can have its own spread, shape, and
orientation. This additional flexibility allows for a more complex,
curved decision boundary.
ËøôÊÑèÂë≥ÁùÄÊØè‰∏™Á±ªÂà´ÂèØ‰ª•Êã•ÊúâËá™Â∑±ÁöÑÊï£Â∫¶„ÄÅÂΩ¢Áä∂ÂíåÊñπÂêë„ÄÇËøôÁßçÈ¢ùÂ§ñÁöÑÁÅµÊ¥ªÊÄß‰ΩøÂæóÂÜ≥Á≠ñËæπÁïåÊõ¥Âä†Â§çÊùÇ„ÄÅÊõ≤Á∫øÂåñ„ÄÇ</p>
<h4 id="key-mathematical-formula"><strong>Key Mathematical
Formula</strong></h4>
<p>The classification is made using a discrimination function, <span
class="math inline">\(\delta_k(x)\)</span>. We assign a data point <span
class="math inline">\(x\)</span> to the class <span
class="math inline">\(k\)</span> for which <span
class="math inline">\(\delta_k(x)\)</span> is largest. The function for
QDA is: <span class="math display">\[\delta_k(x) = -\frac{1}{2}(x -
\mu_k)^\top \Sigma_k^{-1}(x - \mu_k) - \frac{1}{2}\log(|\Sigma_k|) +
\log \pi_k\]</span> The term containing <span
class="math inline">\(x^\top \Sigma_k^{-1} x\)</span> makes this
function a <strong>quadratic</strong> function of <span
class="math inline">\(x\)</span>.</p>
<h3 id="lda-vs.-qda-the-trade-off">## LDA vs.¬†QDA: The Trade-Off</h3>
<p>The choice between LDA and QDA is a classic <strong>bias-variance
trade-off</strong>. Âú® LDA Âíå QDA
‰πãÈó¥ËøõË°åÈÄâÊã©ÊòØÂÖ∏ÂûãÁöÑ<strong>ÂÅèÂ∑Æ-ÊñπÂ∑ÆÊùÉË°°</strong>„ÄÇ</p>
<ul>
<li><p><strong>Use LDA when:</strong></p>
<ul>
<li>The assumption of a common covariance matrix is reasonable (the
classes have similar shapes).</li>
<li>You have a small amount of training data, as LDA is less prone to
overfitting.</li>
<li>Simplicity is preferred. LDA is less flexible (high bias) but has
lower variance.</li>
<li>ÂÅáËÆæÂÖ±ÂêåÂçèÊñπÂ∑ÆÁü©ÈòµÊòØÂêàÁêÜÁöÑÔºàÁ±ªÂà´ÂÖ∑ÊúâÁõ∏‰ººÁöÑÂΩ¢Áä∂Ôºâ„ÄÇ</li>
<li>ËÆ≠ÁªÉÊï∞ÊçÆÈáèËæÉÂ∞ëÔºåÂõ†‰∏∫ LDA ‰∏çÊòìËøáÊãüÂêà„ÄÇ</li>
<li>ÁÆÄÊ¥ÅÊòØÈ¶ñÈÄâ„ÄÇLDA ÁÅµÊ¥ªÊÄßËæÉÂ∑ÆÔºàÂÅèÂ∑ÆËæÉÂ§ßÔºâÔºå‰ΩÜÊñπÂ∑ÆËæÉÂ∞è„ÄÇ</li>
</ul></li>
<li><p><strong>Use QDA when:</strong></p>
<ul>
<li>The classes have clearly different shapes and spreads (different
covariance matrices).</li>
<li>You have a large amount of training data to properly estimate the
separate covariance matrices for each class.</li>
<li>QDA is more flexible (low bias) but can have high variance, meaning
it might overfit on smaller datasets.</li>
<li>Á±ªÂà´ÂÖ∑ÊúâÊòéÊòæ‰∏çÂêåÁöÑÂΩ¢Áä∂ÂíåÂàÜÂ∏ÉÔºà‰∏çÂêåÁöÑÂçèÊñπÂ∑ÆÁü©ÈòµÔºâ„ÄÇ</li>
<li>Êã•ÊúâÂ§ßÈáèËÆ≠ÁªÉÊï∞ÊçÆÔºåÂèØ‰ª•Ê≠£Á°Æ‰º∞ËÆ°ÊØè‰∏™Á±ªÂà´ÁöÑÁã¨Á´ãÂçèÊñπÂ∑ÆÁü©Èòµ„ÄÇ</li>
<li>QDA
Êõ¥ÁÅµÊ¥ªÔºàÂÅèÂ∑ÆËæÉÂ∞èÔºâÔºå‰ΩÜÊñπÂ∑ÆËæÉÂ§ßÔºåËøôÊÑèÂë≥ÁùÄÂÆÉÂèØËÉΩÂú®ËæÉÂ∞èÁöÑÊï∞ÊçÆÈõÜ‰∏äËøáÊãüÂêà„ÄÇ
<strong>Rule of Thumb:</strong> If the class variances are equal or
close, LDA is better. Otherwise, QDA is better.
<strong>ÁªèÈ™åÊ≥ïÂàôÔºö</strong> Â¶ÇÊûúÁ±ªÂà´ÊñπÂ∑ÆÁõ∏Á≠âÊàñÊé•ËøëÔºåÂàô LDA
Êõ¥‰Ω≥„ÄÇÂê¶ÂàôÔºåQDA Êõ¥Â•Ω„ÄÇ</li>
</ul></li>
</ul>
<h3 id="code-understanding-python-equivalent">## Code Understanding
(Python Equivalent)</h3>
<p>The slides show code in R. Here‚Äôs how you would perform LDA and
evaluate it in Python using the popular <code>scikit-learn</code>
library.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, accuracy_score, roc_curve, auc</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;df&#x27; is your DataFrame with features and a &#x27;target&#x27; column</span></span><br><span class="line"><span class="comment"># X = df.drop(&#x27;target&#x27;, axis=1)</span></span><br><span class="line"><span class="comment"># y = df[&#x27;target&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit an LDA model (equivalent to lda() in R)</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line">lda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Make predictions (equivalent to predict() in R)</span></span><br><span class="line">y_pred_lda = lda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># To fit a QDA model, the process is identical:</span></span><br><span class="line"><span class="comment"># qda = QuadraticDiscriminantAnalysis()</span></span><br><span class="line"><span class="comment"># qda.fit(X_train, y_train)</span></span><br><span class="line"><span class="comment"># y_pred_qda = qda.predict(X_test)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Create a confusion matrix (equivalent to table())</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LDA Confusion Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, y_pred_lda))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Plot the ROC Curve (equivalent to the R code for ROC)</span></span><br><span class="line"><span class="comment"># Get prediction probabilities for the positive class</span></span><br><span class="line">y_pred_proba = lda.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate ROC curve points</span></span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate Area Under the Curve (AUC)</span></span><br><span class="line">roc_auc = auc(fpr, tpr)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(fpr, tpr, color=<span class="string">&#x27;blue&#x27;</span>, lw=<span class="number">2</span>, label=<span class="string">f&#x27;LDA ROC curve (area = <span class="subst">&#123;roc_auc:<span class="number">.2</span>f&#125;</span>)&#x27;</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">&#x27;gray&#x27;</span>, lw=<span class="number">2</span>, linestyle=<span class="string">&#x27;--&#x27;</span>) <span class="comment"># Random guess line</span></span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate (1 - Specificity)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate (Sensitivity)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Receiver Operating Characteristic (ROC) Curve&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="understanding-the-roc-curve"><strong>Understanding the ROC
Curve</strong></h4>
<p>The <strong>ROC Curve</strong> is another important image. It helps
you visualize a classifier‚Äôs performance across all possible
classification thresholds. <strong>ROC Êõ≤Á∫ø</strong>
ÊòØÂè¶‰∏Ä‰∏™ÈáçË¶ÅÁöÑÂõæÂÉè„ÄÇÂÆÉÂèØ‰ª•Â∏ÆÂä©ÊÇ®Áõ¥ËßÇÂú∞‰∫ÜËß£ÂàÜÁ±ªÂô®Âú®ÊâÄÊúâÂèØËÉΩÁöÑÂàÜÁ±ªÈòàÂÄº‰∏ãÁöÑÊÄßËÉΩ„ÄÇ</p>
<ul>
<li>The <strong>Y-axis</strong> is the <strong>True Positive
Rate</strong> (Sensitivity): ‚ÄúOf all the actual positives, how many did
we correctly identify?‚Äù</li>
<li>The <strong>X-axis</strong> is the <strong>False Positive
Rate</strong>: ‚ÄúOf all the actual negatives, how many did we incorrectly
label as positive?‚Äù</li>
<li>A perfect classifier would have a curve that goes straight up to the
top-left corner (100% TPR, 0% FPR). The diagonal line represents a
random guess. The <strong>Area Under the Curve (AUC)</strong> summarizes
the model‚Äôs performance; a value closer to 1.0 is better.</li>
<li><strong>Y ËΩ¥</strong>
Ë°®Á§∫<strong>ÁúüÈò≥ÊÄßÁéá</strong>ÔºàÊïèÊÑüÂ∫¶ÔºâÔºö‚ÄúÂú®ÊâÄÊúâÂÆûÈôÖÁöÑÈò≥ÊÄßÊ†∑Êú¨‰∏≠ÔºåÊàë‰ª¨Ê≠£Á°ÆËØÜÂà´‰∫ÜÂ§öÂ∞ë‰∏™Ôºü‚Äù</li>
<li><strong>X ËΩ¥</strong>
Ë°®Á§∫<strong>ÂÅáÈò≥ÊÄßÁéá</strong>Ôºö‚ÄúÂú®ÊâÄÊúâÂÆûÈôÖÁöÑÈò¥ÊÄßÊ†∑Êú¨‰∏≠ÔºåÊàë‰ª¨ÈîôËØØÂú∞Â∞ÜÂ§öÂ∞ë‰∏™Ê†áËÆ∞‰∏∫Èò≥ÊÄßÔºü‚Äù</li>
<li>‰∏Ä‰∏™ÂÆåÁæéÁöÑÂàÜÁ±ªÂô®Â∫îËØ•Êúâ‰∏ÄÊù°Áõ¥Á∫ø‰∏äÂçáÂà∞Â∑¶‰∏äËßíÁöÑÊõ≤Á∫øÔºàÁúüÈò≥ÊÄßÁéá
100%ÔºåÂÅáÈò≥ÊÄßÁéá 0%Ôºâ„ÄÇÂØπËßíÁ∫øË°®Á§∫ÈöèÊú∫ÁåúÊµã„ÄÇ<strong>Êõ≤Á∫ø‰∏ãÈù¢ÁßØ
(AUC)</strong> Ê¶ÇÊã¨‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩÔºõËØ•ÂÄºË∂äÊé•Ëøë 1.0 Ë∂äÂ•Ω„ÄÇ</li>
</ul>
<h1
id="here-is-a-summary-of-the-provided-slides-on-quadratic-discriminant-analysis-qda-including-the-key-formulas-code-explanations-with-python-equivalents-and-a-guide-to-the-most-important-images.">8.
Here is a summary of the provided slides on Quadratic Discriminant
Analysis (QDA), including the key formulas, code explanations with
Python equivalents, and a guide to the most important images.</h1>
<h3 id="core-concept-qda-vs.-lda">## Core Concept: QDA vs.¬†LDA</h3>
<p>The main difference between <strong>Linear Discriminant Analysis
(LDA)</strong> and <strong>Quadratic Discriminant Analysis
(QDA)</strong> lies in their assumptions about the data.
<strong>Á∫øÊÄßÂà§Âà´ÂàÜÊûê (LDA)</strong> Âíå <strong>‰∫åÊ¨°Âà§Âà´ÂàÜÊûê
(QDA)</strong> ÁöÑ‰∏ªË¶ÅÂå∫Âà´Âú®‰∫éÂÆÉ‰ª¨ÂØπÊï∞ÊçÆÁöÑÂÅáËÆæ„ÄÇ * <strong>LDA</strong>
assumes that all classes share the <strong>same covariance
matrix</strong> (<span class="math inline">\(\Sigma\)</span>). It models
each class as a normal distribution with a different mean (<span
class="math inline">\(\mu_k\)</span>) but the same shape and
orientation. This results in a <em>linear</em> decision boundary between
classes. ÂÅáËÆæÊâÄÊúâÁ±ªÂà´ÂÖ±‰∫´<strong>Áõ∏ÂêåÁöÑÂçèÊñπÂ∑ÆÁü©Èòµ</strong> (<span
class="math inline">\(\Sigma\)</span>)„ÄÇÂÆÉÂ∞ÜÊØè‰∏™Á±ªÂà´Âª∫Ê®°‰∏∫ÂùáÂÄº‰∏çÂêå
(<span class="math inline">\(\mu_k\)</span>)
‰ΩÜÂΩ¢Áä∂ÂíåÊñπÂêëÁõ∏ÂêåÁöÑÊ≠£ÊÄÅÂàÜÂ∏É„ÄÇËøô‰ºöÂØºËá¥Á±ªÂà´‰πãÈó¥Âá∫Áé∞ <em>Á∫øÊÄß</em>
ÂÜ≥Á≠ñËæπÁïå„ÄÇ * <strong>QDA</strong> is more flexible. It assumes that each
class <span class="math inline">\(k\)</span> has its <strong>own,
separate covariance matrix</strong> (<span
class="math inline">\(\Sigma_k\)</span>). This allows each class‚Äôs
distribution to have a unique shape, size, and orientation. This
flexibility results in a <em>quadratic</em> decision boundary (like a
parabola, hyperbola, or ellipse). Êõ¥ÁÅµÊ¥ª„ÄÇÂÆÉÂÅáËÆæÊØè‰∏™Á±ªÂà´ <span
class="math inline">\(k\)</span> ÈÉΩÊúâÂÖ∂<strong>Áã¨Á´ãÁöÑÂçèÊñπÂ∑ÆÁü©Èòµ</strong>
(<span
class="math inline">\(\Sigma_k\)</span>)„ÄÇËøô‰ΩøÂæóÊØè‰∏™Á±ªÂà´ÁöÑÂàÜÂ∏ÉÂÖ∑ÊúâÁã¨ÁâπÁöÑÂΩ¢Áä∂„ÄÅÂ§ßÂ∞èÂíåÊñπÂêë„ÄÇËøôÁßçÁÅµÊ¥ªÊÄßÂØºËá¥‰∫Ü<em>‰∫åÊ¨°</em>ÂÜ≥Á≠ñËæπÁïåÔºàÁ±ª‰ºº‰∫éÊäõÁâ©Á∫ø„ÄÅÂèåÊõ≤Á∫øÊàñÊ§≠ÂúÜÔºâ„ÄÇ
<strong>Analogy</strong> üí°: Imagine you‚Äôre drawing boundaries around
different clusters of stars. LDA gives you only straight lines to
separate the clusters. QDA gives you curved lines (circles, ellipses),
which can create a much better fit if the clusters themselves are
elliptical and point in different directions.
ÊÉ≥Ë±°‰∏Ä‰∏ãÔºå‰Ω†Ê≠£Âú®Âõ¥Áªï‰∏çÂêåÁöÑÊòüÂõ¢ÁªòÂà∂ËæπÁïå„ÄÇLDA Âè™Êèê‰æõÁõ¥Á∫øÊù•ÂàÜÈöîÊòüÂõ¢„ÄÇQDA
Êèê‰æõÊõ≤Á∫øÔºàÂúÜÂΩ¢„ÄÅÊ§≠ÂúÜÂΩ¢ÔºâÔºåÂ¶ÇÊûúÊòüÂõ¢Êú¨Ë∫´ÊòØÊ§≠ÂúÜÂΩ¢‰∏îÊåáÂêë‰∏çÂêåÁöÑÊñπÂêëÔºåÂàôÂèØ‰ª•‰∫ßÁîüÊõ¥Â•ΩÁöÑÊãüÂêàÊïàÊûú„ÄÇ</p>
<h3 id="the-math-behind-qda">## The Math Behind QDA</h3>
<p>QDA classifies a new observation <span
class="math inline">\(x\)</span> to the class <span
class="math inline">\(k\)</span> that has the highest discriminant
score, <span class="math inline">\(\delta_k(x)\)</span>. The formula for
this score is what makes the boundary quadratic. QDA Â∞ÜÊñ∞ÁöÑËßÇÊµãÂÄº <span
class="math inline">\(x\)</span> ÂΩíÁ±ªÂà∞ÂÖ∑ÊúâÊúÄÈ´òÂà§Âà´ÂàÜÊï∞ <span
class="math inline">\(\delta_k(x)\)</span> ÁöÑÁ±ª <span
class="math inline">\(k\)</span>
‰∏≠„ÄÇËØ•ÂàÜÊï∞ÁöÑÂÖ¨Âºè‰ΩøÂæóËæπÁïåÂÖ∑Êúâ‰∫åÊ¨°È°π„ÄÇ</p>
<p>The discriminant function for class <span
class="math inline">\(k\)</span> is: <span
class="math display">\[\delta_k(x) = -\frac{1}{2}(x - \mu_k)^T
\Sigma_k^{-1}(x - \mu_k) - \frac{1}{2}\log(|\Sigma_k|) +
\log(\pi_k)\]</span></p>
<p>Let‚Äôs break it down:</p>
<ul>
<li><span class="math inline">\((x - \mu_k)^T \Sigma_k^{-1}(x -
\mu_k)\)</span>: This is a quadratic term (since it involves <span
class="math inline">\(x^T \Sigma_k^{-1} x\)</span>). It measures the
squared Mahalanobis distance from <span class="math inline">\(x\)</span>
to the class mean <span class="math inline">\(\mu_k\)</span>, scaled by
that class‚Äôs specific covariance <span
class="math inline">\(\Sigma_k\)</span>.</li>
<li><span class="math inline">\(\log(|\Sigma_k|)\)</span>: A term that
penalizes classes with larger variance.</li>
<li><span class="math inline">\(\log(\pi_k)\)</span>: The prior
probability of class <span class="math inline">\(k\)</span>. This is our
initial belief about how likely class <span
class="math inline">\(k\)</span> is, before seeing the data.
<ul>
<li><span class="math inline">\((x - \mu_k)^T \Sigma_k^{-1}(x -
\mu_k)\)</span>ÔºöËøôÊòØ‰∏Ä‰∏™‰∫åÊ¨°È°πÔºàÂõ†‰∏∫ÂÆÉÊ∂âÂèä <span
class="math inline">\(x^T \Sigma_k^{-1} x\)</span>Ôºâ„ÄÇÂÆÉÊµãÈáè‰ªé <span
class="math inline">\(x\)</span> Âà∞Á±ªÂùáÂÄº <span
class="math inline">\(\mu_k\)</span>
ÁöÑÂπ≥ÊñπÈ©¨Ê∞èË∑ùÁ¶ªÔºåÂπ∂Ê†πÊçÆËØ•Á±ªÁöÑÁâπÂÆöÂçèÊñπÂ∑Æ <span
class="math inline">\(\Sigma_k\)</span> ËøõË°åÁº©Êîæ„ÄÇ</li>
<li><span
class="math inline">\(\log(|\Sigma_k|)\)</span>ÔºöÁî®‰∫éÊÉ©ÁΩöÊñπÂ∑ÆËæÉÂ§ßÁöÑÁ±ªÁöÑÈ°π„ÄÇ</li>
<li><span class="math inline">\(\log(\pi_k)\)</span>ÔºöÁ±ª <span
class="math inline">\(k\)</span> ÁöÑÂÖàÈ™åÊ¶ÇÁéá„ÄÇËøôÊòØÊàë‰ª¨Âú®ÁúãÂà∞Êï∞ÊçÆ‰πãÂâçÂØπÁ±ª
<span class="math inline">\(k\)</span> ÂèØËÉΩÊÄßÁöÑÂàùÂßã‰ø°Âøµ„ÄÇ Because each
class <span class="math inline">\(k\)</span> has its own <span
class="math inline">\(\Sigma_k\)</span>, the quadratic term doesn‚Äôt
cancel out when comparing scores between classes, leading to a quadratic
boundary. Áî±‰∫éÊØè‰∏™Á±ª <span class="math inline">\(k\)</span> ÈÉΩÊúâÂÖ∂Ëá™Â∑±ÁöÑ
<span
class="math inline">\(\Sigma_k\)</span>ÔºåÂõ†Ê≠§Âú®ÊØîËæÉÁ±ª‰πãÈó¥ÁöÑÂàÜÊï∞Êó∂Ôºå‰∫åÊ¨°È°π‰∏ç‰ºöÊäµÊ∂àÔºå‰ªéËÄåÂØºËá¥‰∫åÊ¨°ËæπÁïå„ÄÇ
<strong>Key Trade-off</strong>:</li>
</ul></li>
<li>If the class variances (<span
class="math inline">\(\Sigma_k\)</span>) are truly different,
<strong>QDA is better</strong>.</li>
<li>If the class variances are similar, <strong>LDA is often
better</strong> because it‚Äôs less flexible and less likely to overfit,
especially with a small number of training samples.</li>
<li>Â¶ÇÊûúÁ±ªÊñπÂ∑Æ (<span class="math inline">\(\Sigma_k\)</span>)
Á°ÆÂÆû‰∏çÂêåÔºå<strong>QDA Êõ¥Â•Ω</strong>„ÄÇ</li>
<li>Â¶ÇÊûúÁ±ªÊñπÂ∑ÆÁõ∏‰ººÔºå<strong>LDA
ÈÄöÂ∏∏Êõ¥Â•Ω</strong>ÔºåÂõ†‰∏∫ÂÆÉÁöÑÁÅµÊ¥ªÊÄßËæÉÂ∑ÆÔºåÂπ∂‰∏î‰∏çÂ§™ÂèØËÉΩËøáÊãüÂêàÔºåÂ∞§ÂÖ∂ÊòØÂú®ËÆ≠ÁªÉÊ†∑Êú¨Êï∞ÈáèËæÉÂ∞ëÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ</li>
</ul>
<h3 id="code-implementation-r-and-python">## Code Implementation: R and
Python</h3>
<p>The slides provide R code for fitting a QDA model and evaluating it.
Below is an explanation of the R code and its equivalent in Python using
the popular <code>scikit-learn</code> library.</p>
<h4 id="r-code-from-the-slides">R Code (from the slides)</h4>
<p>The code uses the <code>MASS</code> library for QDA and the
<code>ROCR</code> library for evaluation.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ######## QDA ##########</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Fit the model on the training data</span></span><br><span class="line"><span class="comment"># This formula `Default~.` means &quot;predict &#x27;Default&#x27; using all other variables&quot;.</span></span><br><span class="line">qda.fit.mod2 <span class="operator">&lt;-</span> qda<span class="punctuation">(</span>Default<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> subset<span class="operator">=</span>train.ids<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Make predictions on the test data</span></span><br><span class="line"><span class="comment"># We are interested in the posterior probabilities for the ROC curve</span></span><br><span class="line">qda.fit.pred3 <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>qda.fit.mod2<span class="punctuation">,</span> Default_test<span class="punctuation">)</span><span class="operator">$</span>posterior<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Evaluate using ROC and AUC</span></span><br><span class="line"><span class="comment"># &#x27;prediction&#x27; and &#x27;performance&#x27; are functions from the ROCR library</span></span><br><span class="line">perf <span class="operator">&lt;-</span> performance<span class="punctuation">(</span>prediction<span class="punctuation">(</span>qda.fit.pred3<span class="punctuation">,</span> Default_test<span class="operator">$</span>Default<span class="punctuation">)</span><span class="punctuation">,</span> <span class="string">&quot;auc&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Get the AUC value</span></span><br><span class="line">auc_value <span class="operator">&lt;-</span> perf<span class="operator">@</span>y.values<span class="punctuation">[[</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">]</span></span><br><span class="line"><span class="comment"># Result from slide: 0.9638683</span></span><br></pre></td></tr></table></figure>
<h4 id="python-equivalent-scikit-learn">Python Equivalent
(<code>scikit-learn</code>)</h4>
<p>Here‚Äôs how you would perform the same steps in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> QuadraticDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score, roc_curve</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Default&#x27; is your DataFrame and &#x27;default&#x27; is the target column</span></span><br><span class="line"><span class="comment"># (preprocessing &#x27;student&#x27; and &#x27;default&#x27; columns to numbers)</span></span><br><span class="line"><span class="comment"># Default[&#x27;default_num&#x27;] = Default[&#x27;default&#x27;].apply(lambda x: 1 if x == &#x27;Yes&#x27; else 0)</span></span><br><span class="line"><span class="comment"># X = Default[[&#x27;balance&#x27;, &#x27;income&#x27;, ...]]</span></span><br><span class="line"><span class="comment"># y = Default[&#x27;default_num&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Initialize and fit the QDA model</span></span><br><span class="line">qda = QuadraticDiscriminantAnalysis()</span><br><span class="line">qda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Predict probabilities on the test set</span></span><br><span class="line"><span class="comment"># We need the probability of the positive class (&#x27;Yes&#x27;) for the AUC calculation</span></span><br><span class="line">y_pred_proba = qda.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Calculate the AUC score</span></span><br><span class="line">auc_score = roc_auc_score(y_test, y_pred_proba)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;AUC Score for QDA: <span class="subst">&#123;auc_score:<span class="number">.7</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also plot the ROC curve</span></span><br><span class="line"><span class="comment"># fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)</span></span><br><span class="line"><span class="comment"># plt.plot(fpr, tpr)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure>
<h3 id="model-evaluation-roc-and-auc">## Model Evaluation: ROC and
AUC</h3>
<p>The slides correctly emphasize using the <strong>ROC curve</strong>
and the <strong>Area Under the Curve (AUC)</strong> to compare model
performance.</p>
<ul>
<li><p><strong>ROC Curve (Receiver Operating Characteristic)</strong>:
This plot shows how well a model can distinguish between two classes. It
plots the <strong>True Positive Rate</strong> (y-axis) against the
<strong>False Positive Rate</strong> (x-axis) at all possible
classification thresholds. A better model has a curve that is closer to
the top-left corner.</p></li>
<li><p><strong>AUC (Area Under the Curve)</strong>: This is a single
number that summarizes the entire ROC curve.</p>
<ul>
<li><strong>AUC = 1</strong>: Perfect classifier.</li>
<li><strong>AUC = 0.5</strong>: A useless classifier (equivalent to
random guessing).</li>
<li><strong>AUC &gt; 0.7</strong>: Generally considered an acceptable
model.</li>
</ul></li>
<li><p><strong>ROC
Êõ≤Á∫øÔºàÊé•Êî∂ËÄÖÊìç‰ΩúÁâπÂæÅÔºâ</strong>ÔºöÊ≠§ÂõæÊòæÁ§∫‰∫ÜÊ®°ÂûãÂå∫ÂàÜ‰∏§‰∏™Á±ªÂà´ÁöÑËÉΩÂäõ„ÄÇÂÆÉÁªòÂà∂‰∫ÜÊâÄÊúâÂèØËÉΩÁöÑÂàÜÁ±ªÈòàÂÄº‰∏ãÁöÑ
<strong>ÁúüÈò≥ÊÄßÁéá</strong>Ôºày ËΩ¥Ôºâ‰∏é <strong>ÂÅáÈò≥ÊÄßÁéá</strong>Ôºàx
ËΩ¥ÔºâÁöÑÂØπÊØîÂõæ„ÄÇÊõ¥Â•ΩÁöÑÊ®°ÂûãÁöÑÊõ≤Á∫øË∂äÈù†ËøëÂ∑¶‰∏äËßíÔºåÊïàÊûúÂ∞±Ë∂äÂ•Ω„ÄÇ</p>
<ul>
<li><p><strong>AUCÔºàÊõ≤Á∫ø‰∏ãÈù¢ÁßØÔºâ</strong>ÔºöËøôÊòØ‰∏Ä‰∏™Ê¶ÇÊã¨Êï¥‰∏™ ROC
Êõ≤Á∫øÁöÑÊï∞ÂÄº„ÄÇ</p></li>
<li><p><strong>AUC = 1</strong>ÔºöÂÆåÁæéÁöÑÂàÜÁ±ªÂô®„ÄÇ</p></li>
<li><p><strong>AUC =
0.5</strong>ÔºöÊó†Áî®ÁöÑÂàÜÁ±ªÂô®ÔºàÁõ∏ÂΩì‰∫éÈöèÊú∫ÁåúÊµãÔºâ„ÄÇ</p></li>
<li><p><strong>AUC &gt;
0.7</strong>ÔºöÈÄöÂ∏∏Ë¢´ËÆ§‰∏∫ÊòØÂèØÊé•ÂèóÁöÑÊ®°Âûã„ÄÇ</p></li>
</ul></li>
</ul>
<p>The slides show that for the <code>Default</code> dataset,
<strong>LDA‚Äôs AUC (0.9647) was slightly higher than QDA‚Äôs
(0.9639)</strong>. This suggests that the assumption of a common
covariance matrix (LDA) was a slightly better fit for this particular
test set, possibly because QDA‚Äôs extra flexibility wasn‚Äôt needed and it
may have slightly overfit the training data.
ËøôË°®ÊòéÔºåÂØπ‰∫éËøô‰∏™ÁâπÂÆöÁöÑÊµãËØïÈõÜÔºåÂÖ¨ÂÖ±ÂçèÊñπÂ∑ÆÁü©Èòµ (LDA)
ÁöÑÂÅáËÆæÊãüÂêàÂ∫¶Áï•È´òÔºåÂèØËÉΩÊòØÂõ†‰∏∫ QDA
ÁöÑÈ¢ùÂ§ñÁÅµÊ¥ªÊÄßÂπ∂ÈùûÂøÖÈúÄÔºåÂπ∂‰∏îÂèØËÉΩÂØπËÆ≠ÁªÉÊï∞ÊçÆÁï•ÂæÆËøáÊãüÂêà„ÄÇ</p>
<h3 id="key-takeaways-and-important-images">## Key Takeaways and
Important Images</h3>
<h3
id="heres-a-ranking-of-the-most-important-visual-aids-in-your-slides">Here‚Äôs
a ranking of the most important visual aids in your slides:</h3>
<ol type="1">
<li><p><strong>Slide 68/69 (Model Assumption &amp; Formula)</strong>:
These are the <strong>most critical</strong> slides. They present the
core theoretical difference between LDA and QDA and provide the
mathematical foundation (the discriminant function formula).
Understanding these is key to understanding QDA.</p></li>
<li><p><strong>Slide 73 (ROC Comparison)</strong>: This is the most
important image for <strong>practical evaluation</strong>. It visually
compares the performance of LDA and QDA side-by-side, making it easy to
see which one performs better on this specific dataset. The concept of
AUC is introduced here as the method for comparison.</p></li>
<li><p><strong>Slide 71 (Decision Boundaries with Different
Thresholds)</strong>: This is an excellent conceptual image. It shows
how the quadratic decision boundary (the curved lines) separates the
data points. It also illustrates how changing the probability threshold
(from 0.1 to 0.5 to 0.9) shifts the boundary, trading off between
precision and recall.</p></li>
</ol>
<p>Of course. Here is a summary of the remaining slides, which compare
QDA to other popular classification models like Logistic Regression and
K-Nearest Neighbors (KNN).</p>
<hr />
<h3 id="visualizing-the-core-trade-off-lda-vs.-qda">Visualizing the Core
Trade-off: LDA vs.¬†QDA</h3>
<p>This is the most important concept in these slides. The choice
between LDA and QDA depends entirely on the underlying structure of your
data.</p>
<p>The slide shows two scenarios: 1. <strong>Left Plot (<span
class="math inline">\(\Sigma_1 = \Sigma_2\)</span>):</strong> When the
true covariance matrices of the classes are the same, the optimal
decision boundary (the Bayes classifier) is a straight line. LDA, which
assumes equal covariances, creates a linear boundary that approximates
this optimal boundary very well. QDA‚Äôs flexible, curved boundary is
unnecessarily complex and might overfit the training data. <strong>In
this case, LDA is better.</strong> 2. <strong>Right Plot (<span
class="math inline">\(\Sigma_1 \neq \Sigma_2\)</span>):</strong> When
the true covariance matrices are different, the optimal decision
boundary is a curve. QDA‚Äôs quadratic model can capture this
non-linearity much better than LDA‚Äôs rigid linear model. <strong>In this
case, QDA is better.</strong></p>
<p>This perfectly illustrates the <strong>bias-variance
tradeoff</strong>. LDA has higher bias (it‚Äôs less flexible) but lower
variance. QDA has lower bias (it‚Äôs more flexible) but higher
variance.</p>
<hr />
<h3 id="comparing-performance-on-the-default-dataset">Comparing
Performance on the ‚ÄúDefault‚Äù Dataset</h3>
<p>The slides compare four different models on the same classification
task. Let‚Äôs look at their performance using the <strong>Area Under the
Curve (AUC)</strong>, where a higher score is better.</p>
<ul>
<li><strong>LDA AUC:</strong> 0.9647</li>
<li><strong>QDA AUC:</strong> 0.9639</li>
<li><strong>Logistic Regression AUC:</strong> 0.9645</li>
<li><strong>K-Nearest Neighbors (KNN):</strong> The plot shows test
error vs.¬†K. The error is lowest around K=4, but it‚Äôs not directly
converted to an AUC score in the slides.</li>
</ul>
<p>Interestingly, for this particular dataset, LDA, QDA, and Logistic
Regression perform almost identically. This suggests that the decision
boundary for this problem is likely very close to linear, meaning the
extra flexibility of QDA isn‚Äôt providing much benefit.</p>
<hr />
<h3 id="pros-and-cons-which-model-to-choose">Pros and Cons: Which Model
to Choose?</h3>
<p>The final slide asks for a comparison of the models. Here‚Äôs a summary
of their key characteristics:</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Decision Boundary</th>
<th style="text-align: left;">Key Pro</th>
<th style="text-align: left;">Key Con</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Logistic Regression</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">Highly interpretable, no strong
assumptions about data distribution.</td>
<td style="text-align: left;">Inflexible; cannot capture non-linear
relationships.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Linear Discriminant Analysis
(LDA)</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">More stable than Logistic Regression when
classes are well-separated.</td>
<td style="text-align: left;">Assumes data is normally distributed with
equal covariance matrices for all classes.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Quadratic Discriminant Analysis
(QDA)</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Quadratic (Curved)</td>
<td style="text-align: left;">More flexible than LDA; can model
non-linear boundaries.</td>
<td style="text-align: left;">Requires more data to estimate parameters
and is more prone to overfitting. Assumes normality.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>K-Nearest Neighbors
(KNN)</strong></td>
<td style="text-align: left;">Non-Parametric</td>
<td style="text-align: left;">Highly Non-linear</td>
<td style="text-align: left;">Extremely flexible; makes no assumptions
about the data‚Äôs distribution.</td>
<td style="text-align: left;">Can be slow on large datasets and suffers
from the ‚Äúcurse of dimensionality.‚Äù Less interpretable.</td>
</tr>
</tbody>
</table>
<h4 id="summary-of-the-comparison">Summary of the Comparison:</h4>
<ul>
<li><strong>Linear Models (Logistic Regression &amp; LDA):</strong>
Choose these for simplicity, interpretability, and when you believe the
relationship between predictors and the class is linear. LDA often
outperforms Logistic Regression if its normality assumptions are
met.</li>
<li><strong>Non-Linear Models (QDA &amp; KNN):</strong> Choose these
when the decision boundary is likely more complex. QDA is a good middle
ground, offering more flexibility than LDA without being as completely
data-driven as KNN. KNN is the most flexible but requires careful tuning
of the parameter K to avoid overfitting or underfitting.</li>
</ul>
<h1
id="here-is-a-more-detailed-slide-by-slide-analysis-of-the-presentation.">9.
Here is a more detailed, slide-by-slide analysis of the
presentation.</h1>
<h2 id="four-classification-methods-comparison-by-simulation">4.6 Four
Classification Methods: Comparison by Simulation</h2>
<p>This section (slides 81-87) introduces four classification methods
and systematically compares their performance on six different simulated
datasets. The goal is to see which method works best under different
conditions (e.g., linear vs.¬†non-linear boundaries, normal
vs.¬†non-normal data).</p>
<p>The four methods being compared are: * <strong>Logistic
Regression:</strong> A linear method that models the log-odds as a
linear function of the predictors. * <strong>Linear Discriminant
Analysis (LDA):</strong> Another linear method. It also assumes a linear
decision boundary but makes stronger assumptions than logistic
regression (e.g., that data within each class is normally distributed
with a common covariance matrix). * <strong>Quadratic Discriminant
Analysis (QDA):</strong> A non-linear method. It assumes the log-odds
are a <em>quadratic</em> function, which creates a more flexible, curved
decision boundary. It assumes data within each class is normally
distributed, but <em>without</em> a common covariance matrix. *
<strong>K-Nearest Neighbors (KNN):</strong> A non-parametric, highly
flexible method. Two versions are tested: * <strong>KNN-1 (<span
class="math inline">\(K=1\)</span>):</strong> A very flexible (high
variance) model. * <strong>KNN-CV:</strong> A tuned model where the best
<span class="math inline">\(K\)</span> is chosen via
cross-validation.</p>
<p>ÊØîËæÉÁöÑÂõõÁßçÊñπÊ≥ïÊòØÔºö *
<strong>ÈÄªËæëÂõûÂΩí</strong>Ôºö‰∏ÄÁßçÂ∞ÜÂØπÊï∞Ê¶ÇÁéáÂª∫Ê®°‰∏∫È¢ÑÊµãÂèòÈáèÁ∫øÊÄßÂáΩÊï∞ÁöÑÁ∫øÊÄßÊñπÊ≥ï„ÄÇ
* <strong>Á∫øÊÄßÂà§Âà´ÂàÜÊûê
(LDA)</strong>ÔºöÂè¶‰∏ÄÁßçÁ∫øÊÄßÊñπÊ≥ï„ÄÇÂÆÉ‰πüÂÅáËÆæÁ∫øÊÄßÂÜ≥Á≠ñËæπÁïåÔºå‰ΩÜÊØîÈÄªËæëÂõûÂΩíÂÅöÂá∫Êõ¥Âº∫ÁöÑÂÅáËÆæÔºà‰æãÂ¶ÇÔºåÊØè‰∏™Á±ª‰∏≠ÁöÑÊï∞ÊçÆÂëàÊ≠£ÊÄÅÂàÜÂ∏ÉÔºå‰∏îÂÖ∑ÊúâÂÖ±ÂêåÁöÑÂçèÊñπÂ∑ÆÁü©ÈòµÔºâ„ÄÇ
* <strong>‰∫åÊ¨°Âà§Âà´ÂàÜÊûê
(QDA)</strong>Ôºö‰∏ÄÁßçÈùûÁ∫øÊÄßÊñπÊ≥ï„ÄÇÂÆÉÂÅáËÆæÂØπÊï∞Ê¶ÇÁéá‰∏∫<em>‰∫åÊ¨°</em>ÂáΩÊï∞Ôºå‰ªéËÄåÂàõÂª∫‰∏Ä‰∏™Êõ¥ÁÅµÊ¥ª„ÄÅÊõ¥ÂºØÊõ≤ÁöÑÂÜ≥Á≠ñËæπÁïå„ÄÇÂÆÉÂÅáËÆæÊØè‰∏™Á±ª‰∏≠ÁöÑÊï∞ÊçÆÊúç‰ªéÊ≠£ÊÄÅÂàÜÂ∏ÉÔºå‰ΩÜ<em>Ê≤°Êúâ</em>ÂÖ±ÂêåÁöÑÂçèÊñπÂ∑ÆÁü©Èòµ„ÄÇ
* <strong>KÊúÄËøëÈÇª
(KNN)</strong>Ôºö‰∏ÄÁßçÈùûÂèÇÊï∞Âåñ„ÄÅÈ´òÂ∫¶ÁÅµÊ¥ªÁöÑÊñπÊ≥ï„ÄÇÊµãËØï‰∫Ü‰∏§‰∏™ÁâàÊú¨Ôºö *
<strong>KNN-1 (<span
class="math inline">\(K=1\)</span>)</strong>Ôºö‰∏Ä‰∏™ÈùûÂ∏∏ÁÅµÊ¥ªÔºàÈ´òÊñπÂ∑ÆÔºâÁöÑÊ®°Âûã„ÄÇ
*
<strong>KNN-CV</strong>Ôºö‰∏Ä‰∏™ÁªèËøáË∞ÉÊï¥ÁöÑÊ®°ÂûãÔºåÈÄöËøá‰∫§ÂèâÈ™åËØÅÈÄâÊã©ÊúÄ‰Ω≥ÁöÑ<span
class="math inline">\(K\)</span>„ÄÇ</p>
<h3 id="analysis-of-simulation-scenarios">Analysis of Simulation
Scenarios</h3>
<p>The performance is measured by the <strong>test error rate</strong>
(lower is better), shown in the boxplots for each scenario.
ÊÄßËÉΩÈÄöËøá<strong>ÊµãËØïÈîôËØØÁéá</strong>ÔºàË∂ä‰ΩéË∂äÂ•ΩÔºâÊù•Ë°°ÈáèÔºåÊØè‰∏™Âú∫ÊôØÁöÑÁÆ±Á∫øÂõæÈÉΩÊòæÁ§∫‰∫ÜËØ•ÈîôËØØÁéá„ÄÇ</p>
<ul>
<li><strong>Scenario 1 (Slide 82):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>linear</strong> decision boundary.
Data is <strong>normally distributed</strong> with <em>uncorrelated</em>
predictors.</li>
<li><strong>Result:</strong> <strong>LDA and Logistic Regression perform
best</strong>. Their test error rates are low and similar. This is
expected, as the setup perfectly matches their core assumption (linear
boundary). QDA is slightly worse because its extra flexibility (being
quadratic) is unnecessary. KNN-1 is the worst, as its high flexibility
leads to high variance (overfitting).</li>
<li><strong>ÁªìÊûúÔºö</strong> <strong>LDA
ÂíåÈÄªËæëÂõûÂΩíË°®Áé∞ÊúÄ‰Ω≥</strong>„ÄÇÂÆÉ‰ª¨ÁöÑÊµãËØïÈîôËØØÁéáËæÉ‰Ωé‰∏îÁõ∏‰ºº„ÄÇËøôÊòØÊÑèÊñô‰πã‰∏≠ÁöÑÔºåÂõ†‰∏∫ËÆæÁΩÆÂÆåÂÖ®Á¨¶ÂêàÂÆÉ‰ª¨ÁöÑÊ†∏ÂøÉÂÅáËÆæÔºàÁ∫øÊÄßËæπÁïåÔºâ„ÄÇQDA
Áï•Â∑ÆÔºåÂõ†‰∏∫ÂÖ∂È¢ùÂ§ñÁöÑÁÅµÊ¥ªÊÄßÔºà‰∫åÊ¨°ÊñπÔºâÊòØ‰∏çÂøÖË¶ÅÁöÑ„ÄÇKNN-1
ÊúÄÂ∑ÆÔºåÂõ†‰∏∫ÂÖ∂È´òÁÅµÊ¥ªÊÄßÂØºËá¥ÊñπÂ∑ÆËæÉÂ§ßÔºàËøáÊãüÂêàÔºâ„ÄÇ</li>
</ul></li>
<li><strong>Scenario 2 (Slide 83):</strong>
<ul>
<li><strong>Setup:</strong> Same as Scenario 1 (<strong>linear</strong>
boundary, <strong>normal</strong> data), but now the two predictors have
a <strong>correlation of 0.5</strong>.</li>
<li><strong>Result:</strong> <strong>Almost no change</strong> from
Scenario 1. <strong>LDA and Logistic Regression are still the
best</strong>. This shows that these linear methods are robust to
correlation between predictors.</li>
<li><strong>ÁªìÊûúÔºö</strong>‰∏éÂú∫ÊôØ 1
Áõ∏ÊØî<strong>Âá†‰πéÊ≤°ÊúâÂèòÂåñ</strong>„ÄÇ<strong>LDA
ÂíåÈÄªËæëÂõûÂΩí‰ªçÁÑ∂ÊòØÊúÄ‰Ω≥</strong>„ÄÇËøôË°®ÊòéËøô‰∫õÁ∫øÊÄßÊñπÊ≥ïÂØπÈ¢ÑÊµãÂõ†Â≠ê‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄßÂÖ∑ÊúâÈ≤ÅÊ£íÊÄß„ÄÇ</li>
</ul></li>
<li><strong>Scenario 3 (Slide 84):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>linear</strong> decision boundary,
but the data is drawn from a <strong>t-distribution</strong> (which is
non-normal and has ‚Äúheavy tails,‚Äù or more extreme outliers).</li>
<li><strong>Result:</strong> <strong>Logistic Regression is the clear
winner</strong>. LDA‚Äôs performance gets worse because its assumption of
<em>normality</em> is violated by the t-distribution. QDA‚Äôs performance
deteriorates significantly due to the non-normality. This highlights a
key difference: logistic regression is more robust to violations of the
normality assumption.</li>
<li><strong>ÁªìÊûúÔºö</strong>ÈÄªËæëÂõûÂΩíÊòéÊòæËÉúÂá∫**„ÄÇLDA ÁöÑÊÄßËÉΩ‰ºöÂèòÂ∑ÆÔºåÂõ†‰∏∫ t
ÂàÜÂ∏ÉËøùÂèç‰∫ÜÂÖ∂Ê≠£ÊÄÅÊÄßÂÅáËÆæ„ÄÇQDA
ÁöÑÊÄßËÉΩÁî±‰∫éÈùûÊ≠£ÊÄÅÊÄßËÄåÊòæËëó‰∏ãÈôç„ÄÇËøôÂá∏Êòæ‰∫Ü‰∏Ä‰∏™ÂÖ≥ÈîÆÂå∫Âà´ÔºöÈÄªËæëÂõûÂΩíÂØπËøùÂèçÊ≠£ÊÄÅÊÄßÂÅáËÆæÁöÑÊÉÖÂÜµÊõ¥Á®≥ÂÅ•„ÄÇ</li>
</ul></li>
<li><strong>Scenario 4 (Slide 85):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>quadratic</strong> decision
boundary. Data is <strong>normally distributed</strong> with different
correlations in each class.</li>
<li><strong>Result:</strong> <strong>QDA is the clear winner</strong> by
a large margin. This setup perfectly matches QDA‚Äôs assumption (quadratic
boundary from normal data with different covariance structures). All
other methods (LDA, Logistic, KNN) are linear or not flexible enough, so
they perform poorly.</li>
<li><strong>ÁªìÊûúÔºö</strong>QDA ÊòéÊòæËÉúÂá∫**Ôºå‰∏îÈÅ•ÈÅ•È¢ÜÂÖà„ÄÇÊ≠§ËÆæÁΩÆÂÆåÂÖ®Á¨¶Âêà
QDA
ÁöÑÂÅáËÆæÔºàÊù•Ëá™ÂÖ∑Êúâ‰∏çÂêåÂçèÊñπÂ∑ÆÁªìÊûÑÁöÑÊ≠£ÊÄÅÊï∞ÊçÆÁöÑ‰∫åÊ¨°ËæπÁïåÔºâ„ÄÇÊâÄÊúâÂÖ∂‰ªñÊñπÊ≥ïÔºàLDA„ÄÅLogistic„ÄÅKNNÔºâÈÉΩÊòØÁ∫øÊÄßÁöÑÊàñ‰∏çÂ§üÁÅµÊ¥ªÔºåÂõ†Ê≠§ÊÄßËÉΩ‰∏ç‰Ω≥„ÄÇ</li>
</ul></li>
<li><strong>Scenario 5 (Slide 86):</strong>
<ul>
<li><strong>Setup:</strong> Another <strong>quadratic</strong> boundary,
but generated in a different way (using a logistic function of quadratic
terms).</li>
<li><strong>Result:</strong> <strong>QDA performs best again</strong>,
closely followed by the flexible <strong>KNN-CV</strong>. The linear
methods (LDA, Logistic) have poor performance because they cannot
capture the curve.</li>
<li><strong>ÁªìÊûúÔºöQDA
ÂÜçÊ¨°Ë°®Áé∞ÊúÄ‰Ω≥</strong>ÔºåÁ¥ßÈöèÂÖ∂ÂêéÁöÑÊòØÁÅµÊ¥ªÁöÑ<strong>KNN-CV</strong>„ÄÇÁ∫øÊÄßÊñπÊ≥ïÔºàLDA„ÄÅLogisticÔºâÊÄßËÉΩËæÉÂ∑ÆÔºåÂõ†‰∏∫ÂÆÉ‰ª¨Êó†Ê≥ïÊçïÊçâÊõ≤Á∫ø„ÄÇ</li>
</ul></li>
<li><strong>Scenario 6 (Slide 87):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>complex, non-linear</strong>
decision boundary (more complex than a simple quadratic curve).</li>
<li><strong>Result:</strong> The <strong>flexible KNN-CV method is the
winner</strong>. Its non-parametric nature allows it to approximate the
complex shape. QDA is not flexible <em>enough</em> and performs worse.
This slide highlights the bias-variance trade-off: the overly simple
KNN-1 is the worst, but the <em>tuned</em> KNN-CV is the best.</li>
<li><strong>ÁªìÊûúÔºö</strong>ÁÅµÊ¥ªÁöÑ KNN-CV
ÊñπÊ≥ïËÉúÂá∫**„ÄÇÂÖ∂ÈùûÂèÇÊï∞ÁâπÊÄß‰ΩøÂÖ∂ËÉΩÂ§üËøë‰ººÂ§çÊùÇÁöÑÂΩ¢Áä∂„ÄÇ QDA
‰∏çÂ§üÁÅµÊ¥ªÔºåÊÄßËÉΩËæÉÂ∑Æ„ÄÇËøôÂº†ÂπªÁÅØÁâáÈáçÁÇπ‰ªãÁªç‰∫ÜÂÅèÂ∑Æ-ÊñπÂ∑ÆÊùÉË°°ÔºöËøá‰∫éÁÆÄÂçïÁöÑ KNN-1
ÊúÄÂ∑ÆÔºåËÄå <em>Ë∞ÉÊï¥ÂêéÁöÑ</em> KNN-CV ÊúÄÂ•Ω„ÄÇ</li>
</ul></li>
</ul>
<h2 id="r-example-on-smarket-data">4.7 R Example on Smarket Data</h2>
<p>This section (slides 88-93) applies Logistic Regression and LDA to
the <code>Smarket</code> dataset from the <code>ISLR</code> package to
predict the stock market‚Äôs <code>Direction</code> (Up or Down).
Êú¨ËäÇÔºàÂπªÁÅØÁâá 88-93ÔºâÂ∞ÜÈÄªËæëÂõûÂΩíÂíå LDA
Â∫îÁî®‰∫é‚ÄúISLR‚ÄùÂåÖ‰∏≠ÁöÑ‚ÄúSmarket‚ÄùÊï∞ÊçÆÈõÜÔºå‰ª•È¢ÑÊµãËÇ°Â∏ÇÁöÑ‚ÄúÊñπÂêë‚ÄùÔºà‰∏äÊ∂®Êàñ‰∏ãË∑åÔºâ„ÄÇ
### Data Preparation (Slides 88, 89, 90)</p>
<ol type="1">
<li><strong>Load Data:</strong> The <code>ISLR</code> library is loaded,
and the <code>Smarket</code> dataset is explored. It contains daily
percentage returns (<code>Lag1</code>‚Ä¶<code>Lag5</code> for the previous
5 days, <code>Today</code>), <code>Volume</code>, and the
<code>Year</code>.</li>
<li><strong>Explore Data:</strong> A correlation matrix
(<code>cor(Smarket[,-9])</code>) is computed, and a plot of
<code>Volume</code> over time is generated.</li>
<li><strong>Split Data:</strong> The data is split into a training set
(Years 2001-2004) and a test set (Year 2005).
<ul>
<li><code>train &lt;- (Year&lt;2005)</code></li>
<li><code>Smarket.2005 &lt;- Smarket[!train,]</code></li>
<li><code>Direction.2005 &lt;- Direction[!train]</code></li>
<li>The test set has 252 observations.</li>
</ul></li>
<li><strong>Âä†ËΩΩÊï∞ÊçÆ</strong>ÔºöÂä†ËΩΩ‚ÄúISLR‚ÄùÂ∫ìÔºåÂπ∂Êé¢Á¥¢‚ÄúSmarket‚ÄùÊï∞ÊçÆÈõÜ„ÄÇËØ•Êï∞ÊçÆÈõÜÂåÖÂê´ÊØèÊó•ÁôæÂàÜÊØîÊî∂ÁõäÁéáÔºàÂâç
5 Â§©ÁöÑ‚ÄúLag1‚Äù‚Ä¶‚ÄúLag5‚ÄùÔºå‚Äú‰ªäÊó•‚ÄùÔºâ„ÄÅ‚ÄúÊàê‰∫§Èáè‚ÄùÂíå‚ÄúÂπ¥‰ªΩ‚Äù„ÄÇ</li>
<li><strong>Êé¢Á¥¢Êï∞ÊçÆ</strong>ÔºöËÆ°ÁÆóÁõ∏ÂÖ≥Áü©Èòµ
(<code>cor(Smarket[,-9])</code>)ÔºåÂπ∂ÁîüÊàê‚ÄúÊàê‰∫§Èáè‚ÄùÈöèÊó∂Èó¥ÂèòÂåñÁöÑÂõæË°®„ÄÇ</li>
<li><strong>ÊãÜÂàÜÊï∞ÊçÆ</strong>ÔºöÂ∞ÜÊï∞ÊçÆÊãÜÂàÜ‰∏∫ËÆ≠ÁªÉÈõÜÔºàÂπ¥‰ªΩ
2001-2004ÔºâÂíåÊµãËØïÈõÜÔºàÂπ¥‰ªΩ 2005Ôºâ„ÄÇ
<ul>
<li><code>train &lt;- (Year&lt;2005)</code></li>
<li><code>Smarket.2005 &lt;- Smarket[!train,]</code></li>
<li><code>Direction.2005 &lt;- Direction[!train]</code></li>
<li>ÊµãËØïÈõÜÂåÖÂê´ 252 ‰∏™ËßÇÊµãÂÄº„ÄÇ</li>
</ul></li>
</ol>
<h3 id="model-1-logistic-regression-all-predictors-slide-90">Model 1:
Logistic Regression (All Predictors) (Slide 90)</h3>
<ul>
<li><strong>Model:</strong> A logistic regression model is fit on the
training data using <em>all</em> predictors.
<ul>
<li><code>glm.fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data=Smarket, family=binomial, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> The model is used to predict the
direction for the 2005 test data.
<ul>
<li><code>glm.probs &lt;- predict(glm.fit, Smarket.2005, type="response")</code></li>
<li>A threshold of 0.5 is used to classify: if <span
class="math inline">\(P(\text{Up}) &gt; 0.5\)</span>, predict ‚ÄúUp‚Äù.</li>
</ul></li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.5198 (or <strong>48.0%
accuracy</strong>).</li>
<li><strong>Conclusion:</strong> This is ‚Äúnot good!‚Äù‚Äîit‚Äôs worse than
flipping a coin. This suggests the model is either too complex or the
predictors are not useful.</li>
</ul></li>
</ul>
<h3 id="model-2-logistic-regression-lag1-lag2-slide-91">Model 2:
Logistic Regression (Lag1 &amp; Lag2) (Slide 91)</h3>
<ul>
<li><strong>Model:</strong> Based on the poor results, a simpler model
is tried, using only <code>Lag1</code> and <code>Lag2</code>.
<ul>
<li><code>glm.fit &lt;- glm(Direction ~ Lag1 + Lag2, data=Smarket, family=binomial, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> Predictions are made on the 2005 test
set.</li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.4404 (or <strong>55.95%
accuracy</strong>). This is an improvement.</li>
<li><strong>Confusion Matrix:</strong> | | True Down | True Up | | :‚Äî |
:‚Äî | :‚Äî | | <strong>Pred Down</strong> | 77 | 69 | | <strong>Pred
Up</strong> | 35 | 71 |</li>
<li><strong>ROC and AUC:</strong> The ROC (Receiver Operating
Characteristic) curve is plotted, and the AUC (Area Under the Curve) is
calculated.</li>
<li><strong>AUC Value:</strong> <strong>0.5584</strong>. This is very
close to 0.5 (which represents a random-chance model), indicating that
the model has very weak predictive power, even though its accuracy is
above 50%.</li>
</ul></li>
</ul>
<h3 id="model-3-lda-lag1-lag2-slide-92">Model 3: LDA (Lag1 &amp; Lag2)
(Slide 92)</h3>
<ul>
<li><strong>Model:</strong> LDA is now performed using the same setup:
<code>Lag1</code> and <code>Lag2</code> as predictors, trained on the
2001-2004 data.
<ul>
<li><code>library(MASS)</code></li>
<li><code>lda.fit &lt;- lda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> Predictions are made on the 2005 test
set.
<ul>
<li><code>lda.pred &lt;- predict(lda.fit, Smarket.2005)</code></li>
</ul></li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.4404 (or <strong>55.95%
accuracy</strong>).</li>
<li><strong>Confusion Matrix:</strong> | | True Down | True Up | | :‚Äî |
:‚Äî | :‚Äî | | <strong>Pred Down</strong> | 77 | 69 | | <strong>Pred
Up</strong> | 35 | 71 |</li>
<li><strong>Observation:</strong> The confusion matrix and accuracy are
<em>identical</em> to the logistic regression model.</li>
</ul></li>
</ul>
<h3 id="final-comparison-slide-93">Final Comparison (Slide 93)</h3>
<ul>
<li><strong>ROC and AUC for LDA:</strong> The ROC curve for the LDA
model is plotted.</li>
<li><strong>AUC Value:</strong> <strong>0.5584</strong>.</li>
<li><strong>Main Conclusion:</strong> As highlighted in the green box,
<strong>‚ÄúLDA has identical performance as Logistic regression!‚Äù</strong>
In this specific practical example, using these two predictors, both
linear methods produce the exact same confusion matrix, the same
accuracy (56%), and the same AUC (0.558). This reinforces the
theoretical idea that both are fitting a linear boundary.</li>
</ul>
<h3 id="ÊúÄÁªàÊØîËæÉÂπªÁÅØÁâá-93">ÊúÄÁªàÊØîËæÉÔºàÂπªÁÅØÁâá 93Ôºâ</h3>
<ul>
<li><strong>LDA ÁöÑ ROC Âíå AUCÔºö</strong>ÁªòÂà∂‰∫Ü LDA Ê®°ÂûãÁöÑ ROC
Êõ≤Á∫ø„ÄÇ</li>
<li><strong>AUC ÂÄºÔºö</strong>0.5584**„ÄÇ</li>
<li><strong>‰∏ªË¶ÅÁªìËÆ∫Ôºö</strong>Â¶ÇÁªøËâ≤ÊñπÊ°ÜÊâÄÁ§∫Ôºå‚ÄúLDA ÁöÑÊÄßËÉΩ‰∏é Logistic
ÂõûÂΩíÁõ∏ÂêåÔºÅ‚Äù**
Âú®Ëøô‰∏™ÂÖ∑‰ΩìÁöÑÂÆûÈôÖÁ§∫‰æã‰∏≠Ôºå‰ΩøÁî®Ëøô‰∏§‰∏™È¢ÑÊµãÂèòÈáèÔºå‰∏§ÁßçÁ∫øÊÄßÊñπÊ≥ïÈÉΩ‰∫ßÁîü‰∫ÜÂÆåÂÖ®Áõ∏ÂêåÁöÑÊ∑∑Ê∑ÜÁü©Èòµ„ÄÅÁõ∏ÂêåÁöÑÂáÜÁ°ÆÁéáÔºà56%ÔºâÂíåÁõ∏ÂêåÁöÑ
AUCÔºà0.558Ôºâ„ÄÇËøôÂº∫Âåñ‰∫Ü‰∏§ËÄÖÂùáÊãüÂêàÁ∫øÊÄßËæπÁïåÁöÑÁêÜËÆ∫ËßÇÁÇπ„ÄÇ</li>
</ul>
<h2 id="r-example-on-smarket-data-continued">4.7 R Example on Smarket
Data (Continued)</h2>
<p>The previous slides showed that Logistic Regression and Linear
Discriminant Analysis (LDA) had <strong>identical performance</strong>
on the Smarket dataset (using <code>Lag1</code> and <code>Lag2</code>),
both achieving 56% test accuracy and an AUC of 0.558. The analysis now
tests a more flexible method, QDA.</p>
<h3 id="model-3-qda-lag1-lag2-slides-94-95">Model 3: QDA (Lag1 &amp;
Lag2) (Slides 94-95)</h3>
<ul>
<li><strong>Model:</strong> A Quadratic Discriminant Analysis (QDA)
model is fit on the same training data (2001-2004) using only the
<code>Lag1</code> and <code>Lag2</code> predictors.
<ul>
<li><code>qda.fit &lt;- qda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> The model is used to predict the market
direction for the 2005 test set.</li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Accuracy:</strong> The model achieves a test accuracy
of <strong>0.5992 (or 60%)</strong>.</li>
<li><strong>AUC:</strong> The Area Under the Curve (AUC) for the QDA
model is <strong>0.562</strong>.</li>
</ul></li>
<li><strong>Conclusion:</strong> As the slide highlights, <strong>‚ÄúQDA
has better test performance than LDA and Logistic
regression!‚Äù</strong></li>
</ul>
<h3 id="smarket-example-summary">Smarket Example Summary</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Model Type</th>
<th style="text-align: left;">Test Accuracy</th>
<th style="text-align: left;">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Logistic Regression</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">~56%</td>
<td style="text-align: left;">0.558</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LDA</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">~56%</td>
<td style="text-align: left;">0.558</td>
</tr>
<tr>
<td style="text-align: left;"><strong>QDA</strong></td>
<td style="text-align: left;">Quadratic</td>
<td style="text-align: left;"><strong>~60%</strong></td>
<td style="text-align: left;"><strong>0.562</strong></td>
</tr>
</tbody>
</table>
<p>This practical example reinforces the lessons from the simulations
(Section 4.6). The two linear methods (LDA, Logistic) had identical
performance. The more flexible, non-linear QDA model performed better,
suggesting that the true decision boundary between ‚ÄúUp‚Äù and ‚ÄúDown‚Äù
(based on <code>Lag1</code> and <code>Lag2</code>) is not perfectly
linear.</p>
<h2 id="kernel-lda">4.8 Kernel LDA</h2>
<p>This new section introduces an even more advanced non-linear method,
Kernel LDA.</p>
<h3 id="the-problem-linear-inseparability-slide-97">The Problem: Linear
Inseparability (Slide 97)</h3>
<p>The section starts with a clear visual example. A dataset of two
concentric circles (a ‚Äúdonut‚Äù shape) is <strong>linearly
inseparable</strong>. It is impossible to draw a single straight line to
separate the inner (purple) class from the outer (yellow) class.</p>
<h3 id="the-solution-the-kernel-trick-slides-97-99">The Solution: The
Kernel Trick (Slides 97, 99)</h3>
<ol type="1">
<li><strong>Nonlinear Transformation:</strong> The data is ‚Äúlifted‚Äù into
a higher-dimensional <em>feature space</em> using a <strong>nonlinear
transformation</strong>, <span class="math inline">\(x \mapsto
\phi(x)\)</span>. In the example on the slide, the 2D data is
transformed, and in this new space, the two classes <em>become</em>
<strong>linearly separable</strong>.</li>
<li><strong>The ‚ÄúKernel Trick‚Äù:</strong> The main idea (from slide 99)
is that we don‚Äôt need to explicitly compute this complex transformation
<span class="math inline">\(\phi(x)\)</span>. LDA (based on Fisher‚Äôs
approach) only requires inner products of the data points. The ‚Äúkernel
trick‚Äù allows us to replace the inner product in the high-dimensional
feature space (<span class="math inline">\(x_i^T x_j\)</span>) with a
simple <strong>kernel function</strong>, <span
class="math inline">\(k(x_i, x_j)\)</span>, computed in the original,
low-dimensional space.
<ul>
<li>An example of such a kernel is the <strong>Gaussian (RBF)
kernel</strong>: <span class="math inline">\(k(x_i, x_j) \propto
e^{-\|x_i - x_j\|^2 / \sigma^2}\)</span>.</li>
</ul></li>
</ol>
<h3 id="academic-foundations-slide-98">Academic Foundations (Slide
98)</h3>
<p>This method is based on foundational academic papers that generalized
linear methods using kernels: * <strong>Fisher discriminant analysis
with kernels</strong> (Mika, 1999) * <strong>Generalized Discriminant
Analysis Using a Kernel Approach</strong> (Baudat, 2000) *
<strong>Kernel principal component analysis</strong> (Sch√∂lkopf,
1997)</p>
<p>In short, Kernel LDA is an extension of LDA that uses the kernel
trick to find a linear boundary in a high-dimensional feature space,
which corresponds to a highly non-linear boundary in the original
space.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/27/QM9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/27/QM9/" class="post-title-link" itemprop="url">QM9 Dataset</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">ÂèëË°®‰∫é</span>

              <time title="ÂàõÂª∫Êó∂Èó¥Ôºö2025-09-27 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-27T21:00:00+08:00">2025-09-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Êõ¥Êñ∞‰∫é</span>
                <time title="‰øÆÊîπÊó∂Èó¥Ôºö2025-09-29 03:57:13" itemprop="dateModified" datetime="2025-09-29T03:57:13+08:00">2025-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">ÂàÜÁ±ª‰∫é</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/dataset/" itemprop="url" rel="index"><span itemprop="name">dataset</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="qm9-Êï∞ÊçÆÈõÜÁöÑxyzÊ†ºÂºèËØ¶Ëß£">1. QM9 Êï∞ÊçÆÈõÜÁöÑXYZÊ†ºÂºèËØ¶Ëß£</h3>
<p>Ëøô‰∏™Êï∞ÊçÆÈõÜ‰ΩøÁî®ÁöÑ ‚ÄúXYZ-like‚Äù
Ê†ºÂºèÊòØ‰∏ÄÁßç<strong>Êâ©Â±ïÁöÑ„ÄÅÈùûÊ†áÂáÜÁöÑXYZÊ†ºÂºè</strong>„ÄÇ</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 43%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr>
<th>Ë°åÂè∑</th>
<th>ÂÜÖÂÆπ</th>
<th>Ëß£Èáä</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Á¨¨ 1 Ë°å</strong></td>
<td><code>na</code></td>
<td>‰∏Ä‰∏™Êï¥Êï∞Ôºå‰ª£Ë°®ÂàÜÂ≠ê‰∏≠ÁöÑÂéüÂ≠êÊÄªÊï∞„ÄÇ</td>
</tr>
<tr>
<td><strong>Á¨¨ 2 Ë°å</strong></td>
<td><code>Properties 1-17</code></td>
<td>ÂåÖÂê´17‰∏™ÁêÜÂåñÊÄßË¥®ÁöÑÊï∞ÂÄºÔºåÁî®Âà∂Ë°®Á¨¶ÊàñÁ©∫Ê†ºÂàÜÈöî„ÄÇ</td>
</tr>
<tr>
<td><strong>Á¨¨ 3 Âà∞ na+2 Ë°å</strong></td>
<td><code>Element  x  y  z  charge</code></td>
<td>ÊØèË°å‰ª£Ë°®‰∏Ä‰∏™ÂéüÂ≠ê„ÄÇ‰æùÊ¨°ÊòØÔºöÂÖÉÁ¥†Á¨¶Âè∑„ÄÅx/y/zÂùêÊ†áÔºàÂçï‰ΩçÔºöÂüÉÔºâ„ÄÅMullikenÈÉ®ÂàÜÁîµËç∑ÔºàÂçï‰ΩçÔºöeÔºâ„ÄÇ</td>
</tr>
<tr>
<td><strong>Á¨¨ na+3 Ë°å</strong></td>
<td><code>Frequencies</code></td>
<td>ÂàÜÂ≠êÁöÑÊåØÂä®È¢ëÁéáÔºà3na-5Êàñ3na-6‰∏™Ôºâ„ÄÇ</td>
</tr>
<tr>
<td><strong>Á¨¨ na+4 Ë°å</strong></td>
<td><code>SMILES_GDB9   SMILES_relaxed</code></td>
<td>Êù•Ëá™GDB9ÁöÑSMILESÂ≠óÁ¨¶‰∏≤ÂíåÂºõË±´ÂêéÁöÑÂá†‰ΩïÊûÑÂûãÁöÑSMILESÂ≠óÁ¨¶‰∏≤„ÄÇ</td>
</tr>
<tr>
<td><strong>Á¨¨ na+5 Ë°å</strong></td>
<td><code>InChI_GDB9    InChI_relaxed</code></td>
<td>ÂØπÂ∫îÁöÑInChIÂ≠óÁ¨¶‰∏≤„ÄÇ</td>
</tr>
</tbody>
</table>
<p><strong>‰∏éÊ†áÂáÜXYZÊ†ºÂºèÂØπÊØîÔºö</strong> *
<strong>Ê†áÂáÜÊ†ºÂºè</strong>Âè™ÊúâÁ¨¨1Ë°åÔºàÂéüÂ≠êÊï∞Ôºâ„ÄÅÁ¨¨2Ë°åÔºàÊ≥®ÈáäÔºâÂíåÂêéÁª≠ÁöÑÂéüÂ≠êÂùêÊ†áË°åÔºà‰ªÖÂê´ÂÖÉÁ¥†ÂíåxyzÂùêÊ†áÔºâ„ÄÇ
*
<strong>QM9Ê†ºÂºè</strong>Âú®Á¨¨2Ë°åÊèíÂÖ•‰∫ÜÂ§ßÈáèÂ±ûÊÄßÊï∞ÊçÆÔºåÂú®ÂéüÂ≠êÂùêÊ†áË°åÂ¢ûÂä†‰∫ÜÁîµËç∑ÂàóÔºåÂπ∂Âú®Êñá‰ª∂Êú´Â∞æÈôÑÂä†‰∫ÜÈ¢ëÁéá„ÄÅSMILESÂíåInChI‰ø°ÊÅØ„ÄÇ</p>
<h3 id="readme">2. readme</h3>
<ol type="1">
<li><strong>Êï∞ÊçÆÈõÜÊ†∏ÂøÉÂÜÖÂÆπ</strong>:
<ul>
<li>ÂÆÉÂåÖÂê´‰∫Ü<strong>133,885‰∏™</strong>Â∞èÂûãÊúâÊú∫ÂàÜÂ≠êÔºàÁî±H, C, N, O,
FÂÖÉÁ¥†ÁªÑÊàêÔºâÁöÑÈáèÂ≠êÂåñÂ≠¶ËÆ°ÁÆóÊï∞ÊçÆ„ÄÇ</li>
<li>ÊâÄÊúâÂàÜÂ≠êÁöÑÂá†‰ΩïÊûÑÂûãÈÉΩÁªèËøá‰∫Ü<strong>DFT/B3LYP/6-31G(2df,p)</strong>Ê∞¥Âπ≥ÁöÑ‰ºòÂåñ„ÄÇ</li>
<li><code>dsC7O2H10nsd.xyz.tar.bz2</code>ÊòØËØ•Êï∞ÊçÆÈõÜÁöÑ‰∏Ä‰∏™Â≠êÈõÜÔºå‰∏ìÈó®ÂåÖÂê´<strong>6,095‰∏™C‚ÇáH‚ÇÅ‚ÇÄO‚ÇÇÁöÑÂêåÂàÜÂºÇÊûÑ‰Ωì</strong>ÔºåÂÖ∂ËÉΩÈáèÂ≠¶ÊÄßË¥®Âú®Êõ¥È´òÁ≤æÂ∫¶ÁöÑ<strong>G4MP2</strong>ÁêÜËÆ∫Ê∞¥Âπ≥‰∏ãËÆ°ÁÆó„ÄÇ</li>
</ul></li>
<li><strong>Êñá‰ª∂ÁªìÊûÑ‰∏éÊ†ºÂºè</strong>:
<ul>
<li>ÊòéÁ°ÆÊåáÂá∫ÊØè‰∏™ÂàÜÂ≠êÂ≠òÂÇ®Âú®ÂçïÁã¨ÁöÑ<code>.xyz</code>Êñá‰ª∂‰∏≠ÔºåÂπ∂ËØ¶ÁªÜÊèèËø∞‰∫Ü‰∏äËø∞ÁöÑ<strong>ÈùûÊ†áÂáÜXYZÊâ©Â±ïÊ†ºÂºè</strong>„ÄÇ</li>
<li>ËØ¶ÁªÜÂàóÂá∫‰∫ÜËÆ∞ÂΩïÂú®Êñá‰ª∂Á¨¨2Ë°åÁöÑ<strong>17ÁßçÁêÜÂåñÊÄßË¥®</strong>ÔºåÂåÖÊã¨ËΩ¨Âä®Â∏∏Êï∞(A,
B,
C)„ÄÅÂÅ∂ÊûÅÁü©(mu)„ÄÅHOMO/LUMOËÉΩÁ∫ß„ÄÅÈõ∂ÁÇπÊåØÂä®ËÉΩ(zpve)„ÄÅÂÜÖËÉΩ(U)„ÄÅÁÑì(H)ÂíåÂêâÂ∏ÉÊñØËá™Áî±ËÉΩ(G)Á≠â„ÄÇ</li>
</ul></li>
<li><strong>Êï∞ÊçÆÊù•Ê∫ê‰∏éËÆ°ÁÆóÊñπÊ≥ï</strong>:
<ul>
<li>Êï∞ÊçÆÊ∫ê‰∫é<strong>GDB-9</strong>ÂåñÂ≠¶Êï∞ÊçÆÂ∫ì„ÄÇ</li>
<li>‰∏ªË¶Å‰ΩøÁî®‰∫Ü‰∏§ÁßçÈáèÂ≠êÂåñÂ≠¶ÁêÜËÆ∫Ê∞¥Âπ≥Ôºö<strong>B3LYP</strong>Áî®‰∫éÂ§ßÈÉ®ÂàÜÂ±ûÊÄßËÆ°ÁÆóÔºå<strong>G4MP2</strong>Áî®‰∫éC‚ÇáH‚ÇÅ‚ÇÄO‚ÇÇÂ≠êÈõÜÁöÑËÉΩÈáèËÆ°ÁÆó„ÄÇ</li>
</ul></li>
<li><strong>ÂºïÁî®Ë¶ÅÊ±Ç</strong>:
<ul>
<li>Êñá‰ª∂ÊòéÁ°ÆË¶ÅÊ±ÇÔºåÂ¶ÇÊûú‰ΩøÁî®ËØ•Êï∞ÊçÆÈõÜÔºåÈúÄË¶ÅÂºïÁî®Raghunathan
RamakrishnanÁ≠â‰∫∫Âú®2014Âπ¥ÂèëË°®‰∫é„ÄäScientific Data„ÄãÁöÑËÆ∫Êñá„ÄÇ</li>
</ul></li>
<li><strong>ÂÖ∂‰ªñ‰ø°ÊÅØ</strong>:
<ul>
<li>Êèê‰æõ‰∫Ü‰∏Ä‰∫õÈ¢ùÂ§ñÊñá‰ª∂ÔºàÂ¶Ç<code>validation.txt</code>,
<code>uncharacterized.txt</code>ÔºâÁöÑËØ¥Êòé„ÄÇ</li>
<li>ÊèêÂà∞‰∫ÜÊï∞ÊçÆÈõÜ‰∏≠ÊúâÂ∞ëÊï∞Âá†‰∏™ÂàÜÂ≠êÂú®Âá†‰Ωï‰ºòÂåñÊó∂Èöæ‰ª•Êî∂Êïõ„ÄÇ</li>
</ul></li>
</ol>
<h3 id="ÂèØËßÜÂåñ">3. ÂèØËßÜÂåñ</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import ase.io</span><br><span class="line">import nglview as nv</span><br><span class="line">import io</span><br><span class="line"></span><br><span class="line">def parse_qm9_xyz(file_path):</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    Parses a QM9 extended XYZ file and returns a standard XYZ string.</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    with open(file_path, <span class="string">&#x27;r&#x27;</span>) as f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First line is the number of atoms</span></span><br><span class="line">    num_atoms = int(lines[0].strip())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The next line is properties (skip it)</span></span><br><span class="line">    <span class="comment"># The next num_atoms lines are the coordinates</span></span><br><span class="line">    coord_lines = lines[2:2+num_atoms]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Rebuild a standard XYZ format string in memory</span></span><br><span class="line">    standard_xyz = f<span class="string">&quot;&#123;num_atoms&#125;\n&quot;</span></span><br><span class="line">    standard_xyz += <span class="string">&quot;Comment line\n&quot;</span> <span class="comment"># Add a standard comment line</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> coord_lines:</span><br><span class="line">        parts = line.split()</span><br><span class="line">        <span class="comment"># Keep only the element and the x, y, z coordinates</span></span><br><span class="line">        standard_xyz += f<span class="string">&quot;&#123;parts[0]&#125; &#123;parts[1]&#125; &#123;parts[2]&#125; &#123;parts[3]&#125;\n&quot;</span></span><br><span class="line">        </span><br><span class="line">    <span class="built_in">return</span> standard_xyz</span><br><span class="line"></span><br><span class="line"><span class="comment"># Path to your data file</span></span><br><span class="line">file_path = <span class="string">&quot;/root/QM9/QM9/Data_for_6095_constitutional_isomers_of_C7H10O2.xyz/dsC7O2H10nsd_0001.xyz&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Parse the special file format into a standard XYZ string</span></span><br><span class="line">standard_xyz_data = parse_qm9_xyz(file_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. ASE reads the standard XYZ data from the string variable</span></span><br><span class="line"><span class="comment">#    We use io.StringIO to make the string behave like a file</span></span><br><span class="line">atoms = ase.io.read(io.StringIO(standard_xyz_data), format=<span class="string">&quot;xyz&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create the nglview visualization widget</span></span><br><span class="line">view = nv.show_ase(atoms)</span><br><span class="line">view.add_ball_and_stick()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the widget in the notebook output</span></span><br><span class="line">view</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol type="1">
<li><strong>ÂÆö‰πâËß£ÊûêÂáΩÊï∞ <code>parse_qm9_xyz</code></strong>:
<ul>
<li><strong>ÁõÆÁöÑ</strong>:
Â∞ÜËøô‰∏™ÂáΩÊï∞‰Ωú‰∏∫‰∏ìÈó®Â§ÑÁêÜQM9ÁâπÊÆäÊ†ºÂºèÁöÑÂ∑•ÂÖ∑„ÄÇ‰ª£Á†Å‰∏ª‰ΩìÊ∏ÖÊô∞ÔºåÊòì‰∫éÂ§çÁî®„ÄÇ</li>
<li><strong>ËØªÂèñÊñá‰ª∂</strong>: <code>with open(...)</code>
ÂÆâÂÖ®Âú∞ÊâìÂºÄÊñá‰ª∂ÔºåÂπ∂Áî® <code>f.readlines()</code>
Â∞ÜÊñá‰ª∂ÊâÄÊúâË°å‰∏ÄÊ¨°ÊÄßËØªÂÖ•‰∏Ä‰∏™ÂàóË°® <code>lines</code> ‰∏≠„ÄÇ</li>
<li><strong>ÊèêÂèñÂéüÂ≠êÊï∞Èáè</strong>:
<code>num_atoms = int(lines[0].strip())</code>
ËØªÂèñÁ¨¨‰∏ÄË°åÔºà<code>lines[0]</code>ÔºâÔºåÂéªÈô§ÂèØËÉΩÂ≠òÂú®ÁöÑÁ©∫Ê†ºÔºà<code>.strip()</code>ÔºâÔºåÂπ∂Â∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫Êï¥Êï∞„ÄÇËøôÊòØÊûÑÂª∫Ê†áÂáÜXYZÊ†ºÂºèÁöÑÂøÖË¶Å‰ø°ÊÅØ„ÄÇ</li>
<li><strong>ÊèêÂèñÂùêÊ†á‰ø°ÊÅØ</strong>:
<code>coord_lines = lines[2:2+num_atoms]</code>
Ê†á‰ø°ÊÅØ‰ªéÁ¨¨3Ë°åÂºÄÂßãÔºàÁ¥¢Âºï‰∏∫2ÔºâÔºåÊåÅÁª≠<code>num_atoms</code>Ë°å„ÄÇÈÄöËøáÂàóË°®ÂàáÁâáÔºåÁ≤æÁ°ÆÂú∞ÊèêÂèñÂá∫ÊâÄÊúâÂåÖÂê´ÂéüÂ≠êÂùêÊ†áÁöÑË°åÔºåË∑≥Ëøá‰∫ÜÁ¨¨2Ë°åÁöÑÂ±ûÊÄß‰ø°ÊÅØ„ÄÇ</li>
<li><strong>ÊûÑÂª∫Ê†áÂáÜXYZÊ†ºÂºèÂ≠óÁ¨¶‰∏≤</strong>:
<ul>
<li>ÂàõÂª∫‰∏Ä‰∏™Âêç‰∏∫ <code>standard_xyz</code> ÁöÑÊñ∞Â≠óÁ¨¶‰∏≤„ÄÇ</li>
<li>È¶ñÂÖàÔºåÂ∞ÜÂéüÂ≠êÊï∞ÈáèÂíåÊç¢Ë°åÁ¨¶ÂÜôÂÖ•„ÄÇ</li>
<li>ÁÑ∂ÂêéÔºåÊ∑ªÂä†‰∏ÄË°åÊ†áÂáÜÁöÑÊ≥®ÈáäÔºà‚ÄúComment
line‚ÄùÔºâÔºåËøôÊòØÊ†áÂáÜXYZÊ†ºÂºèÊâÄË¶ÅÊ±ÇÁöÑ„ÄÇ</li>
<li>ÊúÄÂêéÔºåÈÅçÂéÜÂàöÂàöÊèêÂèñÁöÑ <code>coord_lines</code> ÂàóË°®„ÄÇÂØπ‰∫éÊØè‰∏ÄË°åÔºå‰ΩøÁî®
<code>.split()</code>
Â∞ÜÂÖ∂ÊãÜÂàÜÊàêÂ§ö‰∏™ÈÉ®ÂàÜÔºà‰æãÂ¶ÇÔºö<code>['C', 'x', 'y', 'z', 'charge']</code>Ôºâ„ÄÇÂè™ÂèñÂâçÂõõÈÉ®ÂàÜÔºàÂÖÉÁ¥†Á¨¶Âè∑ÂíåxyzÂùêÊ†áÔºâÔºåÂπ∂ÈáçÊñ∞ÁªÑÂêàÊàêÊñ∞ÁöÑ‰∏ÄË°åÔºå<strong>‰ªéËÄå‰∏¢ÂºÉ‰∫ÜÊú´Â∞æÁöÑMullikenÁîµËç∑Êï∞ÊçÆ</strong>„ÄÇ</li>
</ul></li>
<li><strong>ËøîÂõûÁªìÊûú</strong>:
ÂáΩÊï∞ËøîÂõû‰∏Ä‰∏™ÂåÖÂê´‰∫ÜÊ†áÂáÜXYZÊ†ºÂºèÊï∞ÊçÆÁöÑ„ÄÅÂπ≤ÂáÄÁöÑÂ≠óÁ¨¶‰∏≤„ÄÇ</li>
</ul></li>
<li><strong>‰∏ªÁ®ãÂ∫èÊâßË°åÊµÅÁ®ã</strong>:
<ul>
<li><strong>Ë∞ÉÁî®ÂáΩÊï∞</strong>:
<code>standard_xyz_data = parse_qm9_xyz(file_path)</code>
Ë∞ÉÁî®‰∏äÈù¢ÁöÑÂáΩÊï∞ÔºåÂÆåÊàê‰ªéÊñá‰ª∂Âà∞Ê†áÂáÜÊ†ºÂºèÂ≠óÁ¨¶‰∏≤ÁöÑËΩ¨Êç¢„ÄÇ</li>
<li><strong>Âú®ÂÜÖÂ≠ò‰∏≠ËØªÂèñ</strong>:
<code>ase.io.read(io.StringIO(standard_xyz_data), format="xyz")</code>
Ëøô‰∏ÄÊ≠•ÈùûÂ∏∏È´òÊïà„ÄÇ<code>io.StringIO</code> Â∞ÜÊàë‰ª¨ÁöÑÂ≠óÁ¨¶‰∏≤ÂèòÈáè
<code>standard_xyz_data</code>
Ê®°ÊãüÊàê‰∏Ä‰∏™ÂÜÖÂ≠ò‰∏≠ÁöÑÊñáÊú¨Êñá‰ª∂„ÄÇËøôÊ†∑Ôºå<code>ase.io.read</code>
Â∞±ÂèØ‰ª•Áõ¥Êé•ËØªÂèñÂÆÉÔºåËÄåÊó†ÈúÄÂÖàÂ∞ÜÊ∏ÖÊ¥óÂêéÁöÑÊï∞ÊçÆÂÜôÂÖ•‰∏Ä‰∏™‰∏¥Êó∂Êñá‰ª∂ÂÜçËØªÂèñÔºåËäÇÁúÅ‰∫ÜÁ£ÅÁõòI/OÊìç‰Ωú„ÄÇ</li>
<li><strong>ÂèØËßÜÂåñ</strong>: Êé•‰∏ãÊù•ÁöÑ‰ª£Á†Å (<code>nv.show_ase</code>Á≠â)
Â∞±ÂíåÊúÄÂàùÁöÑËÆæÊÉ≥‰∏ÄÊ†∑‰∫ÜÔºåÂõ†‰∏∫Ê≠§Êó∂ <code>atoms</code>
ÂØπË±°Â∑≤ÁªèÊòØÈÄöËøáÊ†áÂáÜ„ÄÅÂπ≤ÂáÄÁöÑÊï∞ÊçÆÊàêÂäüÂàõÂª∫ÁöÑ‰∫Ü„ÄÇ</li>
</ul></li>
</ol>
<p><img src="/imgs/QM9/C7O2H10/C7O2H10.png" alt="C7O2H10"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/27/fusionnetwork/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/27/fusionnetwork/" class="post-title-link" itemprop="url">FusionProt - ËÆ∫ÊñáÈòÖËØª</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">ÂèëË°®‰∫é</span>

              <time title="ÂàõÂª∫Êó∂Èó¥Ôºö2025-09-27 11:00:00" itemprop="dateCreated datePublished" datetime="2025-09-27T11:00:00+08:00">2025-09-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Êõ¥Êñ∞‰∫é</span>
                <time title="‰øÆÊîπÊó∂Èó¥Ôºö2025-09-29 03:56:14" itemprop="dateModified" datetime="2025-09-29T03:56:14+08:00">2025-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">ÂàÜÁ±ª‰∫é</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Reading/" itemprop="url" rel="index"><span itemprop="name">Paper Reading</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Fusing Sequence and Structural Information for Unified Protein
Representation Learning</p>
<p><a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=imcinaOHod">FusionProt</a></p>
<h2 id="ËõãÁôΩË¥®Ë°®Á§∫Â≠¶‰π†">1 ËõãÁôΩË¥®Ë°®Á§∫Â≠¶‰π†Ôºö</h2>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>:</li>
</ul>
<p>FusionProt :ÂèØÂ≠¶‰π†ËûçÂêà
tokenÂíåËø≠‰ª£ÂèåÂêë‰ø°ÊÅØ‰∫§Êç¢ÔºåÂÆûÁé∞Â∫èÂàó‰∏éÁªìÊûÑÁöÑÂä®ÊÄÅÂçèÂêåÂ≠¶‰π†ÔºåËÄåÈùûÈùôÊÄÅÊãºÊé•„ÄÇ</p>
<h2 id="‰∏ÄÁª¥1dÊ∞®Âü∫ÈÖ∏Â∫èÂàóÂíå‰∏âÁª¥3dÁ©∫Èó¥ÁªìÊûÑ">2.
‰∏ÄÁª¥Ôºà1DÔºâÊ∞®Âü∫ÈÖ∏Â∫èÂàóÂíå‰∏âÁª¥Ôºà3DÔºâÁ©∫Èó¥ÁªìÊûÑÔºö</h2>
<ul>
<li><p><strong>ÂçïÊ®°ÊÄÅ‰æùËµñ:</strong>
ProteinBERT„ÄÅESM-2‰ªÖÂü∫‰∫éÂ∫èÂàó</p></li>
<li><p><strong>ÈùôÊÄÅËûçÂêàÁº∫Èô∑ :</strong>ESM-GearNet„ÄÅSaProt
ÁªìÂêàÂ∫èÂàó‰∏éÁªìÊûÑÔºå‰ΩÜÈááÁî® ‚ÄúÂçïÂêë / ‰∏ÄÊ¨°ÊÄßËûçÂêà‚Äù</p></li>
</ul>
<p>Â•ΩÁöÑÔºåÂÆåÂÖ®Ê≤°ÊúâÈóÆÈ¢ò„ÄÇËøôÊòØÂØπ <code>FusionNetwork</code>
Ê®°ÂûãÊû∂ÊûÑ‰ª£Á†ÅÁöÑ‰∏≠ÊñáÂ§çËø∞ÂàÜÊûê„ÄÇ</p>
<h2 id="Ê®°ÂûãÊÄª‰Ωì">3. Ê®°ÂûãÊÄª‰Ωì</h2>
<p><img src="/imgs/fusionProt/FusionProt.png" alt="fusion">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@R.register(<span class="params"><span class="string">&quot;models.FusionNetwork&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FusionNetwork</span>(nn.Module, core.Configurable):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sequence_model, structure_model, fusion=<span class="string">&quot;series&quot;</span>, cross_dim=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(FusionNetwork, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.sequence_model = sequence_model</span><br><span class="line">        <span class="variable language_">self</span>.structure_model = structure_model</span><br><span class="line">        <span class="variable language_">self</span>.output_dim = sequence_model.output_dim + structure_model.output_dim</span><br><span class="line">        <span class="variable language_">self</span>.inject_step = <span class="number">5</span>   <span class="comment"># (sequence_layers / structure_layers) layers</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong><code>class FusionNetwork(...)</code></strong>:
ÂÆö‰πâ‰∫ÜÊ®°ÂûãÁ±ªÔºåÂÆÉÁªßÊâøËá™ PyTorch ÁöÑÂü∫Á°ÄÊ®°Âùó <code>nn.Module</code>„ÄÇ</li>
<li><strong><code>__init__(...)</code></strong>:
ÊûÑÈÄ†ÂáΩÊï∞ÔºåÊé•Êî∂Â∑≤ÁªèÂàùÂßãÂåñÂ•ΩÁöÑ <code>sequence_model</code> Âíå
<code>structure_model</code> ‰Ωú‰∏∫ËæìÂÖ•„ÄÇ</li>
<li><strong><code>self.output_dim</code></strong>:
ÂÆö‰πâ‰∫ÜÊ®°ÂûãÊúÄÁªàËæìÂá∫ÁâπÂæÅÁöÑÁª¥Â∫¶„ÄÇÂõ†‰∏∫ÊúÄÂêé‰ºöÂ∞Ü‰∏§‰∏™Ê®°ÂûãÁöÑÁâπÂæÅÊãºÊé•Ëµ∑Êù•ÔºåÊâÄ‰ª•ÊòØ‰∏§ËÄÖËæìÂá∫Áª¥Â∫¶‰πãÂíå„ÄÇ</li>
<li><strong><code>self.inject_step = 5</code></strong>:ÂÆö‰πâ‰∫Ü‰ø°ÊÅØ‚ÄúÊ≥®ÂÖ•‚ÄùÊàñ‚Äú‰∫§ÊµÅ‚ÄùÁöÑÈ¢ëÁéá„ÄÇËøôÈáåËÆæÁΩÆ‰∏∫
5ÔºåÊÑèÂë≥ÁùÄ<strong>ÊØèÁªèËøáÂ∫èÂàóÊ®°ÂûãÁöÑ 5
Â±ÇÔºåÂ∞±‰ºöËøõË°å‰∏ÄÊ¨°‰ø°ÊÅØ‰∫§Êç¢</strong>„ÄÇ</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Structure embeddings layer</span></span><br><span class="line">raw_input_dim = <span class="number">21</span>  <span class="comment"># amino acid tokens</span></span><br><span class="line"><span class="variable language_">self</span>.structure_embed_linear = nn.Linear(raw_input_dim, structure_model.input_dim)</span><br><span class="line"><span class="variable language_">self</span>.embedding_batch_norm = nn.BatchNorm1d(structure_model.input_dim)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>self.structure_embed_linear</code></strong>:
‰∏Ä‰∏™Á∫øÊÄßÂ±ÇÔºåÁî®‰∫éÂ∞ÜÂéüÂßãÁöÑÁªìÊûÑËæìÂÖ•ÔºàÊØîÂ¶Ç 21
ÁßçÊ∞®Âü∫ÈÖ∏ÁöÑÁã¨ÁÉ≠ÁºñÁ†ÅÔºâËΩ¨Êç¢‰∏∫ÁªìÊûÑÊ®°ÂûãÔºàGNNÔºâÊâÄÊúüÊúõÁöÑËæìÂÖ•Áª¥Â∫¶„ÄÇ</li>
<li><strong><code>self.embedding_batch_norm</code></strong>:
ÊâπÂΩí‰∏ÄÂåñÂ±ÇÔºåÁî®‰∫éÁ®≥ÂÆöÁªìÊûÑÂµåÂÖ•Â±ÇÁöÑËÆ≠ÁªÉËøáÁ®ã„ÄÇ</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normal Initialization of the 3D structure token</span></span><br><span class="line">structure_token = nn.Parameter(torch.Tensor(structure_model.input_dim).unsqueeze(<span class="number">0</span>))</span><br><span class="line">nn.init.normal_(structure_token, mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>)</span><br><span class="line"><span class="variable language_">self</span>.structure_token = nn.Parameter(structure_token.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>self.structure_token</code></strong>: ‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑÂêëÈáè
(<code>nn.Parameter</code>)„ÄÇËøô‰∏™‚Äú‰ª§Áâå‚Äù‰∏ç‰ª£Ë°®‰ªª‰ΩïÁúüÂÆûÁöÑÂéüÂ≠êÊàñÊ∞®Âü∫ÈÖ∏ÔºåËÄåÊòØ‰∏Ä‰∏™ÊäΩË±°ÁöÑËΩΩ‰Ωì„ÄÇÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÂÆÉÂ∞Ü<strong>Â≠¶‰π†Â¶Ç‰ΩïÁºñÁ†ÅÂíåË°®Á§∫Êï¥‰∏™ËõãÁôΩË¥®ÁöÑÂÖ®Â±Ä
3D ÁªìÊûÑ‰ø°ÊÅØ</strong>„ÄÇÂÆÉÂ∞±ÂÉè‰∏Ä‰∏™‰ø°ÊÅØ‰ø°‰Ωø„ÄÇ</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Linear Transformation between structure to sequential spaces</span></span><br><span class="line"><span class="variable language_">self</span>.structure_linears = nn.ModuleList([...])</span><br><span class="line"><span class="variable language_">self</span>.seq_linears = nn.ModuleList([...])</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>self.structure_linears</code> /
<code>self.seq_linears</code></strong>:
Â∫èÂàóÊ®°ÂûãÂíåÁªìÊûÑÊ®°ÂûãÂÜÖÈÉ®Â§ÑÁêÜÁöÑÁâπÂæÅÂêëÈáèÁª¥Â∫¶ÂèØËÉΩ‰∏çÂêå„ÄÇÂΩì‚Äú3D
‰ª§Áâå‚ÄùÈúÄË¶ÅÂú®‰∏§‰∏™Ê®°Âûã‰πãÈó¥‰º†ÈÄíÊó∂ÔºåËøô‰∫õÁ∫øÊÄßÂ±ÇË¥üË¥£Â∞ÜÂÆÉÁöÑË°®Á§∫‰ªé‰∏Ä‰∏™Ê®°ÂûãÁöÑÁâπÂæÅÁ©∫Èó¥ËΩ¨Êç¢Âà∞Âè¶‰∏Ä‰∏™Ê®°ÂûãÁöÑÁâπÂæÅÁ©∫Èó¥„ÄÇ</li>
</ul>
<h2 id="ÂâçÂêë">4. ÂâçÂêë</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, graph, <span class="built_in">input</span>, all_loss=<span class="literal">None</span>, metric=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># Build a new protein graph with the 3D token (the lase node)</span></span><br><span class="line">    new_graph = <span class="variable language_">self</span>.build_protein_graph_with_3d_token(graph)</span><br></pre></td></tr></table></figure>
<ul>
<li>È¶ñÂÖàË∞ÉÁî®ËæÖÂä©ÂáΩÊï∞ÔºåÂ∞ÜËæìÂÖ•ÁöÑËõãÁôΩË¥®ÂõæË∞±ËøõË°åÊîπÈÄ†Ôºö‰∏∫ÂõæË∞±Â¢ûÂä†‰∏Ä‰∏™‰ª£Ë°®‚Äú3D
‰ª§Áâå‚ÄùÁöÑÊñ∞ËäÇÁÇπÔºåÂπ∂Â∞ÜËøô‰∏™Êñ∞ËäÇÁÇπ‰∏éÂõæ‰∏≠ÊâÄÊúâÂÖ∂‰ªñËäÇÁÇπËøûÊé•Ëµ∑Êù•„ÄÇ</li>
</ul>
<h5 id="Â∫èÂàóÊ®°ÂûãÁöÑÂàùÂßãÂåñ"><strong>Â∫èÂàóÊ®°ÂûãÁöÑÂàùÂßãÂåñ</strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequence (ESM) model initialization</span></span><br><span class="line">sequence_input = <span class="variable language_">self</span>.sequence_model.mapping[graph.residue_type]</span><br><span class="line">sequence_input[sequence_input == -<span class="number">1</span>] = graph.residue_type[sequence_input == -<span class="number">1</span>]</span><br><span class="line">size = graph.num_residues</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check if sequence size is not bigger than max seq length</span></span><br><span class="line"><span class="keyword">if</span> (size &gt; <span class="variable language_">self</span>.sequence_model.max_input_length).<span class="built_in">any</span>():</span><br><span class="line">    starts = size.cumsum(<span class="number">0</span>) - size</span><br><span class="line">    size = size.clamp(<span class="built_in">max</span>=<span class="variable language_">self</span>.sequence_model.max_input_length)</span><br><span class="line">    ends = starts + size</span><br><span class="line">    mask = functional.multi_slice_mask(starts, ends, graph.num_residues)</span><br><span class="line">    sequence_input = sequence_input[mask]</span><br><span class="line">    graph = graph.subresidue(mask)</span><br><span class="line">size_ext = size</span><br><span class="line"></span><br><span class="line"><span class="comment"># BOS == CLS</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.alphabet.prepend_bos:</span><br><span class="line">    bos = torch.ones(graph.batch_size, dtype=torch.long, device=<span class="variable language_">self</span>.sequence_model.device) * <span class="variable language_">self</span>.sequence_model.alphabet.cls_idx</span><br><span class="line">    sequence_input, size_ext = functional._extend(bos, torch.ones_like(size_ext), sequence_input, size_ext)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.alphabet.append_eos:</span><br><span class="line">    eos = torch.ones(graph.batch_size, dtype=torch.long, device=<span class="variable language_">self</span>.sequence_model.device) * <span class="variable language_">self</span>.sequence_model.alphabet.eos_idx</span><br><span class="line">    sequence_input, size_ext = functional._extend(sequence_input, size_ext, eos, torch.ones_like(size_ext))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Padding</span></span><br><span class="line">tokens = functional.variadic_to_padded(sequence_input, size_ext, value=<span class="variable language_">self</span>.sequence_model.alphabet.padding_idx)[<span class="number">0</span>]</span><br><span class="line">repr_layers = [<span class="variable language_">self</span>.sequence_model.repr_layer]</span><br><span class="line"><span class="keyword">assert</span> tokens.ndim == <span class="number">2</span></span><br><span class="line">padding_mask = tokens.eq(<span class="variable language_">self</span>.sequence_model.model.padding_idx)  <span class="comment"># B, T</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Â∫èÂàóÊï∞ÊçÆËøõË°å Transformer Ê®°ÂûãÔºàÂ¶Ç ESMÔºâÊâÄÈúÄÁöÑÊ†áÂáÜÈ¢ÑÂ§ÑÁêÜ„ÄÇ</li>
<li>ÂåÖÊã¨Ê∑ªÂä†Â∫èÂàóÂºÄÂßãÔºàBOSÔºâÂíåÁªìÊùüÔºàEOSÔºâÊ†áËÆ∞Ôºå‰ª•ÂèäÂ∞ÜÊâÄÊúâÂ∫èÂàóÂ°´ÂÖÖÔºàPaddingÔºâÂà∞Áõ∏ÂêåÈïøÂ∫¶Ôºå‰ª•‰æøËøõË°åÊâπÂ§ÑÁêÜ„ÄÇ</li>
</ul>
<h5 id="Ê®°ÂûãÂàùÂßãÂåñ‰∏éÂàùÊ¨°ËûçÂêà"><strong>Ê®°ÂûãÂàùÂßãÂåñ‰∏éÂàùÊ¨°ËûçÂêà</strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequence embedding layer</span></span><br><span class="line">x = <span class="variable language_">self</span>.sequence_model.model.embed_scale * <span class="variable language_">self</span>.sequence_model.model.embed_tokens(tokens)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.model.token_dropout:</span><br><span class="line">    x.masked_fill_((tokens == <span class="variable language_">self</span>.sequence_model.model.mask_idx).unsqueeze(-<span class="number">1</span>), <span class="number">0.0</span>)</span><br><span class="line">    <span class="comment"># x: B x T x C</span></span><br><span class="line">    mask_ratio_train = <span class="number">0.15</span> * <span class="number">0.8</span></span><br><span class="line">    src_lengths = (~padding_mask).<span class="built_in">sum</span>(-<span class="number">1</span>)</span><br><span class="line">    mask_ratio_observed = (tokens == <span class="variable language_">self</span>.sequence_model.model.mask_idx).<span class="built_in">sum</span>(-<span class="number">1</span>).to(x.dtype) / src_lengths</span><br><span class="line">    x = x * (<span class="number">1</span> - mask_ratio_train) / (<span class="number">1</span> - mask_ratio_observed)[:, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Structure model initialization</span></span><br><span class="line">structure_hiddens = []</span><br><span class="line">batch_size = graph.batch_size</span><br><span class="line">structure_embedding = <span class="variable language_">self</span>.embedding_batch_norm(<span class="variable language_">self</span>.structure_embed_linear(<span class="built_in">input</span>))</span><br><span class="line">structure_token_batched = <span class="variable language_">self</span>.structure_token.unsqueeze(<span class="number">0</span>).expand(batch_size, -<span class="number">1</span>)</span><br><span class="line">structure_input = torch.cat([structure_embedding.squeeze(<span class="number">1</span>), structure_token_batched], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the 3D token representation</span></span><br><span class="line">structure_token_expanded = <span class="variable language_">self</span>.structure_token.unsqueeze(<span class="number">0</span>).expand(x.size(<span class="number">0</span>), -<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">x = torch.cat((x[:, :-<span class="number">1</span>], structure_token_expanded, x[:, -<span class="number">1</span>:]), dim=<span class="number">1</span>)</span><br><span class="line">padding_mask = torch.cat([padding_mask[:, :-<span class="number">1</span>],</span><br><span class="line">                          torch.zeros(padding_mask.size(<span class="number">0</span>), <span class="number">1</span>).to(padding_mask), padding_mask[:, -<span class="number">1</span>:]], dim=<span class="number">1</span>)</span><br><span class="line">size_ext += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> padding_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    x = x * (<span class="number">1</span> - padding_mask.unsqueeze(-<span class="number">1</span>).type_as(x))</span><br><span class="line"></span><br><span class="line">repr_layers = <span class="built_in">set</span>(repr_layers)</span><br><span class="line">hidden_representations = &#123;&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="number">0</span> <span class="keyword">in</span> repr_layers:</span><br><span class="line">    hidden_representations[<span class="number">0</span>] = x</span><br><span class="line"></span><br><span class="line"><span class="comment"># (B, T, E) =&gt; (T, B, E)</span></span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> padding_mask.<span class="built_in">any</span>():</span><br><span class="line">    padding_mask = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Â∞Ü 3D ‰ª§ÁâåÊèíÂÖ•Â∫èÂàó„ÄÇ</strong>
<ol type="1">
<li>‰∏∫Â∫èÂàóÊï∞ÊçÆÁîüÊàêÂàùÂßãÁöÑËØçÂµåÂÖ•Ë°®Á§∫ <code>x</code>„ÄÇ</li>
<li>Â∞Ü <code>self.structure_token</code> ÁöÑÂàùÂßãÁä∂ÊÄÅÊèíÂÖ•Âà∞Â∫èÂàóÂµåÂÖ•
<code>x</code> ‰∏≠ÔºåÈÄöÂ∏∏ÊòØÊîæÂú®Â∫èÂàóÁªìÊùüÊ†áËÆ∞ÔºàEOSÔºâ‰πãÂâç„ÄÇ</li>
<li>Â∫èÂàóÊ®°ÂûãÁúãÂà∞ÁöÑËæìÂÖ•Â∫èÂàóÂèòÊàê‰∫Ü
<code>[BOS, ÊÆãÂü∫1, ÊÆãÂü∫2, ..., ÊÆãÂü∫N, **3D‰ª§Áâå**, EOS]</code>
ÁöÑÂΩ¢Âºè„ÄÇ</li>
</ol></li>
</ul>
<h5 id="ËûçÂêàÂæ™ÁéØ"><strong>ËûçÂêàÂæ™ÁéØ </strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> seq_layer_idx, seq_layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.sequence_model.model.layers):</span><br><span class="line">    x, attn = seq_layer(</span><br><span class="line">        x,</span><br><span class="line">        self_attn_padding_mask=padding_mask,</span><br><span class="line">        need_head_weights=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> (seq_layer_idx + <span class="number">1</span>) <span class="keyword">in</span> repr_layers:</span><br><span class="line">        hidden_representations[seq_layer_idx + <span class="number">1</span>] = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>Ê®°ÂûãÂºÄÂßãÈÄêÂ±ÇÈÅçÂéÜÂ∫èÂàóÊ®°ÂûãÁöÑÊâÄÊúâÂ±ÇÔºà‰æãÂ¶Ç Transformer
ÁöÑÁºñÁ†ÅÂô®Â±ÇÔºâ„ÄÇ<code>x</code> Âú®ÊØè‰∏ÄÂ±ÇÈÉΩ‰ºöË¢´Êõ¥Êñ∞„ÄÇ</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> seq_layer_idx &gt; <span class="number">0</span> <span class="keyword">and</span> seq_layer_idx % <span class="variable language_">self</span>.inject_step == <span class="number">0</span>:</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>‰ø°ÊÅØÊ≥®ÂÖ•ÁÇπ</strong>ÔºöÊØèÂΩìÂ±ÇÊï∞ÁöÑÁ¥¢ÂºïËÉΩË¢´
<code>inject_step</code> (Âç≥ 5) Êï¥Èô§Êó∂ÔºåÂ∞±Ëß¶Âèë‰∏ÄÊ¨°‰ø°ÊÅØ‰∫§Êç¢„ÄÇ</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. ‰ªéÂ∫èÂàó‰∏≠ÊèêÂèñ 3D ‰ª§ÁâåÁöÑË°®Á§∫</span></span><br><span class="line"><span class="keyword">if</span> structure_layer_index == <span class="number">0</span>:</span><br><span class="line">    structure_input = torch.cat((structure_input[:-<span class="number">1</span> * batch_size],  x[-<span class="number">2</span>, :, :]), dim=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    structure_input = torch.cat((structure_input[:-<span class="number">1</span> * batch_size],</span><br><span class="line">                                 <span class="variable language_">self</span>.seq_linears[structure_layer_index](x[-<span class="number">2</span>, :, :])), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Áî®ÁªìÊûÑÊ®°ÂûãÁöÑ‰∏ÄÂ±ÇÊù•Â§ÑÁêÜ</span></span><br><span class="line">hidden = <span class="variable language_">self</span>.structure_model.layers[structure_layer_index](new_graph, structure_input)</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.structure_model.short_cut <span class="keyword">and</span> hidden.shape == structure_input.shape:</span><br><span class="line">    hidden = hidden + structure_input</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.structure_model.batch_norm:</span><br><span class="line">    hidden = <span class="variable language_">self</span>.structure_model.batch_norms[structure_layer_index](hidden)</span><br><span class="line"></span><br><span class="line">structure_hiddens.append(hidden)</span><br><span class="line">structure_input = hidden</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Â∞ÜÊõ¥Êñ∞ÂêéÁöÑ 3D ‰ª§ÁâåË°®Á§∫ÊèíÂõûÂ∫èÂàó</span></span><br><span class="line">updated_structure_token = <span class="variable language_">self</span>.structure_linears[...](structure_input[-<span class="number">1</span> * batch_size:])</span><br><span class="line">x = torch.cat((x[:-<span class="number">2</span>, :, :], updated_structure_token.unsqueeze(<span class="number">0</span>), x[-<span class="number">1</span>:, :, :]), dim=<span class="number">0</span>)</span><br><span class="line">structure_layer_index += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>‰ø°ÊÅØÊµÅÁ®ã</strong>Ôºö
<ol type="1">
<li><strong>‰ªéÂ∫èÂàóÂà∞ÁªìÊûÑ</strong>ÔºöÊ®°Âûã‰ªéÂ∫èÂàóË°®Á§∫ <code>x</code>
‰∏≠ÊèêÂèñÂá∫‚Äú3D
‰ª§Áâå‚ÄùÁöÑÊúÄÊñ∞ÂêëÈáè„ÄÇËøô‰∏™ÂêëÈáèÊ≠§Êó∂Â∑≤ÁªèÂê∏Êî∂‰∫ÜÂâçÈù¢Âá†Â±ÇÂ∫èÂàóÊ®°ÂûãÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØ„ÄÇÁÑ∂ÂêéÔºåÈÄöËøáÔºà<code>seq_linears</code>ÔºâÂ∞ÜÂÖ∂ËΩ¨Êç¢ÂêéÔºåÊõ¥Êñ∞Âà∞ÁªìÊûÑÊ®°ÂûãÁöÑËæìÂÖ•‰∏≠„ÄÇ</li>
<li><strong>ÁªìÊûÑ‰ø°ÊÅØÂ§ÑÁêÜ</strong>ÔºöËøêË°å‰∏ÄÂ±ÇÁªìÊûÑÊ®°ÂûãÔºàGNNÔºâ„ÄÇGNN
Ê†πÊçÆÂõæÁöÑËøûÊé•ÂÖ≥Á≥ªÊõ¥Êñ∞ÊâÄÊúâËäÇÁÇπÁöÑË°®Á§∫ÔºåÂΩìÁÑ∂‰πüÂåÖÊã¨‚Äú3D
‰ª§Áâå‚ÄùËøô‰∏™ÁâπÊÆäËäÇÁÇπ„ÄÇ</li>
<li><strong>‰ªéÁªìÊûÑÂà∞Â∫èÂàó</strong>Ôºö‰ªé GNN ÁöÑËæìÂá∫‰∏≠ÔºåÂÜçÊ¨°ÊèêÂèñÂá∫‚Äú3D
‰ª§Áâå‚ÄùÁöÑÂêëÈáè„ÄÇËøô‰∏™ÂêëÈáèÂåÖÂê´Êõ¥Êñ∞ÂêéÁöÑÁªìÊûÑ‰ø°ÊÅØ„ÄÇÂÜçÈÄöËøáÔºà<code>structure_linears</code>ÔºâËΩ¨Êç¢ÂêéÔºåÊääÂÆÉ<strong>ÊèíÂõû</strong>Âà∞Â∫èÂàóË°®Á§∫
<code>x</code> ‰∏≠ÔºåÊõøÊç¢ÊéâÊóßÁöÑÁâàÊú¨„ÄÇ</li>
</ol></li>
</ul>
<p>Ëøô‰∏™Âæ™ÁéØ‰∏çÊñ≠ÈáçÂ§ç„ÄÇ</p>
<h5 id="ËæìÂá∫"><strong>ËæìÂá∫</strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Structural Output</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.structure_model.concat_hidden:</span><br><span class="line">    structure_node_feature = torch.cat(structure_hiddens, dim=-<span class="number">1</span>)[:-<span class="number">1</span> * batch_size]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    structure_node_feature = structure_hiddens[-<span class="number">1</span>][:-<span class="number">1</span> * batch_size]</span><br><span class="line"></span><br><span class="line">structure_graph_feature = <span class="variable language_">self</span>.structure_model.readout(graph, structure_node_feature)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sequence Output</span></span><br><span class="line">x = <span class="variable language_">self</span>.sequence_model.model.emb_layer_norm_after(x)</span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># (T, B, E) =&gt; (B, T, E)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># last hidden representation should have layer norm applied</span></span><br><span class="line"><span class="keyword">if</span> (seq_layer_idx + <span class="number">1</span>) <span class="keyword">in</span> repr_layers:</span><br><span class="line">    hidden_representations[seq_layer_idx + <span class="number">1</span>] = x</span><br><span class="line">x = <span class="variable language_">self</span>.sequence_model.model.lm_head(x)</span><br><span class="line"></span><br><span class="line">output = &#123;<span class="string">&quot;logits&quot;</span>: x, <span class="string">&quot;representations&quot;</span>: hidden_representations&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sequence (ESM) model outputs</span></span><br><span class="line">residue_feature = output[<span class="string">&quot;representations&quot;</span>][<span class="variable language_">self</span>.sequence_model.repr_layer]</span><br><span class="line">residue_feature = functional.padded_to_variadic(residue_feature, size_ext)</span><br><span class="line">starts = size_ext.cumsum(<span class="number">0</span>) - size_ext</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.alphabet.prepend_bos:</span><br><span class="line">    starts = starts + <span class="number">1</span></span><br><span class="line">ends = starts + size</span><br><span class="line">mask = functional.multi_slice_mask(starts, ends, <span class="built_in">len</span>(residue_feature))</span><br><span class="line">residue_feature = residue_feature[mask]</span><br><span class="line">graph_feature = <span class="variable language_">self</span>.sequence_model.readout(graph, residue_feature)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine both models outputs</span></span><br><span class="line">node_feature = torch.cat(...)</span><br><span class="line">graph_feature = torch.cat(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> &#123;<span class="string">&quot;graph_feature&quot;</span>: graph_feature, <span class="string">&quot;node_feature&quot;</span>: node_feature&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ÊèêÂèñËæìÂá∫</strong>ÔºöÂæ™ÁéØÁªìÊùüÂêéÔºåÂàÜÂà´‰ªé‰∏§‰∏™Ê®°Âûã‰∏≠ÊèêÂèñÊúÄÁªàÁöÑÁâπÂæÅË°®Á§∫„ÄÇ</li>
<li><strong>ËØªÂá∫ÔºàReadoutÔºâ</strong>Ôºö‰ΩøÁî®‰∏Ä‰∏™‚ÄúËØªÂá∫ÂáΩÊï∞‚ÄùÔºàÂ¶ÇÊ±ÇÂíåÊàñÂπ≥ÂùáÔºâÂ∞ÜËäÇÁÇπÁ∫ßÂà´ÁöÑÁâπÂæÅËÅöÂêàÊàê‰∏Ä‰∏™‰ª£Ë°®Êï¥‰∏™ËõãÁôΩË¥®ÁöÑÂõæÁ∫ßÂà´ÁâπÂæÅ„ÄÇ</li>
<li><strong>ÊúÄÁªàÁªÑÂêà</strong>ÔºöÂ∞ÜÊù•Ëá™Â∫èÂàóÊ®°ÂûãÂíåÁªìÊûÑÊ®°ÂûãÁöÑËäÇÁÇπÁâπÂæÅÔºà<code>node_feature</code>ÔºâÂíåÂõæÁâπÂæÅÔºà<code>graph_feature</code>ÔºâÂàÜÂà´ÊãºÊé•ÔºàconcatenateÔºâËµ∑Êù•„ÄÇ</li>
<li><strong>ËøîÂõûÁªìÊûú</strong>ÔºöËøîÂõû‰∏Ä‰∏™ÂåÖÂê´ÁªÑÂêàÂêéÁâπÂæÅÁöÑÂ≠óÂÖ∏ÔºåÂèØÁî®‰∫é‰∏ãÊ∏∏‰ªªÂä°ÔºàÂ¶ÇÂäüËÉΩÈ¢ÑÊµã„ÄÅÂ±ûÊÄßÂõûÂΩíÁ≠âÔºâ„ÄÇ</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/26/5120C4-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/26/5120C4-2/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">ÂèëË°®‰∫é</span>
              

              <time title="ÂàõÂª∫Êó∂Èó¥Ôºö2025-09-26 21:00:00 / ‰øÆÊîπÊó∂Èó¥Ôºö20:48:45" itemprop="dateCreated datePublished" datetime="2025-09-26T21:00:00+08:00">2025-09-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">ÂàÜÁ±ª‰∫é</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - ËÆ°ÁÆóËÉΩÊ∫êÊùêÊñôÂíåÁîµÂ≠êÁªìÊûÑÊ®°Êãü Lecture-4</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="monte-carlo-mc-method">1 Monte Carlo (MC) Method:</h2>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>:</li>
</ul>
<p>This whiteboard provides a concise but detailed overview of two
important and related simulation techniques in computational physics and
chemistry: the Metropolis Monte Carlo (MC) method and Hamiltonian (or
Hybrid) Monte Carlo (HMC). Here is a detailed breakdown of the concepts
presented.</p>
<h3 id="metropolis-monte-carlo-mc-method">1. Metropolis Monte Carlo (MC)
Method</h3>
<p>The heading ‚ÄúMetropolis MC method‚Äù introduces a foundational
algorithm in statistical mechanics. Metropolis Monte Carlo is a method
used to generate a sequence of states for a system, allowing for the
calculation of average properties. Â∑¶‰∏äËßíÁöÑËøô‰∏ÄÈÉ®ÂàÜ‰ªãÁªç‰∫ÜÂü∫Á°ÄÁöÑ
<strong>Metropolis Monte Carlo</strong>
ÁÆóÊ≥ï„ÄÇÂÆÉÊòØ‰∏ÄÁßçÁîüÊàêÁä∂ÊÄÅÂ∫èÂàóÁöÑÊñπÊ≥ïÔºå‰ΩøÂæóÂ§Ñ‰∫é‰ªª‰ΩïÁä∂ÊÄÅÁöÑÊ¶ÇÁéáÈÉΩÁ¨¶ÂêàÊúüÊúõÁöÑÊ¶ÇÁéáÂàÜÂ∏ÉÔºàÂú®Áâ©ÁêÜÂ≠¶‰∏≠ÈÄöÂ∏∏ÊòØÁéªÂ∞îÂÖπÊõºÂàÜÂ∏ÉÔºâ„ÄÇ</p>
<ul>
<li><strong>Conceptual Diagram:</strong> The small box with numbered
sites (0-5) and an arrow showing a move from state 0 to 2, and then to
3, illustrates a ‚Äúrandom walk.‚Äù In Metropolis MC, the system transitions
from one state to another by making small, random changes.
Â∞èÊñπÊ°Ü‰∏≠Ê†áÊúâÁºñÂè∑ÁöÑ‰ΩçÁÇπÔºà0-5ÔºâÔºåÁÆ≠Â§¥Ë°®Á§∫‰ªéÁä∂ÊÄÅ 0 Âà∞Áä∂ÊÄÅ 2ÔºåÂÜçÂà∞Áä∂ÊÄÅ 3
ÁöÑÁßªÂä®Ôºå‰ª£Ë°®‚ÄúÈöèÊú∫Ê∏∏Ëµ∞‚Äù„ÄÇÂú® Metropolis MC
‰∏≠ÔºåÁ≥ªÁªüÈÄöËøáËøõË°åÂæÆÂ∞èÁöÑÈöèÊú∫ÂèòÂåñ‰ªé‰∏Ä‰∏™Áä∂ÊÄÅËøáÊ∏°Âà∞Âè¶‰∏Ä‰∏™Áä∂ÊÄÅ„ÄÇ</li>
<li><strong>Random Number Generation:</strong> The notation
<code>rand t \in (0,1)</code> indicates the use of a random number <span
class="math inline">\(t\)</span> drawn from a uniform distribution
between 0 and 1. This is a core component of the algorithm, used to
decide whether to accept or reject a proposed new state. Á¨¶Âè∑
<code>rand t \in (0,1)</code> Ë°®Á§∫‰ΩøÁî®‰ªé 0 Âà∞ 1
‰πãÈó¥ÁöÑÂùáÂåÄÂàÜÂ∏É‰∏≠ÊäΩÂèñÁöÑÈöèÊú∫Êï∞ <span
class="math inline">\(t\)</span>„ÄÇËøôÊòØÁÆóÊ≥ïÁöÑÊ†∏ÂøÉÈÉ®ÂàÜÔºåÁî®‰∫éÂÜ≥ÂÆöÊòØÂê¶Êé•ÂèóÊàñÊãíÁªùÊèêËÆÆÁöÑÊñ∞Áä∂ÊÄÅ„ÄÇ</li>
<li><strong>Detailed Balance Condition:</strong> The equation <span
class="math inline">\(P_o T(o \to n) = P_n T(n \to o)\)</span> is the
principle of detailed balance. It states that in a system at
equilibrium, the probability of being in an old state (<span
class="math inline">\(o\)</span>) and transitioning to a new state
(<span class="math inline">\(n\)</span>) is equal to the probability of
being in the new state and transitioning back to the old one. This
condition is crucial because it ensures that the simulation will
eventually sample states according to their correct thermodynamic
probabilities (the Boltzmann distribution). ÊñπÁ®ã <span
class="math inline">\(P_o T(o \to n) = P_n T(n \to o)\)</span>
ÊòØËØ¶ÁªÜÂπ≥Ë°°ÁöÑÂéüÁêÜ„ÄÇÂÆÉÊåáÂá∫ÔºåÂú®Âπ≥Ë°°Á≥ªÁªü‰∏≠ÔºåÂ§Ñ‰∫éÊóßÁä∂ÊÄÅ (<span
class="math inline">\(o\)</span>) Âπ∂ËΩ¨Âèò‰∏∫Êñ∞Áä∂ÊÄÅ (<span
class="math inline">\(n\)</span>)
ÁöÑÊ¶ÇÁéáÁ≠â‰∫éÂ§Ñ‰∫éÊñ∞Áä∂ÊÄÅÂπ∂ËΩ¨ÂèòÂõûÊóßÁä∂ÊÄÅÁöÑÊ¶ÇÁéá„ÄÇÊ≠§Êù°‰ª∂Ëá≥ÂÖ≥‚Äã‚ÄãÈáçË¶ÅÔºåÂõ†‰∏∫ÂÆÉÁ°Æ‰øùÊ®°ÊãüÊúÄÁªàÂ∞ÜÊ†πÊçÆÊ≠£Á°ÆÁöÑÁÉ≠ÂäõÂ≠¶Ê¶ÇÁéáÔºàÁéªÂ∞îÂÖπÊõºÂàÜÂ∏ÉÔºâÂØπÁä∂ÊÄÅËøõË°åÈááÊ†∑„ÄÇ</li>
<li><strong>Acceptance Rate:</strong> The note <code>\sim 30\%?</code>
likely refers to the target <strong>acceptance rate</strong> for an
efficient Metropolis MC simulation. If new states are accepted too often
or too rarely, the exploration of the system‚Äôs possible configurations
is inefficient. While the famous optimal acceptance rate for certain
high-dimensional problems is around 23.4%, a range of 20-50% is often
considered effective. Ê≥®Èáä‚Äú30%Ôºü‚ÄùÊåáÁöÑÊòØÈ´òÊïà Metropolis
ËíôÁâπÂç°ÁΩóÊ®°ÊãüÁöÑÁõÆÊ†á<strong>Êé•ÂèóÁéá</strong>„ÄÇÂ¶ÇÊûúÊñ∞Áä∂ÊÄÅÊé•ÂèóËøá‰∫éÈ¢ëÁπÅÊàñËøá‰∫éÁ®ÄÂ∞ëÔºåÁ≥ªÁªüÂØπÂèØËÉΩÈÖçÁΩÆÁöÑÊé¢Á¥¢Â∞±‰ºöÂèòÂæó‰ΩéÊïà„ÄÇËôΩÁÑ∂Êüê‰∫õÈ´òÁª¥ÈóÆÈ¢òÁöÑÊúÄ‰Ω≥Êé•ÂèóÁéáÁ∫¶‰∏∫
23.4%Ôºå‰ΩÜÈÄöÂ∏∏ËÆ§‰∏∫ 20-50% ÁöÑËåÉÂõ¥ÊòØÊúâÊïàÁöÑ„ÄÇ</li>
</ul>
<h3 id="hamiltonian-hybrid-monte-carlo-hmc">2. Hamiltonian / Hybrid
Monte Carlo (HMC)</h3>
<p>The second topic, ‚ÄúHamiltonian/Hybrid MC (HMC),‚Äù is a more advanced
Monte Carlo method that uses principles from classical mechanics to
propose new states more intelligently than the simple random-walk
approach of the standard Metropolis method. This often leads to a much
higher acceptance rate and more efficient exploration of the state
space. Á¨¨‰∫å‰∏™‰∏ªÈ¢ò‚ÄúÂìàÂØÜÈ°ø/Ê∑∑ÂêàËíôÁâπÂç°ÁΩó
(HMC)‚ÄùÊòØ‰∏ÄÁßçÊõ¥ÂÖàËøõÁöÑËíôÁâπÂç°ÁΩóÊñπÊ≥ïÔºåÂÆÉÂà©Áî®ÁªèÂÖ∏ÂäõÂ≠¶ÂéüÁêÜÔºåÊØîÊ†áÂáÜ Metropolis
ÊñπÊ≥ï‰∏≠ÁÆÄÂçïÁöÑÈöèÊú∫Ê∏∏Ëµ∞ÊñπÊ≥ïÊõ¥Êô∫ËÉΩÂú∞ÊèêÂá∫Êñ∞Áä∂ÊÄÅ„ÄÇËøôÈÄöÂ∏∏‰ºöÂ∏¶Êù•Êõ¥È´òÁöÑÊé•ÂèóÁéáÂíåÊõ¥È´òÊïàÁöÑÁä∂ÊÄÅÁ©∫Èó¥Êé¢Á¥¢„ÄÇ</p>
<p>The whiteboard outlines a four-step HMC algorithm:</p>
<p><strong>Step 1: Randomize Velocities</strong> The first step is to
randomize the velocities: <span class="math inline">\(\vec{v}_i \sim
\mathcal{N}(0, k_B T)\)</span>. Á¨¨‰∏ÄÊ≠•ÊòØÈöèÊú∫ÂåñÈÄüÂ∫¶Ôºö<span
class="math inline">\(\vec{v}_i \sim \mathcal{N}(0, k_B T)\)</span>„ÄÇ *
This step introduces momentum into the system. For each particle <span
class="math inline">\(i\)</span>, a velocity vector <span
class="math inline">\(\vec{v}_i\)</span> is randomly drawn from a normal
(Gaussian) distribution with a mean of 0 and a variance related to the
temperature <span class="math inline">\(T\)</span> and the Boltzmann
constant <span class="math inline">\(k_B\)</span>.
Ê≠§Ê≠•È™§Â∞ÜÂä®ÈáèÂºïÂÖ•Á≥ªÁªü„ÄÇÂØπ‰∫éÊØè‰∏™Á≤íÂ≠ê <span
class="math inline">\(i\)</span>ÔºåÈÄüÂ∫¶Áü¢Èáè <span
class="math inline">\(\vec{v}_i\)</span>
‰ºöÈöèÊú∫Âú∞‰ªéÊ≠£ÊÄÅÔºàÈ´òÊñØÔºâÂàÜÂ∏É‰∏≠ÊäΩÂèñÔºåËØ•ÂàÜÂ∏ÉÁöÑÂùáÂÄº‰∏∫ 0ÔºåÊñπÂ∑Æ‰∏éÊ∏©Â∫¶ <span
class="math inline">\(T\)</span> ÂíåÁéªÂ∞îÂÖπÊõºÂ∏∏Êï∞ <span
class="math inline">\(k_B\)</span> Áõ∏ÂÖ≥„ÄÇ * The full formula for this
probability distribution, <span
class="math inline">\(f(\vec{v})\)</span>, is the
<strong>Maxwell-Boltzmann distribution</strong>, which is written out
further down the board. ËØ•Ê¶ÇÁéáÂàÜÂ∏ÉÁöÑÂÆåÊï¥ÂÖ¨Âºè <span
class="math inline">\(f(\vec{v})\)</span>
ÊòØ<strong>È∫¶ÂÖãÊñØÈü¶-ÁéªÂ∞îÂÖπÊõºÂàÜÂ∏É</strong>„ÄÇ</p>
<p><strong>Step 2: Molecular Dynamics (MD) Integration</strong> The
board notes this as <code>t=0 \to h \text&#123; or &#125; mh</code>
<code>MD</code> and mentions the <code>Verlet</code> algorithm.</p>
<ul>
<li>This is the ‚ÄúHamiltonian dynamics‚Äù part of the algorithm. Starting
from the current positions and the newly randomized velocities, the
system‚Äôs trajectory is calculated for a short period of time (<span
class="math inline">\(h\)</span> or <span
class="math inline">\(mh\)</span>) using Molecular Dynamics (MD).
ËøôÊòØÁÆóÊ≥ïÁöÑ‚ÄúÂìàÂØÜÈ°øÂä®ÂäõÂ≠¶‚ÄùÈÉ®ÂàÜ„ÄÇ‰ªéÂΩìÂâç‰ΩçÁΩÆÂíåÊñ∞ÈöèÊú∫ÂåñÁöÑÈÄüÂ∫¶ÂºÄÂßãÔºå‰ΩøÁî®ÂàÜÂ≠êÂä®ÂäõÂ≠¶
(MD) ËÆ°ÁÆóÁ≥ªÁªüÂú®Áü≠Êó∂Èó¥ÂÜÖÔºà<span class="math inline">\(h\)</span> Êàñ <span
class="math inline">\(mh\)</span>ÔºâÁöÑËΩ®Ëøπ„ÄÇ</li>
<li>The name <strong>Verlet</strong> refers to the Verlet integration
algorithm, a numerical method used to solve Newton‚Äôs equations of
motion. It is popular in MD simulations because it is time-reversible
and conserves energy well over long simulations. ÊåáÁöÑÊòØ Verlet
ÁßØÂàÜÁÆóÊ≥ïÔºåËøôÊòØ‰∏ÄÁßçÁî®‰∫éÊ±ÇËß£ÁâõÈ°øËøêÂä®ÊñπÁ®ãÁöÑÊï∞ÂÄºÊñπÊ≥ï„ÄÇÂÆÉÂú® MD
Ê®°Êãü‰∏≠ÂæàÂèóÊ¨¢ËøéÔºåÂõ†‰∏∫ÂÆÉÂÖ∑ÊúâÊó∂Èó¥ÂèØÈÄÜÊÄßÔºåÂπ∂‰∏îÂú®ÈïøÊó∂Èó¥Ê®°Êãü‰∏≠ËÉΩÈáèÂÆàÊÅíÊïàÊûúËâØÂ•Ω„ÄÇ</li>
</ul>
<p><strong>Step 3: Calculate Total Energy</strong> The third step is to
<code>calculate total energy</code>: <span class="math inline">\(E_n =
K_n + V_n\)</span>. Á¨¨‰∏âÊ≠•ÊòØ‚ÄúËÆ°ÁÆóÊÄªËÉΩÈáè‚ÄùÔºö<span
class="math inline">\(E_n = K_n + V_n\)</span>„ÄÇ * After the MD
trajectory, the system is in a new state <span
class="math inline">\(n\)</span>. The total energy of this new state,
<span class="math inline">\(E_n\)</span>, is calculated as the sum of
its kinetic energy (<span class="math inline">\(K_n\)</span>, from the
velocities) and its potential energy (<span
class="math inline">\(V_n\)</span>, from the positions). MD
ËΩ®Ëøπ‰πãÂêéÔºåÁ≥ªÁªüÂ§Ñ‰∫éÊñ∞Áä∂ÊÄÅ <span
class="math inline">\(n\)</span>„ÄÇÊñ∞Áä∂ÊÄÅÁöÑÊÄªËÉΩÈáè <span
class="math inline">\(E_n\)</span> Á≠â‰∫éÂÖ∂Âä®ËÉΩ (<span
class="math inline">\(K_n\)</span>ÔºåÁî±ÈÄüÂ∫¶ËÆ°ÁÆóÂæóÂá∫ÔºâÂíåÂäøËÉΩ (<span
class="math inline">\(V_n\)</span>ÔºåÁî±‰ΩçÁΩÆËÆ°ÁÆóÂæóÂá∫)‰πãÂíå„ÄÇ</p>
<p><strong>Step 4: Acceptance Test</strong> The final step is the
acceptance criterion: <span class="math inline">\(\text{acc}(o \to n) =
\min(1, e^{-\beta(E_n - E_o)})\)</span>. ÊúÄÂêé‰∏ÄÊ≠•ÊòØÈ™åÊî∂Ê†áÂáÜÔºö<span
class="math inline">\(\text{acc}(o \to n) = \min(1, e^{-\beta(E_n -
E_o)})\)</span>„ÄÇ * This is the Metropolis acceptance criterion. The
algorithm decides whether to accept the new state <span
class="math inline">\(n\)</span> or reject it and stay in the old state
<span class="math inline">\(o\)</span>. ËøôÊòØ Metropolis
È™åÊî∂Ê†áÂáÜ„ÄÇÁÆóÊ≥ïÂÜ≥ÂÆöÊòØÊé•ÂèóÊñ∞Áä∂ÊÄÅ <span class="math inline">\(n\)</span>
ËøòÊòØÊãíÁªùÂÆÉÂπ∂‰øùÊåÅÊóßÁä∂ÊÄÅ <span class="math inline">\(o\)</span>„ÄÇ * The
probability of acceptance depends on the change in total energy (<span
class="math inline">\(E_n - E_o\)</span>). If the new energy is lower,
the move is always accepted. If the new energy is higher, it might still
be accepted with a probability <span class="math inline">\(e^{-\beta(E_n
- E_o)}\)</span>, where <span class="math inline">\(\beta = 1/(k_B
T)\)</span>. This allows the system to escape from local energy minima.
È™åÊî∂Ê¶ÇÁéáÂèñÂÜ≥‰∫éÊÄªËÉΩÈáèÁöÑÂèòÂåñ (<span class="math inline">\(E_n -
E_o\)</span>)„ÄÇÂ¶ÇÊûúÊñ∞ËÉΩÈáèËæÉ‰ΩéÔºåÂàôÂßãÁªàÊé•ÂèóËØ•ÁßªÂä®„ÄÇÂ¶ÇÊûúÊñ∞ÁöÑËÉΩÈáèÊõ¥È´òÔºåÂÆÉ‰ªçÁÑ∂ÂèØËÉΩ‰ª•Ê¶ÇÁéá
<span class="math inline">\(e^{-\beta(E_n - E_o)}\)</span> Ë¢´Êé•ÂèóÔºåÂÖ∂‰∏≠
<span class="math inline">\(\beta = 1/(k_B
T)\)</span>„ÄÇËøô‰ΩøÂæóÁ≥ªÁªüËÉΩÂ§üÊëÜËÑ±Â±ÄÈÉ®ËÉΩÈáèÊúÄÂ∞èÂÄº„ÄÇ</p>
<h3 id="key-formulas-and-notations">Key Formulas and Notations</h3>
<ul>
<li><p><strong>Maxwell-Boltzmann
DistributionÈ∫¶ÂÖãÊñØÈü¶-ÁéªÂ∞îÂÖπÊõºÂàÜÂ∏É:</strong> The formula for the velocity
distribution is given as: <span class="math inline">\(f(\vec{v}) =
\left(\frac{m}{2\pi k_B T}\right)^{3/2} \exp\left(-\frac{m v^2}{2 k_B
T}\right)\)</span> This gives the probability density for a particle of
mass <span class="math inline">\(m\)</span> to have a velocity <span
class="math inline">\(\vec{v}\)</span> at a given temperature <span
class="math inline">\(T\)</span>.Ë¥®Èáè‰∏∫ <span
class="math inline">\(m\)</span> ÁöÑÁ≤íÂ≠êÈÄüÂ∫¶‰∏∫ ÁöÑÊ¶ÇÁéáÂØÜÂ∫¶</p></li>
<li><p><strong>Energy Conservation and Acceptance Rate:</strong> The
notes <span class="math inline">\(E_n \approx E_o\)</span> and <span
class="math inline">\(75\%\)</span> highlight a key advantage of HMC.
Because the Verlet integrator approximately conserves energy, the final
energy <span class="math inline">\(E_n\)</span> after the MD trajectory
is usually very close to the initial energy <span
class="math inline">\(E_o\)</span>. This means the term <span
class="math inline">\((E_n - E_o)\)</span> is small, and the acceptance
probability is high. The <span class="math inline">\(75\%\)</span>
indicates a typical or target acceptance rate for HMC, which is
significantly higher than for standard Metropolis MC. Ê≥®Èáä <span
class="math inline">\(E_n \approx E_o\)</span> Âíå <span
class="math inline">\(75\%\)</span> Âá∏Êòæ‰∫Ü HMC ÁöÑ‰∏Ä‰∏™ÂÖ≥ÈîÆ‰ºòÂäø„ÄÇÁî±‰∫é
Verlet ÁßØÂàÜÂô®Ëøë‰ººÂú∞ÂÆàÊÅíËÉΩÈáèÔºåMD ËΩ®ËøπÂêéÁöÑÊúÄÁªàËÉΩÈáè <span
class="math inline">\(E_n\)</span> ÈÄöÂ∏∏ÈùûÂ∏∏Êé•ËøëÂàùÂßãËÉΩÈáè <span
class="math inline">\(E_o\)</span>„ÄÇËøôÊÑèÂë≥ÁùÄ <span
class="math inline">\((E_n - E_o)\)</span> È°πÂæàÂ∞èÔºåÊé•ÂèóÊ¶ÇÁéáÂæàÈ´ò„ÄÇ<span
class="math inline">\(75\%\)</span> Ë°®Á§∫ HMC
ÁöÑÂÖ∏ÂûãÊàñÁõÆÊ†áÊé•ÂèóÁéáÔºåÊòéÊòæÈ´ò‰∫éÊ†áÂáÜ Metropolis MC„ÄÇ</p></li>
<li><p><strong>Hamiltonian Operator:</strong> The symbol <span
class="math inline">\(\hat{H}\)</span> written on the adjacent board
represents the Hamiltonian operator, which gives the total energy of the
system. The note <code>Œî Adiabatic</code> suggests that the MD evolution
is ideally an adiabatic process (no heat exchange), during which the
total energy (the Hamiltonian) is conserved. Áõ∏ÈÇªÊùø‰∏äÁöÑÁ¨¶Âè∑ <span
class="math inline">\(\hat{H}\)</span>
‰ª£Ë°®ÂìàÂØÜÈ°øÁÆóÁ¨¶ÔºåÂÆÉÁªôÂá∫‰∫ÜÁ≥ªÁªüÁöÑÊÄªËÉΩÈáè„ÄÇÊ≥®Èáä‚ÄúŒî Adiabatic‚ÄùË°®Êòé MD
ÊºîÂåñÂú®ÁêÜÊÉ≥ÊÉÖÂÜµ‰∏ãÊòØ‰∏Ä‰∏™ÁªùÁÉ≠ËøáÁ®ãÔºàÊó†ÁÉ≠‰∫§Êç¢ÔºâÔºåÂú®Ê≠§ËøáÁ®ã‰∏≠ÊÄªËÉΩÈáèÔºàÂìàÂØÜÈ°øÈáèÔºâÂÆàÊÅí„ÄÇ</p></li>
</ul>
<p>This whiteboard displays the fundamental equation of quantum
chemistry: the time-dependent Schr√∂dinger equation, along with the
detailed breakdown of the molecular Hamiltonian operator. This equation
is the starting point for almost all <em>ab initio</em>
(first-principles) quantum mechanical calculations of molecular systems.
ËøôÂùóÁôΩÊùøÂ±ïÁ§∫‰∫ÜÈáèÂ≠êÂåñÂ≠¶ÁöÑÂü∫Êú¨ÊñπÁ®ãÔºöÂê´Êó∂ËñõÂÆöË∞îÊñπÁ®ãÔºå‰ª•ÂèäÂàÜÂ≠êÂìàÂØÜÈ°øÁÆóÁ¨¶ÁöÑËØ¶ÁªÜÂàÜËß£„ÄÇËØ•ÊñπÁ®ãÊòØÂá†‰πéÊâÄÊúâÂàÜÂ≠êÁ≥ªÁªü<em>‰ªéÂ§¥ÁÆó</em>ÔºàÁ¨¨‰∏ÄÊÄßÂéüÁêÜÔºâÈáèÂ≠êÂäõÂ≠¶ËÆ°ÁÆóÁöÑËµ∑ÁÇπ„ÄÇ</p>
<h3 id="the-time-dependent-schr√∂dinger-equation">3. The Time-Dependent
Schr√∂dinger Equation</h3>
<p>At the top of the board, the fundamental equation governing the
evolution of a quantum mechanical system is presented:
ÁôΩÊùøÈ°∂ÈÉ®ÊòæÁ§∫‰∫ÜÊéßÂà∂ÈáèÂ≠êÂäõÂ≠¶Á≥ªÁªüÊºîÂåñÁöÑÂü∫Êú¨ÊñπÁ®ãÔºö <span
class="math inline">\(i\hbar \frac{\partial \Psi}{\partial t} =
\hat{\mathcal{H}} \Psi\)</span></p>
<ul>
<li><p><strong><span class="math inline">\(\Psi\)</span> (Psi)</strong>
is the <strong>wave function</strong> of the system. It contains all the
information that can be known about the system (e.g., the positions and
momenta of all particles).
ÊòØÁ≥ªÁªüÁöÑ<strong>Ê≥¢ÂáΩÊï∞</strong>„ÄÇÂÆÉÂåÖÂê´‰∫ÜÂÖ≥‰∫éÁ≥ªÁªüÁöÑÊâÄÊúâÂ∑≤Áü•‰ø°ÊÅØÔºà‰æãÂ¶ÇÔºåÊâÄÊúâÁ≤íÂ≠êÁöÑ‰ΩçÁΩÆÂíåÂä®ÈáèÔºâ„ÄÇ</p></li>
<li><p><strong><span
class="math inline">\(\hat{\mathcal{H}}\)</span></strong> is the
<strong>Hamiltonian operator</strong>, which represents the total energy
of the system.
ÊòØ<strong>ÂìàÂØÜÈ°øÁÆóÁ¨¶</strong>ÔºåË°®Á§∫Á≥ªÁªüÁöÑÊÄªËÉΩÈáè„ÄÇ</p></li>
<li><p><strong><span class="math inline">\(i\)</span></strong>
ÊòØËôöÊï∞Âçï‰Ωç„ÄÇ</p></li>
<li><p><strong><span class="math inline">\(i\)</span></strong> is the
imaginary unit.</p></li>
<li><p><strong><span class="math inline">\(\hbar\)</span></strong> is
the <strong>reduced Planck
constant</strong>.ÊòØ<strong>Á∫¶ÂåñÊôÆÊúóÂÖãÂ∏∏Êï∞</strong>„ÄÇ</p></li>
<li><p><strong><span class="math inline">\(\frac{\partial \Psi}{\partial
t}\)</span></strong> represents how the wave function changes over
time.Ë°®Á§∫Ê≥¢ÂáΩÊï∞ÈöèÊó∂Èó¥ÁöÑÂèòÂåñ„ÄÇ</p></li>
</ul>
<p>This equation states that the time evolution of the quantum state is
dictated by the system‚Äôs total energy operator, the Hamiltonian. The
note ‚ÄúŒî Adiabatic process‚Äù likely connects to the context of the
Born-Oppenheimer approximation, where the electronic Schr√∂dinger
equation is solved for fixed nuclear positions, assuming the electrons
adjust adiabatically (instantaneously) to the motion of the nuclei.
ËØ•ÊñπÁ®ãË°®ÊòéÔºåÈáèÂ≠êÊÄÅÁöÑÊó∂Èó¥ÊºîÂåñÁî±Á≥ªÁªüÁöÑÊÄªËÉΩÈáèÁÆóÁ¨¶‚Äî‚ÄîÂìàÂØÜÈ°øÁÆóÁ¨¶ÂÜ≥ÂÆö„ÄÇÊ≥®Èáä‚ÄúŒîÁªùÁÉ≠ËøáÁ®ã‚Äù‰∏éÁéªÊÅ©-Â••Êú¨Êµ∑ÈªòËøë‰ººÁõ∏ÂÖ≥ÔºåÂú®ËØ•Ëøë‰ºº‰∏≠ÔºåÁîµÂ≠êËñõÂÆöË∞îÊñπÁ®ãÊòØÈíàÂØπÂõ∫ÂÆöÂéüÂ≠êÊ†∏‰ΩçÁΩÆÊ±ÇËß£ÁöÑÔºåÂÅáËÆæÁîµÂ≠ê‰ª•ÁªùÁÉ≠ÊñπÂºèÔºàÁû¨Êó∂ÔºâË∞ÉÊï¥‰ª•ÈÄÇÂ∫îÂéüÂ≠êÊ†∏ÁöÑËøêÂä®„ÄÇ</p>
<h3 id="the-full-molecular-hamiltonian-hatmathcalh">4. The Full
Molecular Hamiltonian (<span
class="math inline">\(\hat{\mathcal{H}}\)</span>)</h3>
<p>The main part of the whiteboard is the detailed expression for the
non-relativistic, time-independent molecular Hamiltonian. It is the sum
of the kinetic and potential energies of all the nuclei and electrons in
the system. The equation can be broken down into five distinct terms:
ÁôΩÊùøÁöÑ‰∏ªË¶ÅÈÉ®ÂàÜÊòØÈùûÁõ∏ÂØπËÆ∫ÊÄß„ÄÅÊó∂Èó¥Êó†ÂÖ≥ÁöÑÂàÜÂ≠êÂìàÂØÜÈ°øÈáèÁöÑËØ¶ÁªÜË°®ËææÂºè„ÄÇÂÆÉÊòØÁ≥ªÁªü‰∏≠ÊâÄÊúâÂéüÂ≠êÊ†∏ÂíåÁîµÂ≠êÁöÑÂä®ËÉΩÂíåÂäøËÉΩ‰πãÂíå„ÄÇ</p>
<p>ËØ•ÊñπÁ®ãÂèØ‰ª•ÂàÜËß£‰∏∫‰∫î‰∏™‰∏çÂêåÁöÑÈ°πÔºö</p>
<p><span class="math inline">\(\hat{\mathcal{H}} = -\sum_{I=1}^{P}
\frac{\hbar^2}{2M_I}\nabla_I^2 - \sum_{i=1}^{N}
\frac{\hbar^2}{2m}\nabla_i^2 + \frac{e^2}{2}\sum_{I=1}^{P}\sum_{J \neq
I}^{P} \frac{Z_I Z_J}{|\vec{R}_I - \vec{R}_J|} +
\frac{e^2}{2}\sum_{i=1}^{N}\sum_{j \neq i}^{N} \frac{1}{|\vec{r}_i -
\vec{r}_j|} - e^2\sum_{I=1}^{P}\sum_{i=1}^{N} \frac{Z_I}{|\vec{R}_I -
\vec{r}_i|}\)</span></p>
<p>Let‚Äôs analyze each component:</p>
<p><strong>A. Kinetic Energy Terms Âä®ËÉΩÈ°π</strong></p>
<ol type="1">
<li><strong>Kinetic Energy of the Nuclei ÂéüÂ≠êÊ†∏ÁöÑÂä®ËÉΩ:</strong> <span
class="math inline">\(-\sum_{I=1}^{P}
\frac{\hbar^2}{2M_I}\nabla_I^2\)</span> This term is the sum of the
kinetic energy operators for all the nuclei in the
system.Ê≠§È°πÊòØÁ≥ªÁªü‰∏≠ÊâÄÊúâÂéüÂ≠êÊ†∏ÁöÑÂä®ËÉΩÁÆóÁ¨¶‰πãÂíå„ÄÇ
<ul>
<li>The sum is over all nuclei, indexed by <span
class="math inline">\(I\)</span> from 1 to <span
class="math inline">\(P\)</span>.ËØ•ÂíåÊ∂µÁõñÊâÄÊúâÂéüÂ≠êÊ†∏ÔºåÁ¥¢Âºï‰∏∫ <span
class="math inline">\(I\)</span>Ôºå‰ªé 1 Âà∞ <span
class="math inline">\(P\)</span>„ÄÇ</li>
<li><span class="math inline">\(M_I\)</span> is the mass of nucleus
<span class="math inline">\(I\)</span>.ÊòØÂéüÂ≠êÊ†∏ <span
class="math inline">\(I\)</span> ÁöÑË¥®Èáè„ÄÇ</li>
<li><span class="math inline">\(\nabla_I^2\)</span> is the Laplacian
operator, which involves the second spatial derivatives with respect to
the coordinates of nucleus <span
class="math inline">\(I\)</span>.ÊòØÊãâÊôÆÊãâÊñØÁÆóÁ¨¶ÔºåÂÆÉÊ∂âÂèäÂéüÂ≠êÊ†∏ <span
class="math inline">\(I\)</span> ÂùêÊ†áÁöÑ‰∫åÈò∂Á©∫Èó¥ÂØºÊï∞„ÄÇ</li>
</ul></li>
<li><strong>Kinetic Energy of the Electrons ÁîµÂ≠êÁöÑÂä®ËÉΩ:</strong> <span
class="math inline">\(-\sum_{i=1}^{N}
\frac{\hbar^2}{2m}\nabla_i^2\)</span> This is the corresponding sum of
the kinetic energy operators for all the
electrons.ËøôÊòØÊâÄÊúâÁîµÂ≠êÁöÑÂä®ËÉΩÁÆóÁ¨¶ÁöÑÂØπÂ∫îÂíå„ÄÇ
<ul>
<li>The sum is over all electrons, indexed by <span
class="math inline">\(i\)</span> from 1 to <span
class="math inline">\(N\)</span>.ËØ•ÂíåÊòØÈíàÂØπÊâÄÊúâÁîµÂ≠êÁöÑÔºåÁ¥¢Âºï‰∏∫ <span
class="math inline">\(i\)</span>Ôºå‰ªé 1 Âà∞ <span
class="math inline">\(N\)</span>„ÄÇ</li>
<li><span class="math inline">\(m\)</span> is the mass of an
electron.ÊòØÁîµÂ≠êÁöÑË¥®Èáè„ÄÇ</li>
<li><span class="math inline">\(\nabla_i^2\)</span> is the Laplacian
operator with respect to the coordinates of electron <span
class="math inline">\(i\)</span>.ÊòØÂÖ≥‰∫éÁîµÂ≠ê <span
class="math inline">\(i\)</span> ÂùêÊ†áÁöÑÊãâÊôÆÊãâÊñØÁÆóÁ¨¶„ÄÇ</li>
</ul></li>
</ol>
<p><strong>B. Potential Energy Terms (Electrostatic Interactions)
ÂäøËÉΩÈ°πÔºàÈùôÁîµÁõ∏‰∫í‰ΩúÁî®Ôºâ</strong></p>
<ol start="3" type="1">
<li><strong>Nuclear-Nuclear Repulsion Ê†∏Èó¥ÊéíÊñ•Âäõ:</strong> <span
class="math inline">\(+\frac{e^2}{2}\sum_{I=1}^{P}\sum_{J \neq I}^{P}
\frac{Z_I Z_J}{|\vec{R}_I - \vec{R}_J|}\)</span> This term represents
the potential energy from the electrostatic (Coulomb) repulsion between
all pairs of positively charged
nuclei.ËØ•È°πË°®Á§∫ÊâÄÊúâÂ∏¶Ê≠£ÁîµÂéüÂ≠êÊ†∏ÂØπ‰πãÈó¥ÈùôÁîµÔºàÂ∫ì‰ªëÔºâÊéíÊñ•Âäõ‰∫ßÁîüÁöÑÂäøËÉΩ„ÄÇ
<ul>
<li>The double summation runs over all unique pairs of nuclei (<span
class="math inline">\(I, J\)</span>).ÂØπÊâÄÊúâÂîØ‰∏ÄÁöÑÂéüÂ≠êÊ†∏ÂØπ (<span
class="math inline">\(I, J\)</span>) ËøõË°åÂèåÈáçÊ±ÇÂíå„ÄÇ</li>
<li><span class="math inline">\(Z_I\)</span> is the atomic number (i.e.,
the charge) of nucleus <span class="math inline">\(I\)</span>.ÊòØÂéüÂ≠êÊ†∏
<span class="math inline">\(I\)</span> ÁöÑÂéüÂ≠êÂ∫èÊï∞ÔºàÂç≥ÁîµËç∑Ôºâ„ÄÇ</li>
<li><span class="math inline">\(\vec{R}_I\)</span> is the position
vector of nucleus <span class="math inline">\(I\)</span>.ÊòØÂéüÂ≠êÊ†∏ <span
class="math inline">\(I\)</span> ÁöÑ‰ΩçÁΩÆÁü¢Èáè„ÄÇ</li>
<li><span class="math inline">\(e\)</span> is the elementary
charge.ÊòØÂü∫Êú¨ÁîµËç∑„ÄÇ</li>
</ul></li>
<li><strong>Electron-Electron Repulsion ÁîµÂ≠êÈó¥ÊéíÊñ•Âäõ:</strong> <span
class="math inline">\(+\frac{e^2}{2}\sum_{i=1}^{N}\sum_{j \neq i}^{N}
\frac{1}{|\vec{r}_i - \vec{r}_j|}\)</span> This term represents the
potential energy from the electrostatic repulsion between all pairs of
negatively charged
electrons.ËØ•È°πË°®Á§∫ÊâÄÊúâÂ∏¶Ë¥üÁîµÁöÑÁîµÂ≠êÂØπ‰πãÈó¥ÈùôÁîµÊéíÊñ•ÁöÑÂäøËÉΩ„ÄÇ
<ul>
<li>The double summation runs over all unique pairs of electrons (<span
class="math inline">\(i, j\)</span>).ÂØπÊâÄÊúâ‰∏çÂêåÁöÑÁîµÂ≠êÂØπ (<span
class="math inline">\(i, j\)</span>) ËøõË°åÂèåÈáçÊ±ÇÂíå„ÄÇ</li>
<li><span class="math inline">\(\vec{r}_i\)</span> is the position
vector of electron <span class="math inline">\(i\)</span>.ÊòØÁîµÂ≠ê <span
class="math inline">\(i\)</span> ÁöÑ‰ΩçÁΩÆÁü¢Èáè„ÄÇ</li>
</ul></li>
<li><strong>Nuclear-Electron Attraction Ê†∏-ÁîµÂ≠êÂºïÂäõ:</strong> <span
class="math inline">\(-e^2\sum_{I=1}^{P}\sum_{i=1}^{N}
\frac{Z_I}{|\vec{R}_I - \vec{r}_i|}\)</span> This final term represents
the potential energy from the electrostatic attraction between the
nuclei and the electrons.ËøôÊúÄÂêé‰∏ÄÈ°πË°®Á§∫ÂéüÂ≠êÊ†∏ÂíåÁîµÂ≠ê‰πãÈó¥ÈùôÁîµÂºïÂäõÁöÑÂäøËÉΩ„ÄÇ
<ul>
<li>The summation runs over all nuclei and all
electrons.ËØ•Ê±ÇÂíåÈÄÇÁî®‰∫éÊâÄÊúâÂéüÂ≠êÊ†∏ÂíåÊâÄÊúâÁîµÂ≠ê„ÄÇ</li>
</ul></li>
</ol>
<h3 id="notations-and-conventions">5. Notations and Conventions</h3>
<ul>
<li><strong>Atomic Units:</strong> The note <span
class="math inline">\(\frac{1}{4\pi\epsilon_0} = k = 1\)</span> is a key
indicator of the convention being used. This sets the Coulomb constant
to 1, which is a hallmark of <strong>Hartree atomic units</strong>. In
this system, the elementary charge (<span
class="math inline">\(e\)</span>), electron mass (<span
class="math inline">\(m\)</span>), and reduced Planck constant (<span
class="math inline">\(\hbar\)</span>) are also set to 1. This simplifies
the Hamiltonian significantly, removing the physical constants and
making the equations easier to work with computationally.
ÊòØÊâÄÁî®Á∫¶ÂÆöÁöÑÂÖ≥ÈîÆÊåáÊ†á„ÄÇËøôÂ∞ÜÂ∫ì‰ªëÂ∏∏Êï∞ËÆæÁΩÆ‰∏∫ 1ÔºåËøôÊòØ<strong>Hartree
ÂéüÂ≠êÂçï‰Ωç</strong>ÁöÑÊ†áÂøó„ÄÇÂú®Ëøô‰∏™Á≥ªÁªü‰∏≠ÔºåÂü∫Êú¨ÁîµËç∑ (<span
class="math inline">\(e\)</span>)„ÄÅÁîµÂ≠êË¥®Èáè (<span
class="math inline">\(m\)</span>) Âíå‚Äã‚ÄãÁ∫¶ÂåñÊôÆÊúóÂÖãÂ∏∏Êï∞ (<span
class="math inline">\(\hbar\)</span>) ‰πüËÆæ‰∏∫
1„ÄÇËøôÊòæËëóÁÆÄÂåñ‰∫ÜÂìàÂØÜÈ°øÈáèÔºåÊ∂àÈô§‰∫ÜÁâ©ÁêÜÂ∏∏Êï∞Ôºå‰ΩøÊñπÁ®ãÊõ¥Êòì‰∫éËÆ°ÁÆó„ÄÇ</li>
<li><strong>Interaction Terms:</strong> The notations <span
class="math inline">\(\{i, j\}\)</span>, <span
class="math inline">\(\{i, j, k\}\)</span>, etc., refer to the
‚Äúmany-body‚Äù problem. The Hamiltonian contains two-body terms
(interactions between pairs of particles), and solving the Schr√∂dinger
equation exactly is extremely difficult because the motion of every
particle is correlated with every other particle. Computational methods
are designed to approximate these interactions. Á¨¶Âè∑ <span
class="math inline">\(\{i, j\}\)</span>„ÄÅ<span
class="math inline">\(\{i, j, k\}\)</span>
Á≠âÊåáÁöÑÊòØ‚ÄúÂ§ö‰Ωì‚ÄùÈóÆÈ¢ò„ÄÇÂìàÂØÜÈ°øÈáèÂåÖÂê´‰∫å‰ΩìÈ°πÔºàÁ≤íÂ≠êÂØπ‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®ÔºâÔºåËÄåÁ≤æÁ°ÆÊ±ÇËß£ËñõÂÆöË∞îÊñπÁ®ãÊûÅÂÖ∂Âõ∞ÈöæÔºåÂõ†‰∏∫ÊØè‰∏™Á≤íÂ≠êÁöÑËøêÂä®ÈÉΩ‰∏éÂÖ∂‰ªñÁ≤íÂ≠êÁõ∏ÂÖ≥„ÄÇËÆ°ÁÆóÊñπÊ≥ïÊó®Âú®Ëøë‰ººËøô‰∫õÁõ∏‰∫í‰ΩúÁî®„ÄÇ</li>
</ul>
<p>This whiteboard presents the mathematical foundation for
<strong>non-adiabatic molecular dynamics</strong>, a sophisticated
method in theoretical chemistry and physics used to simulate processes
where the Born-Oppenheimer approximation breaks down. This typically
occurs in photochemistry, electron transfer reactions, and when
molecules interact with intense laser fields.
ËøôÂùóÁôΩÊùøÂ±ïÁ§∫‰∫Ü<strong>ÈùûÁªùÁÉ≠ÂàÜÂ≠êÂä®ÂäõÂ≠¶</strong>ÁöÑÊï∞Â≠¶Âü∫Á°ÄÔºåËøôÊòØÁêÜËÆ∫ÂåñÂ≠¶ÂíåÁâ©ÁêÜÂ≠¶‰∏≠‰∏ÄÁßçÂ§çÊùÇÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÊ®°ÊãüÁéªÊÅ©-Â••Êú¨Êµ∑ÈªòËøë‰ººÂ§±ÊïàÁöÑËøáÁ®ã„ÄÇËøôÈÄöÂ∏∏ÂèëÁîüÂú®ÂÖâÂåñÂ≠¶„ÄÅÁîµÂ≠êËΩ¨ÁßªÂèçÂ∫î‰ª•ÂèäÂàÜÂ≠ê‰∏éÂº∫ÊøÄÂÖâÂú∫Áõ∏‰∫í‰ΩúÁî®Êó∂„ÄÇ</p>
<h3
id="topic-non-adiabatic-molecular-dynamics-md-ÈùûÁªùÁÉ≠ÂàÜÂ≠êÂä®ÂäõÂ≠¶-md">6.
Topic: Non-Adiabatic Molecular Dynamics (MD) ÈùûÁªùÁÉ≠ÂàÜÂ≠êÂä®ÂäõÂ≠¶ (MD)</h3>
<p>The title ‚ÄúŒî non-adiabatic MD‚Äù indicates that the topic moves beyond
the standard Born-Oppenheimer approximation. In this approximation, it
is assumed that the light electrons adjust instantaneously to the motion
of the heavy nuclei, allowing the system to be described by a single
potential energy surface. Non-adiabatic methods, by contrast, account
for the quantum mechanical coupling between multiple electronic
states.</p>
<p>Ê†áÈ¢ò‚ÄúŒî ÈùûÁªùÁÉ≠
MD‚ÄùË°®ÊòéËØ•‰∏ªÈ¢òË∂ÖË∂ä‰∫ÜÊ†áÂáÜÁöÑÁéªÊÅ©-Â••Êú¨Êµ∑ÈªòËøë‰ºº„ÄÇÂú®ËØ•Ëøë‰ºº‰∏≠ÔºåÂÅáËÆæËΩªÁîµÂ≠ê‰ºöÊ†πÊçÆÈáçÂéüÂ≠êÊ†∏ÁöÑËøêÂä®ËøõË°åÁû¨Êó∂Ë∞ÉÊï¥Ôºå‰ªéËÄå‰ΩøÁ≥ªÁªüÂèØ‰ª•Áî®Âçï‰∏™ÂäøËÉΩÈù¢Êù•ÊèèËø∞„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÈùûÁªùÁÉ≠ÊñπÊ≥ïÂàôËÄÉËôë‰∫ÜÂ§ö‰∏™ÁîµÂ≠êÊÄÅ‰πãÈó¥ÁöÑÈáèÂ≠êÂäõÂ≠¶ËÄ¶Âêà„ÄÇ</p>
<h3 id="the-born-huang-ansatz-ÁéªÊÅ©-ÈªÑÊãüËÆæ">7. The Born-Huang Ansatz
ÁéªÊÅ©-ÈªÑÊãüËÆæ</h3>
<p>The starting point for this method is the ‚Äúansatz‚Äù (an educated guess
for the form of the solution). This is the Born-Huang expansion for the
total molecular wave function, <span
class="math inline">\(\Psi\)</span>.
ËØ•ÊñπÊ≥ïÁöÑËµ∑ÁÇπÊòØ‚ÄúÊãüËÆæ‚ÄùÔºàÂØπËß£ÂΩ¢ÂºèÁöÑÂêàÁêÜÁåúÊµãÔºâ„ÄÇËøôÊòØÂàÜÂ≠êÊÄªÊ≥¢ÂáΩÊï∞ <span
class="math inline">\(\Psi\)</span> ÁöÑÁéªÊÅ©-ÈªÑÂ±ïÂºÄÂºè„ÄÇ</p>
<p><span class="math inline">\(\Psi(\vec{R}, \vec{r}, t) = \sum_{n}
\Theta_n(\vec{R}, t) \Phi_n(\vec{R}, \vec{r})\)</span></p>
<ul>
<li><p><strong><span class="math inline">\(\Psi(\vec{R}, \vec{r},
t)\)</span></strong> is the total wave function for the entire molecule.
It depends on the coordinates of all nuclei (<span
class="math inline">\(\vec{R}\)</span>), all electrons (<span
class="math inline">\(\vec{r}\)</span>), and time (<span
class="math inline">\(t\)</span>).
ÊòØÊï¥‰∏™ÂàÜÂ≠êÁöÑÊÄªÊ≥¢ÂáΩÊï∞„ÄÇÂÆÉÂèñÂÜ≥‰∫éÊâÄÊúâÂéüÂ≠êÊ†∏ (<span
class="math inline">\(\vec{R}\)</span>)„ÄÅÊâÄÊúâÁîµÂ≠ê (<span
class="math inline">\(\vec{r}\)</span>) ÂíåÊó∂Èó¥ (<span
class="math inline">\(t\)</span>) ÁöÑÂùêÊ†á„ÄÇ</p></li>
<li><p><strong><span class="math inline">\(\Phi_n(\vec{R},
\vec{r})\)</span></strong> are the <strong>electronic wave
functions</strong>. They are the solutions to the electronic Schr√∂dinger
equation for a fixed nuclear geometry <span
class="math inline">\(\vec{R}\)</span> and form a complete basis set.
The index <span class="math inline">\(n\)</span> labels the electronic
state (e.g., ground state, first excited state, etc.).
ÂÆÉ‰ª¨ÊòØÁªôÂÆöÂéüÂ≠êÊ†∏Âá†‰ΩïÊûÑÂûã <span class="math inline">\(\vec{R}\)</span>
ÁöÑÁîµÂ≠êËñõÂÆöË∞îÊñπÁ®ãÁöÑËß£ÔºåÂπ∂ÊûÑÊàê‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÂü∫ÁªÑ„ÄÇ‰∏ãÊ†á <span
class="math inline">\(n\)</span>
Ê†áËÆ∞ÁîµÂ≠êÊÄÅÔºà‰æãÂ¶ÇÔºåÂü∫ÊÄÅ„ÄÅÁ¨¨‰∏ÄÊøÄÂèëÊÄÅÁ≠âÔºâ„ÄÇ</p></li>
<li><p><strong><span class="math inline">\(\Theta_n(\vec{R},
t)\)</span></strong> are the <strong>nuclear wave functions</strong>.
Each <span class="math inline">\(\Theta_n\)</span> describes the motion
of the nuclei on the potential energy surface of the corresponding
electronic state, <span class="math inline">\(\Phi_n\)</span>.
Crucially, they depend on time. ÊòØ<strong>Ê†∏Ê≥¢ÂáΩÊï∞</strong>„ÄÇÊØè‰∏™ <span
class="math inline">\(\Theta_n\)</span> ÊèèËø∞ÂéüÂ≠êÊ†∏Âú®Áõ∏Â∫îÁîµÂ≠êÊÄÅ <span
class="math inline">\(\Phi_n\)</span>
ÂäøËÉΩÈù¢‰∏äÁöÑËøêÂä®„ÄÇËá≥ÂÖ≥ÈáçË¶ÅÁöÑÊòØÔºåÂÆÉ‰ª¨‰æùËµñ‰∫éÊó∂Èó¥„ÄÇ</p></li>
</ul>
<p>This ansatz expresses the total molecular state as a superposition of
electronic states, where the coefficients of the superposition are the
nuclear wave functions.
ËØ•ÊãüËÆæÂ∞ÜÊÄªÂàÜÂ≠êÊÄÅË°®Á§∫‰∏∫ÁîµÂ≠êÊÄÅÁöÑÂè†Âä†ÔºåÂÖ∂‰∏≠Âè†Âä†ÁöÑÁ≥ªÊï∞ÊòØÊ†∏Ê≥¢ÂáΩÊï∞„ÄÇ</p>
<h3 id="the-partitioned-molecular-hamiltonian-ÂàÜÂâ≤ÂàÜÂ≠êÂìàÂØÜÈ°øÈáè">8. The
Partitioned Molecular Hamiltonian ÂàÜÂâ≤ÂàÜÂ≠êÂìàÂØÜÈ°øÈáè</h3>
<p>The total molecular Hamiltonian, <span
class="math inline">\(\hat{\mathcal{H}}\)</span>, is partitioned into
terms that act on the nuclei and electrons separately. ÊÄªÂàÜÂ≠êÂìàÂØÜÈ°øÈáè
<span class="math inline">\(\hat{\mathcal{H}}\)</span>
Ë¢´ÂàÜÂâ≤ÊàêÂàÜÂà´‰ΩúÁî®‰∫éÂéüÂ≠êÊ†∏ÂíåÁîµÂ≠êÁöÑÈ°π„ÄÇ</p>
<p><span class="math inline">\(\hat{\mathcal{H}} = -\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2 + \hat{\mathcal{H}}_e +
\hat{V}_{nn}\)</span></p>
<ul>
<li><p><strong><span class="math inline">\(-\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2\)</span></strong>: This is the kinetic
energy operator for the nuclei, often denoted as <span
class="math inline">\(\hat{T}_n\)</span>.ËøôÊòØÂéüÂ≠êÊ†∏ÁöÑÂä®ËÉΩÁÆóÁ¨¶ÔºåÈÄöÂ∏∏Ë°®Á§∫‰∏∫
<span class="math inline">\(\hat{T}_n\)</span>„ÄÇ</p></li>
<li><p><strong><span
class="math inline">\(\hat{\mathcal{H}}_e\)</span></strong>: This is the
<strong>electronic Hamiltonian</strong>, which includes the kinetic
energy of the electrons and the potential energy of electron-electron
and electron-nuclear interactions.
ËøôÊòØ<strong>ÁîµÂ≠êÂìàÂØÜÈ°øÈáè</strong>ÔºåÂåÖÂê´ÁîµÂ≠êÁöÑÂä®ËÉΩ‰ª•ÂèäÁîµÂ≠ê-ÁîµÂ≠êÂíåÁîµÂ≠ê-Ê†∏Áõ∏‰∫í‰ΩúÁî®ÁöÑÂäøËÉΩ„ÄÇ</p></li>
<li><p><strong><span
class="math inline">\(\hat{V}_{nn}\)</span></strong>: This is the
potential energy operator for <strong>nuclear-nuclear
repulsion</strong>.ËøôÊòØ<strong>Ê†∏-Ê†∏ÊéíÊñ•</strong>ÁöÑÂäøËÉΩÁÆóÁ¨¶„ÄÇ</p></li>
</ul>
<h3 id="the-electronic-schr√∂dinger-equation-ÁîµÂ≠êËñõÂÆöË∞îÊñπÁ®ã">9. The
Electronic Schr√∂dinger Equation ÁîµÂ≠êËñõÂÆöË∞îÊñπÁ®ã</h3>
<p>The electronic basis functions, <span
class="math inline">\(\Phi_n\)</span>, are defined as the eigenfunctions
of the electronic Hamiltonian (plus the nuclear repulsion term) for a
fixed nuclear configuration <span
class="math inline">\(\vec{R}\)</span>. ÁîµÂ≠êÂü∫ÂáΩÊï∞ <span
class="math inline">\(\Phi_n\)</span> ÂÆö‰πâ‰∏∫ÂØπ‰∫éÂõ∫ÂÆöÁöÑÊ†∏ÊûÑÂûã <span
class="math inline">\(\vec{R}\)</span>ÔºåÁîµÂ≠êÂìàÂØÜÈ°øÈáèÔºàÂä†‰∏äÊ†∏ÊéíÊñ•È°πÔºâÁöÑÊú¨ÂæÅÂáΩÊï∞„ÄÇ</p>
<p><span class="math inline">\((\hat{\mathcal{H}}_e + \hat{V}_{nn})
\Phi_n(\vec{R}, \vec{r}) = E_n(\vec{R}) \Phi_n(\vec{R},
\vec{r})\)</span></p>
<ul>
<li><strong><span class="math inline">\(E_n(\vec{R})\)</span></strong>
are the eigenvalues, which are the <strong>potential energy surfaces
(PES)</strong>. Each electronic state <span
class="math inline">\(n\)</span> has its own PES, which dictates the
forces acting on the nuclei when the molecule is in that electronic
state. ÊòØÁâπÂæÅÂÄºÔºåÂç≥<strong>ÂäøËÉΩÈù¢ (PES)</strong>„ÄÇÊØè‰∏™ÁîµÂ≠êÊÄÅ <span
class="math inline">\(n\)</span>
ÈÉΩÊúâÂÖ∂Ëá™Ë∫´ÁöÑÂäøËÉΩÈù¢ÔºåÂÆÉÂÜ≥ÂÆö‰∫ÜÂàÜÂ≠êÂ§Ñ‰∫éËØ•ÁîµÂ≠êÊÄÅÊó∂‰ΩúÁî®‰∫éÂéüÂ≠êÊ†∏ÁöÑÂäõ„ÄÇ</li>
</ul>
<h3
id="deriving-the-equations-of-motion-for-the-nuclei-Êé®ÂØºÂéüÂ≠êÊ†∏ËøêÂä®ÊñπÁ®ã">10.
Deriving the Equations of Motion for the Nuclei Êé®ÂØºÂéüÂ≠êÊ†∏ËøêÂä®ÊñπÁ®ã</h3>
<p>The final part of the whiteboard begins the derivation of the
time-dependent Schr√∂dinger equation for the nuclear wave functions,
<span class="math inline">\(\Theta_k\)</span>. The process starts with
the full time-dependent Schr√∂dinger equation, <span
class="math inline">\(i\hbar \frac{\partial \Psi}{\partial t} =
\hat{\mathcal{H}} \Psi\)</span>. To find the equation for a specific
nuclear wave function <span class="math inline">\(\Theta_k\)</span>,
this main equation is projected onto the corresponding electronic basis
state <span class="math inline">\(\Phi_k\)</span>.
ÁôΩÊùøÁöÑÊúÄÂêé‰∏ÄÈÉ®ÂàÜÂºÄÂßãÊé®ÂØºÂéüÂ≠êÊ†∏Ê≥¢ÂáΩÊï∞ <span
class="math inline">\(\Theta_k\)</span>
ÁöÑÂê´Êó∂ËñõÂÆöË∞îÊñπÁ®ã„ÄÇËØ•ËøáÁ®ã‰ªéÂÆåÊï¥ÁöÑÂê´Êó∂ËñõÂÆöË∞îÊñπÁ®ã <span
class="math inline">\(i\hbar \frac{\partial \Psi}{\partial t} =
\hat{\mathcal{H}} \Psi\)</span> ÂºÄÂßã„ÄÇ‰∏∫‰∫ÜÊâæÂà∞ÁâπÂÆöÂéüÂ≠êÊ†∏Ê≥¢ÂáΩÊï∞ <span
class="math inline">\(\Theta_k\)</span>
ÁöÑÊñπÁ®ãÔºåÈúÄË¶ÅÂ∞ÜËøô‰∏™‰∏ªÊñπÁ®ãÊäïÂΩ±Âà∞Áõ∏Â∫îÁöÑÁîµÂ≠êÂü∫ÊÄÅ <span
class="math inline">\(\Phi_k\)</span> ‰∏ä„ÄÇ</p>
<p>This is done by multiplying from the left by the complex conjugate of
the electronic wave function, <span
class="math inline">\(\Phi_k^*\)</span>, and integrating over all
electronic coordinates, <span class="math inline">\(d\vec{r}\)</span>.
ÂèØ‰ª•ÈÄöËøá‰ªéÂ∑¶Ëæπ‰πò‰ª•ÁîµÂ≠êÊ≥¢ÂáΩÊï∞ <span
class="math inline">\(\Phi_k^*\)</span> ÁöÑÂ§çÂÖ±ËΩ≠ÔºåÁÑ∂ÂêéÂú®ÊâÄÊúâÁîµÂ≠êÂùêÊ†á
<span class="math inline">\(d\vec{r}\)</span> ‰∏äÁßØÂàÜÊù•ÂÆûÁé∞„ÄÇ</p>
<p><span class="math inline">\(\int \Phi_k^* i\hbar
\frac{\partial}{\partial t} \Psi \,d\vec{r} = \int \Phi_k^*
\hat{\mathcal{H}} \Psi \,d\vec{r}\)</span></p>
<p>The board then shows the result of substituting the Born-Huang ansatz
for <span class="math inline">\(\Psi\)</span> and the partitioned
Hamiltonian for <span class="math inline">\(\hat{\mathcal{H}}\)</span>
into this projected equation: ÁÑ∂ÂêéÔºåÈªëÊùøÊòæÁ§∫Â∞Ü Born-Huang ÊãüËÆæÂºè‰ª£ÂÖ•
<span
class="math inline">\(\Psi\)</span>ÔºåÂ∞ÜÂàÜÂùóÂìàÂØÜÈ°øÈáè‰ª£ÂÖ•‰ª•‰∏ãÊäïÂΩ±ÊñπÁ®ãÁöÑÁªìÊûúÔºö</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k(\vec{R}, t) = \int \Phi_k^* \left( -\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2 + \hat{\mathcal{H}}_e + \hat{V}_{nn}
\right) \sum_n \Theta_n \Phi_n \,d\vec{r}\)</span></p>
<ul>
<li><p><strong>Left Hand Side</strong>: The left side of the projection
has been simplified. Because the electronic basis functions <span
class="math inline">\(\Phi_n\)</span> form an orthonormal set (<span
class="math inline">\(\int \Phi_k^* \Phi_n d\vec{r} =
\delta_{kn}\)</span>), the sum collapses to a single term for <span
class="math inline">\(n=k\)</span>. ÊäïÂΩ±Â∑¶‰æßÂ∑≤ÁÆÄÂåñ„ÄÇÁî±‰∫éÁîµÂ≠êÂü∫ÂáΩÊï∞ <span
class="math inline">\(\Phi_n\)</span> ÊûÑÊàê‰∏Ä‰∏™Ê≠£‰∫§ÈõÜ (<span
class="math inline">\(\int \Phi_k^* \Phi_n d\vec{r} =
\delta_{kn}\)</span>ÔºåÂõ†Ê≠§ÂΩì <span class="math inline">\(n=k\)</span>
Êó∂ÔºåÂíåÂ∞ÜÊäòÂè†‰∏∫‰∏Ä‰∏™È°π„ÄÇ</p></li>
<li><p><strong>Right Hand Side</strong>: This complex integral is the
core of non-adiabatic dynamics. When the nuclear kinetic energy
operator, <span class="math inline">\(\nabla_I^2\)</span>, acts on the
product <span class="math inline">\(\Theta_n \Phi_n\)</span>, it acts on
both functions (via the product rule). The terms that arise from <span
class="math inline">\(\nabla_I\)</span> acting on the electronic wave
functions <span class="math inline">\(\Phi_n\)</span> are known as
<strong>non-adiabatic coupling terms</strong>. These terms are
responsible for enabling transitions between different electronic
potential energy surfaces, which is the essence of non-adiabatic
dynamics. Ëøô‰∏™Â§çÁßØÂàÜÊòØÈùûÁªùÁÉ≠Âä®ÂäõÂ≠¶ÁöÑÊ†∏ÂøÉ„ÄÇÂΩìÊ†∏Âä®ËÉΩÁÆóÁ¨¶ <span
class="math inline">\(\nabla_I^2\)</span> ‰ΩúÁî®‰∫é‰πòÁßØ <span
class="math inline">\(\Theta_n \Phi_n\)</span>
Êó∂ÔºåÂÆÉ‰ºö‰ΩúÁî®‰∫éËøô‰∏§‰∏™ÂáΩÊï∞ÔºàÈÄöËøá‰πòÁßØËßÑÂàôÔºâ„ÄÇÁî± <span
class="math inline">\(\nabla_I\)</span> ‰ΩúÁî®‰∫éÁîµÂ≠êÊ≥¢ÂáΩÊï∞ <span
class="math inline">\(\Phi_n\)</span>
ËÄå‰∫ßÁîüÁöÑÈ°πÁß∞‰∏∫<strong>ÈùûÁªùÁÉ≠ËÄ¶ÂêàÈ°π</strong>„ÄÇËøô‰∫õÊúØËØ≠Ë¥üË¥£ÂÆûÁé∞‰∏çÂêåÁîµÂ≠êÂäøËÉΩÈù¢‰πãÈó¥ÁöÑËΩ¨ÂèòÔºåËøôÊòØÈùûÁªùÁÉ≠Âä®ÂäõÂ≠¶ÁöÑÊú¨Ë¥®„ÄÇ</p></li>
</ul>
<p>This whiteboard continues the mathematical derivation for
non-adiabatic molecular dynamics started in the previous image. It
focuses on expanding the nuclear kinetic energy term to reveal the
crucial couplings between different electronic
states.ËøôÂùóÁôΩÊùøÂª∂Áª≠‰∫Ü‰∏ä‰∏ÄÂº†ÂõæÁâá‰∏≠ÈùûÁªùÁÉ≠ÂàÜÂ≠êÂä®ÂäõÂ≠¶ÁöÑÊï∞Â≠¶Êé®ÂØº„ÄÇÂÆÉÁùÄÈáç‰∫éÊâ©Â±ïÊ†∏Âä®ËÉΩÈ°πÔºå‰ª•Êè≠Á§∫‰∏çÂêåÁîµÂ≠êÊÄÅ‰πãÈó¥ÁöÑÂÖ≥ÈîÆËÄ¶Âêà„ÄÇ</p>
<h3
id="starting-point-the-projected-schr√∂dinger-equation-Ëµ∑ÁÇπÊäïÂΩ±ËñõÂÆöË∞îÊñπÁ®ã">11.
Starting Point: The Projected Schr√∂dinger Equation
Ëµ∑ÁÇπÔºöÊäïÂΩ±ËñõÂÆöË∞îÊñπÁ®ã</h3>
<p>The derivation picks up from the equation for the time evolution of
the nuclear wave function, <span
class="math inline">\(\Theta_k\)</span>. The right-hand side of this
equation is being evaluated. Êé®ÂØºËøáÁ®ãÂèñËá™Ê†∏Ê≥¢ÂáΩÊï∞ <span
class="math inline">\(\Theta_k\)</span>
ÁöÑÊó∂Èó¥ÊºîÂåñÊñπÁ®ã„ÄÇËØ•ÊñπÁ®ãÁöÑÂè≥ËæπÊ≠£Âú®Ê±ÇÂÄº„ÄÇ</p>
<p><span class="math inline">\(= \int \Phi_k^* \left( -\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2 \right) \sum_n \Theta_n \Phi_n \,d\vec{r}
+ E_k \Theta_k\)</span></p>
<p>This equation separates the total energy into two parts
ËØ•ÊñπÁ®ãÂ∞ÜÊÄªËÉΩÈáèÂàÜ‰∏∫‰∏§ÈÉ®ÂàÜ : * The first term is the contribution from the
<strong>nuclear kinetic energy operator</strong>, <span
class="math inline">\(-\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2\)</span>.
Á¨¨‰∏ÄÈ°πÊòØ<strong>Ê†∏Âä®ËÉΩÁÆóÁ¨¶</strong>ÁöÑË¥°ÁåÆ * The second term, <span
class="math inline">\(E_k \Theta_k\)</span>, is the contribution from
the <strong>potential energy</strong>. This term arises from the action
of the electronic Hamiltonian part <span
class="math inline">\((\hat{\mathcal{H}}_e + \hat{V}_{nn})\)</span> on
the basis functions. Due to the orthonormality of the electronic
wavefunctions (<span class="math inline">\(\int \Phi_k^* \Phi_n
\,d\vec{r} = \delta_{kn}\)</span>), the sum over <span
class="math inline">\(n\)</span> collapses to a single term for the
potential energy. Á¨¨‰∫åÈ°πÔºå<span class="math inline">\(E_k
\Theta_k\)</span>ÔºåÊòØ<strong>ÂäøËÉΩ</strong>ÁöÑË¥°ÁåÆ„ÄÇËøô‰∏ÄÈ°πÊ∫ê‰∫éÁîµÂ≠êÂìàÂØÜÈ°øÈáèÈÉ®ÂàÜ
<span class="math inline">\((\hat{\mathcal{H}}_e +
\hat{V}_{nn})\)</span> ÂØπÂü∫ÂáΩÊï∞ÁöÑ‰ΩúÁî®„ÄÇÁî±‰∫éÁîµÂ≠êÊ≥¢ÂáΩÊï∞Ôºà<span
class="math inline">\(\int \Phi_k^* \Phi_n \,d\vec{r} =
\delta_{kn}\)</span>ÔºâÁöÑÊ≠£‰∫§ÊÄßÔºå<span
class="math inline">\(n\)</span>È°πÁöÑÂíå‰ºöÂùçÁº©‰∏∫ÂäøËÉΩÁöÑ‰∏ÄÈ°π„ÄÇ</p>
<p>The challenge, and the core of the physics, lies in evaluating the
first term, as the nuclear derivative <span
class="math inline">\(\nabla_I\)</span> acts on <em>both</em> the
nuclear wave function <span class="math inline">\(\Theta_n\)</span> and
the electronic wave function <span
class="math inline">\(\Phi_n\)</span>.
ÈöæÁÇπÂú®‰∫éÔºå‰πüÊòØÁâ©ÁêÜÁöÑÊ†∏ÂøÉÂú®‰∫éÂ¶Ç‰ΩïËÆ°ÁÆóÁ¨¨‰∏ÄÈ°πÔºåÂõ†‰∏∫Ê†∏ÂØºÊï∞ <span
class="math inline">\(\nabla_I\)</span> ÂêåÊó∂‰ΩúÁî®‰∫éÊ†∏Ê≥¢ÂáΩÊï∞ <span
class="math inline">\(\Theta_n\)</span> ÂíåÁîµÂ≠êÊ≥¢ÂáΩÊï∞ <span
class="math inline">\(\Phi_n\)</span>„ÄÇ</p>
<h3
id="applying-the-product-rule-for-the-laplacian-Â∫îÁî®ÊãâÊôÆÊãâÊñØÁÆóÂ≠êÁöÑ‰πòÁßØËßÑÂàô">12.
Applying the Product Rule for the Laplacian
Â∫îÁî®ÊãâÊôÆÊãâÊñØÁÆóÂ≠êÁöÑ‰πòÁßØËßÑÂàô</h3>
<p>To expand the kinetic energy term, the product rule for the Laplacian
operator acting on two functions (A and B) is used. The board writes
this rule as: ‰∏∫‰∫ÜÂ±ïÂºÄÂä®ËÉΩÈ°πÔºåÊàë‰ª¨Âà©Áî®‰∫ÜÊãâÊôÆÊãâÊñØÁÆóÂ≠ê‰ΩúÁî®‰∫é‰∏§‰∏™ÂáΩÊï∞ÔºàA Âíå
BÔºâÁöÑ‰πòÁßØËßÑÂàô„ÄÇÊ£ãÁõò‰∏äÂ∞ÜËøôÊù°ËßÑÂàôÂÜôÊàêÔºö <span
class="math inline">\(\nabla^2(AB) = (\nabla^2 A)B + 2(\nabla
A)\cdot(\nabla B) + A(\nabla^2 B)\)</span></p>
<p>In our case, <span class="math inline">\(A = \Theta_n(\vec{R},
t)\)</span> and <span class="math inline">\(B = \Phi_n(\vec{R},
\vec{r})\)</span>. The derivative <span
class="math inline">\(\nabla_I\)</span> is with respect to the nuclear
coordinates <span class="math inline">\(\vec{R}_I\)</span>.
Âú®Êàë‰ª¨ÁöÑ‰æãÂ≠ê‰∏≠Ôºå<span class="math inline">\(A = \Theta_n(\vec{R},
t)\)</span>Ôºå<span class="math inline">\(B = \Phi_n(\vec{R},
\vec{r})\)</span>„ÄÇÂØºÊï∞ <span class="math inline">\(\nabla_I\)</span>
ÊòØÂÖ≥‰∫éÂéüÂ≠êÊ†∏ÂùêÊ†á <span class="math inline">\(\vec{R}_I\)</span> ÁöÑ„ÄÇ</p>
<h3 id="expanding-the-kinetic-energy-term-Â±ïÂºÄÂä®ËÉΩÈ°π">13. Expanding the
Kinetic Energy Term Â±ïÂºÄÂä®ËÉΩÈ°π</h3>
<p>Applying this rule, the integral containing the kinetic energy
operator is expanded: Â∫îÁî®Ê≠§ËßÑÂàôÔºåÂ±ïÂºÄÂåÖÂê´Âä®ËÉΩÁÆóÁ¨¶ÁöÑÁßØÂàÜÔºö <span
class="math inline">\(= -\sum_I \frac{\hbar^2}{2M_I} \int \Phi_k^*
\sum_n \left( (\nabla_I^2 \Theta_n)\Phi_n + 2(\nabla_I
\Theta_n)\cdot(\nabla_I \Phi_n) + \Theta_n(\nabla_I^2 \Phi_n) \right)
d\vec{r} + E_k \Theta_k\)</span></p>
<p>This step explicitly shows how the nuclear kinetic energy operator
gives rise to three distinct types of
terms.Ê≠§Ê≠•È™§ÊòéÁ°ÆÂ±ïÁ§∫‰∫ÜÊ†∏Âä®ËÉΩÁÆóÁ¨¶Â¶Ç‰Ωï‰∫ßÁîü‰∏âÁßç‰∏çÂêåÁ±ªÂûãÁöÑÈ°π„ÄÇ</p>
<h3
id="final-result-and-identification-of-coupling-terms-ÊúÄÁªàÁªìÊûúÂèäËÄ¶ÂêàÈ°πÁöÑËØÜÂà´">14.
Final Result and Identification of Coupling Terms
ÊúÄÁªàÁªìÊûúÂèäËÄ¶ÂêàÈ°πÁöÑËØÜÂà´</h3>
<p>The final step is to take the integral over the electronic
coordinates (<span class="math inline">\(d\vec{r}\)</span>) and
rearrange the terms. The expression is simplified by again using the
orthonormality of the electronic wave functions, <span
class="math inline">\(\int \Phi_k^* \Phi_n \, d\vec{r} =
\delta_{kn}\)</span>. ÊúÄÂêé‰∏ÄÊ≠•ÊòØÂØπÁîµÂ≠êÂùêÊ†á (<span
class="math inline">\(d\vec{r}\)</span>)
ËøõË°åÁßØÂàÜÔºåÂπ∂ÈáçÊñ∞ÊéíÂàóÂêÑÈ°π„ÄÇÂÜçÊ¨°Âà©Áî®ÁîµÂ≠êÊ≥¢ÂáΩÊï∞ÁöÑÊ≠£‰∫§ÊÄßÁÆÄÂåñË°®ËææÂºèÔºå<span
class="math inline">\(\int \Phi_k^* \Phi_n \, d\vec{r} =
\delta_{kn}\)</span>„ÄÇ</p>
<p><span class="math inline">\(= -\sum_I \frac{\hbar^2}{2M_I} \left(
\nabla_I^2 \Theta_k + \sum_n 2 \left( \int \Phi_k^* \nabla_I \Phi_n \,
d\vec{r} \right) \cdot \nabla_I \Theta_n + \sum_n \left( \int \Phi_k^*
\nabla_I^2 \Phi_n \, d\vec{r} \right) \Theta_n \right) + E_k
\Theta_k\)</span></p>
<p>This final equation is profound. It represents the time-independent
Schr√∂dinger equation for the nuclear wave function <span
class="math inline">\(\Theta_k\)</span>, but it is coupled to all other
nuclear wave functions <span class="math inline">\(\Theta_n\)</span>.
Let‚Äôs break down the key terms within the parentheses:
ÊúÄÂêé‰∏Ä‰∏™ÊñπÁ®ãÊÑè‰πâÊ∑±Ëøú„ÄÇÂÆÉ‰ª£Ë°®‰∫ÜÊ†∏Ê≥¢ÂáΩÊï∞ <span
class="math inline">\(\Theta_k\)</span>
ÁöÑ‰∏éÊó∂Èó¥Êó†ÂÖ≥ÁöÑËñõÂÆöË∞îÊñπÁ®ãÔºå‰ΩÜÂÆÉ‰∏éÊâÄÊúâÂÖ∂‰ªñÊ†∏Ê≥¢ÂáΩÊï∞ <span
class="math inline">\(\Theta_n\)</span>
ËÄ¶Âêà„ÄÇËÆ©Êàë‰ª¨ÂàÜËß£‰∏Ä‰∏ãÊã¨Âè∑ÂÜÖÁöÑÂÖ≥ÈîÆÈ°πÔºö</p>
<ul>
<li><p><strong><span class="math inline">\(\nabla_I^2
\Theta_k\)</span></strong>: This is the standard kinetic energy term for
the nuclei moving on the potential energy surface of state <span
class="math inline">\(k\)</span>. This is the only term that would
remain in the simple Born-Oppenheimer (adiabatic) approximation.
ËøôÊòØÂéüÂ≠êÊ†∏Âú®ÂäøËÉΩÈù¢ <span class="math inline">\(k\)</span>
‰∏äËøêÂä®ÁöÑÊ†áÂáÜÂä®ËÉΩÈ°π„ÄÇËøôÊòØÂú®ÁÆÄÂçïÁöÑ
Born-OppenheimerÔºàÁªùÁÉ≠ÔºâËøë‰ºº‰∏≠ÂîØ‰∏Ä‰øùÁïôÁöÑÈ°π„ÄÇ</p></li>
<li><p><strong><span class="math inline">\(\left( \int \Phi_k^* \nabla_I
\Phi_n \, d\vec{r} \right)\)</span></strong>: This is the
<strong>first-derivative non-adiabatic coupling term (NACT)</strong>,
often called the derivative coupling. This vector quantity determines
the strength of the coupling between electronic states <span
class="math inline">\(k\)</span> and <span
class="math inline">\(n\)</span> due to the velocity of the nuclei. It
is the primary term responsible for enabling transitions between
different potential energy surfaces. ËøôÊòØ<strong>‰∏ÄÈò∂ÂØºÊï∞ÈùûÁªùÁÉ≠ËÄ¶ÂêàÈ°π
(NACT)</strong>ÔºåÈÄöÂ∏∏Áß∞‰∏∫ÂØºÊï∞ËÄ¶Âêà„ÄÇËØ•Áü¢ÈáèÂÜ≥ÂÆö‰∫ÜÁî±‰∫éÂéüÂ≠êÊ†∏ÈÄüÂ∫¶ËÄåÂØºËá¥ÁöÑÁîµÂ≠êÊÄÅ
<span class="math inline">\(k\)</span> Âíå <span
class="math inline">\(n\)</span>
‰πãÈó¥ËÄ¶ÂêàÁöÑÂº∫Â∫¶„ÄÇÂÆÉÊòØÂÆûÁé∞‰∏çÂêåÂäøËÉΩÈù¢‰πãÈó¥Ë∑ÉËøÅÁöÑ‰∏ªË¶ÅÈ°π„ÄÇ</p></li>
<li><p><strong><span class="math inline">\(\left( \int \Phi_k^*
\nabla_I^2 \Phi_n \, d\vec{r} \right)\)</span></strong>: This is the
<strong>second-derivative non-adiabatic coupling term</strong>, a scalar
quantity. While often smaller than the first-derivative term, it is also
part of the complete description of non-adiabatic effects.
ÊòØ<strong>‰∫åÈò∂ÂØºÊï∞ÈùûÁªùÁÉ≠ËÄ¶ÂêàÈ°π</strong>Ôºå‰∏Ä‰∏™Ê†áÈáè„ÄÇËôΩÁÑ∂ÂÆÉÈÄöÂ∏∏Â∞è‰∫é‰∏ÄÈò∂ÂØºÊï∞È°πÔºå‰ΩÜÂÆÉ‰πüÊòØÈùûÁªùÁÉ≠ÊïàÂ∫îÂÆåÊï¥ÊèèËø∞ÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇ</p></li>
</ul>
<p>In summary, this derivation shows mathematically how the motion of
the nuclei (via the <span class="math inline">\(\nabla_I\)</span>
operator) can induce quantum mechanical transitions between different
electronic states (<span class="math inline">\(\Phi_k \leftrightarrow
\Phi_n\)</span>). The strength of these transitions is governed by the
non-adiabatic coupling terms, which depend on how the electronic wave
functions change as the nuclear geometry changes.
ÊÄª‰πãÔºåËØ•Êé®ÂØº‰ªéÊï∞Â≠¶‰∏äÂ±ïÁ§∫‰∫ÜÂéüÂ≠êÊ†∏ÁöÑËøêÂä®ÔºàÈÄöËøá <span
class="math inline">\(\nabla_I\)</span>
ÁÆóÁ¨¶ÔºâÂ¶Ç‰ΩïËØ±ÂØº‰∏çÂêåÁîµÂ≠êÊÄÅ‰πãÈó¥ÁöÑÈáèÂ≠êÂäõÂ≠¶Ë∑ÉËøÅÔºà<span
class="math inline">\(\Phi_k \leftrightarrow
\Phi_n\)</span>Ôºâ„ÄÇËøô‰∫õË∑ÉËøÅÁöÑÂº∫Â∫¶Áî±ÈùûÁªùÁÉ≠ËÄ¶ÂêàÈ°πÊéßÂà∂ÔºåËÄåÈùûÁªùÁÉ≠ËÄ¶ÂêàÈ°πÂèàÂèñÂÜ≥‰∫éÁîµÂ≠êÊ≥¢ÂáΩÊï∞Â¶Ç‰ΩïÈöèÂéüÂ≠êÊ†∏Âá†‰ΩïÁªìÊûÑÁöÑÂèòÂåñËÄåÂèòÂåñ„ÄÇ</p>
<p>This whiteboard concludes the derivation of the equations for
non-adiabatic molecular dynamics by defining the coupling operator and
then showing how different levels of approximation‚Äîspecifically the
Born-Huang and the more restrictive Born-Oppenheimer
approximations‚Äîarise from neglecting certain coupling terms.
ËøôÂùóÁôΩÊùøÈÄöËøáÂÆö‰πâËÄ¶ÂêàÁÆóÁ¨¶ÔºåÂπ∂Â±ïÁ§∫‰∏çÂêåÁ®ãÂ∫¶ÁöÑËøë‰ºº‚Äî‚ÄîÁâπÂà´ÊòØ Born-Huang
Ëøë‰ººÂíåÊõ¥‰∏•Ê†ºÁöÑ Born-Oppenheimer
Ëøë‰ºº‚Äî‚ÄîÊòØÂ¶Ç‰ΩïÈÄöËøáÂøΩÁï•Êüê‰∫õËÄ¶ÂêàÈ°πËÄå‰∫ßÁîüÁöÑÔºå‰ªéËÄåÊé®ÂØºÂá∫ÈùûÁªùÁÉ≠ÂàÜÂ≠êÂä®ÂäõÂ≠¶ÊñπÁ®ãÁöÑ„ÄÇ</p>
<h3
id="definition-of-the-non-adiabatic-coupling-operator-ÈùûÁªùÁÉ≠ËÄ¶ÂêàÁÆóÁ¨¶ÁöÑÂÆö‰πâ">15.
Definition of the Non-Adiabatic Coupling Operator
ÈùûÁªùÁÉ≠ËÄ¶ÂêàÁÆóÁ¨¶ÁöÑÂÆö‰πâ</h3>
<p>The whiteboard begins by collecting all the non-adiabatic coupling
terms derived previously into a single operator, <span
class="math inline">\(C_{kn}\)</span>.
ÁôΩÊùøÈ¶ñÂÖàÂ∞Ü‰πãÂâçÊé®ÂØºÁöÑÊâÄÊúâÈùûÁªùÁÉ≠ËÄ¶ÂêàÈ°πÂêàÂπ∂‰∏∫‰∏Ä‰∏™ÁÆóÁ¨¶ <span
class="math inline">\(C_{kn}\)</span>„ÄÇ</p>
<p>Let <span class="math inline">\(C_{kn} = -\sum_{I}
\frac{\hbar^2}{2M_I} \left( 2 \left( \int \Phi_k^* \nabla_I \Phi_n \,
d\vec{r} \right) \cdot \nabla_I + \left( \int \Phi_k^* \nabla_I^2 \Phi_n
\, d\vec{r} \right) \right)\)</span></p>
<ul>
<li>This operator, <span class="math inline">\(C_{kn}\)</span>,
represents the total effect of the coupling between electronic state
<span class="math inline">\(k\)</span> and electronic state <span
class="math inline">\(n\)</span>, which is induced by the kinetic energy
of the nuclei. Ê≠§ÁÆóÁ¨¶ <span class="math inline">\(C_{kn}\)</span>
Ë°®Á§∫Áî±ÂéüÂ≠êÊ†∏Âä®ËÉΩÂºïËµ∑ÁöÑÁîµÂ≠êÊÄÅ <span class="math inline">\(k\)</span>
ÂíåÁîµÂ≠êÊÄÅ <span class="math inline">\(n\)</span> ‰πãÈó¥ËÄ¶ÂêàÁöÑÊÄªÊïàÂ∫î„ÄÇ</li>
<li>The operator acts on the nuclear wave function that follows it in
the full equation. The <span class="math inline">\(\nabla_I\)</span>
term acts as a derivative on that wave function.
ËØ•ÁÆóÁ¨¶‰ΩúÁî®‰∫éÂÆåÊï¥ÊñπÁ®ã‰∏≠Ë∑üÈöèÂÆÉÁöÑÊ†∏Ê≥¢ÂáΩÊï∞„ÄÇ<span
class="math inline">\(\nabla_I\)</span> È°πÂÖÖÂΩìËØ•Ê≥¢ÂáΩÊï∞ÁöÑÂØºÊï∞„ÄÇ</li>
</ul>
<h3 id="the-coupled-equations-of-motion-ËÄ¶ÂêàËøêÂä®ÊñπÁ®ã">16. The Coupled
Equations of Motion ËÄ¶ÂêàËøêÂä®ÊñπÁ®ã</h3>
<p>Using this compact definition, the full set of coupled time-dependent
Schr√∂dinger equations for the nuclear wave functions can be written as:
Âü∫‰∫éÊ≠§ÁÆÄÊ¥ÅÂÆö‰πâÔºåÊ†∏Ê≥¢ÂáΩÊï∞ÁöÑÂÆåÊï¥ËÄ¶ÂêàÂê´Êó∂ËñõÂÆöË∞îÊñπÁ®ãÁªÑÂèØ‰ª•ÂÜôÊàêÔºö</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k \right)
\Theta_k + \sum_n C_{kn} \Theta_n\)</span></p>
<p>This is the central result. It shows that the time evolution of the
nuclear wave function on a given potential energy surface <span
class="math inline">\(k\)</span> (described by <span
class="math inline">\(\Theta_k\)</span>) depends on two things:
ËøôÊòØÊ†∏ÂøÉÁªìËÆ∫„ÄÇÂÆÉË°®ÊòéÔºåÊ†∏Ê≥¢ÂáΩÊï∞Âú®ÁªôÂÆöÂäøËÉΩÈù¢ <span
class="math inline">\(k\)</span>ÔºàÁî® <span
class="math inline">\(\Theta_k\)</span>
ÊèèËø∞Ôºâ‰∏äÁöÑÊó∂Èó¥ÊºîÂåñÂèñÂÜ≥‰∫é‰∏§‰∏™Âõ†Á¥†Ôºö 1. The motion on its own surface,
governed by its kinetic energy and the potential <span
class="math inline">\(E_k\)</span>. ÂÖ∂Ëá™Ë∫´Ë°®Èù¢‰∏äÁöÑËøêÂä®ÔºåÁî±ÂÖ∂Âä®ËÉΩÂíåÂäøËÉΩ
<span class="math inline">\(E_k\)</span> ÊéßÂà∂„ÄÇ 2. The influence of the
nuclear wave functions on <em>all other</em> electronic surfaces (<span
class="math inline">\(\Theta_n\)</span>), mediated by the coupling
operators <span class="math inline">\(C_{kn}\)</span>.
Ê†∏Ê≥¢ÂáΩÊï∞ÂØπ<em>ÊâÄÊúâÂÖ∂‰ªñ</em>ÁîµÂ≠êË°®Èù¢Ôºà<span
class="math inline">\(\Theta_n\)</span>ÔºâÁöÑÂΩ±ÂìçÔºåÁî±ËÄ¶ÂêàÁÆóÁ¨¶ <span
class="math inline">\(C_{kn}\)</span> ‰ªãÂØº„ÄÇ</p>
<h3 id="the-born-huang-approximation-ÁéªÊÅ©-ÈªÑËøë‰ºº">17. The Born-Huang
Approximation ÁéªÊÅ©-ÈªÑËøë‰ºº</h3>
<p>The first and most crucial approximation is introduced to simplify
this complex set of coupled equations.
‰∏∫‰∫ÜÁÆÄÂåñËøôÁªÑÂ§çÊùÇÁöÑËÄ¶ÂêàÊñπÁ®ãÔºåÂºïÂÖ•‰∫ÜÁ¨¨‰∏Ä‰∏™‰πüÊòØÊúÄÈáçË¶ÅÁöÑËøë‰ºº„ÄÇ</p>
<p><strong>If <span class="math inline">\(C_{kn} = 0\)</span> for <span
class="math inline">\(k \neq n\)</span> (Born-Huang
approximation)</strong></p>
<p>This approximation assumes that the <strong>off-diagonal</strong>
coupling terms, which are responsible for transitions between different
electronic states, are negligible. However, it retains the
<strong>diagonal</strong> coupling term (<span
class="math inline">\(C_{kk}\)</span>). This leads to a simplified,
uncoupled equation:
ËØ•Ëøë‰ººÂÅáËÆæÂØºËá¥‰∏çÂêåÁîµÂ≠êÊÄÅ‰πãÈó¥Ë∑ÉËøÅÁöÑ<strong>ÈùûÂØπËßí</strong>ËÄ¶ÂêàÈ°πÂèØ‰ª•ÂøΩÁï•‰∏çËÆ°„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰øùÁïô‰∫Ü<strong>ÂØπËßí</strong>ËÄ¶ÂêàÈ°πÔºà<span
class="math inline">\(C_{kk}\)</span>Ôºâ„ÄÇËøôÂèØ‰ª•ÂæóÂà∞‰∏Ä‰∏™ÁÆÄÂåñÁöÑÈùûËÄ¶ÂêàÊñπÁ®ãÔºö</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k +
C_{kk} \right) \Theta_k\)</span></p>
<p>Substituting the definition of <span
class="math inline">\(C_{kk}\)</span>: ‰ª£ÂÖ• <span
class="math inline">\(C_{kk}\)</span> ÁöÑÂÆö‰πâÔºö</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k -
\sum_I \frac{\hbar^2}{2M_I} \left( 2 \left( \int \Phi_k^* \nabla_I
\Phi_k \, d\vec{r} \right) \cdot \nabla_I + \int \Phi_k^* \nabla_I^2
\Phi_k \, d\vec{r} \right) \right) \Theta_k\)</span></p>
<p>The term <span class="math inline">\(C_{kk}\)</span> is known as the
<strong>diagonal Born-Oppenheimer correction (DBOC)</strong>. It
represents a small correction to the potential energy surface <span
class="math inline">\(E_k\)</span> that arises from the fact that the
electrons do not adjust perfectly and instantaneously to the nuclear
motion, even within the same electronic state. <span
class="math inline">\(C_{kk}\)</span>
È°πË¢´Áß∞‰∏∫<strong>ÂØπËßíÁéªÊÅ©-Â••Êú¨Êµ∑Èªò‰øÆÊ≠£ (DBOC)</strong>„ÄÇÂÆÉË°®Á§∫ÂØπÂäøËÉΩÈù¢
<span class="math inline">\(E_k\)</span>
ÁöÑÂæÆÂ∞è‰øÆÊ≠£ÔºåÂÖ∂ÂéüÂõ†ÊòØÂç≥‰ΩøÂú®Áõ∏ÂêåÁöÑÁîµÂ≠êÊÄÅ‰∏ãÔºåÁîµÂ≠ê‰πüÊó†Ê≥ïÂÆåÁæé‰∏îÂç≥Êó∂Âú∞ÈÄÇÂ∫îÊ†∏ËøêÂä®„ÄÇ</p>
<ul>
<li><strong>Note on Real Wavefunctions ÂÖ≥‰∫éÂÆûÊ≥¢ÂáΩÊï∞ÁöÑÊ≥®Èáä</strong>: The
board shows that for real wavefunctions, the first-derivative part of
the diagonal correction vanishes: <span class="math inline">\(\int
\Phi_k \nabla_I \Phi_k \, d\vec{r} = 0\)</span>. This is because the
integral is related to the gradient of the normalization condition,
<span class="math inline">\(\nabla_I \int \Phi_k^2 \, d\vec{r} =
\nabla_I(1) = 0\)</span>, which expands to <span
class="math inline">\(2\int \Phi_k \nabla_I \Phi_k \, d\vec{r} =
0\)</span>. ÈªëÊùøÊòæÁ§∫ÔºåÂØπ‰∫éÂÆûÊ≥¢ÂáΩÊï∞ÔºåÂØπËßí‰øÆÊ≠£ÁöÑ‰∏ÄÈò∂ÂØºÊï∞ÈÉ®ÂàÜ‰∏∫Èõ∂Ôºö<span
class="math inline">\(\int \Phi_k \nabla_I \Phi_k \, d\vec{r} =
0\)</span>„ÄÇËøôÊòØÂõ†‰∏∫ÁßØÂàÜ‰∏éÂΩí‰∏ÄÂåñÊù°‰ª∂ÁöÑÊ¢ØÂ∫¶ÊúâÂÖ≥Ôºå<span
class="math inline">\(\nabla_I \int \Phi_k^2 \, d\vec{r} = \nabla_I(1) =
0\)</span>ÔºåÂÖ∂Â±ïÂºÄ‰∏∫ <span class="math inline">\(2\int \Phi_k \nabla_I
\Phi_k \, d\vec{r} = 0\)</span>„ÄÇ</li>
</ul>
<h3 id="the-born-oppenheimer-approximation-ÁéªÊÅ©-Â••Êú¨Êµ∑ÈªòËøë‰ºº">18. The
Born-Oppenheimer Approximation ÁéªÊÅ©-Â••Êú¨Êµ∑ÈªòËøë‰ºº</h3>
<p>The final and most widely used approximation is the Born-Oppenheimer
approximation. It is more restrictive than the Born-Huang approximation.
ÊúÄÂêé‰∏ÄÁßç‰πüÊòØÊúÄÂπøÊ≥õ‰ΩøÁî®ÁöÑËøë‰ººÊñπÊ≥ïÊòØÁéªÊÅ©-Â••Êú¨Êµ∑ÈªòËøë‰ºº„ÄÇÂÆÉÊØîÁéªÊÅ©-ÈªÑËøë‰ººÊõ¥ÂÖ∑ÈôêÂà∂ÊÄß„ÄÇ</p>
<p><strong>If <span class="math inline">\(C_{kk} = 0\)</span>
(Born-Oppenheimer approximation) Ëã•<span class="math inline">\(C_{kk} =
0\)</span>ÔºàÁéªÊÅ©-Â••Êú¨Êµ∑ÈªòËøë‰ººÔºâ</strong></p>
<p>This assumes that the diagonal correction term is also negligible. By
setting all <span class="math inline">\(C_{kn}=0\)</span> (both diagonal
and off-diagonal), the equations become completely decoupled, and the
nuclear motion evolves independently on each potential energy surface.
ËøôÂÅáËÆæÂØπËßí‰øÆÊ≠£È°π‰πüÂèØÂøΩÁï•‰∏çËÆ°„ÄÇÈÄöËøá‰ª§ÊâÄÊúâ<span
class="math inline">\(C_{kn}=0\)</span>ÔºàÂåÖÊã¨ÂØπËßíÂíåÈùûÂØπËßíÔºâÔºåÊñπÁ®ãÁªÑÂÆåÂÖ®Ëß£ËÄ¶ÔºåÂéüÂ≠êÊ†∏ËøêÂä®Âú®ÊØè‰∏™ÂäøËÉΩÈù¢‰∏äÁã¨Á´ãÊºîÂåñ„ÄÇ</p>
<p>The result is the standard <strong>time-dependent Schr√∂dinger
equation for the nuclei</strong>:
Áî±Ê≠§ÂèØÂæóÊ†áÂáÜÁöÑ<strong>ÂéüÂ≠êÊ†∏ÁöÑÂê´Êó∂ËñõÂÆöË∞îÊñπÁ®ã</strong>Ôºö</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k \right)
\Theta_k\)</span></p>
<p>This equation is the foundation of most of quantum chemistry. It
states that the nuclei move on a static potential energy surface <span
class="math inline">\(E_k(\vec{R})\)</span> provided by the electrons,
without any possibility of transitioning to other electronic states or
having the surface be corrected by their own motion.</p>
<p>ËØ•ÊñπÁ®ãÊòØÂ§ßÂ§öÊï∞ÈáèÂ≠êÂåñÂ≠¶ÁöÑÂü∫Á°Ä„ÄÇÂéüÂ≠êÊ†∏Âú®Áî±ÁîµÂ≠êÊèê‰æõÁöÑÈùôÊÄÅÂäøËÉΩÈù¢ <span
class="math inline">\(E_k(\vec{R})\)</span>
‰∏äËøêÂä®Ôºå‰∏çÂ≠òÂú®Ë∑ÉËøÅÂà∞ÂÖ∂‰ªñÁîµÂ≠êÊÄÅÊàñÂõ†Ëá™Ë∫´ËøêÂä®ËÄå‰øÆÊ≠£ÂäøËÉΩÈù¢ÁöÑÂèØËÉΩÊÄß„ÄÇ</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/18/img_assert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/18/img_assert/" class="post-title-link" itemprop="url">BLOGS - IMG Assert</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">ÂèëË°®‰∫é</span>

              <time title="ÂàõÂª∫Êó∂Èó¥Ôºö2025-09-18 10:00:00" itemprop="dateCreated datePublished" datetime="2025-09-18T10:00:00+08:00">2025-09-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Êõ¥Êñ∞‰∫é</span>
                <time title="‰øÆÊîπÊó∂Èó¥Ôºö2025-09-19 19:24:51" itemprop="dateModified" datetime="2025-09-19T19:24:51+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">ÂàÜÁ±ª‰∫é</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">ÊäÄÊúØ</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="ÈóÆÈ¢ò‰∏ªË¶Å‰∏∫‰∫ÜÂõæÂÉè‰∏çÊòæÁ§∫ÈóÆÈ¢ò">„ÄêÈóÆÈ¢ò„Äë‰∏ªË¶Å‰∏∫‰∫ÜÂõæÂÉè‰∏çÊòæÁ§∫ÈóÆÈ¢ò</h2>
<h3 id="step1Ê†πÁõÆÂΩï‰∏≠ÁöÑÈÖçÁΩÆÊñá‰ª∂">Step1:Ê†πÁõÆÂΩï‰∏≠ÁöÑÈÖçÁΩÆÊñá‰ª∂</h3>
<h3 id="step2Â∞Ü-markdown-Ë°åÊõøÊç¢‰∏∫html-‰ª£Á†Å">Step2:Â∞Ü Markdown
Ë°åÊõøÊç¢‰∏∫HTML ‰ª£Á†Å</h3>
<h3 id="step3ËÆæÁΩÆ‰∏ãÊñπÊ∑ªÂä†root">Step3:ËÆæÁΩÆ‰∏ãÊñπÊ∑ªÂä†ROOT</h3>
<h3
id="step4‰∏çÈúÄË¶ÅÊ≠§Êèí‰ª∂ÁªàÁ´Ø‰∏≠ËøêË°å‰ª•‰∏ãÂëΩ‰ª§Êù•Âç∏ËΩΩÊèí‰ª∂">Step4:‰∏çÈúÄË¶ÅÊ≠§Êèí‰ª∂ÁªàÁ´Ø‰∏≠ËøêË°å‰ª•‰∏ãÂëΩ‰ª§Êù•Âç∏ËΩΩÊèí‰ª∂Ôºö</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment"># URL</span></span><br><span class="line"><span class="comment">## Set your site url here. For example, if you use GitHub Page, set url as &#x27;https://username.github.io/project&#x27;</span></span><br><span class="line">$ url: https://TianyaoBlogs.github.io/</span><br><span class="line"></span><br><span class="line">$ root: /</span><br><span class="line"></span><br><span class="line">$ permalink: :year/:month/:day/:title/</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ &lt;img src=<span class="string">&quot;/imgs/5054C3/General_linear_regression_model.png&quot;</span> alt=<span class="string">&quot;A diagram of the general linear regression model&quot;</span>&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm uninstall hexo-asset-image</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/17/5120C3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/17/5120C3/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W3-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">ÂèëË°®‰∫é</span>

              <time title="ÂàõÂª∫Êó∂Èó¥Ôºö2025-09-17 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-17T21:00:00+08:00">2025-09-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Êõ¥Êñ∞‰∫é</span>
                <time title="‰øÆÊîπÊó∂Èó¥Ôºö2025-09-19 20:28:09" itemprop="dateModified" datetime="2025-09-19T20:28:09+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">ÂàÜÁ±ª‰∫é</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - ËÆ°ÁÆóËÉΩÊ∫êÊùêÊñôÂíåÁîµÂ≠êÁªìÊûÑÊ®°Êãü Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="radial-distribution-function">1 radial distribution
function:</h2>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>:</li>
</ul>
<p>This whiteboard explains the process of calculating the
<strong>radial distribution function</strong>, often denoted as <span
class="math inline">\(g(r)\)</span>, to analyze the atomic structure of
a material, which is referred to here as a ‚Äúfilm‚Äù.
Êú¨ÁôΩÊùøËß£Èáä‰∫ÜËÆ°ÁÆó<strong>ÂæÑÂêëÂàÜÂ∏ÉÂáΩÊï∞</strong>ÔºàÈÄöÂ∏∏Ë°®Á§∫‰∏∫ <span
class="math inline">\(g(r)\)</span>ÔºâÁöÑËøáÁ®ãÔºåÁî®‰∫éÂàÜÊûêÊùêÊñôÔºàÊú¨Êñá‰∏≠Áß∞‰∏∫‚ÄúËñÑËÜú‚ÄùÔºâÁöÑÂéüÂ≠êÁªìÊûÑ„ÄÇ</p>
<p>In simple terms, the radial distribution function tells you the
probability of finding an atom at a certain distance from another
reference atom. It‚Äôs a powerful way to see the local structure in a
disordered system like a liquid or an amorphous solid.</p>
<p>ÁÆÄÂçïÊù•ËØ¥ÔºåÂæÑÂêëÂàÜÂ∏ÉÂáΩÊï∞Ë°®Á§∫Âú®Ë∑ùÁ¶ªÂè¶‰∏Ä‰∏™ÂèÇËÄÉÂéüÂ≠ê‰∏ÄÂÆöË∑ùÁ¶ªÂ§ÑÊâæÂà∞‰∏Ä‰∏™ÂéüÂ≠êÁöÑÊ¶ÇÁéá„ÄÇÂÆÉÊòØËßÇÂØüÊó†Â∫èÁ≥ªÁªüÔºà‰æãÂ¶ÇÊ∂≤‰ΩìÊàñÈùûÊô∂ÊÄÅÂõ∫‰ΩìÔºâÂ±ÄÈÉ®ÁªìÊûÑÁöÑÊúâÊïàÊñπÊ≥ï„ÄÇ</p>
<h3 id="core-concept-radial-distribution-function-ÂæÑÂêëÂàÜÂ∏ÉÂáΩÊï∞">## Core
Concept: Radial Distribution Function ÂæÑÂêëÂàÜÂ∏ÉÂáΩÊï∞</h3>
<p>The main goal is to compute the radial distribution function, <span
class="math inline">\(g(r)\)</span>, which is defined as the ratio of
the actual number of atoms found in a thin shell at a distance <span
class="math inline">\(r\)</span> to the number of atoms you‚Äôd expect to
find if the material were an ideal gas (completely random).
‰∏ªË¶ÅÁõÆÊ†áÊòØËÆ°ÁÆóÂæÑÂêëÂàÜÂ∏ÉÂáΩÊï∞ <span
class="math inline">\(g(r)\)</span>ÔºåÂÖ∂ÂÆö‰πâ‰∏∫Âú®Ë∑ùÁ¶ª <span
class="math inline">\(r\)</span>
ÁöÑËñÑÂ£≥Â±Ç‰∏≠ÂÆûÈôÖÂèëÁé∞ÁöÑÂéüÂ≠êÊï∞‰∏éÊùêÊñô‰∏∫ÁêÜÊÉ≥Ê∞î‰ΩìÔºàÂÆåÂÖ®ÈöèÊú∫ÔºâÊó∂È¢ÑÊúüÂèëÁé∞ÁöÑÂéüÂ≠êÊï∞‰πãÊØî„ÄÇ</p>
<p>The formula is expressed as: <span class="math display">\[g(r)dr =
\frac{n(r)}{\text{ideal gas}}\]</span></p>
<ul>
<li><strong><span class="math inline">\(n(r)\)</span></strong>:
Represents the average number of atoms found in a thin spherical shell
between a distance <span class="math inline">\(r\)</span> and <span
class="math inline">\(r+dr\)</span> from a central atom.
Ë°®Á§∫Ë∑ùÁ¶ª‰∏≠ÂøÉÂéüÂ≠ê <span class="math inline">\(r\)</span> Âà∞ <span
class="math inline">\(r+dr\)</span> ‰πãÈó¥ÁöÑËñÑÁêÉÂ£≥‰∏≠ÂéüÂ≠êÁöÑÂπ≥ÂùáÊï∞Èáè„ÄÇ</li>
<li><strong>ideal gas</strong>: Represents the number of atoms you would
expect in that same shell if the atoms were distributed completely
randomly with the same average density (<span
class="math inline">\(\rho\)</span>). The volume of this shell is
approximately <span class="math inline">\(4\pi r^2
dr\)</span>.Ë°®Á§∫Â¶ÇÊûúÂéüÂ≠êÂÆåÂÖ®ÈöèÊú∫ÂàÜÂ∏É‰∏îÂπ≥ÂùáÂØÜÂ∫¶ (<span
class="math inline">\(\rho\)</span>)
Áõ∏ÂêåÔºåÂàôËØ•ÁêÉÂ£≥‰∏≠ÂéüÂ≠êÁöÑÊï∞Èáè„ÄÇËØ•ÁêÉÂ£≥ÁöÑ‰ΩìÁßØÁ∫¶‰∏∫ <span
class="math inline">\(4\pi r^2 dr\)</span>„ÄÇ</li>
</ul>
<p>A peak in the <span class="math inline">\(g(r)\)</span> plot
indicates a high probability of finding neighboring atoms at that
specific distance, revealing the material‚Äôs structural shells (e.g.,
nearest neighbors, second-nearest neighbors, etc.).<span
class="math inline">\(g(r)\)</span>
Âõæ‰∏≠ÁöÑÂ≥∞ÂÄºË°®Á§∫Âú®ËØ•ÁâπÂÆöË∑ùÁ¶ªÂ§ÑÊâæÂà∞Áõ∏ÈÇªÂéüÂ≠êÁöÑÊ¶ÇÁéáÂæàÈ´òÔºå‰ªéËÄåÊè≠Á§∫‰∫ÜÊùêÊñôÁöÑÁªìÊûÑÂ£≥Ôºà‰æãÂ¶ÇÔºåÊúÄËøëÈÇª„ÄÅÊ¨°ËøëÈÇªÁ≠âÔºâ„ÄÇ</p>
<h3 id="calculation-method">## Calculation Method</h3>
<p>The board outlines a two-step averaging process to get a
statistically meaningful result from simulation data (a ‚Äúfilm‚Äù at 20
frames per second).</p>
<ol type="1">
<li><p><strong>Average over atoms:</strong> In a single frame (a
snapshot in time), you pick one atom as the center. Then, you count how
many other atoms (<span class="math inline">\(n(r)\)</span>) are in
concentric spherical shells around it. This process is repeated,
treating each atom in the frame as the center, and the results are
averaged.</p></li>
<li><p><strong>Average over frames:</strong> The entire process
described above is repeated for multiple frames from the simulation or
video. This time-averaging ensures that the final result represents the
typical structure of the material over time, smoothing out random
fluctuations.</p></li>
</ol>
<p>The board notes ‚Äúdx = bin width 0.01√Ö‚Äù, which is a practical detail
for the calculation. To create a histogram, the distance <code>r</code>
is divided into small segments (bins) of 0.01 angstroms.</p>
<h3 id="connection-to-experiments">## Connection to Experiments</h3>
<p>Finally, the whiteboard mentions <strong>‚Äúframe X-ray
scattering‚Äù</strong>. This is a crucial point because it connects this
computational analysis to real-world experiments. Experimental
techniques like X-ray or neutron scattering can be used to measure a
quantity called the structure factor, <span
class="math inline">\(S(q)\)</span>, which is directly related to the
radial distribution function <span class="math inline">\(g(r)\)</span>
through a mathematical operation called a Fourier transform. This allows
scientists to directly compare the structure produced in their
simulations with the structure of a real material measured in a lab.
ÊúÄÂêéÔºåÁôΩÊùø‰∏äÊèêÂà∞‰∫Ü<strong>‚ÄúÂ∏ß X
Â∞ÑÁ∫øÊï£Â∞Ñ‚Äù</strong>„ÄÇËøô‰∏ÄÁÇπËá≥ÂÖ≥ÈáçË¶ÅÔºåÂõ†‰∏∫ÂÆÉÂ∞ÜËÆ°ÁÆóÂàÜÊûê‰∏éÂÆûÈôÖÂÆûÈ™åËÅîÁ≥ªËµ∑Êù•„ÄÇXÂ∞ÑÁ∫øÊàñ‰∏≠Â≠êÊï£Â∞ÑÁ≠âÂÆûÈ™åÊäÄÊúØÂèØ‰ª•Áî®Êù•ÊµãÈáè‰∏Ä‰∏™Áß∞‰∏∫ÁªìÊûÑÂõ†Â≠ê<span
class="math inline">\(S(q)\)</span>ÁöÑÈáèÔºåËØ•ÈáèÈÄöËøáÂÇÖÈáåÂè∂ÂèòÊç¢ÁöÑÊï∞Â≠¶ËøêÁÆó‰∏éÂæÑÂêëÂàÜÂ∏ÉÂáΩÊï∞<span
class="math inline">\(g(r)\)</span>Áõ¥Êé•Áõ∏ÂÖ≥„ÄÇËøô‰ΩøÂæóÁßëÂ≠¶ÂÆ∂ËÉΩÂ§üÁõ¥Êé•Â∞ÜÊ®°Êãü‰∏≠‰∫ßÁîüÁöÑÁªìÊûÑ‰∏éÂÆûÈ™åÂÆ§ÊµãÈáèÁöÑÁúüÂÆûÊùêÊñôÁªìÊûÑËøõË°åÊØîËæÉ„ÄÇ</p>
<p>The board correctly links <span class="math inline">\(g(r)\)</span>
to X-ray scattering experiments. The quantity measured in these
experiments is the <strong>static structure factor</strong>, <span
class="math inline">\(S(q)\)</span>, which describes how the material
scatters radiation. The relationship between the two is a Fourier
transform: ËØ•ÊùøÊ≠£Á°ÆÂú∞Â∞Ü<span
class="math inline">\(g(r)\)</span>‰∏éXÂ∞ÑÁ∫øÊï£Â∞ÑÂÆûÈ™åËÅîÁ≥ªËµ∑Êù•„ÄÇËøô‰∫õÂÆûÈ™å‰∏≠ÊµãÈáèÁöÑÈáèÊòØ<strong>ÈùôÊÄÅÁªìÊûÑÂõ†Â≠ê</strong><span
class="math inline">\(S(q)\)</span>ÔºåÂÆÉÊèèËø∞‰∫ÜÊùêÊñôÂ¶Ç‰ΩïÊï£Â∞ÑËæêÂ∞Ñ„ÄÇ‰∏§ËÄÖ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÊòØÂÇÖÈáåÂè∂ÂèòÊç¢Ôºö
<span class="math display">\[S(q) = 1 + 4 \pi \rho \int_0^\infty [g(r) -
1] r^2 \frac{\sin(qr)}{qr} dr\]</span> This equation is crucial because
it bridges the gap between computer simulations (which calculate <span
class="math inline">\(g(r)\)</span>) and physical experiments (which
measure <span class="math inline">\(S(q)\)</span>).
Ëøô‰∏™ÊñπÁ®ãËá≥ÂÖ≥ÈáçË¶ÅÔºåÂõ†‰∏∫ÂÆÉÂº•Âêà‰∫ÜËÆ°ÁÆóÊú∫Ê®°ÊãüÔºàËÆ°ÁÆó <span
class="math inline">\(g(r)\)</span>ÔºâÂíåÁâ©ÁêÜÂÆûÈ™åÔºàÊµãÈáè <span
class="math inline">\(S(q)\)</span>Ôºâ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ</p>
<h3
id="the-gaussian-distribution-probability-of-particle-position-È´òÊñØÂàÜÂ∏ÉÁ≤íÂ≠ê‰ΩçÁΩÆÁöÑÊ¶ÇÁéá">##
2. The Gaussian Distribution: Probability of Particle Position
È´òÊñØÂàÜÂ∏ÉÔºöÁ≤íÂ≠ê‰ΩçÁΩÆÁöÑÊ¶ÇÁéá</h3>
<p>The board starts with the formula for a one-dimensional
<strong>Gaussian (or normal) distribution</strong>:
ÁôΩÊùøÈ¶ñÂÖàÂ±ïÁ§∫ÁöÑÊòØ‰∏ÄÁª¥<strong>È´òÊñØÔºàÊàñÊ≠£ÊÄÅÔºâÂàÜÂ∏É</strong>ÁöÑÂÖ¨ÂºèÔºö</p>
<p><span class="math display">\[f(x | \mu, \sigma^2) =
\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</span></p>
<p>This equation describes the probability of finding a particle at a
specific position <code>x</code> after a certain amount of time has
passed. * <strong><span class="math inline">\(\mu\)</span> (mu)</strong>
is the <strong>mean</strong> or average position. For a simple diffusion
process starting at the origin, the particles spread out symmetrically,
so the average position remains at the origin (<span
class="math inline">\(\mu = 0\)</span>). * <strong><span
class="math inline">\(\sigma^2\)</span> (sigma squared)</strong> is the
<strong>variance</strong>, which measures how spread out the particles
are from the mean position. A larger variance means the particles have,
on average, traveled farther from the starting point.
Ëøô‰∏™ÊñπÁ®ãÊèèËø∞‰∫ÜÁªèËøá‰∏ÄÂÆöÊó∂Èó¥ÂêéÔºåÂú®ÁâπÂÆö‰ΩçÁΩÆ‚Äúx‚ÄùÊâæÂà∞Á≤íÂ≠êÁöÑÊ¶ÇÁéá„ÄÇ *
<strong><span class="math inline">\(\mu\)</span> (mu)</strong>
ÊòØ<strong>Âπ≥ÂùáÂÄº</strong>ÊàñÂπ≥Âùá‰ΩçÁΩÆ„ÄÇÂØπ‰∫é‰ªéÂéüÁÇπÂºÄÂßãÁöÑÁÆÄÂçïÊâ©Êï£ËøáÁ®ãÔºåÁ≤íÂ≠êÂØπÁß∞Êâ©Êï£ÔºåÂõ†Ê≠§Âπ≥Âùá‰ΩçÁΩÆ‰øùÊåÅÂú®ÂéüÁÇπÔºà<span
class="math inline">\(\mu = 0\)</span>Ôºâ„ÄÇ * <strong><span
class="math inline">\(\sigma^2\)</span>Ôºàsigma Âπ≥ÊñπÔºâ</strong>
ÊòØ<strong>ÊñπÂ∑Æ</strong>ÔºåÁî®‚Äã‚Äã‰∫éË°°ÈáèÁ≤íÂ≠ê‰∏éÂπ≥Âùá‰ΩçÁΩÆÁöÑÊâ©Êï£Á®ãÂ∫¶„ÄÇÊñπÂ∑ÆË∂äÂ§ßÔºåÊÑèÂë≥ÁùÄÁ≤íÂ≠êÂπ≥ÂùáË∑ùÁ¶ªËµ∑ÁÇπË∂äËøú„ÄÇ</p>
<p>The note ‚ÄúBlack-Scholes‚Äù is a side reference. The Black-Scholes
model, famous in financial mathematics for pricing options, uses similar
mathematical principles based on Brownian motion to model the random
fluctuations of stock prices. ‚ÄúBlack-Scholes‚ÄùÊ≥®Èáä‰ªÖ‰æõÂèÇËÄÉ„ÄÇBlack-Scholes
Ê®°ÂûãÂú®ÈáëËûçÊï∞Â≠¶‰∏≠‰ª•ÊúüÊùÉÂÆö‰ª∑ËÄåÈóªÂêçÔºåÂÆÉ‰ΩøÁî®Âü∫‰∫éÂ∏ÉÊúóËøêÂä®ÁöÑÁ±ª‰ººÊï∞Â≠¶ÂéüÁêÜÊù•Ê®°ÊãüËÇ°Á•®‰ª∑Ê†ºÁöÑÈöèÊú∫Ê≥¢Âä®„ÄÇ</p>
<h3
id="mean-squared-displacement-msd-quantifying-the-spread-ÂùáÊñπ‰ΩçÁßª-msdÈáèÂåñÊâ©Êï£">##
3. Mean Squared Displacement (MSD): Quantifying the Spread ÂùáÊñπ‰ΩçÁßª
(MSD)ÔºöÈáèÂåñÊâ©Êï£</h3>
<p>The core of the board is dedicated to the <strong>Mean Squared
Displacement (MSD)</strong>. This is the primary tool used to measure
how far, on average, particles have moved over a time interval
<code>t</code>. Êú¨ÁâàÂùóÁöÑÊ†∏ÂøÉÂÜÖÂÆπÊòØ<strong>ÂùáÊñπ‰ΩçÁßª
(MSD)</strong>„ÄÇËøôÊòØÁî®‰∫éÊµãÈáèÁ≤íÂ≠êÂú®Êó∂Èó¥Èó¥Èöî‚Äút‚ÄùÂÜÖÂπ≥ÂùáÁßªÂä®Ë∑ùÁ¶ªÁöÑ‰∏ªË¶ÅÂ∑•ÂÖ∑„ÄÇ</p>
<p>The variance <span class="math inline">\(\sigma^2\)</span> is
formally defined as the average of the squared deviations from the mean:
<span class="math display">\[\sigma^2 = \langle x^2(t) \rangle - \langle
x(t) \rangle^2\]</span> * <span class="math inline">\(\langle x(t)
\rangle\)</span> is the average displacement. As mentioned, for simple
diffusion, <span class="math inline">\(\langle x(t) \rangle =
0\)</span>. * <span class="math inline">\(\langle x^2(t)
\rangle\)</span> is the average of the <em>square</em> of the
displacement. ÊñπÂ∑Æ<span
class="math inline">\(\sigma^2\)</span>ÁöÑÊ≠£ÂºèÂÆö‰πâ‰∏∫‰∏éÂπ≥ÂùáÂÄºÂÅèÂ∑ÆÂπ≥ÊñπÁöÑÂπ≥ÂùáÂÄºÔºö
<span class="math display">\[\sigma^2 = \langle x^2(t) \rangle - \langle
x(t) \rangle^2\]</span> * <span class="math inline">\(\langle x(t)
\rangle\)</span>ÊòØÂπ≥Âùá‰ΩçÁßª„ÄÇÂ¶Ç‰∏äÊâÄËø∞ÔºåÂØπ‰∫éÁÆÄÂçïÊâ©Êï£Ôºå<span
class="math inline">\(\langle x(t) \rangle = 0\)</span>„ÄÇ * <span
class="math inline">\(\langle x^2(t)
\rangle\)</span>ÊòØ‰ΩçÁßª<em>Âπ≥Êñπ</em>ÁöÑÂπ≥ÂùáÂÄº„ÄÇ</p>
<p>Since <span class="math inline">\(\langle x(t) \rangle = 0\)</span>,
the variance is simply equal to the MSD: <span
class="math display">\[\sigma^2 = \langle x^2(t) \rangle\]</span> Áî±‰∫é
<span class="math inline">\(\langle x(t) \rangle =
0\)</span>ÔºåÊñπÂ∑ÆÁ≠â‰∫éÂùáÊñπÂ∑Æ (MSD)Ôºö <span class="math display">\[\sigma^2
= \langle x^2(t) \rangle\]</span></p>
<p>The crucial insight for a diffusive process is that the <strong>MSD
grows linearly with time</strong>. The rate of this growth is determined
by the <strong>diffusion coefficient, D</strong>. The board shows this
relationship for different dimensions: Êâ©Êï£ËøáÁ®ãÁöÑÂÖ≥ÈîÆÂú®‰∫é<strong>MSD
ÈöèÊó∂Èó¥Á∫øÊÄßÂ¢ûÈïø</strong>„ÄÇÂÖ∂Â¢ûÈïøÁéáÁî±<strong>Êâ©Êï£Á≥ªÊï∞
D</strong>ÂÜ≥ÂÆö„ÄÇÊ£ãÁõòÊòæÁ§∫‰∫Ü‰∏çÂêåÁª¥Â∫¶‰∏ãÁöÑËøôÁßçÂÖ≥Á≥ªÔºö</p>
<ul>
<li><strong>1D:</strong> <span class="math inline">\(\langle x^2(t)
\rangle = 2Dt\)</span> (Movement along a line) ÔºàÊ≤øÁõ¥Á∫øËøêÂä®Ôºâ</li>
<li><strong>2D:</strong> The board has a slight typo or ambiguity with
<span class="math inline">\(\langle z^2(t) \rangle = 2Dt\)</span>. For
2D motion in the x-y plane, the total MSD would be <span
class="math inline">\(\langle r^2(t) \rangle = \langle x^2(t) \rangle +
\langle y^2(t) \rangle = 4Dt\)</span>. The note on the board might be
referring to just one component of motion. **Ê£ãÁõò‰∏äÁöÑ <span
class="math inline">\(\langle z^2(t) \rangle = 2Dt\)</span>
Â≠òÂú®ËΩªÂæÆÊãºÂÜôÈîôËØØÊàñÊ≠ß‰πâ„ÄÇÂØπ‰∫é x-y Âπ≥Èù¢‰∏äÁöÑ‰∫åÁª¥ËøêÂä®ÔºåÊÄªÂπ≥ÂùáÊï£Â∞ÑÂ∑Æ (MSD) ‰∏∫
<span class="math inline">\(\langle r^2(t) \rangle = \langle x^2(t)
\rangle + \langle y^2(t) \rangle =
4Dt\)</span>„ÄÇÈªëÊùø‰∏äÁöÑÊ≥®ÈáäÂèØËÉΩ‰ªÖÊåáËøêÂä®ÁöÑ‰∏Ä‰∏™ÂàÜÈáè„ÄÇ</li>
<li><strong>3D:</strong> <span class="math inline">\(\langle r^2(t)
\rangle = \langle |\vec{r}(t) - \vec{r}(0)|^2 \rangle = 6Dt\)</span>
(Movement in 3D space, which is the most common case in molecular
simulations) Ôºà‰∏âÁª¥Á©∫Èó¥‰∏≠ÁöÑËøêÂä®ÔºåËøôÊòØÂàÜÂ≠êÊ®°Êãü‰∏≠ÊúÄÂ∏∏ËßÅÁöÑÊÉÖÂÜµÔºâ Here,
<span class="math inline">\(\vec{r}(t)\)</span> is the position vector
of a particle at time <code>t</code>. The quantity <span
class="math inline">\(\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle\)</span> is the average of the squared distance a particle has
traveled from its initial position <span
class="math inline">\(\vec{r}(0)\)</span>. ËøôÈáåÔºå<span
class="math inline">\(\vec{r}(t)\)</span> ÊòØÁ≤íÂ≠êÂú®Êó∂Èó¥ <code>t</code>
ÁöÑ‰ΩçÁΩÆÁü¢Èáè„ÄÇ <span class="math inline">\(\langle |\vec{r}(t) -
\vec{r}(0)|^2 \rangle\)</span> ÊòØÁ≤íÂ≠ê‰ªéÂÖ∂ÂàùÂßã‰ΩçÁΩÆ <span
class="math inline">\(\vec{r}(0)\)</span> Ë°åËøõË∑ùÁ¶ªÁöÑÂπ≥ÊñπÂπ≥ÂùáÂÄº„ÄÇ</li>
</ul>
<h3
id="the-einstein-relation-connecting-microscopic-motion-to-a-macroscopic-property-Áà±Âõ†ÊñØÂù¶ÂÖ≥Á≥ªÂ∞ÜÂæÆËßÇËøêÂä®‰∏éÂÆèËßÇÁâπÊÄßËÅîÁ≥ªËµ∑Êù•">##
4. The Einstein Relation: Connecting Microscopic Motion to a Macroscopic
Property Áà±Âõ†ÊñØÂù¶ÂÖ≥Á≥ªÔºöÂ∞ÜÂæÆËßÇËøêÂä®‰∏éÂÆèËßÇÁâπÊÄßËÅîÁ≥ªËµ∑Êù•</h3>
<p>Finally, the board presents the famous <strong>Einstein
relation</strong>, which rearranges the 3D MSD equation to solve for the
diffusion coefficient <code>D</code>:</p>
<p><span class="math display">\[D = \lim_{t \to \infty} \frac{\langle
|\vec{r}(t) - \vec{r}(0)|^2 \rangle}{6t}\]</span></p>
<p>This is a cornerstone equation in statistical mechanics. It provides
a practical way to calculate a macroscopic property‚Äîthe
<strong>diffusion coefficient <code>D</code></strong>‚Äîfrom the
microscopic movements of individual particles observed in a computer
simulation.
ËøôÊòØÁªüËÆ°ÂäõÂ≠¶‰∏≠ÁöÑ‰∏Ä‰∏™Âü∫Áü≥ÊñπÁ®ã„ÄÇÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÁßçÂÆûÁî®ÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•ÈÄöËøáËÆ°ÁÆóÊú∫Ê®°Êãü‰∏≠ËßÇÂØüÂà∞ÁöÑÂçï‰∏™Á≤íÂ≠êÁöÑÂæÆËßÇËøêÂä®Êù•ËÆ°ÁÆóÂÆèËßÇÂ±ûÊÄß‚Äî‚ÄîÊâ©Êï£Á≥ªÊï∞‚ÄúD‚Äù„ÄÇ</p>
<p>In practice, one would: 1. Run a simulation of particles.
ËøêË°åÁ≤íÂ≠êÊ®°Êãü„ÄÇ 2. Track the position of each particle over time.
Ë∑üË∏™ÊØè‰∏™Á≤íÂ≠êÈöèÊó∂Èó¥ÁöÑ‰ΩçÁΩÆ„ÄÇ 3. Calculate the squared displacement <span
class="math inline">\(|\vec{r}(t) - \vec{r}(0)|^2\)</span> for each
particle at various time intervals <code>t</code>.
ËÆ°ÁÆóÊØè‰∏™Á≤íÂ≠êÂú®‰∏çÂêåÊó∂Èó¥Èó¥Èöî‚Äút‚ÄùÁöÑ‰ΩçÁßªÂπ≥Êñπ<span
class="math inline">\(|\vec{r}(t) - \vec{r}(0)|^2\)</span>„ÄÇ 4. Average
this value over all particles to get the MSD, <span
class="math inline">\(\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle\)</span>. ÂØπÊâÄÊúâÁ≤íÂ≠êÂèñÂπ≥ÂùáÂÄºÔºåÂæóÂà∞ÂùáÊñπÂ∑ÆÔºàMSDÔºâÔºåÂç≥<span
class="math inline">\(\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle\)</span>„ÄÇ 5. Plot the MSD as a function of time.
Â∞ÜMSDÁªòÂà∂ÊàêÊó∂Èó¥ÂáΩÊï∞„ÄÇ 6. The slope of this line, divided by 6, gives the
diffusion coefficient <code>D</code>. The <code>lim t‚Üí‚àû</code> indicates
that this linear relationship is most accurate for long time scales,
after initial transient effects have died down.
ËøôÊù°Áõ¥Á∫øÁöÑÊñúÁéáÈô§‰ª•6ÔºåÂç≥Êâ©Êï£Á≥ªÊï∞‚ÄúD‚Äù„ÄÇ‚Äúlim
t‚Üí‚àû‚ÄùË°®ÊòéÔºåÂú®ÂàùÂßãÁû¨ÊÄÅÊïàÂ∫îÊ∂àÈÄÄÂêéÔºåËøôÁßçÁ∫øÊÄßÂÖ≥Á≥ªÂú®ÈïøÊó∂Èó¥Â∞∫Â∫¶‰∏äÊúÄ‰∏∫ÂáÜÁ°Æ„ÄÇ</p>
<h3 id="right-board-green-kubo-relations">## 5. Right Board: Green-Kubo
Relations</h3>
<p>This board introduces a more advanced and powerful method to
calculate transport coefficients like the diffusion coefficient, known
as the <strong>Green-Kubo relations</strong>.
Êú¨Èù¢Êùø‰ªãÁªç‰∫Ü‰∏ÄÁßçÊõ¥ÂÖàËøõ„ÄÅÊõ¥Âº∫Â§ßÁöÑÊñπÊ≥ïÊù•ËÆ°ÁÆóÊâ©Êï£Á≥ªÊï∞Á≠â‰º†ËæìÁ≥ªÊï∞ÔºåÂç≥<strong>Green-Kubo
ÂÖ≥Á≥ª</strong>„ÄÇ</p>
<h4 id="velocity-autocorrelation-function-vacf-ÈÄüÂ∫¶Ëá™Áõ∏ÂÖ≥ÂáΩÊï∞-vacf">###
<strong>Velocity Autocorrelation Function (VACF)</strong> ÈÄüÂ∫¶Ëá™Áõ∏ÂÖ≥ÂáΩÊï∞
(VACF)</h4>
<p>The key idea is to look at how a particle‚Äôs velocity at one point in
time is related to its velocity at a later time. This is measured by the
<strong>Velocity Autocorrelation Function (VACF)</strong>: <span
class="math display">\[C_{vv}(t) = \langle \vec{v}(t&#39;) \cdot
\vec{v}(t&#39; + t) \rangle\]</span> This function tells us how long a
particle ‚Äúremembers‚Äù its velocity. For a typical liquid, the velocity is
quickly randomized by collisions, so the VACF decays to zero rapidly.
ÂÖ∂Ê†∏ÂøÉÊÄùÊÉ≥ÊòØËÄÉÂØüÁ≤íÂ≠êÂú®Êüê‰∏ÄÊó∂Èó¥ÁÇπÁöÑÈÄüÂ∫¶‰∏éÂÖ∂Âú®‰πãÂêéÊó∂Èó¥ÁÇπÁöÑÈÄüÂ∫¶‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇËøôÂèØ‰ª•ÈÄöËøá<strong>ÈÄüÂ∫¶Ëá™Áõ∏ÂÖ≥ÂáΩÊï∞
(VACF)</strong>Êù•ÊµãÈáèÔºö <span class="math display">\[C_{vv}(t) = \langle
\vec{v}(t&#39;) \cdot \vec{v}(t&#39; + t) \rangle\]</span>
Ê≠§ÂáΩÊï∞ÂëäËØâÊàë‰ª¨Á≤íÂ≠ê‚ÄúËÆ∞‰Ωè‚ÄùÂÖ∂ÈÄüÂ∫¶ÁöÑÊó∂Èó¥„ÄÇÂØπ‰∫éÂÖ∏ÂûãÁöÑÊ∂≤‰ΩìÔºåÈÄüÂ∫¶‰ºöÂõ†Á¢∞ÊíûËÄåËøÖÈÄüÈöèÊú∫ÂåñÔºåÂõ†Ê≠§
VACF ‰ºöËøÖÈÄüË°∞Âáè‰∏∫Èõ∂„ÄÇ</p>
<h4 id="connecting-msd-and-vacf">### <strong>Connecting MSD and
VACF</strong></h4>
<p>The board shows the mathematical link between the MSD and the VACF.
Starting with the definition of position as the integral of velocity,
<span class="math inline">\(\vec{r}(t) = \int_0^t \vec{v}(t&#39;)
dt&#39;\)</span>, one can show that the MSD is a double integral of the
VACF. The board writes this as: <span class="math display">\[\langle
x^2(t) \rangle = \left\langle \left( \int_0^t v(t&#39;) dt&#39; \right)
\left( \int_0^t v(t&#39;&#39;) dt&#39;&#39; \right) \right\rangle =
\int_0^t dt&#39; \int_0^t dt&#39;&#39; \langle v(t&#39;) v(t&#39;&#39;)
\rangle\]</span> This shows that the two pictures of motion‚Äîthe
particle‚Äôs displacement (MSD) and its velocity fluctuations (VACF)‚Äîare
deeply connected. ËØ•Èù¢ÊùøÂ±ïÁ§∫‰∫Ü MSD Âíå VACF
‰πãÈó¥ÁöÑÊï∞Â≠¶ËÅîÁ≥ª„ÄÇ‰ªé‰ΩçÁΩÆÂÆö‰πâ‰∏∫ÈÄüÂ∫¶ÁöÑÁßØÂàÜÂºÄÂßãÔºå<span
class="math inline">\(\vec{r}(t) = \int_0^t \vec{v}(t&#39;)
dt&#39;\)</span>ÔºåÂèØ‰ª•ËØÅÊòé MSD ÊòØ VACF ÁöÑ‰∫åÈáçÁßØÂàÜ„ÄÇÈªëÊùø‰∏äÂÜôÁùÄÔºö <span
class="math display">\[\langle x^2(t) \rangle = \left\langle \left(
\int_0^t v(t&#39;) dt&#39; \right) \left( \int_0^t v(t&#39;&#39;)
dt&#39;&#39; \right) \right\rangle = \int_0^t dt&#39; \int_0^t
dt&#39;&#39; \langle v(t&#39;) v(t&#39;&#39;) \rangle\]</span>
ËøôË°®ÊòéÔºåÁ≤íÂ≠êËøêÂä®ÁöÑ‰∏§ÂπÖÂõæÂÉè‚Äî‚ÄîÁ≤íÂ≠êÁöÑ‰ΩçÁßªÔºàMSDÔºâÂíåÈÄüÂ∫¶Ê∂®ËêΩÔºàVACFÔºâ‚Äî‚Äî‰πãÈó¥Â≠òÂú®ÁùÄÊ∑±ÂàªÁöÑËÅîÁ≥ª„ÄÇ</p>
<h4 id="the-green-kubo-formula-for-diffusion-Êâ©Êï£ÁöÑÊ†ºÊûó-‰πÖ‰øùÂÖ¨Âºè">###
<strong>The Green-Kubo Formula for Diffusion
Êâ©Êï£ÁöÑÊ†ºÊûó-‰πÖ‰øùÂÖ¨Âºè</strong></h4>
<p>By combining the Einstein relation with the integral of the VACF, one
arrives at the Green-Kubo formula for the diffusion coefficient: <span
class="math display">\[D = \frac{1}{3} \int_0^\infty \langle \vec{v}(0)
\cdot \vec{v}(t) \rangle dt\]</span> This incredible result states that
the <strong>macroscopic</strong> property of diffusion (<span
class="math inline">\(D\)</span>) is determined by the integral of the
<strong>microscopic</strong> velocity correlations. It‚Äôs often a more
efficient way to compute <span class="math inline">\(D\)</span> in
simulations than calculating the long-time limit of the MSD.
Â∞ÜÁà±Âõ†ÊñØÂù¶ÂÖ≥Á≥ª‰∏éVACFÁßØÂàÜÁõ∏ÁªìÂêàÔºåÂèØ‰ª•ÂæóÂà∞Êâ©Êï£Á≥ªÊï∞ÁöÑÊ†ºÊûó-‰πÖ‰øùÂÖ¨ÂºèÔºö <span
class="math display">\[D = \frac{1}{3} \int_0^\infty \langle \vec{v}(0)
\cdot \vec{v}(t) \rangle dt\]</span>
Ëøô‰∏™‰ª§‰∫∫Èöæ‰ª•ÁΩÆ‰ø°ÁöÑÁªìÊûúË°®ÊòéÔºåÊâ©Êï£ÁöÑ<strong>ÂÆèËßÇ</strong>ÁâπÊÄßÔºà<span
class="math inline">\(D\)</span>ÔºâÁî±<strong>ÂæÆËßÇ</strong>ÈÄüÂ∫¶ÂÖ≥ËÅîÁöÑÁßØÂàÜÂÜ≥ÂÆö„ÄÇÂú®Ê®°Êãü‰∏≠ÔºåËøôÈÄöÂ∏∏ÊòØËÆ°ÁÆó<span
class="math inline">\(D\)</span>ÊØîËÆ°ÁÆóMSDÁöÑÈïøÊúüÊûÅÈôêÊõ¥ÊúâÊïàÁöÑÊñπÊ≥ï„ÄÇ</p>
<h3 id="the-grand-narrative-from-micro-to-macro-ÂÆèÂ§ßÂèô‰∫ã‰ªéÂæÆËßÇÂà∞ÂÆèËßÇ">##
6. The Grand Narrative: From Micro to Macro ÂÆèÂ§ßÂèô‰∫ãÔºö‰ªéÂæÆËßÇÂà∞ÂÆèËßÇ</h3>
<p>The previous whiteboards gave us two ways to calculate the
<strong>diffusion constant, D</strong>, from the microscopic random walk
of individual atoms:
‰πãÂâçÁöÑÁôΩÊùøÊèê‰æõ‰∫Ü‰∏§Áßç‰ªéÂçï‰∏™ÂéüÂ≠êÁöÑÂæÆËßÇÈöèÊú∫Ê∏∏Âä®ËÆ°ÁÆó<strong>Êâ©Êï£Â∏∏Êï∞
D</strong>ÁöÑÊñπÊ≥ïÔºö 1. <strong>Einstein Relation:</strong> From the
long-term slope of the Mean Squared Displacement (MSD). Ê†πÊçÆÂùáÊñπ‰ΩçÁßª
(MSD) ÁöÑÈïøÊúüÊñúÁéá„ÄÇ 2. <strong>Green-Kubo Relation:</strong> From the
integral of the Velocity Autocorrelation Function (VACF).
Ê†πÊçÆÈÄüÂ∫¶Ëá™Áõ∏ÂÖ≥ÂáΩÊï∞ (VACF) ÁöÑÁßØÂàÜ„ÄÇ</p>
<p>This new whiteboard shows how that single microscopic parameter,
<code>D</code>, governs the large-scale, observable process of diffusion
described by <strong>Fick‚Äôs Laws</strong> and the <strong>Diffusion
Equation</strong>. ËøôÂùóÊñ∞ÁöÑÁôΩÊùøÂ±ïÁ§∫‰∫ÜÂçï‰∏™ÂæÆËßÇÂèÇÊï∞ <code>D</code>
Â¶Ç‰ΩïÊéßÂà∂<strong>Ëè≤ÂÖãÂÆöÂæã</strong>Âíå<strong>Êâ©Êï£ÊñπÁ®ã</strong>ÊâÄÊèèËø∞ÁöÑÂ§ßËßÑÊ®°ÂèØËßÇÊµãÊâ©Êï£ËøáÁ®ã„ÄÇ</p>
<h3 id="the-starting-point-a-liquids-structure-Ëµ∑ÁÇπÊ∂≤‰ΩìÁöÑÁªìÊûÑ">## 1. The
Starting Point: A Liquid‚Äôs Structure Ëµ∑ÁÇπÔºöÊ∂≤‰ΩìÁöÑÁªìÊûÑ</h3>
<p>The plot on the top left is the <strong>Radial Distribution Function,
<span class="math inline">\(g(r)\)</span></strong>, which we discussed
in detail from the first whiteboard. Â∑¶‰∏äËßíÁöÑÂõæÊòØ<strong>ÂæÑÂêëÂàÜÂ∏ÉÂáΩÊï∞
<span
class="math inline">\(g(r)\)</span></strong>ÔºåÊàë‰ª¨Âú®Á¨¨‰∏Ä‰∏™ÁôΩÊùø‰∏äËØ¶ÁªÜËÆ®ËÆ∫ËøáÂÆÉ„ÄÇ</p>
<ul>
<li><strong>The Plot:</strong> It shows the characteristic structure of
a liquid. The peaks are labeled ‚Äú1st‚Äù, ‚Äú2nd‚Äù, and ‚Äú3rd‚Äù, corresponding
to the first, second, and third <strong>solvation shells</strong>
(layers of neighboring atoms).
ÂÆÉÊòæÁ§∫‰∫ÜÊ∂≤‰ΩìÁöÑÁâπÂæÅÁªìÊûÑ„ÄÇÂ≥∞ÂàÜÂà´Ê†áËÆ∞‰∏∫‚ÄúÁ¨¨‰∏Ä‚Äù„ÄÅ‚ÄúÁ¨¨‰∫å‚ÄùÂíå‚ÄúÁ¨¨‰∏â‚ÄùÔºåÂàÜÂà´ÂØπÂ∫î‰∫éÁ¨¨‰∏Ä„ÄÅÁ¨¨‰∫åÂíåÁ¨¨‰∏â<strong>Ê∫∂ÂâÇÂåñÂ£≥Â±Ç</strong>ÔºàÁõ∏ÈÇªÂéüÂ≠êÂ±ÇÔºâ„ÄÇ</li>
<li><strong>The Limit:</strong> The note <code>lim r‚Üí‚àû g(r) = 1</code>
confirms that at large distances, the liquid has no long-range order, as
expected.Ê≥®Èáä‚Äúlim r‚Üí‚àû g(r) =
1‚ÄùËØÅÂÆû‰∫ÜÂú®ËøúË∑ùÁ¶ª‰∏ãÔºåÊ∂≤‰ΩìÊ≤°ÊúâÈïøÁ®ãÊúâÂ∫èÔºåËøô‰∏éÈ¢ÑÊúü‰∏ÄËá¥„ÄÇ</li>
<li><strong>System Parameters:</strong> The values <code>T = 0.71</code>
and <code>œÅ = 0.844</code> are the temperature and density of the
simulated system (likely in reduced or ‚ÄúLennard-Jones‚Äù units) for which
this <span class="math inline">\(g(r)\)</span> was calculated. ÂÄº‚ÄúT =
0.71‚ÄùÂíå‚ÄúœÅ =
0.844‚ÄùÂàÜÂà´ÊòØÊ®°ÊãüÁ≥ªÁªüÁöÑÊ∏©Â∫¶ÂíåÂØÜÂ∫¶ÔºàÂèØËÉΩÈááÁî®Á∫¶ÂåñÊàñ‚ÄúLennard-Jones‚ÄùÂçï‰ΩçÔºâÔºåÁî®‰∫éËÆ°ÁÆóÊ≠§
<span class="math inline">\(g(r)\)</span>„ÄÇ</li>
</ul>
<p>This section sets the stage: we are looking at the dynamics within a
system that has this specific liquid-like structure.
Êú¨ËäÇÂ•†ÂÆö‰∫ÜÂü∫Á°ÄÔºöÊàë‰ª¨Â∞ÜÁ†îÁ©∂ÂÖ∑ÊúâÁâπÂÆöÁ±ªÊ∂≤‰ΩìÁªìÊûÑÁöÑÁ≥ªÁªüÂÜÖÁöÑÂä®ÂäõÂ≠¶„ÄÇ</p>
<h3 id="the-macroscopic-laws-of-diffusion-ÂÆèËßÇÊâ©Êï£ÂÆöÂæã">## 2. The
Macroscopic Laws of Diffusion ÂÆèËßÇÊâ©Êï£ÂÆöÂæã</h3>
<p>The bottom-left and top-right sections introduce the continuum
equations that describe how concentration changes in space and time.
Â∑¶‰∏ãËßíÂíåÂè≥‰∏äËßíÈÉ®ÂàÜ‰ªãÁªç‰∫ÜÊèèËø∞ÊµìÂ∫¶ÈöèÁ©∫Èó¥ÂíåÊó∂Èó¥ÂèòÂåñÁöÑËøûÁª≠ÊñπÁ®ã„ÄÇÂ∑¶‰∏ãËßíÂíåÂè≥‰∏äËßíÈÉ®ÂàÜ‰ªãÁªç‰∫ÜÊèèËø∞ÊµìÂ∫¶ÈöèÁ©∫Èó¥ÂíåÊó∂Èó¥ÂèòÂåñÁöÑËøûÁª≠ÊñπÁ®ã„ÄÇ</p>
<h4 id="ficks-first-law-Ëè≤ÂÖãÁ¨¨‰∏ÄÂÆöÂæã">### <strong>Fick‚Äôs First Law
Ëè≤ÂÖãÁ¨¨‰∏ÄÂÆöÂæã</strong></h4>
<p><span class="math display">\[\vec{J} = -D \nabla C\]</span> This is
Fick‚Äôs first law of diffusion. It states that there is a
<strong>flux</strong> of particles (<span
class="math inline">\(\vec{J}\)</span>), meaning a net flow. This flow
is directed from high concentration to low concentration (hence the
minus sign) and its magnitude is proportional to the
<strong>concentration gradient</strong> (<span
class="math inline">\(\nabla C\)</span>).
ËøôÊòØËè≤ÂÖãÁ¨¨‰∏ÄÊâ©Êï£ÂÆöÂæã„ÄÇÂÆÉÊåáÂá∫Â≠òÂú®Á≤íÂ≠êÁöÑ<strong>ÈÄöÈáè</strong> (<span
class="math inline">\(\vec{J}\)</span>)ÔºåÂç≥ÂáÄÊµÅÈáè„ÄÇËØ•ÊµÅÈáè‰ªéÈ´òÊµìÂ∫¶ÊµÅÂêë‰ΩéÊµìÂ∫¶ÔºàÂõ†Ê≠§Â∏¶ÊúâË¥üÂè∑ÔºâÔºåÂÖ∂Â§ßÂ∞è‰∏é<strong>ÊµìÂ∫¶Ê¢ØÂ∫¶</strong>
(<span class="math inline">\(\nabla C\)</span>) ÊàêÊ≠£ÊØî„ÄÇ</p>
<p><strong>The Crucial Link:</strong> The proportionality constant is
<strong>D</strong>, the very same <strong>diffusion constant</strong> we
calculated from the microscopic random walk (MSD/VACF). This is the key
connection: the collective result of countless individual random walks
is a predictable net flow of particles.
ÊØî‰æãÂ∏∏Êï∞ÊòØ<strong>D</strong>Ôºå‰∏éÊàë‰ª¨Ê†πÊçÆÂæÆËßÇÈöèÊú∫Ê∏∏Ëµ∞ (MSD/VACF)
ËÆ°ÁÆóÂá∫ÁöÑ<strong>Êâ©Êï£Â∏∏Êï∞</strong>ÂÆåÂÖ®Áõ∏Âêå„ÄÇËøôÊòØÂÖ≥ÈîÆÁöÑËÅîÁ≥ªÔºöÊó†Êï∞‰∏™‰ΩìÈöèÊú∫Ê∏∏Âä®ÁöÑÈõÜÂêàÁªìÊûúÊòØÂèØÈ¢ÑÊµãÁöÑÁ≤íÂ≠êÂáÄÊµÅ„ÄÇ</p>
<h4
id="the-diffusion-equation-ficks-second-law-Êâ©Êï£ÊñπÁ®ãËè≤ÂÖãÁ¨¨‰∫åÂÆöÂæã">###
<strong>The Diffusion Equation (Fick‚Äôs Second Law)
Êâ©Êï£ÊñπÁ®ãÔºàËè≤ÂÖãÁ¨¨‰∫åÂÆöÂæãÔºâ</strong></h4>
<p><span class="math display">\[\frac{\partial C(\vec{r},t)}{\partial t}
= D \nabla^2 C(\vec{r},t)\]</span> This is the <strong>diffusion
equation</strong>, one of the most important equations in physics and
chemistry (also called the heat equation, as noted). It‚Äôs derived from
Fick‚Äôs first law and the principle of mass conservation (<span
class="math inline">\(\frac{\partial C}{\partial t} + \nabla \cdot
\vec{J} = 0\)</span>). It‚Äôs a differential equation that tells you
exactly how the concentration at any point, <span
class="math inline">\(C(\vec{r},t)\)</span>, will change over time.
ËøôÂ∞±ÊòØ<strong>Êâ©Êï£ÊñπÁ®ã</strong>ÔºåÂÆÉÊòØÁâ©ÁêÜÂ≠¶ÂíåÂåñÂ≠¶‰∏≠ÊúÄÈáçË¶ÅÁöÑÊñπÁ®ã‰πã‰∏ÄÔºà‰πüÁß∞‰∏∫ÁÉ≠ÊñπÁ®ãÔºâ„ÄÇÂÆÉÊ∫ê‰∫éËè≤ÂÖãÁ¨¨‰∏ÄÂÆöÂæãÂíåË¥®ÈáèÂÆàÊÅíÂÆöÂæãÔºà<span
class="math inline">\(\frac{\partial C}{\partial t} + \nabla \cdot
\vec{J} = 0\)</span>Ôºâ„ÄÇÂÆÉÊòØ‰∏Ä‰∏™ÂæÆÂàÜÊñπÁ®ãÔºåÂèØ‰ª•Á≤æÁ°ÆÂú∞ÂëäËØâ‰Ω†‰ªªÊÑè‰∏ÄÁÇπÁöÑÊµìÂ∫¶
<span class="math inline">\(C(\vec{r},t)\)</span> ÈöèÊó∂Èó¥ÁöÑÂèòÂåñ„ÄÇ</p>
<h3
id="the-solution-connecting-back-to-the-random-walk-‰∏éÈöèÊú∫Ê∏∏Âä®ËÅîÁ≥ªËµ∑Êù•">##
3. The Solution: Connecting Back to the Random Walk
‰∏éÈöèÊú∫Ê∏∏Âä®ËÅîÁ≥ªËµ∑Êù•</h3>
<p>This is the most beautiful part. The board shows the solution to the
diffusion equation for a very specific scenario, linking the macroscopic
equation directly back to the microscopic random walk.
ÈªëÊùø‰∏äÂ±ïÁ§∫‰∫Ü‰∏Ä‰∏™ÈùûÂ∏∏ÂÖ∑‰ΩìÂú∫ÊôØ‰∏ãÊâ©Êï£ÊñπÁ®ãÁöÑËß£ÔºåÂ∞ÜÂÆèËßÇÊñπÁ®ãÁõ¥Êé•‰∏éÂæÆËßÇÈöèÊú∫Ê∏∏Âä®ËÅîÁ≥ªËµ∑Êù•„ÄÇ</p>
<h4 id="the-initial-condition-ÂàùÂßãÊù°‰ª∂">### <strong>The Initial
Condition ÂàùÂßãÊù°‰ª∂</strong></h4>
<p>The problem is set up by assuming all particles start at a single
point at time zero: <span class="math display">\[C(\vec{r}, 0) =
\delta(\vec{r})\]</span> This is a <strong>Dirac delta
function</strong>, representing an infinitely concentrated point source
at the origin. ÈóÆÈ¢òÂÅáËÆæÊâÄÊúâÁ≤íÂ≠êÂú®Êó∂Èó¥Èõ∂ÁÇπÂ§Ñ‰ªé‰∏Ä‰∏™ÁÇπÂºÄÂßãÔºö <span
class="math display">\[C(\vec{r}, 0) = \delta(\vec{r})\]</span>
ËøôÊòØ‰∏Ä‰∏™<strong>ÁãÑÊãâÂÖãÂáΩÊï∞</strong>ÔºåË°®Á§∫‰∏Ä‰∏™Âú®ÂéüÁÇπÂ§ÑÊó†ÈôêÈõÜ‰∏≠ÁöÑÁÇπÊ∫ê„ÄÇ</p>
<h4 id="the-fundamental-solution-greens-function-Âü∫Êú¨Ëß£Ê†ºÊûóÂáΩÊï∞">###
<strong>The Fundamental Solution (Green‚Äôs Function)
Âü∫Êú¨Ëß£ÔºàÊ†ºÊûóÂáΩÊï∞Ôºâ</strong></h4>
<p>The solution to the diffusion equation with this starting condition
is called the <strong>fundamental solution</strong> or <strong>Green‚Äôs
function</strong>. For one dimension, it is: <span
class="math display">\[C(x,t) = \frac{1}{\sqrt{4\pi Dt}}
\exp\left(-\frac{x^2}{4Dt}\right)\]</span></p>
<p><strong>The ‚ÄúAha!‚Äù Moment:</strong> This is a <strong>Gaussian
distribution</strong>. Let‚Äôs compare it to the formula from the second
whiteboard: * The mean is <span class="math inline">\(\mu=0\)</span>.
ÂùáÂÄº‰∏∫ <span class="math inline">\(\mu=0\)</span>„ÄÇ * The variance is
<span class="math inline">\(\sigma^2 = 2Dt\)</span>. ÊñπÂ∑Æ‰∏∫ <span
class="math inline">\(\sigma^2 = 2Dt\)</span>„ÄÇ</p>
<p>This is an incredible result. The macroscopic diffusion equation
predicts that a concentration pulse will spread out over time, and the
shape of the concentration profile will be a Gaussian curve. The width
of this curve, measured by its variance <span
class="math inline">\(\sigma^2\)</span>, is <strong>exactly the Mean
Squared Displacement, <span class="math inline">\(\langle x^2(t)
\rangle\)</span>, of the individual random-walking particles.</strong>
ÂÆèËßÇÊâ©Êï£ÊñπÁ®ãÈ¢ÑÊµãÊµìÂ∫¶ËÑâÂÜ≤‰ºöÈöèÊó∂Èó¥Êâ©Êï£ÔºåÊµìÂ∫¶ÂàÜÂ∏ÉÁöÑÂΩ¢Áä∂Â∞ÜÊòØÈ´òÊñØÊõ≤Á∫ø„ÄÇËøôÊù°Êõ≤Á∫øÁöÑÂÆΩÂ∫¶ÔºåÁî®ÂÖ∂ÊñπÂ∑Æ
<span class="math inline">\(\sigma^2\)</span>
Êù•Ë°°ÈáèÔºå<strong>ÊÅ∞Â•ΩÊòØÂçï‰∏™ÈöèÊú∫Ê∏∏Âä®Á≤íÂ≠êÁöÑÂùáÊñπ‰ΩçÁßª <span
class="math inline">\(\langle x^2(t) \rangle\)</span>„ÄÇ</strong></p>
<p>This perfectly unites the two perspectives: * <strong>MicroscopicÂæÆËßÇ
(Board 2):</strong> Particles undergo a random walk, and their average
squared displacement from the origin grows as <span
class="math inline">\(\langle x^2(t) \rangle = 2Dt\)</span>.
Á≤íÂ≠êËøõË°åÈöèÊú∫Ê∏∏Âä®ÔºåÂÆÉ‰ª¨Áõ∏ÂØπ‰∫éÂéüÁÇπÁöÑÂπ≥ÂùáÂπ≥Êñπ‰ΩçÁßªÈöèÁùÄ <span
class="math inline">\(\langle x^2(t) \rangle = 2Dt\)</span>
ÁöÑÂ¢ûÈïøËÄåÂ¢ûÈïø„ÄÇ * <strong>MacroscopicÂÆèËßÇ (This Board):</strong> A
collection of these particles, described by a continuum concentration
<code>C</code>, spreads out in a Gaussian profile whose variance is
<span class="math inline">\(\sigma^2 = 2Dt\)</span>.
Ëøô‰∫õÁ≤íÂ≠êÁöÑÈõÜÂêàÔºåÁî®ËøûÁª≠ÊµìÂ∫¶‚ÄúC‚ÄùÊù•ÊèèËø∞ÔºåÂëàÊñπÂ∑Æ‰∏∫ <span
class="math inline">\(\sigma^2 = 2Dt\)</span> ÁöÑÈ´òÊñØÂàÜÂ∏É„ÄÇ</p>
<p>The two pictures are mathematically identical.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/17/5120C3-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/17/5120C3-2/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W3-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">ÂèëË°®‰∫é</span>

              <time title="ÂàõÂª∫Êó∂Èó¥Ôºö2025-09-17 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-17T21:00:00+08:00">2025-09-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Êõ¥Êñ∞‰∫é</span>
                <time title="‰øÆÊîπÊó∂Èó¥Ôºö2025-09-21 05:21:00" itemprop="dateModified" datetime="2025-09-21T05:21:00+08:00">2025-09-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">ÂàÜÁ±ª‰∫é</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - ËÆ°ÁÆóËÉΩÊ∫êÊùêÊñôÂíåÁîµÂ≠êÁªìÊûÑÊ®°Êãü Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="radial-distribution-function-rdfÈùôÊÄÅÁªìÊûÑ">1 radial distribution
function RDFÈùôÊÄÅÁªìÊûÑ:</h2>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>: This whiteboard serves as an excellent
summary, pulling together all the key concepts we‚Äôve discussed into a
single, cohesive picture. Let‚Äôs connect everything on this slide to our
detailed conversation.</li>
</ul>
<h3 id="rdf-the-static-structure-rdfÈùôÊÄÅÁªìÊûÑ">1. RDF: The Static
Structure RDFÈùôÊÄÅÁªìÊûÑ</h3>
<p>On the top left, you see <strong>RDF (Radial Distribution
Function)</strong>.</p>
<ul>
<li><strong>The Plots:</strong> The board shows the familiar <span
class="math inline">\(g(r)\)</span> plot with its characteristic peaks
for a liquid. Below it is a plot of the interatomic potential energy,
<span class="math inline">\(V(r)\)</span>. This addition is very
insightful! It shows <em>why</em> the first peak in <span
class="math inline">\(g(r)\)</span> exists: it corresponds to the
minimum energy distance (<span class="math inline">\(\sigma\)</span>)
where particles are most stable and likely to be found.
ÁôΩÊùøÂ±ïÁ§∫‰∫ÜÊàë‰ª¨ÁÜüÊÇâÁöÑ<span
class="math inline">\(g(r)\)</span>ÂõæÔºåÂÆÉÂ∏¶ÊúâÊ∂≤‰ΩìÁöÑÁâπÂæÅÂ≥∞„ÄÇ‰∏ãÊñπÊòØÂéüÂ≠êÈó¥ÂäøËÉΩ<span
class="math inline">\(V(r)\)</span>ÁöÑÂõæ„ÄÇËøô‰∏™Ë°•ÂÖÖÈùûÂ∏∏ÊúâËßÅÂú∞ÔºÅÂÆÉËß£Èáä‰∫Ü‰∏∫‰ªÄ‰πà
<span class="math inline">\(g(r)\)</span>
‰∏≠ÁöÑÁ¨¨‰∏Ä‰∏™Â≥∞ÂÄºÂ≠òÂú®ÔºöÂÆÉÂØπÂ∫î‰∫éÁ≤íÂ≠êÊúÄÁ®≥ÂÆö‰∏îÊúÄÊúâÂèØËÉΩË¢´ÂèëÁé∞ÁöÑÊúÄÂ∞èËÉΩÈáèË∑ùÁ¶ª
(<span class="math inline">\(\sigma\)</span>)„ÄÇ</li>
<li><strong>Connection:</strong> This section summarizes our first
discussion. It‚Äôs the starting point for our analysis‚Äîa static snapshot
of the material‚Äôs average atomic arrangement before we consider how the
atoms move.
Êú¨ËäÇÊÄªÁªì‰∫ÜÊàë‰ª¨ÁöÑÁ¨¨‰∏Ä‰∏™ËÆ®ËÆ∫„ÄÇËøôÊòØÊàë‰ª¨ÂàÜÊûêÁöÑËµ∑ÁÇπ‚Äî‚ÄîÂú®Êàë‰ª¨ËÄÉËôëÂéüÂ≠êÂ¶Ç‰ΩïËøêÂä®‰πãÂâçÔºåÂÆÉÊòØÊùêÊñôÂπ≥ÂùáÂéüÂ≠êÊéíÂàóÁöÑÈùôÊÄÅÂø´ÁÖß„ÄÇ</li>
</ul>
<h3
id="msd-and-the-einstein-relation-the-displacement-picture-ÂùáÊñπ‰ΩçÁßª-msd-ÂíåÁà±Âõ†ÊñØÂù¶ÂÖ≥Á≥ª‰ΩçÁßªÂõæÂÉè">2.
MSD and The Einstein Relation: The Displacement Picture ÂùáÊñπ‰ΩçÁßª (MSD)
ÂíåÁà±Âõ†ÊñØÂù¶ÂÖ≥Á≥ªÔºö‰ΩçÁßªÂõæÂÉè</h3>
<p>The board then moves to dynamics, presenting two methods to calculate
the <strong>diffusion constant, D</strong>. The first is the
<strong>Einstein relation</strong>. ‰∏§ÁßçËÆ°ÁÆó<strong>Êâ©Êï£Â∏∏Êï∞
D</strong>ÁöÑÊñπÊ≥ï„ÄÇÁ¨¨‰∏ÄÁßçÊòØ<strong>Áà±Âõ†ÊñØÂù¶ÂÖ≥Á≥ª</strong>„ÄÇ</p>
<ul>
<li><strong>The Formula:</strong> It correctly states that the Mean
Squared Displacement (MSD), <span class="math inline">\(\langle r^2
\rangle\)</span>, is equal to <span class="math inline">\(6Dt\)</span>
in three dimensions. It then rearranges this to solve for <span
class="math inline">\(D\)</span>: ÂÆÉÊ≠£Á°ÆÂú∞ÊåáÂá∫‰∫ÜÂùáÊñπ‰ΩçÁßª (MSD)Ôºå<span
class="math inline">\(\langle r^2 \rangle\)</span>ÔºåÂú®‰∏âÁª¥Á©∫Èó¥‰∏≠Á≠â‰∫é
<span class="math inline">\(6Dt\)</span>„ÄÇÁÑ∂ÂêéÈáçÊñ∞ÊéíÂàóËØ•ÂÖ¨Âºè‰ª•Ê±ÇËß£ <span
class="math inline">\(D\)</span>Ôºö <span class="math display">\[D =
\lim_{t\to\infty} \frac{\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle}{6t}\]</span></li>
<li><strong>The Diagram:</strong> The central diagram beautifully
illustrates the concept. It shows a particle in a simulation box (with
‚ÄúN=108‚Äù likely being the number of particles simulated) moving from an
initial position <span class="math inline">\(\vec{r}_i(0)\)</span> to a
final position <span class="math inline">\(\vec{r}_i(t_j)\)</span>. The
MSD is the average of the square of this displacement over all particles
and many time origins. The graph labeled ‚ÄúMSD‚Äù shows how you would plot
this data and find the slope (‚Äúfitting‚Äù) to calculate <span
class="math inline">\(D\)</span>.
‰∏≠Èó¥ÁöÑÂõæË°®ÂÆåÁæéÂú∞ÈòêÈáä‰∫ÜËøô‰∏™Ê¶ÇÂøµ„ÄÇÂÆÉÂ±ïÁ§∫‰∫Ü‰∏Ä‰∏™Á≤íÂ≠êÂú®Ê®°ÊãüÊ°Ü‰∏≠Ôºà‚ÄúN=108‚Äù
ÂèØËÉΩÊòØÊ®°ÊãüÁ≤íÂ≠êÁöÑÊï∞ÈáèÔºâ‰ªéÂàùÂßã‰ΩçÁΩÆ <span
class="math inline">\(\vec{r}_i(0)\)</span> ÁßªÂä®Âà∞ÊúÄÁªà‰ΩçÁΩÆ <span
class="math inline">\(\vec{r}_i(t_j)\)</span>„ÄÇMSD
ÊòØËØ•‰ΩçÁßªÂπ≥ÊñπÂú®ÊâÄÊúâÁ≤íÂ≠êÂíåÂ§ö‰∏™Êó∂Èó¥ÂéüÁÇπ‰∏äÁöÑÂπ≥ÂùáÂÄº„ÄÇÊ†áÊúâ‚ÄúMSD‚ÄùÁöÑÂõæË°®ÊòæÁ§∫‰∫ÜÂ¶Ç‰ΩïÁªòÂà∂Ëøô‰∫õÊï∞ÊçÆÂπ∂ÊâæÂà∞ÊñúÁéáÔºà‚ÄúÊãüÂêà‚ÄùÔºâÊù•ËÆ°ÁÆó
<span class="math inline">\(D\)</span>„ÄÇ</li>
<li><strong>Connection:</strong> This is a perfect summary of the
‚ÄúDisplacement Picture‚Äù we analyzed on the second whiteboard. It‚Äôs the
most intuitive way to think about diffusion: how far particles spread
out over
time.ËøôÂÆåÁæéÂú∞ÊÄªÁªì‰∫ÜÊàë‰ª¨Âú®Á¨¨‰∫å‰∏™ÁôΩÊùø‰∏äÂàÜÊûêÁöÑ‚Äú‰ΩçÁßªÂõæ‚Äù„ÄÇËøôÊòØÊÄùËÄÉÊâ©Êï£ÊúÄÁõ¥ËßÇÁöÑÊñπÂºèÔºöÁ≤íÂ≠êÈöèÊó∂Èó¥Êâ©Êï£ÁöÑË∑ùÁ¶ª„ÄÇ</li>
</ul>
<h3
id="the-green-kubo-relation-the-fluctuation-picture-Ê†ºÊûó-‰πÖ‰øùÂÖ≥Á≥ªÊ∂®ËêΩÂõæ">3.
The Green-Kubo Relation: The Fluctuation Picture
Ê†ºÊûó-‰πÖ‰øùÂÖ≥Á≥ªÔºöÊ∂®ËêΩÂõæ</h3>
<p>Finally, the board presents the more advanced but often more
practical method: the <strong>Green-Kubo relation</strong>.</p>
<ul>
<li><strong>The Equations:</strong> This section displays the two key
equations from our last discussion:
<ol type="1">
<li>The MSD as the double integral of the Velocity Autocorrelation
Function (VACF). ÈÄüÂ∫¶Ëá™Áõ∏ÂÖ≥ÂáΩÊï∞ (VACF) ÁöÑ‰∫åÈáçÁßØÂàÜÁöÑÂùáÊñπÂ∑Æ (MSD)„ÄÇ</li>
<li>The crucial derivative step: <span
class="math inline">\(\frac{d\langle x^2(t)\rangle}{dt} = 2 \int_0^t
dt&#39;&#39; \langle V_x(t) V_x(t&#39;&#39;) \rangle\)</span>.
ÂÖ≥ÈîÆÁöÑÂØºÊï∞Ê≠•È™§Ôºö<span class="math inline">\(\frac{d\langle
x^2(t)\rangle}{dt} = 2 \int_0^t dt&#39;&#39; \langle V_x(t)
V_x(t&#39;&#39;) \rangle\)</span>„ÄÇ</li>
</ol></li>
<li><strong>The Diagram:</strong> The small diagram of a square with
axes <span class="math inline">\(t&#39;\)</span> and <span
class="math inline">\(t&#39;&#39;\)</span> visually represents the
two-dimensional domain of integration for the double integral.
‰∏Ä‰∏™Â∏¶ÊúâËΩ¥ <span class="math inline">\(t&#39;\)</span> Âíå <span
class="math inline">\(t&#39;&#39;\)</span>
ÁöÑÂ∞èÊ≠£ÊñπÂΩ¢ÂõæÁõ¥ËßÇÂú∞Ë°®Á§∫‰∫Ü‰∫åÈáçÁßØÂàÜÁöÑ‰∫åÁª¥ÁßØÂàÜÂüü„ÄÇ</li>
<li><strong>Connection:</strong> This summarizes the ‚ÄúFluctuation
Picture.‚Äù It shows the mathematical heart of the derivation that proves
the Einstein and Green-Kubo methods are equivalent. As we concluded,
this method is often numerically superior because it involves
integrating a rapidly decaying function (the VACF) rather than finding
the slope of a noisy, unbounded function (the MSD).
ËøôÊ¶ÇÊã¨‰∫Ü‚ÄúÊ∂®ËêΩÂõæ‚Äù„ÄÇÂÆÉÂ±ïÁ§∫‰∫ÜËØÅÊòéÁà±Âõ†ÊñØÂù¶ÊñπÊ≥ïÂíåÊ†ºÊûó-‰πÖ‰øùÊñπÊ≥ïÁ≠â‰ª∑ÁöÑÊé®ÂØºËøáÁ®ãÁöÑÊï∞Â≠¶Ê†∏ÂøÉ„ÄÇÊ≠£Â¶ÇÊàë‰ª¨ÊÄªÁªìÁöÑÈÇ£Ê†∑ÔºåËøôÁßçÊñπÊ≥ïÈÄöÂ∏∏Âú®Êï∞ÂÄº‰∏äÊõ¥ËÉú‰∏ÄÁ≠πÔºåÂõ†‰∏∫ÂÆÉÊ∂âÂèäÂØπÂø´ÈÄüË°∞ÂáèÂáΩÊï∞ÔºàVACFÔºâËøõË°åÁßØÂàÜÔºåËÄå‰∏çÊòØÊ±ÇÂô™Â£∞Êó†ÁïåÂáΩÊï∞ÔºàMSDÔºâÁöÑÊñúÁéá„ÄÇ</li>
</ul>
<p>In essence, this single whiteboard is a complete roadmap for
analyzing diffusion in a molecular simulation. It shows how to first
characterize the material‚Äôs <strong>structure</strong> (<span
class="math inline">\(g(r)\)</span>) and then how to compute its key
dynamic property‚Äîthe <strong>diffusion constant
<code>D</code></strong>‚Äîusing two powerful, interconnected methods.
Êú¨Ë¥®‰∏äÔºåËøôÂùóÁôΩÊùøÂ∞±ÊòØÂàÜÂ≠êÊ®°Êãü‰∏≠ÂàÜÊûêÊâ©Êï£ÁöÑÂÆåÊï¥Ë∑ØÁ∫øÂõæ„ÄÇÂÆÉÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÈ¶ñÂÖàË°®ÂæÅÊùêÊñôÁöÑ<strong>ÁªìÊûÑ</strong>Ôºà<span
class="math inline">\(g(r)\)</span>ÔºâÔºåÁÑ∂ÂêéÂ¶Ç‰Ωï‰ΩøÁî®‰∏§ÁßçÂº∫Â§ß‰∏îÁõ∏‰∫íÂÖ≥ËÅîÁöÑÊñπÊ≥ïËÆ°ÁÆóÂÖ∂ÂÖ≥ÈîÆÁöÑÂä®ÊÄÅÁâπÊÄß‚Äî‚Äî<strong>Êâ©Êï£Â∏∏Êï∞
<code>D</code></strong>„ÄÇ</p>
<p>This whiteboard beautifully concludes the derivation of the
Green-Kubo relation, showing the final formulas and how they are used in
practice. It provides the punchline to the mathematical story we‚Äôve been
following.</p>
<p>Let‚Äôs break down the details.</p>
<h3 id="finalizing-the-derivation">4. Finalizing the Derivation</h3>
<p>The top lines of the board show the final step in connecting the Mean
Squared Displacement (MSD) to the Velocity Autocorrelation Function
(VACF).</p>
<p><span class="math display">\[\lim_{t\to\infty} \frac{d\langle x^2
\rangle}{dt} = 2 \int_0^\infty d\tau \langle V_x(0) V_x(\tau)
\rangle\]</span></p>
<ul>
<li><strong>The Left Side:</strong> As we know from the <strong>Einstein
relation</strong>, the long-time limit of the derivative of the 1D MSD,
<span class="math inline">\(\lim_{t\to\infty} \frac{d\langle x^2
\rangle}{dt}\)</span>, is simply equal to <strong><span
class="math inline">\(2D\)</span></strong>.</li>
<li><strong>The Right Side:</strong> This is the result of the
mathematical derivation from the previous slide. It shows that this same
quantity is also equal to twice the total integral of the VACF.</li>
</ul>
<p>By equating these two, we can solve for the diffusion coefficient,
<code>D</code>.</p>
<h3 id="the-velocity-autocorrelation-function-vacf">5. The Velocity
Autocorrelation Function (VACF)</h3>
<p>The board explicitly names the key quantity here:</p>
<p><span class="math display">\[\Phi(\tau) = \langle V_x(0) V_x(\tau)
\rangle\]</span></p>
<p>This is the <strong>‚ÄúVelocity autocorrelation function‚Äù</strong>
(abbreviated as VAF on the board), which we‚Äôve denoted as VACF. The
variable has been changed from <code>t</code> to <code>œÑ</code> (tau) to
represent a ‚Äútime lag‚Äù or interval, which is common notation.</p>
<ul>
<li><strong>The Plot:</strong> The graph on the board shows a typical
plot of the VACF, <span class="math inline">\(\Phi(\tau)\)</span>,
versus the time lag <span class="math inline">\(\tau\)</span>.
<ul>
<li>It starts at a maximum positive value at <span
class="math inline">\(\tau=0\)</span> (when the velocity is perfectly
correlated with itself).</li>
<li>It rapidly decays towards zero as the particle undergoes collisions
that randomize its velocity.</li>
</ul></li>
<li><strong>The Integral:</strong> The shaded area under this curve
represents the value of the integral <span
class="math inline">\(\int_0^\infty \Phi(\tau) d\tau\)</span>. The
Green-Kubo formula states that the diffusion coefficient is directly
proportional to this area.</li>
</ul>
<h3 id="the-green-kubo-formulas-for-the-diffusion-coefficient">6. The
Green-Kubo Formulas for the Diffusion Coefficient</h3>
<p>After canceling the factor of 2, the board presents the final,
practical formulas for <code>D</code>.</p>
<ul>
<li><strong>In 1 Dimension:</strong> <span class="math display">\[D =
\int_0^\infty d\tau \langle V_x(0) V_x(\tau) \rangle\]</span></li>
<li><strong>In 3 Dimensions:</strong> This is the more general and
useful formula. <span class="math display">\[D = \frac{1}{3}
\int_0^\infty d\tau \langle \vec{v}(0) \cdot \vec{v}(\tau)
\rangle\]</span> There are two important changes for 3D:
<ol type="1">
<li>We use the full <strong>velocity vectors</strong> and their dot
product, <span class="math inline">\(\vec{v}(0) \cdot
\vec{v}(\tau)\)</span>, to capture motion in all directions.</li>
<li>We divide by <strong>3</strong> to get the average contribution to
diffusion in any one direction (x, y, or z).</li>
</ol></li>
</ul>
<h3 id="practical-calculation-in-a-simulation">7. Practical Calculation
in a Simulation</h3>
<p>The last formula on the board shows how this is implemented in a
computer simulation with a finite number of atoms.</p>
<p><span class="math display">\[D = \frac{1}{3N} \int_0^\infty d\tau
\sum_{i=1}^{N} \langle \vec{v}_i(0) \cdot \vec{v}_i(\tau)
\rangle\]</span></p>
<ul>
<li><strong><span
class="math inline">\(\sum_{i=1}^{N}\)</span></strong>: This
<strong>summation</strong> symbol indicates that you must compute the
VACF for <em>each individual atom</em> (from atom <code>i=1</code> to
atom <code>N</code>).</li>
<li><strong><span class="math inline">\(\frac{1}{N}\)</span></strong>:
You then <strong>average</strong> the results over all <code>N</code>
atoms in your simulation box.</li>
<li><strong><span class="math inline">\(\langle \dots
\rangle\)</span></strong>: The angle brackets here still imply an
additional average over multiple different starting times
(<code>t=0</code>) to get good statistics.</li>
</ul>
<p>This formula is the practical recipe: to get the diffusion
coefficient, you track the velocity of every atom, calculate each one‚Äôs
VACF, average them together, and then integrate the result over
time.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/16/5054C3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/16/5054C3/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">ÂèëË°®‰∫é</span>

              <time title="ÂàõÂª∫Êó∂Èó¥Ôºö2025-09-16 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-16T21:00:00+08:00">2025-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Êõ¥Êñ∞‰∫é</span>
                <time title="‰øÆÊîπÊó∂Èó¥Ôºö2025-09-19 19:24:11" itemprop="dateModified" datetime="2025-09-19T19:24:11+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">ÂàÜÁ±ª‰∫é</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>ÁªüËÆ°Êú∫Âô®Â≠¶‰π†Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="general-linear-regression-model.">1. General linear regression
model.</h1>
<p><img src="/imgs/5054C3/General_linear_regression_model.png" alt="Diagram of a linear regression model">
## 1.1 general linear regression model - <strong>ÂÜÖÂÆπ</strong>:
<strong>general linear regression model</strong>.</p>
<p>the fundamental equation:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1x_{i1} + \dots +
\beta_px_{ip} + \epsilon_i\]</span></p>
<p>And it correctly identifies the main goal: to <strong>estimate the
parameters</strong> (the coefficients <span
class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span>) from
data so we can make predictions on new data.</p>
<p>Ê†∏ÂøÉÁõÆÊ†áÔºöÈÄöËøáÊï∞ÊçÆ<strong>‰º∞ËÆ°ÂèÇÊï∞</strong>ÔºàÂç≥Á≥ªÊï∞ <span
class="math inline">\(\beta_0, \beta_1, \dots,
\beta_p\)</span>ÔºâÔºå‰ªéËÄåÂØπÊñ∞Êï∞ÊçÆËøõË°åÈ¢ÑÊµã„ÄÇ</p>
<h2
id="how-we-actually-find-the-best-values-for-the-Œ≤-coefficients-parameter-estimation">1.2
How we actually find the best values for the <span
class="math inline">\(Œ≤\)</span> coefficients (parameter
estimation)?:</h2>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>: We find the best values for the <span
class="math inline">\(\beta\)</span> coefficients by finding the values
that <strong>minimize the overall error</strong> of the model. The most
common and fundamental method for this is called <strong>Ordinary Least
Squares (OLS)</strong>.</li>
</ul>
<h3
id="the-main-method-ordinary-least-squares-ols-ÊôÆÈÄöÊúÄÂ∞è‰∫å‰πòÊ≥ï-ols">##
The Main Method: Ordinary Least Squares (OLS) ÊôÆÈÄöÊúÄÂ∞è‰∫å‰πòÊ≥ï (OLS)</h3>
<p>The core idea of OLS is to find the line (or hyperplane in multiple
dimensions) that is as close as possible to all the data points
simultaneously. OLS
ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÊâæÂà∞‰∏ÄÊù°Â∞ΩÂèØËÉΩÂêåÊó∂Êé•ËøëÊâÄÊúâÊï∞ÊçÆÁÇπÁöÑÁõ¥Á∫øÔºàÊàñÂ§öÁª¥Ë∂ÖÂπ≥Èù¢Ôºâ„ÄÇ</p>
<h4 id="define-the-error-residuals-ËØØÂ∑Æ">1. Define the Error (Residuals)
ËØØÂ∑Æ</h4>
<p>First, we need to define what ‚Äúerror‚Äù means. For any single data
point, the error is the difference between the actual value (<span
class="math inline">\(y_i\)</span>) and the value predicted by our model
(<span class="math inline">\(\hat{y}_i\)</span>). This difference is
called the <strong>residual</strong>.
È¶ñÂÖàÔºåÈúÄË¶ÅÂÆö‰πâ‚ÄúËØØÂ∑Æ‚ÄùÁöÑÂê´‰πâ„ÄÇÂØπ‰∫é‰ªª‰ΩïÂçï‰∏™Êï∞ÊçÆÁÇπÔºåËØØÂ∑ÆÊòØÂÆûÈôÖÂÄº (<span
class="math inline">\(y_i\)</span>) ‰∏éÊ®°ÂûãÈ¢ÑÊµãÂÄº (<span
class="math inline">\(\hat{y}_i\)</span>)
‰πãÈó¥ÁöÑÂ∑ÆÂÄº„ÄÇËøô‰∏™Â∑ÆÂÄºÁß∞‰∏∫<strong>ÊÆãÂ∑Æ</strong>„ÄÇ</p>
<p><strong>Residual</strong> = Actual Value - Predicted Value
<strong>ÊÆãÂ∑Æ</strong> = ÂÆûÈôÖÂÄº - È¢ÑÊµãÂÄº <span class="math display">\[e_i
= y_i - \hat{y}_i\]</span></p>
<p>You can visualize residuals as the vertical distance from each data
point to the regression line.
ÂèØ‰ª•Â∞ÜÊÆãÂ∑ÆÂèØËßÜÂåñ‰∏∫ÊØè‰∏™Êï∞ÊçÆÁÇπÂà∞ÂõûÂΩíÁ∫øÁöÑÂûÇÁõ¥Ë∑ùÁ¶ª„ÄÇ</p>
<h4
id="the-cost-function-sum-of-squared-residuals-ÊàêÊú¨ÂáΩÊï∞ÊÆãÂ∑ÆÂπ≥ÊñπÂíå">2.
The Cost Function: Sum of Squared Residuals ÊàêÊú¨ÂáΩÊï∞ÔºöÊÆãÂ∑ÆÂπ≥ÊñπÂíå</h4>
<p>We want to make all these residuals as small as possible. We can‚Äôt
just add them up, because some are positive and some are negative, and
they would cancel each other out.
ÊâÄÊúâÊÆãÂ∑ÆÂ∞ΩÂèØËÉΩÂ∞è„ÄÇ‰∏çËÉΩÁÆÄÂçïÂú∞Â∞ÜÂÆÉ‰ª¨Áõ∏Âä†ÔºåÂõ†‰∏∫Êúâ‰∫õÊòØÊ≠£Êï∞ÔºåÊúâ‰∫õÊòØË¥üÊï∞ÔºåÂÆÉ‰ª¨‰ºöÁõ∏‰∫íÊäµÊ∂à„ÄÇ</p>
<p>So, we square each residual (which makes them all positive) and then
sum them up. This gives us the <strong>Sum of Squared Residuals
(SSR)</strong>, which is our ‚Äúcost function.‚Äù
Âõ†Ê≠§ÔºåÂ∞ÜÊØè‰∏™ÊÆãÂ∑ÆÊ±ÇÂπ≥ÊñπÔºà‰ΩøÂÆÉ‰ª¨ÈÉΩ‰∏∫Ê≠£Êï∞ÔºâÔºåÁÑ∂ÂêéÂ∞ÜÂÆÉ‰ª¨Áõ∏Âä†„ÄÇËøôÂ∞±ÂæóÂà∞‰∫Ü<strong>ÊÆãÂ∑ÆÂπ≥ÊñπÂíå
(SSR)</strong>Ôºå‰πüÂ∞±ÊòØ‚ÄúÊàêÊú¨ÂáΩÊï∞‚Äù„ÄÇ</p>
<p><span class="math display">\[SSR = \sum_{i=1}^{n} e_i^2 =
\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span></p>
<p>The goal of OLS is simple: <strong>find the values of <span
class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span> that
make this SSR value as small as possible.</strong></p>
<h4
id="solving-for-the-coefficients-the-normal-equation-Ê±ÇËß£Á≥ªÊï∞Ê≠£ÊÄÅÊñπÁ®ã">3.
Solving for the Coefficients: The Normal Equation
Ê±ÇËß£Á≥ªÊï∞ÔºöÊ≠£ÊÄÅÊñπÁ®ã</h4>
<p>For linear regression, calculus provides a direct, exact solution to
this minimization problem. By taking the derivative of the SSR function
with respect to each <span class="math inline">\(\beta\)</span>
coefficient and setting it to zero, we can solve for the optimal values.
ÂØπ‰∫éÁ∫øÊÄßÂõûÂΩíÔºåÂæÆÁßØÂàÜ‰∏∫Ëøô‰∏™ÊúÄÂ∞èÂåñÈóÆÈ¢òÊèê‰æõ‰∫ÜÁõ¥Êé•„ÄÅÁ≤æÁ°ÆÁöÑËß£„ÄÇÈÄöËøáÂØπ SSR
ÂáΩÊï∞ÁöÑÊØè‰∏™ <span class="math inline">\(\beta\)</span>
Á≥ªÊï∞Ê±ÇÂØºÂπ∂Â∞ÜÂÖ∂ËÆæ‰∏∫Èõ∂ÔºåÂ∞±ÂèØ‰ª•Ê±ÇËß£Âá∫ÊúÄ‰ºòÂÄº„ÄÇ</p>
<p>This process results in a formula known as the <strong>Normal
Equation</strong>, which can be expressed cleanly using matrix algebra:
Ëøô‰∏™ËøáÁ®ã‰ºöÂæóÂà∞‰∏Ä‰∏™Áß∞‰∏∫<strong>Ê≠£ÊÄÅÊñπÁ®ã</strong>ÁöÑÂÖ¨ÂºèÔºåÂÆÉÂèØ‰ª•Áî®Áü©Èòµ‰ª£Êï∞Ê∏ÖÊô∞Âú∞Ë°®Á§∫Âá∫Êù•Ôºö</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the
vector of our estimated coefficients.‰º∞ËÆ°Á≥ªÊï∞ÁöÑÂêëÈáè„ÄÇ</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is a matrix where
each row is an observation and each column is a feature (with an added
column of 1s for the intercept <span
class="math inline">\(\beta_0\)</span>).ÂÖ∂‰∏≠ÊØè‰∏ÄË°å‰ª£Ë°®‰∏Ä‰∏™ËßÇÊµãÂÄºÔºåÊØè‰∏ÄÂàó‰ª£Ë°®‰∏Ä‰∏™ÁâπÂæÅÔºàÊà™Ë∑ù
<span class="math inline">\(\beta_0\)</span> Â¢ûÂä†‰∫Ü‰∏ÄÂàóÂÖ®‰∏∫ 1
ÁöÑÂÄºÔºâ„ÄÇ</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of the
actual response values.ÂÆûÈôÖÂìçÂ∫îÂÄºÁöÑÂêëÈáè„ÄÇ</li>
</ul>
<p>Statistical software and programming libraries (like Scikit-learn in
Python) use this equation (or more computationally stable versions of
it) to find the best coefficients for you instantly.</p>
<h3 id="an-alternative-method-gradient-descent-Ê¢ØÂ∫¶‰∏ãÈôç">## An
Alternative Method: Gradient Descent Ê¢ØÂ∫¶‰∏ãÈôç</h3>
<p>While the Normal Equation gives a direct answer, it can be very slow
if you have a massive number of features (e.g., hundreds of thousands).
An alternative, iterative method used across machine learning is
<strong>Gradient Descent</strong>.</p>
<p><strong>The Intuition:</strong> Imagine the SSR cost function is a
big valley. Your initial (random) <span
class="math inline">\(\beta\)</span> coefficients place you somewhere on
the slope of this valley.</p>
<ol type="1">
<li><strong>Check the slope</strong> (the gradient) at your current
position. <strong>Ê£ÄÊü•ÊÇ®ÂΩìÂâç‰ΩçÁΩÆÁöÑÊñúÁéá</strong>ÔºàÊ¢ØÂ∫¶Ôºâ„ÄÇ</li>
<li><strong>Take a small step</strong> in the steepest <em>downhill</em>
direction. <strong>ÊúùÊúÄÈô°ÁöÑ<em>‰∏ãÂù°</em>ÊñπÂêë</strong>ËøàÂá∫‰∏ÄÂ∞èÊ≠•**„ÄÇ</li>
<li><strong>Repeat.</strong> You keep taking steps downhill until you
reach the bottom of the valley. The bottom of the valley represents the
minimum SSR, and your coordinates at that point are the optimal <span
class="math inline">\(\beta\)</span> coefficients.
<strong>ÈáçÂ§ç</strong>„ÄÇÊÇ®ÁªßÁª≠Âêë‰∏ãËµ∞ÔºåÁõ¥Âà∞Âà∞ËææÂ±±Ë∞∑Â∫ïÈÉ®„ÄÇË∞∑Â∫ï‰ª£Ë°®ÊúÄÂ∞èSSRÔºåËØ•ÁÇπÁöÑÂùêÊ†áÂç≥‰∏∫ÊúÄ‰ºò<span
class="math inline">\(\beta\)</span>Á≥ªÊï∞„ÄÇ</li>
</ol>
<p>The size of each ‚Äústep‚Äù you take is controlled by a parameter called
the <strong>learning rate</strong>. Gradient Descent is the foundational
optimization algorithm for many complex models, including neural
networks.
ÊØèÊ¨°‚ÄúÊ≠•Ëøõ‚ÄùÁöÑÂ§ßÂ∞èÁî±‰∏Ä‰∏™Áß∞‰∏∫<strong>Â≠¶‰π†Áéá</strong>ÁöÑÂèÇÊï∞ÊéßÂà∂„ÄÇÊ¢ØÂ∫¶‰∏ãÈôçÊòØËÆ∏Â§öÂ§çÊùÇÊ®°ÂûãÔºàÂåÖÊã¨Á•ûÁªèÁΩëÁªúÔºâÁöÑÂü∫Á°Ä‰ºòÂåñÁÆóÊ≥ï„ÄÇ</p>
<h3 id="summary-ols-vs.-gradient-descent">## Summary: OLS vs.¬†Gradient
Descent</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Ordinary Least Squares (OLS)</th>
<th style="text-align: left;">Gradient Descent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>How it works</strong></td>
<td style="text-align: left;">Direct calculation using the Normal
Equation.</td>
<td style="text-align: left;">Iterative; takes steps to minimize
error.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Pros</strong></td>
<td style="text-align: left;">Provides an exact, optimal solution. No
parameters to tune.</td>
<td style="text-align: left;">More efficient for very large datasets.
Very versatile.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Cons</strong></td>
<td style="text-align: left;">Can be computationally expensive with many
features.</td>
<td style="text-align: left;">Requires choosing a learning rate. May not
find the exact minimum.</td>
</tr>
</tbody>
</table>
<h1 id="simple-linear-regression">2. Simple Linear Regression</h1>
<p><img src="/imgs/5054C3/Simple_Linear_Regression.png" alt="Simple_Linear_Regression"></p>
<h2 id="simple-linear-regression-1">2.1 Simple Linear Regression</h2>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>: <strong>Simple Linear Regression:</strong> a
special case of the general model you showed earlier where you only have
<strong>one</strong> predictor variable (<span
class="math inline">\(p=1\)</span>).</li>
</ul>
<h3 id="the-model-and-the-goal-Ê®°ÂûãÂíåÁõÆÊ†á">## The Model and the Goal
Ê®°ÂûãÂíåÁõÆÊ†á</h3>
<p>Sets up the simplified equation for a line: <span
class="math display">\[y_i = \beta_0 + \beta_1x_i + \epsilon_i\]</span>
* <span class="math inline">\(y_i\)</span> is the outcome you want to
predict.Ë¶ÅÈ¢ÑÊµãÁöÑÁªìÊûú„ÄÇ * <span class="math inline">\(x_i\)</span> is
your single input feature or covariate.Âçï‰∏™ËæìÂÖ•ÁâπÂæÅÊàñÂçèÂèòÈáè„ÄÇ * <span
class="math inline">\(\beta_1\)</span> is the <strong>slope</strong> of
the line. It tells you how much <span class="math inline">\(y\)</span>
is expected to increase for a one-unit increase in <span
class="math inline">\(x\)</span>.Ë°®Á§∫ <span
class="math inline">\(x\)</span> ÊØèÂ¢ûÂä†‰∏Ä‰∏™Âçï‰ΩçÔºå<span
class="math inline">\(y\)</span> È¢ÑËÆ°‰ºöÂ¢ûÂä†Â§öÂ∞ë„ÄÇ * <span
class="math inline">\(\beta_0\)</span> is the
<strong>intercept</strong>. It‚Äôs the predicted value of <span
class="math inline">\(y\)</span> when <span
class="math inline">\(x\)</span> is zero.ÂΩì <span
class="math inline">\(x\)</span> ‰∏∫Èõ∂Êó∂ <span
class="math inline">\(y\)</span> ÁöÑÈ¢ÑÊµãÂÄº„ÄÇ * <span
class="math inline">\(\epsilon_i\)</span> is the random error
term.ÊòØÈöèÊú∫ËØØÂ∑ÆÈ°π„ÄÇ</p>
<p>The goal, stated as ‚ÄúMinimize the sum of squares of err,‚Äù is exactly
the <strong>Ordinary Least Squares (OLS)</strong> method we just
discussed. It‚Äôs written here as: <span class="math display">\[\min_{a,b}
\sum_{i=1}^{n} (y_i - a - bx_i)^2\]</span> This is just a different way
of writing the same thing, where they use <code>a</code> for the
intercept (<span class="math inline">\(\beta_0\)</span>) and
<code>b</code> for the slope (<span
class="math inline">\(\beta_1\)</span>). You‚Äôre trying to find the
specific values of the slope and intercept that make the sum of all the
squared errors as small as possible.
ÁõÆÊ†áÔºåÂç≥‚ÄúÊúÄÂ∞èÂåñËØØÂ∑ÆÂπ≥ÊñπÂíå‚ÄùÔºåÊ≠£ÊòØ<strong>ÊôÆÈÄöÊúÄÂ∞è‰∫å‰πòÊ≥ï
(OLS)</strong>„ÄÇÔºö <span class="math display">\[\min_{a,b}
\sum_{i=1}^{n} (y_i - a - bx_i)^2\]</span> ËøôÊòØÂè¶‰∏ÄÁßçÂÜôÊ≥ïÔºåÂÖ∂‰∏≠Áî®
<code>a</code> Ë°®Á§∫Êà™Ë∑ù (<span
class="math inline">\(\beta_0\)</span>)Ôºå<code>b</code> Ë°®Á§∫ÊñúÁéá (<span
class="math inline">\(\beta_1\)</span>)„ÄÇÂ∞ùËØïÊâæÂà∞ÊñúÁéáÂíåÊà™Ë∑ùÁöÑÂÖ∑‰ΩìÂÄºÔºå‰ΩøÂæóÊâÄÊúâÂπ≥ÊñπËØØÂ∑Æ‰πãÂíåÂ∞ΩÂèØËÉΩÂ∞è„ÄÇ</p>
<h3 id="the-solution-the-estimator-formulas-Ëß£ÂÜ≥ÊñπÊ°à‰º∞ËÆ°ÂÖ¨Âºè">## The
Solution: The Estimator Formulas Ëß£ÂÜ≥ÊñπÊ°àÔºö‰º∞ËÆ°ÂÖ¨Âºè</h3>
<p>The most important part of this slide is the
<strong>solution</strong>. For the simple case with only one variable,
you don‚Äôt need complex matrix algebra (the Normal Equation). Instead,
the minimization problem can be solved with these two straightforward
formulas:
ÂØπ‰∫éÂè™Êúâ‰∏Ä‰∏™ÂèòÈáèÁöÑÁÆÄÂçïÊÉÖÂÜµÔºå‰∏çÈúÄË¶ÅÂ§çÊùÇÁöÑÁü©Èòµ‰ª£Êï∞ÔºàÊ≠£ÊÄÅÊñπÁ®ãÔºâ„ÄÇÁõ∏ÂèçÔºåÊúÄÂ∞èÂåñÈóÆÈ¢òÂèØ‰ª•Áî®‰ª•‰∏ã‰∏§‰∏™ÁÆÄÂçïÁöÑÂÖ¨ÂºèÊù•Ëß£ÂÜ≥Ôºö</p>
<h4 id="the-slope-hatbeta_1">1. The Slope: <span
class="math inline">\(\hat{\beta}_1\)</span></h4>
<p><span class="math display">\[\hat{\beta}_1 = \frac{\sum_{i=1}^{n}
(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i -
\bar{x})^2}\]</span> * <strong>Intuition:</strong> This formula might
look complex, but it‚Äôs actually very intuitive. * The numerator, <span
class="math inline">\(\sum(x_i - \bar{x})(y_i - \bar{y})\)</span>, is
closely related to the <strong>covariance</strong> between X and Y. It
measures whether X and Y tend to move in the same direction (positive
slope) or in opposite directions (negative slope). ‰∏é X Âíå Y
‰πãÈó¥ÁöÑ<strong>ÂçèÊñπÂ∑Æ</strong>ÂØÜÂàáÁõ∏ÂÖ≥„ÄÇÂÆÉË°°Èáè X Âíå Y
ÊòØÂÄæÂêë‰∫éÊúùÁõ∏ÂêåÊñπÂêëÔºàÊ≠£ÊñúÁéáÔºâËøòÊòØÊúùÁõ∏ÂèçÊñπÂêëÔºàË¥üÊñúÁéáÔºâÁßªÂä®„ÄÇ * The
denominator, <span class="math inline">\(\sum(x_i - \bar{x})^2\)</span>,
is related to the <strong>variance</strong> of X. It measures how much X
varies on its own. ÂÆÉË°°Èáè X Ëá™Ë∫´ÁöÑÂèòÂåñÈáè„ÄÇ * <strong>In short, the slope
is a measure of how X and Y vary together, scaled by how much X varies
by itself.</strong> ÊñúÁéáË°°ÈáèÁöÑÊòØ X Âíå Y ÂÖ±ÂêåÂèòÂåñÁöÑÁ®ãÂ∫¶ÔºåÂπ∂‰ª• X
Ëá™Ë∫´ÁöÑÂèòÂåñÈáè‰∏∫Ê†áÂ∫¶„ÄÇ</p>
<h4 id="the-intercept-hatbeta_0-Êà™Ë∑ù">2. The Intercept: <span
class="math inline">\(\hat{\beta}_0\)</span> Êà™Ë∑ù</h4>
<p><span class="math display">\[\hat{\beta}_0 = \bar{y} -
\hat{\beta}_1\bar{x}\]</span> * <strong>Intuition:</strong> This formula
is even simpler and has a wonderful geometric meaning. It ensures that
the <strong>line of best fit always passes through the ‚Äúcenter of mass‚Äù
of the data</strong>, which is the point of averages <span
class="math inline">\((\bar{x}, \bar{y})\)</span>.
ÂÆÉÁ°Æ‰øù<strong>ÊúÄ‰Ω≥ÊãüÂêàÁ∫øÂßãÁªàÁ©øËøáÊï∞ÊçÆÁöÑ‚ÄúË¥®ÂøÉ‚Äù</strong>ÔºåÂç≥Âπ≥ÂùáÂÄº <span
class="math inline">\((\bar{x}, \bar{y})\)</span> ÁöÑÁÇπ„ÄÇËÆ°ÁÆóÂá∫ÊúÄ‰Ω≥ÊñúÁéá
(<span class="math inline">\(\hat{\beta}_1\)</span>)
ÂêéÔºåÂ∞±ÂèØ‰ª•Â∞ÜÂÖ∂‰ª£ÂÖ•Ê≠§ÂÖ¨Âºè„ÄÇÁÑ∂ÂêéÔºåÂèØ‰ª•Ë∞ÉÊï¥Êà™Ë∑ù (<span
class="math inline">\(\hat{\beta}_0\)</span>)Ôºå‰ΩøÁõ¥Á∫øÂÆåÁæéÂú∞Âõ¥ÁªïÊï∞ÊçÆ‰∫ëÁöÑ‰∏≠ÂøÉÁÇπÊóãËΩ¨„ÄÇ
* Once you‚Äôve calculated the best slope (<span
class="math inline">\(\hat{\beta}_1\)</span>), you can plug it into this
formula. You then adjust the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) so that the line pivots
perfectly around the central point of your data cloud.</p>
<p>In summary, this slide provides the precise, closed-form formulas to
calculate the slope and intercept for the line of best fit in a simple
linear regression model.</p>
<h1 id="statistical-inference">3. Statistical Inference</h1>
<p><img src="/imgs/5054C3/Statistical_Inference1.png" alt="Statistical_Inference1">
<img src="/imgs/5054C3/Statistical_Inference2.png" alt="Statistical_Inference2">
## 3.1 Statistical Inference - <strong>ÂÜÖÂÆπ</strong>:
<strong>Statistical Inference:</strong> These two slides are deeply
connected and explain how we go from just <em>calculating</em> the
coefficients to understanding how <em>accurate</em> and
<em>reliable</em> they are.
Ëß£Èáä‰∫ÜÊàë‰ª¨Â¶Ç‰Ωï‰ªé‰ªÖ‰ªÖ<em>ËÆ°ÁÆó</em>Á≥ªÊï∞Âà∞ÁêÜËß£ÂÆÉ‰ª¨ÁöÑ<em>ÂáÜÁ°ÆÊÄß</em>Âíå<em>ÂèØÈù†ÊÄß</em>„ÄÇ</p>
<h3 id="the-core-problem-quantifying-uncertainty-ÈáèÂåñ‰∏çÁ°ÆÂÆöÊÄß">## The
Core Problem: Quantifying Uncertainty ÈáèÂåñ‰∏çÁ°ÆÂÆöÊÄß</h3>
<p>The second slide poses the fundamental questions: * ‚ÄúHow accurate are
<span class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span>?‚ÄùÂáÜÁ°ÆÊÄßÂ¶Ç‰ΩïÔºü * ‚ÄúWhat are
the distributions of <span class="math inline">\(\hat{\beta}_0\)</span>
and <span class="math inline">\(\hat{\beta}_1\)</span>?‚ÄùÂàÜÂ∏ÉÊòØ‰ªÄ‰πàÔºü</p>
<p>The reason we ask this is that our estimated coefficients (<span
class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>) were
calculated from a <strong>specific sample of data</strong>. If we
collected a different random sample from the same population, we would
get slightly different estimates.‰º∞ËÆ°ÁöÑÁ≥ªÊï∞ (<span
class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>)
ÊòØÊ†πÊçÆ<strong>ÁâπÂÆöÁöÑÊï∞ÊçÆÊ†∑Êú¨</strong>ËÆ°ÁÆóÂá∫Êù•ÁöÑ„ÄÇÂ¶ÇÊûúÊàë‰ª¨‰ªéÂêå‰∏ÄÊÄª‰Ωì‰∏≠ÈöèÊú∫ÊäΩÂèñ‰∏çÂêåÁöÑÊ†∑Êú¨ÔºåÊàë‰ª¨ÂæóÂà∞ÁöÑ‰º∞ËÆ°ÂÄº‰ºöÁï•Êúâ‰∏çÂêå„ÄÇ</p>
<p>The goal of statistical inference is to use the estimates from our
single sample to make conclusions about the <strong>true, unknown
population parameters</strong> (<span class="math inline">\(\beta_0,
\beta_1\)</span>) and to quantify our uncertainty about
them.ÁªüËÆ°Êé®Êñ≠ÁöÑÁõÆÊ†áÊòØÂà©Áî®Âçï‰∏™Ê†∑Êú¨ÁöÑ‰º∞ËÆ°ÂÄºÂæóÂá∫ÂÖ≥‰∫é<strong>ÁúüÂÆû„ÄÅÊú™Áü•ÁöÑÊÄª‰ΩìÂèÇÊï∞</strong>Ôºà<span
class="math inline">\(\beta_0,
\beta_1\)</span>ÔºâÁöÑÁªìËÆ∫ÔºåÂπ∂ÈáèÂåñÂØπËøô‰∫õÂèÇÊï∞ÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇ</p>
<h3
id="the-key-assumption-that-makes-it-possible-ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†áÁöÑÂÖ≥ÈîÆÂÅáËÆæ">##
The Key Assumption That Makes It Possible ÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†áÁöÑÂÖ≥ÈîÆÂÅáËÆæ</h3>
<p>To figure out the distribution of our estimates, we must make an
assumption about the distribution of the errors. This is the most
important assumption in linear regression for inference:
‰∏∫‰∫ÜÁ°ÆÂÆö‰º∞ËÆ°ÂÄºÁöÑÂàÜÂ∏ÉÔºåÂøÖÈ°ªÂØπËØØÂ∑ÆÁöÑÂàÜÂ∏ÉÂÅöÂá∫ÂÅáËÆæ„ÄÇËøôÊòØÁ∫øÊÄßÂõûÂΩíÊé®Êñ≠‰∏≠ÊúÄÈáçË¶ÅÁöÑÂÅáËÆæÔºö
<strong>Assumption:</strong> <span class="math inline">\(\epsilon_i
\stackrel{\text{i.i.d.}}{\sim} N(0, \sigma^2)\)</span></p>
<p>This means we assume the random error terms are: * <strong>Normally
distributed</strong> (<span class="math inline">\(N\)</span>).*
<strong>Ê≠£ÊÄÅÂàÜÂ∏É</strong>Ôºà<span class="math inline">\(N\)</span>Ôºâ„ÄÇ *
Have a mean of <strong>zero</strong> (our model is correct on average).*
ÂùáÂÄº‰∏∫<strong>Èõ∂</strong>ÔºàÊ®°ÂûãÂπ≥ÂùáËÄåË®ÄÊòØÊ≠£Á°ÆÁöÑÔºâ„ÄÇ * Have a constant
variance <strong><span class="math inline">\(\sigma^2\)</span></strong>
(homoscedasticity).* ÊñπÂ∑Æ‰∏∫Â∏∏Êï∞<strong><span
class="math inline">\(\sigma^2\)</span></strong>ÔºàÊñπÂ∑ÆÈΩêÊÄßÔºâ„ÄÇ * Are
<strong>independent and identically distributed</strong> (i.i.d.),
meaning each error is independent of the others.*
ÊòØ<strong>Áã¨Á´ãÂêåÂàÜÂ∏É</strong>Ôºài.i.d.ÔºâÁöÑÔºåËøôÊÑèÂë≥ÁùÄÊØè‰∏™ËØØÂ∑ÆÈÉΩÁã¨Á´ã‰∫éÂÖ∂‰ªñËØØÂ∑Æ„ÄÇ</p>
<p><strong>Why is this important?</strong> Because our coefficients
<span class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span> are calculated as weighted
sums of the <span class="math inline">\(y_i\)</span> values, and the
<span class="math inline">\(y_i\)</span> values depend on the errors
<span class="math inline">\(\epsilon_i\)</span>. This assumption about
the errors allows us to prove that our estimated coefficients themselves
are also normally distributed. Á≥ªÊï∞ <span
class="math inline">\(\hat{\beta}_0\)</span> Âíå <span
class="math inline">\(\hat{\beta}_1\)</span> ÊòØÈÄöËøá <span
class="math inline">\(y_i\)</span> ÂÄºÁöÑÂä†ÊùÉÂíåËÆ°ÁÆóÁöÑÔºåËÄå <span
class="math inline">\(y_i\)</span> ÂÄºÂèñÂÜ≥‰∫éËØØÂ∑Æ <span
class="math inline">\(\epsilon_i\)</span>„ÄÇËøô‰∏™ÂÖ≥‰∫éËØØÂ∑ÆÁöÑÂÅáËÆæ‰ΩøËÉΩÂ§üËØÅÊòé‰º∞ËÆ°ÁöÑÁ≥ªÊï∞Êú¨Ë∫´‰πüÊúç‰ªéÊ≠£ÊÄÅÂàÜÂ∏É„ÄÇ</p>
<h3
id="the-solution-the-theorem-and-the-t-distribution-ÂÆöÁêÜÂíå-t-ÂàÜÂ∏É">##
The Solution: The Theorem and the t-distribution ÂÆöÁêÜÂíå t ÂàÜÂ∏É</h3>
<p>The first slide provides the central theorem that allows us to
perform inference. It tells us exactly how to standardize our estimated
coefficients so they follow a known distribution.
Á¨¨‰∏ÄÂº†ÂπªÁÅØÁâáÊèê‰æõ‰∫ÜËøõË°åÊé®Êñ≠ÁöÑÊ†∏ÂøÉÂÆöÁêÜ„ÄÇÂÆÉÂáÜÁ°ÆÂú∞ÂëäËØâÊàë‰ª¨Â¶Ç‰ΩïÂØπ‰º∞ËÆ°ÁöÑÁ≥ªÊï∞ËøõË°åÊ†áÂáÜÂåñÔºå‰ΩøÂÖ∂Êúç‰ªéÂ∑≤Áü•ÁöÑÂàÜÂ∏É„ÄÇ</p>
<h4 id="the-standard-error-s.e.-Ê†áÂáÜËØØÂ∑Æ-s.e.">1. The Standard Error
(s.e.) Ê†áÂáÜËØØÂ∑Æ (s.e.)</h4>
<p>First, look at the denominators in the red dotted boxes. These are
the <strong>standard errors</strong> of the coefficients,
<code>s.e.($\hat&#123;\beta&#125;_1$)</code> and
<code>s.e.($\hat&#123;\beta&#125;_0$)</code>.
Á¨¨‰∏ÄÂº†ÂπªÁÅØÁâáÊèê‰æõ‰∫ÜËøõË°åÊé®Êñ≠ÁöÑÊ†∏ÂøÉÂÆöÁêÜ„ÄÇÂÆÉÂáÜÁ°ÆÂú∞ÂëäËØâÊàë‰ª¨Â¶Ç‰ΩïÂØπ‰º∞ËÆ°ÁöÑÁ≥ªÊï∞ËøõË°åÊ†áÂáÜÂåñÔºå‰ΩøÂÖ∂Êúç‰ªéÂ∑≤Áü•ÁöÑÂàÜÂ∏É„ÄÇ</p>
<ul>
<li><strong>What it is:</strong> The standard error is the estimated
<strong>standard deviation of the coefficient‚Äôs sampling
distribution</strong>. In simpler terms, it‚Äôs a measure of the average
amount by which our estimate <span
class="math inline">\(\hat{\beta}_1\)</span> would differ from the true
<span class="math inline">\(\beta_1\)</span> if we were to repeat the
experiment many times.
Ê†áÂáÜËØØÂ∑ÆÊòØÁ≥ªÊï∞ÊäΩÊ†∑ÂàÜÂ∏ÉÁöÑ<strong>Ê†áÂáÜÂ∑Æ</strong>‰º∞ËÆ°ÂÄº„ÄÇÁÆÄÂçïÊù•ËØ¥ÔºåÂÆÉË°°ÈáèÁöÑÊòØÂ¶ÇÊûúÊàë‰ª¨ÈáçÂ§çÂÆûÈ™åÂ§öÊ¨°ÔºåÊàë‰ª¨‰º∞ËÆ°ÁöÑ
<span class="math inline">\(\hat{\beta}_1\)</span> ‰∏éÁúüÂÆûÁöÑ <span
class="math inline">\(\beta_1\)</span> ‰πãÈó¥ÁöÑÂπ≥ÂùáÂ∑ÆÂºÇ„ÄÇ</li>
<li><strong>A smaller standard error means a more precise and reliable
estimate.</strong>
<strong>Ê†áÂáÜËØØÂ∑ÆË∂äÂ∞èÔºå‰º∞ËÆ°ÂÄºË∂äÁ≤æÁ°ÆÂèØÈù†„ÄÇ</strong></li>
</ul>
<h4 id="the-t-statistic-t-ÁªüËÆ°Èáè">2. The t-statistic t ÁªüËÆ°Èáè</h4>
<p>The theorem shows two fractions that form a
<strong>t-statistic</strong>. The general structure for this is:
ËØ•ÂÆöÁêÜÂ±ïÁ§∫‰∫Ü‰∏§‰∏™ÊûÑÊàê<strong>t ÁªüËÆ°Èáè</strong>ÁöÑÂàÜÊï∞„ÄÇÂÖ∂‰∏ÄËà¨ÁªìÊûÑÂ¶Ç‰∏ãÔºö
<span class="math display">\[t = \frac{\text{ (Sample Estimate - True
Value) }}{\text{ Standard Error of the Estimate }}\]</span></p>
<p>For <span class="math inline">\(\beta_1\)</span>, this is: <span
class="math inline">\(\frac{\hat{\beta}_1 -
\beta_1}{\text{s.e.}(\hat{\beta}_1)}\)</span>.</p>
<p>The key insight is that this specific quantity follows a
<strong>Student‚Äôs t-distribution</strong> with <strong><span
class="math inline">\(n-2\)</span> degrees of freedom</strong>.
ÂÖ≥ÈîÆÂú®‰∫éÔºåËøô‰∏™ÁâπÂÆöÈáèÊúç‰ªé<strong>Â≠¶Áîü t
ÂàÜÂ∏É</strong>ÔºåÂÖ∂Ëá™Áî±Â∫¶‰∏∫<strong><span
class="math inline">\(n-2\)</span>„ÄÇ * </strong>Student‚Äôs
t-distribution:** This is a probability distribution that looks very
similar to the normal distribution but has slightly ‚Äúheavier‚Äù tails. We
use it instead of the normal distribution because we had to
<em>estimate</em> the standard deviation of the errors (<code>s</code>
in the formula), which adds extra uncertainty.
ËøôÊòØ‰∏ÄÁßçÊ¶ÇÁéáÂàÜÂ∏ÉÔºå‰∏éÊ≠£ÊÄÅÂàÜÂ∏ÉÈùûÂ∏∏Áõ∏‰ººÔºå‰ΩÜÂ∞æÈÉ®Áï•Èáç„ÄÇ‰ΩøÁî®ÂÆÉÊù•‰ª£ÊõøÊ≠£ÊÄÅÂàÜÂ∏ÉÔºåÊòØÂõ†‰∏∫ÂøÖÈ°ª<em>‰º∞ËÆ°</em>ËØØÂ∑ÆÁöÑÊ†áÂáÜÂ∑ÆÔºàÂÖ¨Âºè‰∏≠ÁöÑ
<code>s</code>ÔºâÔºåËøô‰ºöÂ¢ûÂä†È¢ùÂ§ñÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇ * <strong>Degrees of Freedom
(n-2):</strong> We start with <code>n</code> data points, but we lose
two degrees of freedom because we used the data to estimate two
parameters: <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>. ‰ªé <code>n</code>
‰∏™Êï∞ÊçÆÁÇπÂºÄÂßãÔºå‰ΩÜÁî±‰∫éÁî®Ëøô‰∫õÊï∞ÊçÆ‰º∞ËÆ°‰∫Ü‰∏§‰∏™ÂèÇÊï∞Ôºö<span
class="math inline">\(\beta_0\)</span> Âíå <span
class="math inline">\(\beta_1\)</span>ÔºåÂõ†Ê≠§ÊçüÂ§±‰∫Ü‰∏§‰∏™Ëá™Áî±Â∫¶„ÄÇ #### 3.
Estimating the Error Variance (<span
class="math inline">\(s^2\)</span>)‰º∞ËÆ°ËØØÂ∑ÆÊñπÂ∑Æ (<span
class="math inline">\(s^2\)</span>) To calculate the standard errors, we
need a value for <code>s</code>, which is our estimate of the true error
standard deviation <span class="math inline">\(\sigma\)</span>. This is
calculated from the <strong>Residual Sum of Squares (RSS)</strong>.
‰∏∫‰∫ÜËÆ°ÁÆóÊ†áÂáÜËØØÂ∑ÆÔºåÊàë‰ª¨ÈúÄË¶Å‰∏Ä‰∏™ <code>s</code> ÁöÑÂÄºÔºåÂÆÉÊòØÂØπÁúüÂÆûËØØÂ∑ÆÊ†áÂáÜÂ∑Æ
<span class="math inline">\(\sigma\)</span>
ÁöÑ‰º∞ËÆ°ÂÄº„ÄÇËØ•ÂÄºÁî±<strong>ÊÆãÂ∑ÆÂπ≥ÊñπÂíå (RSS)</strong> ËÆ°ÁÆóÂæóÂá∫„ÄÇ *
<strong>RSS:</strong> First, we calculate the RSS = <span
class="math inline">\(\sum(y_i - \hat{y}_i)^2\)</span>, which is the sum
of all the squared errors.* <strong>RSS</strong>ÔºöÈ¶ñÂÖàÔºåËÆ°ÁÆó RSS = <span
class="math inline">\(\sum(y_i -
\hat{y}_i)^2\)</span>ÔºåÂç≥ÊâÄÊúâÂπ≥ÊñπËØØÂ∑Æ‰πãÂíå„ÄÇ * <strong><span
class="math inline">\(s^2\)</span>:</strong> Then, we find the estimate
of the error variance: <span class="math inline">\(s^2 = \text{RSS} /
(n-2)\)</span>. We divide by <span class="math inline">\(n-2\)</span> to
get an unbiased estimate. * <strong><span
class="math inline">\(s^2\)</span></strong>ÔºöÁÑ∂ÂêéÔºåËÆ°ÁÆóËØØÂ∑ÆÊñπÂ∑ÆÁöÑ‰º∞ËÆ°ÂÄºÔºö<span
class="math inline">\(s^2 = \text{RSS} / (n-2)\)</span>„ÄÇÊàë‰ª¨Â∞ÜÂÖ∂Èô§‰ª•
<span class="math inline">\(n-2\)</span> Âç≥ÂèØÂæóÂà∞Êó†ÂÅè‰º∞ËÆ°ÂÄº„ÄÇ *
<code>s</code> is simply the square root of <span
class="math inline">\(s^2\)</span>. This <code>s</code> is the value
used in the standard error formulas.* <code>s</code> Â∞±ÊòØ <span
class="math inline">\(s^2\)</span> ÁöÑÂπ≥ÊñπÊ†π„ÄÇËøô‰∏™ <code>s</code>
ÊòØÊ†áÂáÜËØØÂ∑ÆÂÖ¨Âºè‰∏≠‰ΩøÁî®ÁöÑÂÄº„ÄÇ</p>
<h3 id="what-this-allows-us-to-do-the-practical-use">## What This Allows
Us To Do (The Practical Use)</h3>
<p>Because we know the exact distribution of our t-statistic, we can now
achieve our goal of quantifying uncertainty: Âõ†‰∏∫Áü•ÈÅì t
ÁªüËÆ°ÈáèÁöÑÁ≤æÁ°ÆÂàÜÂ∏ÉÔºåÊâÄ‰ª•Áé∞Âú®ÂèØ‰ª•ÂÆûÁé∞ÈáèÂåñ‰∏çÁ°ÆÂÆöÊÄßÁöÑÁõÆÊ†áÔºö</p>
<ol type="1">
<li><strong>Hypothesis Testing:</strong> We can test if a predictor is
actually useful. The most common test is for the null hypothesis <span
class="math inline">\(H_0: \beta_1 = 0\)</span>. If we can prove the
observed <span class="math inline">\(\hat{\beta}_1\)</span> is very
unlikely to occur if the true <span
class="math inline">\(\beta_1\)</span> were zero, we can conclude there
is a statistically significant relationship between <span
class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span>.
ÂèØ‰ª•Ê£ÄÈ™å‰∏Ä‰∏™È¢ÑÊµãÂèòÈáèÊòØÂê¶ÁúüÁöÑÊúâÁî®„ÄÇÊúÄÂ∏∏ËßÅÁöÑÊ£ÄÈ™åÊòØÈõ∂ÂÅáËÆæ <span
class="math inline">\(H_0: \beta_1 = 0\)</span>„ÄÇÂ¶ÇÊûúËÉΩËØÅÊòéÔºåÂΩìÁúüÂÆûÁöÑ
<span class="math inline">\(\beta_1\)</span> ‰∏∫Èõ∂Êó∂ÔºåËßÇÊµãÂà∞ÁöÑ <span
class="math inline">\(\hat{\beta}_1\)</span>
‰∏çÂ§™ÂèØËÉΩÂèëÁîüÔºåÈÇ£‰πàÂ∞±ÂèØ‰ª•ÂæóÂá∫ÁªìËÆ∫Ôºå<span class="math inline">\(x\)</span>
Âíå <span class="math inline">\(y\)</span>
‰πãÈó¥Â≠òÂú®ÁªüËÆ°Â≠¶‰∏äÁöÑÊòæËëóÂÖ≥Á≥ª„ÄÇ</li>
<li><strong>Confidence Intervals:</strong> We can construct a range of
plausible values for the true coefficient. For example, we can calculate
a 95% confidence interval for <span
class="math inline">\(\beta_1\)</span>. This gives us a range where we
are 95% confident the true value of <span
class="math inline">\(\beta_1\)</span> lies.
ÂèØ‰ª•‰∏∫ÁúüÂÆûÁ≥ªÊï∞ÊûÑÂª∫‰∏ÄÁ≥ªÂàóÂêàÁêÜÁöÑÂÄº„ÄÇ</li>
</ol>
<h1 id="multiple-linear-regression">4. Multiple Linear Regression</h1>
<p><img src="/imgs/5054C3/Multiple_Linear Regression1.png" alt="Multiple_Linear Regression1">
<img src="/imgs/5054C3/Multiple_Linear Regression2.png" alt="Multiple_Linear Regression2">
## 4.1 Multiple Linear Regression - <strong>ÂÜÖÂÆπ</strong>:
<strong>Multiple Linear Regression:</strong></p>
<p>Here‚Äôs a detailed breakdown that connects both slides.</p>
<h3
id="the-model-from-one-to-many-predictors-‰ªéÂçïÈ¢ÑÊµãÂèòÈáèÂà∞Â§öÈ¢ÑÊµãÂèòÈáè">##
The Model: From One to Many Predictors ‰ªéÂçïÈ¢ÑÊµãÂèòÈáèÂà∞Â§öÈ¢ÑÊµãÂèòÈáè</h3>
<p>The first slide introduces the <strong>Multiple Linear Regression
model</strong>. This is a direct extension of the simple model, but
instead of using just one predictor variable, we use multiple (<span
class="math inline">\(p\)</span>) predictors to explain our response
variable.
Â§öÂÖÉÁ∫øÊÄßÂõûÂΩíÊ®°ÂûãÊòØÁÆÄÂçïÊ®°ÂûãÁöÑÁõ¥Êé•Êâ©Â±ïÔºå‰ΩÜ‰∏çÊòØÂè™‰ΩøÁî®‰∏Ä‰∏™È¢ÑÊµãÂèòÈáèÔºåËÄåÊòØ‰ΩøÁî®Â§ö‰∏™Ôºà<span
class="math inline">\(p\)</span>ÔºâÈ¢ÑÊµãÂèòÈáèÊù•Ëß£ÈáäÂìçÂ∫îÂèòÈáè„ÄÇ</p>
<p>The general formula is: <span class="math display">\[y_i = \beta_0 +
\beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip} +
\epsilon_i\]</span></p>
<h4 id="key-change-in-interpretation">Key Change in Interpretation</h4>
<p>This is the most important new concept. In simple regression, <span
class="math inline">\(\beta_1\)</span> was just the slope. In multiple
regression, each coefficient has a more nuanced meaning:
Âú®ÁÆÄÂçïÂõûÂΩí‰∏≠Ôºå<span class="math inline">\(\beta_1\)</span>
Âè™ÊòØÊñúÁéá„ÄÇÂú®Â§öÂÖÉÂõûÂΩí‰∏≠ÔºåÊØè‰∏™Á≥ªÊï∞ÈÉΩÊúâÊõ¥ÂæÆÂ¶ôÁöÑÂê´‰πâÔºö</p>
<p><strong><span class="math inline">\(\beta_j\)</span> is the average
change in <span class="math inline">\(y\)</span> for a one-unit increase
in <span class="math inline">\(x_j\)</span>, while holding all other
predictors constant.</strong></p>
<p>This is incredibly powerful. Using the advertising example from your
slide: * <span class="math inline">\(y_i = \beta_0 +
\beta_1(\text{TV}_i) + \beta_2(\text{Radio}_i) +
\beta_3(\text{Newspaper}_i) + \epsilon_i\)</span> * <span
class="math inline">\(\beta_1\)</span> represents the effect of TV
advertising on sales, <strong>after controlling for</strong> the amount
spent on Radio and Newspaper ads. This allows you to isolate the unique
contribution of each advertising
channel.Ë°®Á§∫Âú®<strong>ÊéßÂà∂</strong>ÂπøÊí≠ÂíåÊä•Á∫∏ÂπøÂëäÊîØÂá∫ÂêéÔºåÁîµËßÜÂπøÂëäÂØπÈîÄÂîÆÈ¢ùÁöÑÂΩ±Âìç„ÄÇËøôÂèØ‰ª•ËÆ©ÊÇ®Âå∫ÂàÜÊØè‰∏™ÂπøÂëäÊ∏†ÈÅìÁöÑÁã¨ÁâπË¥°ÁåÆ„ÄÇ</p>
<h3 id="the-solution-deriving-the-normal-equation-Êé®ÂØºÊ≠£ÊÄÅÊñπÁ®ã">## The
Solution: Deriving the Normal Equation Êé®ÂØºÊ≠£ÊÄÅÊñπÁ®ã</h3>
<p>The second slide shows the mathematical process for finding the best
coefficients (<span class="math inline">\(\beta_0, \beta_1, \dots,
\beta_p\)</span>) using the <strong>Ordinary Least Squares
(OLS)</strong> method. It‚Äôs essentially a condensed derivation of the
<strong>Normal Equation</strong>. ‰ΩøÁî®<strong>ÊôÆÈÄöÊúÄÂ∞è‰∫å‰πòÊ≥ï
(OLS)</strong> ÂØªÊâæÊúÄ‰Ω≥Á≥ªÊï∞ (<span class="math inline">\(\beta_0,
\beta_1, \dots, \beta_p\)</span>)
ÁöÑÊï∞Â≠¶ËøáÁ®ã„ÄÇÂÆÉÊú¨Ë¥®‰∏äÊòØ<strong>Ê≠£ÊÄÅÊñπÁ®ã</strong>ÁöÑÁÆÄÂåñÊé®ÂØº„ÄÇ</p>
<h4 id="the-goal-minimizing-the-sum-of-squares-ÊúÄÂ∞èÂåñÂπ≥ÊñπÂíå">1. The
Goal: Minimizing the Sum of Squares ÊúÄÂ∞èÂåñÂπ≥ÊñπÂíå</h4>
<p>Just like before, our goal is to minimize the sum of the squared
errors (or residuals): ÁõÆÊ†áÊòØÊúÄÂ∞èÂåñÂπ≥ÊñπËØØÂ∑ÆÔºàÊàñÊÆãÂ∑ÆÔºâ‰πãÂíå„ÄÇ</p>
<ul>
<li><strong>Scalar Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_{i1} -
\beta_2x_{i2} - \beta_3x_{i3})^2\)</span>
<ul>
<li>This is easy to read but gets very long with more variables.
‰ª£Á†ÅÊòì‰∫éÈòÖËØªÔºå‰ΩÜÂèòÈáèË∂äÂ§öÔºå‰ª£Á†ÅË∂äÈïø„ÄÇ</li>
</ul></li>
<li><strong>Vector Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \boldsymbol{\beta}^T
\mathbf{x}_i)^2\)</span>
<ul>
<li>This is a more compact and powerful way to write the same thing
using linear algebra, where <span
class="math inline">\(\boldsymbol{\beta}^T \mathbf{x}_i\)</span> is the
dot product that calculates the entire predicted value <span
class="math inline">\(\hat{y}_i\)</span>.
ËøôÊòØ‰∏ÄÁßçÊõ¥ÁÆÄÊ¥Å„ÄÅÊõ¥Âº∫Â§ßÁöÑÁ∫øÊÄß‰ª£Êï∞Ë°®Á§∫ÊñπÊ≥ïÔºåÂÖ∂‰∏≠ <span
class="math inline">\(\boldsymbol{\beta}^T \mathbf{x}_i\)</span>
ÊòØËÆ°ÁÆóÊï¥‰∏™È¢ÑÊµãÂÄº <span class="math inline">\(\hat{y}_i\)</span>
ÁöÑÁÇπÁßØ„ÄÇ</li>
</ul></li>
</ul>
<h4
id="the-method-using-calculus-to-find-the-minimum-‰ΩøÁî®ÂæÆÁßØÂàÜÊ±ÇÊúÄÂ∞èÂÄº">2.
The Method: Using Calculus to Find the Minimum ‰ΩøÁî®ÂæÆÁßØÂàÜÊ±ÇÊúÄÂ∞èÂÄº</h4>
<p>To find the set of <span class="math inline">\(\beta\)</span> values
that results in the lowest possible error, we use calculus.</p>
<ul>
<li><p><strong>The Derivative (Gradient):</strong> Since our error
function depends on multiple <span class="math inline">\(\beta\)</span>
coefficients, we can‚Äôt take a simple derivative. Instead, we take the
<strong>gradient</strong>, which is a vector of partial derivatives (one
for each coefficient). This tells us the ‚Äúslope‚Äù of the error function
in every direction. ÂØºÊï∞ÔºàÊ¢ØÂ∫¶Ôºâ ËØØÂ∑ÆÂáΩÊï∞‰æùËµñ‰∫éÂ§ö‰∏™ <span
class="math inline">\(\beta\)</span>
Á≥ªÊï∞ÔºåÂõ†Ê≠§Êàë‰ª¨‰∏çËÉΩÁÆÄÂçïÂú∞Ê±ÇÂØºÊï∞„ÄÇÁõ∏ÂèçÔºåÈááÁî®<strong>Ê¢ØÂ∫¶</strong>ÔºåÂÆÉÊòØ‰∏Ä‰∏™Áî±ÂÅèÂØºÊï∞ÁªÑÊàêÁöÑÂêëÈáèÔºàÊØè‰∏™Á≥ªÊï∞ÂØπÂ∫î‰∏Ä‰∏™ÂÅèÂØºÊï∞Ôºâ„ÄÇËøôÂëäËØâËØØÂ∑ÆÂáΩÊï∞Âú®ÂêÑ‰∏™ÊñπÂêë‰∏äÁöÑ‚ÄúÊñúÁéá‚Äù„ÄÇ</p></li>
<li><p><strong>Setting the Gradient to Zero:</strong> The minimum of a
function occurs where its slope is zero (the very bottom of the error
‚Äúvalley‚Äù). The slide shows the result of taking this gradient and
setting it to
zero.ÂáΩÊï∞ÁöÑÊúÄÂ∞èÂÄºÂá∫Áé∞Âú®ÂÖ∂ÊñúÁéá‰∏∫Èõ∂ÁöÑÂú∞ÊñπÔºàÂç≥ËØØÂ∑Æ‚ÄúË∞∑Â∫ï‚ÄùÁöÑÊúÄ‰ΩéÁÇπÔºâ„ÄÇÂπªÁÅØÁâáÂ±ïÁ§∫‰∫ÜÂèñÊ≠§Ê¢ØÂ∫¶Âπ∂Â∞ÜÂÖ∂ËÆæ‰∏∫Èõ∂ÁöÑÁªìÊûú„ÄÇ</p></li>
</ul>
<p>The equation shown on the slide: <span class="math display">\[2
\sum_{i=1}^{n} (\boldsymbol{\beta}^T \mathbf{x}_i - y_i)\mathbf{x}_i^T =
0\]</span> ‚Ä¶is the result of this calculus step. The goal is now to
algebraically rearrange this equation to solve for <span
class="math inline">\(\boldsymbol{\beta}\)</span>.
ÊòØËøô‰∏ÄÂæÆÁßØÂàÜÊ≠•È™§ÁöÑÁªìÊûú„ÄÇÁé∞Âú®ÁöÑÁõÆÊ†áÊòØÁî®‰ª£Êï∞ÊñπÊ≥ïÈáçÊñ∞ÊéíÂàóËøô‰∏™ÊñπÁ®ãÔºå‰ª•Ê±ÇËß£
<span class="math inline">\(\boldsymbol{\beta}\)</span>„ÄÇ</p>
<h4 id="the-result-the-normal-equation-Ê≠£ÂàôÊñπÁ®ã">3. The Result: The
Normal Equation Ê≠£ÂàôÊñπÁ®ã</h4>
<p>After rearranging the equation from the previous step and expressing
the sums in their full matrix form, we arrive at a clean and beautiful
solution. While the slide doesn‚Äôt show the final step, the result of
‚ÄúSetting the gradient zero and solve <span
class="math inline">\(\beta\)</span>‚Äù is the <strong>Normal
Equation</strong>:
ÈáçÊñ∞ÊéíÂàó‰∏ä‰∏ÄÊ≠•‰∏≠ÁöÑÊñπÁ®ãÔºåÂπ∂Â∞ÜÂíåË°®Á§∫‰∏∫ÂÆåÊï¥ÁöÑÁü©ÈòµÂΩ¢ÂºèÂêéÔºåÂæóÂà∞‰∫Ü‰∏Ä‰∏™ÁÆÄÊ¥ÅÁæéËßÇÁöÑËß£„ÄÇËôΩÁÑ∂ÂπªÁÅØÁâáÊ≤°ÊúâÂ±ïÁ§∫ÊúÄÂêé‰∏ÄÊ≠•Ôºå‚ÄúËÆæÁΩÆÊ¢ØÂ∫¶Èõ∂ÁÇπÂπ∂Ê±ÇËß£
<span class="math inline">\(\beta\)</span>‚Äù
ÁöÑÁªìÊûúÂ∞±ÊòØ<strong>Ê≠£ÊÄÅÊñπÁ®ã</strong>Ôºö</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the
vector of our optimal coefficient estimates.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is the ‚Äúdesign
matrix‚Äù where each row is an observation and each column is a predictor
variable. <span class="math inline">\(\mathbf{X}\)</span>
ÊòØ‚ÄúËÆæËÆ°Áü©Èòµ‚ÄùÔºåÂÖ∂‰∏≠ÊØè‰∏ÄË°å‰ª£Ë°®‰∏Ä‰∏™ËßÇÊµãÂÄºÔºåÊØè‰∏ÄÂàó‰ª£Ë°®‰∏Ä‰∏™È¢ÑÊµãÂèòÈáè„ÄÇ</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of our
response variable. <span class="math inline">\(\mathbf{y}\)</span>
ÊòØÊàë‰ª¨ÁöÑÂìçÂ∫îÂèòÈáèÁöÑÂêëÈáè„ÄÇ</li>
</ul>
<p>This single equation is the general solution for finding the OLS
coefficients for <strong>any</strong> linear regression model, no matter
how many predictors you have. This is what statistical software
calculates for you under the hood.
Êó†ËÆ∫ÊúâÂ§öÂ∞ë‰∏™È¢ÑÊµãÂèòÈáèÔºåËøô‰∏™ÁÆÄÂçïÁöÑÊñπÁ®ãÈÉΩÊòØ<strong>‰ªª‰Ωï</strong>Á∫øÊÄßÂõûÂΩíÊ®°Âûã‰∏≠
OLS Á≥ªÊï∞ÁöÑÈÄöËß£„ÄÇ</p>
<h1 id="matrix-notatio">5. matrix notatio</h1>
<p><img src="/imgs/5054C3/matrix_notatio.png"></p>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>: This slide introduces the <strong>matrix
notation</strong> for multiple linear regression, which is a powerful
way to represent the entire system of equations in a compact form. This
notation isn‚Äôt just for tidiness‚Äîit‚Äôs the foundation for how the
solutions are derived and calculated in software.</li>
</ul>
<p>Â§öÂÖÉÁ∫øÊÄßÂõûÂΩíÁöÑ<strong>Áü©ÈòµÁ¨¶Âè∑</strong>ÔºåËøôÊòØ‰∏ÄÁßç‰ª•Á¥ßÂáëÂΩ¢ÂºèË°®Á§∫Êï¥‰∏™ÊñπÁ®ãÁªÑÁöÑÊúâÊïàÊñπÊ≥ï„ÄÇËøôÁßçÁ¨¶Âè∑‰∏ç‰ªÖ‰ªÖÊòØ‰∏∫‰∫ÜÁÆÄÊ¥ÅÔºåÂÆÉËøòÊòØËΩØ‰ª∂‰∏≠Êé®ÂØºÂíåËÆ°ÁÆóËß£ÁöÑÂü∫Á°Ä„ÄÇ
Here is a more detailed breakdown.</p>
<h3 id="why-use-matrix-notation">## Why Use Matrix Notation?</h3>
<p>Imagine you have 10,000 observations (<span
class="math inline">\(n=10,000\)</span>) and 5 predictor variables
(<span class="math inline">\(p=5\)</span>). Writing out the model
equation for each observation would be impossible: <span
class="math inline">\(y_1 = \beta_0 + \beta_1x_{11} + \dots +
\beta_5x_{15} + \epsilon_1\)</span> <span class="math inline">\(y_2 =
\beta_0 + \beta_1x_{21} + \dots + \beta_5x_{25} + \epsilon_2\)</span>
‚Ä¶and so on for 10,000 lines.</p>
<p>ÂÅáËÆæ‰Ω†Êúâ 10,000 ‰∏™ËßÇÊµãÂÄºÔºàn=10,000ÔºâÂíå 5
‰∏™È¢ÑÊµãÂèòÈáèÔºàp=5Ôºâ„ÄÇ‰∏∫ÊØè‰∏™ËßÇÊµãÂÄºÂÜôÂá∫Ê®°ÂûãÊñπÁ®ãÊòØ‰∏çÂèØËÉΩÁöÑÔºö <span
class="math inline">\(y_1 = \beta_0 + \beta_1x_{11} + \dots +
\beta_5x_{15} + \epsilon_1\)</span> <span class="math inline">\(y_2 =
\beta_0 + \beta_1x_{21} + \dots + \beta_5x_{25} + \epsilon_2\)</span>
‚Ä¶‚Ä¶‰ª•Ê≠§Á±ªÊé®ÔºåÁõ¥Âà∞ 10,000 Ë°å„ÄÇ Matrix notation allows us to consolidate
this entire system into a single, elegant
equation:Áü©ÈòµÁ¨¶Âè∑‰ΩøÊàë‰ª¨ËÉΩÂ§üÂ∞ÜÊï¥‰∏™Á≥ªÁªüÂêàÂπ∂Êàê‰∏Ä‰∏™ÁÆÄÊ¥ÅÁöÑÊñπÁ®ãÔºö <span
class="math display">\[\mathbf{y} = \mathbf{X}\boldsymbol{\beta} +
\boldsymbol{\epsilon}\]</span> Let‚Äôs break down each component shown on
your slide.</p>
<h3 id="the-components-explained">## The Components Explained</h3>
<h4 id="the-design-matrix-mathbfx-ËÆæËÆ°Áü©Èòµ">1. The Design Matrix: <span
class="math inline">\(\mathbf{X}\)</span> ËÆæËÆ°Áü©Èòµ</h4>
<p><span class="math display">\[\mathbf{X} = \begin{pmatrix} 1 &amp;
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\ 1 &amp; x_{21} &amp;
x_{22} &amp; \cdots &amp; x_{2p} \\ \vdots &amp; \vdots &amp; \vdots
&amp; \ddots &amp; \vdots \\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots
&amp; x_{np} \end{pmatrix}\]</span> This is the most important matrix.
It contains all of your predictor variable
data.ËøôÊòØÊúÄÈáçË¶ÅÁöÑÁü©Èòµ„ÄÇÂÆÉÂåÖÂê´ÊâÄÊúâÈ¢ÑÊµãÂèòÈáèÊï∞ÊçÆ„ÄÇ * <strong>Rows:</strong>
Each row represents a single observation (e.g., a person, a company, a
day). There are <strong>n</strong>
rows.ÊØè‰∏ÄË°å‰ª£Ë°®‰∏Ä‰∏™ËßÇÂØüÂÄºÔºà‰æãÂ¶ÇÔºå‰∏Ä‰∏™‰∫∫„ÄÅ‰∏ÄÂÆ∂ÂÖ¨Âè∏„ÄÅ‰∏ÄÂ§©Ôºâ„ÄÇÂÖ±Êúâ
<strong>n</strong> Ë°å„ÄÇ * <strong>Columns:</strong> Each column
represents a predictor variable. There are <strong>p</strong> predictor
columns, plus one special column.ÊØèÂàó‰ª£Ë°®‰∏Ä‰∏™È¢ÑÊµãÂèòÈáè„ÄÇÂÖ±Êúâ
<strong>p</strong> ‰∏™È¢ÑÊµãÂàóÔºåÂ§ñÂä†‰∏Ä‰∏™ÁâπÊÆäÂàó„ÄÇ * <strong>The Column of
Ones:</strong> This is a crucial detail. This first column of all ones
is a placeholder for the <strong>intercept term (<span
class="math inline">\(\beta_0\)</span>)</strong>. When you perform
matrix multiplication, this <code>1</code> gets multiplied by <span
class="math inline">\(\beta_0\)</span>, ensuring the intercept is
included in the model for every single observation.
ËøôÊòØ‰∏Ä‰∏™Ëá≥ÂÖ≥ÈáçË¶ÅÁöÑÁªÜËäÇ„ÄÇÁ¨¨‰∏ÄÂàóÔºàÂÖ® 1ÔºâÊòØ<strong>Êà™Ë∑ùÈ°π (<span
class="math inline">\(\beta_0\)</span>)</strong>
ÁöÑÂç†‰ΩçÁ¨¶„ÄÇÊâßË°åÁü©Èòµ‰πòÊ≥ïÊó∂ÔºåËøô‰∏™ <code>1</code> ‰ºö‰πò‰ª• <span
class="math inline">\(\beta_0\)</span>Ôºå‰ª•Á°Æ‰øùÊà™Ë∑ùÂåÖÂê´Âú®Ê®°Âûã‰∏≠ÔºåÈÄÇÁî®‰∫éÊØè‰∏™ËßÇÊµãÂÄº„ÄÇ</p>
<h4 id="the-coefficient-vector-boldsymbolbeta-Á≥ªÊï∞ÂêëÈáè">2. The
Coefficient Vector: <span
class="math inline">\(\boldsymbol{\beta}\)</span> Á≥ªÊï∞ÂêëÈáè</h4>
<p><span class="math display">\[\boldsymbol{\beta} = \begin{pmatrix}
\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix}\]</span> This is a
column vector that contains all the model parameters‚Äîthe unknown values
we want to estimate. The goal of linear regression is to find the
numerical values for this vector.</p>
<h4 id="the-response-vector-mathbfy-ÂìçÂ∫îÂêëÈáè">3. The Response Vector:
<span class="math inline">\(\mathbf{y}\)</span> ÂìçÂ∫îÂêëÈáè</h4>
<p><span class="math display">\[\mathbf{y} = \begin{pmatrix} y_1 \\
\vdots \\ y_n \end{pmatrix}\]</span> This is a column vector containing
all the observed outcomes you are trying to predict (e.g., sales, test
scores, stock prices).</p>
<h4 id="the-error-vector-boldsymbolepsilon-ËØØÂ∑ÆÂêëÈáè">4. The Error
Vector: <span class="math inline">\(\boldsymbol{\epsilon}\)</span>
ËØØÂ∑ÆÂêëÈáè</h4>
<p><span class="math display">\[\boldsymbol{\epsilon} = \begin{pmatrix}
\epsilon_1 \\ \vdots \\ \epsilon_n \end{pmatrix}\]</span> This column
vector bundles together all the individual, unobserved random errors. It
represents the portion of <strong>y</strong> that our model cannot
explain with <strong>X</strong>.</p>
<h3 id="putting-it-all-together">## Putting It All Together</h3>
<p>When you write the equation <span class="math inline">\(\mathbf{y} =
\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span>, you are
actually representing the entire system of individual equations.</p>
<p>Let‚Äôs look at the multiplication <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>: <span
class="math display">\[\begin{pmatrix} 1 &amp; x_{11} &amp; \dots &amp;
x_{1p} \\ 1 &amp; x_{21} &amp; \dots &amp; x_{2p} \\ \vdots &amp; \vdots
&amp; \ddots &amp; \vdots \\ 1 &amp; x_{n1} &amp; \dots &amp; x_{np}
\end{pmatrix} \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
\end{pmatrix} = \begin{pmatrix} 1\cdot\beta_0 + x_{11}\cdot\beta_1 +
\dots + x_{1p}\cdot\beta_p \\ 1\cdot\beta_0 + x_{21}\cdot\beta_1 + \dots
+ x_{2p}\cdot\beta_p \\ \vdots \\ 1\cdot\beta_0 + x_{n1}\cdot\beta_1 +
\dots + x_{np}\cdot\beta_p \end{pmatrix}\]</span> As you can see, the
result of this multiplication is a single column vector where each row
is the ‚Äúpredictor‚Äù part of the regression equation for that observation.
Ê≠§‰πòÊ≥ïÁöÑÁªìÊûúÊòØ‰∏Ä‰∏™ÂçïÂàóÂêëÈáèÔºåÂÖ∂‰∏≠ÊØè‰∏ÄË°åÈÉΩÊòØËØ•ËßÇÊµãÂÄºÁöÑÂõûÂΩíÊñπÁ®ãÁöÑ‚ÄúÈ¢ÑÊµãÂèòÈáè‚ÄùÈÉ®ÂàÜ„ÄÇ</p>
<p>By setting this equal to <span class="math inline">\(\mathbf{y} -
\boldsymbol{\epsilon}\)</span>, you perfectly recreate the entire set of
<code>n</code> equations in one clean statement. This compact form is
what allows us to easily derive and compute the <strong>Normal
Equation</strong> solution: <span
class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>.ËøôÁßçÁ¥ßÂáëÂΩ¢Âºè‰ΩøÊàë‰ª¨ËÉΩÂ§üËΩªÊùæÊé®ÂØºÂíåËÆ°ÁÆó<strong>Ê≠£ÊÄÅÊñπÁ®ã</strong>ÁöÑËß£</p>
<h1
id="the-core-mathematical-conclusion-of-ordinary-least-squares-ols">6.
the core mathematical conclusion of Ordinary Least Squares (OLS)</h1>
<p><img src="/imgs/5054C3/OLS1.png">
<img src="/imgs/5054C3/OLS2.png"></p>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>: Of course. These slides present the core
mathematical conclusion of Ordinary Least Squares (OLS) and a key
geometric property that explains <em>why</em> this solution works.
Â±ïÁ§∫‰∫ÜÊôÆÈÄöÊúÄÂ∞è‰∫å‰πòÊ≥ï (OLS)
ÁöÑÊ†∏ÂøÉÊï∞Â≠¶ÁªìËÆ∫Ôºå‰ª•Âèä‰∏Ä‰∏™ÂÖ≥ÈîÆÁöÑÂá†‰ΩïÊÄßË¥®ÔºåËß£Èáä‰∫ÜËØ•Ëß£ÂÜ≥ÊñπÊ°à<em>‰∏∫‰Ωï</em>ÊúâÊïà„ÄÇ
Let‚Äôs break down the concepts and the calculation processes in
detail.</li>
</ul>
<hr />
<h3 id="part-1-the-objective-and-the-solution-slide-1-ÊúÄÂ∞èÂåñÂá†‰ΩïË∑ùÁ¶ª">##
Part 1: The Objective and the Solution (Slide 1) ÊúÄÂ∞èÂåñÂá†‰ΩïË∑ùÁ¶ª</h3>
<p>This slide summarizes the entire OLS problem and its solution in the
language of matrix algebra.</p>
<h4 id="the-concept-minimizing-geometric-distance"><strong>The Concept:
Minimizing Geometric Distance</strong></h4>
<p>‚ÄúÊúÄÂ∞è‰∫å‰πòÂáÜÂàô‚ÄùÊòØÊàë‰ª¨Ê®°ÂûãÁöÑÁõÆÊ†á„ÄÇ The ‚Äúleast squares criterion‚Äù is the
objective of our model. The slide shows it in two equivalent forms:</p>
<ol type="1">
<li><strong>Summation Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_{i1} -
\dots - \beta_px_{ip})^2\)</span> This is the sum of the squared
differences between the actual values (<span
class="math inline">\(y_i\)</span>) and the predicted values. ËøôÊòØÂÆûÈôÖÂÄº
(<span class="math inline">\(y_i\)</span>) ‰∏éÈ¢ÑÊµãÂÄº‰πãÂ∑ÆÁöÑÂπ≥ÊñπÂíå„ÄÇ</li>
<li><strong>Matrix Form:</strong> <span
class="math inline">\(||\mathbf{y} - \mathbf{X}\boldsymbol{\beta}||^2 =
(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} -
\mathbf{X}\boldsymbol{\beta})\)</span> This is the more powerful way to
view the problem. Think of <span
class="math inline">\(\mathbf{y}\)</span> (the vector of all actual
outcomes) and <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> (the vector
of all predicted outcomes) as two points in an n-dimensional space. The
expression <span class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{\beta}||^2\)</span> represents the <strong>squared
Euclidean distance</strong> between these two points. Â∞Ü <span
class="math inline">\(\mathbf{y}\)</span>ÔºàÊâÄÊúâÂÆûÈôÖÁªìÊûúÁöÑÂêëÈáèÔºâÂíå <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>ÔºàÊâÄÊúâÈ¢ÑÊµãÁªìÊûúÁöÑÂêëÈáèÔºâËßÜ‰∏∫
n Áª¥Á©∫Èó¥‰∏≠ÁöÑ‰∏§‰∏™ÁÇπ„ÄÇË°®ËææÂºè <span class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{\beta}||^2\)</span>
Ë°®Á§∫Ëøô‰∏§ÁÇπ‰πãÈó¥ÁöÑ<strong>Âπ≥ÊñπÊ¨ßÊ∞èË∑ùÁ¶ª</strong>„ÄÇ Therefore, the OLS
problem is a geometric one: <strong>Find the coefficient vector <span
class="math inline">\(\boldsymbol{\beta}\)</span> that makes the
predicted values vector <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> as close as
possible to the actual values vector <span
class="math inline">\(\mathbf{y}\)</span>.</strong> Âõ†Ê≠§ÔºåOLS
ÈóÆÈ¢òÊòØ‰∏Ä‰∏™Âá†‰ΩïÈóÆÈ¢òÔºö<strong>ÊâæÂà∞‰∏Ä‰∏™Á≥ªÊï∞ÂêëÈáè <span
class="math inline">\(\boldsymbol{\beta}\)</span>Ôºå‰ΩøÈ¢ÑÊµãÂÄºÂêëÈáè <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>
Â∞ΩÂèØËÉΩÊé•ËøëÂÆûÈôÖÂÄºÂêëÈáè <span
class="math inline">\(\mathbf{y}\)</span>„ÄÇ</strong></li>
</ol>
<h4
id="the-solution-the-least-squares-estimator-lseÊúÄÂ∞è‰∫å‰πò‰º∞ËÆ°Âô®-lse"><strong>The
Solution: The Least Squares Estimator (LSE)</strong>ÊúÄÂ∞è‰∫å‰πò‰º∞ËÆ°Âô®
(LSE)</h4>
<p>The slide provides the direct solution to this minimization problem,
which is the <strong>Normal
Equation</strong>:Ê≠§ÊúÄÂ∞èÂåñÈóÆÈ¢òÁöÑÁõ¥Êé•Ëß£ÔºåÂç≥<strong>Ê≠£ÊÄÅÊñπÁ®ã</strong>Ôºö</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<p>This formula gives you the exact vector of coefficients <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that minimizes
the squared distance. We get this formula by taking the gradient (the
multidimensional version of a derivative) of the distance function with
respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>,
setting it to zero, and solving, as hinted at in your previous slides.
ÁªôÂá∫‰∫Ü‰ΩøÂπ≥ÊñπË∑ùÁ¶ªÊúÄÂ∞èÂåñÁöÑÁ≤æÁ°ÆÁ≥ªÊï∞ÂêëÈáè ÈÄöËøáÂèñË∑ùÁ¶ªÂáΩÊï∞ÂÖ≥‰∫é <span
class="math inline">\(\boldsymbol{\beta}\)</span>
ÁöÑÊ¢ØÂ∫¶ÔºàÂØºÊï∞ÁöÑÂ§öÁª¥ÁâàÊú¨ÔºâÔºåÂ∞ÜÂÖ∂ËÆæ‰∏∫Èõ∂ÔºåÁÑ∂ÂêéÊ±ÇËß£ÔºåÂç≥ÂèØÂæóÂà∞Ê≠§ÂÖ¨Âºè„ÄÇ
Finally, the slide defines: * <strong>Fitted values:</strong> <span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span> (The vector of predictions
using our optimal coefficients). ÊãüÂêàÂÄº * <strong>Residuals:</strong>
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span> (The vector of errors, representing the
difference between actuals and
predictions).ËØØÂ∑ÆÂêëÈáèÔºåË°®Á§∫ÂÆûÈôÖÂÄº‰∏éÈ¢ÑÊµãÂÄº‰πãÈó¥ÁöÑÂ∑ÆÂºÇ</p>
<h3
id="part-2-the-geometric-property-and-proofs-slide-2Âá†‰ΩïÊÄßË¥®ÂèäËØÅÊòé">##
Part 2: The Geometric Property and Proofs (Slide 2)Âá†‰ΩïÊÄßË¥®ÂèäËØÅÊòé</h3>
<p>This slide explains a beautiful and fundamental property of the least
squares solution:
<strong>orthogonality</strong>.Ëß£Èáä‰∫ÜÊúÄÂ∞è‰∫å‰πòËß£ÁöÑ‰∏Ä‰∏™ÁæéÂ¶ôËÄåÂü∫Êú¨ÁöÑÊÄßË¥®Ôºö<strong>Ê≠£‰∫§ÊÄß</strong>„ÄÇ</p>
<h4 id="the-concept-orthogonality-of-residualsÊÆãÂ∑ÆÁöÑÊ≠£‰∫§ÊÄß"><strong>The
Concept: Orthogonality of Residuals</strong>ÊÆãÂ∑ÆÁöÑÊ≠£‰∫§ÊÄß</h4>
<p>The main idea is that the residual vector <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> is
<strong>orthogonal</strong> (perpendicular) to every predictor variable
in your model. ‰∏ªË¶ÅÊÄùÊÉ≥ÊòØÊÆãÂ∑ÆÂêëÈáè <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>
‰∏éÊ®°Âûã‰∏≠ÁöÑÊØè‰∏™È¢ÑÊµãÂèòÈáè<strong>Ê≠£‰∫§</strong>ÔºàÂûÇÁõ¥Ôºâ„ÄÇ</p>
<ul>
<li><p><strong>Geometric Intuition:</strong> Think of the columns of
your matrix <span class="math inline">\(\mathbf{X}\)</span> (i.e., your
predictors and the intercept) as defining a flat surface, or a
‚Äúhyperplane,‚Äù in a high-dimensional space. Your actual data vector <span
class="math inline">\(\mathbf{y}\)</span> exists somewhere in this
space, likely not on the hyperplane. The OLS process finds the point on
that hyperplane, <span class="math inline">\(\hat{\mathbf{y}}\)</span>,
that is closest to <span class="math inline">\(\mathbf{y}\)</span>. The
shortest line from a point to a plane is always one that is
<strong>perpendicular</strong> to the plane. The residual vector, <span
class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>, <em>is</em> that line. Â∞ÜÁü©Èòµ <span
class="math inline">\(\mathbf{X}\)</span>
ÁöÑÂàóÔºàÂç≥È¢ÑÊµãÂèòÈáèÂíåÊà™Ë∑ùÔºâÊÉ≥Ë±°ÊàêÂú®È´òÁª¥Á©∫Èó¥‰∏≠ÂÆö‰πâ‰∏Ä‰∏™Âπ≥Èù¢Êàñ‚ÄúË∂ÖÂπ≥Èù¢‚Äù„ÄÇÂÆûÈôÖÊï∞ÊçÆÂêëÈáè
<span class="math inline">\(\mathbf{y}\)</span>
Â≠òÂú®‰∫éËØ•Á©∫Èó¥ÁöÑÊüê‰∏™‰ΩçÁΩÆÔºåÂèØËÉΩ‰∏çÂú®Ë∂ÖÂπ≥Èù¢‰∏ä„ÄÇOLS ËøáÁ®ã‰ºöÂú®ËØ•Ë∂ÖÂπ≥Èù¢ <span
class="math inline">\(\hat{\mathbf{y}}\)</span> ‰∏äÊâæÂà∞‰∏é <span
class="math inline">\(\mathbf{y}\)</span>
ÊúÄÊé•ËøëÁöÑÁÇπ„ÄÇ‰ªé‰∏Ä‰∏™ÁÇπÂà∞‰∏Ä‰∏™Âπ≥Èù¢ÁöÑÊúÄÁü≠Á∫øÂßãÁªàÊòØ‰∏éËØ•Âπ≥Èù¢<strong>ÂûÇÁõ¥</strong>ÁöÑÁ∫ø„ÄÇÊÆãÂ∑ÆÂêëÈáè
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span> Â∞±ÊòØËøôÊù°Áõ¥Á∫ø„ÄÇ</p></li>
<li><p><strong>Mathematical Statement:</strong> This geometric property
is stated as <span class="math inline">\(\mathbf{X}^T
\hat{\boldsymbol{\epsilon}} = \mathbf{0}\)</span>. This equation means
that the dot product of the residual vector with every column of <span
class="math inline">\(\mathbf{X}\)</span> is zero, which is the
mathematical definition of orthogonality. ËØ•Á≠âÂºèÊÑèÂë≥ÁùÄÊÆãÂ∑ÆÂêëÈáè‰∏é <span
class="math inline">\(\mathbf{X}\)</span>
ÊØè‰∏ÄÂàóÁöÑÁÇπÁßØÈÉΩ‰∏∫Èõ∂ÔºåËøôÊ≠£ÊòØÊ≠£‰∫§ÊÄßÁöÑÊï∞Â≠¶ÂÆö‰πâ„ÄÇ</p></li>
</ul>
<h4 id="the-calculation-process-the-proofs"><strong>The Calculation
Process (The Proofs)</strong></h4>
<p><strong>1. Proof of Orthogonality:</strong> The slide shows a
step-by-step calculation to prove that <span
class="math inline">\(\mathbf{X}^T \hat{\boldsymbol{\epsilon}}\)</span>
is indeed zero. * <strong>Step 1:</strong> Start with the expression to
be proven: <span class="math inline">\(\mathbf{X}^T
\hat{\boldsymbol{\epsilon}}\)</span> ‰ªéÂæÖËØÅÊòéÁöÑË°®ËææÂºèÂºÄÂßãÔºö *
<strong>Step 2:</strong> Substitute the definition of the residual,
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\mathbf{X}^T (\mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}})\]</span> ‰ª£ÂÖ•ÊÆãÂ∑ÆÁöÑÂÆö‰πâ *
<strong>Step 3:</strong> Distribute the <span
class="math inline">\(\mathbf{X}^T\)</span>: <span
class="math display">\[\mathbf{X}^T \mathbf{y} -
\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}}\]</span><em>ÂàÜÈÖç </em>
<strong>Step 4:</strong> Substitute the Normal Equation for <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\mathbf{X}^T \mathbf{y} - \mathbf{X}^T\mathbf{X}
[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}]\]</span> *
<strong>Step 5:</strong> The key step is the cancellation. A matrix
<span class="math inline">\((\mathbf{X}^T\mathbf{X})\)</span> multiplied
by its inverse <span
class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span> equals the
identity matrix <span class="math inline">\(\mathbf{I}\)</span>, which
acts like the number 1 in multiplication. <span
class="math display">\[\mathbf{X}^T \mathbf{y} - \mathbf{I}
\mathbf{X}^T\mathbf{y} = \mathbf{X}^T \mathbf{y} -
\mathbf{X}^T\mathbf{y} = \mathbf{0}\]</span> ÂÖ≥ÈîÆÊ≠•È™§ÊòØÊ∂àÂéª„ÄÇ This
completes the proof, showing that the orthogonality property is a direct
consequence of the Normal Equation solution.</p>
<p><strong>2. Proof of LSE:</strong> This is a more abstract proof
showing that our <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> truly gives the
minimum possible error. It uses the orthogonality property and the
Pythagorean theorem for vectors. It essentially shows that for any other
possible coefficient vector <span
class="math inline">\(\boldsymbol{v}\)</span>, the error <span
class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{v}||^2\)</span> will always be greater than or
equal to the error from our LSE, <span
class="math inline">\(||\mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}}||^2\)</span>.</p>
<h1 id="geometric-interpretation">7.geometric interpretation</h1>
<p><img src="/imgs/5054C3/geometric_interpretation1.png">
<img src="/imgs/5054C3/geometric_interpretation2.png"></p>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>:</li>
</ul>
<p>These two slides together provide a powerful geometric interpretation
of how Ordinary Least Squares (OLS) works, centered on the concepts of
<strong>orthogonality</strong> and <strong>projection</strong>.
‰ª•<strong>Ê≠£‰∫§ÊÄß</strong>Âíå<strong>ÊäïÂΩ±</strong>ÁöÑÊ¶ÇÂøµ‰∏∫‰∏≠ÂøÉÔºå‰ªéÂá†‰ΩïËßíÂ∫¶ÊúâÂäõÂú∞ËØ†Èáä‰∫ÜÊôÆÈÄöÊúÄÂ∞è‰∫å‰πòÊ≥ï
(OLS) ÁöÑÂ∑•‰ΩúÂéüÁêÜ„ÄÇ</p>
<p>Here‚Äôs a detailed summary of the concepts and the processes they
describe.</p>
<h3 id="summary">## Summary</h3>
<p>These slides explain that the process of finding the ‚Äúbest fit‚Äù line
in regression is geometrically equivalent to <strong>projecting</strong>
the actual data vector (<span class="math inline">\(\mathbf{y}\)</span>)
onto a hyperplane defined by the predictor variables (<span
class="math inline">\(\mathbf{X}\)</span>). This projection splits the
actual data into two perpendicular components:
Ëß£Èáä‰∫ÜÂõûÂΩíÂàÜÊûê‰∏≠ÂØªÊâæ‚ÄúÊúÄ‰Ω≥ÊãüÂêà‚ÄùÁõ¥Á∫øÁöÑËøáÁ®ãÔºåÂÖ∂Âá†‰ΩïÊÑè‰πâÁ≠âÂêå‰∫éÂ∞ÜÂÆûÈôÖÊï∞ÊçÆÂêëÈáè
(<span class="math inline">\(\mathbf{y}\)</span>)
<strong>ÊäïÂΩ±</strong>Âà∞Áî±È¢ÑÊµãÂèòÈáè (<span
class="math inline">\(\mathbf{X}\)</span>)
ÂÆö‰πâÁöÑË∂ÖÂπ≥Èù¢‰∏ä„ÄÇÊ≠§ÊäïÂΩ±Â∞ÜÂÆûÈôÖÊï∞ÊçÆÊãÜÂàÜ‰∏∫‰∏§‰∏™ÂûÇÁõ¥ÂàÜÈáèÔºö</p>
<ol type="1">
<li><strong>The Fitted Values (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>):</strong> The part of
the data that is perfectly explained by the model (the projection).
Êï∞ÊçÆ‰∏≠ËÉΩÂ§üË¢´Ê®°ÂûãÂÆåÁæéËß£ÈáäÁöÑÈÉ®ÂàÜÔºàÊäïÂΩ±Ôºâ„ÄÇ</li>
<li><strong>The Residuals (<span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>):</strong>
The part of the data that is unexplained (the error), which is
perpendicular to the explained part.
Êï∞ÊçÆ‰∏≠Êó†Ê≥ïËß£ÈáäÁöÑÈÉ®ÂàÜÔºàËØØÂ∑ÆÔºâÔºåÂÆÉ‰∏éË¢´Ëß£ÈáäÈÉ®ÂàÜÂûÇÁõ¥„ÄÇ A special tool called
the <strong>projection matrix (H)</strong>, or ‚Äúhat matrix,‚Äù is
introduced as the operator that performs this projection.
ÂºïÂÖ•‰∏Ä‰∏™Áß∞‰∏∫<strong>ÊäïÂΩ±Áü©Èòµ
(H)</strong>ÔºàÊàñÁß∞‚ÄúÂ∏ΩÂ≠êÁü©Èòµ‚ÄùÔºâÁöÑÁâπÊÆäÂ∑•ÂÖ∑Ôºå‰Ωú‰∏∫ÊâßË°åÊ≠§ÊäïÂΩ±ÁöÑËøêÁÆóÁ¨¶„ÄÇ</li>
</ol>
<h3 id="concepts-and-process-explained-in-detail">## Concepts and
Process Explained in Detail</h3>
<h4 id="the-fitted-values-as-a-linear-combination-ÊãüÂêàÂÄº‰Ωú‰∏∫Á∫øÊÄßÁªÑÂêà">1.
The Fitted Values as a Linear Combination ÊãüÂêàÂÄº‰Ωú‰∏∫Á∫øÊÄßÁªÑÂêà</h4>
<p>The first slide starts by stating that the fitted value vector <span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span> is a <strong>linear
combination</strong> of the columns of <span
class="math inline">\(\mathbf{X}\)</span> (your predictors).</p>
<ul>
<li><p><strong>Concept:</strong> This means that the vector of fitted
values, <span class="math inline">\(\hat{\mathbf{y}}\)</span>, must lie
within the geometric space (a line, plane, or hyperplane) spanned by
your predictor variables. The model is incapable of producing a
prediction that does not live in this space. ËøôÊÑèÂë≥ÁùÄÊãüÂêàÂÄºÂêëÈáè <span
class="math inline">\(\hat{\mathbf{y}}\)</span>
ÂøÖÈ°ª‰Ωç‰∫éÈ¢ÑÊµãÂèòÈáèÊâÄÊûÑÊàêÁöÑÂá†‰ΩïÁ©∫Èó¥ÔºàÁõ¥Á∫ø„ÄÅÂπ≥Èù¢ÊàñË∂ÖÂπ≥Èù¢ÔºâÂÜÖ„ÄÇÊ®°ÂûãÊó†Ê≥ïÁîüÊàê‰∏çÂ≠òÂú®‰∫éÊ≠§Á©∫Èó¥ÁöÑÈ¢ÑÊµã„ÄÇ
#### 2. The Projection Matrix (The ‚ÄúHat Matrix‚Äù) ÊäïÂΩ±Áü©ÈòµÔºà‚ÄúÂ∏ΩÂ≠êÁü©Èòµ‚ÄùÔºâ
The second slide introduces the tool that makes this projection happen:
the <strong>projection matrix</strong>, also called the <strong>hat
matrix</strong>, <strong>H</strong>.</p></li>
<li><p><strong>Definition:</strong> <span
class="math inline">\(\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span></p></li>
<li><p><strong>Process:</strong> This matrix has a special job. When you
multiply it by any vector (like our data vector <span
class="math inline">\(\mathbf{y}\)</span>), it projects that vector onto
the space spanned by the columns of <span
class="math inline">\(\mathbf{X}\)</span>. We can see this by starting
with our definition of fitted values and substituting the normal
equation solution for <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\hat{\mathbf{y}} =
\mathbf{X}\hat{\boldsymbol{\beta}} =
\mathbf{X}[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}] =
[\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T]\mathbf{y}\]</span>
This shows that: <span class="math display">\[\hat{\mathbf{y}} =
\mathbf{H}\mathbf{y}\]</span> This is why <strong>H</strong> is
nicknamed the <strong>hat matrix</strong>‚Äîit ‚Äúputs the hat‚Äù on <span
class="math inline">\(\mathbf{y}\)</span>.
Ëøô‰∏™Áü©ÈòµÊúâÂÖ∂ÁâπÊÆäÁöÑÁî®ÈÄî„ÄÇÂΩì‰Ω†Â∞ÜÂÆÉ‰πò‰ª•‰ªª‰ΩïÂêëÈáèÔºà‰æãÂ¶ÇÊàë‰ª¨ÁöÑÊï∞ÊçÆÂêëÈáè <span
class="math inline">\(\mathbf{y}\)</span>ÔºâÊó∂ÔºåÂÆÉ‰ºöÂ∞ÜËØ•ÂêëÈáèÊäïÂΩ±Âà∞Áî±
<span class="math inline">\(\mathbf{X}\)</span> ÁöÑÂàóÊâÄË∑®Ë∂äÁöÑÁ©∫Èó¥‰∏ä„ÄÇ
#### 3. The Orthogonality of Fitted Values and Residuals
ÊãüÂêàÂÄºÂíåÊÆãÂ∑ÆÁöÑÊ≠£‰∫§ÊÄß This is the central concept of the first slide and
a fundamental property of least squares.</p></li>
<li><p><strong>Concept:</strong> The fitted value vector (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>) and the residual vector
(<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>) are <strong>orthogonal</strong>
(perpendicular) to each other.</p></li>
<li><p><strong>Mathematical Statement:</strong> Their dot product is
zero: <span class="math inline">\(\hat{\mathbf{y}}^T(\mathbf{y} -
\hat{\mathbf{y}}) = 0\)</span>.</p></li>
<li><p><strong>Geometric Intuition:</strong> This means the vectors
<span class="math inline">\(\mathbf{y}\)</span>, <span
class="math inline">\(\hat{\mathbf{y}}\)</span>, and <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> form a
<strong>right-angled triangle</strong> in n-dimensional space. The
actual data vector <span class="math inline">\(\mathbf{y}\)</span> is
the hypotenuse, while the model‚Äôs prediction <span
class="math inline">\(\hat{\mathbf{y}}\)</span> and the error <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> are the two
perpendicular legs. ËøôÊÑèÂë≥ÁùÄÂêëÈáè <span
class="math inline">\(\mathbf{y}\)</span>„ÄÅ<span
class="math inline">\(\hat{\mathbf{y}}\)</span> Âíå <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> Âú® n
Áª¥Á©∫Èó¥‰∏≠ÊûÑÊàê‰∏Ä‰∏™<strong>Áõ¥Ëßí‰∏âËßíÂΩ¢</strong>„ÄÇÂÆûÈôÖÊï∞ÊçÆÂêëÈáè <span
class="math inline">\(\mathbf{y}\)</span> ÊòØÊñúËæπÔºåËÄåÊ®°ÂûãÁöÑÈ¢ÑÊµãÂÄº <span
class="math inline">\(\hat{\mathbf{y}}\)</span> ÂíåËØØÂ∑ÆÂÄº <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>
ÊòØ‰∏§Êù°ÂûÇÁõ¥Ëæπ„ÄÇ</p></li>
</ul>
<h4 id="the-pythagorean-theorem-of-least-squares">4. The Pythagorean
Theorem of Least Squares</h4>
<p>The orthogonality relationship directly implies the Pythagorean
theorem.</p>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(||\mathbf{y}||^2 = ||\hat{\mathbf{y}}||^2 +
||\mathbf{y} - \hat{\mathbf{y}}||^2\)</span></li>
<li><strong>Concept:</strong> This is one of the most important
equations in statistics, as it partitions the total variance in the
data. ËøôÊòØÁªüËÆ°Â≠¶‰∏≠ÊúÄÈáçË¶ÅÁöÑÊñπÁ®ã‰πã‰∏ÄÔºåÂõ†‰∏∫ÂÆÉÂèØ‰ª•ÂàÜÂâ≤Êï∞ÊçÆ‰∏≠ÁöÑÊÄªÊñπÂ∑Æ„ÄÇ
<ul>
<li><span class="math inline">\(||\mathbf{y}||^2\)</span> is
proportional to the <strong>Total Sum of Squares (TSS):</strong> The
total variation of the response variable around its
mean.ÂìçÂ∫îÂèòÈáèÂõ¥ÁªïÂÖ∂ÂùáÂÄºÁöÑÊÄªÂèòÂºÇ„ÄÇ</li>
<li><span class="math inline">\(||\hat{\mathbf{y}}||^2\)</span> is
proportional to the <strong>Explained Sum of Squares (ESS):</strong> The
portion of the total variation that is explained by your regression
model.ÂõûÂΩíÊ®°ÂûãÂèØ‰ª•Ëß£ÈáäÁöÑÊÄªÂèòÂºÇÈÉ®ÂàÜ„ÄÇ</li>
<li><span class="math inline">\(||\mathbf{y} -
\hat{\mathbf{y}}||^2\)</span> is the <strong>Residual Sum of Squares
(RSS):</strong> The portion of the total variation that is left
unexplained (the error).ÊÄªÂèòÂºÇ‰∏≠Êú™Ëß£ÈáäÁöÑÈÉ®ÂàÜÔºàÂç≥ËØØÂ∑ÆÔºâ„ÄÇ</li>
</ul></li>
</ul>
<p>This relationship, <strong>Total Variation = Explained Variation +
Unexplained Variation</strong>, is the foundation for calculating
metrics like <strong>R-squared (<span
class="math inline">\(R^2\)</span>)</strong>, which measures the
goodness of fit of your model. <strong>ÊÄªÂèòÂºÇ = Ëß£ÈáäÂèòÂºÇ +
Êú™Ëß£ÈáäÂèòÂºÇ</strong>ÔºåÊòØËÆ°ÁÆó<strong>R Âπ≥Êñπ (<span
class="math inline">\(R^2\)</span>)</strong>
Á≠âÊåáÊ†áÁöÑÂü∫Á°ÄÔºåËØ•ÊåáÊ†áÁî®‰∫éË°°ÈáèÊ®°ÂûãÁöÑÊãüÂêà‰ºòÂ∫¶„ÄÇ</p>
<h4 id="residuals-and-the-identity-matrix-ÊÆãÂ∑ÆÂíåÂçï‰ΩçÁü©Èòµ">5. Residuals
and the Identity Matrix ÊÆãÂ∑ÆÂíåÂçï‰ΩçÁü©Èòµ</h4>
<p>Finally, the second slide shows that just as <strong>H</strong>
projects onto the ‚Äúmodel space,‚Äù a related matrix projects onto the
‚Äúerror space.‚Äù ÊúÄÂêéÔºåÁ¨¨‰∫åÂº†ÂπªÁÅØÁâáÊòæÁ§∫ÔºåÊ≠£Â¶Ç<strong>H</strong>
ÊäïÂΩ±Âà∞‚ÄúÊ®°ÂûãÁ©∫Èó¥‚Äù‰∏ÄÊ†∑ÔºåÁõ∏ÂÖ≥Áü©Èòµ‰πü‰ºöÊäïÂΩ±Âà∞‚ÄúËØØÂ∑ÆÁ©∫Èó¥‚Äù„ÄÇ *
<strong>Process:</strong> We can express the residuals using the hat
matrix: <span class="math display">\[\hat{\boldsymbol{\epsilon}} =
\mathbf{y} - \hat{\mathbf{y}} = \mathbf{y} - \mathbf{H}\mathbf{y} =
(\mathbf{I} - \mathbf{H})\mathbf{y}\]</span> The matrix <span
class="math inline">\((\mathbf{I} - \mathbf{H})\)</span> is also a
projection matrix. It takes the original data vector <span
class="math inline">\(\mathbf{y}\)</span> and projects it onto the space
that is orthogonal to all of your predictors, giving you the residual
vector directly.</p>
<h1
id="visualization-of-ordinary-least-squares-ols-regression">8.visualization
of Ordinary Least Squares (OLS) regression</h1>
<p><img src="/imgs/5054C3/visualization_of_Ordinary_Least_Squares_(OLS)_regression.png"></p>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>:</li>
</ul>
<p>This slide provides an excellent geometric visualization of what‚Äôs
happening ‚Äúunder the hood‚Äù in Ordinary Least Squares (OLS) regression.
It translates the algebraic formulas into a more intuitive spatial
concept. ËøôÂº†ÂπªÁÅØÁâá‰ª•Âá∫Ëâ≤ÁöÑÂá†‰ΩïÂèØËßÜÂåñÊñπÂºèÂ±ïÁé∞‰∫ÜÊôÆÈÄöÊúÄÂ∞è‰∫å‰πò (OLS)
ÂõûÂΩíÁöÑ‚ÄúÂπïÂêé‚ÄùÊú∫Âà∂„ÄÇÂÆÉÂ∞Ü‰ª£Êï∞ÂÖ¨ÂºèËΩ¨Âåñ‰∏∫Êõ¥Áõ¥ËßÇÁöÑÁ©∫Èó¥Ê¶ÇÂøµ„ÄÇ</p>
<h3 id="summary-1">## Summary</h3>
<p>The image shows that the process of finding the least squares
estimates is geometrically equivalent to taking the <strong>actual
outcome vector</strong> (<span
class="math inline">\(\mathbf{y}\)</span>) and finding its
<strong>orthogonal projection</strong> (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>) onto a
<strong>hyperplane</strong> formed by the predictor variables (<span
class="math inline">\(\mathbf{x}_1\)</span> and <span
class="math inline">\(\mathbf{x}_2\)</span>). The projection <span
class="math inline">\(\hat{\mathbf{y}}\)</span> is the vector of fitted
values, representing the closest possible approximation of the real data
that the model can achieve.</p>
<p>ËØ•ÂõæÊòæÁ§∫ÔºåÂØªÊâæÊúÄÂ∞è‰∫å‰πò‰º∞ËÆ°ÂÄºÁöÑËøáÁ®ãÂú®Âá†‰Ωï‰∏äÁ≠âÂêå‰∫éÂ∞Ü<strong>ÂÆûÈôÖÁªìÊûúÂêëÈáè</strong>
(<span class="math inline">\(\mathbf{y}\)</span>)
Ê±ÇÂá∫ÂÖ∂<strong>Ê≠£‰∫§ÊäïÂΩ±</strong> (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>) Âà∞Áî±È¢ÑÊµãÂèòÈáè (<span
class="math inline">\(\mathbf{x}_1\)</span> Âíå <span
class="math inline">\(\mathbf{x}_2\)</span>
ÊûÑÊàêÁöÑ<strong>Ë∂ÖÂπ≥Èù¢</strong>‰∏ä„ÄÇÊäïÂΩ± <span
class="math inline">\(\hat{\mathbf{y}}\)</span>
ÊòØÊãüÂêàÂÄºÁöÑÂêëÈáèÔºåË°®Á§∫Ê®°ÂûãËÉΩÂ§üËææÂà∞ÁöÑ‰∏éÁúüÂÆûÊï∞ÊçÆÊúÄÊé•ËøëÁöÑËøë‰ººÂÄº„ÄÇ</p>
<h3 id="the-concepts-explained-spatiallyÁ©∫Èó¥Ê¶ÇÂøµËß£Èáä">## The Concepts
Explained SpatiallyÁ©∫Èó¥Ê¶ÇÂøµËß£Èáä</h3>
<p>Let‚Äôs break down each element of the diagram and its meaning:</p>
<h4 id="the-space-itself-Á©∫Èó¥Êú¨Ë∫´">1. The Space Itself Á©∫Èó¥Êú¨Ë∫´</h4>
<ul>
<li><strong>Concept:</strong> We are not in a simple 2D or 3D graph
where axes are X and Y. Instead, we are in an <strong>n-dimensional
space</strong>, where <strong>n is the number of observations</strong>
in your dataset. Each axis in this space corresponds to one observation
(e.g., one person, one day).
Êàë‰ª¨Âπ∂ÈùûË∫´Â§Ñ‰∏Ä‰∏™ÁÆÄÂçïÁöÑ‰∫åÁª¥Êàñ‰∏âÁª¥ÂõæÂΩ¢‰∏≠ÔºåÂÖ∂‰∏≠ÂùêÊ†áËΩ¥‰∏∫ X Âíå
Y„ÄÇÁõ∏ÂèçÔºåÊàë‰ª¨Ë∫´Â§Ñ‰∏Ä‰∏™ <strong>n Áª¥Á©∫Èó¥</strong>ÔºåÂÖ∂‰∏≠ <strong>n
ÊòØÊï∞ÊçÆÈõÜ‰∏≠ÁöÑËßÇÊµãÂÄºÊï∞Èáè</strong>„ÄÇÊ≠§Á©∫Èó¥‰∏≠ÁöÑÊØè‰∏™ËΩ¥ÂØπÂ∫î‰∏Ä‰∏™ËßÇÊµãÂÄºÔºà‰æãÂ¶ÇÔºå‰∏Ä‰∏™‰∫∫Ôºå‰∏ÄÂ§©Ôºâ„ÄÇ</li>
<li><strong>Meaning:</strong> A vector like <strong>y</strong> or
<strong>x‚ÇÅ</strong> is a single point in this high-dimensional space.
For example, if you have 50 data points, <strong>y</strong> is a vector
pointing to a specific location in a 50-dimensional space. ÂÉè
<strong>y</strong> Êàñ <strong>x‚ÇÅ</strong>
ËøôÊ†∑ÁöÑÂêëÈáèÊòØËøô‰∏™È´òÁª¥Á©∫Èó¥‰∏≠ÁöÑÂçï‰∏™ÁÇπ„ÄÇ‰æãÂ¶ÇÔºåÂ¶ÇÊûúÊÇ®Êúâ 50
‰∏™Êï∞ÊçÆÁÇπÔºå<strong>y</strong> Â∞±ÊòØÊåáÂêë 50 Áª¥Á©∫Èó¥‰∏≠ÁâπÂÆö‰ΩçÁΩÆÁöÑÂêëÈáè„ÄÇ</li>
</ul>
<h4
id="the-predictor-hyperplane-the-yellow-surfaceÈ¢ÑÊµãÂèòÈáèË∂ÖÂπ≥Èù¢ÈªÑËâ≤Ë°®Èù¢">2.
The Predictor Hyperplane (The Yellow
Surface)È¢ÑÊµãÂèòÈáèË∂ÖÂπ≥Èù¢ÔºàÈªÑËâ≤Ë°®Èù¢Ôºâ</h4>
<ul>
<li><strong>Concept:</strong> The vectors for your predictor variables,
<strong>x‚ÇÅ</strong> and <strong>x‚ÇÇ</strong>, define a flat surface. If
you had only one predictor, this would be a line. With two, it‚Äôs a
plane. With more, it‚Äôs a <strong>hyperplane</strong>.È¢ÑÊµãÂèòÈáèÁöÑÂêëÈáè
<strong>x‚ÇÅ</strong> Âíå <strong>x‚ÇÇ</strong>
ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™Âπ≥Èù¢„ÄÇÂ¶ÇÊûúÂè™Êúâ‰∏Ä‰∏™È¢ÑÊµãÂèòÈáèÔºåÂÆÉÂ∞±ÊòØ‰∏ÄÊù°Á∫ø„ÄÇÂ¶ÇÊûúÊúâ‰∏§‰∏™ÔºåÂÆÉÂ∞±ÊòØ‰∏Ä‰∏™Âπ≥Èù¢„ÄÇÂ¶ÇÊûúÊúâÊõ¥Â§öÁöÑÈ¢ÑÊµãÂèòÈáèÔºåÂÆÉÂ∞±ÊòØ‰∏Ä‰∏™<strong>Ë∂ÖÂπ≥Èù¢</strong>„ÄÇ</li>
<li><strong>Meaning:</strong> This yellow plane represents the
<strong>‚Äúworld of possible predictions‚Äù</strong> that your model is
allowed to make. Any linear combination of your predictors‚Äîwhich is what
a linear regression model calculates‚Äîwill result in a vector that lies
<em>somewhere</em> on this surface.
Ëøô‰∏™ÈªÑËâ≤Âπ≥Èù¢‰ª£Ë°®‰Ω†ÁöÑÊ®°ÂûãÂèØ‰ª•ÂÅöÂá∫ÁöÑ<strong>‚ÄúÂèØËÉΩÈ¢ÑÊµãÁöÑ‰∏ñÁïå‚Äù</strong>„ÄÇ‰ªª‰ΩïÈ¢ÑÊµãÂèòÈáèÁöÑÁ∫øÊÄßÁªÑÂêàÔºà‰πüÂ∞±ÊòØÁ∫øÊÄßÂõûÂΩíÊ®°ÂûãËÆ°ÁÆóÁöÑÁªìÊûúÔºâÈÉΩ‰ºö‰∫ßÁîü‰∏Ä‰∏™‰Ωç‰∫éËøô‰∏™Âπ≥Èù¢<em>ÊüêÂ§Ñ</em>ÁöÑÂêëÈáè„ÄÇ
#### 3. The Actual Outcome Vector (y)ÂÆûÈôÖÁªìÊûúÂêëÈáè (y)</li>
<li><strong>Concept:</strong> The red vector <strong>y</strong>
represents your actual, observed data. It‚Äôs a single point in the
n-dimensional space. Á∫¢Ëâ≤ÂêëÈáè <strong>y</strong>
‰ª£Ë°®‰Ω†ÂÆûÈôÖËßÇÂØüÂà∞ÁöÑÊï∞ÊçÆ„ÄÇÂÆÉÊòØ n Áª¥Á©∫Èó¥‰∏≠ÁöÑ‰∏Ä‰∏™ÁÇπ„ÄÇ</li>
<li><strong>Meaning:</strong> Critically, this vector usually does
<strong>not</strong> lie on the predictor hyperplane. If it did, your
model would be a perfect fit with zero error. The fact that it‚Äôs ‚Äúoff
the plane‚Äù represents the real-world noise and variation that the model
cannot fully capture.
Ëá≥ÂÖ≥ÈáçË¶ÅÁöÑÊòØÔºåËøô‰∏™ÂêëÈáèÈÄöÂ∏∏<strong>‰∏ç</strong>‰Ωç‰∫éÈ¢ÑÊµãÂèòÈáèË∂ÖÂπ≥Èù¢‰∏ä„ÄÇÂ¶ÇÊûúÂÆÉ‰Ωç‰∫éË∂ÖÂπ≥Èù¢‰∏äÔºå‰Ω†ÁöÑÊ®°ÂûãÂ∞ÜÂÆåÁæéÊãüÂêàÔºåËØØÂ∑Æ‰∏∫Èõ∂„ÄÇÂÆÉ‚ÄúÂÅèÁ¶ªÂπ≥Èù¢‚Äù‰ª£Ë°®‰∫ÜÊ®°ÂûãÊó†Ê≥ïÂÆåÂÖ®ÊçïÊçâÂà∞ÁöÑÁúüÂÆû‰∏ñÁïåÁöÑÂô™Â£∞ÂíåÂèòÂåñ„ÄÇ</li>
</ul>
<h4 id="the-fitted-value-vector-≈∑ÊãüÂêàÂÄºÂêëÈáè-≈∑">4. The Fitted Value
Vector (≈∑)ÊãüÂêàÂÄºÂêëÈáè (≈∑)</h4>
<ul>
<li><strong>Concept:</strong> Since <strong>y</strong> is not on the
plane, we find the point on the plane that is <strong>geometrically
closest</strong> to <strong>y</strong>. This closest point is found by
dropping a perpendicular line from <strong>y</strong> to the plane. The
point where it lands is the <strong>orthogonal projection</strong>,
labeled <strong>≈∑</strong> (y-hat). Áî±‰∫é <strong>y</strong>
‰∏çÂú®Âπ≥Èù¢‰∏äÔºåÂõ†Ê≠§Êàë‰ª¨Âú®Âπ≥Èù¢‰∏äÊâæÂà∞‰∏é <strong>y</strong>
<strong>Âá†‰Ωï‰∏äÊúÄÊé•ËøëÁöÑÁÇπ„ÄÇËøô‰∏™ÊúÄÊé•ËøëÁÇπÊòØÈÄöËøá‰ªé </strong>y**
Âà∞Âπ≥Èù¢ÂÅö‰∏ÄÊù°ÂûÇÁõ¥Á∫øÊâæÂà∞ÁöÑ„ÄÇÂûÇÁõ¥Á∫øÊâÄÂú®ÁöÑÁÇπÂ∞±ÊòØ<strong>Ê≠£‰∫§ÊäïÂΩ±</strong>ÔºåÊ†áËÆ∞‰∏∫
<strong>≈∑</strong> (y-hat)„ÄÇ</li>
<li><strong>Meaning:</strong> <strong>≈∑ is the vector of your model‚Äôs
fitted values.</strong> It is the ‚Äúbest‚Äù possible approximation of the
real data that can be created using the given predictors because it
minimizes the distance (and therefore the squared error) between the
actual data (<strong>y</strong>) and the model‚Äôs prediction. <strong>≈∑
ÊòØÊ®°ÂûãÊãüÂêàÂÄºÁöÑÂêëÈáè„ÄÇ</strong>ÂÆÉÊòØ‰ΩøÁî®ÁªôÂÆöÈ¢ÑÊµãÂèòÈáèÂèØ‰ª•ÂàõÂª∫ÁöÑÂØπÁúüÂÆûÊï∞ÊçÆÁöÑ‚ÄúÊúÄ‰Ω≥‚ÄùËøë‰ººÂÄºÔºåÂõ†‰∏∫ÂÆÉÊúÄÂ∞èÂåñ‰∫ÜÂÆûÈôÖÊï∞ÊçÆ
(<strong>y</strong>)
‰∏éÊ®°ÂûãÈ¢ÑÊµãÂÄº‰πãÈó¥ÁöÑË∑ùÁ¶ªÔºà‰ªéËÄåÊúÄÂ∞èÂåñ‰∫ÜÂπ≥ÊñπËØØÂ∑ÆÔºâ„ÄÇ</li>
</ul>
<h4 id="the-residual-vector-the-dashed-lineÊÆãÂ∑ÆÂêëÈáèËôöÁ∫ø">5. The Residual
Vector (The Dashed Line)ÊÆãÂ∑ÆÂêëÈáèÔºàËôöÁ∫øÔºâ</h4>
<ul>
<li><strong>Concept:</strong> The dashed line connecting the tip of
<strong>y</strong> to the tip of <strong>≈∑</strong> is the
<strong>residual vector</strong> (<span
class="math inline">\(\boldsymbol{\epsilon} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>). Its length is the shortest possible distance
from <strong>y</strong> to the hyperplane.
ËøûÊé•<strong>y</strong>È°∂ÁÇπÂíå<strong>≈∑</strong>È°∂ÁÇπÁöÑËôöÁ∫øÊòØ<strong>ÊÆãÂ∑ÆÂêëÈáè</strong>
(<span class="math inline">\(\boldsymbol{\epsilon} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>)„ÄÇÂÖ∂ÈïøÂ∫¶ÊòØ‰ªé<strong>y</strong>Âà∞Ë∂ÖÂπ≥Èù¢ÁöÑÊúÄÁü≠ÂèØËÉΩË∑ùÁ¶ª„ÄÇ</li>
<li><strong>Meaning:</strong> This vector represents the
<strong>error</strong> of the model‚Äîthe part of the actual data that is
left over after accounting for the predictors. The right-angle symbol
(‚îî) is the most important part of the diagram, as it shows this error is
<strong>orthogonal</strong> (perpendicular) to the prediction and to all
the predictors. This visualizes the core property that the model‚Äôs
errors are uncorrelated with the predictors.
ËøûÊé•<strong>y</strong>È°∂ÁÇπÂíå<strong>≈∑</strong>È°∂ÁÇπÁöÑËôöÁ∫øÊòØ<strong>ÊÆãÂ∑ÆÂêëÈáè</strong>
(<span class="math inline">\(\boldsymbol{\epsilon} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>)„ÄÇÂÖ∂ÈïøÂ∫¶ÊòØ‰ªé<strong>y</strong>Âà∞Ë∂ÖÂπ≥Èù¢ÁöÑÊúÄÁü≠ÂèØËÉΩË∑ùÁ¶ª„ÄÇ</li>
</ul>
<h1 id="singular-value-decomposition-svd-Â•áÂºÇÂÄºÂàÜËß£-svd">9.Singular
Value Decomposition (SVD) Â•áÂºÇÂÄºÂàÜËß£ (SVD)</h1>
<p><img src="/imgs/5054C3/SVD1.png">
<img src="/imgs/5054C3/SVD2.png"></p>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>:</li>
</ul>
<p>These slides delve into the more advanced linear algebra behind the
projection matrix (<strong>H</strong>), explaining its fundamental
properties and offering a new way to construct it using <strong>Singular
Value Decomposition (SVD)</strong>. Êé¢ËÆ®‰∫ÜÊäïÂΩ±Áü©Èòµ (<strong>H</strong>)
ËÉåÂêéÊõ¥È´òÁ∫ßÁöÑÁ∫øÊÄß‰ª£Êï∞ÔºåËß£Èáä‰∫ÜÂÆÉÁöÑÂü∫Êú¨ÊÄßË¥®ÔºåÂπ∂Êèê‰æõ‰∫Ü‰∏ÄÁßç‰ΩøÁî®<strong>Â•áÂºÇÂÄºÂàÜËß£
(SVD)</strong> ÊûÑÈÄ†ÂÆÉÁöÑÊñ∞ÊñπÊ≥ï„ÄÇ</p>
<h3 id="summary-2">## Summary</h3>
<p>These slides show that the <strong>projection matrix (H)</strong>,
which is central to least squares, has two key mathematical properties:
it‚Äôs <strong>symmetric</strong> and <strong>idempotent</strong>
(projecting twice is the same as projecting once). These properties
dictate that its eigenvalues must be either 1 or 0. Singular Value
Decomposition (SVD) of the data matrix <strong>X</strong> provides an
elegant and numerically stable way to express <strong>H</strong> as
<strong>UU·µÄ</strong>, which makes these fundamental properties easier to
understand and prove. Ëøô‰∫õÂπªÁÅØÁâáÂ±ïÁ§∫‰∫Ü<strong>ÊäïÂΩ±Áü©Èòµ
(H)</strong>ÔºàÊúÄÂ∞è‰∫å‰πòÊ≥ïÁöÑÊ†∏ÂøÉÔºâÁöÑ‰∏§‰∏™ÂÖ≥ÈîÆÊï∞Â≠¶ÊÄßË¥®Ôºö<strong>ÂØπÁß∞</strong>Âíå<strong>ÂπÇÁ≠â</strong>ÔºàÊäïÂΩ±‰∏§Ê¨°Á≠â‰∫éÊäïÂΩ±‰∏ÄÊ¨°Ôºâ„ÄÇËøô‰∫õÊÄßË¥®ÂÜ≥ÂÆö‰∫ÜÂÆÉÁöÑÁâπÂæÅÂÄºÂøÖÈ°ª‰∏∫
1 Êàñ 0„ÄÇÊï∞ÊçÆÁü©Èòµ <strong>X</strong> ÁöÑÂ•áÂºÇÂÄºÂàÜËß£ (SVD)
Êèê‰æõ‰∫Ü‰∏ÄÁßç‰ºòÈõÖ‰∏îÊï∞ÂÄºÁ®≥ÂÆöÁöÑÊñπÂºèÔºåÂ∞Ü<strong>H</strong> Ë°®Á§∫‰∏∫
<strong>UU·µÄ</strong>ÔºåËøô‰ΩøÂæóËøô‰∫õÂü∫Êú¨ÊÄßË¥®Êõ¥ÂÆπÊòìÁêÜËß£ÂíåËØÅÊòé„ÄÇ</p>
<h3 id="concepts-and-process-explained-in-detail-1">## Concepts and
Process Explained in Detail</h3>
<h4 id="singular-value-decomposition-svd">1. Singular Value
Decomposition (SVD)</h4>
<p>The first slide introduces SVD, a powerful method for factoring any
matrix.</p>
<ul>
<li><strong>Concept:</strong> SVD breaks down your data matrix
<strong>X</strong> into three simpler matrices: <strong>X =
UDV·µÄ</strong>. Think of this as revealing the fundamental structure of
your data.SVD Â∞ÜÊï∞ÊçÆÁü©Èòµ <strong>X</strong>
ÂàÜËß£‰∏∫‰∏â‰∏™Êõ¥ÁÆÄÂçïÁöÑÁü©ÈòµÔºö<strong>X =
UDV·µÄ</strong>„ÄÇËøôÂèØ‰ª•ÁêÜËß£‰∏∫Êè≠Á§∫Êï∞ÊçÆÁöÑÂü∫Êú¨ÁªìÊûÑ„ÄÇ
<ul>
<li><strong>U:</strong> An <strong>orthogonal matrix</strong> whose
columns form a perfect, orthonormal basis for the space spanned by your
predictors (the column space of <strong>X</strong>). These columns
represent the principal directions of your data‚Äôs
space.‰∏Ä‰∏™<strong>Ê≠£‰∫§Áü©Èòµ</strong>ÔºåÂÖ∂ÂàóÊûÑÊàêÈ¢ÑÊµãÂèòÈáèÊâÄÂç†Á©∫Èó¥Ôºà<strong>X</strong>
ÁöÑÂàóÁ©∫Èó¥ÔºâÁöÑÂÆåÁæéÊ≠£‰∫§Âü∫„ÄÇËøô‰∫õÂàó‰ª£Ë°®Êï∞ÊçÆÁ©∫Èó¥ÁöÑ‰∏ªÊñπÂêë„ÄÇ</li>
<li><strong>D:</strong> A <strong>diagonal matrix</strong> containing
the ‚Äúsingular values,‚Äù which measure the importance or magnitude of each
of these principal
directions.‰∏Ä‰∏™<strong>ÂØπËßíÁü©Èòµ</strong>ÔºåÂåÖÂê´‚ÄúÂ•áÂºÇÂÄº‚ÄùÔºåÁî®‰∫éË°°ÈáèÊØè‰∏™‰∏ªÊñπÂêëÁöÑÈáçË¶ÅÊÄßÊàñÂ§ßÂ∞è„ÄÇ</li>
<li><strong>V:</strong> Another <strong>orthogonal
matrix</strong>.Âè¶‰∏Ä‰∏™<strong>Ê≠£‰∫§Áü©Èòµ</strong>„ÄÇ</li>
</ul></li>
<li><strong>Process (How SVD simplifies the Projection Matrix) SVD
Â¶Ç‰ΩïÁÆÄÂåñÊäïÂΩ±Áü©Èòµ:</strong> The main takeaway from this slide is the new,
simpler formula for the hat matrix: <span
class="math display">\[\mathbf{H} = \mathbf{UU}^T\]</span> This result
is derived by substituting <strong>X = UDV·µÄ</strong> into the original,
more complex formula for <strong>H</strong>: <span
class="math display">\[\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\]</span> When you
perform this substitution and use the fact that for orthogonal matrices
<strong>U</strong> and <strong>V</strong>, we have <strong>U·µÄU =
I</strong> and <strong>V·µÄV = I</strong>, the <strong>D</strong> and
<strong>V</strong> matrices completely cancel out, leaving the
beautifully simple form <strong>H = UU·µÄ</strong>. This tells us that
projection is fundamentally about the basis vectors (<strong>U</strong>)
of the predictor space. ÊâßË°åÊ≠§‰ª£ÂÖ•Âπ∂Âà©Áî®Ê≠£‰∫§Áü©Èòµ <strong>U</strong> Âíå
<strong>V</strong> ÁöÑÂÖ¨ÂºèÔºåÂç≥ <strong>U·µÄU = I</strong> Âíå <strong>V·µÄV =
I</strong>Ôºå<strong>D</strong> Âíå <strong>V</strong>
Áü©ÈòµÂÆåÂÖ®ÊäµÊ∂àÔºåÂâ©‰∏ãÁÆÄÊ¥ÅÁöÑÂΩ¢Âºè <strong>H =
UU·µÄ</strong>„ÄÇËøôÂëäËØâÊàë‰ª¨ÔºåÊäïÂΩ±Êú¨Ë¥®‰∏äÊòØÂÖ≥‰∫éÈ¢ÑÊµãÁ©∫Èó¥ÁöÑÂü∫ÂêëÈáèÔºà<strong>U</strong>ÔºâÁöÑ„ÄÇ</li>
</ul>
<h4 id="the-properties-of-the-projection-matrix-h-ÊäïÂΩ±Áü©Èòµ-h-ÁöÑÊÄßË¥®">2.
The Properties of the Projection Matrix (H) ÊäïÂΩ±Áü©Èòµ (H) ÁöÑÊÄßË¥®</h4>
<p>The second slide describes the essential nature of any projection
matrix.</p>
<ul>
<li><p><strong>Symmetric (H = H·µÄ):</strong> This property ensures that
the projection is orthogonal (i.e., it finds the closest point by moving
perpendicularly).
Ê≠§ÊÄßË¥®Á°Æ‰øùÊäïÂΩ±ÊòØÊ≠£‰∫§ÁöÑÔºàÂç≥ÔºåÂÆÉÈÄöËøáÂûÇÁõ¥ÁßªÂä®ÊâæÂà∞ÊúÄËøëÁöÑÁÇπÔºâ„ÄÇ</p></li>
<li><p><strong>Idempotent (H¬≤ = H):</strong> This is the most intuitive
property of a projection. ËøôÊòØÊäïÂΩ±ÊúÄÁõ¥ËßÇÁöÑÊÄßË¥®„ÄÇ</p>
<ul>
<li><strong>Concept:</strong> ‚ÄúDoing it twice is the same as doing it
once.‚Äù ‚Äú‰∏§Ê¨°Âíå‰∏ÄÊ¨°Áõ∏Âêå„ÄÇ‚Äù</li>
<li><strong>Geometric Meaning:</strong> Imagine you project a point onto
a flat tabletop. That projected point is now <em>on the table</em>. If
you try to project it onto the table <em>again</em>, it doesn‚Äôt move.
The projection of a projection is just the projection itself.
Mathematically, this is <strong>H(Hv) = Hv</strong>, which simplifies to
<strong>H¬≤ = H</strong>.
ÊÉ≥Ë±°‰∏Ä‰∏ãÔºå‰Ω†Â∞Ü‰∏Ä‰∏™ÁÇπÊäïÂΩ±Âà∞Âπ≥Âù¶ÁöÑÊ°åÈù¢‰∏ä„ÄÇËøô‰∏™ÊäïÂΩ±ÁÇπÁé∞Âú®<em>Âú®Ê°åÂ≠ê‰∏ä</em>„ÄÇÂ¶ÇÊûú‰Ω†Â∞ùËØï<em>ÂÜçÊ¨°</em>Â∞ÜÂÆÉÊäïÂΩ±Âà∞Ê°åÂ≠ê‰∏äÔºåÂÆÉ‰∏ç‰ºöÁßªÂä®„ÄÇÊäïÂΩ±ÁöÑÊäïÂΩ±Â∞±ÊòØÊäïÂΩ±Êú¨Ë∫´„ÄÇ‰ªéÊï∞Â≠¶‰∏äËÆ≤ÔºåËøôÊòØ<strong>H(Hv)
= Hv</strong>ÔºåÁÆÄÂåñ‰∏∫<strong>H¬≤ = H</strong>„ÄÇ</li>
</ul></li>
</ul>
<h4 id="eigenvalues-and-eigenspaces-ÁâπÂæÅÂÄºÂíåÁâπÂæÅÁ©∫Èó¥">3. Eigenvalues and
Eigenspaces ÁâπÂæÅÂÄºÂíåÁâπÂæÅÁ©∫Èó¥</h4>
<p>The idempotency property has a profound consequence for the matrix‚Äôs
eigenvalues.</p>
<ul>
<li><strong>Concept:</strong> The eigenvalues of <strong>H</strong> can
only be <strong>1 or 0</strong>.
<strong>H</strong>ÁöÑÁâπÂæÅÂÄºÂè™ËÉΩÊòØ<strong>1</strong>Êàñ0**„ÄÇ</li>
<li><strong>Process (The Proof):</strong>
<ol type="1">
<li>Let <strong>v</strong> be an eigenvector of <strong>H</strong> with
eigenvalue <span class="math inline">\(\lambda\)</span>. By definition,
<strong>Hv</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>.
ËÆæ<strong>v</strong>ÊòØ<strong>H</strong>ÁöÑÁâπÂæÅÂêëÈáèÔºåÂÖ∂ÁâπÂæÅÂÄº‰∏∫<span
class="math inline">\(\lambda\)</span>„ÄÇÊ†πÊçÆÂÆö‰πâÔºå<strong>Hv</strong> =
<span class="math inline">\(\lambda\)</span><strong>v</strong>„ÄÇ</li>
<li>If we apply <strong>H</strong> again, we get <strong>H(Hv)</strong>
= <strong>H</strong>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda\)</span>(<strong>Hv</strong>) = <span
class="math inline">\(\lambda\)</span>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>.
Â¶ÇÊûúÊàë‰ª¨ÂÜçÊ¨°Â∫îÁî®<strong>H</strong>ÔºåÊàë‰ª¨ÂæóÂà∞<strong>H(Hv)</strong> =
<strong>H</strong>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda\)</span>(<strong>Hv</strong>) = <span
class="math inline">\(\lambda\)</span>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>„ÄÇ</li>
<li>So, we have <strong>H¬≤v</strong> = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>.
Âõ†Ê≠§ÔºåÊàë‰ª¨Êúâ<strong>H¬≤v</strong> = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>„ÄÇ</li>
<li>But since <strong>H</strong> is idempotent, <strong>H¬≤ = H</strong>,
which means <strong>H¬≤v = Hv</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>.
‰ΩÜÁî±‰∫é<strong>H</strong>ÊòØÂπÇÁ≠âÁöÑÔºå<strong>H¬≤ =
H</strong>ÔºåËøôÊÑèÂë≥ÁùÄ<strong>H¬≤v = Hv</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>„ÄÇ</li>
<li>Therefore, we must have <span
class="math inline">\(\lambda^2\)</span><strong>v</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>, which means
<span class="math inline">\(\lambda^2 = \lambda\)</span>. The only two
numbers in existence that satisfy this equation are <strong>0 and
1</strong>. Âõ†Ê≠§ÔºåÊàë‰ª¨ÂøÖÈ°ªÊúâ<span
class="math inline">\(\lambda^2\)</span><strong>v</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>ÔºåËøôÊÑèÂë≥ÁùÄ<span
class="math inline">\(\lambda^2 =
\lambda\)</span>„ÄÇÊª°Ë∂≥Ê≠§Á≠âÂºèÁöÑ‰ªÖÊúâ‰∏§‰∏™Êï∞Â≠óÊòØ<strong>0</strong>Âíå<strong>1</strong>„ÄÇ</li>
</ol></li>
<li><strong>Connecting Eigenvalues to the Model
Â∞ÜÁâπÂæÅÂÄºËøûÊé•Âà∞Ê®°Âûã:</strong>
<ul>
<li><p><strong>Eigenvalue = 1:</strong> The eigenvectors associated with
an eigenvalue of 1 are the vectors that <strong>do not change</strong>
when projected. This is only possible if they were already in the space
being projected onto. Therefore, the space <code>L‚ÇÅ</code> is the
<strong>column space of X</strong>‚Äîthe ‚Äúmodel space.‚Äù <strong>H</strong>
is the projection onto this space. <strong>‰∏éÁâπÂæÅÂÄº‰∏∫ 1
Áõ∏ÂÖ≥ËÅîÁöÑÁâπÂæÅÂêëÈáèÊòØÊäïÂΩ±Êó∂</strong>‰∏ç‰ºöÊîπÂèò<strong>ÁöÑÂêëÈáè„ÄÇÂè™ÊúâÂΩìÂÆÉ‰ª¨Â∑≤ÁªèÂ≠òÂú®‰∫éÊäïÂΩ±Âà∞ÁöÑÁ©∫Èó¥‰∏≠Êó∂ÔºåËøôÁßçÊÉÖÂÜµÊâçÊúâÂèØËÉΩÂèëÁîü„ÄÇÂõ†Ê≠§ÔºåÁ©∫Èó¥
<code>L‚ÇÅ</code> ÊòØ X ÁöÑ</strong>ÂàóÁ©∫Èó¥<strong>‚Äî‚Äî‚ÄúÊ®°ÂûãÁ©∫Èó¥‚Äù„ÄÇ</strong>H**
ÊòØÂà∞ËØ•Á©∫Èó¥ÁöÑÊäïÂΩ±„ÄÇ</p></li>
<li><p><strong>Eigenvalue = 0:</strong> The eigenvectors associated with
an eigenvalue of 0 are the vectors that get sent to the zero vector when
projected. This happens to vectors that are <strong>orthogonal</strong>
to the projection space. Therefore, the space <code>L‚ÇÄ</code> is the
<strong>orthogonal ‚Äúerror‚Äù space</strong>. The matrix <strong>I -
H</strong> is the projection onto this space.</p></li>
</ul>
<strong>‰∏éÁâπÂæÅÂÄº‰∏∫ 0
Áõ∏ÂÖ≥ËÅîÁöÑÁâπÂæÅÂêëÈáèÊòØÊäïÂΩ±Êó∂Ë¢´ÂèëÈÄÅÂà∞Èõ∂ÂêëÈáèÁöÑÂêëÈáè„ÄÇËøôÁßçÊÉÖÂÜµÂèëÁîüÂú®‰∏éÊäïÂΩ±Á©∫Èó¥</strong>Ê≠£‰∫§<strong>ÁöÑÂêëÈáè‰∏ä„ÄÇÂõ†Ê≠§ÔºåÁ©∫Èó¥
<code>L‚ÇÄ</code> ÊòØ</strong>Ê≠£‰∫§‚ÄúËØØÂ∑Æ‚ÄùÁ©∫Èó¥<strong>„ÄÇÁü©Èòµ </strong>I - H**
ÊòØÂà∞ËØ•Á©∫Èó¥ÁöÑÊäïÂΩ±„ÄÇ</li>
</ul>
<h1 id="statistical-inference-1">10.statistical inference</h1>
<p><img src="/imgs/5054C3/statistical_inference_in_linear_regression1.png">
<img src="/imgs/5054C3/statistical_inference_in_linear_regression2.png"></p>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>:</li>
</ul>
<p>These slides cover the theoretical backbone of statistical inference
in linear regression. They explain the necessary assumptions and the
resulting probability distributions of our estimates, which is what
allows us to perform hypothesis tests and create confidence
intervals.</p>
<p>Ëøô‰∫õÂπªÁÅØÁâáÊ∂µÁõñ‰∫ÜÁ∫øÊÄßÂõûÂΩí‰∏≠ÁªüËÆ°Êé®Êñ≠ÁöÑÁêÜËÆ∫Âü∫Á°Ä„ÄÇÂÆÉ‰ª¨Ëß£Èáä‰∫ÜÂøÖË¶ÅÁöÑÂÅáËÆæ‰ª•ÂèäÁî±Ê≠§ÂæóÂá∫ÁöÑ‰º∞ËÆ°Ê¶ÇÁéáÂàÜÂ∏ÉÔºåËøô‰ΩøÊàë‰ª¨ËÉΩÂ§üËøõË°åÂÅáËÆæÊ£ÄÈ™åÂπ∂ÂàõÂª∫ÁΩÆ‰ø°Âå∫Èó¥„ÄÇ</p>
<h3 id="summary-3">## Summary</h3>
<p>These slides lay out the statistical assumptions required for the
Least Squares Estimator (LSE). The core idea is that if we assume the
errors are independent and normally distributed, we can then prove that:
Ëøô‰∫õÂπªÁÅØÁâáÂàóÂá∫‰∫ÜÊúÄÂ∞è‰∫å‰πò‰º∞ËÆ°Èáè (LSE)
ÊâÄÈúÄÁöÑÁªüËÆ°ÂÅáËÆæ„ÄÇÂÖ∂Ê†∏ÂøÉÊÄùÊÉ≥ÊòØÔºåÂ¶ÇÊûúÊàë‰ª¨ÂÅáËÆæËØØÂ∑ÆÊòØÁã¨Á´ãÁöÑ‰∏îÊúç‰ªéÊ≠£ÊÄÅÂàÜÂ∏ÉÔºåÈÇ£‰πàÊàë‰ª¨ÂèØ‰ª•ËØÅÊòéÔºö</p>
<ol type="1">
<li><p>Our estimated coefficients (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) also follow a
<strong>Normal distribution</strong> (or a
<strong>t-distribution</strong> when standardized). Êàë‰ª¨ÁöÑ‰º∞ËÆ°Á≥ªÊï∞
(<span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>)
‰πüÊúç‰ªé<strong>Ê≠£ÊÄÅÂàÜÂ∏É</strong>ÔºàÊ†áÂáÜÂåñÂêéÊúç‰ªé<strong>t
ÂàÜÂ∏É</strong>Ôºâ„ÄÇ</p></li>
<li><p>Our summed-up squared errors (RSS) follow a <strong>Chi-squared
distribution</strong>. Êàë‰ª¨ÁöÑÂπ≥ÊñπËØØÂ∑ÆÊÄªÂíå (RSS)
Êúç‰ªé<strong>Âç°ÊñπÂàÜÂ∏É</strong>„ÄÇ</p></li>
<li><p>A specific ratio of the explained variance to the unexplained
variance follows an <strong>F-distribution</strong>, which is used to
test the overall significance of the model.
Ëß£ÈáäÊñπÂ∑Æ‰∏éÊú™Ëß£ÈáäÊñπÂ∑ÆÁöÑÁâπÂÆöÊØîÁéáÊúç‰ªé<strong>F
ÂàÜÂ∏É</strong>ÔºåËØ•ÂàÜÂ∏ÉÁî®‰∫éÊ£ÄÈ™åÊ®°ÂûãÁöÑÊï¥‰ΩìÊòæËëóÊÄß„ÄÇ</p></li>
</ol>
<p>These known distributions are the foundation for all statistical
inference in linear
models.Ëøô‰∫õÂ∑≤Áü•ÁöÑÂàÜÂ∏ÉÊòØÁ∫øÊÄßÊ®°Âûã‰∏≠ÊâÄÊúâÁªüËÆ°Êé®Êñ≠ÁöÑÂü∫Á°Ä„ÄÇ</p>
<h3 id="deeper-dive-into-concepts-and-processes">## Deeper Dive into
Concepts and Processes</h3>
<h4 id="the-model-assumptions-the-foundation-Ê®°ÂûãÂÅáËÆæÂü∫Á°Ä">1. The Model
Assumptions (The Foundation) Ê®°ÂûãÂÅáËÆæÔºàÂü∫Á°ÄÔºâ</h4>
<p>The first slide states the two assumptions that are critical for
everything that follows. Without them, we can‚Äôt make claims about the
statistical properties of our estimates.
Á¨¨‰∏ÄÂº†ÂπªÁÅØÁâáÈòêËø∞‰∫ÜÂØπÂêéÁª≠ÊâÄÊúâÂÜÖÂÆπÈÉΩËá≥ÂÖ≥ÈáçË¶ÅÁöÑ‰∏§‰∏™ÂÅáËÆæ„ÄÇÊ≤°ÊúâÂÆÉ‰ª¨ÔºåÊàë‰ª¨Â∞±Êó†Ê≥ïÊñ≠Ë®Ä‰º∞ËÆ°ÂÄºÁöÑÁªüËÆ°ÁâπÊÄß„ÄÇ</p>
<ul>
<li><strong>Assumption 1: <span class="math inline">\(\epsilon_i \sim
N(0, \sigma^2)\)</span></strong>
<ul>
<li><strong>Concept:</strong> This assumes that the error terms (the
part of <code>y</code> that the model can‚Äôt explain) are drawn from a
normal (bell-curve) distribution with a mean of zero and a constant
variance <span class="math inline">\(\sigma^2\)</span>.
**ÂÅáËÆæËØØÂ∑ÆÈ°πÔºàÊ®°ÂûãÊó†Ê≥ïËß£ÈáäÁöÑ y
ÂÄºÈÉ®ÂàÜÔºâÊúç‰ªéÊ≠£ÊÄÅÔºàÈíüÂΩ¢Êõ≤Á∫øÔºâÂàÜÂ∏ÉÔºåËØ•ÂàÜÂ∏ÉÁöÑÂùáÂÄº‰∏∫Èõ∂ÔºåÊñπÂ∑Æ‰∏∫Â∏∏Êï∞ <span
class="math inline">\(\sigma^2\)</span>„ÄÇ</li>
<li><strong>Meaning in Plain English:</strong>
<ul>
<li><strong>Mean of 0:</strong> The model is ‚Äúcorrect on average.‚Äù The
errors are not systematically positive or negative.
**Ê®°Âûã‚ÄúÂπ≥ÂùáÊ≠£Á°Æ‚Äù„ÄÇËØØÂ∑ÆÂπ∂ÈùûÁ≥ªÁªüÂú∞‰∏∫Ê≠£ÊàñË¥ü„ÄÇ</li>
<li><strong>Normal Distribution:</strong> Small errors are more likely
than large errors. This is a common assumption for random noise.
**Â∞èËØØÂ∑ÆÊØîÂ§ßËØØÂ∑ÆÊõ¥ÊúâÂèØËÉΩÂá∫Áé∞„ÄÇËøôÊòØÈöèÊú∫Âô™Â£∞ÁöÑÂ∏∏ËßÅÂÅáËÆæ„ÄÇ</li>
<li><strong>Constant Variance (<span
class="math inline">\(\sigma^2\)</span>):</strong> The amount of random
scatter around the regression line is the same at all levels of the
predictor variables. This is called <strong>homoscedasticity</strong>.
ÂõûÂΩíÁ∫øÂë®Âõ¥ÁöÑÈöèÊú∫Êï£Â∫¶Âú®È¢ÑÊµãÂèòÈáèÁöÑÂêÑ‰∏™Ê∞¥Âπ≥‰∏äÈÉΩÊòØÁõ∏ÂêåÁöÑ„ÄÇËøôË¢´Áß∞‰∏∫<strong>ÂêåÊñπÂ∑ÆÊÄß</strong>„ÄÇ</li>
</ul></li>
</ul></li>
<li><strong>Assumption 2: Observations are independent</strong>
ËßÇÊµãÂÄºÊòØÁã¨Á´ãÁöÑ**
<ul>
<li><strong>Concept:</strong> Each data point <span
class="math inline">\((x_i, y_i)\)</span> is an independent piece of
information. The value of the error for one observation gives no
information about the error for another observation. ÊØè‰∏™Êï∞ÊçÆÁÇπ <span
class="math inline">\((x_i, y_i)\)</span>
ÈÉΩÊòØ‰∏ÄÊù°Áã¨Á´ãÁöÑ‰ø°ÊÅØ„ÄÇ‰∏Ä‰∏™ËßÇÊµãÂÄºÁöÑËØØÂ∑ÆÂÄºÂπ∂‰∏çËÉΩÂèçÊò†Âè¶‰∏Ä‰∏™ËßÇÊµãÂÄºÁöÑËØØÂ∑Æ„ÄÇ</li>
<li><strong>Meaning:</strong> This is often true for cross-sectional
data (e.g., a random sample of people) but can be violated in
time-series data where today‚Äôs error might be correlated with
yesterday‚Äôs.
ËøôÈÄöÂ∏∏ÈÄÇÁî®‰∫éÊ®™Êà™Èù¢Êï∞ÊçÆÔºà‰æãÂ¶ÇÔºåÈöèÊú∫ÊäΩÊ†∑ÁöÑ‰∫∫Áæ§ÔºâÔºå‰ΩÜÂú®Êó∂Èó¥Â∫èÂàóÊï∞ÊçÆ‰∏≠ÂèØËÉΩ‰∏çÊàêÁ´ãÔºåÂõ†‰∏∫‰ªäÂ§©ÁöÑËØØÂ∑ÆÂèØËÉΩ‰∏éÊò®Â§©ÁöÑËØØÂ∑ÆÁõ∏ÂÖ≥„ÄÇ</li>
</ul></li>
</ul>
<h4
id="the-distribution-of-the-coefficients-theorem-of-lse-Á≥ªÊï∞ÂàÜÂ∏ÉÊúÄÂ∞è‰∫å‰πòÊ≥ïÂÆöÁêÜ">2.
The Distribution of the Coefficients (Theorem of LSE)
Á≥ªÊï∞ÂàÜÂ∏ÉÔºàÊúÄÂ∞è‰∫å‰πòÊ≥ïÂÆöÁêÜÔºâ</h4>
<p>This is the most important result for understanding the accuracy of
our individual predictors.</p>
<ul>
<li><strong>Concept 1: The Sampling Distribution of <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></strong>
<ul>
<li><p><strong>Formula:</strong> <span
class="math inline">\(\hat{\boldsymbol{\beta}} \sim
N(\boldsymbol{\beta},
\sigma^2(\mathbf{X}^T\mathbf{X})^{-1})\)</span></p></li>
<li><p><strong>Meaning:</strong> If you were to take many different
random samples from the population and calculate the coefficients <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> for each sample,
the distribution of those coefficients would be a multivariate normal
distribution. **Â¶ÇÊûú‰ªéÊÄª‰Ωì‰∏≠ÈöèÊú∫ÊäΩÂèñËÆ∏Â§ö‰∏çÂêåÁöÑÊ†∑Êú¨ÔºåÂπ∂ËÆ°ÁÆóÊØè‰∏™Ê†∑Êú¨ÁöÑÁ≥ªÊï∞
<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>ÔºåÂàôËøô‰∫õÁ≥ªÊï∞ÁöÑÂàÜÂ∏ÉÂ∞ÜÊúç‰ªéÂ§öÂÖÉÊ≠£ÊÄÅÂàÜÂ∏É„ÄÇ</p>
<ul>
<li>The center of this distribution is the <strong>true population
coefficient vector <span
class="math inline">\(\boldsymbol{\beta}\)</span></strong>. This means
our estimator is <strong>unbiased</strong>‚Äîon average, it finds the
right answer. ËØ•ÂàÜÂ∏ÉÁöÑ‰∏≠ÂøÉÊòØ<strong>ÁúüÂÆûÁöÑÊÄª‰ΩìÁ≥ªÊï∞ÂêëÈáè <span
class="math inline">\(\boldsymbol{\beta}\)</span></strong>„ÄÇËøôÊÑèÂë≥ÁùÄÊàë‰ª¨ÁöÑ‰º∞ËÆ°Âô®ÊòØ<strong>Êó†ÂÅèÁöÑ</strong>‚Äî‚ÄîÂπ≥ÂùáËÄåË®ÄÔºåÂÆÉËÉΩÂ§üÊâæÂà∞Ê≠£Á°ÆÁöÑÁ≠îÊ°à„ÄÇ</li>
<li>The ‚Äúspread‚Äù of this distribution is its variance-covariance matrix,
<span
class="math inline">\(\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\)</span>.
This tells us the uncertainty in our estimates.
ËØ•ÂàÜÂ∏ÉÁöÑ‚ÄúÊï£Â∫¶‚ÄùÊòØÂÖ∂ÊñπÂ∑Æ-ÂçèÊñπÂ∑ÆÁü©Èòµ</li>
</ul></li>
</ul></li>
<li><strong>Concept 2: The t-statistic</strong> t ÁªüËÆ°Èáè
<ul>
<li><strong>Formula:</strong> The standardized coefficient, <span
class="math inline">\(\frac{\hat{\beta}_j -
\beta_j}{\text{s.e.}(\hat{\beta}_j)}\)</span>, follows a
<strong>t-distribution</strong> with <strong><span
class="math inline">\(n-p-1\)</span> degrees of freedom</strong>.</li>
<li><strong>Process &amp; Meaning:</strong> In the real world, we don‚Äôt
know the true error variance <span
class="math inline">\(\sigma^2\)</span>. We have to estimate it using
our sample data, which gives us <span
class="math inline">\(s^2\)</span>. Because we are using an
<em>estimate</em> of the variance, we introduce extra uncertainty. The
t-distribution is like a normal distribution but with slightly ‚Äúfatter‚Äù
tails to account for this additional uncertainty. The degrees of
freedom, <span class="math inline">\(n-p-1\)</span>, reflect the number
of data points (<code>n</code>) minus the number of parameters we had to
estimate (<code>p</code> slopes + 1 intercept). This is the basis for
t-tests and confidence intervals for each coefficient.
Âú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÔºåÊàë‰ª¨‰∏çÁü•ÈÅìÁúüÂÆûÁöÑËØØÂ∑ÆÊñπÂ∑Æ <span
class="math inline">\(\sigma^2\)</span>„ÄÇÊàë‰ª¨ÂøÖÈ°ª‰ΩøÁî®Ê†∑Êú¨Êï∞ÊçÆÊù•‰º∞ËÆ°ÂÆÉÔºå‰ªéËÄåÂæóÂà∞
<span
class="math inline">\(s^2\)</span>„ÄÇÁî±‰∫éÊàë‰ª¨‰ΩøÁî®ÁöÑÊòØÊñπÂ∑ÆÁöÑ<em>‰º∞ËÆ°ÂÄº</em>ÔºåÂõ†Ê≠§ÂºïÂÖ•‰∫ÜÈ¢ùÂ§ñÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇ
t ÂàÜÂ∏ÉÁ±ª‰ºº‰∫éÊ≠£ÊÄÅÂàÜÂ∏ÉÔºå‰ΩÜÂ∞æÈÉ®Áï•ÂæÆ‚Äú‰∏∞Êª°‚ÄùÔºå‰ª•Ëß£ÈáäËøôÁßçÈ¢ùÂ§ñÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇËá™Áî±Â∫¶
<span class="math inline">\(n-p-1\)</span>
Ë°®Á§∫Êï∞ÊçÆÁÇπÁöÑÊï∞ÈáèÔºà<code>n</code>ÔºâÂáèÂéªÊàë‰ª¨ÈúÄË¶Å‰º∞ËÆ°ÁöÑÂèÇÊï∞Êï∞ÈáèÔºà<code>p</code>
‰∏™ÊñúÁéá + 1 ‰∏™Êà™Ë∑ùÔºâ„ÄÇËøôÊòØ t Ê£ÄÈ™åÂíåÊØè‰∏™Á≥ªÊï∞ÁΩÆ‰ø°Âå∫Èó¥ÁöÑÂü∫Á°Ä„ÄÇ</li>
</ul></li>
</ul>
<h4
id="the-distribution-of-the-error-theorem-of-residual-ËØØÂ∑ÆÂàÜÂ∏ÉÊÆãÂ∑ÆÂÆöÁêÜ">3.
The Distribution of the Error (Theorem of Residual)
ËØØÂ∑ÆÂàÜÂ∏ÉÔºàÊÆãÂ∑ÆÂÆöÁêÜÔºâ</h4>
<p>This theorem helps us understand the properties of our model‚Äôs
overall error.</p>
<ul>
<li><p><strong>Concept:</strong> The <strong>Residual Sum of Squares
(RSS)</strong>, when scaled by the true variance, follows a
<strong>Chi-squared (<span class="math inline">\(\chi^2\)</span>)
distribution</strong> with <span class="math inline">\(n-p-1\)</span>
degrees of freedom. <strong>ÊÆãÂ∑ÆÂπ≥ÊñπÂíå (RSS)</strong>
ÁªèÁúüÂÆûÊñπÂ∑ÆÁº©ÊîæÂêéÔºåÊúç‰ªéËá™Áî±Â∫¶‰∏∫ <span
class="math inline">\(n-p-1\)</span> ÁöÑ<strong>Âç°Êñπ (<span
class="math inline">\(\chi^2\)</span>) ÂàÜÂ∏É</strong>„ÄÇ</p></li>
<li><p><strong>Process &amp; Meaning:</strong> The Chi-squared
distribution often arises when dealing with sums of squared normal
variables. This theorem provides a formal probability distribution for
our total model error. Its most important consequence is that it allows
us to prove that:
**Âç°ÊñπÂàÜÂ∏ÉÈÄöÂ∏∏Áî®‰∫éÂ§ÑÁêÜÊ≠£ÊÄÅÂèòÈáèÁöÑÂπ≥ÊñπÂíå„ÄÇËØ•ÂÆöÁêÜ‰∏∫Êàë‰ª¨Ê®°ÂûãÁöÑÊÄª‰ΩìËØØÂ∑ÆÊèê‰æõ‰∫Ü‰∏Ä‰∏™Ê≠£ÂºèÁöÑÊ¶ÇÁéáÂàÜÂ∏É„ÄÇÂÆÉÊúÄÈáçË¶ÅÁöÑÊé®ËÆ∫ÊòØÔºåÂÆÉ‰ΩøÊàë‰ª¨ËÉΩÂ§üËØÅÊòéÔºö</p>
<p><span class="math display">\[s^2 = \text{RSS}/(n - p - 1)\]</span> is
an <strong>unbiased estimate</strong> of the true error variance <span
class="math inline">\(\sigma^2\)</span>. This <span
class="math inline">\(s^2\)</span> is a critical ingredient for
calculating the standard errors of our coefficients. <span
class="math display">\[s^2 = \text{RSS}/(n - p - 1)\]</span>
ÊòØÁúüÂÆûËØØÂ∑ÆÊñπÂ∑Æ <span class="math inline">\(\sigma^2\)</span>
ÁöÑ<strong>Êó†ÂÅè‰º∞ËÆ°</strong>„ÄÇËøô‰∏™ <span
class="math inline">\(s^2\)</span>
ÊòØËÆ°ÁÆóÁ≥ªÊï∞Ê†áÂáÜËØØÂ∑ÆÁöÑÂÖ≥ÈîÆÂõ†Á¥†„ÄÇ</p></li>
</ul>
<h4 id="the-f-distribution-and-the-overall-model-test">4. The
F-Distribution and the Overall Model Test</h4>
<p>This final theorem combines our findings about the coefficients and
the residuals.</p>
<ul>
<li><p><strong>Concept:</strong> The F-statistic, which is essentially a
ratio of the variance explained by the model to the variance left
unexplained, follows an <strong>F-distribution</strong>. F
ÁªüËÆ°ÈáèÊú¨Ë¥®‰∏äÊòØÊ®°ÂûãËß£ÈáäÁöÑÊñπÂ∑Æ‰∏éÊú™Ëß£ÈáäÊñπÂ∑ÆÁöÑÊØîÁéáÔºåÊúç‰ªé F ÂàÜÂ∏É„ÄÇ</p></li>
<li><p><strong>Process &amp; Meaning:</strong> This result relies on the
fact that our coefficient estimates (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) are independent
of our total error (RSS). The F-distribution is used for the
<strong>F-test of overall significance</strong>. This test checks the
null hypothesis that <em>all</em> of your slope coefficients are
simultaneously zero (<span class="math inline">\(\beta_1 = \beta_2 =
\dots = \beta_p = 0\)</span>). If the F-test gives a small p-value, you
can conclude that your model, as a whole, is statistically significant
and provides a better fit than a model with no predictors. Â¶ÇÊûú F
Ê£ÄÈ™åÂæóÂá∫ÁöÑ p
ÂÄºËæÉÂ∞èÔºåÂàôÂèØ‰ª•ÂæóÂá∫ÁªìËÆ∫ÔºåÊÇ®ÁöÑÊ®°ÂûãÊï¥‰Ωì‰∏äÂÖ∑ÊúâÁªüËÆ°ÊòæËëóÊÄßÔºåÂπ∂‰∏îÊØîÊ≤°ÊúâÈ¢ÑÊµãÂõ†Â≠êÁöÑÊ®°ÂûãÊãüÂêàÊïàÊûúÊõ¥Â•Ω„ÄÇ</p></li>
</ul>
<h1 id="construct-different-types-of-intervals">11.construct different
types of intervals</h1>
<p><img src="/imgs/5054C3/construct_different_types_of_intervals1.png">
<img src="/imgs/5054C3/construct_different_types_of_intervals2.png"></p>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>:</li>
</ul>
<p>These slides explain how to use the statistical properties of the
least squares estimates to construct different types of intervals, which
are essential for quantifying the uncertainty in your model‚Äôs
predictions and parameters.
Ëøô‰∫õÂπªÁÅØÁâáËß£Èáä‰∫ÜÂ¶Ç‰ΩïÂà©Áî®ÊúÄÂ∞è‰∫å‰πò‰º∞ËÆ°ÁöÑÁªüËÆ°ÁâπÊÄßÊù•ÊûÑÂª∫‰∏çÂêåÁ±ªÂûãÁöÑÂå∫Èó¥ÔºåËøôÂØπ‰∫éÈáèÂåñÊ®°ÂûãÈ¢ÑÊµãÂíåÂèÇÊï∞‰∏≠ÁöÑ‰∏çÁ°ÆÂÆöÊÄßËá≥ÂÖ≥ÈáçË¶Å„ÄÇ</p>
<h3 id="summary-4">Summary</h3>
<p>These slides show how to calculate three distinct types of intervals
in linear regression, each answering a different question about
uncertainty:
Â±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïËÆ°ÁÆóÁ∫øÊÄßÂõûÂΩí‰∏≠‰∏âÁßç‰∏çÂêåÁ±ªÂûãÁöÑÂå∫Èó¥ÔºåÊØèÁßçÂå∫Èó¥ÂàÜÂà´ÂõûÁ≠î‰∫ÜÂÖ≥‰∫é‰∏çÁ°ÆÂÆöÊÄßÁöÑ‰∏çÂêåÈóÆÈ¢òÔºö</p>
<ol type="1">
<li><strong>Confidence Interval for a Parameter (<span
class="math inline">\(\beta_j\)</span>):</strong> Provides a plausible
range for a single, true unknown coefficient in the model.
‰∏∫Ê®°Âûã‰∏≠Âçï‰∏™ÁúüÂÆûÊú™Áü•Á≥ªÊï∞Êèê‰æõ‰∏Ä‰∏™ÂêàÁêÜÁöÑËåÉÂõ¥„ÄÇ</li>
<li><strong>Confidence Interval for the Mean Response:</strong> Provides
a plausible range for the <em>average</em> outcome for a given set of
predictor values.
‰∏∫ÁªôÂÆö‰∏ÄÁªÑÈ¢ÑÊµãÂèòÈáèÂÄºÁöÑ<em>Âπ≥Âùá</em>ÁªìÊûúÊèê‰æõ‰∏Ä‰∏™ÂêàÁêÜÁöÑËåÉÂõ¥„ÄÇ</li>
<li><strong>Prediction Interval:</strong> Provides a plausible range for
a <em>single future</em> outcome for a given set of predictor values.
This interval is always wider than the confidence interval for the mean
response because it must also account for individual random error.
‰∏∫ÁªôÂÆö‰∏ÄÁªÑÈ¢ÑÊµãÂèòÈáèÂÄºÁöÑ<em>Âçï‰∏™Êú™Êù•</em>ÁªìÊûúÊèê‰æõ‰∏Ä‰∏™ÂêàÁêÜÁöÑËåÉÂõ¥„ÄÇËØ•Âå∫Èó¥ÂßãÁªàÊØîÂπ≥ÂùáÂìçÂ∫îÁöÑÁΩÆ‰ø°Âå∫Èó¥Êõ¥ÂÆΩÔºåÂõ†‰∏∫ÂÆÉËøòÂøÖÈ°ªËÄÉËôëÂçï‰∏™ÈöèÊú∫ËØØÂ∑Æ„ÄÇ</li>
</ol>
<h3 id="deeper-dive-into-concepts-and-processes-1">Deeper Dive into
Concepts and Processes</h3>
<h4
id="confidence-interval-for-a-single-parameter-Âçï‰∏™ÂèÇÊï∞ÁöÑÁΩÆ‰ø°Âå∫Èó¥">1.
Confidence Interval for a Single Parameter Âçï‰∏™ÂèÇÊï∞ÁöÑÁΩÆ‰ø°Âå∫Èó¥</h4>
<p>This interval addresses the uncertainty around one specific
coefficient, like the slope for your most important predictor.
Ê≠§Âå∫Èó¥Áî®‰∫éËß£ÂÜ≥Âõ¥ÁªïÊüê‰∏™ÁâπÂÆöÁ≥ªÊï∞ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºå‰æãÂ¶ÇÊúÄÈáçË¶ÅÁöÑÈ¢ÑÊµãÂõ†Â≠êÁöÑÊñúÁéá„ÄÇ</p>
<ul>
<li><strong>The Question It Answers:</strong> ‚ÄúI‚Äôve calculated a slope
of <span class="math inline">\(\hat{\beta}_1 = 10.5\)</span>. How sure
am I about this number? What is a plausible range for the <em>true</em>
population slope?‚Äù ÊàëËÆ°ÁÆóÂá∫‰∫ÜÊñúÁéá‰∏∫ <span
class="math inline">\(\hat{\beta}_1 =
10.5\)</span>„ÄÇÊàëÂØπËøô‰∏™Êï∞Â≠óÊúâÂ§öÁ°ÆÂÆöÔºü<em>ÁúüÂÆû</em>ÊÄª‰ΩìÊñúÁéáÁöÑÂêàÁêÜËåÉÂõ¥ÊòØÂ§öÂ∞ëÔºü‚Äù</li>
<li><strong>The Formula:</strong> <span
class="math inline">\(\hat{\beta}_j \pm t_{n-p-1}(\alpha/2) s
\sqrt{c_{jj}}\)</span>
<ul>
<li><strong><span class="math inline">\(\hat{\beta}_j\)</span></strong>:
This is your best point estimate for the coefficient, taken directly
from the model output. ËøôÊòØËØ•Á≥ªÊï∞ÁöÑÊúÄ‰Ω≥ÁÇπ‰º∞ËÆ°ÂÄºÔºåÁõ¥Êé•ÂèñËá™Ê®°ÂûãËæìÂá∫„ÄÇ</li>
<li><strong><span
class="math inline">\(t_{n-p-1}(\alpha/2)\)</span></strong>: This is the
<strong>critical value</strong> from a t-distribution. It‚Äôs a multiplier
that sets the width of the interval based on your desired confidence
level (e.g., for 95% confidence, <span
class="math inline">\(\alpha=0.05\)</span>). ËøôÊòØ t
ÂàÜÂ∏ÉÁöÑ<strong>‰∏¥ÁïåÂÄº</strong>„ÄÇÂÆÉÊòØ‰∏Ä‰∏™‰πòÊï∞ÔºåÊ†πÊçÆÊÇ®ÊâÄÈúÄÁöÑÁΩÆ‰ø°Ê∞¥Âπ≥ËÆæÁΩÆÂå∫Èó¥ÂÆΩÂ∫¶Ôºà‰æãÂ¶ÇÔºåÂØπ‰∫é
95% ÁöÑÁΩÆ‰ø°Â∫¶Ôºå<span class="math inline">\(\alpha=0.05\)</span>Ôºâ„ÄÇ</li>
<li><strong><span class="math inline">\(s
\sqrt{c_{jj}}\)</span></strong>: This whole term is the <strong>standard
error</strong> of the coefficient <span
class="math inline">\(\hat{\beta}_j\)</span>. It measures the precision
of your estimate. A smaller standard error means a narrower, more
precise interval. ËøôÊï¥‰∏™È°πÊòØÁ≥ªÊï∞ <span
class="math inline">\(\hat{\beta}_j\)</span>
ÁöÑ<strong>Ê†áÂáÜËØØÂ∑Æ</strong>„ÄÇÂÆÉË°°ÈáèÊÇ®‰º∞ËÆ°ÁöÑÁ≤æÂ∫¶„ÄÇÊ†áÂáÜËØØÂ∑ÆË∂äÂ∞èÔºåÂå∫Èó¥Ë∂äÁ™ÑÔºåÁ≤æÂ∫¶Ë∂äÈ´ò„ÄÇ</li>
</ul></li>
</ul>
<h4 id="confidence-interval-for-the-mean-response-Âπ≥ÂùáÂìçÂ∫îÁöÑÁΩÆ‰ø°Âå∫Èó¥">2.
Confidence Interval for the Mean Response Âπ≥ÂùáÂìçÂ∫îÁöÑÁΩÆ‰ø°Âå∫Èó¥</h4>
<p>This interval addresses the uncertainty about the location of the
regression line itself. Ëøô‰∏™Âå∫Èó¥Ëß£ÂÜ≥‰∫ÜÂõûÂΩíÁ∫øÊú¨Ë∫´‰ΩçÁΩÆÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇ</p>
<ul>
<li><strong>The Question It Answers:</strong> ‚ÄúFor a house with 3
bedrooms and 2 bathrooms, what is the plausible range for the
<em>average</em> selling price of <em>all such houses</em>?‚Äù
<strong>ÂÆÉÂõûÁ≠îÁöÑÈóÆÈ¢ò</strong>Ôºö‚ÄúÂØπ‰∫é‰∏ÄÊ†ãÊúâ 3 Èó¥ÂçßÂÆ§Âíå 2
Èó¥Êµ¥ÂÆ§ÁöÑÊàøÂ≠êÔºå<em>ÊâÄÊúâÊ≠§Á±ªÊàøÂ±ã</em>ÁöÑ<em>Âπ≥Âùá</em>ÂîÆ‰ª∑ÁöÑÂêàÁêÜËåÉÂõ¥ÊòØÂ§öÂ∞ëÔºü‚Äù</li>
<li><strong>The Formula:</strong> <span
class="math inline">\(\hat{\boldsymbol{\beta}}^T \mathbf{x} \pm
t_{n-p-1}(\alpha/2)s\sqrt{\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}}\)</span>
<ul>
<li><strong><span class="math inline">\(\hat{\boldsymbol{\beta}}^T
\mathbf{x}\)</span></strong>: This is your point prediction, <span
class="math inline">\(\hat{y}\)</span>, for the given input vector
<strong>x</strong>. ËøôÊòØÁªôÂÆöËæìÂÖ•ÂêëÈáè <strong>x</strong> ÁöÑÁÇπÈ¢ÑÊµã <span
class="math inline">\(\hat{y}\)</span>„ÄÇ</li>
<li><strong><span
class="math inline">\(s\sqrt{\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}}\)</span></strong>:
This is the standard error of the mean response. Its value depends on
how far the input vector <strong>x</strong> is from the center of the
data. This means the confidence interval is narrowest near the average
of your data and gets wider as you move toward the extremes.
ËøôÊòØÂπ≥ÂùáÂìçÂ∫îÁöÑÊ†áÂáÜËØØÂ∑Æ„ÄÇÂÖ∂ÂÄºÂèñÂÜ≥‰∫éËæìÂÖ•ÂêëÈáè <strong>x</strong>
Ë∑ùÁ¶ªÊï∞ÊçÆ‰∏≠ÂøÉÁöÑË∑ùÁ¶ª„ÄÇËøôÊÑèÂë≥ÁùÄÁΩÆ‰ø°Âå∫Èó¥Âú®Êï∞ÊçÆÂπ≥ÂùáÂÄºÈôÑËøëÊúÄÁ™ÑÔºåÂπ∂‰∏îÈöèÁùÄÊé•ËøëÊûÅÂÄºËÄåÂèòÂÆΩ„ÄÇ</li>
</ul></li>
</ul>
<h4
id="prediction-interval-for-an-individual-response-Âçï‰∏™ÂìçÂ∫îÁöÑÈ¢ÑÊµãÂå∫Èó¥">3.
Prediction Interval for an Individual Response Âçï‰∏™ÂìçÂ∫îÁöÑÈ¢ÑÊµãÂå∫Èó¥</h4>
<p>This is the most comprehensive interval and is often the most useful
for making real-world predictions.
ËøôÊòØÊúÄÂÖ®Èù¢ÁöÑÂå∫Èó¥ÔºåÈÄöÂ∏∏ÂØπ‰∫éËøõË°åÂÆûÈôÖÈ¢ÑÊµãÊúÄÊúâÁî®„ÄÇ</p>
<ul>
<li><strong>The Question It Answers:</strong> ‚ÄúI want to predict the
selling price for <em>one specific house</em> that has 3 bedrooms and 2
bathrooms. What is a plausible price range for this <em>single
house</em>?‚Äù</li>
<li><strong>The Formula:</strong> <span
class="math inline">\(\hat{\boldsymbol{\beta}}^T \mathbf{x} \pm
t_{n-p-1}(\alpha/2)s\sqrt{1 +
\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}}\)</span></li>
<li><strong>The Key Difference:</strong> Notice the formula is identical
to the one above, except for the <strong><code>1 + ...</code></strong>
inside the square root. This ‚Äú1‚Äù is critically important. It accounts
for the second source of uncertainty.
<strong>ËØ∑Ê≥®ÊÑèÔºåËØ•ÂÖ¨Âºè‰∏é‰∏äÈù¢ÁöÑÂÖ¨ÂºèÂÆåÂÖ®Áõ∏ÂêåÔºåÂè™ÊòØÂπ≥ÊñπÊ†π‰∏≠ÁöÑ</strong><code>1 + ...</code>**‰∏çÂêå„ÄÇËøô‰∏™‚Äú1‚ÄùËá≥ÂÖ≥ÈáçË¶Å„ÄÇÂÆÉËß£Èáä‰∫ÜÁ¨¨‰∫å‰∏™‰∏çÁ°ÆÂÆöÊÄßÊù•Ê∫ê„ÄÇ
<ol type="1">
<li><strong>Uncertainty in the model:</strong> We are not perfectly
certain about the true location of the regression line. This is captured
by the <span
class="math inline">\(\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}\)</span>
term. **Êàë‰ª¨Êó†Ê≥ïÂÆåÂÖ®Á°ÆÂÆöÂõûÂΩíÁ∫øÁöÑÁúüÂÆû‰ΩçÁΩÆ„ÄÇËøôÂèØ‰ª•ÈÄöËøá <span
class="math inline">\(\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}\)</span>
È°πÊù•ÊçïÊçâ„ÄÇ</li>
<li><strong>Uncertainty in the individual data point:</strong> Even if
we knew the true regression line perfectly, individual data points would
still be scattered around it due to random error (<span
class="math inline">\(\epsilon\)</span>). The ‚Äú1‚Äù in the formula
accounts for this irreducible, random error of a single observation.
Âç≥‰ΩøÊàë‰ª¨ÂÆåÂÖ®‰∫ÜËß£ÁúüÂÆûÁöÑÂõûÂΩíÁ∫øÔºåÁî±‰∫éÈöèÊú∫ËØØÂ∑Æ (<span
class="math inline">\(\epsilon\)</span>)ÔºåÂçï‰∏™Êï∞ÊçÆÁÇπ‰ªçÁÑ∂‰ºöÊï£Â∏ÉÂú®ÂÖ∂Âë®Âõ¥„ÄÇÂÖ¨Âºè‰∏≠ÁöÑ‚Äú1‚ÄùËß£Èáä‰∫ÜÂçï‰∏™ËßÇÊµãÂÄº‰∏≠ËøôÁßç‰∏çÂèØÁ∫¶ÁöÑÈöèÊú∫ËØØÂ∑Æ„ÄÇ</li>
</ol></li>
</ul>
<p>Because it accounts for both types of uncertainty, the
<strong>prediction interval is always wider than the confidence interval
for the mean</strong>.
Áî±‰∫éÂÆÉÂêåÊó∂ËÄÉËôë‰∫Ü‰∏§Áßç‰∏çÁ°ÆÂÆöÊÄßÔºåÂõ†Ê≠§<strong>È¢ÑÊµãÂå∫Èó¥</strong>ÊÄªÊòØÊØîÂùáÂÄºÁöÑÁΩÆ‰ø°Âå∫Èó¥Êõ¥ÂÆΩ„ÄÇ</p>
<h4 id="the-core-difference-an-analogy-‰∏Ä‰∏™Á±ªÊØî">The Core Difference: An
Analogy ‰∏Ä‰∏™Á±ªÊØî</h4>
<ul>
<li><p><strong>Confidence Interval (Mean) ÂùáÂÄºÁΩÆ‰ø°Âå∫Èó¥:</strong> Like
predicting the <strong>average</strong> arrival time for a specific
flight that runs every day. After observing it for a year, you can
predict the average very accurately (e.g., 10:05 AM ¬± 2 minutes).
Â∞±ÂÉèÈ¢ÑÊµãÊØèÂ§©ÁâπÂÆöËà™Áè≠ÁöÑ<strong>Âπ≥Âùá</strong>Âà∞ËææÊó∂Èó¥„ÄÇÁªèËøá‰∏ÄÂπ¥ÁöÑËßÇÂØüÔºåÊÇ®ÂèØ‰ª•ÈùûÂ∏∏ÂáÜÁ°ÆÂú∞È¢ÑÊµãÂπ≥ÂùáÂÄºÔºà‰æãÂ¶ÇÔºå‰∏äÂçà
10:05 ¬± 2 ÂàÜÈíüÔºâ„ÄÇ</p></li>
<li><p><strong>Prediction Interval (Individual) ‰∏™‰ΩìÈ¢ÑÊµãÂå∫Èó¥:</strong>
Like predicting the arrival time for that same flight on <strong>one
specific day</strong> next week. You have to account for the uncertainty
in the average <em>plus</em> the potential for random, one-time events
like weather or air traffic delays. Your prediction must be wider to be
safe (e.g., 10:05 AM ¬± 15 minutes).
Â∞±ÂÉèÈ¢ÑÊµãÂêå‰∏ÄËà™Áè≠‰∏ãÂë®<strong>Êüê‰∏ÄÂ§©</strong>ÁöÑÂà∞ËææÊó∂Èó¥„ÄÇÊÇ®ÂøÖÈ°ªËÄÉËôëÂπ≥ÂùáÂÄºÁöÑ‰∏çÁ°ÆÂÆöÊÄßÔºå‰ª•Âèä*ÂèØËÉΩÂá∫Áé∞ÁöÑÈöèÊú∫„ÄÅ‰∏ÄÊ¨°ÊÄß‰∫ã‰ª∂Ôºå‰æãÂ¶ÇÂ§©Ê∞îÊàñÁ©∫‰∏≠‰∫§ÈÄöÂª∂ËØØ„ÄÇÊÇ®ÁöÑÈ¢ÑÊµãËåÉÂõ¥ÂøÖÈ°ªÊõ¥ÂπøÊâçËÉΩÂÆâÂÖ®Ôºà‰æãÂ¶ÇÔºå‰∏äÂçà
10:05 ¬± 15 ÂàÜÈíüÔºâ„ÄÇ</p></li>
</ul>
<h1 id="construct-different-types-of-intervals-1">12.construct different
types of intervals</h1>
<p><img src="/imgs/5054C3/ANOVA1.png">
<img src="/imgs/5054C3/ANOVA2.png"></p>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>:</li>
</ul>
<p>These slides explain <strong>Analysis of Variance (ANOVA)</strong>, a
method used in regression to break down the total variability in your
data to test if your model is statistically significant as a
whole.Ëøô‰∫õÂπªÁÅØÁâáËÆ≤Ëß£‰∫Ü<strong>ÊñπÂ∑ÆÂàÜÊûê
(ANOVA)</strong>ÔºåËøôÊòØ‰∏ÄÁßçÁî®‰∫éÂõûÂΩíÂàÜÊûêÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂàÜËß£Êï∞ÊçÆ‰∏≠ÁöÑÊÄªÂèòÂºÇÊÄßÔºå‰ª•Ê£ÄÈ™åÊ®°ÂûãÊï¥‰ΩìÊòØÂê¶ÂÖ∑ÊúâÁªüËÆ°ÊòæËëóÊÄß„ÄÇ</p>
<h3 id="summary-5">Summary</h3>
<p>The core idea is to decompose the total variation in the response
variable (<strong>Total Sum of Squares, SS_total</strong>) into two
parts: the variation that is explained by your regression model
(<strong>Regression Sum of Squares, SS_reg</strong>) and the variation
that is left unexplained (<strong>Error Sum of Squares,
SS_error</strong>).
ÂÖ∂Ê†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÂìçÂ∫îÂèòÈáèÁöÑÊÄªÂèòÂºÇÔºà<strong>ÊÄªÂπ≥ÊñπÂíåÔºåSS_total</strong>ÔºâÂàÜËß£‰∏∫‰∏§ÈÉ®ÂàÜÔºöÂõûÂΩíÊ®°ÂûãÂèØ‰ª•Ëß£ÈáäÁöÑÂèòÂºÇÔºà<strong>ÂõûÂΩíÂπ≥ÊñπÂíåÔºåSS_reg</strong>ÔºâÂíåÊú™Ëß£ÈáäÁöÑÂèòÂºÇÔºà<strong>ËØØÂ∑ÆÂπ≥ÊñπÂíåÔºåSS_error</strong>Ôºâ„ÄÇ</p>
<p>By comparing the size of the explained variation to the unexplained
variation using an <strong>F-statistic</strong>, we can formally test
the hypothesis that our model has predictive power. This entire process
is neatly organized in an <strong>ANOVA table</strong>.
ÈÄöËøá‰ΩøÁî®<strong>F
ÁªüËÆ°Èáè</strong>ÊØîËæÉÂ∑≤Ëß£ÈáäÂèòÂºÇ‰∏éÊú™Ëß£ÈáäÂèòÂºÇÁöÑÂ§ßÂ∞èÔºåÊàë‰ª¨ÂèØ‰ª•Ê≠£ÂºèÊ£ÄÈ™åÊ®°ÂûãÂÖ∑ÊúâÈ¢ÑÊµãËÉΩÂäõÁöÑÂÅáËÆæ„ÄÇÊï¥‰∏™ËøáÁ®ãÈÉΩÊï¥ÈΩêÂú∞ÁªÑÁªáÂú®<strong>ÊñπÂ∑ÆÂàÜÊûêË°®</strong>‰∏≠„ÄÇ</p>
<h3 id="deeper-dive-into-concepts-and-connections">Deeper Dive into
Concepts and Connections</h3>
<h4
id="the-decomposition-of-variances-the-core-equation-ÊñπÂ∑ÆÂàÜËß£Ê†∏ÂøÉÊñπÁ®ã">1.
The Decomposition of Variances (The Core Equation)
ÊñπÂ∑ÆÂàÜËß£ÔºàÊ†∏ÂøÉÊñπÁ®ãÔºâ</h4>
<p>The first slide starts with the fundamental equation of ANOVA, which
stems directly from the geometric properties of least squares:
Á¨¨‰∏ÄÂº†ÂπªÁÅØÁâá‰ª•ÊñπÂ∑ÆÂàÜÊûêÁöÑÂü∫Êú¨ÊñπÁ®ãÂºÄÂ§¥ÔºåËØ•ÊñπÁ®ãÁõ¥Êé•Ê∫ê‰∫éÊúÄÂ∞è‰∫å‰πòÁöÑÂá†‰ΩïÊÄßË¥®Ôºö</p>
<p><span class="math display">\[SS_{total} = SS_{reg} +
SS_{error}\]</span></p>
<ul>
<li><strong>SS_total (Total Sum of Squares):</strong> <span
class="math inline">\(\sum(y_i - \bar{y})^2\)</span>
<ul>
<li><strong>Concept:</strong> This measures the <strong>total
variation</strong> in your response variable, <code>y</code>. Imagine
you didn‚Äôt have a model and your only prediction for any <code>y</code>
was its overall average, <code>»≥</code>. SS_total is the total squared
error of this simple ‚Äúmean-only‚Äù model. It represents the total amount
of variation you are trying to explain.
ËøôÊµãÈáèÁöÑÊòØÂìçÂ∫îÂèòÈáè‚Äúy‚ÄùÁöÑ<strong>ÊÄªÂèòÂºÇ</strong>„ÄÇÂÅáËÆæ‰Ω†Ê≤°ÊúâÊ®°ÂûãÔºå‰Ω†ÂØπ‰ªª‰Ωï‚Äúy‚ÄùÁöÑÂîØ‰∏ÄÈ¢ÑÊµãÊòØÂÆÉÁöÑÊï¥‰ΩìÂπ≥ÂùáÂÄº‚Äú»≥‚Äù„ÄÇSS_total
ÊòØËøô‰∏™ÁÆÄÂçïÁöÑ‚Äú‰ªÖÂùáÂÄº‚ÄùÊ®°ÂûãÁöÑÊÄªÂπ≥ÊñπËØØÂ∑Æ„ÄÇÂÆÉ‰ª£Ë°®‰∫Ü‰Ω†ËØïÂõæËß£ÈáäÁöÑÂèòÂºÇÊÄªÈáè„ÄÇ</li>
</ul></li>
<li><strong>SS_reg (Regression Sum of Squares):</strong> <span
class="math inline">\(\sum(\hat{y}_i - \bar{y})^2\)</span>
<ul>
<li><strong>Concept:</strong> This measures the <strong>explained
variation</strong>. It‚Äôs the amount of variation in <code>y</code> that
is captured by your regression model. It calculates the difference
between your model‚Äôs predictions (<code>≈∑</code>) and the simple average
(<code>»≥</code>). A large SS_reg means your model‚Äôs predictions are a
big improvement over just guessing the average.
<strong>ÂÆÉË°°Èáè</strong>Ëß£ÈáäÂèòÂºÇ**„ÄÇÂÆÉÊòØÂõûÂΩíÊ®°ÂûãÊçïÊçâÂà∞ÁöÑ y
ÁöÑÂèòÂºÇÈáè„ÄÇÂÆÉËÆ°ÁÆóÊ®°ÂûãÈ¢ÑÊµãÂÄºÔºà‚Äú≈∑‚ÄùÔºâ‰∏éÁÆÄÂçïÂπ≥ÂùáÂÄºÔºà‚Äú»≥‚ÄùÔºâ‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇËæÉÂ§ßÁöÑ
SS_reg ÊÑèÂë≥ÁùÄÊ®°ÂûãÁöÑÈ¢ÑÊµãÁªìÊûúÊØî‰ªÖ‰ªÖÁåúÊµãÂπ≥ÂùáÂÄºÊúâÊòæËëóÊîπÂñÑ„ÄÇ</li>
</ul></li>
<li><strong>SS_error (Error Sum of Squares):</strong> <span
class="math inline">\(\sum(y_i - \hat{y}_i)^2\)</span>
<ul>
<li><strong>Concept:</strong> This measures the <strong>unexplained
variation</strong> (also called the Residual Sum of Squares). It‚Äôs the
amount of variation your model <em>failed</em> to capture. It‚Äôs the sum
of the squared differences between the actual data (<code>y</code>) and
your model‚Äôs predictions (<code>≈∑</code>).
<strong>ÂÆÉË°°Èáè</strong>Êú™Ëß£ÈáäÂèòÂºÇ**Ôºà‰πüÁß∞‰∏∫ÊÆãÂ∑ÆÂπ≥ÊñπÂíåÔºâ„ÄÇÂÆÉÊòØÊ®°Âûã<em>Êú™ËÉΩ</em>ÊçïÊçâÂà∞ÁöÑÂèòÂºÇÈáè„ÄÇÂÆÉÊòØÂÆûÈôÖÊï∞ÊçÆ
(<code>y</code>) ‰∏éÊ®°ÂûãÈ¢ÑÊµãÂÄº (<code>≈∑</code>) ‰πãÈó¥Âπ≥ÊñπÂ∑Æ‰πãÂíå„ÄÇ</li>
</ul></li>
</ul>
<p>The <strong>R-squared</strong> value is a direct consequence of this
decomposition. It‚Äôs the proportion of the total variance that is
explained by the model: <strong>R Âπ≥Êñπ</strong>
ÂÄºÊòØËøôÁßçÂàÜËß£ÁöÑÁõ¥Êé•ÁªìÊûú„ÄÇÂÆÉÊòØÊ®°ÂûãËß£ÈáäÁöÑÊÄªÊñπÂ∑ÆÁöÑÊØî‰æãÔºö</p>
<p><span class="math display">\[R^2 =
\frac{SS_{reg}}{SS_{total}}\]</span></p>
<h4 id="the-anova-table-and-the-f-test">2. The ANOVA Table and the
F-test</h4>
<p>ÊñπÂ∑ÆÂàÜÊûêË°®Âíå F Ê£ÄÈ™å The second slide organizes these sums of squares
to perform a formal hypothesis test.
Á¨¨‰∫åÂº†ÂπªÁÅØÁâáÊï¥ÁêÜ‰∫ÜËøô‰∫õÂπ≥ÊñπÂíåÔºå‰ª•ËøõË°åÊ≠£ÂºèÁöÑÂÅáËÆæÊ£ÄÈ™å„ÄÇ</p>
<ul>
<li><strong>The Question:</strong> ‚ÄúIs there <em>any</em> relationship
between my set of predictors and the response variable?‚Äù or ‚ÄúIs my model
better than nothing?‚Äù
‚ÄúÊàëÁöÑÈ¢ÑÊµãÂèòÈáèÈõÜÂíåÂìçÂ∫îÂèòÈáè‰πãÈó¥ÊòØÂê¶Â≠òÂú®<em>‰ªª‰Ωï</em>ÂÖ≥Á≥ªÔºü‚ÄùÊàñ‚ÄúÊàëÁöÑÊ®°ÂûãÊØîÊ≤°ÊúâÊ®°ÂûãÂ•ΩÂêóÔºü‚Äù</li>
<li><strong>The Hypotheses:</strong>
<ul>
<li><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>):</strong> <span
class="math inline">\(\beta_1 = \beta_2 = \dots = \beta_p = 0\)</span>.
(None of the predictors have a relationship with the response; the model
is useless). <strong>Èõ∂ÂÅáËÆæ (<span
class="math inline">\(H_0\)</span>)</strong>Ôºö<span
class="math inline">\(\beta_1 = \beta_2 = \dots = \beta_p = 0\)</span>„ÄÇ
ÔºàÊâÄÊúâÈ¢ÑÊµãÂèòÈáèÈÉΩ‰∏éÂìçÂ∫îÂèòÈáèÊó†ÂÖ≥ÔºõËØ•Ê®°ÂûãÊØ´Êó†Áî®Â§ÑÔºâ„ÄÇ</li>
<li><strong>Alternative Hypothesis (<span
class="math inline">\(H_1\)</span>):</strong> At least one <span
class="math inline">\(\beta_j\)</span> is not zero. (The model has some
predictive value). <strong>Â§áÊã©ÂÅáËÆæ (<span
class="math inline">\(H_1\)</span>)Ôºö</strong>Ëá≥Â∞ëÊúâ‰∏Ä‰∏™ <span
class="math inline">\(\beta_j\)</span>
‰∏ç‰∏∫Èõ∂„ÄÇÔºàËØ•Ê®°ÂûãÂÖ∑Êúâ‰∏ÄÂÆöÁöÑÈ¢ÑÊµãÂÄºÔºâ„ÄÇ</li>
</ul></li>
</ul>
<p>To test this, we can‚Äôt just compare the raw SS values, because they
depend on the number of data points and predictors. We need to normalize
them. ‰∏∫‰∫ÜÈ™åËØÅËøô‰∏ÄÁÇπÔºåÊàë‰ª¨‰∏çËÉΩ‰ªÖ‰ªÖÊØîËæÉÂéüÂßãÁöÑ SS
ÂÄºÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÂèñÂÜ≥‰∫éÊï∞ÊçÆÁÇπÂíåÈ¢ÑÊµãÂèòÈáèÁöÑÊï∞Èáè„ÄÇÊàë‰ª¨ÈúÄË¶ÅÂØπÂÆÉ‰ª¨ËøõË°åÂΩí‰∏ÄÂåñ„ÄÇ</p>
<ul>
<li><strong>Mean Squares (MS):</strong> This is the ‚Äúaverage‚Äù variation.
We calculate it by dividing the Sum of Squares by its <strong>degrees of
freedom (df)</strong>.
<strong>ËøôÊòØ‚ÄúÂπ≥Âùá‚ÄùÂèòÂºÇ„ÄÇÊàë‰ª¨ÈÄöËøáÂ∞ÜÂπ≥ÊñπÂíåÈô§‰ª•ÂÖ∂</strong>Ëá™Áî±Â∫¶ (df)**
Êù•ËÆ°ÁÆóÂÆÉ„ÄÇ
<ul>
<li><strong>MS_reg</strong> = <span class="math inline">\(SS_{reg} /
p\)</span>. This is the average explained variation <em>per
predictor</em>. ËøôÊòØ<em>ÊØè‰∏™È¢ÑÊµãÂèòÈáè</em>ÁöÑÂπ≥ÂùáËß£ÈáäÂèòÂºÇ„ÄÇ</li>
<li><strong>MS_error</strong> = <span class="math inline">\(SS_{error} /
(n - p - 1)\)</span>. This is the average unexplained variation, which
is our estimate of the error variance, <span
class="math inline">\(s^2\)</span>. ËøôÊòØÂπ≥ÂùáÊú™Ëß£ÈáäÂèòÂºÇÔºåÂç≥Êàë‰ª¨ÂØπËØØÂ∑ÆÊñπÂ∑Æ
<span class="math inline">\(s^2\)</span> ÁöÑ‰º∞ËÆ°ÂÄº„ÄÇ</li>
</ul></li>
</ul>
<h4 id="the-connection-the-f-statistic-ËÅîÁ≥ªf-ÁªüËÆ°Èáè">3. The Connection:
The F-statistic ËÅîÁ≥ªÔºöF ÁªüËÆ°Èáè</h4>
<p>The <strong>F-statistic</strong> is the key that connects everything.
It‚Äôs the ratio of the two mean squares: <strong>F
ÁªüËÆ°Èáè</strong>ÊòØËøûÊé•‰∏ÄÂàáÁöÑÂÖ≥ÈîÆ„ÄÇÂÆÉÊòØ‰∏§‰∏™ÂùáÊñπÁöÑÊØîÂÄºÔºö <span
class="math display">\[F = \frac{\text{Mean Squared
Regression}}{\text{Mean Squared Error}} =
\frac{MS_{reg}}{MS_{error}}\]</span></p>
<ul>
<li><strong>Intuitive Meaning:</strong> The F-statistic is a ratio of
the <strong>average explained variation</strong> to the <strong>average
unexplained variation</strong>. F
ÁªüËÆ°ÈáèÊòØ<strong>Âπ≥ÂùáËß£ÈáäÂèòÂºÇ</strong>‰∏é<strong>Âπ≥ÂùáÊú™Ëß£ÈáäÂèòÂºÇ</strong>ÁöÑÊØîÂÄº„ÄÇ
<ul>
<li>If your model is useless (<span class="math inline">\(H_0\)</span>
is true), the explained variation should be about the same as the
random, unexplained variation. The F-statistic will be close to 1.
Â¶ÇÊûú‰Ω†ÁöÑÊ®°ÂûãÊó†ÊïàÔºàH_0$
‰∏∫ÁúüÔºâÔºåÂàôËß£ÈáäÂèòÂºÇÂ∫îËØ•‰∏éÈöèÊú∫ÁöÑÊú™Ëß£ÈáäÂèòÂºÇÂ§ßËá¥Áõ∏Âêå„ÄÇF ÁªüËÆ°ÈáèÊé•Ëøë 1„ÄÇ</li>
<li>If your model is useful (<span class="math inline">\(H_1\)</span> is
true), the explained variation should be significantly larger than the
unexplained variation. The F-statistic will be much greater than 1.
Â¶ÇÊûú‰Ω†ÁöÑÊ®°ÂûãÊúâÊïàÔºàH_1$ ‰∏∫ÁúüÔºâÔºåÂàôËß£ÈáäÂèòÂºÇÂ∫îËØ•ÊòæËëóÂ§ß‰∫éÊú™Ëß£ÈáäÂèòÂºÇ„ÄÇ F
ÁªüËÆ°ÈáèÂ∞ÜËøúÂ§ß‰∫é 1„ÄÇ</li>
</ul></li>
</ul>
<p>We compare our calculated F-statistic to an
<strong>F-distribution</strong> to get a <strong>p-value</strong>. A
small p-value (&lt; 0.05) provides strong evidence to reject the null
hypothesis and conclude that your model, as a whole, is statistically
significant. Êàë‰ª¨Â∞ÜËÆ°ÁÆóÂá∫ÁöÑ F ÁªüËÆ°Èáè‰∏é<strong>F
ÂàÜÂ∏É</strong>ËøõË°åÊØîËæÉÔºåÂæóÂá∫<strong>p ÂÄº</strong>„ÄÇËæÉÂ∞èÁöÑ p ÂÄºÔºà&lt;
0.05ÔºâÂèØ‰ª•Êèê‰æõÂº∫ÊúâÂäõÁöÑËØÅÊçÆÊù•ÊãíÁªùÈõ∂ÂÅáËÆæÔºåÂπ∂ÂæóÂá∫ÊÇ®ÁöÑÊ®°ÂûãÊï¥‰ΩìÂÖ∑ÊúâÁªüËÆ°ÊòæËëóÊÄßÁöÑÁªìËÆ∫„ÄÇ</p>
<h1 id="construct-different-types-of-intervals-2">12.construct different
types of intervals</h1>
<p><img src="/imgs/5054C3/Gauss_Markov1.png">
<img src="/imgs/5054C3/Gauss_Markov2.png"></p>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>: These slides explain the <strong>Gauss-Markov
theorem</strong>, a cornerstone result in statistics that establishes
why the Least Squares Estimator (LSE) is considered the gold standard
for fitting linear models under a specific set of assumptions.
Ëøô‰∫õÂπªÁÅØÁâáËß£Èáä‰∫Ü<strong>È´òÊñØ-È©¨Â∞îÂèØÂ§´ÂÆöÁêÜ</strong>ÔºåËøôÊòØÁªüËÆ°Â≠¶‰∏≠ÁöÑ‰∏Ä‰∏™Âü∫Áü≥ÊÄßÊàêÊûúÔºåÂÆÉÈòêÊòé‰∫Ü‰∏∫‰ªÄ‰πàÊúÄÂ∞è‰∫å‰πò‰º∞ËÆ°Èáè
(LSE) Ë¢´ËÆ§‰∏∫ÊòØÂú®ÁâπÂÆöÂÅáËÆæÊù°‰ª∂‰∏ãÊãüÂêàÁ∫øÊÄßÊ®°ÂûãÁöÑÈªÑÈáëÊ†áÂáÜ„ÄÇ</li>
</ul>
<h3 id="summary-6">Summary</h3>
<p>The slides argue for the superiority of the Least Squares Estimator
(LSE) by highlighting its key properties: it‚Äôs easy to compute,
consistent, and efficient. This culminates in the <strong>Gauss-Markov
Theorem</strong>, which proves that LSE is <strong>BLUE</strong>: the
<strong>B</strong>est <strong>L</strong>inear <strong>U</strong>nbiased
<strong>E</strong>stimator. This means that among all estimators that
are both linear and unbiased, the LSE is the ‚Äúbest‚Äù because it has the
smallest possible variance, making it the most precise. The second slide
provides the key steps for the mathematical proof of this important
theorem. Ëøô‰∫õÂπªÁÅØÁâáÈÄöËøáÂº∫Ë∞ÉÊúÄÂ∞è‰∫å‰πò‰º∞ËÆ°Èáè (LSE)
ÁöÑÂÖ≥ÈîÆÁâπÊÄßÊù•ËÆ∫ËØÅÂÖ∂‰ºòË∂äÊÄßÔºöÊòì‰∫éËÆ°ÁÆó„ÄÅ‰∏ÄËá¥ÊÄßÈ´ò‰∏îÈ´òÊïà„ÄÇÊúÄÁªàÂæóÂá∫‰∫Ü<strong>È´òÊñØ-È©¨Â∞îÂèØÂ§´ÂÆöÁêÜ</strong>ÔºåËØ•ÂÆöÁêÜËØÅÊòé‰∫Ü
LSE
ÊòØ<strong>BLUE</strong>Ôºö<strong>ÊúÄ‰Ω≥</strong>Á∫øÊÄß<strong>Êó†ÂÅè</strong>‰º∞ËÆ°Èáè„ÄÇËøôÊÑèÂë≥ÁùÄÂú®ÊâÄÊúâÁ∫øÊÄß‰∏îÊó†ÂÅèÁöÑ‰º∞ËÆ°Èáè‰∏≠ÔºåLSE
ÊòØ‚ÄúÊúÄ‰Ω≥‚ÄùÁöÑÔºåÂõ†‰∏∫ÂÆÉÂÖ∑ÊúâÊúÄÂ∞èÁöÑÊñπÂ∑ÆÔºåÂõ†Ê≠§Á≤æÂ∫¶ÊúÄÈ´ò„ÄÇÁ¨¨‰∫åÂº†ÂπªÁÅØÁâáÊèê‰æõ‰∫ÜËøô‰∏ÄÈáçË¶ÅÂÆöÁêÜÁöÑÊï∞Â≠¶ËØÅÊòéÁöÑÂÖ≥ÈîÆÊ≠•È™§„ÄÇ</p>
<h3 id="deeper-dive-into-the-concepts">Deeper Dive into the
Concepts</h3>
<h4 id="properties-of-lse-slide-1-Â±ÄÈÉ®Ê≠£‰∫§‰º∞ËÆ°-lse-ÁöÑÊÄßË¥®">Properties of
LSE (Slide 1) Â±ÄÈÉ®Ê≠£‰∫§‰º∞ËÆ° (LSE) ÁöÑÊÄßË¥®</h4>
<ul>
<li><strong>Easy ComputationÊòì‰∫éËÆ°ÁÆó:</strong> The LSE has a direct,
closed-form solution called the Normal Equation (<span
class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>). You can
calculate it directly without needing complex iterative algorithms.</li>
<li><strong>Consistency‰∏ÄËá¥ÊÄß:</strong> As your sample size gets larger
and larger, the LSE estimate (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) is guaranteed
to get closer and closer to the true population value (<span
class="math inline">\(\boldsymbol{\beta}\)</span>). With enough data, it
will find the truth. ÈöèÁùÄÊ†∑Êú¨ÈáèË∂äÊù•Ë∂äÂ§ßÔºåLSE ‰º∞ËÆ°ÂÄº (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>)
ÂøÖÁÑ∂‰ºöË∂äÊù•Ë∂äÊé•ËøëÁúüÂÆûÁöÑÊÄª‰ΩìÂÄº (<span
class="math inline">\(\boldsymbol{\beta}\)</span>)„ÄÇÂè™Ë¶ÅÊúâË∂≥Â§üÁöÑÊï∞ÊçÆÔºåÂÆÉÂ∞±ËÉΩÊâæÂà∞ÁúüÁõ∏„ÄÇ</li>
<li><strong>EfficiencyÊïàÁéá:</strong> An efficient estimator is the one
with the lowest possible variance. This means its estimates are the most
precise and least spread out.
È´òÊïàÁöÑ‰º∞ËÆ°Âô®ÊòØÊñπÂ∑ÆÂ∞ΩÂèØËÉΩ‰ΩéÁöÑ‰º∞ËÆ°Âô®„ÄÇËøôÊÑèÂë≥ÁùÄÂÆÉÁöÑ‰º∞ËÆ°ÂÄºÊúÄÁ≤æÁ°ÆÔºå‰∏îÂàÜÂ∏ÉÊúÄÂùáÂåÄ„ÄÇ</li>
<li><strong>BLUE (Best Linear Unbiased
Estimator)BLUEÔºàÊúÄ‰Ω≥Á∫øÊÄßÊó†ÂÅè‰º∞ËÆ°Âô®Ôºâ:</strong> This acronym elegantly
summarizes the Gauss-Markov theorem.
Ëøô‰∏™Áº©ÂÜôÂÆåÁæéÂú∞Ê¶ÇÊã¨‰∫ÜÈ´òÊñØ-È©¨Â∞îÂèØÂ§´ÂÆöÁêÜ„ÄÇ
<ul>
<li><strong>Linear:</strong> The estimator is a linear function of the
response variable <strong>y</strong>. We can write it as <span
class="math inline">\(\hat{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span> for some matrix <strong>A</strong>.
‰º∞ËÆ°Âô®ÊòØÂìçÂ∫îÂèòÈáè<strong>y</strong>ÁöÑÁ∫øÊÄßÂáΩÊï∞„ÄÇÂØπ‰∫éÊüê‰∏™Áü©Èòµ<strong>A</strong>ÔºåÊàë‰ª¨ÂèØ‰ª•Â∞ÜÂÖ∂ÂÜôÊàê
<span class="math inline">\(\hat{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span>„ÄÇ</li>
<li><strong>Unbiased:</strong> The estimator does not systematically
overestimate or underestimate the true parameter. On average, its
expected value is the true value: <span
class="math inline">\(E[\hat{\boldsymbol{\beta}}] =
\boldsymbol{\beta}\)</span>.
‰º∞ËÆ°Âô®‰∏ç‰ºöÁ≥ªÁªüÊÄßÂú∞È´ò‰º∞Êàñ‰Ωé‰º∞ÁúüÂÆûÂèÇÊï∞„ÄÇÂπ≥ÂùáËÄåË®ÄÔºåÂÖ∂È¢ÑÊúüÂÄºÂç≥‰∏∫ÁúüÂÆûÂÄºÔºö<span
class="math inline">\(E[\hat{\boldsymbol{\beta}}] =
\boldsymbol{\beta}\)</span>„ÄÇ</li>
<li><strong>Best:</strong> It has the <strong>minimum variance</strong>
of all possible linear unbiased estimators. It‚Äôs the most precise and
reliable estimator in its class.
Âú®ÊâÄÊúâÂèØËÉΩÁöÑÁ∫øÊÄßÊó†ÂÅè‰º∞ËÆ°Âô®‰∏≠ÔºåÂÆÉÁöÑ<strong>ÊñπÂ∑Æ</strong>ÊúÄÂ∞è„ÄÇÂÆÉÊòØÂêåÁ±ª‰∏≠ÊúÄÁ≤æÁ°Æ„ÄÅÊúÄÂèØÈù†ÁöÑ‰º∞ËÆ°Âô®„ÄÇ</li>
</ul></li>
</ul>
<h4 id="the-gauss-markov-theorem-È´òÊñØ-È©¨Â∞îÂèØÂ§´ÂÆöÁêÜ">The Gauss-Markov
Theorem È´òÊñØ-È©¨Â∞îÂèØÂ§´ÂÆöÁêÜ</h4>
<p>The theorem provides the theoretical justification for using OLS.
ËØ•ÂÆöÁêÜ‰∏∫‰ΩøÁî®ÊúÄÂ∞è‰∫å‰πòÊ≥ï (OLS) Êèê‰æõ‰∫ÜÁêÜËÆ∫‰æùÊçÆ„ÄÇ * <strong>The Core
Idea:</strong> You could invent many different ways to estimate the
coefficients of a linear model. As long as your proposed methods are
both linear and unbiased, the Gauss-Markov theorem guarantees that none
of them will be more precise than the standard least squares method. LSE
gives the ‚Äúsharpest‚Äù possible estimates.
‰Ω†ÂèØ‰ª•ÂèëÊòéËÆ∏Â§ö‰∏çÂêåÁöÑÊñπÊ≥ïÊù•‰º∞ËÆ°Á∫øÊÄßÊ®°ÂûãÁöÑÁ≥ªÊï∞„ÄÇÂè™Ë¶Å‰Ω†ÊèêÂá∫ÁöÑÊñπÊ≥ïÊòØÁ∫øÊÄßÁöÑ‰∏îÊó†ÂÅèÁöÑÔºåÈ´òÊñØ-È©¨Â∞îÂèØÂ§´ÂÆöÁêÜÂ∞±ËÉΩ‰øùËØÅÔºåÂÆÉ‰ª¨ÈÉΩ‰∏ç‰ºöÊØîÊ†áÂáÜÊúÄÂ∞è‰∫å‰πòÊ≥ïÊõ¥Á≤æÁ°Æ„ÄÇÊúÄÂ∞è‰∫å‰πòÊ≥ï
(LSE) ÁªôÂá∫‰∫Ü‚ÄúÊúÄÁ≤æÁ°Æ‚ÄùÁöÑ‰º∞ËÆ°ÂÄº„ÄÇ</p>
<ul>
<li><strong>The Logic of the Proof (Slide 2) ËØÅÊòéÈÄªËæë:</strong> The
proof is a clever comparison of variances. **ËØ•ËØÅÊòéÂ∑ßÂ¶ôÂú∞ÊØîËæÉ‰∫ÜÊñπÂ∑Æ„ÄÇ
<ol type="1">
<li>It starts by defining <strong>any</strong> other linear unbiased
estimator as <span class="math inline">\(\tilde{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span>.
È¶ñÂÖàÔºåÂ∞Ü<strong>‰ªª‰Ωï</strong>ÂÖ∂‰ªñÁ∫øÊÄßÊó†ÂÅè‰º∞ËÆ°ÈáèÂÆö‰πâ‰∏∫ <span
class="math inline">\(\tilde{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span>„ÄÇ</li>
<li>It uses the ‚Äúunbiased‚Äù property to force a condition on the matrix
<strong>A</strong>, which ultimately leads to the insight that
<strong>A</strong> can be written in terms of the LSE matrix plus some
other matrix <strong>D</strong>, where <strong>DX = 0</strong>.
ÂÆÉÂà©Áî®‚ÄúÊó†ÂÅè‚ÄùÊÄßË¥®ÂØπÁü©Èòµ<strong>A</strong>Âº∫Âà∂ÊñΩÂä†‰∏Ä‰∏™Êù°‰ª∂ÔºåÊúÄÁªàÂæóÂá∫<strong>A</strong>ÂèØ‰ª•ÂÜôÊàêLSEÁü©ÈòµÂä†‰∏äÂè¶‰∏Ä‰∏™Áü©Èòµ<strong>D</strong>ÔºåÂÖ∂‰∏≠<strong>DX
= 0</strong>„ÄÇ</li>
<li>It then calculates the variance of this other estimator, which turns
out to be: <span class="math display">\[Var(\tilde{\boldsymbol{\beta}})
= Var(\text{LSE}) + \text{a non-negative term involving }
\mathbf{D}\]</span> ÁÑ∂ÂêéËÆ°ÁÆóÂè¶‰∏Ä‰∏™‰º∞ËÆ°ÈáèÁöÑÊñπÂ∑ÆÔºåÁªìÊûú‰∏∫Ôºö <span
class="math display">\[Var(\tilde{\boldsymbol{\beta}}) = Var(\text{LSE})
+ \text{‰∏Ä‰∏™ÂåÖÂê´ } \mathbf{D} ÁöÑÈùûË¥üÈ°π\]</span></li>
<li>Since the variance of any other linear unbiased estimator is the
variance of the LSE <em>plus</em> something non-negative, the variance
of the LSE must be the smallest possible value.
Áî±‰∫é‰ªª‰ΩïÂÖ∂‰ªñÁ∫øÊÄßÊó†ÂÅè‰º∞ËÆ°ÈáèÁöÑÊñπÂ∑ÆÈÉΩÊòØLSEÁöÑÊñπÂ∑Æ<em>Âä†‰∏ä</em>‰∏Ä‰∏™ÈùûË¥üÈ°πÔºåÂõ†Ê≠§LSEÁöÑÊñπÂ∑ÆÂøÖÈ°ªÊòØÊúÄÂ∞èÁöÑÂèØËÉΩÂÄº„ÄÇ</li>
</ol></li>
</ul>
<h3 id="further-understandings-beyond-the-slides">Further Understandings
Beyond the Slides</h3>
<h4 id="what-are-the-required-assumptionsÈúÄË¶ÅÂì™‰∫õÂÅáËÆæ">1. What are the
required assumptions?ÈúÄË¶ÅÂì™‰∫õÂÅáËÆæÔºü</h4>
<p>The Gauss-Markov theorem is powerful, but it‚Äôs not magic. It only
holds if a set of assumptions about the model‚Äôs errors (<span
class="math inline">\(\epsilon\)</span>) are met:
È´òÊñØ-È©¨Â∞îÂèØÂ§´ÂÆöÁêÜËôΩÁÑ∂Âº∫Â§ßÔºå‰ΩÜÂπ∂ÈùûÈ≠îÊ≥ï„ÄÇÂÆÉ‰ªÖÂú®Êª°Ë∂≥‰ª•‰∏ãÂÖ≥‰∫éÊ®°ÂûãËØØÂ∑Æ (<span
class="math inline">\(\epsilon\)</span>) ÁöÑÂÅáËÆæÊó∂ÊàêÁ´ãÔºö * <strong>Zero
MeanÈõ∂ÂùáÂÄº:</strong> The average of the errors is zero (<span
class="math inline">\(E[\epsilon] = 0\)</span>). ËØØÂ∑ÆÁöÑÂπ≥ÂùáÂÄº‰∏∫Èõ∂ (<span
class="math inline">\(E[\epsilon] = 0\)</span>)„ÄÇ * <strong>Constant
Variance (Homoscedasticity)ÊÅíÂÆöÊñπÂ∑ÆÔºàÂêåÊñπÂ∑ÆÊÄßÔºâ:</strong> The errors
have the same variance, <span class="math inline">\(\sigma^2\)</span>,
at all levels of the predictors.
<strong>Âú®È¢ÑÊµãÂèòÈáèÁöÑÂêÑ‰∏™Ê∞¥Âπ≥‰∏äÔºåËØØÂ∑ÆÂÖ∑ÊúâÁõ∏ÂêåÁöÑÊñπÂ∑Æ <span
class="math inline">\(\sigma^2\)</span>„ÄÇ * </strong>Uncorrelated
Errors‰∏çÁõ∏ÂÖ≥ËØØÂ∑Æ:** The error for one observation is not correlated with
the error for another. ‰∏Ä‰∏™ËßÇÊµãÂÄºÁöÑËØØÂ∑Æ‰∏éÂè¶‰∏Ä‰∏™ËßÇÊµãÂÄºÁöÑËØØÂ∑Æ‰∏çÁõ∏ÂÖ≥„ÄÇ *
<strong>No Perfect MulticollinearityÈùûÂÆåÂÖ®Â§öÈáçÂÖ±Á∫øÊÄß:</strong> The
predictor variables are not perfectly linearly related.
È¢ÑÊµãÂèòÈáèÂπ∂ÈùûÂÆåÂÖ®Á∫øÊÄßÁõ∏ÂÖ≥„ÄÇ</p>
<p><strong>Crucially, the Gauss-Markov theorem does NOT require the
errors to be normally distributed.</strong> The normality assumption is
only needed later for constructing confidence intervals and conducting
t-tests and F-tests.
Ëá≥ÂÖ≥ÈáçË¶ÅÁöÑÊòØÔºåÈ´òÊñØ-È©¨Â∞îÂèØÂ§´ÂÆöÁêÜÂπ∂‰∏çË¶ÅÊ±ÇËØØÂ∑ÆÊúç‰ªéÊ≠£ÊÄÅÂàÜÂ∏É„ÄÇ**Ê≠£ÊÄÅÊÄßÂÅáËÆæ‰ªÖÂú®ÊûÑÂª∫ÁΩÆ‰ø°Âå∫Èó¥‰ª•ÂèäËøõË°å
t Ê£ÄÈ™åÂíå F Ê£ÄÈ™åÊó∂ÈúÄË¶Å„ÄÇ</p>
<h4
id="when-is-lse-not-the-best-the-bias-variance-tradeoff-‰ªÄ‰πàÊó∂ÂÄô-lse-‰∏çÊòØÊúÄ‰Ω≥ÈÄâÊã©-ÂÅèÂ∑Æ-ÊñπÂ∑ÆÊùÉË°°">2.
When is LSE NOT the Best? (The Bias-Variance Tradeoff) ‰ªÄ‰πàÊó∂ÂÄô LSE
‰∏çÊòØÊúÄ‰Ω≥ÈÄâÊã©Ôºü ÔºàÂÅèÂ∑Æ-ÊñπÂ∑ÆÊùÉË°°Ôºâ</h4>
<p>While LSE is the best <em>unbiased</em> estimator, sometimes we can
get better predictive performance by accepting a little bit of bias in
exchange for a large reduction in variance. This is the core idea behind
modern regularization methods: ËôΩÁÑ∂ LSE
ÊòØÊúÄÂ•ΩÁöÑ<em>Êó†ÂÅè</em>‰º∞ËÆ°Âô®Ôºå‰ΩÜÊúâÊó∂Êàë‰ª¨ÂèØ‰ª•ÈÄöËøáÊé•ÂèóÂ∞ëÈáèÂÅèÂ∑ÆÊù•Â§ßÂπÖÈôç‰ΩéÊñπÂ∑ÆÔºå‰ªéËÄåËé∑ÂæóÊõ¥Â•ΩÁöÑÈ¢ÑÊµãÊÄßËÉΩ„ÄÇËøôÊòØÁé∞‰ª£Ê≠£ÂàôÂåñÊñπÊ≥ïËÉåÂêéÁöÑÊ†∏ÂøÉÊÄùÊÉ≥Ôºö
* <strong>Ridge Regression and LASSOÂ≤≠ÂõûÂΩíÂíå LASSO:</strong> These are
popular techniques that produce <em>biased</em> estimates of the
coefficients. However, by introducing this small amount of bias, they
can often produce models with a lower overall error (Mean Squared Error)
than LSE, especially when predictors are highly correlated.
Ëøô‰∫õÊòØ‰∫ßÁîü<em>ÊúâÂÅè</em>Á≥ªÊï∞‰º∞ËÆ°ÁöÑÊµÅË°åÊäÄÊúØ„ÄÇÁÑ∂ËÄåÔºåÈÄöËøáÂºïÂÖ•Â∞ëÈáèÂÅèÂ∑ÆÔºåÂÆÉ‰ª¨ÈÄöÂ∏∏ÂèØ‰ª•ÁîüÊàêÊØî
LSE
ÂÖ∑ÊúâÊõ¥‰ΩéÊÄª‰ΩìËØØÂ∑ÆÔºàÂùáÊñπËØØÂ∑ÆÔºâÁöÑÊ®°ÂûãÔºåÂ∞§ÂÖ∂ÊòØÂú®È¢ÑÊµãÂèòÈáèÈ´òÂ∫¶Áõ∏ÂÖ≥ÁöÑÊÉÖÂÜµ‰∏ã„ÄÇ
Therefore, while LSE is the theoretical champion in the world of
unbiased estimators, in the world of predictive modeling, methods that
intentionally introduce bias can sometimes be superior. Âõ†Ê≠§ÔºåËôΩÁÑ∂ LSE
ÊòØÊó†ÂÅè‰º∞ËÆ°È¢ÜÂüüÁöÑÁêÜËÆ∫ÂÜ†ÂÜõÔºå‰ΩÜÂú®È¢ÑÊµãÊ®°ÂûãÈ¢ÜÂüüÔºåÊúâÊÑèÂºïÂÖ•ÂÅèÂ∑ÆÁöÑÊñπÊ≥ïÊúâÊó∂‰ºöÊõ¥ËÉú‰∏ÄÁ≠π„ÄÇ</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/16/2025_9_15%E4%BC%9A%E8%AE%AE%E7%BA%AA%E8%A6%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/16/2025_9_15%E4%BC%9A%E8%AE%AE%E7%BA%AA%E8%A6%81/" class="post-title-link" itemprop="url">MEETING - AI4Chemistry conference notes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">ÂèëË°®‰∫é</span>

              <time title="ÂàõÂª∫Êó∂Èó¥Ôºö2025-09-16 01:00:00" itemprop="dateCreated datePublished" datetime="2025-09-16T01:00:00+08:00">2025-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Êõ¥Êñ∞‰∫é</span>
                <time title="‰øÆÊîπÊó∂Èó¥Ôºö2025-09-21 06:46:20" itemprop="dateModified" datetime="2025-09-21T06:46:20+08:00">2025-09-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">ÂàÜÁ±ª‰∫é</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/meeting-notes/" itemprop="url" rel="index"><span itemprop="name">meeting notes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Meeting Notes</p>
<h1 id="‰πùÊúà‰ªΩËßÑÂàí">1. ‰πùÊúà‰ªΩËßÑÂàíÔºö</h1>
<ul>
<li><strong>ÂÜÖÂÆπ</strong>:</li>
</ul>
<h2 id="ÁªèÂÖ∏ËÆ∫ÊñáÊ®°ÂûãÂ§çÁé∞">1.1 ÁªèÂÖ∏ËÆ∫ÊñáÊ®°ÂûãÂ§çÁé∞Ôºö</h2>
<ul>
<li><strong>1.1.1</strong> <a
target="_blank" rel="noopener" href="https://www.biorxiv.org/content/10.1101/2025.08.06.668973v2">MetaAI-3DËõãÁôΩË¥®ÁªìÊûÑÂØπÊØîÂ≠¶‰π†</a>
<a target="_blank" rel="noopener" href="https://github.com/kalifadan/FusionProt">github</a></li>
<li><strong>1.1.2</strong> <a
target="_blank" rel="noopener" href="https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-024-02030-9">DrugDAGT</a>
<a target="_blank" rel="noopener" href="https://github.com/codejiajia/DrugDAGT">github</a> <a
target="_blank" rel="noopener" href="https://zenodo.org/records/6568518#.YrtTKHbMK38">!!! orbnet
qm9ÁöÑgraph basedÁöÑfeature</a> <a
target="_blank" rel="noopener" href="https://zenodo.org/records/6568518#.YrtTKHbMK38">github</a></li>
<li><strong>1.1.3 GCL &amp; GCFORMER</strong> <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.13902">ÁªèÂÖ∏ÂØπÊØîÂ≠¶‰π†ËÆ∫Êñá</a> <a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=u6FuiKzT1K">!! Graph contrastive
learning former - NIPS2024</a> <a
target="_blank" rel="noopener" href="https://github.com/JHL-HUST/GCFormer">github</a></li>
<li><strong>GCL</strong> <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.13902">Graph Contrastive Learning with
Augmentations - NIPS</a> <a
target="_blank" rel="noopener" href="https://github.com/Shen-Lab/GraphCL">github</a></li>
</ul>
<h2 id="Êï∞ÊçÆÈõÜ">1.2 Êï∞ÊçÆÈõÜÔºö</h2>
<p><a
target="_blank" rel="noopener" href="https://quantum-machine.org/datasets/">quantum-machine-QM9</a> <a
target="_blank" rel="noopener" href="https://github.com/bigdata-ustc/QM9nano4USTC">‰∏≠ÁßëÂ§ßÂÆûÈ™åËØæÂÅö‰∫Ü‰∏™QM9Êï∞ÊçÆÈõÜÁöÑdemo</a></p>
<ul>
<li><p><strong>Èó≠Â£≥Â±ÇÂàÜÂ≠êÊï∞ÊçÆÈõÜ</strong></p></li>
<li><p><strong>ÂàÜÂ≠êËßÑÊ®°</strong>ÔºöÂê´ 13.4
‰∏á‰∏™Á®≥ÂÆöÁöÑÂ∞èÂàÜÂ≠êÊúâÊú∫Áâ©</p></li>
<li><p><strong>ÂÖÉÁ¥†ÁªÑÊàê</strong>Ôºö‰ªÖÂåÖÂê´
HÔºàÊ∞¢Ôºâ„ÄÅCÔºàÁ¢≥Ôºâ„ÄÅNÔºàÊ∞ÆÔºâ„ÄÅOÔºàÊ∞ßÔºâ„ÄÅFÔºàÊ∞üÔºâ5 ÁßçÂÖÉÁ¥†</p></li>
<li><p><strong>ËÆ°ÁÆóÁêÜËÆ∫Ê∞¥Âπ≥</strong>ÔºöÊâÄÊúâÂàÜÂ≠êÂ±ûÊÄßÂü∫‰∫éDFTÔºàÂØÜÂ∫¶Ê≥õÂáΩÁêÜËÆ∫Ôºâ/B3LYP
Ê≥õÂáΩ / 6-31G (2df,p) Âü∫ÁªÑËÆ°ÁÆó</p></li>
<li><p><strong>ÂåÖÂê´Â±ûÊÄß</strong>ÔºöÂÅ∂ÊûÅÁü©„ÄÅHOMOÔºàÊúÄÈ´òÂç†ÊçÆÂàÜÂ≠êËΩ®ÈÅìÔºâËÉΩÈáè„ÄÅLUMOÔºàÊúÄ‰ΩéÊú™Âç†ÊçÆÂàÜÂ≠êËΩ®ÈÅìÔºâËÉΩÈáè„ÄÅ0K
ÂÜÖËÉΩ„ÄÅ298.15K ÂÜÖËÉΩÁ≠âÔºåÊòØÈó≠Â£≥Â±ÇÂàÜÂ≠êÊÄßË¥®È¢ÑÊµãÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜ</p></li>
<li><p><strong>ËÆ≠ÁªÉËæìÂÖ•</strong>ÔºöÈó≠Â£≥Â±Ç‰∏ãÁöÑÈáèÂ≠êÂåñÂ≠¶Áü©ÈòµÔºåÂç≥ Fock
Áü©ÈòµÔºàFÔºâ„ÄÅÂØÜÂ∫¶Áü©ÈòµÔºàPÔºâ„ÄÅÂìàÂØÜÈ°øÁü©ÈòµÔºàHÔºâ„ÄÅÈáçÂè†Áü©ÈòµÔºàSÔºâÔºåÊûÑÊàêÂêëÈáè T
ÁöÑÈó≠Â£≥Â±ÇÂΩ¢ÂºèÔºàÂõ†Èó≠Â£≥Â±ÇËá™ÊóãÂØπÁß∞ÔºåÊó†ÈúÄÂå∫ÂàÜ Œ±„ÄÅŒ≤ Ëá™ÊóãÔºåÊïÖ
T=[F,P,H,S]Ôºâ</p></li>
<li><p><strong>ÁâπÂæÅÊú¨Ë¥®</strong>ÔºöËøô‰∫õÁü©ÈòµÁºñÁ†Å‰∫ÜÂàÜÂ≠êÁöÑÁîµÂ≠êÁªìÊûÑ‰ø°ÊÅØÔºàÂ¶ÇËΩ®ÈÅìÈó¥Áõ∏‰∫í‰ΩúÁî®„ÄÅÁîµÂ≠êÂØÜÂ∫¶ÂàÜÂ∏ÉÔºâÔºåÊòØ
OrbNet-Equi Â≠¶‰π† ‚ÄúÂàÜÂ≠êÁªìÊûÑ - ËÉΩÈáè‚Äù Êò†Â∞ÑÁöÑÊ†∏ÂøÉ‰æùÊçÆ</p></li>
<li><p><strong>ËÆ≠ÁªÉËæìÂá∫</strong>ÔºöQM9 ‰∏≠ÁöÑ0K
ÂÜÖËÉΩ‰Ωú‰∏∫Ê†∏ÂøÉËÆ≠ÁªÉÁõÆÊ†áËæìÂá∫Ôºå0K
ÂÜÖËÉΩÊòØÂàÜÂ≠êÂäøËÉΩÈù¢ÔºàPESÔºâËÆ°ÁÆóÁöÑÊ†∏ÂøÉÂ±ûÊÄßÔºåÁõ¥Êé•ÂÖ≥ËÅîÂàÜÂ≠êÁ®≥ÂÆöÊÄß‰∏éÂèçÂ∫îËÉΩÂûíÈ¢ÑÊµãÔºåÁõ∏ÊØîÂÖ∂‰ªñÂ±ûÊÄßÔºàÂ¶ÇÂÅ∂ÊûÅÁü©ÔºâÔºåËÉΩÈáèÊòØÈó≠Â£≥Â±Ç‰∏éÂºÄÂ£≥Â±ÇÁ≥ªÁªüÂÖ±ÊúâÁöÑÂÖ≥ÈîÆÊåáÊ†áÔºå‰æø‰∫éÂêéÁª≠Êâ©Â±ïÂà∞ÂºÄÂ£≥Â±ÇËÉΩÈáèÈ¢ÑÊµã„ÄÇ</p></li>
</ul>
<h2 id="Â§çÁé∞ÊñπÂºè">1.3 Â§çÁé∞ÊñπÂºèÔºö</h2>
<p>Êï∞ÊçÆÂ∞èÂûãÂåñÂ§çÁé∞Ôºö setp1: 5k bonds and edges</p>
<p>setp2: 10k bonds and edges</p>
<p>setp3: 20k bonds and edges</p>
<h2 id="‰∏§ÁßçÊï∞ÊçÆÁ©∫Èó¥‰∏Ä‰∏™ÂæÖÁêÜËß£ÁöÑÊ¶ÇÂøµopen-shell">1.4
‰∏§ÁßçÊï∞ÊçÆÁ©∫Èó¥‰∏Ä‰∏™ÂæÖÁêÜËß£ÁöÑÊ¶ÇÂøµOpen-shellÔºö</h2>
<ul>
<li><strong>AO (Atomic
Orbital)</strong>Ôºö<strong>ÂéüÂ≠êËΩ®ÈÅì</strong>„ÄÇ</li>
<li><strong>MO (Molecular
Orbital)</strong>Ôºö<strong>ÂàÜÂ≠êËΩ®ÈÅì</strong>„ÄÇ</li>
<li><strong>Open-shell</strong>Ôºö<strong>ÂºÄÂ£≥Â±ÇÁªÑÊÄÅ</strong>„ÄÇ</li>
</ul>
<h4 id="ao-atomic-orbital---ÂéüÂ≠êËΩ®ÈÅìÂ±ÇÈù¢-‰ª•ÂéüÂ≠ê‰∏∫‰∏≠ÂøÉÁöÑÊ®°Âûã">1. AO
(Atomic Orbital) - ÂéüÂ≠êËΩ®ÈÅìÂ±ÇÈù¢ / ‰ª•ÂéüÂ≠ê‰∏∫‰∏≠ÂøÉÁöÑÊ®°Âûã</h4>
<p>Â∞ÜÂàÜÂ≠êÁúã‰ΩúÊòØÂéüÂ≠êÔºàËäÇÁÇπÔºâÂíåÂåñÂ≠¶ÈîÆÔºàËæπÔºâÊûÑÊàêÁöÑÂõæ„ÄÇÊ®°ÂûãÂ≠¶‰π†ÊØè‰∏™ÂéüÂ≠ê‰ª•ÂèäÂÖ∂Âë®Âõ¥Â±ÄÈÉ®ÁéØÂ¢ÉÁöÑrepresentationÔºåÈ¢ÑÊµãÊï¥‰∏™ÂàÜÂ≠êÁöÑÊÄßË¥®„ÄÇ</p>
<ul>
<li><p><strong>key</strong>:
ÂàÜÂ≠êÁöÑÊÄßË¥®ÊòØÁî±ÂÖ∂ÁªÑÊàêÂéüÂ≠ê‰ª•ÂèäÂéüÂ≠êÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®ÂÜ≥ÂÆöÁöÑ„ÄÇ</p></li>
<li><p><strong>ËæìÂÖ•</strong>:
ÂéüÂ≠êÁöÑÂùêÊ†á„ÄÅÂéüÂ≠êÁ±ªÂûã„ÄÅ‰ª•ÂèäÂéüÂ≠êÈó¥ÁöÑË∑ùÁ¶ªÊàñÈîÆÂêàÂÖ≥Á≥ª„ÄÇ</p></li>
<li><p><strong>ÊñπÂºè</strong>: <strong>Ê∂àÊÅØ‰º†ÈÄíÂõæÁ•ûÁªèÁΩëÁªú (Message
Passing Neural Network, MPNN)</strong>
„ÄÇÊØè‰∏™ÂéüÂ≠êÔºàËäÇÁÇπÔºâ‰ªéÂÖ∂ÈÇªÂ±ÖÂéüÂ≠êÈÇ£ÈáåÊé•Êî∂‚ÄúÊ∂àÊÅØ‚ÄùÔºà‰ø°ÊÅØÔºâÔºåÊõ¥Êñ∞Ëá™Â∑±ÁöÑÁä∂ÊÄÅÔºàÁâπÂæÅÂêëÈáèÔºâ„ÄÇËøáÁ®ã‰ºöÈáçÂ§çÂ§öÊ¨°ÔºàÂØπÂ∫îÂõæÁ•ûÁªèÁΩëÁªúÁöÑÂ§ö‰∏™GCLÂ±ÇÔºâÔºå‰ø°ÊÅØÂèØ‰ª•Âú®Êï¥‰∏™ÂàÜÂ≠ê‰∏≠‰º†Êí≠„ÄÇ</p>
<ul>
<li><strong>EGNN (E(n) Equivariant Graph Neural Network)</strong>:
ÂÖ∏ÂûãÁöÑ‰ª•ÂéüÂ≠ê‰∏∫‰∏≠ÂøÉÁöÑÊ®°Âûã„ÄÇ<strong>Á≠âÂèòÊÄß
(Equivariance)</strong>ÔºåÊóãËΩ¨ÊàñÁßªÂä®Êï¥‰∏™ÂàÜÂ≠êÊó∂ÔºåÊ®°ÂûãÂÜÖÈÉ®Â≠¶‰π†Âà∞ÁöÑÂéüÂ≠êË°®Á§∫‰πü‰ºöÁõ∏Â∫îÂú∞ÊóãËΩ¨ÊàñÁßªÂä®ÔºåÊúÄÁªàÈ¢ÑÊµãÁöÑËÉΩÈáèÁ≠âÊ†áÈáèÂ±ûÊÄß‰øùÊåÅ‰∏çÂèò„ÄÇÁ¨¶ÂêàÁâ©ÁêÜËßÑÂæãÔºåÊÄßËÉΩÂá∫Ëâ≤„ÄÇÂÆÉÁõ¥Êé•Âú®ÂéüÂ≠êÁöÑ3DÂùêÊ†á‰∏äËøõË°åÊìç‰Ωú„ÄÇ</li>
<li><strong>OrbNet</strong>:
<strong>Êï∞ÊçÆÊù•Ê∫ê</strong>ÈááÁî®ÂçäÁªèÈ™åÊñπÊ≥ïÔºàGFN1-xTBÔºâÁîüÊàêÈáèÂ≠êÂåñÂ≠¶Áü©ÈòµÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨ÔºåÂêåÊó∂‰øùÁïô‰∫ÜÂÖ≥ÈîÆÁâ©ÁêÜ‰ø°ÊÅØ,ÊîØÊåÅÊï∞ÂçÉÂéüÂ≠êËßÑÊ®°ÁöÑÂàÜÂ≠êÊ®°Êãü„ÄÇ
Èó≠Â£≥Â±ÇÔºàClosed-shellÔºâ‰∏éÂºÄÂ£≥Â±ÇÔºàOpen-shellÔºâÁ≥ªÁªüÁöÑÂå∫Âà´ÔºöÈó≠Â£≥Â±ÇÁîµÂ≠êËá™ÊóãÂÖ®ÈÖçÂØπÔºà‰ªÖÈúÄËÄÉËôëÁ©∫Èó¥Ëá™Áî±Â∫¶ÔºâÔºåÂºÄÂ£≥Â±ÇÂê´Êú™ÈÖçÂØπÁîµÂ≠êÔºàÈúÄÂêåÊó∂ËÄÉËôëÁ©∫Èó¥ÂíåËá™ÊóãËá™Áî±Â∫¶ÔºâÔºåÂºÄÂ£≥Â±ÇÂú®Ëá™Áî±Âü∫„ÄÅÂèçÂ∫î‰∏≠Èó¥‰ΩìÁ≠âÂú∫ÊôØÁöÑÂÖ≥ÈîÆÊÑè‰πâ„ÄÇ
<ul>
<li><strong>key</strong>:
Âü∫‰∫éÂéüÂ≠êËΩ®ÈÅìÔºàAOÔºâÁâπÂæÅÔºàËá™Ê¥ΩÂú∫ÔºàSCFÔºâÊî∂ÊïõËøáÁ®ã‰∏≠ÁöÑÈáèÂ≠êÂåñÂ≠¶Áü©ÈòµÔºâÈ¢ÑÊµãÂàÜÂ≠êËÉΩÈáè„ÄÇ</li>
<li><strong>ÁâπÂæÅË°®Á§∫</strong>: ÈááÁî®ÂØπÁß∞ÈÄÇÈÖçÂéüÂ≠êËΩ®ÈÅìÔºàSAAOÔºâÂü∫ÁªÑÔºåÂ∞Ü AO
ÁâπÂæÅÁºñÁ†Å‰∏∫ÂõæÁªìÊûÑÊï∞ÊçÆ„ÄÇ</li>
<li><strong>Ê®°ÂûãÊû∂ÊûÑ</strong>:
Âü∫‰∫éÂõæÁ•ûÁªèÁΩëÁªúÔºàGNNÔºâÔºåËß£Á†ÅËæìÂá∫Âº†ÈáèÂπ∂Ê±ÇÂíåÂæóÂà∞ÂàÜÂ≠êËÉΩÈáè„ÄÇ</li>
</ul></li>
</ul></li>
</ul>
<h4 id="mo-molecular-orbital---ÂàÜÂ≠êËΩ®ÈÅìÂ±ÇÈù¢-‰ª•ÂàÜÂ≠ê‰∏∫Êï¥‰ΩìÁöÑÊ®°Âûã">2. MO
(Molecular Orbital) - ÂàÜÂ≠êËΩ®ÈÅìÂ±ÇÈù¢ / ‰ª•ÂàÜÂ≠ê‰∏∫Êï¥‰ΩìÁöÑÊ®°Âûã</h4>
<p>Áõ¥Êé•Â≠¶‰π†ÊàñÈ¢ÑÊµãÊï¥‰∏™ÂàÜÂ≠êÁöÑÂÖ®Â±ÄÂ±ûÊÄßÔºåÂàÜÂ≠êÊï¥‰ΩìÁîµÂ≠êÁªìÊûÑÁõ∏ÂÖ≥ÁöÑÂ±ûÊÄß„ÄÇÂàÜÂ≠êËΩ®ÈÅìÊú¨Ë∫´Â∞±ÊòØÁî±ÊâÄÊúâÂéüÂ≠êËΩ®ÈÅìÁ∫øÊÄßÁªÑÂêàËÄåÊàêÁöÑÔºåÊèèËø∞‰∫ÜÁîµÂ≠êÂú®Êï¥‰∏™ÂàÜÂ≠ê‰∏≠ÁöÑËøêÂä®Áä∂ÊÄÅ„ÄÇ</p>
<ul>
<li><strong>key</strong>:
Áõ¥Êé•ÂØπÂàÜÂ≠êÁöÑÂÖ®Â±ÄÁâπÂæÅÊàñÂÖ∂ÁîµÂ≠êÁªìÊûÑÁöÑÂÆèËßÇË°®Áé∞ÔºàÂ¶ÇËΩ®ÈÅìËÉΩÁ∫ßÔºâËøõË°åÂª∫Ê®°„ÄÇ</li>
<li><strong>ÂÖ∏ÂûãËæìÂÖ•</strong>: Êï¥‰∏™ÂàÜÂ≠êÁöÑÊèèËø∞Á¨¶Ôºà‰æãÂ¶ÇÂàÜÂ≠êÊåáÁ∫π
fingerprintÔºâÔºåÊàñËÄÖÁõ¥Êé•Â∞ÜÂàÜÂ≠êÁªìÊûÑ‰Ωú‰∏∫ËæìÂÖ•Êù•È¢ÑÊµãÂàÜÂ≠êËΩ®ÈÅìÁöÑÊÄßË¥®„ÄÇ</li>
<li><strong>Â∑•‰ΩúÊñπÂºè</strong>:
ËøôÁ±ªÊ®°ÂûãÂèØËÉΩ‰∏çÂÆåÂÖ®‰æùËµñ‰∫éÂéüÂ≠êÈó¥ÁöÑÊ∂àÊÅØ‰º†ÈÄíÔºåËÄåÊòØÊó®Âú®Áõ¥Êé•ÊûÑÂª∫‰∏Ä‰∏™‰ªéÂàÜÂ≠êÂà∞ÂÖ∂ÂÖ®Â±ÄÂ±ûÊÄßÁöÑÊò†Â∞Ñ„ÄÇ‰æãÂ¶ÇÔºåÈ¢ÑÊµãÂàÜÂ≠êÁöÑÊúÄÈ´òÂç†ÊçÆÂàÜÂ≠êËΩ®ÈÅì
(HOMO) ÂíåÊúÄ‰ΩéÊú™Âç†ÊçÆÂàÜÂ≠êËΩ®ÈÅì (LUMO) ÁöÑËÉΩÈáè„ÄÇ</li>
</ul>
<h4 id="open-shell---ÂºÄÂ£≥Â±ÇÁªÑÊÄÅ">3. Open-shell - ÂºÄÂ£≥Â±ÇÁªÑÊÄÅ</h4>
<p><a
target="_blank" rel="noopener" href="https://www.zhihu.com/question/620638044?write">Open-shell</a></p>
<p>Èöè‰æøÁúãÁúãÁöÑ‰∏ÄÁØáICML-2024 <a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=XC9IoAsyEN">ICML-WORKSHOP-2024</a></p>
<p><a
target="_blank" rel="noopener" href="https://www.nature.com/articles/s41524-022-00863-y">NPJ-2022</a></p>
<p><a
target="_blank" rel="noopener" href="https://www.pnas.org/doi/abs/10.1073/pnas.2205221119">PNAS-2022
OrbNet-Equi</a> <a
target="_blank" rel="noopener" href="https://zenodo.org/records/6568518#.YrtTKHbMK38">!!! orbnet
qm9ÁöÑgraph basedÁöÑfeature</a></p>
<h1 id="ÂçÅÊúà‰ªΩËßÑÂàí">2. ÂçÅÊúà‰ªΩËßÑÂàíÔºö</h1>
<h2 id="what-we-need-to-do">2.1 What we need to do?</h2>
<h2 id="Êàë‰ª¨ÈúÄË¶ÅÂàÜÊûêao-Âíå-mo-ÁöÑË°®Áé∞">Êàë‰ª¨ÈúÄË¶ÅÂàÜÊûêAO Âíå MO ÁöÑË°®Áé∞„ÄÇ</h2>
<p>Êàë‰ª¨‰∏çÁ°ÆÂÆöMOÂíåAOÁöÑvariabilityÁöÑÂ∑ÆÂºÇÔºåÊòØÁî±EGNNËøòÊòØGPRÂ∏¶Êù•ÁöÑ
‰ªñ‰ª¨ÁöÑinformation‰∏ç‰∏ÄÊ†∑„ÄÇ</p>
<p>Learning curve - learnabilityÂõæ</p>
<p>Êàë‰ª¨ÈúÄË¶ÅÈÄöËøáÂØπÊØîÂ≠¶‰π†ÊâæÂà∞ AO Âíå MO ÁöÑ similarity„ÄÇ</p>
<p>ÁêÜÊÉ≥ÂåñÁªìÊûúÔºö
Êàë‰ª¨Â∏åÊúõAOÈÄöËøáÂØπÊØîÂ≠¶‰π†ËææÂà∞MOÁöÑÁ®ãÂ∫¶ÔºåÊàë‰ª¨Â∏åÊúõÂØπÊØîÂ≠¶‰π†ÂØπAOÊõ¥ÊúâÁî®„ÄÇ</p>
<p>Êàë‰ª¨Â∏åÊúõËææÂà∞GCL + AO</p>
<p>AO‰ªéÁâ©ÁêÜÊÑè‰πâ‰∏äÊõ¥Êú¨Ë¥®ÔºåMOÁöÑÊÄßË¥®Êõ¥Â•Ω„ÄÇ</p>
<p>Final goal Inverse design ÈúÄË¶ÅÁîüÊàê AO</p>
<h2 id="linear-combination-of-atomic-orbitals">2.2 (Linear Combination
of Atomic Orbitals)</h2>
<p><strong>LCAO</strong> <strong>ÂéüÂ≠êËΩ®ÈÅìÁ∫øÊÄßÁªÑÂêà (Linear Combination of
Atomic Orbitals)</strong>„ÄÇ</p>
<ul>
<li><p><strong>key</strong>:
ÂàÜÂ≠êÁöÑÂ§çÊùÇË°å‰∏∫ÔºàÁî±ÂàÜÂ≠êËΩ®ÈÅìMOÊèèËø∞ÔºâÂèØ‰ª•Ëøë‰ººÂú∞ÈÄöËøáÂÖ∂ÁªÑÊàêÂéüÂ≠êÁöÑÊõ¥ÁÆÄÂçïÁöÑË°å‰∏∫ÔºàÁî±ÂéüÂ≠êËΩ®ÈÅìAOÊèèËø∞ÔºâÊù•ÊûÑÂª∫„ÄÇ‰∏Ä‰∏™<strong>ÂàÜÂ≠êËΩ®ÈÅì
(MO)</strong> ÂèØ‰ª•Ë°®Á§∫‰∏∫Â§ö‰∏™<strong>ÂéüÂ≠êËΩ®ÈÅì (AO)</strong>
ÁöÑÂä†ÊùÉÂíå„ÄÇ</p></li>
<li><p><strong>Êï∞Â≠¶ÂΩ¢Âºè</strong>: ‰∏Ä‰∏™ÂàÜÂ≠êËΩ®ÈÅì <span
class="math inline">\(\Psi_{MO}\)</span>ÔºåÂÆÉÂèØ‰ª•Ë°®Á§∫‰∏∫Ôºö <span
class="math display">\[\Psi_{MO} = c_1\phi_1 + c_2\phi_2 + \dots +
c_n\phi_n = \sum_{i=1}^{n} c_i\phi_i\]</span> ÂÖ∂‰∏≠Ôºö</p>
<ul>
<li><span class="math inline">\(\Psi_{MO}\)</span>
ÊòØ‰∏Ä‰∏™ÂàÜÂ≠êËΩ®ÈÅìÊ≥¢ÂáΩÊï∞„ÄÇ</li>
<li><span class="math inline">\(\phi_i\)</span> ÊòØÁ¨¨ <span
class="math inline">\(i\)</span> ‰∏™ÂéüÂ≠êÁöÑÂéüÂ≠êËΩ®ÈÅìÊ≥¢ÂáΩÊï∞„ÄÇ</li>
<li><span class="math inline">\(c_i\)</span>
ÊòØÊØè‰∏™ÂéüÂ≠êËΩ®ÈÅìÁöÑ<strong>ÁªÑÂêàÁ≥ªÊï∞
(coefficient)</strong>ÔºåÂÆÉÊòØ‰∏Ä‰∏™ÊùÉÈáçÂÄºÔºåË°®Á§∫ËØ•ÂéüÂ≠êËΩ®ÈÅìÂØπËøô‰∏™ÂàÜÂ≠êËΩ®ÈÅìÁöÑË¥°ÁåÆÂ§ßÂ∞è„ÄÇÁ≥ªÊï∞ÈÄöËøáÊ±ÇËß£ËñõÂÆöË∞îÊñπÁ®ãÔºàÈÄöÂ∏∏‰ΩøÁî®Hartree-FockÁ≠âËøë‰ººÊñπÊ≥ïÔºâÂæóÂà∞ÁöÑ„ÄÇ</li>
</ul></li>
<li><p><strong>ex</strong>:
Ê∞¢ÂàÜÂ≠êÔºàH‚ÇÇÔºâ„ÄÇÊúâ‰∏§‰∏™Ê∞¢ÂéüÂ≠êÔºåÊØè‰∏™Ê∞¢ÂéüÂ≠êÊúâ‰∏Ä‰∏™1sÂéüÂ≠êËΩ®ÈÅìÔºà<span
class="math inline">\(\phi_A\)</span> Âíå <span
class="math inline">\(\phi_B\)</span>Ôºâ„ÄÇËøô‰∏§‰∏™ÂéüÂ≠êËΩ®ÈÅìÂèØ‰ª•ÈÄöËøá‰∏§ÁßçÊñπÂºèÁ∫øÊÄßÁªÑÂêàÔºåÂΩ¢Êàê‰∏§‰∏™ÂàÜÂ≠êËΩ®ÈÅìÔºö</p>
<ol type="1">
<li><strong>ÊàêÈîÆËΩ®ÈÅì (Bonding MO)</strong>: <span
class="math inline">\(\Psi_{\sigma} = c_A\phi_A +
c_B\phi_B\)</span>„ÄÇÁîµÂ≠êÂ§Ñ‰∫éËøô‰∏™ËΩ®ÈÅìÊó∂Ôºå‰ºö‰∏ªË¶ÅÂàÜÂ∏ÉÂú®‰∏§‰∏™ÂéüÂ≠êÊ†∏‰πãÈó¥ÔºåÂΩ¢ÊàêÁ®≥ÂÆöÁöÑÂåñÂ≠¶ÈîÆ„ÄÇËÉΩÈáèÊØîÂéüÊù•ÁöÑAOÊõ¥‰Ωé„ÄÇ</li>
<li><strong>ÂèçÈîÆËΩ®ÈÅì (Antibonding MO)</strong>: <span
class="math inline">\(\Psi_{\sigma^*} = c&#39;_A\phi_A -
c&#39;_B\phi_B\)</span>„ÄÇÁîµÂ≠êÂ§Ñ‰∫éËøô‰∏™ËΩ®ÈÅìÊó∂Ôºå‰ºö‰∏ªË¶ÅÂàÜÂ∏ÉÂú®ÂéüÂ≠êÊ†∏ÁöÑÂ§ñ‰æßÔºåÊéíÊñ•‰∏§‰∏™ÂéüÂ≠êÊ†∏Ôºå‰∏çÂà©‰∫éÊàêÈîÆ„ÄÇËÉΩÈáèÊØîÂéüÊù•ÁöÑAOÊõ¥È´ò„ÄÇ</li>
</ol></li>
</ul>
<h3 id="localization-ÂàÜÂ≠êËΩ®ÈÅìÂ±ÄÂüüÂåñ">2.3 Localization
(ÂàÜÂ≠êËΩ®ÈÅìÂ±ÄÂüüÂåñ)</h3>
<p>ÂàÜÂ≠êËΩ®ÈÅìÔºàMOsÔºâÔºåÂ∞§ÂÖ∂ÊòØÈÄöËøáÊ†áÂáÜËÆ°ÁÆóÊñπÊ≥ïÔºàÂ¶ÇHartree-FockÔºâÁõ¥Êé•Ê±ÇËß£Âá∫Êù•ÁöÑÔºåÈÄöÂ∏∏ÊòØ<strong>Á¶ªÂüüÁöÑ
(delocalized)</strong>„ÄÇËøôÊÑèÂë≥ÁùÄÊØè‰∏™MOÈÉΩÂèØËÉΩÊâ©Â±ïÂà∞Êï¥‰∏™ÂàÜÂ≠êÔºåÁî±ÂàÜÂ≠ê‰∏≠Âá†‰πéÊâÄÊúâÂéüÂ≠êÁöÑAOsË¥°ÁåÆÊûÑÊàê„ÄÇ‰æãÂ¶ÇÔºåÂú®ËãØÁéØ‰∏≠ÔºåËÆ°ÁÆóÂá∫ÁöÑœÄÁîµÂ≠êMO‰ºöÂùáÂåÄÂú∞ÂàÜÂ∏ÉÂú®ÂÖ≠‰∏™Á¢≥ÂéüÂ≠ê‰∏ä„ÄÇ</p>
<p><strong>ÂàÜÂ≠êËΩ®ÈÅìÂ±ÄÂüüÂåñ (Localization of Molecular Orbitals)</strong>
Â∞±ÊòØ‰∏Ä‰∏™Êï∞Â≠¶ÂèòÊç¢ËøáÁ®ãÔºåÂÆÉÂ∞ÜËøô‰∫õÁ¶ªÂüüÁöÑMOsËΩ¨Âåñ‰∏∫‰∏ÄÁªÑÊñ∞ÁöÑ<strong>Â±ÄÂüüÂåñÂàÜÂ≠êËΩ®ÈÅì
(Localized Molecular Orbitals, LMOs)</strong>„ÄÇ</p>
<ul>
<li><strong>Ê†∏ÂøÉÁõÆÊ†á</strong>:
Âú®‰∏çÊîπÂèòÂàÜÂ≠êÊï¥‰ΩìÊ≥¢ÂáΩÊï∞ÂíåÊÄªËÉΩÈáèÁöÑÂâçÊèê‰∏ãÔºåÂ∞ÜÂàÜÂ≠êËΩ®ÈÅìÂ∞ΩÂèØËÉΩÂú∞ÈôêÂà∂Âú®Á©∫Èó¥‰∏≠ÁöÑ‰∏ÄÂ∞èÂùóÂå∫ÂüüÂÜÖ„ÄÇ</li>
<li><strong>ÂèòÊç¢ÁªìÊûú</strong>:
<ul>
<li>Á¶ªÂüüÁöÑÊàêÈîÆËΩ®ÈÅì <span class="math inline">\(\rightarrow\)</span>
ÂØπÂ∫î‰∫éÁâπÂÆö <strong>ÂåñÂ≠¶ÈîÆ</strong>
ÁöÑÂ±ÄÂüüËΩ®ÈÅìÔºà‰æãÂ¶ÇC-HÈîÆÔºåC=CÂèåÈîÆÔºâ„ÄÇ</li>
<li>Á¶ªÂüüÁöÑÈùûÈîÆËΩ®ÈÅì <span class="math inline">\(\rightarrow\)</span>
ÂØπÂ∫î‰∫éÁâπÂÆöÂéüÂ≠ê‰∏äÁöÑ <strong>Â≠§ÂØπÁîµÂ≠ê (lone pair)</strong> Êàñ
<strong>ÂÜÖÂ±ÇÁîµÂ≠ê</strong>„ÄÇ</li>
</ul></li>
<li><strong>Â±ÄÂüüÂåñ</strong>:
<ol type="1">
<li><strong>ÂåñÂ≠¶Áõ¥ËßÇÊÄß</strong>:
LMOsÊèê‰æõ‰∫ÜÊ∏ÖÊô∞ÁöÑÂåñÂ≠¶ÂõæÂÉèÔºå‰æø‰∫éÁêÜËß£ÂíåÂàÜÊûêÂåñÂ≠¶ÊàêÈîÆÊÉÖÂÜµ„ÄÇ</li>
</ol></li>
</ul>
<h3 id="lcao-Âíå-mo-ÁöÑÂÖ≥Á≥ª">3. LCAO Âíå MO ÁöÑÂÖ≥Á≥ª</h3>
<ol type="1">
<li><strong>LCAOÊòØÊûÑÂª∫MOÁöÑÊñπÊ≥ï</strong>:
LCAOÊòØÁî®‰∫éËøë‰ººËÆ°ÁÆóÂíåË°®Á§∫ÂàÜÂ≠êËΩ®ÈÅìÔºàMOÔºâÁöÑÊï∞Â≠¶Ê°ÜÊû∂„ÄÇÊàë‰ª¨ÂÅáËÆæMOÂèØ‰ª•Áî±‰∏ÄÁªÑÂ∑≤Áü•ÁöÑÂü∫ÂáΩÊï∞ÔºàÂç≥ÂéüÂ≠êËΩ®ÈÅìAOÔºâÁ∫øÊÄßÁªÑÂêàËÄåÊàê„ÄÇ</li>
<li><strong>MOÊòØLCAOÊñπÊ≥ïÁöÑÁªìÊûú</strong>:
ÈÄöËøáLCAOÊñπÊ≥ïÔºåÁªìÂêàÈáèÂ≠êÂäõÂ≠¶ÂèòÂàÜÂéüÁêÜÊ±ÇËß£ËñõÂÆöË∞îÊñπÁ®ãÔºåÊàë‰ª¨ÊúÄÁªàÂæóÂà∞‰∫Ü‰∏ÄÁ≥ªÂàóÂàÜÂ≠êËΩ®ÈÅìÔºàMOsÔºâÁöÑÂÖ∑‰ΩìÂΩ¢ÂºèÔºàÂç≥ÊØè‰∏™AOÁöÑË¥°ÁåÆÁ≥ªÊï∞<span
class="math inline">\(c_i\)</span>Ôºâ‰ª•ÂèäÂÆÉ‰ª¨ÁöÑËÉΩÈáè„ÄÇ</li>
</ol>
<p><strong>ÂéüÂ≠êËΩ®ÈÅì (AO) [ËæìÂÖ•]</strong> <span
class="math inline">\(\xrightarrow{\text{LCAOÊñπÊ≥ï [ËøáÁ®ã/Ê°ÜÊû∂]}}\)</span>
<strong>ÂàÜÂ≠êËΩ®ÈÅì (MO) [ËæìÂá∫/ÁªìÊûú]</strong></p>
<h3 id="È´òÊñØËøáÁ®ãÂõûÂΩí-gaussian-process-regression-gpr">4. È´òÊñØËøáÁ®ãÂõûÂΩí
(Gaussian Process Regression, GPR)</h3>
<p><strong>È´òÊñØËøáÁ®ãÂõûÂΩí (GPR)</strong>
ÊòØ‰∏ÄÁßçÂü∫‰∫éË¥ùÂè∂ÊñØÊÄùÊÉ≥ÁöÑÈùûÂèÇÊï∞ÂõûÂΩíÊñπÊ≥ï„ÄÇÂÆÉÂú®Â§ÑÁêÜÂ∞èÊ†∑Êú¨„ÄÅÈ´òÁª¥Â∫¶„ÄÅÈúÄË¶Å‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°ÁöÑÂ§çÊùÇÂõûÂΩíÈóÆÈ¢òÊó∂ÁâπÂà´ÊúâÊïà„ÄÇ</p>
<h4 id="key">key</h4>
<p><strong>GPRÁöÑÊ†∏ÂøÉÊòØÁõ¥Êé•ÂØπÂáΩÊï∞Êú¨Ë∫´ËøõË°åÂª∫Ê®°</strong>„ÄÇÂÆÉÂÅáËÆæÊàë‰ª¨ÊÉ≥Ë¶ÅÂª∫Ê®°ÁöÑÁõÆÊ†áÂáΩÊï∞
<span class="math inline">\(f(x)\)</span> ÊòØ‰∏Ä‰∏™Êúç‰ªé<strong>È´òÊñØËøáÁ®ã
(Gaussian Process, GP)</strong> ÁöÑÈöèÊú∫ÂáΩÊï∞„ÄÇ</p>
<ul>
<li><strong>È´òÊñØËøáÁ®ã (GP)</strong>
‰∏Ä‰∏™È´òÊñØËøáÁ®ãÊòØÊó†Á©∑Â§ö‰∏™ÈöèÊú∫ÂèòÈáèÁöÑÈõÜÂêàÔºåÂÖ∂‰∏≠‰ªªÊÑèÊúâÈôê‰∏™ÈöèÊú∫ÂèòÈáèÁöÑÁªÑÂêàÈÉΩÊúç‰ªé‰∏Ä‰∏™ËÅîÂêàÈ´òÊñØÂàÜÂ∏É„ÄÇ
‰∏Ä‰∏™GPÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÂÖ≥‰∫é<strong>ÂáΩÊï∞ÁöÑÂàÜÂ∏É (a distribution over
functions)</strong>„ÄÇÂΩìÊàë‰ª¨‰ªéËøô‰∏™GP‰∏≠‚ÄúÈááÊ†∑‚ÄùÊó∂ÔºåÊàë‰ª¨ÂæóÂà∞ÁöÑ‰∏çÊòØ‰∏Ä‰∏™Êï∞ÂÄºÔºåËÄåÊòØ‰∏ÄÊï¥‰∏™ÂáΩÊï∞„ÄÇ</li>
</ul>
<p>‰∏Ä‰∏™È´òÊñØËøáÁ®ãÂÆåÂÖ®Áî±‰∏§ÈÉ®ÂàÜÂÆö‰πâÔºö 1. <strong>ÂùáÂÄºÂáΩÊï∞ (Mean Function)
<span class="math inline">\(m(x)\)</span></strong>:
ÂÆö‰πâ‰∫ÜÂáΩÊï∞ÂàÜÂ∏ÉÁöÑ‚ÄúÊúüÊúõ‚ÄùÊàñ‚Äú‰∏≠ÂøÉË∂ãÂäø‚Äù„ÄÇÈÄöÂ∏∏‰∏∫‰∫ÜÁÆÄÂåñÔºå‰ºöÂÅáËÆæÂùáÂÄº‰∏∫Èõ∂„ÄÇ 2.
<strong>ÂçèÊñπÂ∑ÆÂáΩÊï∞ (Covariance Function) Êàñ Ê†∏ÂáΩÊï∞ (Kernel) <span
class="math inline">\(k(x, x&#39;)\)</span></strong>:
ÂÆö‰πâ‰∫ÜÂáΩÊï∞Âú®‰∏çÂêåËæìÂÖ•ÁÇπ <span class="math inline">\(x\)</span> Âíå <span
class="math inline">\(x&#39;\)</span>
Â§ÑÁöÑÂÄº‰πãÈó¥ÁöÑ‚ÄúÁõ∏ÂÖ≥ÊÄß‚ÄùÊàñ‚ÄúÁõ∏‰ººÊÄß‚Äù„ÄÇÂ¶ÇÊûú <span
class="math inline">\(x\)</span> Âíå <span
class="math inline">\(x&#39;\)</span> ÂæàÊé•ËøëÔºåÊ†∏ÂáΩÊï∞ÁöÑÂÄºÂ∞±ÂæàÂ§ßÔºåÊÑèÂë≥ÁùÄ
<span class="math inline">\(f(x)\)</span> Âíå <span
class="math inline">\(f(x&#39;)\)</span>
ÁöÑÂÄº‰ºöÂæàÁõ∏‰ºº„ÄÇËøôÁºñÁ†Å‰∫ÜÊàë‰ª¨ÂØπÂáΩÊï∞Âπ≥ÊªëÊÄßÁöÑÂÖàÈ™å‰ø°Âøµ„ÄÇ</p>
<h4 id="gpr-Â∑•‰Ωú">GPR Â∑•‰Ωú</h4>
<p>GPRÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºö</p>
<p><strong>Á¨¨‰∏ÄÊ≠•ÔºöÂÆö‰πâÂÖàÈ™åÂàÜÂ∏É (Prior Distribution)</strong>
Âú®ÁúãÂà∞‰ªª‰ΩïËÆ≠ÁªÉÊï∞ÊçÆ‰πãÂâçÔºåÊàë‰ª¨È¶ñÂÖàÊ†πÊçÆÂÖàÈ™åÁü•ËØÜÈÄâÊã©‰∏Ä‰∏™ÂùáÂÄºÂáΩÊï∞ÔºàÈÄöÂ∏∏‰∏∫0ÔºâÂíå‰∏Ä‰∏™Ê†∏ÂáΩÊï∞Ôºà‰æãÂ¶ÇÂ∏∏Áî®ÁöÑ<strong>ÂæÑÂêëÂü∫ÂáΩÊï∞Ê†∏/RBFÊ†∏</strong>Ôºâ„ÄÇËøô‰∏™GPÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÂáΩÊï∞ÁöÑÂÖàÈ™åÂàÜÂ∏ÉÔºåÂåÖÂê´‰∫ÜÊàë‰ª¨ËÉΩÊÉ≥Âà∞ÁöÑÊâÄÊúâ‚ÄúÂèØËÉΩ‚ÄùÁöÑÂáΩÊï∞„ÄÇ</p>
<p><strong>Á¨¨‰∫åÊ≠•ÔºöËÆ°ÁÆóÂêéÈ™åÂàÜÂ∏É (Posterior Distribution)</strong>
ÂΩìÊàë‰ª¨ÂæóÂà∞‰∏ÄÁªÑËÆ≠ÁªÉÊï∞ÊçÆ <span class="math inline">\((X_{train},
Y_{train})\)</span>
ÂêéÔºåÊàë‰ª¨Âà©Áî®Ë¥ùÂè∂ÊñØÂÆöÁêÜÊù•Êõ¥Êñ∞Êàë‰ª¨ÁöÑÂáΩÊï∞ÂàÜÂ∏É„ÄÇÊàë‰ª¨‰ªéÂÖàÈ™åÂàÜÂ∏É‰∏≠‚ÄúÁ≠õÈÄâ‚ÄùÊéâÈÇ£‰∫õ‰∏éËÆ≠ÁªÉÊï∞ÊçÆ‰∏çÁ¨¶ÁöÑÂáΩÊï∞ÔºåÂæóÂà∞‰∏Ä‰∏™<strong>ÂêéÈ™åÂàÜÂ∏É
(Posterior Distribution)</strong>„ÄÇ</p>
<p>Ëøô‰∏™ÂêéÈ™åÂàÜÂ∏É‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™È´òÊñØËøáÁ®ãÔºåÂÖ∂ÂùáÂÄºÂíåÂçèÊñπÂ∑ÆÊúâËß£ÊûêËß£ÔºàÂèØ‰ª•Áõ¥Êé•ËÆ°ÁÆóÂá∫Êù•ÔºâÔºå‰∏çÈúÄË¶ÅÂ§çÊùÇÁöÑËø≠‰ª£‰ºòÂåñ„ÄÇ</p>
<h4 id="ËøõË°åÈ¢ÑÊµã">ËøõË°åÈ¢ÑÊµã</h4>
<p>ÂØπ‰∫é‰∏Ä‰∏™Êñ∞ÁöÑÊµãËØïÁÇπ <span
class="math inline">\(x_{test}\)</span>ÔºåÊàë‰ª¨ÊÉ≥È¢ÑÊµãÂØπÂ∫îÁöÑ <span
class="math inline">\(y_{test}\)</span>„ÄÇÂú®ÂêéÈ™åÂàÜÂ∏É‰∏ãÔºå<span
class="math inline">\(y_{test}\)</span>
ÁöÑÈ¢ÑÊµãÂÄºÊúç‰ªé‰∏Ä‰∏™‰∏ÄÁª¥È´òÊñØÂàÜÂ∏ÉÔºåËøô‰∏™ÂàÜÂ∏ÉÊúâÔºö 1. <strong>È¢ÑÊµãÂùáÂÄº
(Predicted Mean)</strong>: ËøôÂ∞±ÊòØÊàë‰ª¨ÂØπ <span
class="math inline">\(y_{test}\)</span>
ÁöÑÊúÄ‰Ω≥ÁÇπ‰º∞ËÆ°„ÄÇÂÆÉÊòØÁî±ËÆ≠ÁªÉÊï∞ÊçÆÁÇπÁöÑÂä†ÊùÉÂπ≥ÂùáËÆ°ÁÆóÂæóÂá∫ÁöÑÔºåÊùÉÈáçÁî±Ê†∏ÂáΩÊï∞ÂÜ≥ÂÆö„ÄÇ
2. <strong>È¢ÑÊµãÊñπÂ∑Æ (Predicted Variance)</strong>:
ËøôË°°Èáè‰∫ÜÊàë‰ª¨ÂØπÈ¢ÑÊµãÁªìÊûúÁöÑ<strong>‰∏çÁ°ÆÂÆöÊÄß</strong>„ÄÇÂú®Èù†ËøëËÆ≠ÁªÉÊï∞ÊçÆÁÇπÁöÑÂú∞ÊñπÔºåÊñπÂ∑Æ‰ºöÂæàÂ∞èÔºàÈ¢ÑÊµãÂæàËá™‰ø°ÔºâÔºõÂú®ËøúÁ¶ªËÆ≠ÁªÉÊï∞ÊçÆÁÇπÁöÑÊú™Áü•Âå∫ÂüüÔºåÊñπÂ∑Æ‰ºöÂæàÂ§ßÔºàÈ¢ÑÊµãÂæà‰∏çÁ°ÆÂÆöÔºâ„ÄÇ</p>
<h1 id="ÂêéÁª≠ËßÑÂàí">3. ÂêéÁª≠ËßÑÂàíÔºö</h1>
<p>IF AO ÁöÑÂ≠¶‰π†Ë°®Áé∞ÊØî MO Ë¶ÅÂ•Ω Êàë‰ª¨Â∞Ü‰ºöËÅöÁÑ¶‰∫é AO ÔºàAtomic representation
vs atomic orbitalÔºâ</p>
<ol type="1">
<li><strong>AO one body decomposition</strong></li>
<li><strong>MO two body decomposition</strong></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="‰∏ã‰∏ÄÈ°µ"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          ÊñáÁ´†ÁõÆÂΩï
        </li>
        <li class="sidebar-nav-overview">
          Á´ôÁÇπÊ¶ÇËßà
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">Êó•Âøó</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">ÂàÜÁ±ª</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">Ê†áÁ≠æ</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Áî± <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> Âº∫ÂäõÈ©±Âä®
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
