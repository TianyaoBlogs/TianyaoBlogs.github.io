<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianyaoblogs.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="TianyaoBlogs">
<meta property="og:url" content="https://tianyaoblogs.github.io/index.html">
<meta property="og:site_name" content="TianyaoBlogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://tianyaoblogs.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>TianyaoBlogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TianyaoBlogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/13/5054C6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/13/5054C6/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L6</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-10-13 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-13T21:00:00+08:00">2025-10-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-10-20 03:45:06" itemprop="dateModified" datetime="2025-10-20T03:45:06+08:00">2025-10-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-6</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1
id="linear-model-selection-and-regularization-çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–">1.
Linear Model Selection and Regularization çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–</h1>
<h2 id="summary-of-core-concepts">Summary of Core Concepts</h2>
<p><strong>Chapter 6: Linear Model Selection and
Regularization</strong>, focusing specifically on <strong>Section 6.1:
Subset Selection</strong>.
<strong>ç¬¬å…­ç« ï¼šçº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–</strong>ï¼Œ<strong>6.1èŠ‚ï¼šå­é›†é€‰æ‹©</strong></p>
<ul>
<li><p><strong>The Problem:</strong> You have a dataset with many
potential predictor variables (features). If you include all of them
(like <strong>Model 1</strong> with <span
class="math inline">\(p\)</span> predictors in slide
<code>...221320.png</code>), you risk including â€œnoiseâ€ variables. These
irrelevant features can decrease model accuracy (overfitting) and make
the model difficult to interpret.
æ•°æ®é›†åŒ…å«è®¸å¤šæ½œåœ¨çš„é¢„æµ‹å˜é‡ï¼ˆç‰¹å¾ï¼‰ã€‚å¦‚æœåŒ…å«æ‰€æœ‰è¿™äº›å˜é‡ï¼ˆä¾‹å¦‚å¹»ç¯ç‰‡â€œâ€¦221320.pngâ€ä¸­å¸¦æœ‰<span
class="math inline">\(p\)</span>ä¸ªé¢„æµ‹å˜é‡çš„<strong>æ¨¡å‹1</strong>ï¼‰ï¼Œåˆ™å¯èƒ½ä¼šåŒ…å«â€œå™ªå£°â€å˜é‡ã€‚è¿™äº›ä¸ç›¸å…³çš„ç‰¹å¾ä¼šé™ä½æ¨¡å‹çš„å‡†ç¡®ç‡ï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼Œå¹¶ä½¿æ¨¡å‹éš¾ä»¥è§£é‡Šã€‚</p></li>
<li><p><strong>The Goal:</strong> Identify a smaller subset of variables
that are truly related to the response. This creates a simpler, more
interpretable, and often more accurate model (like <strong>Model
2</strong> with <span class="math inline">\(q\)</span> predictors).
æ‰¾å‡ºä¸€ä¸ªä¸å“åº”çœŸæ­£ç›¸å…³çš„è¾ƒå°å˜é‡å­é›†ã€‚è¿™å°†åˆ›å»ºä¸€ä¸ªæ›´ç®€å•ã€æ›´æ˜“äºè§£é‡Šä¸”é€šå¸¸æ›´å‡†ç¡®çš„æ¨¡å‹ï¼ˆä¾‹å¦‚å¸¦æœ‰<span
class="math inline">\(q\)</span>ä¸ªé¢„æµ‹å˜é‡çš„<strong>æ¨¡å‹2</strong>ï¼‰ã€‚</p></li>
<li><p><strong>The Main Method Discussed: Best Subset
Selection</strong></p></li>
<li><p><strong>ä¸»è¦è®¨è®ºçš„æ–¹æ³•ï¼šæœ€ä½³å­é›†é€‰æ‹©</strong> This is an
<em>exhaustive search</em> algorithm. It checks <em>every possible
combination</em> of predictors to find the â€œbestâ€ model. With <span
class="math inline">\(p\)</span> variables, this means checking <span
class="math inline">\(2^p\)</span> total models.
è¿™æ˜¯ä¸€ç§<em>ç©·ä¸¾æœç´¢</em>ç®—æ³•ã€‚å®ƒæ£€æŸ¥<em>æ‰€æœ‰å¯èƒ½çš„é¢„æµ‹å˜é‡ç»„åˆ</em>ï¼Œä»¥æ‰¾åˆ°â€œæœ€ä½³â€æ¨¡å‹ã€‚å¯¹äº
<span class="math inline">\(p\)</span> ä¸ªå˜é‡ï¼Œè¿™æ„å‘³ç€éœ€è¦æ£€æŸ¥æ€»å…±
<span class="math inline">\(2^p\)</span> ä¸ªæ¨¡å‹ã€‚</p>
<p>The algorithm (from slide <code>...221333.png</code>) works in three
steps:</p>
<ol type="1">
<li><p><strong>Step 1:</strong> Fit the â€œnull modelâ€ <span
class="math inline">\(M_0\)</span>, which has no predictors (it just
predicts the average of the response). æ‹Ÿåˆâ€œç©ºæ¨¡å‹â€<span
class="math inline">\(M_0\)</span>ï¼Œå®ƒæ²¡æœ‰é¢„æµ‹å˜é‡ï¼ˆå®ƒåªé¢„æµ‹å“åº”çš„å¹³å‡å€¼ï¼‰ã€‚</p></li>
<li><p><strong>Step 2:</strong> For each <span
class="math inline">\(k\)</span> (from 1 to <span
class="math inline">\(p\)</span>):</p>
<ul>
<li><p>Fit all <span class="math inline">\(\binom{p}{k}\)</span> models
that contain exactly <span class="math inline">\(k\)</span> predictors.
(e.g., fit all models with 1 predictor, then all models with 2
predictors, etc.).</p></li>
<li><p>æ‹Ÿåˆæ‰€æœ‰åŒ…å« <span class="math inline">\(k\)</span> ä¸ªé¢„æµ‹å˜é‡çš„
<span class="math inline">\(\binom{p}{k}\)</span>
ä¸ªæ¨¡å‹ã€‚ï¼ˆä¾‹å¦‚ï¼Œå…ˆæ‹Ÿåˆæ‰€æœ‰åŒ…å« 1 ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œç„¶åæ‹Ÿåˆæ‰€æœ‰åŒ…å« 2
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œç­‰ç­‰ï¼‰ã€‚</p></li>
<li><p>From this group, select the single best model <em>for that size
<span class="math inline">\(k\)</span></em>. This â€œbestâ€ model is the
one with the highest <strong><span
class="math inline">\(R^2\)</span></strong> (or lowest
<strong>RSS</strong> - Residual Sum of Squares) on the <em>training
data</em>. Call this model <span
class="math inline">\(M_k\)</span>.</p></li>
<li><p>ä»è¿™ç»„ä¸­ï¼Œé€‰æ‹© <em>å¯¹äºè¯¥è§„æ¨¡ <span
class="math inline">\(k\)</span></em> çš„æœ€ä½³æ¨¡å‹ã€‚è¿™ä¸ªâ€œæœ€ä½³â€æ¨¡å‹æ˜¯åœ¨
<em>è®­ç»ƒæ•°æ®</em> ä¸Šå…·æœ‰æœ€é«˜ <strong><span
class="math inline">\(R^2\)</span></strong>ï¼ˆæˆ–æœ€ä½ <strong>RSS</strong>
- æ®‹å·®å¹³æ–¹å’Œï¼‰çš„æ¨¡å‹ã€‚å°†æ­¤æ¨¡å‹ç§°ä¸º <span
class="math inline">\(M_k\)</span>ã€‚</p></li>
</ul></li>
<li><p><strong>Step 3:</strong> You now have <span
class="math inline">\(p+1\)</span> models: <span
class="math inline">\(M_0, M_1, \dots, M_p\)</span>. You must select the
single best one from this list. To do this, you <strong>cannot</strong>
use training <span class="math inline">\(R^2\)</span> (as it will always
pick the biggest model <span class="math inline">\(M_p\)</span>).
Instead, you must use a metric that estimates <em>test error</em>, such
as: <strong>ç°åœ¨ä½ æœ‰ <span class="math inline">\(p+1\)</span>
ä¸ªæ¨¡å‹ï¼š<span class="math inline">\(M_0, M_1, \dots,
M_p\)</span>ã€‚ä½ å¿…é¡»ä»åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä½³æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œä½ </strong>ä¸èƒ½**ä½¿ç”¨è®­ç»ƒ
<span class="math inline">\(R^2\)</span>ï¼ˆå› ä¸ºå®ƒæ€»æ˜¯ä¼šé€‰æ‹©æœ€å¤§çš„æ¨¡å‹
<span
class="math inline">\(M_p\)</span>ï¼‰ã€‚ç›¸åï¼Œä½ å¿…é¡»ä½¿ç”¨ä¸€ä¸ªèƒ½å¤Ÿä¼°è®¡<em>æµ‹è¯•è¯¯å·®</em>çš„æŒ‡æ ‡ï¼Œä¾‹å¦‚ï¼š</p>
<ul>
<li><strong>Cross-Validation (CV) äº¤å‰éªŒè¯ (CV)</strong> (This is what
the Python code uses)</li>
<li><strong>AIC</strong> (Akaike Information Criterion
èµ¤æ± ä¿¡æ¯å‡†åˆ™)</li>
<li><strong>BIC</strong> (Bayesian Information Criterion
è´å¶æ–¯ä¿¡æ¯å‡†åˆ™)</li>
<li><strong>Adjusted <span class="math inline">\(R^2\)</span> è°ƒæ•´åçš„
<span class="math inline">\(R^2\)</span></strong></li>
</ul></li>
</ol></li>
<li><p><strong>Key Takeaway:</strong> The slides show this â€œsubset
selectionâ€ concept can be applied <em>beyond</em> linear models. The
Python code demonstrates this by applying best subset selection to a
<strong>K-Nearest Neighbors (KNN) Regressor</strong>, a non-linear
model.â€œå­é›†é€‰æ‹©â€çš„æ¦‚å¿µå¯ä»¥åº”ç”¨äºçº¿æ€§æ¨¡å‹<em>ä¹‹å¤–</em>ã€‚</p></li>
</ul>
<h2
id="mathematical-understanding-key-questions-æ•°å­¦ç†è§£ä¸å…³é”®é—®é¢˜">Mathematical
Understanding &amp; Key Questions æ•°å­¦ç†è§£ä¸å…³é”®é—®é¢˜</h2>
<p>This section directly answers the questions posed on your slides.</p>
<h3 id="how-to-compare-which-model-is-better">How to compare which model
is better?</h3>
<p>(From slides <code>...221320.png</code> and
<code>...221326.png</code>)</p>
<p>You cannot use <strong>training error</strong> (like <span
class="math inline">\(R^2\)</span> or RSS) to compare models with
<em>different numbers of predictors</em>. A model with more predictors
will almost always have a better <em>training</em> score, even if those
extra predictors are just noise. This is called
<strong>overfitting</strong>. ä¸èƒ½ä½¿ç”¨<strong>è®­ç»ƒè¯¯å·®</strong>ï¼ˆä¾‹å¦‚
<span class="math inline">\(R^2\)</span> æˆ–
RSSï¼‰æ¥æ¯”è¾ƒå…·æœ‰<em>ä¸åŒæ•°é‡é¢„æµ‹å˜é‡</em>çš„æ¨¡å‹ã€‚å…·æœ‰æ›´å¤šé¢„æµ‹å˜é‡çš„æ¨¡å‹å‡ ä¹æ€»æ˜¯å…·æœ‰æ›´å¥½çš„<em>è®­ç»ƒ</em>åˆ†æ•°ï¼Œå³ä½¿è¿™äº›é¢å¤–çš„é¢„æµ‹å˜é‡åªæ˜¯å™ªå£°ã€‚è¿™è¢«ç§°ä¸º<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚</p>
<p>To compare models of different sizes (like Model 1 vs.Â Model 2, or
<span class="math inline">\(M_2\)</span> vs.Â <span
class="math inline">\(M_5\)</span>), you <strong>must</strong> use a
method that estimates <strong>test error</strong> (how the model
performs on new, unseen data). The slides mention:
è¦æ¯”è¾ƒä¸åŒå¤§å°çš„æ¨¡å‹ï¼ˆä¾‹å¦‚æ¨¡å‹ 1 ä¸æ¨¡å‹ 2ï¼Œæˆ– <span
class="math inline">\(M_2\)</span> ä¸ <span
class="math inline">\(M_5\)</span>ï¼‰ï¼Œæ‚¨<strong>å¿…é¡»</strong>ä½¿ç”¨ä¸€ç§ä¼°ç®—<strong>æµ‹è¯•è¯¯å·®</strong>ï¼ˆæ¨¡å‹åœ¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ä¸Šçš„è¡¨ç°ï¼‰çš„æ–¹æ³•ã€‚</p>
<ul>
<li><p><strong>Cross-Validation (CV):</strong> This is the gold
standard. You split your data into â€œfolds,â€ train the model on some
folds, and test it on the remaining fold. You repeat this and average
the test scores. The model with the best (e.g., lowest) average CV error
is chosen.
å°†æ•°æ®åˆ†æˆâ€œæŠ˜å â€ï¼Œåœ¨ä¸€äº›æŠ˜å ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨å‰©ä½™çš„æŠ˜å ä¸Šæµ‹è¯•æ¨¡å‹ã€‚é‡å¤æ­¤æ“ä½œå¹¶å–æµ‹è¯•åˆ†æ•°çš„å¹³å‡å€¼ã€‚é€‰æ‹©å¹³å‡
CV è¯¯å·®æœ€å°ï¼ˆä¾‹å¦‚ï¼Œæœ€å°ï¼‰çš„æ¨¡å‹ã€‚</p></li>
<li><p><strong>AIC &amp; BIC:</strong> These are mathematical
adjustments to the training error (like RSS) that add a <em>penalty</em>
for having more predictors. They balance model <em>fit</em> with model
<em>complexity</em>. è¿™äº›æ˜¯å¯¹è®­ç»ƒè¯¯å·®ï¼ˆå¦‚
RSSï¼‰çš„æ•°å­¦è°ƒæ•´ï¼Œä¼šå› é¢„æµ‹å˜é‡è¾ƒå¤šè€Œå¢åŠ <em>æƒ©ç½š</em>ã€‚å®ƒä»¬å¹³è¡¡äº†æ¨¡å‹<em>æ‹Ÿåˆåº¦</em>å’Œæ¨¡å‹<em>å¤æ‚åº¦</em>ã€‚</p></li>
</ul>
<h3 id="why-use-r2-in-step-2">Why use <span
class="math inline">\(R^2\)</span> in Step 2?</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p>In Step 2, you are only comparing models <strong>of the same
size</strong> (i.e., all models that have exactly <span
class="math inline">\(k\)</span> predictors). For models with the same
number of parameters, a higher <span class="math inline">\(R^2\)</span>
(or lower RSS) on the training data directly corresponds to a better
fit. You donâ€™t need to penalize for complexity because all models being
compared <em>have the same complexity</em>.
åªæ¯”è¾ƒ<strong>å¤§å°ç›¸åŒ</strong>çš„æ¨¡å‹ï¼ˆå³æ‰€æœ‰æ°å¥½å…·æœ‰ <span
class="math inline">\(k\)</span>
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼‰ã€‚å¯¹äºå‚æ•°æ•°é‡ç›¸åŒçš„æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®ä¸Šæ›´é«˜çš„ <span
class="math inline">\(R^2\)</span>ï¼ˆæˆ–æ›´ä½çš„
RSSï¼‰ç›´æ¥å¯¹åº”ç€æ›´å¥½çš„æ‹Ÿåˆåº¦ã€‚æ‚¨ä¸éœ€è¦å¯¹å¤æ‚åº¦è¿›è¡Œæƒ©ç½šï¼Œå› ä¸ºæ‰€æœ‰è¢«æ¯”è¾ƒçš„æ¨¡å‹<em>éƒ½å…·æœ‰ç›¸åŒçš„å¤æ‚åº¦</em>ã€‚</p>
<h3 id="why-cant-we-use-training-error-in-step-3">Why canâ€™t we use
training error in Step 3?</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p>In Step 3, you are comparing models <strong>of different
sizes</strong> (<span class="math inline">\(M_0\)</span> vs.Â <span
class="math inline">\(M_1\)</span> vs.Â <span
class="math inline">\(M_2\)</span>, etc.). As you add predictors, the
training <span class="math inline">\(R^2\)</span> will <em>always</em>
go up (or stay the same), and the training RSS will <em>always</em> go
down (or stay the same). If you used <span
class="math inline">\(R^2\)</span> to pick the best model in Step 3, you
would <em>always</em> pick the most complex model <span
class="math inline">\(M_p\)</span>, which is almost certainly overfit.
å°†æ¯”è¾ƒ<strong>ä¸åŒå¤§å°</strong>çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ <span
class="math inline">\(M_0\)</span> vs.Â <span
class="math inline">\(M_1\)</span> vs.Â <span
class="math inline">\(M_2\)</span> ç­‰ï¼‰ã€‚éšç€æ‚¨æ·»åŠ é¢„æµ‹å˜é‡ï¼Œè®­ç»ƒ <span
class="math inline">\(R^2\)</span>
å°†<em>å§‹ç»ˆ</em>ä¸Šå‡ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ï¼Œè€Œè®­ç»ƒ RSS
å°†<em>å§‹ç»ˆ</em>ä¸‹é™ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ã€‚å¦‚æœæ‚¨åœ¨æ­¥éª¤ 3 ä¸­ä½¿ç”¨ <span
class="math inline">\(R^2\)</span>
æ¥é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼Œé‚£ä¹ˆæ‚¨<em>å§‹ç»ˆ</em>ä¼šé€‰æ‹©æœ€å¤æ‚çš„æ¨¡å‹ <span
class="math inline">\(M_p\)</span>ï¼Œè€Œè¯¥æ¨¡å‹å‡ ä¹è‚¯å®šä¼šè¿‡æ‹Ÿåˆã€‚</p>
<p>Therefore, you <em>must</em> use a metric that estimates test error
(like CV) or penalizes for complexity (like AIC, BIC, or Adjusted <span
class="math inline">\(R^2\)</span>) to find the right balance between
fit and simplicity. å› æ­¤ï¼Œæ‚¨<em>å¿…é¡»</em>ä½¿ç”¨ä¸€ä¸ªå¯ä»¥ä¼°ç®—æµ‹è¯•è¯¯å·®ï¼ˆä¾‹å¦‚
CVï¼‰æˆ–æƒ©ç½šå¤æ‚åº¦ï¼ˆä¾‹å¦‚ AICã€BIC æˆ–è°ƒæ•´åçš„ <span
class="math inline">\(R^2\)</span>ï¼‰çš„æŒ‡æ ‡æ¥æ‰¾åˆ°æ‹Ÿåˆåº¦å’Œç®€å•æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚</p>
<h2 id="code-analysis">Code Analysis</h2>
<p>The Python code (slides <code>...221249.jpg</code> and
<code>...221303.jpg</code>) implements the <strong>Best Subset
Selection</strong> algorithm using <strong>KNN Regression</strong>.</p>
<h3 id="key-functions">Key Functions</h3>
<ul>
<li><code>main()</code>:
<ol type="1">
<li><strong>Loads Data:</strong> Reads the <code>Credit.csv</code>
file.</li>
<li><strong>Preprocesses Data:</strong>
<ul>
<li>Converts categorical features (â€˜Genderâ€™, â€˜Studentâ€™, â€˜Marriedâ€™,
â€˜Ethnicityâ€™) into numerical ones (dummy variables).
å°†åˆ†ç±»ç‰¹å¾ï¼ˆâ€œæ€§åˆ«â€ã€â€œå­¦ç”Ÿâ€ã€â€œå·²å©šâ€ã€â€œç§æ—â€ï¼‰è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾ï¼ˆè™šæ‹Ÿå˜é‡ï¼‰ã€‚</li>
<li>Creates the feature matrix <code>X</code> and target variable
<code>y</code> (â€˜Balanceâ€™). åˆ›å»ºç‰¹å¾çŸ©é˜µ <code>X</code> å’Œç›®æ ‡å˜é‡
<code>y</code>ï¼ˆâ€œä½™é¢â€ï¼‰ã€‚</li>
<li><strong>Scales</strong> the features using
<code>StandardScaler</code>. This is crucial for KNN, which is sensitive
to the scale of features. ç”¨ <code>StandardScaler</code>
å¯¹ç‰¹å¾è¿›è¡Œ<strong>ç¼©æ”¾</strong>ã€‚è¿™å¯¹äº KNN
è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¯¹ç‰¹å¾çš„ç¼©æ”¾éå¸¸æ•æ„Ÿã€‚</li>
</ul></li>
<li><strong>Adds Noise (in the second example):</strong> Slide
<code>...221303.jpg</code> shows code that <em>adds 20 new â€œnoisyâ€
columns</em> to the data. This is to test if the selection algorithm is
smart enough to ignore them. å‘æ•°æ®ä¸­æ·»åŠ  20
ä¸ªæ–°çš„â€œå™ªå£°â€åˆ—çš„ä»£ç ã€‚è¿™æ˜¯ä¸ºäº†æµ‹è¯•é€‰æ‹©ç®—æ³•æ˜¯å¦è¶³å¤Ÿæ™ºèƒ½ï¼Œèƒ½å¤Ÿå¿½ç•¥å®ƒä»¬ã€‚</li>
<li><strong>Runs Selection:</strong> Calls
<code>best_subset_selection_parallel</code> to do the main work.</li>
<li><strong>Prints Results:</strong> Finds the best subset (lowest
error) and prints the top 20 best-performing subsets.
æ‰¾åˆ°æœ€ä½³å­é›†ï¼ˆè¯¯å·®æœ€å°ï¼‰ï¼Œå¹¶æ‰“å°å‡ºè¡¨ç°æœ€ä½³çš„å‰ 20 ä¸ªå­é›†ã€‚</li>
<li><strong>Final Evaluation:</strong> It re-trains a KNN model on
<em>only</em> the best subset and calculates the final cross-validated
RMSE. ä»…åŸºäºæœ€ä½³å­é›†é‡æ–°è®­ç»ƒ KNN æ¨¡å‹ï¼Œå¹¶è®¡ç®—æœ€ç»ˆçš„äº¤å‰éªŒè¯ RMSEã€‚</li>
</ol></li>
<li><code>evaluate_subset(subset, ...)</code>:
<ul>
<li>This is the â€œworkerâ€ function. Itâ€™s called for <em>every single</em>
possible subset.</li>
<li>It takes a <code>subset</code> (a list of feature names, e.g.,
<code>['Income', 'Limit']</code>).</li>
<li>It creates a new <code>X_subset</code> containing <em>only</em>
those columns.</li>
<li>It runs 5-fold cross-validation (<code>cross_val_score</code>) on a
KNN model using this <code>X_subset</code>.</li>
<li>It uses <code>'neg_mean_squared_error'</code> as the metric. This is
negative MSE; a <em>higher</em> score (closer to 0) is better.
å®ƒä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„â€œX_subsetâ€<em>ï¼Œä»…åŒ…å«è¿™äº›åˆ—ã€‚ å®ƒä¼šä½¿ç”¨æ­¤â€œX_subsetâ€åœ¨
KNN æ¨¡å‹ä¸Šè¿è¡Œ 5 å€äº¤å‰éªŒè¯ï¼ˆâ€œcross_val_scoreâ€ï¼‰ã€‚
å®ƒä½¿ç”¨â€œneg_mean_squared_errorâ€ä½œä¸ºåº¦é‡æ ‡å‡†ã€‚è¿™æ˜¯è´Ÿ
MSEï¼›</em>æ›´é«˜*çš„åˆ†æ•°ï¼ˆè¶Šæ¥è¿‘ 0ï¼‰è¶Šå¥½ã€‚</li>
<li>It returns the subset and its average CV score.</li>
</ul></li>
<li><code>best_subset_selection_parallel(model, ...)</code>:
<ul>
<li>This is the â€œmanagerâ€ function.è¿™æ˜¯â€œç®¡ç†å™¨â€å‡½æ•°ã€‚</li>
<li>It iterates from <code>k=1</code> up to the total number of
features.å®ƒä»â€œk=1â€è¿­ä»£åˆ°ç‰¹å¾æ€»æ•°ã€‚</li>
<li>For each <code>k</code>, it generates <em>all combinations</em> of
features of that size (this is the <span
class="math inline">\(\binom{p}{k}\)</span> part).
å¯¹äºæ¯ä¸ªâ€œkâ€ï¼Œå®ƒä¼šç”Ÿæˆè¯¥å¤§å°çš„ç‰¹å¾çš„<em>æ‰€æœ‰ç»„åˆ</em>ï¼ˆè¿™æ˜¯ <span
class="math inline">\(\binom{p}{k}\)</span> éƒ¨åˆ†ï¼‰ã€‚</li>
<li>It uses <code>Parallel</code> and <code>delayed</code> (from
<code>joblib</code>) to run <code>evaluate_subset</code> for all these
combinations <em>in parallel</em>, speeding up the process
significantly. å®ƒä½¿ç”¨ <code>Parallel</code> å’Œ
<code>delayed</code>ï¼ˆæ¥è‡ª
<code>joblib</code>ï¼‰å¯¹æ‰€æœ‰è¿™äº›ç»„åˆ<em>å¹¶è¡Œ</em>è¿è¡Œ
<code>evaluate_subset</code>ï¼Œä»è€Œæ˜¾è‘—åŠ å¿«äº†å¤„ç†é€Ÿåº¦ã€‚</li>
<li>It collects all the results and returns
them.å®ƒæ”¶é›†æ‰€æœ‰ç»“æœå¹¶è¿”å›ã€‚</li>
</ul></li>
</ul>
<h3 id="analysis-of-the-output">Analysis of the Output</h3>
<ul>
<li><strong>Slide <code>...221255.png</code> (Original Data):</strong>
<ul>
<li>The code runs subset selection on the original dataset.</li>
<li>The â€œTop 20 Best Feature Subsetsâ€ are shown. The CV scores are
negative (they are <code>neg_mean_squared_error</code>), so the scores
<em>closest to zero</em> (smallest magnitude) are best.</li>
<li>The <strong>Best feature subset</strong> is found to be
<code>('Income', 'Limit', 'Rating', 'Student')</code>.</li>
<li>The final cross-validated RMSE for this model is
<strong>105.41</strong>.</li>
</ul></li>
<li><strong>Slide <code>...221309.png</code> (Data with 20 Noisy
Variables):</strong>
<ul>
<li>The code is re-run after adding 20 useless â€œNoisyâ€ features.</li>
<li>The algorithm <em>still</em> works. It correctly identifies that the
â€œNoisyâ€ variables are useless.</li>
<li>The <strong>Best feature subset</strong> is now
<code>('Income', 'Limit', 'Student')</code>. (Note: â€˜Ratingâ€™ was
dropped, likely because itâ€™s highly correlated with â€˜Limitâ€™, and the
noisy data made the simpler model perform slightly better in CV).</li>
<li>The final RMSE is <strong>114.94</strong>. This is <em>higher</em>
than the original 105.41, which is expectedâ€”the presence of so many
noise variables makes the selection problem harder, but the final model
is still good and, most importantly, <em>it successfully excluded all 20
noisy features</em>. æœ€ç»ˆçš„ RMSE ä¸º <strong>114.94</strong>ã€‚è¿™æ¯”æœ€åˆçš„
105.41<em>æ›´é«˜</em>ï¼Œè¿™æ˜¯é¢„æœŸçš„â€”â€”å¦‚æ­¤å¤šçš„å™ªå£°å˜é‡çš„å­˜åœ¨ä½¿å¾—é€‰æ‹©é—®é¢˜æ›´åŠ å›°éš¾ï¼Œä½†æœ€ç»ˆæ¨¡å‹ä»ç„¶å¾ˆå¥½ï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œ<em>å®ƒæˆåŠŸåœ°æ’é™¤äº†æ‰€æœ‰
20 ä¸ªå™ªå£°ç‰¹å¾</em>ã€‚</li>
</ul></li>
</ul>
<h2 id="conceptual-overview-the-why">Conceptual Overview: The â€œWhyâ€</h2>
<p>Slides cover <strong>Chapter 6: Linear Model Selection and
Regularization</strong>, which is all about a fundamental trade-off in
machine learning: the <strong>bias-variance trade-off</strong>.
è¯¥éƒ¨åˆ†ä¸»è¦è®¨è®ºæœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªåŸºæœ¬æƒè¡¡ï¼š<strong>åå·®-æ–¹å·®æƒè¡¡</strong>ã€‚</p>
<ul>
<li><p><strong>The Problem (Slide <code>...221320.png</code>):</strong>
Imagine you have a dataset with 50 predictors (<span
class="math inline">\(p=50\)</span>). You want to predict a response
<span class="math inline">\(y\)</span>. å‡è®¾ä½ æœ‰ä¸€ä¸ªåŒ…å« 50
ä¸ªé¢„æµ‹å˜é‡ï¼ˆp=50ï¼‰çš„æ•°æ®é›†ã€‚ä½ æƒ³è¦é¢„æµ‹å“åº” <span
class="math inline">\(y\)</span>ã€‚</p>
<ul>
<li><strong>Model 1 (Full Model):</strong> You use all 50 predictors.
This model is very <strong>flexible</strong>. It will fit the
<em>training data</em> extremely well, resulting in a low
<strong>bias</strong>. However, itâ€™s highly likely that many of those 50
predictors are just â€œnoiseâ€ (random, unrelated variables). By fitting to
this noise, the model will be <strong>overfit</strong>. When you show it
new, unseen data (the <em>test data</em>), it will perform poorly. This
is called <strong>high variance</strong>. ä½ ä½¿ç”¨äº†æ‰€æœ‰ 50
ä¸ªé¢„æµ‹å˜é‡ã€‚è¿™ä¸ªæ¨¡å‹éå¸¸<strong>çµæ´»</strong>ã€‚å®ƒèƒ½å¾ˆå¥½åœ°æ‹Ÿåˆ<em>è®­ç»ƒæ•°æ®</em>ï¼Œä»è€Œäº§ç”Ÿè¾ƒä½çš„<strong>åå·®</strong>ã€‚ç„¶è€Œï¼Œè¿™
50
ä¸ªé¢„æµ‹å˜é‡ä¸­å¾ˆå¯èƒ½æœ‰å¾ˆå¤šåªæ˜¯â€œå™ªå£°â€ï¼ˆéšæœºçš„ã€ä¸ç›¸å…³çš„å˜é‡ï¼‰ã€‚ç”±äºæ‹Ÿåˆè¿™äº›å™ªå£°ï¼Œæ¨¡å‹ä¼š<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚å½“ä½ å‘å®ƒå±•ç¤ºæ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ï¼ˆ<em>æµ‹è¯•æ•°æ®</em>ï¼‰æ—¶ï¼Œå®ƒçš„è¡¨ç°ä¼šå¾ˆå·®ã€‚è¿™è¢«ç§°ä¸º<strong>é«˜æ–¹å·®</strong>ã€‚</li>
<li><strong>Model 2 (Subset Model):</strong> You intelligently select
only the 3 predictors (<span class="math inline">\(q=3\)</span>) that
are <em>actually</em> related to <span class="math inline">\(y\)</span>.
This model is less flexible. It wonâ€™t fit the <em>training data</em> as
perfectly as Model 1 (it has higher <strong>bias</strong>). But, because
itâ€™s <em>not</em> fitting the noise, it will generalize much better to
new data. It will have a much lower <strong>variance</strong>, and thus
a lower overall <em>test error</em>. ä½ æ™ºèƒ½åœ°åªé€‰æ‹©ä¸ <span
class="math inline">\(y\)</span> <em>çœŸæ­£</em>ç›¸å…³çš„ 3 ä¸ªé¢„æµ‹å˜é‡ (<span
class="math inline">\(q=3\)</span>)ã€‚è¿™ä¸ªæ¨¡å‹çš„çµæ´»æ€§è¾ƒå·®ã€‚å®ƒå¯¹
<em>è®­ç»ƒæ•°æ®</em> çš„æ‹Ÿåˆåº¦ä¸å¦‚æ¨¡å‹ 1
å®Œç¾ï¼ˆå®ƒçš„<strong>åå·®</strong>æ›´é«˜ï¼‰ã€‚ä½†æ˜¯ï¼Œç”±äºå®ƒå¯¹å™ªå£°çš„æ‹Ÿåˆåº¦æ›´é«˜ï¼Œå› æ­¤å¯¹æ–°æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ä¼šæ›´å¥½ã€‚å®ƒçš„<strong>æ–¹å·®</strong>ä¼šæ›´ä½ï¼Œå› æ­¤æ€»ä½“çš„<em>æµ‹è¯•è¯¯å·®</em>ä¹Ÿä¼šæ›´ä½ã€‚</li>
</ul></li>
<li><p><strong>The Goal:</strong> The goal is to find the model that has
the <strong>lowest test error</strong>. We need a formal method to
<em>find</em> the best subset (like Model 2) without just guessing.
<strong>ç›®æ ‡æ˜¯æ‰¾åˆ°</strong>æµ‹è¯•è¯¯å·®**æœ€ä½çš„æ¨¡å‹ã€‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ­£å¼çš„æ–¹æ³•æ¥<em>æ‰¾åˆ°</em>æœ€ä½³å­é›†ï¼ˆä¾‹å¦‚æ¨¡å‹
2ï¼‰ï¼Œè€Œä¸æ˜¯ä»…ä»…é çŒœæµ‹ã€‚</p></li>
<li><p><strong>Two Main Strategies (Slide
<code>...221314.png</code>):</strong></p>
<ol type="1">
<li><p><strong>Subset Selection (Section 6.1):</strong> This is what
weâ€™re focused on. Itâ€™s an â€œall-or-nothingâ€ approach. You either
<em>keep</em> a variable in the model or you <em>discard</em> it
completely. The â€œBest Subset Selectionâ€ algorithm is the most extreme,
â€œbrute-forceâ€ way to do this.
æ˜¯æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹ã€‚è¿™æ˜¯ä¸€ç§â€œå…¨æœ‰æˆ–å…¨æ— â€çš„æ–¹æ³•ã€‚ä½ è¦ä¹ˆåœ¨æ¨¡å‹ä¸­â€œä¿ç•™â€ä¸€ä¸ªå˜é‡ï¼Œè¦ä¹ˆâ€œå½»åº•ä¸¢å¼ƒâ€å®ƒã€‚â€œæœ€ä½³å­é›†é€‰æ‹©â€ç®—æ³•æ˜¯æœ€æç«¯ã€æœ€â€œæš´åŠ›â€çš„åšæ³•ã€‚</p></li>
<li><p><strong>Shrinkage/Regularization (Section 6.2):</strong> This is
a more subtle approach (e.g., Ridge Regression, LASSO). Instead of
discarding variables, you <em>keep all <span
class="math inline">\(p\)</span> variables</em> but add a penalty to the
model that â€œshrinksâ€ the coefficients (<span
class="math inline">\(\beta\)</span>) of the useless variables towards
zero.
è¿™æ˜¯ä¸€ç§æ›´å·§å¦™çš„æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œå²­å›å½’ã€LASSOï¼‰ã€‚ä½ ä¸æ˜¯ä¸¢å¼ƒå˜é‡ï¼Œè€Œæ˜¯<em>ä¿ç•™æ‰€æœ‰
<span class="math inline">\(p\)</span>
ä¸ªå˜é‡</em>ï¼Œä½†ä¼šç»™æ¨¡å‹æ·»åŠ ä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œå°†æ— ç”¨å˜é‡çš„ç³»æ•°ï¼ˆ<span
class="math inline">\(\beta\)</span>ï¼‰â€œæ”¶ç¼©â€åˆ°é›¶ã€‚</p></li>
</ol></li>
</ul>
<h2 id="questions">Questions ğŸ¯</h2>
<h3 id="q1-how-to-compare-which-model-is-better">Q1: â€œHow to compare
which model is better?â€</h3>
<p>(From slides <code>...221320.png</code> and
<code>...221326.png</code>)</p>
<p>This is the most important question. You <strong>cannot</strong> use
metrics based on <em>training data</em> (like <span
class="math inline">\(R^2\)</span> or RSS - Residual Sum of Squares) to
compare models with <em>different numbers of predictors</em>.
è¿™æ˜¯æœ€é‡è¦çš„é—®é¢˜ã€‚æ‚¨<strong>ä¸èƒ½</strong>ä½¿ç”¨åŸºäº<em>è®­ç»ƒæ•°æ®</em>çš„æŒ‡æ ‡ï¼ˆä¾‹å¦‚
R^2 æˆ– RSS - æ®‹å·®å¹³æ–¹å’Œï¼‰æ¥æ¯”è¾ƒå…·æœ‰<em>ä¸åŒæ•°é‡é¢„æµ‹å˜é‡</em>çš„æ¨¡å‹ã€‚</p>
<ul>
<li><p><strong>The Trap:</strong> A model with more predictors will
<em>always</em> have a higher <span class="math inline">\(R^2\)</span>
(or lower RSS) on the data it was trained on. <span
class="math inline">\(R^2\)</span> will <em>always</em> increase as you
add variables, even if they are pure noise. If you used <span
class="math inline">\(R^2\)</span> to compare a 3-predictor model to a
10-predictor model, the 10-predictor model would <em>always</em> look
better on paper, even if itâ€™s terribly overfit.
å…·æœ‰æ›´å¤šé¢„æµ‹å˜é‡çš„æ¨¡å‹åœ¨å…¶è®­ç»ƒæ•°æ®ä¸Š<em>æ€»æ˜¯</em>å…·æœ‰æ›´é«˜çš„
R^2ï¼ˆæˆ–æ›´ä½çš„ RSSï¼‰ã€‚éšç€å˜é‡çš„å¢åŠ ï¼ŒR^2
ä¼š<em>æ€»æ˜¯</em>å¢åŠ ï¼Œå³ä½¿è¿™äº›å˜é‡æ˜¯çº¯å™ªå£°ã€‚å¦‚æœæ‚¨ä½¿ç”¨ R^2 æ¥æ¯”è¾ƒ 3
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹å’Œ 10 ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œé‚£ä¹ˆ 10
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹åœ¨çº¸é¢ä¸Š<em>æ€»æ˜¯</em>çœ‹èµ·æ¥æ›´å¥½ï¼Œå³ä½¿å®ƒä¸¥é‡è¿‡æ‹Ÿåˆã€‚</p></li>
<li><p><strong>The Correct Way:</strong> You must use a metric that
estimates the <strong>test error</strong>. The slides and code show two
ways:æ‚¨å¿…é¡»ä½¿ç”¨ä¸€ä¸ªèƒ½å¤Ÿä¼°è®¡<strong>æµ‹è¯•è¯¯å·®</strong>çš„æŒ‡æ ‡ã€‚</p>
<ol type="1">
<li><strong>Cross-Validation (CV):</strong> This is the method used in
your Python code. It works by:
<ul>
<li>Splitting your training data into <span
class="math inline">\(k\)</span> â€œfoldsâ€ (e.g., 5 folds).
å°†è®­ç»ƒæ•°æ®æ‹†åˆ†æˆ <span class="math inline">\(k\)</span> ä¸ªâ€œæŠ˜å â€ï¼ˆä¾‹å¦‚ 5
ä¸ªæŠ˜å ï¼‰ã€‚</li>
<li>Training the model on 4 folds and testing it on the 5th fold.
ä½¿ç”¨å…¶ä¸­ 4 ä¸ªæŠ˜å è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ç¬¬ 5 ä¸ªæŠ˜å è¿›è¡Œæµ‹è¯•ã€‚</li>
<li>Repeating this 5 times, so each fold gets to be the test set once.
é‡å¤æ­¤æ“ä½œ 5 æ¬¡ï¼Œä½¿æ¯ä¸ªæŠ˜å éƒ½ä½œä¸ºæµ‹è¯•é›†ä¸€æ¬¡ã€‚</li>
<li>Averaging the 5 test errors. å¯¹ 5 ä¸ªæµ‹è¯•è¯¯å·®æ±‚å¹³å‡å€¼ã€‚ This gives
you a robust estimate of how your model will perform on <em>unseen
data</em>. You then choose the model with the best (lowest) average CV
error.
è¿™å¯ä»¥è®©ä½ å¯¹æ¨¡å‹åœ¨<em>æœªè§æ•°æ®</em>ä¸Šçš„è¡¨ç°æœ‰ä¸€ä¸ªç¨³å¥çš„ä¼°è®¡ã€‚ç„¶åï¼Œä½ å¯ä»¥é€‰æ‹©å¹³å‡
CV è¯¯å·®æœ€å°ï¼ˆæœ€ä½³ï¼‰çš„æ¨¡å‹ã€‚</li>
</ul></li>
<li><strong>Mathematical Adjustments (AIC, BIC, Adjusted <span
class="math inline">\(R^2\)</span>):</strong> These are formulas that
take the training error (like RSS) and add a <em>penalty</em> for each
predictor (<span class="math inline">\(k\)</span>) you add.
<ul>
<li><span class="math inline">\(AIC \approx RSS +
2k\sigma^2\)</span></li>
<li><span class="math inline">\(BIC \approx RSS +
\log(n)k\sigma^2\)</span> A model with more predictors (larger <span
class="math inline">\(k\)</span>) gets a bigger penalty. To be chosen, a
more complex model must <em>significantly</em> improve the RSS to
overcome this penalty. é¢„æµ‹å˜é‡è¶Šå¤šï¼ˆk
è¶Šå¤§ï¼‰çš„æ¨¡å‹ï¼Œæƒ©ç½šè¶Šå¤§ã€‚è¦è¢«é€‰ä¸­ï¼Œæ›´å¤æ‚çš„æ¨¡å‹å¿…é¡»<em>æ˜¾è‘—</em>æå‡ RSS
ä»¥å…‹æœæ­¤æƒ©ç½šã€‚</li>
</ul></li>
</ol></li>
</ul>
<h3 id="q2-why-using-r2-for-step-2">Q2: â€œWhy using <span
class="math inline">\(R^2\)</span> for step 2?â€</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p><strong>Step 2</strong> of the â€œBest Subset Selectionâ€ algorithm
says: â€œFor <span class="math inline">\(k = 1, \dots, p\)</span>: Fit all
<span class="math inline">\(\binom{p}{k}\)</span> modelsâ€¦ Pick the best
model, that with the largest <span class="math inline">\(R^2\)</span>, â€¦
and call it <span class="math inline">\(M_k\)</span>.â€ â€œå¯¹äº <span
class="math inline">\(k = 1, \dots, p\)</span>ï¼šæ‹Ÿåˆæ‰€æœ‰ <span
class="math inline">\(\binom{p}{k}\)</span> ä¸ªæ¨¡å‹â€¦â€¦é€‰æ‹©å…·æœ‰æœ€å¤§ <span
class="math inline">\(R^2\)</span> çš„æœ€ä½³æ¨¡å‹â€¦â€¦å¹¶å°†å…¶å‘½åä¸º <span
class="math inline">\(M_k\)</span>ã€‚â€</p>
<ul>
<li><strong>The Reason:</strong> In Step 2, you are <em>only</em>
comparing models <strong>of the same size</strong>. For example, when
<span class="math inline">\(k=3\)</span>, you are comparing all possible
3-predictor models: æ­¥éª¤ 2
ä¸­ï¼Œæ‚¨<em>ä»…</em>æ¯”è¾ƒ**ç›¸åŒå¤§å°çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå½“ <span
class="math inline">\(k=3\)</span> æ—¶ï¼Œæ‚¨å°†æ¯”è¾ƒæ‰€æœ‰å¯èƒ½çš„ 3
é¢„æµ‹å˜é‡æ¨¡å‹ï¼š
<ul>
<li>Model A: (<span class="math inline">\(X_1, X_2, X_3\)</span>)</li>
<li>Model B: (<span class="math inline">\(X_1, X_2, X_4\)</span>)</li>
<li>Model C: (<span class="math inline">\(X_1, X_3, X_5\)</span>)</li>
<li>â€¦and so on.</li>
</ul>
Since all these models have the <em>exact same complexity</em> (they all
have <span class="math inline">\(k=3\)</span> predictors), there is no
risk of unfairly favoring a more complex model. Therefore, you are free
to use a training metric like <span class="math inline">\(R^2\)</span>
(or RSS). The model with the highest <span
class="math inline">\(R^2\)</span> is, by definition, the one that
<em>best fits the training data</em> for that specific size <span
class="math inline">\(k\)</span>.
ç”±äºæ‰€æœ‰è¿™äº›æ¨¡å‹éƒ½å…·æœ‰<em>å®Œå…¨ç›¸åŒçš„å¤æ‚åº¦</em>ï¼ˆå®ƒä»¬éƒ½å…·æœ‰ <span
class="math inline">\(k=3\)</span>
ä¸ªé¢„æµ‹å˜é‡ï¼‰ï¼Œå› æ­¤ä¸å­˜åœ¨ä¸å…¬å¹³åœ°åå‘æ›´å¤æ‚æ¨¡å‹çš„é£é™©ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥è‡ªç”±ä½¿ç”¨åƒ
<span class="math inline">\(R^2\)</span>ï¼ˆæˆ–
RSSï¼‰è¿™æ ·çš„è®­ç»ƒæŒ‡æ ‡ã€‚æ ¹æ®å®šä¹‰ï¼Œå…·æœ‰æœ€é«˜ <span
class="math inline">\(R^2\)</span> çš„æ¨¡å‹å°±æ˜¯åœ¨ç‰¹å®šå¤§å° <span
class="math inline">\(k\)</span>
ä¸‹<em>ä¸è®­ç»ƒæ•°æ®æ‹Ÿåˆåº¦</em>æœ€é«˜çš„æ¨¡å‹ã€‚</li>
</ul>
<h3
id="q3-cannot-use-training-error-in-step-3.-why-not-æ­¥éª¤-3-ä¸­ä¸èƒ½ä½¿ç”¨è®­ç»ƒè¯¯å·®-ä¸ºä»€ä¹ˆ">Q3:
â€œCannot use training error in Step 3.â€ Why not? â€œæ­¥éª¤ 3
ä¸­ä¸èƒ½ä½¿ç”¨è®­ç»ƒè¯¯å·®ã€‚â€ ä¸ºä»€ä¹ˆï¼Ÿ</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p><strong>Step 3</strong> says: â€œSelect a single best model from <span
class="math inline">\(M_0, M_1, \dots, M_p\)</span> by cross validation,
AIC, or BIC.â€â€œé€šè¿‡äº¤å‰éªŒè¯ã€AIC æˆ– BICï¼Œä» <span
class="math inline">\(M_0ã€M_1ã€\dotsã€M_p\)</span>
ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä½³æ¨¡å‹ã€‚â€</p>
<ul>
<li><p><strong>The Reason:</strong> In Step 3, you are now comparing
models <strong>of different sizes</strong>. You are comparing the best
1-predictor model (<span class="math inline">\(M_1\)</span>) vs.Â the
best 2-predictor model (<span class="math inline">\(M_2\)</span>)
vs.Â the best 3-predictor model (<span
class="math inline">\(M_3\)</span>), and so on, all the way up to <span
class="math inline">\(M_p\)</span>. åœ¨æ­¥éª¤ 3
ä¸­ï¼Œæ‚¨æ­£åœ¨æ¯”è¾ƒ<strong>ä¸åŒå¤§å°</strong>çš„æ¨¡å‹ã€‚æ‚¨æ­£åœ¨æ¯”è¾ƒæœ€ä½³çš„å•é¢„æµ‹æ¨¡å‹
(<span class="math inline">\(M_1\)</span>)ã€æœ€ä½³çš„åŒé¢„æµ‹æ¨¡å‹ (<span
class="math inline">\(M_2\)</span>) å’Œæœ€ä½³çš„ä¸‰é¢„æµ‹æ¨¡å‹ (<span
class="math inline">\(M_3\)</span>)ï¼Œä¾æ­¤ç±»æ¨ï¼Œç›´åˆ° <span
class="math inline">\(M_p\)</span>ã€‚</p>
<p>As explained in Q1, if you used a training error metric like <span
class="math inline">\(R^2\)</span> here, the <span
class="math inline">\(R^2\)</span> would just keep going up, and you
would <em>always</em> select the largest, most complex model, <span
class="math inline">\(M_p\)</span>. This completely defeats the purpose
of model selection. å¦‚é—®é¢˜ 1 æ‰€è¿°ï¼Œå¦‚æœæ‚¨åœ¨æ­¤å¤„ä½¿ç”¨åƒ <span
class="math inline">\(R^2\)</span> è¿™æ ·çš„è®­ç»ƒè¯¯å·®æŒ‡æ ‡ï¼Œé‚£ä¹ˆ <span
class="math inline">\(R^2\)</span>
ä¼šæŒç»­ä¸Šå‡ï¼Œå¹¶ä¸”æ‚¨<em>æ€»æ˜¯</em>ä¼šé€‰æ‹©æœ€å¤§ã€æœ€å¤æ‚çš„æ¨¡å‹ <span
class="math inline">\(M_p\)</span>ã€‚è¿™å®Œå…¨è¿èƒŒäº†æ¨¡å‹é€‰æ‹©çš„ç›®çš„ã€‚</p>
<p>Therefore, in Step 3, you <em>must</em> use a method that estimates
<strong>test error</strong> (like Cross-Validation) or one that
<strong>penalizes for complexity</strong> (like AIC or BIC) to find the
â€œsweet spotâ€ model that balances fit and simplicity. å› æ­¤ï¼Œåœ¨æ­¥éª¤ 3
ä¸­ï¼Œæ‚¨<em>å¿…é¡»</em>ä½¿ç”¨ä¸€ç§ä¼°ç®—<strong>æµ‹è¯•è¯¯å·®</strong>çš„æ–¹æ³•ï¼ˆä¾‹å¦‚äº¤å‰éªŒè¯ï¼‰æˆ–<strong>æƒ©ç½šå¤æ‚æ€§</strong>çš„æ–¹æ³•ï¼ˆä¾‹å¦‚
AIC æˆ–
BICï¼‰ï¼Œä»¥æ‰¾åˆ°åœ¨æ‹Ÿåˆåº¦å’Œç®€å•æ€§ä¹‹é—´å–å¾—å¹³è¡¡çš„â€œæœ€ä½³ç‚¹â€æ¨¡å‹ã€‚</p></li>
</ul>
<h2 id="mathematical-deep-dive">Mathematical Deep Dive ğŸ§®</h2>
<ul>
<li><strong><span class="math inline">\(Y = \beta_0 + \beta_1X_1 + \dots
+ \beta_pX_p + \epsilon\)</span>:</strong> The full linear model. The
goal of subset selection is to find a subset of <span
class="math inline">\(X_j\)</span>â€™s where <span
class="math inline">\(\beta_j \neq 0\)</span> and set all other <span
class="math inline">\(\beta\)</span>â€™s to 0.
å®Œæ•´çš„çº¿æ€§æ¨¡å‹ã€‚å­é›†é€‰æ‹©çš„ç›®æ ‡æ˜¯æ‰¾åˆ° <span
class="math inline">\(X_j\)</span> çš„ä¸€ä¸ªå­é›†ï¼Œå…¶ä¸­ $_j ç­‰äº
0ï¼Œå¹¶å°†æ‰€æœ‰å…¶ä»– <span class="math inline">\(\beta\)</span> è®¾ç½®ä¸º
0ã€‚</li>
<li><strong><span class="math inline">\(2^p\)</span>
combinations:</strong> (Slide <code>...221333.png</code>) This is the
total number of models you have to check. For each of the <span
class="math inline">\(p\)</span> variables, you have two choices: either
it is <strong>IN</strong> the model or it is
<strong>OUT</strong>.è¿™æ˜¯ä½ éœ€è¦æ£€æŸ¥çš„æ¨¡å‹æ€»æ•°ã€‚å¯¹äºæ¯ä¸ª <span
class="math inline">\(p\)</span>
ä¸ªå˜é‡ï¼Œä½ æœ‰ä¸¤ä¸ªé€‰æ‹©ï¼šè¦ä¹ˆå®ƒåœ¨æ¨¡å‹<strong>å†…éƒ¨</strong>ï¼Œè¦ä¹ˆå®ƒåœ¨æ¨¡å‹<strong>å¤–éƒ¨</strong>ã€‚
<ul>
<li>Example: <span class="math inline">\(p=3\)</span> (variables <span
class="math inline">\(X_1, X_2, X_3\)</span>)</li>
<li>The <span class="math inline">\(2^3 = 8\)</span> possible models
are:
<ol type="1">
<li>{} (The null model, <span class="math inline">\(M_0\)</span>)</li>
<li>{ <span class="math inline">\(X_1\)</span> }</li>
<li>{ <span class="math inline">\(X_2\)</span> }</li>
<li>{ <span class="math inline">\(X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_2\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_2, X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_2, X_3\)</span> } (The full
model, <span class="math inline">\(M_3\)</span>)</li>
</ol></li>
<li>This is why this method is called an <strong>â€œexhaustive
searchâ€</strong>. It literally checks every single one. For <span
class="math inline">\(p=20\)</span>, <span
class="math inline">\(2^{20}\)</span> is over a million
models!è¿™å°±æ˜¯è¯¥æ–¹æ³•è¢«ç§°ä¸º<strong>â€œç©·ä¸¾æœç´¢â€</strong>çš„åŸå› ã€‚å®ƒå®é™…ä¸Šä¼šæ£€æŸ¥æ¯ä¸€ä¸ªæ¨¡å‹ã€‚å¯¹äº
<span class="math inline">\(p=20\)</span>ï¼Œ<span
class="math inline">\(2^{20}\)</span> å°±è¶…è¿‡ä¸€ç™¾ä¸‡ä¸ªæ¨¡å‹ï¼</li>
</ul></li>
<li><strong><span class="math inline">\(\binom{p}{k} =
\frac{p!}{k!(p-k)!}\)</span>:</strong> (Slide
<code>...221333.png</code>) This is the â€œcombinationsâ€ formula. It tells
you <em>how many</em> models you fit <em>in Step 2</em> for a specific
<span
class="math inline">\(k\)</span>.è¿™æ˜¯â€œç»„åˆâ€å…¬å¼ã€‚å®ƒå‘Šè¯‰ä½ ï¼Œå¯¹äºç‰¹å®šçš„
<span class="math inline">\(k\)</span>ï¼Œ<em>åœ¨æ­¥éª¤ 2</em>ä¸­ï¼Œä½ æ‹Ÿåˆäº†
<em>å¤šå°‘</em> ä¸ªæ¨¡å‹ã€‚
<ul>
<li>Example: <span class="math inline">\(p=10\)</span> total
predictors.</li>
<li>For <span class="math inline">\(k=1\)</span>: You fit <span
class="math inline">\(\binom{10}{1} = 10\)</span> models.</li>
<li>For <span class="math inline">\(k=2\)</span>: You fit <span
class="math inline">\(\binom{10}{2} = \frac{10 \times 9}{2 \times 1} =
45\)</span> models.</li>
<li>For <span class="math inline">\(k=3\)</span>: You fit <span
class="math inline">\(\binom{10}{3} = \frac{10 \times 9 \times 8}{3
\times 2 \times 1} = 120\)</span> models.</li>
<li>â€¦and so on. The sum of all these <span
class="math inline">\(\binom{p}{k}\)</span> from <span
class="math inline">\(k=0\)</span> to <span
class="math inline">\(k=p\)</span> equals <span
class="math inline">\(2^p\)</span>.</li>
</ul></li>
</ul>
<h2 id="detailed-code-analysis">Detailed Code Analysis ğŸ’»</h2>
<p>Your slides show Python code that applies the <strong>Best Subset
Selection algorithm</strong> to a <strong>KNN Regressor</strong>. This
is a great example of how the <em>selection algorithm</em> is
independent of the <em>model type</em> (as mentioned in slide
<code>...221314.png</code>).</p>
<h3 id="key-functions-1">Key Functions</h3>
<ul>
<li><strong><code>main()</code></strong>
<ol type="1">
<li><strong>Load &amp; Preprocess:</strong> Reads
<code>Credit.csv</code>. The most important step here is converting
categorical text (like â€˜Maleâ€™/â€˜Femaleâ€™) into numbers (1/0).</li>
<li><strong>Scale Data:</strong> <code>scaler = StandardScaler()</code>
and <code>X_scaled = scaler.fit_transform(X)</code>.
<ul>
<li><strong>WHY?</strong> This is <strong>CRITICAL</strong> for KNN. KNN
works by measuring distance. If â€˜Incomeâ€™ (e.g., 50,000) is on a vastly
different scale than â€˜Cardsâ€™ (e.g., 3), the â€˜Incomeâ€™ feature will
completely dominate the distance calculation, making â€˜Cardsâ€™ irrelevant.
Scaling resizes all features to have a mean of 0 and standard deviation
of 1, so they all contribute fairly.</li>
</ul></li>
<li><strong>Handle Noisy Data (Slide
<code>...221303.jpg</code>):</strong> This version of the code
<em>intentionally</em> adds 20 columns of useless, random numbers. This
is a test to see if the algorithm is smart enough to ignore them.</li>
<li><strong>Run Selection:</strong>
<code>results_df = best_subset_selection_parallel(...)</code>. This
function does all the heavy lifting (explained next).</li>
<li><strong>Find Best Model:</strong>
<code>results_df.sort_values(by='CV_Score', ascending=False)</code>.
<ul>
<li><strong>WHY <code>ascending=False</code>?</strong> The code uses the
metric <code>'neg_mean_squared_error'</code>. This is MSE, but negative
(e.g., -15000). A <em>better</em> model has an error closer to 0 (e.g.,
-10000). Since -10000 is <em>greater than</em> -15000, you sort in
descending (high-to-low) order to put the best models at the top.</li>
</ul></li>
<li><strong>Final Evaluation (Step 3):</strong>
<code>final_scores = cross_val_score(knn, X_best, y, ...)</code>
<ul>
<li>This is the implementation of Step 3. It takes <em>only</em> the
single best subset (<code>X_best</code>) and runs a <em>new</em>
cross-validation on it. This gives a final, unbiased estimate of how
good that one model is.</li>
</ul></li>
<li><strong>Print RMSE:</strong>
<code>final_rmse = np.sqrt(-final_scores)</code>. It converts the
negative MSE back into a positive RMSE (Root Mean Squared Error), which
is in the same units as the target <span
class="math inline">\(y\)</span> (in this case, â€˜Balanceâ€™ in
dollars).</li>
</ol></li>
<li><strong><code>best_subset_selection_parallel(model, ...)</code></strong>
<ol type="1">
<li>This is the â€œmanagerâ€ function. It implements the loop from Step
2.</li>
<li><code>for k in range(1, n_features + 1):</code> This is the loop
â€œFor <span class="math inline">\(k = 1, \dots, p\)</span>â€.</li>
<li><code>subsets = list(combinations(feature_names, k))</code>: This
generates the <span class="math inline">\(\binom{p}{k}\)</span>
combinations for the current <span
class="math inline">\(k\)</span>.</li>
<li><code>results = Parallel(n_jobs=n_jobs)(...)</code>: This is a
non-core, â€œspeed-upâ€ command. It uses the <code>joblib</code> library to
run the evaluations on all your computerâ€™s CPU cores at once (in
parallel). Without this, checking millions of models would take
days.</li>
<li><code>subset_scores = ... [delayed(evaluate_subset)(...) ...]</code>
This line farms out the <em>actual work</em> to the
<code>evaluate_subset</code> function for every single subset.</li>
</ol></li>
<li><strong><code>evaluate_subset(subset, ...)</code></strong>
<ol type="1">
<li>This is the â€œworkerâ€ function. It gets called thousands or millions
of times.</li>
<li>Its job is to evaluate <em>one single subset</em> (e.g.,
<code>('Income', 'Limit', 'Student')</code>).</li>
<li><code>X_subset = X[list(subset)]</code>: It slices the data to get
<em>only</em> these columns.</li>
<li><code>scores = cross_val_score(model, X_subset, ...)</code>:
<strong>This is the most important line.</strong> It takes the subset
and performs a full 5-fold cross-validation on it.</li>
<li><code>return (subset, np.mean(scores))</code>: It returns the subset
and its average CV score.</li>
</ol></li>
</ul>
<h3 id="summary-of-outputs-slides-...221255.png-...221309.png">Summary
of Outputs (Slides <code>...221255.png</code> &amp;
<code>...221309.png</code>)</h3>
<ul>
<li><strong>Original Data (Slide <code>...221255.png</code>):</strong>
<ul>
<li><strong>Best Subset:</strong>
<code>('Income', 'Limit', 'Rating', 'Student')</code></li>
<li><strong>Final RMSE:</strong> ~105.4</li>
</ul></li>
<li><strong>Data with 20 â€œNoisyâ€ Variables (Slide
<code>...221309.png</code>):</strong>
<ul>
<li><strong>Best Subset:</strong>
<code>('Income', 'Limit', 'Student')</code></li>
<li><strong>Result:</strong> The algorithm <em>successfully</em>
identified that all 20 â€œNoisyâ€ variables were useless and
<strong>excluded every single one of them</strong> from the best
models.</li>
<li><strong>Final RMSE:</strong> ~114.9</li>
<li><strong>Key Takeaway:</strong> The RMSE is slightly higher, which
makes sense because the selection problem was much harder. But the
<em>method worked perfectly</em>. It filtered all the â€œnoiseâ€ and found
a simple, powerful model, just as the theory on slide
<code>...221320.png</code> predicted.</li>
</ul></li>
</ul>
<h1
id="the-core-problem-training-error-vs.-test-error-æ ¸å¿ƒé—®é¢˜è®­ç»ƒè¯¯å·®-vs.-æµ‹è¯•è¯¯å·®">2.
The Core Problem: Training Error vs.Â Test Error æ ¸å¿ƒé—®é¢˜ï¼šè®­ç»ƒè¯¯å·®
vs.Â æµ‹è¯•è¯¯å·®</h1>
<p>The central theme of these slides is finding the â€œbestâ€ model. The
problem is that a model with more predictors (more complex) will
<em>always</em> fit the data it was trained on better. This is a trap.
å¯»æ‰¾â€œæœ€ä½³â€æ¨¡å‹ã€‚é—®é¢˜åœ¨äºï¼Œé¢„æµ‹å› å­è¶Šå¤šï¼ˆè¶Šå¤æ‚ï¼‰çš„æ¨¡å‹<em>æ€»æ˜¯</em>èƒ½æ›´å¥½åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚è¿™æ˜¯ä¸€ä¸ªé™·é˜±ã€‚</p>
<ul>
<li><strong>Training Error:</strong> How well the model fits the data we
used to build it. <strong><span class="math inline">\(R^2\)</span> and
<span class="math inline">\(RSS\)</span> measure this.</strong>
æ¨¡å‹ä¸æˆ‘ä»¬æ„å»ºæ¨¡å‹æ—¶æ‰€ç”¨æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ã€‚<strong><span
class="math inline">\(R^2\)</span> å’Œ <span
class="math inline">\(RSS\)</span> è¡¡é‡äº†è¿™ä¸€ç‚¹ã€‚</strong></li>
<li><strong>Test Error:</strong> How well the model predicts new, unseen
data. This is what we <em>actually</em> care about. A model that is too
complex (e.g., has 10 predictors when only 3 are useful) will have low
training error but very high test error. This is called
<strong>overfitting</strong>.
æ¨¡å‹é¢„æµ‹æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®çš„å‡†ç¡®ç¨‹åº¦ã€‚è¿™æ‰æ˜¯æˆ‘ä»¬<em>çœŸæ­£</em>å…³å¿ƒçš„ã€‚è¿‡äºå¤æ‚çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œæœ‰
10 ä¸ªé¢„æµ‹å› å­ï¼Œä½†åªæœ‰ 3
ä¸ªæœ‰ç”¨ï¼‰çš„è®­ç»ƒè¯¯å·®ä¼šå¾ˆä½ï¼Œä½†æµ‹è¯•è¯¯å·®ä¼šå¾ˆé«˜ã€‚è¿™è¢«ç§°ä¸º<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚</li>
</ul>
<p>The goal is to choose a model that has the lowest <em>test
error</em>. The metrics below (Adjusted <span
class="math inline">\(R^2\)</span>, AIC, BIC) are all attempts to
<em>estimate</em> this test error without having to actually collect new
data. They do this by adding a <strong>penalty</strong> for complexity.
ç›®æ ‡æ˜¯é€‰æ‹©ä¸€ä¸ªå…·æœ‰æœ€ä½<em>æµ‹è¯•è¯¯å·®</em>çš„æ¨¡å‹ã€‚ä»¥ä¸‹æŒ‡æ ‡ï¼ˆè°ƒæ•´åçš„ <span
class="math inline">\(R^2\)</span>ã€AICã€BICï¼‰éƒ½æ˜¯åœ¨æ— éœ€å®é™…æ”¶é›†æ–°æ•°æ®çš„æƒ…å†µä¸‹å°è¯•<em>ä¼°è®¡</em>æ­¤æµ‹è¯•è¯¯å·®ã€‚ä»–ä»¬é€šè¿‡å¢åŠ <strong>å¤æ‚åº¦æƒ©ç½š</strong>æ¥å®ç°è¿™ä¸€ç‚¹ã€‚</p>
<h2 id="basic-metrics-measures-of-fit">Basic Metrics (Measures of
Fit)</h2>
<p>These formulas from slide 13 describe how well a model fits the
<em>training data</em>.</p>
<h3 id="residue-error-æ®‹å·®è¯¯å·®">Residue (Error) æ®‹å·®ï¼ˆè¯¯å·®ï¼‰</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\epsilon}_i = y_i - \hat{y}_i = y_i -
\hat{\beta}_0 - \sum_{j=1}^{p} \hat{\beta}_j x_{ij}\)</span></li>
<li><strong>Concept:</strong> This is the most basic building block.
Itâ€™s the difference between the <em>actual</em> observed value (<span
class="math inline">\(y_i\)</span>) and the value your model
<em>predicted</em> (<span class="math inline">\(\hat{y}_i\)</span>). It
is the â€œerrorâ€ for a single data point.
è¿™æ˜¯æœ€åŸºæœ¬çš„æ„å»ºå—ã€‚å®ƒæ˜¯<em>å®é™…</em>è§‚æµ‹å€¼ (<span
class="math inline">\(y_i\)</span>) ä¸æ¨¡å‹*é¢„æµ‹å€¼ (<span
class="math inline">\(\hat{y}_i\)</span>)
ä¹‹é—´çš„å·®å€¼ã€‚å®ƒæ˜¯å•ä¸ªæ•°æ®ç‚¹çš„â€œè¯¯å·®â€ã€‚</li>
</ul>
<h3 id="residual-sum-of-squares-rss-æ®‹å·®å¹³æ–¹å’Œ-rss">Residual Sum of
Squares (RSS) æ®‹å·®å¹³æ–¹å’Œ (RSS)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(RSS =
\sum_{i=1}^{n} \hat{\epsilon}_i^2\)</span></li>
<li><strong>Concept:</strong> This is the overall measure of model
error. You square all the individual errors (residues) to make them
positive and then add them all up.
è¿™æ˜¯æ¨¡å‹è¯¯å·®çš„æ€»ä½“åº¦é‡ã€‚å°†æ‰€æœ‰å•ä¸ªè¯¯å·®ï¼ˆæ®‹å·®ï¼‰å¹³æ–¹ï¼Œä½¿å…¶ä¸ºæ­£ï¼Œç„¶åå°†å®ƒä»¬å…¨éƒ¨ç›¸åŠ ã€‚</li>
<li><strong>Goal:</strong> The entire process of linear regression
(called â€œOrdinary Least Squaresâ€) is designed to find the <span
class="math inline">\(\hat{\beta}\)</span> coefficients that make this
<strong>RSS value as small as possible</strong>.
æ•´ä¸ªçº¿æ€§å›å½’è¿‡ç¨‹ï¼ˆç§°ä¸ºâ€œæ™®é€šæœ€å°äºŒä¹˜æ³•â€ï¼‰æ—¨åœ¨æ‰¾åˆ°ä½¿<strong>RSS
å€¼å°½å¯èƒ½å°</strong>çš„ <span class="math inline">\(\hat{\beta}\)</span>
ä¸ªç³»æ•°ã€‚</li>
<li><strong>The Flaw ç¼ºé™·:</strong> <span
class="math inline">\(RSS\)</span> will <em>always</em> decrease (or
stay the same) as you add more predictors (<span
class="math inline">\(p\)</span>). A model with all 10 predictors will
have a lower <span class="math inline">\(RSS\)</span> than a model with
9, even if that 10th predictor is useless. Therefore, <span
class="math inline">\(RSS\)</span> is useless for choosing
<em>between</em> models of different sizes. éšç€é¢„æµ‹å˜é‡ (<span
class="math inline">\(p\)</span>) çš„å¢åŠ ï¼Œ<span
class="math inline">\(RSS\)</span>
æ€»æ˜¯ä¼šå‡å°ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ã€‚ä¸€ä¸ªåŒ…å«æ‰€æœ‰ 10 ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹çš„ <span
class="math inline">\(RSS\)</span> ä¼šä½äºä¸€ä¸ªåŒ…å« 9
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œå³ä½¿ç¬¬ 10 ä¸ªé¢„æµ‹å˜é‡æ¯«æ— ç”¨å¤„ã€‚å› æ­¤ï¼Œ<span
class="math inline">\(RSS\)</span>
å¯¹äºåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¹‹é—´è¿›è¡Œé€‰æ‹©æ¯«æ— ç”¨å¤„ã€‚</li>
</ul>
<h3 id="r-squared-r2">R-squared (<span
class="math inline">\(R^2\)</span>)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(R^2 = 1 -
\frac{SS_{error}}{SS_{total}} = 1 - \frac{RSS}{\sum_{i=1}^{n} (y_i -
\bar{y})^2}\)</span></li>
<li><strong>Concept:</strong> This metric reframes <span
class="math inline">\(RSS\)</span> into a more interpretable
percentage.æ­¤æŒ‡æ ‡å°† <span class="math inline">\(RSS\)</span>
é‡æ–°å®šä¹‰ä¸ºæ›´æ˜“äºè§£é‡Šçš„ç™¾åˆ†æ¯”ã€‚
<ul>
<li><span class="math inline">\(SS_{total}\)</span> (the denominator)
represents the <em>total variance</em> of the data. Itâ€™s the error you
would get if your â€œmodelâ€ was just guessing the average value (<span
class="math inline">\(\bar{y}\)</span>) for every single observation.
ï¼ˆåˆ†æ¯ï¼‰è¡¨ç¤ºæ•°æ®çš„<em>æ€»æ–¹å·®</em>ã€‚å¦‚æœä½ çš„â€œæ¨¡å‹â€åªæ˜¯çŒœæµ‹æ¯ä¸ªè§‚æµ‹å€¼çš„å¹³å‡å€¼
(<span
class="math inline">\(\bar{y}\)</span>)ï¼Œé‚£ä¹ˆä½ å°±ä¼šå¾—åˆ°è¿™ä¸ªè¯¯å·®ã€‚</li>
<li><span class="math inline">\(SS_{error}\)</span> (the <span
class="math inline">\(RSS\)</span>) is the error <em>after</em> using
your model. æ˜¯â€œæ¨¡å‹è§£é‡Šçš„æ€»æ–¹å·®çš„æ¯”ä¾‹â€ã€‚ <span
class="math inline">\(R^2\)</span> ä¸º 0.75
æ„å‘³ç€ä½ çš„æ¨¡å‹å¯ä»¥è§£é‡Šå“åº”å˜é‡ 75% çš„å˜å¼‚ã€‚</li>
<li><span class="math inline">\(R^2\)</span> is the â€œproportion of total
variance explained by the model.â€ An <span
class="math inline">\(R^2\)</span> of 0.75 means your model can explain
75% of the variation in the response variable.</li>
</ul></li>
<li><strong>The Flaw ç¼ºé™·:</strong> Just like <span
class="math inline">\(RSS\)</span>, <span
class="math inline">\(R^2\)</span> will <em>always</em> increase (or
stay the same) as you add more predictors. This is visually confirmed in
Figure 6.1, where the red line for <span
class="math inline">\(R^2\)</span> only goes up. It will always pick the
most complex model. ä¸ <span class="math inline">\(RSS\)</span>
ä¸€æ ·ï¼Œéšç€é¢„æµ‹å˜é‡çš„å¢åŠ ï¼Œ<span class="math inline">\(R^2\)</span>
ä¼š<em>å§‹ç»ˆ</em>å¢åŠ ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ã€‚å›¾ 6.1 ç›´è§‚åœ°è¯å®äº†è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­ <span
class="math inline">\(R^2\)</span>
çš„çº¢çº¿åªä¼šä¸Šå‡ã€‚å®ƒæ€»æ˜¯ä¼šé€‰æ‹©æœ€å¤æ‚çš„æ¨¡å‹ã€‚</li>
</ul>
<h2
id="advanced-metrics-for-model-selection-é«˜çº§æŒ‡æ ‡ç”¨äºæ¨¡å‹é€‰æ‹©">Advanced
Metrics (For Model Selection) é«˜çº§æŒ‡æ ‡ï¼ˆç”¨äºæ¨¡å‹é€‰æ‹©ï¼‰</h2>
<p>These metrics â€œfixâ€ the flaw of <span
class="math inline">\(R^2\)</span> by including a penalty for the number
of predictors.</p>
<h3 id="adjusted-r2">Adjusted <span
class="math inline">\(R^2\)</span></h3>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
  \text{Adjusted } R^2 = 1 - \frac{RSS / (n - p - 1)}{SS_{total} / (n -
1)}
  \]</span></li>
<li><strong>Mathematical Concept:</strong> This formula replaces the
â€œSum of Squaresâ€ (<span class="math inline">\(SS\)</span>) with â€œMean
Squaresâ€ (<span class="math inline">\(MS\)</span>).
<ul>
<li><span class="math inline">\(MS_{error} =
\frac{RSS}{n-p-1}\)</span></li>
<li><span class="math inline">\(MS_{total} =
\frac{SS_{total}}{n-1}\)</span></li>
</ul></li>
<li><strong>The â€œPenaltyâ€ Explained:</strong> The penalty is
<strong>degrees of freedom</strong>.
<ul>
<li><span class="math inline">\(n\)</span> = number of data points.</li>
<li><span class="math inline">\(p\)</span> = number of predictors.</li>
<li>The term <span class="math inline">\(n-p-1\)</span> is the degrees
of freedom for the residuals. You start with <span
class="math inline">\(n\)</span> data points, but you â€œuse upâ€ one
degree of freedom to estimate the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) and <span
class="math inline">\(p\)</span> more to estimate the <span
class="math inline">\(p\)</span> slopes.</li>
</ul></li>
<li><strong>How it Works:</strong>
<ol type="1">
<li>When you add a new predictor (increase <span
class="math inline">\(p\)</span>), <span
class="math inline">\(RSS\)</span> goes down, which makes the numerator
(<span class="math inline">\(MS_{error}\)</span>) smaller.</li>
<li>â€¦But, increasing <span class="math inline">\(p\)</span>
<em>also</em> decreases the denominator (<span
class="math inline">\(n-p-1\)</span>), which makes the numerator (<span
class="math inline">\(MS_{error}\)</span>) <em>larger</em>.</li>
</ol>
<ul>
<li>This creates a â€œtug-of-war.â€ If the new predictor is
<strong>useful</strong>, it will drop <span
class="math inline">\(RSS\)</span> a lot, and Adjusted <span
class="math inline">\(R^2\)</span> will <strong>increase</strong>. If
the new predictor is <strong>useless</strong>, <span
class="math inline">\(RSS\)</span> will barely change, and the penalty
from decreasing the denominator will win, causing Adjusted <span
class="math inline">\(R^2\)</span> to <strong>decrease</strong>.</li>
</ul></li>
<li><strong>Goal:</strong> You select the model with the
<strong>highest</strong> Adjusted <span
class="math inline">\(R^2\)</span>.</li>
</ul>
<h3 id="akaike-information-criterion-aic">Akaike Information Criterion
(AIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Concept Breakdown:</strong>
<ul>
<li><span class="math inline">\(\ell(\hat{\theta})\)</span>: This is the
<strong>Maximized Likelihood Function</strong>.
<ul>
<li>The <strong>Likelihood Function</strong> <span
class="math inline">\(\ell(\theta)\)</span> asks: â€œGiven a set of model
parameters <span class="math inline">\(\theta\)</span>, how probable is
the data we observed?â€</li>
<li>The <strong>Maximum Likelihood Estimate (MLE)</strong> <span
class="math inline">\(\hat{\theta}\)</span> is the specific set of
parameters (the <span class="math inline">\(\hat{\beta}\)</span>â€™s) that
<em>maximizes</em> this probability.</li>
</ul></li>
<li><span class="math inline">\(\log \ell(\hat{\theta})\)</span>: The
<strong>log-likelihood</strong>. This is just a number that represents
the <em>best possible fit</em> the model can achieve for the data. A
higher number is a better fit.</li>
<li><span class="math inline">\(-2 \log \ell(\hat{\theta})\)</span>:
This is the <strong>Deviance</strong>. Since a higher log-likelihood is
better, a <em>lower</em> deviance is better. This term measures
<strong>poorness-of-fit</strong>.</li>
<li><span class="math inline">\(d\)</span>: The number of parameters
estimated by the model. (e.g., <span class="math inline">\(p\)</span>
predictors + 1 intercept).</li>
<li><span class="math inline">\(2d\)</span>: This is the <strong>Penalty
Term</strong>.</li>
</ul></li>
<li><strong>How it Works:</strong> <span class="math inline">\(AIC =
(\text{Poorness-of-Fit}) + (\text{Complexity Penalty})\)</span>. As you
add predictors, the fit gets better (the deviance term goes down), but
the penalty term (<span class="math inline">\(2d\)</span>) goes up.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> AIC.</li>
</ul>
<h3 id="bayesian-information-criterion-bic">Bayesian Information
Criterion (BIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(BIC =
-2 \log \ell(\hat{\theta}) + \log(n)d\)</span></li>
<li><strong>Concept:</strong> This is mathematically identical to AIC,
but the penalty term is different.
<ul>
<li><strong>AIC Penalty:</strong> <span
class="math inline">\(2d\)</span></li>
<li><strong>BIC Penalty:</strong> <span
class="math inline">\(\log(n)d\)</span></li>
</ul></li>
<li><strong>Comparison:</strong>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations
in your dataset.</li>
<li>As long as your dataset has 8 or more observations (<span
class="math inline">\(n \ge 8\)</span>), <span
class="math inline">\(\log(n)\)</span> will be greater than 2.</li>
<li>This means <strong>BIC applies a much harsher penalty for
complexity</strong> than AIC.</li>
</ul></li>
<li><strong>Consequence:</strong> BIC will tend to choose
<em>simpler</em> models (fewer predictors) than AIC.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> BIC.</li>
</ul>
<h2 id="the-deeper-theory-why-aic-works">The Deeper Theory: Why AIC
Works</h2>
<p>Slide 27 (â€œUnderstanding AICâ€) gives the deep mathematical
justification.</p>
<ul>
<li><strong>Goal:</strong> We have a <em>true</em>, unknown process
<span class="math inline">\(p\)</span> that generates our data. We are
creating a model <span class="math inline">\(\hat{p}_j\)</span>. We want
our model to be as â€œcloseâ€ to the truth as possible.</li>
<li><strong>Kullback-Leibler (K-L) Distance:</strong> This is a function
<span class="math inline">\(K(p, \hat{p}_j)\)</span> that measures the
â€œinformation lostâ€ when you use your model <span
class="math inline">\(\hat{p}_j\)</span> to approximate the truth <span
class="math inline">\(p\)</span>. You want to <em>minimize</em> this
distance.</li>
<li><strong>The Math:</strong>
<ol type="1">
<li><span class="math inline">\(K(p, \hat{p}_j) = \int p(y) \log \left(
\frac{p(y)}{\hat{p}_j(y)} \right) dy\)</span></li>
<li>This splits into: <span class="math inline">\(K(p, \hat{p}_j) =
\underbrace{\int p(y) \log(p(y)) dy}_{\text{Constant}} -
\underbrace{\int p(y) \log(\hat{p}_j(y)) dy}_{\text{This is what we need
to maximize}}\)</span></li>
</ol></li>
<li><strong>The Problem:</strong> We canâ€™t calculate that second term
because it requires knowing the <em>true</em> function <span
class="math inline">\(p\)</span>.</li>
<li><strong>Akaikeâ€™s Insight:</strong> Akaike proved that the
log-likelihood we <em>can</em> calculate, <span
class="math inline">\(\log \ell(\hat{\theta})\)</span>, is a
<em>biased</em> estimator of that target. He also proved that the bias
is approximately <span class="math inline">\(-d\)</span>.</li>
<li><strong>The Solution:</strong> An <em>unbiased</em> estimate of the
target is <span class="math inline">\(\log \ell(\hat{\theta}) -
d\)</span>.</li>
<li><strong>Final Step:</strong> For historical and statistical reasons,
he multiplied this by <span class="math inline">\(-2\)</span> to create
the final AIC formula.</li>
<li><strong>Conclusion:</strong> AIC is not just a random formula. It is
a carefully derived estimate of how much information your model loses
compared to the â€œtruthâ€ (i.e., its expected performance on new
data).</li>
</ul>
<h2 id="aicbic-for-linear-regression">AIC/BIC for Linear Regression</h2>
<p>Slide 26 shows how these general formulas simplify for linear
regression (assuming normal, Gaussian errors).</p>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Linear Regression Formula:</strong> <span
class="math inline">\(AIC = \frac{1}{n\hat{\sigma}^2}(RSS +
2d\hat{\sigma}^2)\)</span></li>
</ul>
<p><strong>Key Insight:</strong> For linear regression, the
â€œpoorness-of-fitâ€ term (<span class="math inline">\(-2 \log
\ell(\hat{\theta})\)</span>) is <em>directly proportional to</em> the
<span class="math inline">\(RSS\)</span>.</p>
<p>This makes it much easier to understand. You can just think of the
formulas as: * <strong>AIC <span class="math inline">\(\approx\)</span>
<span class="math inline">\(RSS + 2d\hat{\sigma}^2\)</span></strong> *
<strong>BIC <span class="math inline">\(\approx\)</span> <span
class="math inline">\(RSS + \log(n)d\hat{\sigma}^2\)</span></strong></p>
<p>(Here <span class="math inline">\(\hat{\sigma}^2\)</span> is an
estimate of the error variance, which can often be treated as a
constant).</p>
<p>This clearly shows the trade-off: We want a model with a low
<strong><span class="math inline">\(RSS\)</span></strong> (good fit) and
a low <strong><span class="math inline">\(d\)</span></strong> (low
complexity). These two goals are in direct competition.</p>
<p><strong>Mallowâ€™s <span class="math inline">\(C_p\)</span>:</strong>
The slide notes that <span class="math inline">\(C_p\)</span> is
equivalent to AIC for linear regression. The <span
class="math inline">\(C_p\)</span> formula is <span
class="math inline">\(C_p = \frac{1}{n}(RSS +
2d\hat{\sigma}^2_{full})\)</span>, where <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> is the error
variance estimated from the <em>full</em> model. Since <span
class="math inline">\(n\)</span> and <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> are constants,
minimizing <span class="math inline">\(C_p\)</span> is mathematically
identical to minimizing <span class="math inline">\(RSS +
2d\hat{\sigma}^2_{full}\)</span>, which is the same logic as AIC.</p>
<p>Here is a detailed breakdown of the mathematical formulas and
concepts from your slides.</p>
<h2 id="the-core-problem-training-error-vs.-test-error">The Core
Problem: Training Error vs.Â Test Error</h2>
<p>The central theme of these slides is finding the â€œbestâ€ model. The
problem is that a model with more predictors (more complex) will
<em>always</em> fit the data it was trained on better. This is a
trap.</p>
<ul>
<li><strong>Training Error:</strong> How well the model fits the data we
used to build it. <strong><span class="math inline">\(R^2\)</span> and
<span class="math inline">\(RSS\)</span> measure this.</strong></li>
<li><strong>Test Error:</strong> How well the model predicts new, unseen
data. This is what we <em>actually</em> care about. A model that is too
complex (e.g., has 10 predictors when only 3 are useful) will have low
training error but very high test error. This is called
<strong>overfitting</strong>.</li>
</ul>
<p>The goal is to choose a model that has the lowest <em>test
error</em>. The metrics below (Adjusted <span
class="math inline">\(R^2\)</span>, AIC, BIC) are all attempts to
<em>estimate</em> this test error without having to actually collect new
data. They do this by adding a <strong>penalty</strong> for
complexity.</p>
<h2 id="basic-metrics-measures-of-fit-1">Basic Metrics (Measures of
Fit)</h2>
<p>These formulas from slide 13 describe how well a model fits the
<em>training data</em>.</p>
<h3 id="residue-error">Residue (Error)</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\epsilon}_i = y_i - \hat{y}_i = y_i -
\hat{\beta}_0 - \sum_{j=1}^{p} \hat{\beta}_j x_{ij}\)</span></li>
<li><strong>Concept:</strong> This is the most basic building block.
Itâ€™s the difference between the <em>actual</em> observed value (<span
class="math inline">\(y_i\)</span>) and the value your model
<em>predicted</em> (<span class="math inline">\(\hat{y}_i\)</span>). It
is the â€œerrorâ€ for a single data point.</li>
</ul>
<h3 id="residual-sum-of-squares-rss">Residual Sum of Squares (RSS)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(RSS =
\sum_{i=1}^{n} \hat{\epsilon}_i^2\)</span></li>
<li><strong>Concept:</strong> This is the overall measure of model
error. You square all the individual errors (residues) to make them
positive and then add them all up.</li>
<li><strong>Goal:</strong> The entire process of linear regression
(called â€œOrdinary Least Squaresâ€) is designed to find the <span
class="math inline">\(\hat{\beta}\)</span> coefficients that make this
<strong>RSS value as small as possible</strong>.</li>
<li><strong>The Flaw:</strong> <span class="math inline">\(RSS\)</span>
will <em>always</em> decrease (or stay the same) as you add more
predictors (<span class="math inline">\(p\)</span>). A model with all 10
predictors will have a lower <span class="math inline">\(RSS\)</span>
than a model with 9, even if that 10th predictor is useless. Therefore,
<span class="math inline">\(RSS\)</span> is useless for choosing
<em>between</em> models of different sizes.</li>
</ul>
<h3 id="r-squared-r2-1">R-squared (<span
class="math inline">\(R^2\)</span>)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(R^2 = 1 -
\frac{SS_{error}}{SS_{total}} = 1 - \frac{RSS}{\sum_{i=1}^{n} (y_i -
\bar{y})^2}\)</span></li>
<li><strong>Concept:</strong> This metric reframes <span
class="math inline">\(RSS\)</span> into a more interpretable percentage.
<ul>
<li><span class="math inline">\(SS_{total}\)</span> (the denominator)
represents the <em>total variance</em> of the data. Itâ€™s the error you
would get if your â€œmodelâ€ was just guessing the average value (<span
class="math inline">\(\bar{y}\)</span>) for every single
observation.</li>
<li><span class="math inline">\(SS_{error}\)</span> (the <span
class="math inline">\(RSS\)</span>) is the error <em>after</em> using
your model.</li>
<li><span class="math inline">\(R^2\)</span> is the â€œproportion of total
variance explained by the model.â€ An <span
class="math inline">\(R^2\)</span> of 0.75 means your model can explain
75% of the variation in the response variable.</li>
</ul></li>
<li><strong>The Flaw:</strong> Just like <span
class="math inline">\(RSS\)</span>, <span
class="math inline">\(R^2\)</span> will <em>always</em> increase (or
stay the same) as you add more predictors. This is visually confirmed in
Figure 6.1, where the red line for <span
class="math inline">\(R^2\)</span> only goes up. It will always pick the
most complex model.</li>
</ul>
<h2 id="advanced-metrics-for-model-selection">Advanced Metrics (For
Model Selection)</h2>
<p>These metrics â€œfixâ€ the flaw of <span
class="math inline">\(R^2\)</span> by including a penalty for the number
of predictors.</p>
<h3 id="adjusted-r2-1">Adjusted <span
class="math inline">\(R^2\)</span></h3>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
  \text{Adjusted } R^2 = 1 - \frac{RSS / (n - p - 1)}{SS_{total} / (n -
1)}
  \]</span></li>
<li><strong>Mathematical Concept:</strong> This formula replaces the
â€œSum of Squaresâ€ (<span class="math inline">\(SS\)</span>) with â€œMean
Squaresâ€ (<span class="math inline">\(MS\)</span>).
<ul>
<li><span class="math inline">\(MS_{error} =
\frac{RSS}{n-p-1}\)</span></li>
<li><span class="math inline">\(MS_{total} =
\frac{SS_{total}}{n-1}\)</span></li>
</ul></li>
<li><strong>The â€œPenaltyâ€ Explained:</strong> The penalty is
<strong>degrees of freedom</strong>.
<ul>
<li><span class="math inline">\(n\)</span> = number of data points.</li>
<li><span class="math inline">\(p\)</span> = number of predictors.</li>
<li>The term <span class="math inline">\(n-p-1\)</span> is the degrees
of freedom for the residuals. You start with <span
class="math inline">\(n\)</span> data points, but you â€œuse upâ€ one
degree of freedom to estimate the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) and <span
class="math inline">\(p\)</span> more to estimate the <span
class="math inline">\(p\)</span> slopes.</li>
</ul></li>
<li><strong>How it Works:</strong>
<ol type="1">
<li>When you add a new predictor (increase <span
class="math inline">\(p\)</span>), <span
class="math inline">\(RSS\)</span> goes down, which makes the numerator
(<span class="math inline">\(MS_{error}\)</span>) smaller.</li>
<li>â€¦But, increasing <span class="math inline">\(p\)</span>
<em>also</em> decreases the denominator (<span
class="math inline">\(n-p-1\)</span>), which makes the numerator (<span
class="math inline">\(MS_{error}\)</span>) <em>larger</em>.</li>
</ol>
<ul>
<li>This creates a â€œtug-of-war.â€ If the new predictor is
<strong>useful</strong>, it will drop <span
class="math inline">\(RSS\)</span> a lot, and Adjusted <span
class="math inline">\(R^2\)</span> will <strong>increase</strong>. If
the new predictor is <strong>useless</strong>, <span
class="math inline">\(RSS\)</span> will barely change, and the penalty
from decreasing the denominator will win, causing Adjusted <span
class="math inline">\(R^2\)</span> to <strong>decrease</strong>.</li>
</ul></li>
<li><strong>Goal:</strong> You select the model with the
<strong>highest</strong> Adjusted <span
class="math inline">\(R^2\)</span>.</li>
</ul>
<h3 id="akaike-information-criterion-aic-1">Akaike Information Criterion
(AIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Concept Breakdown:</strong>
<ul>
<li><span class="math inline">\(\ell(\hat{\theta})\)</span>: This is the
<strong>Maximized Likelihood Function</strong>.
<ul>
<li>The <strong>Likelihood Function</strong> <span
class="math inline">\(\ell(\theta)\)</span> asks: â€œGiven a set of model
parameters <span class="math inline">\(\theta\)</span>, how probable is
the data we observed?â€</li>
<li>The <strong>Maximum Likelihood Estimate (MLE)</strong> <span
class="math inline">\(\hat{\theta}\)</span> is the specific set of
parameters (the <span class="math inline">\(\hat{\beta}\)</span>â€™s) that
<em>maximizes</em> this probability.</li>
</ul></li>
<li><span class="math inline">\(\log \ell(\hat{\theta})\)</span>: The
<strong>log-likelihood</strong>. This is just a number that represents
the <em>best possible fit</em> the model can achieve for the data. A
higher number is a better fit.</li>
<li><span class="math inline">\(-2 \log \ell(\hat{\theta})\)</span>:
This is the <strong>Deviance</strong>. Since a higher log-likelihood is
better, a <em>lower</em> deviance is better. This term measures
<strong>poorness-of-fit</strong>.</li>
<li><span class="math inline">\(d\)</span>: The number of parameters
estimated by the model. (e.g., <span class="math inline">\(p\)</span>
predictors + 1 intercept).</li>
<li><span class="math inline">\(2d\)</span>: This is the <strong>Penalty
Term</strong>.</li>
</ul></li>
<li><strong>How it Works:</strong> <span class="math inline">\(AIC =
(\text{Poorness-of-Fit}) + (\text{Complexity Penalty})\)</span>. As you
add predictors, the fit gets better (the deviance term goes down), but
the penalty term (<span class="math inline">\(2d\)</span>) goes up.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> AIC.</li>
</ul>
<h3 id="bayesian-information-criterion-bic-1">Bayesian Information
Criterion (BIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(BIC =
-2 \log \ell(\hat{\theta}) + \log(n)d\)</span></li>
<li><strong>Concept:</strong> This is mathematically identical to AIC,
but the penalty term is different.
<ul>
<li><strong>AIC Penalty:</strong> <span
class="math inline">\(2d\)</span></li>
<li><strong>BIC Penalty:</strong> <span
class="math inline">\(\log(n)d\)</span></li>
</ul></li>
<li><strong>Comparison:</strong>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations
in your dataset.</li>
<li>As long as your dataset has 8 or more observations (<span
class="math inline">\(n \ge 8\)</span>), <span
class="math inline">\(\log(n)\)</span> will be greater than 2.</li>
<li>This means <strong>BIC applies a much harsher penalty for
complexity</strong> than AIC.</li>
</ul></li>
<li><strong>Consequence:</strong> BIC will tend to choose
<em>simpler</em> models (fewer predictors) than AIC.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> BIC.</li>
</ul>
<h2 id="the-deeper-theory-why-aic-works-1">The Deeper Theory: Why AIC
Works</h2>
<p>Slide 27 (â€œUnderstanding AICâ€) gives the deep mathematical
justification.</p>
<ul>
<li><strong>Goal:</strong> We have a <em>true</em>, unknown process
<span class="math inline">\(p\)</span> that generates our data. We are
creating a model <span class="math inline">\(\hat{p}_j\)</span>. We want
our model to be as â€œcloseâ€ to the truth as possible.</li>
<li><strong>Kullback-Leibler (K-L) Distance:</strong> This is a function
<span class="math inline">\(K(p, \hat{p}_j)\)</span> that measures the
â€œinformation lostâ€ when you use your model <span
class="math inline">\(\hat{p}_j\)</span> to approximate the truth <span
class="math inline">\(p\)</span>. You want to <em>minimize</em> this
distance.</li>
<li><strong>The Math:</strong>
<ol type="1">
<li><span class="math inline">\(K(p, \hat{p}_j) = \int p(y) \log \left(
\frac{p(y)}{\hat{p}_j(y)} \right) dy\)</span></li>
<li>This splits into: <span class="math inline">\(K(p, \hat{p}_j) =
\underbrace{\int p(y) \log(p(y)) dy}_{\text{Constant}} -
\underbrace{\int p(y) \log(\hat{p}_j(y)) dy}_{\text{This is what we need
to maximize}}\)</span></li>
</ol></li>
<li><strong>The Problem:</strong> We canâ€™t calculate that second term
because it requires knowing the <em>true</em> function <span
class="math inline">\(p\)</span>.</li>
<li><strong>Akaikeâ€™s Insight:</strong> Akaike proved that the
log-likelihood we <em>can</em> calculate, <span
class="math inline">\(\log \ell(\hat{\theta})\)</span>, is a
<em>biased</em> estimator of that target. He also proved that the bias
is approximately <span class="math inline">\(-d\)</span>.</li>
<li><strong>The Solution:</strong> An <em>unbiased</em> estimate of the
target is <span class="math inline">\(\log \ell(\hat{\theta}) -
d\)</span>.</li>
<li><strong>Final Step:</strong> For historical and statistical reasons,
he multiplied this by <span class="math inline">\(-2\)</span> to create
the final AIC formula.</li>
<li><strong>Conclusion:</strong> AIC is not just a random formula. It is
a carefully derived estimate of how much information your model loses
compared to the â€œtruthâ€ (i.e., its expected performance on new
data).</li>
</ul>
<h2 id="aicbic-for-linear-regression-1">AIC/BIC for Linear
Regression</h2>
<p>Slide 26 shows how these general formulas simplify for linear
regression (assuming normal, Gaussian errors).</p>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Linear Regression Formula:</strong> <span
class="math inline">\(AIC = \frac{1}{n\hat{\sigma}^2}(RSS +
2d\hat{\sigma}^2)\)</span></li>
</ul>
<p><strong>Key Insight:</strong> For linear regression, the
â€œpoorness-of-fitâ€ term (<span class="math inline">\(-2 \log
\ell(\hat{\theta})\)</span>) is <em>directly proportional to</em> the
<span class="math inline">\(RSS\)</span>.</p>
<p>This makes it much easier to understand. You can just think of the
formulas as: * <strong>AIC <span class="math inline">\(\approx\)</span>
<span class="math inline">\(RSS + 2d\hat{\sigma}^2\)</span></strong> *
<strong>BIC <span class="math inline">\(\approx\)</span> <span
class="math inline">\(RSS + \log(n)d\hat{\sigma}^2\)</span></strong></p>
<p>(Here <span class="math inline">\(\hat{\sigma}^2\)</span> is an
estimate of the error variance, which can often be treated as a
constant).</p>
<p>This clearly shows the trade-off: We want a model with a low
<strong><span class="math inline">\(RSS\)</span></strong> (good fit) and
a low <strong><span class="math inline">\(d\)</span></strong> (low
complexity). These two goals are in direct competition.</p>
<p><strong>Mallowâ€™s <span class="math inline">\(C_p\)</span>:</strong>
The slide notes that <span class="math inline">\(C_p\)</span> is
equivalent to AIC for linear regression. The <span
class="math inline">\(C_p\)</span> formula is <span
class="math inline">\(C_p = \frac{1}{n}(RSS +
2d\hat{\sigma}^2_{full})\)</span>, where <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> is the error
variance estimated from the <em>full</em> model. Since <span
class="math inline">\(n\)</span> and <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> are constants,
minimizing <span class="math inline">\(C_p\)</span> is mathematically
identical to minimizing <span class="math inline">\(RSS +
2d\hat{\sigma}^2_{full}\)</span>, which is the same logic as AIC.</p>
<h1 id="variable-selection">3. Variable Selection</h1>
<h2 id="core-concept-the-problem-of-variable-selection">Core Concept:
The Problem of Variable Selection</h2>
<p>In regression, we want to model a response variable <span
class="math inline">\(Y\)</span> using a set of <span
class="math inline">\(p\)</span> predictor variables <span
class="math inline">\(X_1, X_2, ..., X_p\)</span>.</p>
<ul>
<li><p><strong>The â€œKitchen Sinkâ€ Problem:</strong> A common temptation
is to include all available predictors in the model: <span
class="math display">\[Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... +
\beta_pX_p + \epsilon\]</span> This often leads to
<strong>overfitting</strong>. The model may fit the training data well
but will perform poorly on new, unseen data. Itâ€™s also hard to interpret
a model with dozens of predictors.</p></li>
<li><p><strong>The Solution: Subset Selection.</strong> The goal is to
find a smaller subset of the predictors that builds a model that is:</p>
<ol type="1">
<li><strong>Accurate:</strong> Has low prediction error.</li>
<li><strong>Parsimonious:</strong> Uses the fewest predictors
necessary.</li>
<li><strong>Interpretable:</strong> Is simple enough for a human to
understand.</li>
</ol></li>
</ul>
<p>Your slides present two main methods to achieve this: <strong>Best
Subset Selection</strong> and <strong>Forward Stepwise
Selection</strong>.</p>
<h2 id="method-1-best-subset-selection-bss">Method 1: Best Subset
Selection (BSS)</h2>
<p>This is the â€œbrute forceâ€ approach. It considers <em>every single
possible model</em>.</p>
<h3 id="conceptual-algorithm">Conceptual Algorithm</h3>
<ol type="1">
<li>Fit all models with <span class="math inline">\(k=1\)</span>
predictor (there are <span class="math inline">\(p\)</span> of these).
Find the best one (lowest RSS) and call it <span
class="math inline">\(M_1\)</span>.</li>
<li>Fit all models with <span class="math inline">\(k=2\)</span>
predictors (there are <span class="math inline">\(\binom{p}{2}\)</span>
of these). Find the best one and call it <span
class="math inline">\(M_2\)</span>.</li>
<li>â€¦</li>
<li>Fit the one model with <span class="math inline">\(k=p\)</span>
predictors (the full model), <span
class="math inline">\(M_p\)</span>.</li>
<li>You now have a list of <span class="math inline">\(p\)</span> â€œbestâ€
models: <span class="math inline">\(M_1, M_2, ..., M_p\)</span>.</li>
<li>Use a selection criterion (like <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>BIC</strong>,
<strong>AIC</strong>, or <strong><span
class="math inline">\(C_p\)</span></strong>) to choose the single best
model from this list.</li>
</ol>
<h3
id="mathematical-computational-cost-from-slide-225641.png">Mathematical
&amp; Computational Cost (from slide <code>225641.png</code>)</h3>
<ul>
<li>For each predictor, there are two possibilities: itâ€™s either
<strong>IN</strong> the model or <strong>OUT</strong>.</li>
<li>With <span class="math inline">\(p\)</span> predictors, the total
number of models to test is <span class="math inline">\(2 \times 2
\times ... \times 2\)</span> (<span class="math inline">\(p\)</span>
times).</li>
<li><strong>Total Models = <span
class="math inline">\(2^p\)</span></strong></li>
<li>This is a â€œcombinatorial explosion.â€ As the slide notes, if <span
class="math inline">\(p=20\)</span>, <span class="math inline">\(2^{20}
= 1,048,576\)</span> models. This is computationally infeasible for
large <span class="math inline">\(p\)</span>.</li>
</ul>
<h2 id="method-2-forward-stepwise-selection-fss">Method 2: Forward
Stepwise Selection (FSS)</h2>
<p>This is a â€œgreedyâ€ algorithm. Itâ€™s an efficient alternative to BSS
that does <em>not</em> test every model.</p>
<h3
id="conceptual-algorithm-from-slides-225645.png-225648.png">Conceptual
Algorithm (from slides <code>225645.png</code> &amp;
<code>225648.png</code>)</h3>
<ul>
<li><p><strong>Step 1:</strong> Start with the <strong>null
model</strong>, <span class="math inline">\(M_0\)</span>, which has no
predictors. <span class="math display">\[M_0: Y = \beta_0 +
\epsilon\]</span> The prediction is just the sample mean of <span
class="math inline">\(Y\)</span>.</p></li>
<li><p><strong>Step 2 (Iterative):</strong></p>
<ul>
<li><strong>For <span class="math inline">\(k=0\)</span> (to get <span
class="math inline">\(M_1\)</span>):</strong> Fit all <span
class="math inline">\(p\)</span> models that add <em>one</em> predictor
to <span class="math inline">\(M_0\)</span>. Choose the best one (lowest
<strong>RSS</strong> or highest <strong><span
class="math inline">\(R^2\)</span></strong>). This is <span
class="math inline">\(M_1\)</span>. Letâ€™s say it contains <span
class="math inline">\(X_1\)</span>.</li>
<li><strong>For <span class="math inline">\(k=1\)</span> (to get <span
class="math inline">\(M_2\)</span>):</strong> <em>Keep</em> <span
class="math inline">\(X_1\)</span> in the model. Fit all <span
class="math inline">\(p-1\)</span> models that add <em>one more</em>
predictor to <span class="math inline">\(M_1\)</span> (e.g., <span
class="math inline">\(M_1+X_2\)</span>, <span
class="math inline">\(M_1+X_3\)</span>, â€¦). Choose the best of these.
This is <span class="math inline">\(M_2\)</span>.</li>
<li><strong>Repeat:</strong> Continue this process, adding one variable
at a time, until all <span class="math inline">\(p\)</span> predictors
are in the model <span class="math inline">\(M_p\)</span>.</li>
</ul></li>
<li><p><strong>Step 3:</strong> You now have a sequence of <span
class="math inline">\(p+1\)</span> models: <span
class="math inline">\(M_0, M_1, ..., M_p\)</span>. Choose the single
best model from this sequence using <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>AIC</strong>,
<strong>BIC</strong>, or <strong><span
class="math inline">\(C_p\)</span></strong>.</p></li>
</ul>
<h3
id="mathematical-computational-cost-from-slide-225651.png">Mathematical
&amp; Computational Cost (from slide <code>225651.png</code>)</h3>
<ul>
<li>To find <span class="math inline">\(M_1\)</span>, you fit <span
class="math inline">\(p\)</span> models.</li>
<li>To find <span class="math inline">\(M_2\)</span>, you fit <span
class="math inline">\(p-1\)</span> models.</li>
<li>To find <span class="math inline">\(M_p\)</span>, you fit <span
class="math inline">\(1\)</span> model.</li>
<li>The null model <span class="math inline">\(M_0\)</span> is 1
model.</li>
<li><strong>Total Models = <span class="math inline">\(1 +
\sum_{k=0}^{p-1} (p-k) = 1 + p + (p-1) + ... + 1 = 1 +
\frac{p(p+1)}{2}\)</span></strong></li>
<li>As the slide notes, if <span class="math inline">\(p=20\)</span>,
this is only <span class="math inline">\(1 + 20(21)/2 = 211\)</span>
models. This is vastly more efficient than BSS.</li>
<li><strong>Key weakness:</strong> The method is â€œgreedy.â€ If it adds
<span class="math inline">\(X_1\)</span> in Step 1, it can
<em>never</em> be removed. Itâ€™s possible the true best 2-variable model
is <span class="math inline">\((X_2, X_3)\)</span>, but if FSS chose
<span class="math inline">\(X_1\)</span> as the best 1-variable model,
it will never find <span class="math inline">\((X_2, X_3)\)</span>.</li>
</ul>
<h2 id="how-to-choose-the-best-model-the-criteria">4. How to Choose the
â€œBestâ€ Model: The Criteria</h2>
<p>You canâ€™t use <strong>RSS</strong> or <strong><span
class="math inline">\(R^2\)</span></strong> to compare models with
<em>different numbers of predictors</em> (<span
class="math inline">\(k\)</span>). This is because RSS always decreases
(and <span class="math inline">\(R^2\)</span> always increases) as you
add more variables. You <em>must</em> use a criterion that penalizes
complexity.</p>
<ul>
<li><p><strong>RSS (Residual Sum of Squares):</strong> Goal is to
<strong>minimize</strong>. <span class="math display">\[RSS =
\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span> Good for comparing models
<em>of the same size <span
class="math inline">\(k\)</span></em>.</p></li>
<li><p><strong>Adjusted R-squared (<span class="math inline">\(Adj.
R^2\)</span>):</strong> Goal is to <strong>maximize</strong>. <span
class="math display">\[Adj. R^2 = 1 -
\frac{(1-R^2)(n-1)}{n-p-1}\]</span> This â€œadjustsâ€ <span
class="math inline">\(R^2\)</span> by adding a penalty for having more
predictors (<span class="math inline">\(p\)</span>). Adding a useless
predictor will make <span class="math inline">\(Adj. R^2\)</span> go
down.</p></li>
<li><p><strong>Mallowâ€™s <span
class="math inline">\(C_p\)</span>:</strong> Goal is to
<strong>minimize</strong>. <span class="math display">\[C_p \approx
\frac{1}{n}(RSS + 2p\hat{\sigma}^2)\]</span> Here, <span
class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the
error variance from the <em>full model</em> (with all <span
class="math inline">\(p\)</span> predictors). A good model will have
<span class="math inline">\(C_p \approx p\)</span>.</p></li>
<li><p><strong>AIC (Akaike Information Criterion) &amp; BIC (Bayesian
Information Criterion):</strong> Goal is to <strong>minimize</strong>.
<span class="math display">\[AIC = 2p - 2\ln(\hat{L})\]</span> <span
class="math display">\[BIC = p\ln(n) - 2\ln(\hat{L})\]</span> Here,
<span class="math inline">\(\hat{L}\)</span> is the maximized likelihood
of the model. You donâ€™t need to calculate this by hand; software
provides it.</p>
<ul>
<li><strong>Key difference:</strong> BICâ€™s penalty for <span
class="math inline">\(p\)</span> is <span
class="math inline">\(p\ln(n)\)</span>, while AICâ€™s is <span
class="math inline">\(2p\)</span>. Since <span
class="math inline">\(\ln(n)\)</span> is almost always <span
class="math inline">\(&gt; 2\)</span> (for <span
class="math inline">\(n&gt;7\)</span>), <strong>BIC applies a much
heavier penalty for complexity</strong>.</li>
<li>This means <strong>BIC tends to choose smaller, more parsimonious
models</strong> than AIC or <span class="math inline">\(Adj.
R^2\)</span>.</li>
</ul></li>
</ul>
<h2 id="python-code-analysis-slide-225546.jpg">5. Python Code Analysis
(Slide <code>225546.jpg</code>)</h2>
<p>This slide shows the Python code for <strong>Best Subset
Selection</strong> (BSS).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations <span class="comment"># &lt;-- This is the BSS engine</span></span><br></pre></td></tr></table></figure>
<h3 id="block-1-load-the-credit-dataset">Block 1: Load the Credit
dataset</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Load the Credit dataset</span></span><br><span class="line">Credit = pd.read_csv(<span class="string">&#x27;Credit.csv&#x27;</span>)</span><br><span class="line">Credit[<span class="string">&#x27;ID&#x27;</span>] = Credit[<span class="string">&#x27;ID&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">(num_samples, num_predictors) = Credit.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert categorical text data to numerical (dummy variables)</span></span><br><span class="line">Credit[<span class="string">&#x27;Gender&#x27;</span>] = Credit[<span class="string">&#x27;Gender&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Male&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Female&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Student&#x27;</span>] = Credit[<span class="string">&#x27;Student&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Yes&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;No&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Married&#x27;</span>] = Credit[<span class="string">&#x27;Married&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Yes&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;No&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Ethnicity&#x27;</span>] = Credit[<span class="string">&#x27;Ethnicity&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Asian&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Caucasian&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;African American&#x27;</span>: <span class="number">0</span>&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>pd.read_csv</code></strong>: Reads the data into a
<code>pandas</code> DataFrame.</li>
<li><strong><code>.map()</code></strong>: This is a crucial
preprocessing step. Regression models require numbers, not text like
â€˜Yesâ€™ or â€˜Maleâ€™. This line converts those strings into <code>1</code>s
and <code>0</code>s.</li>
</ul>
<h3 id="block-2-plot-scatterplot-matrix">Block 2: Plot scatterplot
matrix</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. Plot scatterplot matrix</span></span><br><span class="line">selected_columns = [<span class="string">&#x27;Balance&#x27;</span>, <span class="string">&#x27;Education&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Cards&#x27;</span>, <span class="string">&#x27;Rating&#x27;</span>, <span class="string">&#x27;Limit&#x27;</span>, <span class="string">&#x27;Income&#x27;</span>]</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&quot;ticks&quot;</span>)</span><br><span class="line">sns.pairplot(Credit[selected_columns], diag_kind=<span class="string">&#x27;kde&#x27;</span>)</span><br><span class="line">plt.suptitle(<span class="string">&#x27;Scatterplot Matrix&#x27;</span>, y=<span class="number">1.02</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>sns.pairplot</code></strong>: A powerful visualization
from the <code>seaborn</code> library. The resulting plot (right side of
the slide) is a grid.
<ul>
<li><strong>Diagonal plots (kde)</strong>: Show the distribution (Kernel
Density Estimate) of a single variable (e.g., â€˜Balanceâ€™ is skewed
right).</li>
<li><strong>Off-diagonal plots (scatter)</strong>: Show the relationship
between two variables (e.g., â€˜Limitâ€™ and â€˜Ratingâ€™ are almost perfectly
linear). This helps you visually spot potentially strong
predictors.</li>
</ul></li>
</ul>
<h3 id="block-3-best-subset-selection">Block 3: Best Subset
Selection</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Best Subset Selection</span></span><br><span class="line"><span class="comment"># (This code is incomplete on the slide, I&#x27;ll fill in the logic)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define target and predictors</span></span><br><span class="line">target = <span class="string">&#x27;Balance&#x27;</span></span><br><span class="line">predictors = [col <span class="keyword">for</span> col <span class="keyword">in</span> Credit.columns <span class="keyword">if</span> col != target] </span><br><span class="line">nvmax = <span class="number">10</span> <span class="comment"># Max number of predictors to test (up to 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize lists to store model statistics</span></span><br><span class="line">model_stats = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over number of predictors from 1 to nvmax</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, nvmax + <span class="number">1</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Generate all possible combinations of predictors of size k</span></span><br><span class="line">    <span class="comment"># This is the core of BSS</span></span><br><span class="line">    <span class="keyword">for</span> subset <span class="keyword">in</span> <span class="built_in">list</span>(combinations(predictors, k)):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the design matrix (X)</span></span><br><span class="line">        X_subset = Credit[<span class="built_in">list</span>(subset)]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Add a constant (intercept) term to the model</span></span><br><span class="line">        <span class="comment"># Y = B0 + B1*X1 -&gt; statsmodels needs B0 to be added manually</span></span><br><span class="line">        X_subset_const = sm.add_constant(X_subset)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the target variable (y)</span></span><br><span class="line">        y_target = Credit[target]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Fit the Ordinary Least Squares (OLS) model</span></span><br><span class="line">        model = sm.OLS(y_target, X_subset_const).fit()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate RSS</span></span><br><span class="line">        RSS = ((model.resid) ** <span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (The full code would also calculate R-squared, Adj. R-sq, BIC, etc. here)</span></span><br><span class="line">        <span class="comment"># model_stats.append(&#123;&#x27;k&#x27;: k, &#x27;subset&#x27;: subset, &#x27;RSS&#x27;: RSS, ...&#125;)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>for k in range(1, nvmax + 1)</code></strong>: This is
the <em>outer</em> loop that iterates from <span
class="math inline">\(k=1\)</span> (1 predictor) to <span
class="math inline">\(k=10\)</span> (10 predictors).</li>
<li><strong><code>list(combinations(predictors, k))</code></strong>:
This is the <em>inner</em> loop and the <strong>most important
line</strong>. The <code>itertools.combinations</code> function is a
highly efficient way to generate all unique subsets.
<ul>
<li>When <span class="math inline">\(k=1\)</span>, it returns
<code>[('Income',), ('Limit',), ('Rating',), ...]</code>.</li>
<li>When <span class="math inline">\(k=2\)</span>, it returns
<code>[('Income', 'Limit'), ('Income', 'Rating'), ('Limit', 'Rating'), ...]</code>.</li>
<li>This is what generates the <span class="math inline">\(2^p\)</span>
(or in this case, <span class="math inline">\(\sum_{k=1}^{10}
\binom{p}{k}\)</span>) models to test.</li>
</ul></li>
<li><strong><code>sm.add_constant(X_subset)</code></strong>: Your
regression equation is <span class="math inline">\(Y = \beta_0 +
\beta_1X_1\)</span>. The <span class="math inline">\(X_1\)</span> is
your <code>X_subset</code>. The <code>sm.add_constant</code> function
adds a column of <code>1</code>s to your data, which allows the
<code>statsmodels</code> library to estimate the <span
class="math inline">\(\beta_0\)</span> (intercept) term.</li>
<li><strong><code>sm.OLS(y_target, X_subset_const).fit()</code></strong>:
This fits the Ordinary Least Squares (OLS) model, which finds the <span
class="math inline">\(\beta\)</span> coefficients that <strong>minimize
the RSS</strong>.</li>
<li><strong><code>model.resid</code></strong>: This attribute of the
fitted model contains the residuals (<span class="math inline">\(e_i =
y_i - \hat{y}_i\)</span>) for each data point.</li>
<li><strong><code>((model.resid) ** 2).sum()</code></strong>: This line
is the direct code implementation of the formula <span
class="math inline">\(RSS = \sum e_i^2\)</span>.</li>
</ul>
<h2 id="synthesizing-the-results-the-plots">Synthesizing the Results
(The Plots)</h2>
<p>After running the BSS code, you get the data used in the plots and
the table.</p>
<ul>
<li><p><strong>Image <code>225550.png</code> (Adjusted
R-squared)</strong></p>
<ul>
<li><strong>Goal:</strong> Maximize.</li>
<li><strong>What it shows:</strong> The gray dots are <em>all</em> the
models tested for each <span class="math inline">\(k\)</span>. The red
line connects the single <em>best</em> model for each <span
class="math inline">\(k\)</span>.</li>
<li><strong>Conclusion:</strong> The plot shows a sharp â€œelbow.â€ The
<span class="math inline">\(Adj. R^2\)</span> increases dramatically up
to <span class="math inline">\(k=4\)</span>, then increases very slowly.
The maximum is around <span class="math inline">\(k=6\)</span> or <span
class="math inline">\(k=7\)</span>, but the gain after <span
class="math inline">\(k=4\)</span> is minimal.</li>
</ul></li>
<li><p><strong>Image <code>225554.png</code> (BIC)</strong></p>
<ul>
<li><strong>Goal:</strong> Minimize.</li>
<li><strong>What it shows:</strong> BIC heavily penalizes
complexity.</li>
<li><strong>Conclusion:</strong> The plot shows a very clear minimum.
The BIC value plummets from <span class="math inline">\(k=2\)</span> to
<span class="math inline">\(k=3\)</span> and hits its lowest point at
<strong><span class="math inline">\(k=4\)</span></strong>. After <span
class="math inline">\(k=4\)</span>, the penalty for adding more
variables is <em>larger</em> than the benefit in model fit, so the BIC
score starts to rise. This is a very strong vote for the 4-predictor
model.</li>
</ul></li>
<li><p><strong>Image <code>225635.png</code> (Mallowâ€™s <span
class="math inline">\(C_p\)</span>)</strong></p>
<ul>
<li><strong>Goal:</strong> Minimize.</li>
<li><strong>What it shows:</strong> A very similar story to BIC.</li>
<li><strong>Conclusion:</strong> The <span
class="math inline">\(C_p\)</span> value drops significantly and hits
its minimum at <strong><span
class="math inline">\(k=4\)</span></strong>.</li>
</ul></li>
<li><p><strong>Image <code>225638.png</code> (Summary
Table)</strong></p>
<ul>
<li>This is the <strong>most important image</strong> for the final
conclusion. It summarizes the red line from all the plots.</li>
<li>Look at the row for <code>Num_Predictors = 4</code>. The predictors
are <strong>(Income, Limit, Cards, Student)</strong>.</li>
<li>Now look at the columns for <code>BIC</code> and <code>Cp</code>.
<ul>
<li><strong>BIC:</strong> <code>4841.615607</code>. This is the lowest
value in the entire <code>BIC</code> column (the value at <span
class="math inline">\(k=3\)</span> is <code>4865.352851</code>).</li>
<li><strong>Cp:</strong> <code>7.122228</code>. This is also the lowest
value in the <code>Cp</code> column.</li>
</ul></li>
<li>The <code>Adj_R_squared</code> at <span
class="math inline">\(k=4\)</span> is <code>0.953580</code>, which is
very close to its maximum of <code>~0.954</code> at <span
class="math inline">\(k=7-10\)</span>.</li>
</ul></li>
</ul>
<p><strong>Final Conclusion:</strong> All three â€œpenalizedâ€ criteria
(Adjusted <span class="math inline">\(R^2\)</span>, BIC, and <span
class="math inline">\(C_p\)</span>) point to the same conclusion. While
<span class="math inline">\(Adj. R^2\)</span> is a bit ambiguous,
<strong>BIC and <span class="math inline">\(C_p\)</span> provide a clear
signal that the best, most parsimonious model is the 4-predictor model
using <code>Income</code>, <code>Limit</code>, <code>Cards</code>, and
<code>Student</code></strong>.</p>
<h1 id="subset-selection">4. Subset Selection</h1>
<h2 id="summary-of-subset-selection">Summary of Subset Selection</h2>
<p>These slides introduce <strong>subset selection</strong>, a process
in statistical learning used to identify the best subset of predictors
(variables) for a regression model. The goal is to find a model that has
low prediction error and avoids overfitting by excluding irrelevant
variables.</p>
<p>The slides cover two main â€œgreedyâ€ (stepwise) algorithms and the
criteria used to select the final best model.</p>
<h2 id="stepwise-selection-algorithms">Stepwise Selection
Algorithms</h2>
<p>Instead of testing all <span class="math inline">\(2^p\)</span>
possible models (which is â€œbest subset selectionâ€ and computationally
unfeasible), stepwise methods build a single path of models.</p>
<h3 id="forward-stepwise-selection">Forward Stepwise Selection</h3>
<p>This is an <strong>additive</strong> (bottom-up) approach:</p>
<ol type="1">
<li><strong>Start</strong> with the null model (no predictors).</li>
<li><strong>Find</strong> the best 1-variable model (the one that gives
the lowest Residual Sum of Squares, or RSS).</li>
<li><strong>Add</strong> the single variable that, when added to the
current model, results in the <em>new</em> best model (lowest RSS).</li>
<li><strong>Repeat</strong> this process until all <span
class="math inline">\(p\)</span> predictors are in the model.</li>
<li>This generates a sequence of <span
class="math inline">\(p+1\)</span> models, from <span
class="math inline">\(\mathcal{M}_0\)</span> to <span
class="math inline">\(\mathcal{M}_p\)</span>.</li>
</ol>
<h3 id="backward-stepwise-selection">Backward Stepwise Selection</h3>
<p>This is a <strong>subtractive</strong> (top-down) approach:</p>
<ol type="1">
<li><strong>Start</strong> with the full model containing all <span
class="math inline">\(p\)</span> predictors.</li>
<li><strong>Find</strong> the best <span
class="math inline">\((p-1)\)</span>-variable model by <em>removing</em>
the single variable that results in the <em>lowest RSS</em> (or highest
<span class="math inline">\(R^2\)</span>). This variable is considered
the least significant.</li>
<li><strong>Remove</strong> the next variable that, when removed from
the current best model, gives the new best model.</li>
<li><strong>Repeat</strong> until only the null model remains.</li>
<li>This also generates a sequence of <span
class="math inline">\(p+1\)</span> models.</li>
</ol>
<h4 id="pros-and-cons-backward-selection">Pros and Cons (Backward
Selection)</h4>
<ul>
<li><strong>Pro:</strong> Computationally efficient compared to best
subset. It fits <span class="math inline">\(1 + \sum_{k=0}^{p-1}(p-k) =
\mathbf{1 + p(p+1)/2}\)</span> models, which is much less than <span
class="math inline">\(2^p\)</span>. (e.g., for <span
class="math inline">\(p=20\)</span>, itâ€™s 211 models vs.Â &gt;1
million).</li>
<li><strong>Con:</strong> <strong>Cannot be used if <span
class="math inline">\(p &gt; n\)</span></strong> (more predictors than
observations), because the initial full model cannot be fit.</li>
<li><strong>Con (for both):</strong> These methods are
<strong>greedy</strong>. A variable added in forward selection is
<em>never removed</em>, and a variable removed in backward selection is
<em>never added back</em>. This means they are not guaranteed to find
the true best model.</li>
</ul>
<h2 id="choosing-the-final-best-model">Choosing the Final Best
Model</h2>
<p>Both forward and backward selection give you a set of candidate
models (e.g., the best 1-variable model, best 2-variable model, etc.).
You must then choose the <em>single best</em> one. The slides show two
main approaches:</p>
<h3 id="a.-direct-error-estimation">A. Direct Error Estimation</h3>
<p>Use a validation set or cross-validation (CV) to estimate the test
error for each model (e.g., the 1-variable, 2-variableâ€¦ models).
<strong>Choose the model with the lowest estimated test
error.</strong></p>
<h3 id="b.-adjusted-metrics-penalizing-for-complexity">B. Adjusted
Metrics (Penalizing for Complexity)</h3>
<p>Standard RSS and <span class="math inline">\(R^2\)</span> will always
improve as you add variables, leading to overfitting. Instead, use
metrics that <em>penalize</em> the model for having too many
predictors.</p>
<ul>
<li><p><strong>Mallowsâ€™ <span
class="math inline">\(C_p\)</span>:</strong> An estimate of test Mean
Squared Error (MSE). <span class="math display">\[C_p = \frac{1}{n} (RSS
+ 2d\hat{\sigma}^2)\]</span> (where <span
class="math inline">\(d\)</span> is the number of predictors, and <span
class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the
error variance). <strong>You want to find the model with the
<em>minimum</em> <span
class="math inline">\(C_p\)</span>.</strong></p></li>
<li><p><strong>BIC (Bayesian Information Criterion):</strong> <span
class="math display">\[BIC = \frac{1}{n} (RSS +
\log(n)d\hat{\sigma}^2)\]</span> BICâ€™s penalty <span
class="math inline">\(\log(n)\)</span> is stronger than <span
class="math inline">\(C_p\)</span>â€™s (or AICâ€™s) penalty of <span
class="math inline">\(2\)</span>, so it tends to select <em>smaller</em>
(more parsimonious) models. <strong>You want to find the model with the
<em>minimum</em> BIC.</strong></p></li>
<li><p><strong>Adjusted <span
class="math inline">\(R^2\)</span>:</strong> <span
class="math display">\[R^2_{adj} = 1 -
\frac{RSS/(n-d-1)}{TSS/(n-1)}\]</span> (where <span
class="math inline">\(TSS\)</span> is the Total Sum of Squares). Unlike
<span class="math inline">\(R^2\)</span>, this metric can decrease if
adding a variable doesnâ€™t help enough. <strong>You want to find the
model with the <em>maximum</em> Adjusted <span
class="math inline">\(R^2\)</span>.</strong></p></li>
</ul>
<h2 id="python-code-understanding">Python Code Understanding</h2>
<p>The slides use the <code>regsubsets()</code> function from the
<code>leaps</code> package in <strong>R</strong>.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># R Code from slides</span></span><br><span class="line">library<span class="punctuation">(</span>leaps<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Forward Selection</span></span><br><span class="line">regfit.fwd <span class="operator">&lt;-</span> regsubsets<span class="punctuation">(</span>Balance<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Credit<span class="punctuation">,</span> method<span class="operator">=</span><span class="string">&quot;forward&quot;</span><span class="punctuation">,</span> nvmax<span class="operator">=</span><span class="number">11</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Backward Selection</span></span><br><span class="line">regfit.bwd <span class="operator">&lt;-</span> regsubsets<span class="punctuation">(</span>Balance<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Credit<span class="punctuation">,</span> method<span class="operator">=</span><span class="string">&quot;backward&quot;</span><span class="punctuation">,</span> nvmax<span class="operator">=</span><span class="number">11</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>In <strong>Python</strong>, the standard tool for this is
<code>SequentialFeatureSelector</code> from
<strong><code>scikit-learn</code></strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SequentialFeatureSelector</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Credit&#x27; is a pandas DataFrame with &#x27;Balance&#x27; as the target</span></span><br><span class="line">X = Credit.drop(<span class="string">&#x27;Balance&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = Credit[<span class="string">&#x27;Balance&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the linear regression estimator</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Forward Selection ---</span></span><br><span class="line"><span class="comment"># direction=&#x27;forward&#x27; starts with 0 features and adds them</span></span><br><span class="line"><span class="comment"># To get the best 4-variable model, for example:</span></span><br><span class="line">sfs_forward = SequentialFeatureSelector(</span><br><span class="line">    model,</span><br><span class="line">    n_features_to_select=<span class="number">4</span>,</span><br><span class="line">    direction=<span class="string">&#x27;forward&#x27;</span>,</span><br><span class="line">    cv=<span class="literal">None</span> <span class="comment"># Or use cross-validation, e.g., cv=10</span></span><br><span class="line">)</span><br><span class="line">sfs_forward.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Forward selection best 4 features:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(sfs_forward.get_feature_names_out())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Backward Selection ---</span></span><br><span class="line"><span class="comment"># direction=&#x27;backward&#x27; starts with all features and removes them</span></span><br><span class="line">sfs_backward = SequentialFeatureSelector(</span><br><span class="line">    model,</span><br><span class="line">    n_features_to_select=<span class="number">4</span>,</span><br><span class="line">    direction=<span class="string">&#x27;backward&#x27;</span>,</span><br><span class="line">    cv=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line">sfs_backward.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nBackward selection best 4 features:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(sfs_backward.get_feature_names_out())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: To replicate the plots, you would loop this process,</span></span><br><span class="line"><span class="comment"># changing &#x27;n_features_to_select&#x27; from 1 to p,</span></span><br><span class="line"><span class="comment"># record the model scores (e.g., RSS, AIC, BIC) at each step,</span></span><br><span class="line"><span class="comment"># and then plot the results.</span></span><br></pre></td></tr></table></figure>
<h2 id="important-images">Important Images</h2>
<ol type="1">
<li><p><strong>Slide <code>...230014.png</code> (Forward Selection
Plots) &amp; <code>...230036.png</code> (Backward Selection
Plots):</strong></p>
<ul>
<li><strong>What they are:</strong> These <span class="math inline">\(2
\times 2\)</span> plot grids are the most important visuals. They show
<strong>Residual Sum of Squares (RSS)</strong>, <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>BIC</strong>, and
<strong>Mallowsâ€™ <span class="math inline">\(C_p\)</span></strong>
plotted against the <em>Number of Variables</em>.</li>
<li><strong>Why theyâ€™re important:</strong> They are the
<strong>decision-making tool</strong>. You use these plots to choose the
best model.
<ul>
<li>You look for the â€œelbowâ€ or <strong>minimum</strong> value for BIC
and <span class="math inline">\(C_p\)</span>.</li>
<li>You look for the â€œpeakâ€ or <strong>maximum</strong> value for
Adjusted <span class="math inline">\(R^2\)</span>.</li>
<li>(RSS is not used for selection as it always decreases).</li>
</ul></li>
</ul></li>
<li><p><strong>Slide <code>...230040.png</code> (Find the best
model):</strong></p>
<ul>
<li><strong>What it is:</strong> This slide shows a close-up of the
<span class="math inline">\(C_p\)</span>, BIC, and Adjusted <span
class="math inline">\(R^2\)</span> plots, with the â€œbestâ€ model (the
min/max) marked with a blue â€˜xâ€™.</li>
<li><strong>Why itâ€™s important:</strong> It explicitly states the
selection criteria. The text highlights that BIC suggests a 4-variable
model, while the other two are â€œrather flatâ€ after 4, making the choice
less obvious but pointing to a simple model.</li>
</ul></li>
<li><p><strong>Slide <code>...230045.png</code> (BIC vs.Â Validation
vs.Â CV):</strong></p>
<ul>
<li><strong>What it is:</strong> This shows three plots for selecting
the best model using different criteria: BIC, Validation Set Error, and
Cross-Validation Error.</li>
<li><strong>Why itâ€™s important:</strong> It shows that <strong>different
selection criteria can lead to different â€œbestâ€ models</strong>. Here,
BIC (a mathematical adjustment) picks a 4-variable model, while
validation and CV (direct error estimation) both pick a 6-variable
model.</li>
</ul></li>
</ol>
<p>The slides use the <code>Credit</code> dataset to demonstrate two key
tasks: 1. <strong>Running</strong> different subset selection algorithms
(forward, backward, best). 2. <strong>Using</strong> various statistical
metrics (BIC, <span class="math inline">\(C_p\)</span>, CV error) to
choose the single best model.</p>
<h2 id="comparing-selection-algorithms-the-path">Comparing Selection
Algorithms (The Path)</h2>
<p>This part of the example compares the <em>sequence</em> of models
selected by â€œForward Stepwiseâ€ selection versus â€œBest Subsetâ€
selection.</p>
<p><strong>Key Result (from Table 6.1):</strong></p>
<p>This table is the most important result for comparing the
algorithms.</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Variables</th>
<th style="text-align: left;">Best Subset</th>
<th style="text-align: left;">Forward Stepwise</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>one</strong></td>
<td style="text-align: left;"><code>rating</code></td>
<td style="text-align: left;"><code>rating</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>two</strong></td>
<td style="text-align: left;"><code>rating</code>,
<code>income</code></td>
<td style="text-align: left;"><code>rating</code>,
<code>income</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>three</strong></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>four</strong></td>
<td style="text-align: left;"><code>cards</code>, <code>income</code>,
<code>student</code>, <code>limit</code></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code>, <code>limit</code></td>
</tr>
</tbody>
</table>
<p><strong>Summary of this result:</strong></p>
<ul>
<li><strong>Identical for 1, 2, and 3 variables:</strong> Both methods
agree on the best one-variable model (<code>rating</code>), the best
two-variable model (<code>rating</code>, <code>income</code>), and the
best three-variable model (<code>rating</code>, <code>income</code>,
<code>student</code>).</li>
<li><strong>They Diverge at 4 variables:</strong>
<ul>
<li><strong>Forward selection</strong> is <em>greedy</em>. It started
with <code>rating</code>, <code>income</code>, <code>student</code> and
was â€œstuckâ€ with them. It then added <code>limit</code>, as that was the
best variable to <em>add</em> to its existing 3-variable model.</li>
<li><strong>Best subset selection</strong> is <em>not</em> greedy. It
tests all possible 4-variable combinations. It discovered that the model
<code>cards</code>, <code>income</code>, <code>student</code>,
<code>limit</code> has a slightly lower RSS than the model forward
selection found.</li>
</ul></li>
<li><strong>Main Takeaway:</strong> This demonstrates the limitation of
a greedy algorithm. Forward selection missed the â€œtrueâ€ best 4-variable
model because it was locked into its previous choices and couldnâ€™t â€œswap
outâ€ <code>rating</code> for <code>cards</code>.</li>
</ul>
<h2 id="choosing-the-single-best-model-the-destination">Choosing the
Single Best Model (The Destination)</h2>
<p>This is the most critical part of the analysis. After running a
selection algorithm (like forward, backward, or best subset), you get a
list of the â€œbestâ€ models for each size (best 1-variable, best
2-variable, etc.). Now you must decide: <strong>is the best model the
4-variable one, the 6-variable one, or another?</strong></p>
<p>The slides show several plots to help make this decision, all plotted
against the â€œNumber of Predictors.â€</p>
<p><strong>Summary of Plot Results:</strong></p>
<p>Hereâ€™s what each plot tells you:</p>
<ul>
<li><strong>Residual Sum of Squares (RSS)</strong> (e.g., in slide
<code>...230014.png</code>, top-left)
<ul>
<li><strong>What it shows:</strong> RSS <em>always</em> decreases as you
add more variables. It drops sharply until 4 variables, then flattens
out.</li>
<li><strong>Conclusion:</strong> This plot is <strong>not useful for
picking the best model</strong> because it will always pick the full
model, which is overfit. Itâ€™s only used to see the diminishing returns
of adding new variables.</li>
</ul></li>
<li><strong>Adjusted <span class="math inline">\(R^2\)</span></strong>
(e.g., in slide <code>...230040.png</code>, right)
<ul>
<li><strong>What it shows:</strong> This metric penalizes adding useless
variables. The plot rises quickly, then flattens, peaking at its
<strong>maximum value around 6 or 7 variables</strong>.</li>
<li><strong>Conclusion:</strong> This metric suggests a <strong>6 or
7-variable model</strong>.</li>
</ul></li>
<li><strong>Mallowsâ€™ <span class="math inline">\(C_p\)</span></strong>
(e.g., in slide <code>...230040.png</code>, left)
<ul>
<li><strong>What it shows:</strong> This is an estimate of test error.
We want the model with the <strong>minimum <span
class="math inline">\(C_p\)</span></strong>. The plot drops to a low
value at 4 variables and stays low, with its absolute minimum around
<strong>6 or 7 variables</strong>.</li>
<li><strong>Conclusion:</strong> This metric also suggests a <strong>6
or 7-variable model</strong>.</li>
</ul></li>
<li><strong>BIC (Bayesian Information Criterion)</strong> (e.g., in
slide <code>...230040.png</code>, center)
<ul>
<li><strong>What it shows:</strong> This is another estimate of test
error, but it has a <em>stronger penalty</em> for model complexity. The
plot shows a clear â€œUâ€ shape, reaching its <strong>minimum value at 4
variables</strong> and then <em>increasing</em> afterward.</li>
<li><strong>Conclusion:</strong> This metric strongly suggests a
<strong>4-variable model</strong>.</li>
</ul></li>
<li><strong>Validation Set &amp; Cross-Validation (CV) Error</strong>
(Slide <code>...230045.png</code>)
<ul>
<li><strong>What it shows:</strong> These plots show the <em>direct</em>
estimate of test error (not a mathematical adjustment like BIC or <span
class="math inline">\(C_p\)</span>). Both the validation set error and
the 10-fold CV error show a â€œUâ€ shape.</li>
<li><strong>Conclusion:</strong> Both methods reach their
<strong>minimum error at 6 variables</strong>. This is considered a very
reliable result.</li>
</ul></li>
</ul>
<h2 id="final-summary-of-results">Final Summary of Results</h2>
<p>The analysis of the <code>Credit</code> dataset reveals two strong
candidates for the â€œbestâ€ model, depending on your goal:</p>
<ol type="1">
<li><p><strong>The 6-Variable Model:</strong> This model is supported by
the <strong>Adjusted <span class="math inline">\(R^2\)</span></strong>,
<strong>Mallowsâ€™ <span class="math inline">\(C_p\)</span></strong>, and
(most importantly) the <strong>Validation Set</strong> and
<strong>10-fold Cross-Validation</strong> results. These metrics all
indicate that the 6-variable model has the <strong>lowest prediction
error</strong> on new data.</p></li>
<li><p><strong>The 4-Variable Model:</strong> This model is supported by
<strong>BIC</strong>. Because BIC penalizes complexity more heavily, it
selects a simpler (more <em>parsimonious</em>) model.</p></li>
</ol>
<p><strong>Overall Conclusion:</strong> If your primary goal is
<strong>maximum predictive accuracy</strong>, you should choose the
<strong>6-variable model</strong>. If your goal is a <strong>simpler,
more interpretable model</strong> that is still very good (and avoids
any risk of overfitting), the <strong>4-variable model</strong> is an
excellent choice.</p>
<h1
id="two-main-strategies-for-controlling-model-complexity-in-linear-regression">5.
Two main strategies for controlling model complexity in linear
regression</h1>
<p>This presentation covers two main strategies for controlling model
complexity in linear regression: <strong>Subset Selection</strong>
(choosing <em>which</em> variables to include) and <strong>Shrinkage
Methods</strong> (keeping all variables but <em>reducing the impact</em>
of their coefficients).</p>
<h2 id="subset-selection-1">Subset Selection</h2>
<p>This method involves selecting a subset of the <span
class="math inline">\(p\)</span> total predictors to use in the
model.</p>
<h3 id="key-concepts-formulas">Key Concepts &amp; Formulas</h3>
<ul>
<li><p><strong>The Model:</strong> The standard linear regression model
is represented in matrix form: <span class="math display">\[\mathbf{y} =
\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\]</span> The goal
of subset selection is to find a coefficient vector <span
class="math inline">\(\boldsymbol{\beta}\)</span> that is
<strong>sparse</strong>, meaning it has many zero entries.</p></li>
<li><p><strong>Forward Selection:</strong> This is a <em>greedy
algorithm</em> that starts with an empty model and iteratively adds the
single predictor that most improves the fit.</p></li>
<li><p><strong>Theoretical Guarantee:</strong> Can forward selection
find the <em>true</em> sparse set of variables?</p>
<ul>
<li>Yes, <em>if</em> the predictors are not strongly correlated.</li>
<li>This is quantified by the <strong>Mutual Coherence
Condition</strong>. Assuming the predictors <span
class="math inline">\(\mathbf{x}_i\)</span> are normalized, the method
is guaranteed to work if: <span class="math display">\[\mu = \max_{i
\neq j} |\langle \mathbf{x}_i, \mathbf{x}_j \rangle| &lt; \frac{1}{2s -
1}\]</span> where <span class="math inline">\(s\)</span> is the number
of true non-zero coefficients and <span class="math inline">\(\langle
\mathbf{x}_i, \mathbf{x}_j \rangle\)</span> represents the correlation
between predictors.</li>
</ul></li>
</ul>
<h3 id="practical-application-finding-the-best-model-size">Practical
Application: Finding the Best Model Size</h3>
<p>How do you know whether to choose a model with 3, 4, or 5 variables?
You use <strong>Cross-Validation (CV)</strong>.</p>
<ul>
<li><p><strong>Important Image:</strong> The plot titled â€œ10-fold CVâ€
(from the first slide) is the most important visual. It plots the
estimated test error (CV Error) on the y-axis against the number of
variables in the model on the x-axis.</p></li>
<li><p><strong>The â€œOne Standard Deviation Ruleâ€:</strong> Looking at
the plot, the error drops sharply and then flattens. The absolute
minimum error might be at 6 variables, but itâ€™s only slightly better
than the 3-variable model.</p>
<ol type="1">
<li>Find the model with the <em>lowest</em> CV error.</li>
<li>Calculate the standard error for that error estimate.</li>
<li>Select the <strong>simplest model</strong> (fewest variables) whose
error is <em>within one standard deviation</em> of the minimum.</li>
<li>This follows <strong>Occamâ€™s razor</strong>: choose the simplest
explanation (model) that fits the data well enough. In the example
given, this rule selects the 3-variable model.</li>
</ol></li>
</ul>
<h3 id="code-interpretation-r-vs.-python">Code Interpretation (R
vs.Â Python)</h3>
<p>The R code in the first slide performs this 10-fold CV manually for
forward selection:</p>
<ol type="1">
<li>It loops from <code>p = 1</code> to <code>10</code> (model
sizes).</li>
<li>Inside the loop, it identifies the <code>p</code> variables chosen
by a pre-computed forward selection model
(<code>regfit.fwd</code>).</li>
<li>It fits a new model (<code>glm.fit</code>) using <em>only</em> those
<code>p</code> variables.</li>
<li>It runs 10-fold CV (<code>cv.glm</code>) on <em>that specific
model</em> to get its test error.</li>
<li>It stores the error in <code>CV10.err[p]</code>.</li>
<li>Finally, it plots the results.</li>
</ol>
<p><strong>In Python (with <code>scikit-learn</code>):</strong> This
entire process is often automated.</p>
<ul>
<li>You would use <code>sklearn.feature_selection.RFECV</code>
(Recursive Feature Elimination with Cross-Validation).</li>
<li><code>RFECV</code> automatically performs cross-validation to find
the optimal number of features, effectively producing the same plot and
result as the R code.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Conceptual Python equivalent for finding the best model size</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFECV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"></span><br><span class="line"><span class="comment"># X, y = load_your_data()</span></span><br><span class="line">X, y = make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">10</span>, n_informative=<span class="number">3</span>, noise=<span class="number">10</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">estimator = LinearRegression()</span><br><span class="line"><span class="comment"># RFECV will test models with 1 feature, 2 features, etc.,</span></span><br><span class="line"><span class="comment"># and use cross-validation (cv=10) to find the best number.</span></span><br><span class="line">selector = RFECV(estimator, step=<span class="number">1</span>, cv=<span class="number">10</span>, scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>)</span><br><span class="line">selector = selector.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal number of features: <span class="subst">&#123;selector.n_features_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># You can plot selector.cv_results_[&#x27;mean_test_score&#x27;] to get the CV curve</span></span><br></pre></td></tr></table></figure>
<h2 id="shrinkage-methods-regularization">Shrinkage Methods
(Regularization)</h2>
<p>Instead of explicitly removing variables, shrinkage methods keep all
<span class="math inline">\(p\)</span> variables but <em>shrink</em>
their coefficients <span class="math inline">\(\beta_j\)</span> towards
zero.</p>
<h3 id="ridge-regression">Ridge Regression</h3>
<p>Ridge regression is a prime example of a shrinkage method.</p>
<ul>
<li><p><strong>Objective Function:</strong> It finds the coefficients
<span class="math inline">\(\boldsymbol{\beta}\)</span> that minimize a
new quantity: <span class="math display">\[\underbrace{\sum_{i=1}^{n}
(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2}_{\text{RSS (Goodness
of Fit)}} + \underbrace{\lambda \sum_{j=1}^{p}
\beta_j^2}_{\text{$\ell_2$ Penalty (Shrinkage)}}\]</span></p></li>
<li><p><strong>The <span class="math inline">\(\lambda\)</span> Tuning
Parameter:</strong> This parameter controls the strength of the
penalty:</p>
<ul>
<li><strong>If <span class="math inline">\(\lambda =
0\)</span>:</strong> The penalty term disappears. Ridge regression is
identical to standard Ordinary Least Squares (OLS).</li>
<li><strong>If <span class="math inline">\(\lambda \to
\infty\)</span>:</strong> The penalty is â€œinfinitelyâ€ strong. To
minimize the function, all coefficients <span
class="math inline">\(\beta_j\)</span> (for <span
class="math inline">\(j=1...p\)</span>) are forced to be zero. The model
becomes an intercept-only model.</li>
<li><strong>Note:</strong> The intercept <span
class="math inline">\(\beta_0\)</span> is <em>not penalized</em>.</li>
</ul></li>
<li><p><strong>The Bias-Variance Trade-off:</strong> This is the core
concept of regularization.</p>
<ul>
<li>Standard OLS has low bias but can have high variance (it
overfits).</li>
<li>Ridge regression adds a <em>small amount of bias</em> (the
coefficients are â€œwrongâ€ on purpose) to <strong>significantly reduce the
modelâ€™s variance</strong>.</li>
<li>This trade-off often leads to a model with a lower overall test
error.</li>
</ul></li>
<li><p><strong>Matrix Solution:</strong> The discussion slide asks â€œWhat
is the solution?â€. While OLS has the solution <span
class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>, the Ridge
solution is: <span class="math display">\[\hat{\boldsymbol{\beta}}^R =
(\mathbf{X}^T\mathbf{X} + \lambda
\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</span> where <span
class="math inline">\(\mathbf{I}\)</span> is the identity matrix. The
<span class="math inline">\(\lambda \mathbf{I}\)</span> term adds a
â€œridgeâ€ to the diagonal, making the matrix invertible even if <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is singular (which
happens if <span class="math inline">\(p &gt; n\)</span> or predictors
are collinear).</p></li>
</ul>
<h3 id="an-essential-step-standardization">An Essential Step:
Standardization</h3>
<ul>
<li><strong>Problem:</strong> The <span
class="math inline">\(\ell_2\)</span> penalty <span
class="math inline">\(\lambda \sum \beta_j^2\)</span> is applied equally
to all coefficients. If predictor <span
class="math inline">\(x_1\)</span> (e.g., house size in sq-ft) is on a
much larger scale than <span class="math inline">\(x_2\)</span> (e.g.,
number of rooms), its coefficient <span
class="math inline">\(\beta_1\)</span> will naturally be much smaller
than <span class="math inline">\(\beta_2\)</span>. The penalty will
unfairly punish <span class="math inline">\(\beta_2\)</span> more.</li>
<li><strong>Solution:</strong> You <strong>must standardize</strong>
your inputs <em>before</em> fitting a Ridge model.</li>
<li><strong>Formula:</strong> For each predictor <span
class="math inline">\(X_j\)</span>, all its observations <span
class="math inline">\(x_{ij}\)</span> are rescaled: <span
class="math display">\[\tilde{x}_{ij} = \frac{x_{ij} -
\bar{x}_j}{\sigma_j}\]</span> (where <span
class="math inline">\(\bar{x}_j\)</span> is the mean of the predictor
and <span class="math inline">\(\sigma_j\)</span> is its standard
deviation). This puts all predictors on a common scale (mean=0,
std=1).</li>
</ul>
<p><strong>In Python (with <code>scikit-learn</code>):</strong></p>
<ul>
<li>You use <code>sklearn.preprocessing.StandardScaler</code> to
standardize your data.</li>
<li>You use <code>sklearn.linear_model.Ridge</code> to fit the
model.</li>
<li>You use <code>sklearn.linear_model.RidgeCV</code> to automatically
find the best value for <span class="math inline">\(\lambda\)</span>
(called <code>alpha</code> in scikit-learn) using cross-validation.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Conceptual Python code for Ridge Regression</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># X, y = load_your_data()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a pipeline that first standardizes the data,</span></span><br><span class="line"><span class="comment"># then fits a Ridge model.</span></span><br><span class="line"><span class="comment"># RidgeCV tests a range of alphas (lambdas) automatically.</span></span><br><span class="line">model = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    RidgeCV(alphas=[<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">100.0</span>], scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best alpha (lambda): <span class="subst">&#123;model.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].alpha_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model coefficients: <span class="subst">&#123;model.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].coef_&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="subset-selection-2">Subset Selection</h2>
<p>This section is about choosing <em>which</em> predictors (variables)
to include in your linear model. The main idea is to find a â€œsparseâ€
model (one with few variables) that performs well.</p>
<h3 id="the-model-and-the-goal">The Model and The Goal</h3>
<ul>
<li><strong>Slide: â€œForward selection in Linear
Regressionâ€</strong></li>
<li><strong>Formula:</strong> The standard linear regression model is
<span class="math inline">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} +
\boldsymbol{\epsilon}\)</span>
<ul>
<li><span class="math inline">\(\mathbf{y}\)</span> is the <span
class="math inline">\(n \times 1\)</span> vector of outcomes.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is the <span
class="math inline">\(n \times (p+1)\)</span> matrix of predictors (with
a leading column of 1s for the intercept).</li>
<li><span class="math inline">\(\boldsymbol{\beta}\)</span> is the <span
class="math inline">\((p+1) \times 1\)</span> vector of coefficients
(<span class="math inline">\(\beta_0, \beta_1, ...,
\beta_p\)</span>).</li>
<li><span class="math inline">\(\boldsymbol{\epsilon}\)</span> is the
<span class="math inline">\(n \times 1\)</span> vector of irreducible
error.</li>
</ul></li>
<li><strong>Key Question:</strong> â€œIf <span
class="math inline">\(\boldsymbol{\beta}\)</span> is sparse with at most
<span class="math inline">\(s\)</span> non-zero entries, can forward
selection find those variables?â€
<ul>
<li><strong>Sparse</strong> means most coefficients are zero.</li>
<li><strong>Forward Selection</strong> is a <em>greedy algorithm</em>:
<ol type="1">
<li>Start with no variables.</li>
<li>Add the one variable that gives the best fit.</li>
<li>Add the <em>next</em> best variable to the existing model.</li>
<li>Repeat until you have a model with <span
class="math inline">\(s\)</span> variables.</li>
</ol></li>
<li>The slide suggests the answer is <strong>yes</strong>, but only
under certain conditions.</li>
</ul></li>
</ul>
<h3 id="the-condition-for-success">The Condition for Success</h3>
<ul>
<li><strong>Slide: â€œOrthogonal Matching Pursuitâ€</strong></li>
<li><strong>Key Concept:</strong> Forward selection can provably find
the correct variables if those variables are not strongly
correlated.</li>
<li><strong>Formula:</strong> This is formalized by the <strong>Mutual
Coherence Condition</strong>: <span class="math display">\[\mu = \max_{i
\neq j} |\langle \mathbf{x}_i, \mathbf{x}_j \rangle| &lt; \frac{1}{2s -
1}\]</span>
<ul>
<li><strong>What it means:</strong>
<ul>
<li><code>assuming $\mathbf&#123;x&#125;_i$'s are normalized</code> means weâ€™ve
scaled them to have a length of 1.</li>
<li><span class="math inline">\(\langle \mathbf{x}_i, \mathbf{x}_j
\rangle\)</span> is the dot product, which is just their
<strong>correlation</strong> since they are normalized.</li>
<li><span class="math inline">\(\mu\)</span> (mu) is the <strong>largest
absolute correlation</strong> you can find between any two
<em>different</em> predictors.</li>
<li><span class="math inline">\(s\)</span> is the true number of
important variables.</li>
</ul></li>
<li><strong>In English:</strong> If the maximum correlation between any
of your predictors is less than this threshold, the greedy forward
selection algorithm is guaranteed to find the true, sparse set of
variables.</li>
</ul></li>
</ul>
<h3 id="how-to-choose-the-model-size-practice">How to Choose the Model
Size (Practice)</h3>
<p>The theory is nice, but in practice, you donâ€™t know <span
class="math inline">\(s\)</span>. How many variables should you
pick?</p>
<ul>
<li><p><strong>Slide: â€œ10-fold CV Errorsâ€</strong></p></li>
<li><p><strong>This is the most important practical slide for this
section.</strong></p></li>
<li><p><strong>What the plot shows:</strong></p>
<ul>
<li><strong>X-axis:</strong> â€œNumber of Variablesâ€ (from 1 to 10).</li>
<li><strong>Y-axis:</strong> â€œCV Errorâ€ (the 10-fold cross-validated
Mean Squared Error).</li>
<li><strong>The Curve:</strong> The error drops very fast as we add the
first 2-3 variables. Then, it flattens out. Adding more than 3 variables
doesnâ€™t really help much.</li>
</ul></li>
<li><p><strong>Slide: â€œThe one standard deviation
ruleâ€</strong></p></li>
<li><p>This rule helps you pick the â€œbestâ€ model from the CV plot.</p>
<ol type="1">
<li>Find the model with the absolute <em>minimum</em> CV error (in the
plot, this looks to be around 6 or 7 variables).</li>
<li>Calculate the standard error of that minimum CV error.</li>
<li>Draw a â€œtoleranceâ€ line at
<code>(minimum error) + (one standard error)</code>.</li>
<li>Choose the <strong>simplest model</strong> (fewest variables) whose
CV error is <em>below</em> this tolerance line.</li>
</ol>
<!-- end list -->
<ul>
<li>The slide states this rule â€œgives the model with 3 variableâ€ for
this example. This is because the 3-variable model is much simpler than
the 6-variable one, and its error is â€œgood enoughâ€ (within one standard
deviation of the minimum). This is an application of <strong>Occamâ€™s
razor</strong>.</li>
</ul></li>
</ul>
<h3 id="code-r-vs.-python">Code: R vs.Â Python</h3>
<p>The R code on the â€œ10-fold CV Errorsâ€ slide generates that exact
plot.</p>
<ul>
<li><p><strong>R Code Explained:</strong></p>
<ul>
<li><code>library(boot)</code>: Loads the cross-validation library.</li>
<li><code>CV10.err=rep(0,10)</code>: Creates an empty vector to store
the 10 error scores.</li>
<li><code>for(p in 1:10)</code>: A loop that will test model sizes from
1 to 10.</li>
<li><code>x&lt;-which(summary(regfit.fwd)$which[p,])</code>: Gets the
<em>names</em> of the <span class="math inline">\(p\)</span> variables
chosen by a pre-run forward selection (<code>regfit.fwd</code>).</li>
<li><code>glm.fit=glm(Balance~.,data=newCred)</code>: Fits a model using
<em>only</em> those <span class="math inline">\(p\)</span>
variables.</li>
<li><code>cv.err=cv.glm(newCred,glm.fit,K=10)</code>: Performs 10-fold
CV on <em>that specific <span class="math inline">\(p\)</span>-variable
model</em>.</li>
<li><code>CV10.err[p]&lt;-cv.err$delta[1]</code>: Stores the CV
error.</li>
<li><code>plot(...)</code>: Plots the 10 errors against the 10 model
sizes.</li>
</ul></li>
<li><p><strong>Python Equivalent (Conceptual):</strong></p>
<ul>
<li>In <code>scikit-learn</code>, this process is often automated. You
wouldnâ€™t write the CV loop yourself.</li>
<li>You would use <code>sklearn.feature_selection.RFECV</code>
(Recursive Feature Elimination with Cross-Validation). This tool
automatically wraps a model (like <code>LinearRegression</code>),
performs cross-validation, and finds the optimal number of features,
effectively producing the same plot and result.</li>
</ul></li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- Python equivalent for 6.1 ---</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFECV</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="comment"># Assume X and y are your data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create a pipeline</span></span><br><span class="line"><span class="comment"># (Note: It&#x27;s good practice to scale, even for OLS, if you&#x27;re comparing)</span></span><br><span class="line">pipeline = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    LinearRegression()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create the RFECV (Recursive Feature Elimination w/ CV) object</span></span><br><span class="line"><span class="comment"># This is an *alternative* to forward selection, but serves the same purpose</span></span><br><span class="line"><span class="comment"># It will test models with 1, 2, 3... features using 10-fold CV</span></span><br><span class="line">feature_selector = RFECV(</span><br><span class="line">    estimator=pipeline, </span><br><span class="line">    min_features_to_select=<span class="number">1</span>, </span><br><span class="line">    step=<span class="number">1</span>, </span><br><span class="line">    cv=<span class="number">10</span>, </span><br><span class="line">    scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span> <span class="comment"># We want to minimize error</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit it</span></span><br><span class="line">feature_selector.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal number of features found: <span class="subst">&#123;feature_selector.n_features_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You could then plot feature_selector.cv_results_[&#x27;mean_test_score&#x27;]</span></span><br><span class="line"><span class="comment"># to replicate the R plot.</span></span><br></pre></td></tr></table></figure>
<h2 id="shrinkage-methods-by-regularization">Shrinkage Methods by
Regularization</h2>
<p>This is a different approach. Instead of <em>removing</em> variables,
we keep all <span class="math inline">\(p\)</span> variables but
<em>shrink</em> their coefficients <span
class="math inline">\(\beta_j\)</span> towards 0.</p>
<h3 id="ridge-regression-the-core-idea">Ridge Regression: The Core
Idea</h3>
<ul>
<li><strong>Slide: â€œRidge regressionâ€</strong></li>
<li><strong>Formula:</strong> Ridge regression minimizes a new objective
function: <span class="math display">\[\min_{\boldsymbol{\beta}} \left(
\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 +
\lambda \sum_{j=1}^{p} \beta_j^2 \right)\]</span>
<ul>
<li><strong>Term 1: <span class="math inline">\(\text{RSS}\)</span>
(Residual Sum of Squares).</strong> This is the original OLS â€œgoodness
of fitâ€ term. We want this to be small.</li>
<li><strong>Term 2: <span class="math inline">\(\lambda \sum
\beta_j^2\)</span>.</strong> This is the <strong><span
class="math inline">\(\ell_2\)</span> penalty</strong> or â€œshrinkage
penaltyâ€. It adds a â€œcostâ€ for having large coefficients.</li>
</ul></li>
<li><strong>The <span class="math inline">\(\lambda\)</span> (lambda)
Parameter:</strong>
<ul>
<li>This is the <strong>tuning parameter</strong> that controls the
trade-off between fit and simplicity.</li>
<li><code>$\lambda = 0$</code>: No penalty. The objective is just to
minimize RSS. The solution <span
class="math inline">\(\hat{\boldsymbol{\beta}}^R\)</span> is identical
to the OLS solution <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</li>
<li><code>$\lambda = \infty$</code>: Infinite penalty. The only way to
minimize the cost is to make all <span class="math inline">\(\beta_j =
0\)</span> (for <span class="math inline">\(j \ge 1\)</span>). The model
becomes an intercept-only model.</li>
<li><code>Large $\lambda$</code>: Heavy penalty, more shrinkage.</li>
<li><strong>Crucial Note:</strong> The intercept <span
class="math inline">\(\beta_0\)</span> is <strong>not
penalized</strong>. This is because <span
class="math inline">\(\beta_0\)</span> just represents the mean of <span
class="math inline">\(y\)</span> when all <span
class="math inline">\(x\)</span>â€™s are 0; shrinking it makes no
sense.</li>
</ul></li>
</ul>
<h3 id="the-need-for-standardization">The Need for Standardization</h3>
<ul>
<li><strong>Slide: â€œStandardize the inputsâ€</strong></li>
<li><strong>Problem:</strong> The penalty <span
class="math inline">\(\lambda \sum \beta_j^2\)</span> is applied to all
coefficients. But what if <span class="math inline">\(x_1\)</span> is
â€œhouse size in sq-ftâ€ (values 1000-5000) and <span
class="math inline">\(x_2\)</span> is â€œnumber of bedroomsâ€ (values 1-5)?
<ul>
<li>The coefficient <span class="math inline">\(\beta_1\)</span> for
house size will naturally be <em>tiny</em>, while the coefficient <span
class="math inline">\(\beta_2\)</span> for bedrooms will be
<em>large</em>, even if they are equally important.</li>
<li>Ridge regression would unfairly and heavily penalize <span
class="math inline">\(\beta_2\)</span> while barely touching <span
class="math inline">\(\beta_1\)</span>.</li>
</ul></li>
<li><strong>Solution:</strong> You <strong>must</strong> standardize all
predictors <em>before</em> fitting a Ridge model.</li>
<li><strong>Formula:</strong> For each observation <span
class="math inline">\(i\)</span> of each predictor <span
class="math inline">\(j\)</span>: <span
class="math display">\[\tilde{x}_{ij} = \frac{x_{ij} -
\bar{x}_j}{\sqrt{(1/n) \sum_{i=1}^{n} (x_{ij} - \bar{x}_j)^2}}\]</span>
<ul>
<li>This formula rescales every predictor to have a mean of 0 and a
standard deviation of 1.</li>
<li>Now, all coefficients <span class="math inline">\(\beta_j\)</span>
are on a â€œlevel playing fieldâ€ and can be penalized fairly.</li>
</ul></li>
</ul>
<h3 id="answering-the-discussion-questions">Answering the Discussion
Questions</h3>
<ul>
<li><strong>Slide: â€œDISCUSSIONâ€</strong>
<ul>
<li><code>What is the solution of Ridge regression?</code></li>
<li><code>What is the bias and the variance?</code></li>
</ul></li>
</ul>
<h4 id="what-is-the-solution-of-ridge-regression">1. What is the
solution of Ridge regression?</h4>
<p>The solution can be written in matrix form, which is very
elegant.</p>
<ul>
<li><p><strong>Standard OLS Solution:</strong> The coefficients <span
class="math inline">\(\hat{\boldsymbol{\beta}}^{\text{OLS}}\)</span>
that minimize RSS are found by: <span
class="math display">\[\hat{\boldsymbol{\beta}}^{\text{OLS}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p></li>
<li><p><strong>Ridge Regression Solution:</strong> The coefficients
<span class="math inline">\(\hat{\boldsymbol{\beta}}^{R}\)</span> that
minimize the Ridge objective are: <span
class="math display">\[\hat{\boldsymbol{\beta}}^{R} =
(\mathbf{X}^T\mathbf{X} + \lambda
\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><strong>Explanation:</strong>
<ul>
<li><span class="math inline">\(\mathbf{I}\)</span> is the
<strong>identity matrix</strong> (a matrix of 1s on the diagonal, 0s
everywhere else).</li>
<li>By adding <span class="math inline">\(\lambda\mathbf{I}\)</span>, we
are adding a positive value <span class="math inline">\(\lambda\)</span>
to the <em>diagonal</em> of the <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> matrix.</li>
<li>This addition <strong>stabilizes</strong> the matrix. <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> might not be
invertible (if <span class="math inline">\(p &gt; n\)</span> or if
predictors are perfectly collinear), but <span
class="math inline">\((\mathbf{X}^T\mathbf{X} + \lambda
\mathbf{I})\)</span> is <em>always</em> invertible for <span
class="math inline">\(\lambda &gt; 0\)</span>.</li>
<li>This addition is what mathematically â€œshrinksâ€ the coefficients
toward zero.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="what-is-the-bias-and-the-variance">2. What is the bias and the
variance?</h4>
<p>This is the <strong>most important concept</strong> in
regularization. Itâ€™s the <strong>bias-variance trade-off</strong>.</p>
<ul>
<li><p><strong>Standard OLS (where <span
class="math inline">\(\lambda=0\)</span>):</strong></p>
<ul>
<li><strong>Bias: Low.</strong> The OLS estimator is
<strong>unbiased</strong>, meaning that if you took many samples and fit
many OLS models, their average <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> would be the
<em>true</em> <span
class="math inline">\(\boldsymbol{\beta}\)</span>.</li>
<li><strong>Variance: High.</strong> The OLS solution can be
<em>highly</em> sensitive to the training data. If you change a few data
points, the coefficients can swing wildly. This is especially true if
<span class="math inline">\(p\)</span> is large or predictors are
correlated. This â€œsensitivityâ€ is high variance, which leads to
<strong>overfitting</strong>.</li>
</ul></li>
<li><p><strong>Ridge Regression (where <span
class="math inline">\(\lambda &gt; 0\)</span>):</strong></p>
<ul>
<li><strong>Bias: High(er).</strong> Ridge regression is a
<strong>biased</strong> estimator. By adding the penalty, we are
<em>purposefully</em> pulling the coefficients away from the OLS
solution and towards zero. The average <span
class="math inline">\(\hat{\boldsymbol{\beta}}^R\)</span> from many
samples will <em>not</em> equal the true <span
class="math inline">\(\boldsymbol{\beta}\)</span>. We have
<em>introduced</em> bias into our model.</li>
<li><strong>Variance: Low(er).</strong> In exchange for this bias, we
get a massive <em>reduction in variance</em>. The <span
class="math inline">\(\lambda\mathbf{I}\)</span> term stabilizes the
solution. The coefficients wonâ€™t change wildly even if the training data
changes. The model is more robust and less sensitive.</li>
</ul></li>
</ul>
<p><strong>The Trade-off:</strong> The total expected test error of a
model is: <span class="math inline">\(\text{Error} = \text{Bias}^2 +
\text{Variance} + \text{Irreducible Error}\)</span></p>
<p>By using Ridge regression, we <em>increase</em> the <span
class="math inline">\(\text{Bias}^2\)</span> term a little, but we
<em>decrease</em> the <span
class="math inline">\(\text{Variance}\)</span> term a lot. The goal is
to find a <span class="math inline">\(\lambda\)</span> where the
<em>total error</em> is minimized. Ridge regression reduces variance
<em>at the cost of</em> increased bias.</p>
<h3 id="python-equivalent-for-6.2">Python Equivalent for 6.2</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- Python equivalent for 6.2 ---</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="comment"># Assume X and y are your data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create a pipeline that AUTOMATICALLY</span></span><br><span class="line"><span class="comment">#    - Standardizes the data</span></span><br><span class="line"><span class="comment">#    - Fits a Ridge Regression model</span></span><br><span class="line"><span class="comment">#    - Uses Cross-Validation to find the BEST lambda (alpha in scikit-learn)</span></span><br><span class="line">alphas_to_test = [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">100.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># RidgeCV handles everything for us</span></span><br><span class="line">pipeline = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    RidgeCV(alphas=alphas_to_test, scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>, cv=<span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit the pipeline</span></span><br><span class="line">pipeline.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Get the results</span></span><br><span class="line">best_lambda = pipeline.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].alpha_</span><br><span class="line">ridge_coefficients = pipeline.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].coef_</span><br><span class="line">intercept = pipeline.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].intercept_</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha) found by CV: <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model intercept (beta_0): <span class="subst">&#123;intercept&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model coefficients (beta_j): <span class="subst">&#123;ridge_coefficients&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="the-why-of-ridge-regression">6. The â€œWhyâ€ of Ridge
Regression</h1>
<h2 id="core-concepts-the-why-of-ridge-regression">Core Concepts: The
â€œWhyâ€ of Ridge Regression</h2>
<p>Your slides explain that ridge regression is a â€œshrinkage methodâ€
designed to solve a major problem with standard Ordinary Least Squares
(OLS) regression: <strong>high variance</strong>.</p>
<h3 id="the-bias-variance-tradeoff-slide-3">The Bias-Variance Tradeoff
(Slide 3)</h3>
<p>This is the most important theoretical concept. In prediction, the
total error (Mean Squared Error, or MSE) of a model is composed of three
parts: <span class="math inline">\(\text{Error} = \text{Variance} +
\text{Bias}^2 + \text{Irreducible Error}\)</span></p>
<ul>
<li><strong>Ordinary Least Squares (OLS):</strong> Aims to be unbiased
(low bias). However, when you have many predictors (<span
class="math inline">\(p\)</span>), especially if they are correlated, or
if <span class="math inline">\(p\)</span> is large compared to the
number of samples <span class="math inline">\(n\)</span> (<span
class="math inline">\(p \approx n\)</span> or <span
class="math inline">\(p &gt; n\)</span>), the OLS model becomes highly
<em>unstable</em>. A small change in the training data can cause the
coefficients to change wildly. This is <strong>high variance</strong>.
(See Slide 6, â€œRemarksâ€).</li>
<li><strong>Ridge Regression:</strong> By adding a penalty, ridge
<em>intentionally</em> introduces a small amount of
<strong>bias</strong> (it pulls coefficients away from their â€œtrueâ€ OLS
values). In return, it achieves a <em>massive</em> reduction in
<strong>variance</strong>.</li>
</ul>
<p>As <strong>Slide 3</strong> shows:</p>
<ul>
<li>The <strong>green line (Variance)</strong> starts very high for low
<span class="math inline">\(\lambda\)</span> (left side) and drops
quickly.</li>
<li>The <strong>black line (Squared Bias)</strong> starts at zero (for
OLS at <span class="math inline">\(\lambda=0\)</span>) and slowly
increases as <span class="math inline">\(\lambda\)</span> grows.</li>
<li>The <strong>purple line (Test MSE)</strong> is the sum of the two.
Itâ€™s U-shaped. The goal of ridge is to find the <span
class="math inline">\(\lambda\)</span> (marked by the â€˜xâ€™) at the
<em>bottom</em> of this â€œU,â€ which gives the lowest possible total
error.</li>
</ul>
<h3 id="why-is-it-called-ridge-the-3d-spatial-meaning-slide-5">Why Is It
Called â€œRidgeâ€? The 3D Spatial Meaning (Slide 5)</h3>
<p>This slide explains the problem of <strong>collinearity</strong> and
the origin of the name.</p>
<ul>
<li><strong>Left Plot (Least Squares):</strong> Imagine a model with two
correlated predictors, <span class="math inline">\(\beta_1\)</span> and
<span class="math inline">\(\beta_2\)</span>. The y-axis (SS1) is the
error (RSS). Because the predictors are correlated, there isnâ€™t one
single â€œpointâ€ that is the minimum. Instead, thereâ€™s a long, flat
<em>valley</em> or <em>trough</em> (marked â€œunstableâ€). Many different
combinations of <span class="math inline">\(\beta_1\)</span> and <span
class="math inline">\(\beta_2\)</span> along this valley give a
similarly low error. The OLS solution is unstable because it can pick
<em>any</em> point in this flat-bottomed valley.</li>
<li><strong>Right Plot (Ridge):</strong> The ridge objective function
adds a penalty term: <span class="math inline">\(\lambda(\beta_1^2 +
\beta_2^2)\)</span>. This penalty term, by itself, is a perfect circular
bowl centered at (0,0). When you add this â€œbowlâ€ to the OLS â€œvalley,â€ it
<em>stabilizes</em> the function. It pulls the minimum towards (0,0) and
creates a single, stable, well-defined minimum.</li>
<li><strong>The â€œRidgeâ€ Name:</strong> The penalty <span
class="math inline">\(\lambda\mathbf{I}\)</span> (from the matrix
formula) adds a â€œridgeâ€ of values to the diagonal of the <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> matrix, which
geometrically turns the unstable flat valley into a stable bowl.</li>
</ul>
<h2 id="mathematical-formulas">Mathematical Formulas</h2>
<p>The key difference between OLS and Ridge is the function they try to
minimize.</p>
<ol type="1">
<li><p><strong>OLS Objective Function:</strong> Minimize the Residual
Sum of Squares (RSS). <span class="math display">\[\text{RSS} =
\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}
\right)^2\]</span></p></li>
<li><p><strong>Ridge Objective Function (Slide 6):</strong> Minimize the
RSS <em>plus</em> an L2 penalty term. <span
class="math display">\[\text{Minimize: } \left[ \sum_{i=1}^{n} \left(
y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 \right] +
\lambda \sum_{j=1}^{p} \beta_j^2\]</span></p>
<ul>
<li><span class="math inline">\(\lambda\)</span> is the <strong>tuning
parameter</strong> controlling the penalty strength.</li>
<li><span class="math inline">\(\sum_{j=1}^{p} \beta_j^2\)</span> is the
<strong>L2-norm</strong> (squared) of the coefficients. It penalizes
large coefficients.</li>
</ul></li>
<li><p><strong>L2 Norm (Slide 1):</strong> The L2 norm of a vector <span
class="math inline">\(\mathbf{a}\)</span> is its standard Euclidean
length. The plot on Slide 1 uses this to show the <em>total
magnitude</em> of the ridge coefficients. <span
class="math display">\[\|\mathbf{a}\|_2 = \sqrt{\sum_{j=1}^p
a_j^2}\]</span></p></li>
<li><p><strong>Matrix Solution (Slide 6):</strong> This is the
â€œclosed-formâ€ solution for the ridge coefficients <span
class="math inline">\(\hat{\beta}^R\)</span>. <span
class="math display">\[\hat{\beta}^R = (\mathbf{X}^T\mathbf{X} +
\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{I}\)</span> is the identity
matrix.</li>
<li>The term <span class="math inline">\(\lambda\mathbf{I}\)</span> is
what stabilizes the <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> matrix, making it
invertible even if itâ€™s singular (due to <span class="math inline">\(p
&gt; n\)</span> or collinearity).</li>
</ul></li>
</ol>
<h2 id="walkthrough-of-the-credit-data-example-all-slides">Walkthrough
of the â€œCredit Dataâ€ Example (All Slides)</h2>
<p>Here is the logical story of the R code, from start to finish.</p>
<h3 id="step-1-data-preparation-slide-8">Step 1: Data Preparation (Slide
8)</h3>
<ul>
<li><code>x=scale(model.matrix(Balance~., Credit)[,-1])</code>
<ul>
<li><code>model.matrix(...)</code> creates the predictor matrix
<code>x</code>.</li>
<li><code>scale(...)</code> is <strong>critically important</strong>. It
standardizes all predictors to have a mean of 0 and a standard deviation
of 1. This is necessary because the ridge penalty <span
class="math inline">\(\lambda \sum \beta_j^2\)</span> is
<em>unit-dependent</em>. If <code>Income</code> (in 10,000s) and
<code>Cards</code> (1-10) were unscaled, the penalty would unfairly
crush the <code>Income</code> coefficient. Scaling puts all predictors
on a level playing field.</li>
</ul></li>
<li><code>y=Credit$Balance</code>
<ul>
<li>This sets the <code>y</code> (target) variable.</li>
</ul></li>
</ul>
<h3 id="step-2-fit-the-ridge-model-slide-8">Step 2: Fit the Ridge Model
(Slide 8)</h3>
<ul>
<li><code>grid=10^seq(4,-2,length=100)</code>
<ul>
<li>This creates a <em>grid</em> of 100 <span
class="math inline">\(\lambda\)</span> values to test, ranging from
<span class="math inline">\(10^4\)</span> (a huge penalty) down to <span
class="math inline">\(10^{-2}\)</span> (a tiny penalty).</li>
</ul></li>
<li><code>ridge.mod=glmnet(x,y,alpha=0,lambda=grid)</code>
<ul>
<li>This is the main command. It fits a <em>separate</em> ridge model
for <em>every single <span class="math inline">\(\lambda\)</span></em>
in the <code>grid</code>.</li>
<li><code>alpha=0</code> is the specific command that tells
<code>glmnet</code> to perform <strong>Ridge Regression</strong>.
(Setting <code>alpha=1</code> would be LASSO).</li>
</ul></li>
<li><code>coef(ridge.mod)[,50]</code>
<ul>
<li>This inspects the model. It pulls out the vector of coefficients for
the 50th <span class="math inline">\(\lambda\)</span> in the grid (which
is <span class="math inline">\(\lambda=10.72\)</span>).</li>
</ul></li>
</ul>
<h3
id="step-3-visualize-the-coefficient-solution-path-slides-1-4-9">Step 3:
Visualize the Coefficient â€œSolution Pathâ€ (Slides 1, 4, 9)</h3>
<p>These plots all show the same thing: how the coefficients change as
<span class="math inline">\(\lambda\)</span> changes.</p>
<ul>
<li><strong>Slide 9 Plot:</strong> This plots the standardized
coefficients for 4 predictors (<code>Income</code>, <code>Limit</code>,
<code>Rating</code>, <code>Student</code>) against the <em>index</em> (1
to 100). Index 1 (left) is the largest <span
class="math inline">\(\lambda\)</span>, and index 100 (right) is the
smallest <span class="math inline">\(\lambda\)</span> (closest to OLS).
You can see the coefficients â€œgrowâ€ from 0 as the penalty (<span
class="math inline">\(\lambda\)</span>) gets smaller.</li>
<li><strong>Slide 1 (Left Plot):</strong> This is the <em>same plot</em>
as Slide 9, but more professional. It plots the coefficients against
<span class="math inline">\(\lambda\)</span> on a log scale. You can
clearly see all coefficients (gray lines) being â€œshrunkâ€ toward zero as
<span class="math inline">\(\lambda\)</span> increases (moves right).
The key predictors (<code>Income</code>, <code>Rating</code>, etc.) are
highlighted.</li>
<li><strong>Slide 1 (Right Plot):</strong> This is the <em>exact same
data</em> again, but with a different x-axis: <span
class="math inline">\(\|\hat{\beta}_\lambda^R\|_2 /
\|\hat{\beta}\|_2\)</span>.
<ul>
<li><strong>1.0</strong> on the right means <span
class="math inline">\(\lambda=0\)</span>. The ratio of the ridge norm to
the OLS norm is 1 (they are the same).</li>
<li><strong>0.0</strong> on the left means <span
class="math inline">\(\lambda=\infty\)</span>. The ridge coefficients
are all 0, so their norm is 0.</li>
<li>This axis shows the â€œfractionâ€ of the full OLS coefficient magnitude
that the model is using.</li>
</ul></li>
<li><strong>Slide 4 Plot:</strong> This plots the <em>total L2 norm</em>
of <em>all</em> coefficients (<span
class="math inline">\(\|\hat{\beta}_\lambda^R\|_2\)</span>) against the
index. As the index goes from 1 to 100 (i.e., <span
class="math inline">\(\lambda\)</span> gets smaller), the total
magnitude of the coefficients gets larger, which is exactly what we
expect.</li>
</ul>
<h3
id="step-4-find-the-best-lambda-using-cross-validation-slides-4-7">Step
4: Find the <em>Best</em> <span class="math inline">\(\lambda\)</span>
using Cross-Validation (Slides 4 &amp; 7)</h3>
<p>We have 100 models. Which one is best?</p>
<ul>
<li><p><strong>The â€œManualâ€ Way (Slide 4):</strong></p>
<ul>
<li>The code splits the data into a <code>train</code> and
<code>test</code> set.</li>
<li>It fits a model <em>only</em> on the <code>train</code> set.</li>
<li>It tests two <span class="math inline">\(\lambda\)</span> values:
<ul>
<li><code>s=4</code>: Gives a test MSE of <code>10293.33</code>.</li>
<li><code>s=10</code>: Gives a test MSE of <code>168981.1</code> (much
worse!).</li>
</ul></li>
<li>This shows that <span class="math inline">\(\lambda=4\)</span> is
better than <span class="math inline">\(\lambda=10\)</span>, but we
donâ€™t know if itâ€™s the <em>best</em>.</li>
</ul></li>
<li><p><strong>The â€œAutomaticâ€ Way (Slide 7):</strong></p>
<ul>
<li><code>cv.out=cv.glmnet(x[train,], y[train], alpha=0)</code></li>
<li>This runs <strong>10-fold Cross-Validation</strong> on the training
set. It automatically splits the training set into 10 â€œfolds,â€ trains on
9, tests on 1, and repeats this 10 times for <em>every <span
class="math inline">\(\lambda\)</span></em>.</li>
<li><strong>The Plot:</strong> The plot on this slide is the result. It
shows the average MSE (y-axis) for each <span
class="math inline">\(\log(\lambda)\)</span> (x-axis). This is the
<em>real-data version</em> of the theoretical purple curve from Slide
3.</li>
<li><code>bestlam=cv.out$lambda.min</code></li>
<li>This command finds the <span class="math inline">\(\lambda\)</span>
at the <em>very bottom</em> of the U-shaped curve. The output shows
<code>bestlam</code> is <strong>41.6</strong>.</li>
<li><code>ridge.pred=predict(ridge.mod, s=bestlam, newx=x[test,])</code></li>
<li>Now, we use this <em>one best <span
class="math inline">\(\lambda\)</span></em> to make predictions on our
held-out <code>test</code> set.</li>
<li><code>mean((ridge.pred-y.test)^2)</code></li>
<li>The final, reliable test MSE is <strong>16129.68</strong>. This is
our best estimate of how the model will perform on new, unseen
data.</li>
</ul></li>
</ul>
<h2 id="python-scikit-learn-equivalents">Python
(<code>scikit-learn</code>) Equivalents</h2>
<p>Here is how you would perform the entire R workflow from your slides
in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. Load and Prepare Data (like Slide 8) ---</span></span><br><span class="line"><span class="comment"># Assuming &#x27;Credit&#x27; is a pandas DataFrame</span></span><br><span class="line"><span class="comment"># X = Credit.drop(&#x27;Balance&#x27;, axis=1)</span></span><br><span class="line"><span class="comment"># y = Credit[&#x27;Balance&#x27;]</span></span><br><span class="line"><span class="comment"># ... (need to handle categorical variables first, e.g., with pd.get_dummies) ...</span></span><br><span class="line"><span class="comment"># For this example, let&#x27;s assume X and y are already loaded and numeric.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the predictors (CRITICAL)</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. Train/Test Split (like Slide 4) ---</span></span><br><span class="line"><span class="comment"># test_size=0.5 and random_state=1 mimic the R code</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X_scaled, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 3. Find Best Lambda (alpha) with Cross-Validation (like Slide 7) ---</span></span><br><span class="line"><span class="comment"># Create the same log-spaced grid of lambdas (sklearn calls it &#x27;alpha&#x27;)</span></span><br><span class="line">lambda_grid = np.logspace(<span class="number">4</span>, -<span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># RidgeCV performs cross-validation to find the best alpha</span></span><br><span class="line"><span class="comment"># cv=10 matches the 10-fold CV</span></span><br><span class="line"><span class="comment"># store_cv_values=True is needed to plot the CV error curve</span></span><br><span class="line">cv_model = RidgeCV(alphas=lambda_grid, store_cv_values=<span class="literal">True</span>, cv=<span class="number">10</span>)</span><br><span class="line">cv_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best lambda found</span></span><br><span class="line">best_lambda = cv_model.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha) found by CV: <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the CV error curve (like Slide 7 plot)</span></span><br><span class="line"><span class="comment"># cv_model.cv_values_ has shape (n_samples, n_alphas)</span></span><br><span class="line"><span class="comment"># We need to average over the samples for each alpha</span></span><br><span class="line">mse_path = np.mean(cv_model.cv_values_, axis=<span class="number">0</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(np.log10(cv_model.alphas_), mse_path, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Log(lambda)&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Mean Squared Error&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Cross-Validation Error Path&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 4. Evaluate on Test Set (like Slide 7) ---</span></span><br><span class="line"><span class="comment"># &#x27;cv_model&#x27; is already refit on the full training set using the best_lambda</span></span><br><span class="line">test_pred = cv_model.predict(X_test)</span><br><span class="line">final_test_mse = mean_squared_error(y_test, test_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Final Test MSE with best lambda: <span class="subst">&#123;final_test_mse&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 5. Get Final Coefficients (like Slide 7, bottom) ---</span></span><br><span class="line"><span class="comment"># The coefficients from the CV-trained model:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Intercept: <span class="subst">&#123;cv_model.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Coefficients:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> coef, feature <span class="keyword">in</span> <span class="built_in">zip</span>(cv_model.coef_, X.columns):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;  <span class="subst">&#123;feature&#125;</span>: <span class="subst">&#123;coef&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 6. Plot the Solution Path (like Slide 1) ---</span></span><br><span class="line"><span class="comment"># To do this, we fit a Ridge model for each lambda and store the coefficients</span></span><br><span class="line">coefs = []</span><br><span class="line"><span class="keyword">for</span> lam <span class="keyword">in</span> lambda_grid:</span><br><span class="line">    model = Ridge(alpha=lam)</span><br><span class="line">    model.fit(X_scaled, y)  <span class="comment"># Fit on all data</span></span><br><span class="line">    coefs.append(model.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(np.log10(lambda_grid), coefs)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Log(lambda)&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Standardized Coefficients&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Ridge Solution Path&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="shrinkage-methods-regularization-1">7. Shrinkage Methods
(Regularization)</h1>
<p>These slides cover <strong>Shrinkage Methods</strong>, also known as
<strong>Regularization</strong>, which are techniques used to improve on
the standard least squares model, particularly when dealing with many
variables or multicollinearity. The main focus is on
<strong>LASSO</strong> regression.</p>
<h2 id="key-mathematical-formulas">Key Mathematical Formulas</h2>
<p>The slides present two main, but equivalent, ways to formulate these
methods.</p>
<h3 id="penalized-formulation-slide-1">1. Penalized Formulation (Slide
1)</h3>
<p>This is the most common formulation. The goal is to minimize a
function that is a combination of the <strong>Residual Sum of Squares
(RSS)</strong> and a <strong>penalty term</strong>. The penalty
discourages large coefficients.</p>
<ul>
<li><strong>LASSO (Least Absolute Shrinkage and Selection
Operator):</strong> The goal is to find coefficients (<span
class="math inline">\(\beta_0, \beta_j\)</span>) that minimize: <span
class="math display">\[\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p}
\beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j|\]</span>
<ul>
<li><strong>Penalty:</strong> The <span
class="math inline">\(L_1\)</span> norm (<span
class="math inline">\(\|\beta\|_1\)</span>), which is the sum of the
<em>absolute values</em> of the coefficients.</li>
<li><strong>Key Property:</strong> This penalty can force some
coefficients to be <strong>exactly zero</strong>, effectively performing
automatic variable selection.</li>
</ul></li>
</ul>
<h3 id="constrained-formulation-slide-2">2. Constrained Formulation
(Slide 2)</h3>
<p>This alternative formulation minimizes the RSS <em>subject to a
constraint</em> (a â€œbudgetâ€) on the size of the coefficients.</p>
<ul>
<li><p><strong>For Lasso:</strong> Minimize RSS subject to: <span
class="math display">\[\sum_{j=1}^{p} |\beta_j| \le s\]</span> (The sum
of the absolute values of the coefficients must be less than some budget
<span class="math inline">\(s\)</span>.)</p></li>
<li><p><strong>For Ridge:</strong> Minimize RSS subject to: <span
class="math display">\[\sum_{j=1}^{p} \beta_j^2 \le s\]</span> (The sum
of the <em>squares</em> of the coefficients (<span
class="math inline">\(L_2\)</span> norm) must be less than <span
class="math inline">\(s\)</span>.)</p></li>
</ul>
<p><strong>Equivalence (Slide 3):</strong> For any penalty value <span
class="math inline">\(\lambda\)</span> used in the first formulation,
there is a corresponding budget <span class="math inline">\(s\)</span>
in the second formulation that will give the exact same set of
coefficients. <span class="math inline">\(\lambda\)</span> and <span
class="math inline">\(s\)</span> are inversely related: a large <span
class="math inline">\(\lambda\)</span> (high penalty) corresponds to a
small <span class="math inline">\(s\)</span> (small budget).</p>
<h2 id="important-plots-and-interpretation">Important Plots and
Interpretation</h2>
<p>Your slides show the two most important plots for understanding and
using LASSO.</p>
<h3 id="the-cross-validation-cv-plot-slide-5">1. The Cross-Validation
(CV) Plot (Slide 5)</h3>
<p>This plot is crucial for <strong>choosing the best tuning parameter
(<span class="math inline">\(\lambda\)</span>)</strong>.</p>
<ul>
<li><strong>X-axis:</strong> <span
class="math inline">\(\text{Log}(\lambda)\)</span>. This is the penalty
strength.
<ul>
<li><strong>Right side (high <span
class="math inline">\(\lambda\)</span>):</strong> High penalty, simple
model (many coefficients are 0), high bias, high Mean-Squared Error
(MSE).</li>
<li><strong>Left side (low <span
class="math inline">\(\lambda\)</span>):</strong> Low penalty, complex
model (like standard linear regression), high variance, MSE starts to
increase (overfitting).</li>
</ul></li>
<li><strong>Y-axis:</strong> Mean-Squared Error (MSE) from
cross-validation.</li>
<li><strong>Goal:</strong> Find the <span
class="math inline">\(\lambda\)</span> at the <strong>bottom of the â€œUâ€
shape</strong>, which gives the <em>lowest</em> MSE. This is the optimal
trade-off between bias and variance. The top axis shows how many
variables are included in the model at each <span
class="math inline">\(\lambda\)</span>.</li>
</ul>
<h3 id="the-coefficient-path-plot-slide-6">2. The Coefficient Path Plot
(Slide 6)</h3>
<p>This plot is the best visualization for <strong>understanding what
LASSO does</strong>.</p>
<ul>
<li><strong>Left Plot (vs.Â <span
class="math inline">\(\lambda\)</span>):</strong>
<ul>
<li><strong>X-axis:</strong> The penalty strength <span
class="math inline">\(\lambda\)</span>.</li>
<li><strong>Y-axis:</strong> The standardized value of each
coefficient.</li>
<li><strong>How to read it:</strong> Start from the
<strong>right</strong> (high <span
class="math inline">\(\lambda\)</span>). All coefficients are 0. As you
move <strong>left</strong>, <span class="math inline">\(\lambda\)</span>
<em>decreases</em>, and the penalty is relaxed. Variables â€œenterâ€ the
model one by one (their coefficients become non-zero). You can see that
â€˜Ratingâ€™, â€˜Incomeâ€™, and â€˜Studentâ€™ are the most important variables, as
they are the first to become non-zero.</li>
</ul></li>
<li><strong>Right Plot (vs.Â <span class="math inline">\(L_1\)</span>
Norm Ratio):</strong>
<ul>
<li>This shows the exact same information as the left plot, but the
x-axis is reversed and rescaled. An axis value of 0.0 means full penalty
(all <span class="math inline">\(\beta=0\)</span>), and 1.0 means no
penalty.</li>
</ul></li>
</ul>
<h2 id="code-understanding-r-to-python">Code Understanding (R to
Python)</h2>
<p>The slides use the <code>glmnet</code> package in R. The equivalent
and most popular library in Python is <strong>scikit-learn</strong>.</p>
<h3 id="finding-the-best-lambda-cv">1. Finding the Best <span
class="math inline">\(\lambda\)</span> (CV)</h3>
<p>The R code <code>cv.out=cv.glmnet(x[train,],y[train],alpha=1)</code>
performs cross-validation to find the best <span
class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><strong>Python Equivalent:</strong> Use <code>LassoCV</code>. It
does the same thing: tests many <span
class="math inline">\(\lambda\)</span> values (called
<code>alphas</code> in scikit-learn) and picks the best one.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the LassoCV object</span></span><br><span class="line"><span class="comment"># cv=5 means 5-fold cross-validation</span></span><br><span class="line">lasso_cv = LassoCV(cv=<span class="number">5</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model to the training data</span></span><br><span class="line">lasso_cv.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best lambda (called alpha_ in sklearn)</span></span><br><span class="line">best_lambda = lasso_cv.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha): <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the MSEs</span></span><br><span class="line"><span class="comment"># This is what&#x27;s plotted in the CV plot</span></span><br><span class="line"><span class="built_in">print</span>(lasso_cv.mse_path_)</span><br></pre></td></tr></table></figure>
<h3 id="fitting-with-the-best-lambda-and-getting-coefficients">2.
Fitting with the Best <span class="math inline">\(\lambda\)</span> and
Getting Coefficients</h3>
<p>The R code
<code>lasso.coef=predict(out,type="coefficients",s=bestlam)</code> gets
the coefficients for the best <span
class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><strong>Python Equivalent:</strong> The <code>LassoCV</code> object
is <em>already</em> refitted on the full training data using the best
<span class="math inline">\(\lambda\)</span>. You can also fit a new
<code>Lasso</code> model with that specific <span
class="math inline">\(\lambda\)</span>.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Option 1: Use the already-fitted LassoCV object ---</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Coefficients from LassoCV:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(lasso_cv.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on the test set</span></span><br><span class="line">y_pred = lasso_cv.predict(X_test)</span><br><span class="line">test_mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test MSE: <span class="subst">&#123;test_mse&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Option 2: Fit a new Lasso model with the best lambda ---</span></span><br><span class="line">final_lasso = Lasso(alpha=best_lambda)</span><br><span class="line">final_lasso.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get coefficients (Slide 7 shows this)</span></span><br><span class="line"><span class="comment"># Note how some are 0!</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nCoefficients from new Lasso model:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(final_lasso.coef_)</span><br></pre></td></tr></table></figure>
<h2 id="the-core-problem-two-equivalent-formulas">The Core Problem: Two
Equivalent Formulas</h2>
<p>The slides show two ways of writing the <em>same problem</em>.
Understanding this equivalence is key.</p>
<h3 id="formulation-1-the-penalized-method-slides-1-4">Formulation 1:
The Penalized Method (Slides 1 &amp; 4)</h3>
<ul>
<li><p><strong>Formula:</strong> <span
class="math display">\[\min_{\beta} \left( \sum_{i=1}^{n} (y_i -
\mathbf{x}_i^T \beta)^2 + \lambda \|\beta\|_1 \right)\]</span></p>
<ul>
<li><strong><span class="math inline">\(\sum (y_i - \mathbf{x}_i^T
\beta)^2\)</span></strong>: This is the normal <strong>Residual Sum of
Squares (RSS)</strong>. We want to make this small (fit the data
well).</li>
<li><strong><span class="math inline">\(\lambda
\|\beta\|_1\)</span></strong>: This is the <strong><span
class="math inline">\(L_1\)</span> penalty</strong>.
<ul>
<li><span class="math inline">\(\|\beta\|_1 = \sum_{j=1}^{p}
|\beta_j|\)</span> is the sum of the absolute values of the
coefficients.</li>
<li><span class="math inline">\(\lambda\)</span> (lambda) is a tuning
parameter. Think of it as a <strong>â€œpenalty knobâ€</strong>.</li>
</ul></li>
</ul></li>
<li><p><strong>How to think about <span
class="math inline">\(\lambda\)</span></strong>:</p>
<ul>
<li><strong>If <span class="math inline">\(\lambda =
0\)</span>:</strong> There is no penalty. This is just standard Ordinary
Least Squares (OLS) regression. The model will likely overfit.</li>
<li><strong>If <span class="math inline">\(\lambda\)</span> is
<em>small</em>:</strong> Thereâ€™s a small penalty. Coefficients will
shrink a <em>little</em> bit.</li>
<li><strong>If <span class="math inline">\(\lambda\)</span> is <em>very
large</em>:</strong> The penalty is severe. The <em>only</em> way to
make the penalty term small is to make the coefficients (<span
class="math inline">\(\beta\)</span>) themselves small. The model will
eventually shrink all coefficients to <strong>exactly 0</strong>.</li>
</ul></li>
</ul>
<h3 id="formulation-2-the-constrained-method-slides-2-3">Formulation 2:
The Constrained Method (Slides 2 &amp; 3)</h3>
<ul>
<li><p><strong>Formula:</strong> <span
class="math display">\[\min_{\beta} \sum_{i=1}^{n} (y_i - \mathbf{x}_i^T
\beta)^2 \quad \text{subject to} \quad \|\beta\|_1 \le
s\]</span></p></li>
<li><p><strong>How to think about <span
class="math inline">\(s\)</span></strong>:</p>
<ul>
<li>This says: â€œFind the best-fitting model (minimize RSS) <em>but</em>
you have a limited <strong>â€˜budgetâ€™ <span
class="math inline">\(s\)</span></strong> for the total size of your
coefficients.â€</li>
<li><strong>If <span class="math inline">\(s\)</span> is <em>very
large</em>:</strong> The budget is huge. This constraint does nothing.
You get the standard OLS solution.</li>
<li><strong>If <span class="math inline">\(s\)</span> is
<em>small</em>:</strong> The budget is tight. You <em>must</em> shrink
your coefficients to stay under the budget <span
class="math inline">\(s\)</span>. To get the best fit, the model will be
forced to set unimportant coefficients to 0 and only â€œspendâ€ its budget
on the most important variables.</li>
</ul></li>
</ul>
<p><strong>The Equivalence:</strong> These two forms are equivalent. For
any <span class="math inline">\(\lambda\)</span> you pick, thereâ€™s a
corresponding budget <span class="math inline">\(s\)</span> that gives
the <em>exact same solution</em>.</p>
<ul>
<li>High <span class="math inline">\(\lambda\)</span> (strong penalty)
<span class="math inline">\(\iff\)</span> Small <span
class="math inline">\(s\)</span> (tight budget)</li>
<li>Low <span class="math inline">\(\lambda\)</span> (weak penalty)
<span class="math inline">\(\iff\)</span> Large <span
class="math inline">\(s\)</span> (loose budget)</li>
</ul>
<p>This equivalence is why you see plots with both <span
class="math inline">\(\lambda\)</span> and <span
class="math inline">\(L_1\)</span> Norm on the x-axis. They are just two
different ways of looking at the same â€œpenaltyâ€ spectrum.</p>
<h2 id="detailed-plot-code-analysis">Detailed Plot &amp; Code
Analysis</h2>
<p>Letâ€™s look at the plots and code, which answer the practical
questions: <strong>(1)</strong> How do we pick the <em>best</em> <span
class="math inline">\(\lambda\)</span>? and <strong>(2)</strong> What
does LASSO <em>do</em> to the coefficients?</p>
<h3 id="question-1-how-to-pick-the-best-lambda-slide-5">Question 1: How
to pick the best <span class="math inline">\(\lambda\)</span>? (Slide
5)</h3>
<p>This is the <strong>Cross-Validation (CV) Plot</strong>. Its one and
only job is to help you find the optimal <span
class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><strong>R Code:</strong>
<code>cv.out=cv.glmnet(x[train,],y[train],alpha=1)</code>
<ul>
<li><code>cv.glmnet</code>: This R function <em>automatically</em> does
K-fold cross-validation. <code>alpha=1</code> explicitly tells it to use
<strong>LASSO</strong> (alpha=0 would be Ridge).</li>
<li>It tries a whole range of <span
class="math inline">\(\lambda\)</span> values, calculates the
Mean-Squared Error (MSE) for each, and stores the results in
<code>cv.out</code>.</li>
</ul></li>
<li><strong>Plot Analysis:</strong>
<ul>
<li><strong>X-axis:</strong> <span
class="math inline">\(\text{Log}(\lambda)\)</span>. The penalty
strength. <strong>Right = High Penalty</strong> (simple model),
<strong>Left = Low Penalty</strong> (complex model).</li>
<li><strong>Y-axis:</strong> Mean-Squared Error (MSE). <strong>Lower is
better.</strong></li>
<li><strong>Red Dots:</strong> The average MSE for each <span
class="math inline">\(\lambda\)</span>.</li>
<li><strong>Gray Bars:</strong> The error bars (standard error).</li>
<li><strong>The â€œUâ€ Shape:</strong> This is the classic
<strong>bias-variance trade-off</strong>.
<ul>
<li><strong>Right Side (High <span
class="math inline">\(\lambda\)</span>):</strong> The model is <em>too
simple</em> (too many coefficients are 0). Itâ€™s â€œunderfitting.â€ The
error is high (high bias).</li>
<li><strong>Left Side (High <span
class="math inline">\(\lambda\)</span>):</strong> The model is <em>too
complex</em> (low penalty, like OLS). Itâ€™s â€œoverfittingâ€ the training
data. The error on new data is high (high variance).</li>
<li><strong>Bottom of the â€œUâ€:</strong> This is the â€œsweet spot.â€ The
<span class="math inline">\(\lambda\)</span> at the very bottom (marked
by the left vertical dotted line) gives the <strong>lowest possible
MSE</strong>. This is <code>lambda.min</code>.</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Answer:</strong> You pick the <span
class="math inline">\(\lambda\)</span> that corresponds to the lowest
point on this graph.</p>
<h3 id="question-2-what-does-lasso-do-slides-5-6-7">Question 2: What
does LASSO <em>do</em>? (Slides 5, 6, 7)</h3>
<p>These slides all show the <em>effect</em> of LASSO.</p>
<p><strong>A. The Coefficient Path Plots (Slides 5 &amp; 6)</strong></p>
<p>These plots visualize how coefficients change. They show the <em>same
information</em> just with different x-axes.</p>
<ul>
<li><strong>Left Plot (Slide 6) vs.Â <span
class="math inline">\(\lambda\)</span>:</strong>
<ul>
<li><strong>How to read:</strong> Read from <strong>RIGHT to
LEFT</strong>.</li>
<li>At the far right (<span class="math inline">\(\lambda\)</span> is
large), all coefficients are 0.</li>
<li>As you move left, <span class="math inline">\(\lambda\)</span> gets
smaller, and the penalty is relaxed. Variables â€œenterâ€ the model one by
one as their coefficients become non-zero.</li>
<li>You can see â€˜Ratingâ€™ (red-dashed), â€˜Studentâ€™ (black-solid), and
â€˜Incomeâ€™ (blue-dotted) are the first to enter, suggesting they are the
most important predictors.</li>
</ul></li>
<li><strong>Right Plot (Slide 6) vs.Â <span
class="math inline">\(L_1\)</span> Norm Ratio:</strong>
<ul>
<li>This is the <em>same plot</em>, just flipped and rescaled. The
x-axis is <span class="math inline">\(\|\hat{\beta}_\lambda\|_1 /
\|\hat{\beta}_{OLS}\|_1\)</span>.</li>
<li><strong>How to read:</strong> Read from <strong>LEFT to
RIGHT</strong>.</li>
<li><strong>At 0.0:</strong> This is a â€œ0% budgetâ€ (like <span
class="math inline">\(s=0\)</span> or <span
class="math inline">\(\lambda=\infty\)</span>). All coefficients are
0.</li>
<li><strong>At 1.0:</strong> This is a â€œ100% budgetâ€ (like <span
class="math inline">\(s=\infty\)</span> or <span
class="math inline">\(\lambda=0\)</span>). This is the full OLS
model.</li>
<li>This view clearly shows the coefficients â€œgrowingâ€ from 0 as their
â€œbudgetâ€ (<span class="math inline">\(L_1\)</span> Norm) increases.</li>
</ul></li>
</ul>
<p><strong>B. The Code Output (Slide 7) - This is the most important
â€œanswerâ€</strong></p>
<p>This slide <em>explicitly demonstrates</em> variable selection by
comparing the coefficients from two different <span
class="math inline">\(\lambda\)</span> values.</p>
<ul>
<li><p><strong>First Block (The â€œOptimalâ€ Model):</strong></p>
<ul>
<li><code>bestlam.cv &lt;- cv.out$lambda.min</code>: This gets the <span
class="math inline">\(\lambda\)</span> from the bottom of the â€œUâ€ in the
CV plot.</li>
<li><code>lasso.conf &lt;- predict(out,type="coefficients",s=bestlam.cv)[1:12,]</code>:
This gets the coefficients using that <em>best</em> <span
class="math inline">\(\lambda\)</span>.</li>
<li><code>lasso.conf[lasso.conf!=0]</code>: This R command filters the
list to show <em>only the non-zero coefficients</em>.</li>
<li><strong>Result:</strong> The optimal model <em>still keeps 10
variables</em> (â€˜Incomeâ€™, â€˜Limitâ€™, â€˜Ratingâ€™, etc.). It has shrunk them,
but it hasnâ€™t set many to 0.</li>
</ul></li>
<li><p><strong>Second Block (The â€œHigh Penaltyâ€ Model):</strong></p>
<ul>
<li>The slide text says â€œif we choose a larger regularization
parameter.â€ Here, theyâ€™ve picked an arbitrary <em>larger</em> value,
<code>s=10</code>. (Note: Râ€™s <code>predict.glmnet</code> can be
confusing; <code>s=10</code> here means <span
class="math inline">\(\lambda=10\)</span>).</li>
<li><code>lasso.conf &lt;- predict(out,type="coefficients",s=10)[1:12,]</code>:
This gets the coefficients using a <em>stronger penalty</em> (<span
class="math inline">\(\lambda=10\)</span>).</li>
<li><code>lasso.conf[lasso.conf!=0]</code>: Again, show only the
non-zero coefficients.</li>
<li><strong>Result:</strong> Look! The list is much shorter. The
coefficients for â€˜Ageâ€™, â€˜Educationâ€™, â€˜GenderFemaleâ€™, â€˜MarriedYesâ€™, and
â€˜Ethnicityâ€™ are <em>all gone</em> (shrunk to 0.000000). The model has
decided these are not important enough to â€œspendâ€ budget on.</li>
</ul></li>
</ul>
<p><strong>Conclusion:</strong> LASSO performs <strong>automatic
variable selection</strong>. By increasing <span
class="math inline">\(\lambda\)</span>, you create a
<strong>sparser</strong> (simpler) model. Slide 7 is the concrete
proof.</p>
<h2 id="python-equivalents-in-more-detail">Python Equivalents (in more
detail)</h2>
<p>Here is how you would replicate the <em>entire</em> workflow from the
slides in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso, LassoCV, lasso_path</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Assume X_train, y_train, X_test, y_test are loaded ---</span></span><br><span class="line"><span class="comment"># Example: </span></span><br><span class="line"><span class="comment"># data = pd.read_csv(&#x27;Credit.csv&#x27;)</span></span><br><span class="line"><span class="comment"># X = pd.get_dummies(data.drop([&#x27;ID&#x27;, &#x27;Balance&#x27;], axis=1), drop_first=True)</span></span><br><span class="line"><span class="comment"># y = data[&#x27;Balance&#x27;]</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It&#x27;s CRITICAL to scale data before regularization</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train_scaled = scaler.fit_transform(X_train)</span><br><span class="line">X_test_scaled = scaler.transform(X_test)</span><br><span class="line">feature_names = X.columns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Replicate the CV Plot (Slide 5: ...000200.png)</span></span><br><span class="line"><span class="comment"># LassoCV does what cv.glmnet does: finds the best lambda (alpha)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Running LassoCV to find best lambda (alpha)...&quot;</span>)</span><br><span class="line"><span class="comment"># &#x27;alphas&#x27; is the list of lambdas to try. We can let it choose automatically.</span></span><br><span class="line"><span class="comment"># cv=10 means 10-fold cross-validation.</span></span><br><span class="line">lasso_cv = LassoCV(cv=<span class="number">10</span>, random_state=<span class="number">1</span>, max_iter=<span class="number">10000</span>)</span><br><span class="line">lasso_cv.fit(X_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The best lambda found</span></span><br><span class="line">best_lambda = lasso_cv.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha) found: <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Plotting the CV (MSE vs. Log(Lambda)) ---</span></span><br><span class="line"><span class="comment"># This recreates the R plot</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"><span class="comment"># lasso_cv.mse_path_ is a (n_alphas, n_folds) array of MSEs</span></span><br><span class="line"><span class="comment"># We take the mean across the folds (axis=1)</span></span><br><span class="line">mean_mses = np.mean(lasso_cv.mse_path_, axis=<span class="number">1</span>)</span><br><span class="line">log_lambdas = np.log10(lasso_cv.alphas_)</span><br><span class="line"></span><br><span class="line">plt.plot(log_lambdas, mean_mses, <span class="string">&#x27;r.-&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Log(Lambda / Alpha)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Mean-Squared Error&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;LASSO Cross-Validation Path (Replicating R Plot)&#x27;</span>)</span><br><span class="line"><span class="comment"># Plot a vertical line at the best lambda</span></span><br><span class="line">plt.axvline(np.log10(best_lambda), linestyle=<span class="string">&#x27;--&#x27;</span>, color=<span class="string">&#x27;k&#x27;</span>, label=<span class="string">f&#x27;Best Lambda (alpha) = <span class="subst">&#123;best_lambda:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.gca().invert_xaxis() <span class="comment"># High lambda is on the right in R plot</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Replicate the Coefficient Path Plot (Slide 6: ...000206.png)</span></span><br><span class="line"><span class="comment"># We can use the lasso_path function, or just use the CV object</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The lasso_cv object already calculated the paths!</span></span><br><span class="line">coefs = lasso_cv.path(X_train_scaled, y_train, alphas=lasso_cv.alphas_)[<span class="number">1</span>].T</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X_train_scaled.shape[<span class="number">1</span>]):</span><br><span class="line">    plt.plot(log_lambdas, coefs[:, i], label=feature_names[i])</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Log(Lambda / Alpha)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Standardized Coefficients&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;LASSO Coefficient Path (Replicating R Plot)&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">plt.gca().invert_xaxis()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Replicate the Code Output (Slide 7: ...000202.png)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Replicating R Output ---&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- First Block: Coefficients with BEST lambda ---</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Coefficients using best lambda (alpha = <span class="subst">&#123;best_lambda:<span class="number">.4</span>f&#125;</span>):&quot;</span>)</span><br><span class="line"><span class="comment"># The lasso_cv object is already fitted with the best lambda</span></span><br><span class="line">best_coefs = lasso_cv.coef_</span><br><span class="line">coef_series_best = pd.Series(best_coefs, index=feature_names)</span><br><span class="line"><span class="comment"># This is like R&#x27;s `lasso.conf[lasso.conf != 0]`</span></span><br><span class="line"><span class="built_in">print</span>(coef_series_best[coef_series_best != <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Second Block: Coefficients with a LARGER lambda ---</span></span><br><span class="line"><span class="comment"># Let&#x27;s pick a larger lambda, e.g., 10 (like the slide)</span></span><br><span class="line">large_lambda = <span class="number">10</span> </span><br><span class="line">lasso_high_penalty = Lasso(alpha=large_lambda)</span><br><span class="line">lasso_high_penalty.fit(X_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nCoefficients using larger lambda (alpha = <span class="subst">&#123;large_lambda&#125;</span>):&quot;</span>)</span><br><span class="line">high_pen_coefs = lasso_high_penalty.coef_</span><br><span class="line">coef_series_high = pd.Series(high_pen_coefs, index=feature_names)</span><br><span class="line"><span class="comment"># This is the second R command: `lasso.conf[lasso.conf != 0]`</span></span><br><span class="line"><span class="built_in">print</span>(coef_series_high[coef_series_high != <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Final Prediction ---</span></span><br><span class="line"><span class="comment"># This is R&#x27;s `mean((lasso.pred-y.test)^2)`</span></span><br><span class="line">y_pred = lasso_cv.predict(X_test_scaled)</span><br><span class="line">test_mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nTest MSE using best lambda: <span class="subst">&#123;test_mse:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="the-game-of-regularization">The â€œGameâ€ of Regularization</h3>
<p>First, letâ€™s understand what these plots are showing. This is a â€œmapâ€
of a constrained optimization problem.</p>
<ul>
<li><strong>The Red Ellipses (RSS Contours):</strong> Think of these as
contour lines on a topographic map.
<ul>
<li><strong>The Center (<span
class="math inline">\(\hat{\beta}\)</span>):</strong> This point is the
â€œbottom of the valley.â€ It represents the <em>perfect</em>,
unconstrained solutionâ€”the standard Ordinary Least Squares (OLS)
coefficients. This point has the lowest possible Residual Sum of Squares
(RSS), or error.</li>
<li><strong>The Lines:</strong> Every point on a single red ellipse has
the <em>exact same</em> RSS. As the ellipses get bigger (moving away
from the center <span class="math inline">\(\hat{\beta}\)</span>), the
error gets higher.</li>
</ul></li>
<li><strong>The Blue Shaded Area (Constraint Region):</strong> This is
the â€œruleâ€ of the game.
<ul>
<li>This is our â€œbudget.â€ We are <em>only allowed</em> to pick a
solution (<span class="math inline">\(\beta_1, \beta_2\)</span>) from
<em>inside or on the boundary</em> of this blue shape.</li>
<li><strong>LASSO:</strong> The constraint is <span
class="math inline">\(|\beta_1| + |\beta_2| \le s\)</span>. This
equation forms a <strong>diamond</strong> (or a rotated square).</li>
<li><strong>Ridge:</strong> The constraint is <span
class="math inline">\(\beta_1^2 + \beta_2^2 \le s\)</span>. This
equation forms a <strong>circle</strong>.</li>
</ul></li>
<li><strong>The Goal:</strong> Find the â€œbestâ€ point that is <em>inside
the blue area</em>.
<ul>
<li>The â€œbestâ€ point is the one with the lowest possible error
(RSS).</li>
<li>Geometrically, this means we start at the center (<span
class="math inline">\(\hat{\beta}\)</span>) and expand our ellipse
outward. The <em>very first point</em> where the ellipse
<strong>touches</strong> the blue constraint region is our
solution.</li>
</ul></li>
</ul>
<h3 id="why-lasso-performs-variable-selection-the-diamond">Why LASSO
Performs Variable Selection (The Diamond) ğŸ¯</h3>
<p>This is the most important concept. Look at the LASSO diagrams.</p>
<ul>
<li><strong>The Shape:</strong> The LASSO constraint is a
<strong>diamond</strong>.</li>
<li><strong>The Key Feature:</strong> This diamond has <strong>sharp
corners</strong> (vertices). And most importantly, these corners lie
<strong>exactly on the axes</strong>.
<ul>
<li>The top corner is at <span class="math inline">\((\beta_1=0,
\beta_2=s)\)</span>.</li>
<li>The right corner is at <span class="math inline">\((\beta_1=s,
\beta_2=0)\)</span>.</li>
</ul></li>
<li><strong>The â€œCollisionâ€:</strong> Now, imagine the red ellipses
(representing our error) expanding from the OLS solution (<span
class="math inline">\(\hat{\beta}\)</span>). They will almost always
â€œhitâ€ the blue diamond at one of its <strong>sharp corners</strong>.
<ul>
<li>Look at your textbook diagram (slide <code>...000304.png</code>).
The ellipse clearly makes contact with the diamond at the top corner,
where <span class="math inline">\(\beta_1 = 0\)</span>.</li>
<li>Look at your example (slide <code>...000259.jpg</code>). The center
of the ellipses is at (4, 0.1). The closest point on the diamond that
the expanding ellipses will hit is the corner at (2, 0). At this
solution, <strong><span class="math inline">\(y\)</span> is exactly
0</strong>.</li>
</ul></li>
</ul>
<p><strong>Conclusion:</strong> Because the <span
class="math inline">\(L_1\)</span> â€œdiamondâ€ has corners on the axes,
the optimal solution is very likely to land on one of them. When it
does, the coefficient for the <em>other</em> axis is set to
<strong>exactly zero</strong>. This is the <strong>variable selection
property</strong>.</p>
<h3 id="why-ridge-regression-only-shrinks-the-circle">Why Ridge
Regression Only Shrinks (The Circle) ğŸ¤</h3>
<p>Now, look at the Ridge regression diagram.</p>
<ul>
<li><strong>The Shape:</strong> The Ridge constraint is a
<strong>circle</strong>.</li>
<li><strong>The Key Feature:</strong> A circle is perfectly smooth and
has <strong>no corners</strong>.</li>
<li><strong>The â€œCollisionâ€:</strong> Imagine the same ellipses
expanding and hitting the blue circle. The contact point will be a
<em>tangent</em> point.
<ul>
<li>Because the circle is round, this tangent point can be
<em>anywhere</em> on its circumference.</li>
<li>It is <em>extremely unlikely</em> that the contact point will be
exactly on an axis (e.g., at <span class="math inline">\((\beta_1=0,
\beta_2=s)\)</span>). This would only happen if the OLS solution <span
class="math inline">\(\hat{\beta}\)</span> was <em>already</em>
perfectly aligned with that axis.</li>
</ul></li>
<li><strong>Conclusion:</strong> The Ridge solution will find a point
where <em>both</em> <span class="math inline">\(\beta_1\)</span> and
<span class="math inline">\(\beta_2\)</span> are non-zero. The
coefficients are â€œshrunkâ€ (pulled in from <span
class="math inline">\(\hat{\beta}\)</span> towards the origin), but they
<strong>never become zero</strong>. This is why Ridge is called a
â€œshrinkageâ€ method, but not a â€œvariable selectionâ€ method.</li>
</ul>
<h3 id="summary-diamond-vs.-circle">Summary: Diamond vs.Â Circle</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">LASSO (<span
class="math inline">\(L_1\)</span> Norm)</th>
<th style="text-align: left;">Ridge (<span
class="math inline">\(L_2\)</span> Norm)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Constraint Shape</strong></td>
<td style="text-align: left;"><strong>Diamond</strong> (or
hyper-rhombus)</td>
<td style="text-align: left;"><strong>Circle</strong> (or
hypersphere)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Key Feature</strong></td>
<td style="text-align: left;"><strong>Sharp corners</strong> on the
axes</td>
<td style="text-align: left;"><strong>Smooth curve</strong> with no
corners</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Geometric Solution</strong></td>
<td style="text-align: left;">Ellipses hit the
<strong>corners</strong></td>
<td style="text-align: left;">Ellipses hit a <strong>smooth
part</strong></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Result</strong></td>
<td style="text-align: left;">Forces some coefficients to
<strong>exactly 0</strong></td>
<td style="text-align: left;">Shrinks all coefficients <em>towards</em>
0</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Name</strong></td>
<td style="text-align: left;"><strong>Variable Selection</strong></td>
<td style="text-align: left;"><strong>Shrinkage</strong></td>
</tr>
</tbody>
</table>
<p>The â€œspace meaningâ€ is that the <strong>sharp corners of the <span
class="math inline">\(L_1\)</span> diamond are what make variable
selection possible</strong>. The smooth circle of the <span
class="math inline">\(L_2\)</span> norm does not have these corners and
thus cannot force coefficients to zero.</p>
<h1 id="shrinkage-methods-lasso-vs.-ridge">8. Shrinkage Methods (Lasso
vs.Â Ridge)</h1>
<h2 id="core-concept-shrinkage-methods">Core Concept: Shrinkage
Methods</h2>
<p>Both <strong>Ridge (L2)</strong> and <strong>Lasso (L1)</strong> are
regularization techniques used to improve upon standard <strong>Ordinary
Least Squares (OLS)</strong> regression.</p>
<p>Their main goal is to manage the <strong>bias-variance
tradeoff</strong>. OLS often has low bias but very high variance,
especially when you have many predictors (<span
class="math inline">\(p\)</span>) or when predictors are correlated.
Ridge and Lasso improve prediction accuracy by <em>shrinking</em> the
regression coefficients towards zero. This adds a small amount of bias
but significantly <em>reduces</em> the variance, leading to a lower
overall Test Mean Squared Error (MSE).</p>
<h2 id="the-key-difference-math-how-they-shrink">The Key Difference:
Math &amp; How They Shrink</h2>
<p>The slides show that the two methods use different penalties, which
leads to very different mathematical forms and practical outcomes.</p>
<ul>
<li><strong>Ridge Regression (L2 Penalty):</strong> Minimizes <span
class="math inline">\(RSS + \lambda \sum_{j=1}^{p}
\beta_j^2\)</span></li>
<li><strong>Lasso Regression (L1 Penalty):</strong> Minimizes <span
class="math inline">\(RSS + \lambda \sum_{j=1}^{p}
|\beta_j|\)</span></li>
</ul>
<p>Slide 80 provides the exact formulas for their coefficient estimates
in a simple, orthogonal case (where predictors are independent):</p>
<h3 id="ridge-regression-proportional-shrinkage">Ridge Regression
(Proportional Shrinkage)</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\beta}_j^R = \hat{\beta}_j^{LSE} / (1 +
\lambda)\)</span></li>
<li><strong>What this means:</strong> Ridge <em>shrinks</em> every least
squares coefficient by a proportional amount. It will make coefficients
<em>smaller</em>, but it will <strong>never set them to exactly
zero</strong> (unless <span class="math inline">\(\lambda\)</span> is
<span class="math inline">\(\infty\)</span>).</li>
</ul>
<h3 id="lasso-regression-soft-thresholding">Lasso Regression
(Soft-Thresholding)</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\beta}_j^L =
\text{sign}(\hat{\beta}_j^{LSE})(|\hat{\beta}_j^{LSE}| -
\lambda/2)_+\)</span></li>
<li><strong>What this means:</strong> This is a â€œsoft-thresholdingâ€
operator.
<ul>
<li>If the original coefficient <span
class="math inline">\(\hat{\beta}_j^{LSE}\)</span> is small (its
absolute value is less than <span
class="math inline">\(\lambda/2\)</span>), Lasso <strong>sets it to
exactly zero</strong>.</li>
<li>If the coefficient is large, Lasso subtracts <span
class="math inline">\(\lambda/2\)</span> from its absolute value,
shrinking it towards zero.</li>
</ul></li>
<li><strong>Key Property:</strong> Because of this, Lasso performs
<strong>automatic feature selection</strong> by eliminating
predictors.</li>
</ul>
<h2 id="important-images-explained">Important Images Explained</h2>
<h3 id="most-important-figure-6.10-slide-82">Most Important: Figure 6.10
(Slide 82)</h3>
<p>This is the best visual for understanding the <em>mathematical
difference</em> from the formulas above.</p>
<ul>
<li><strong>Left (Ridge):</strong> The red line shows the Ridge estimate
vs.Â the OLS estimate. Itâ€™s a straight, diagonal line with a slope less
than 1. It shrinks everything <em>proportionally</em>.</li>
<li><strong>Right (Lasso):</strong> The red line shows the Lasso
estimate. Itâ€™s â€œflatâ€ at zero for a range, showing it <strong>sets small
coefficients to zero</strong>. Then, it slopes up, but itâ€™s shifted (it
shrinks the large coefficients by a fixed amount).</li>
</ul>
<h3 id="scenario-1-figure-6.8-slide-76">Scenario 1: Figure 6.8 (Slide
76)</h3>
<p>This plot shows what happens when <strong>all 45 predictors are truly
related to the response</strong>.</p>
<ul>
<li><strong>Result (Slide 77):</strong> <strong>Ridge performs slightly
better</strong> (has a lower minimum MSE, shown by the dotted purple
line).</li>
<li><strong>Why:</strong> Lassoâ€™s assumption (that some coefficients are
zero) is <em>wrong</em> in this case. By forcing some relevant
predictors to zero, it adds too much bias. Ridge, by just
<em>shrinking</em> all of them, finds a better balance.</li>
</ul>
<h3 id="scenario-2-figure-6.9-slide-78">Scenario 2: Figure 6.9 (Slide
78)</h3>
<p>This plot shows the <em>opposite</em> scenario: <strong>only 2 out of
45 predictors are truly related</strong> (a â€œsparseâ€ model).</p>
<ul>
<li><strong>Result:</strong> <strong>Lasso performs much better</strong>
(its solid purple line has a much lower minimum MSE).</li>
<li><strong>Why:</strong> Lassoâ€™s assumption is <em>correct</em>. It
successfully sets the 43 â€œnoiseâ€ predictors to zero, which dramatically
reduces variance, while correctly keeping the 2 important ones.</li>
</ul>
<h2 id="python-code-understanding-1">Python &amp; Code
Understanding</h2>
<p>The slides donâ€™t contain Python code, but they describe the exact
concepts you would use, primarily in <code>scikit-learn</code>.</p>
<ul>
<li><p><strong>Implementing Ridge &amp; Lasso:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, Lasso, RidgeCV, LassoCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># It&#x27;s crucial to scale data before regularization</span></span><br><span class="line"><span class="comment"># alpha is the same as the Î» (lambda) in your slides</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Ridge ---</span></span><br><span class="line"><span class="comment"># The math for Ridge is a &quot;closed-form solution&quot; (Slide 80)</span></span><br><span class="line"><span class="comment"># ridge_model = make_pipeline(StandardScaler(), Ridge(alpha=1.0))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Lasso ---</span></span><br><span class="line"><span class="comment"># Lasso requires a numerical solver (like coordinate descent)</span></span><br><span class="line"><span class="comment"># lasso_model = make_pipeline(StandardScaler(), Lasso(alpha=0.1))</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>The Soft-Thresholding Formula:</strong> The math from
Slide 80, <span class="math inline">\(\text{sign}(y)(|y| -
\lambda/2)_+\)</span>, is the core operation in the â€œcoordinate descentâ€
algorithm used to solve Lasso. You could write it in Python/Numpy:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">soft_threshold</span>(<span class="params">x, lambda_val</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements the Lasso soft-thresholding formula.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">return</span> np.sign(x) * np.maximum(<span class="number">0</span>, np.<span class="built_in">abs</span>(x) - (lambda_val / <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example:</span></span><br><span class="line"><span class="comment"># ols_coefficient = 1.5</span></span><br><span class="line"><span class="comment"># threshold = 4.0</span></span><br><span class="line"><span class="comment"># lasso_coefficient = soft_threshold(ols_coefficient, threshold) </span></span><br><span class="line"><span class="comment"># print(lasso_coefficient) # Output: 0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ols_coefficient = 3.0</span></span><br><span class="line"><span class="comment"># threshold = 4.0</span></span><br><span class="line"><span class="comment"># lasso_coefficient = soft_threshold(ols_coefficient, threshold) </span></span><br><span class="line"><span class="comment"># print(lasso_coefficient) # Output: 1.0 (it was 3.0, shrunk by 4/2 = 2)</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>Choosing <span class="math inline">\(\lambda\)</span>
(alpha):</strong> Slide 79 says to â€œUse cross validation to determine
which one has better prediction.â€ In <code>scikit-learn</code>, this is
done for you with <code>RidgeCV</code> and <code>LassoCV</code>, which
automatically test a range of <code>alpha</code> values.</p></li>
</ul>
<h2 id="summary-lasso-vs.-ridge">Summary: Lasso vs.Â Ridge</h2>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Ridge (L2)</th>
<th style="text-align: left;">Lasso (L1)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Penalty</strong></td>
<td style="text-align: left;"><span class="math inline">\(L_2\)</span>
norm: <span class="math inline">\(\lambda \sum \beta_j^2\)</span></td>
<td style="text-align: left;"><span class="math inline">\(L_1\)</span>
norm: <span class="math inline">\(\lambda \sum |\beta_j|\)</span></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Coefficient
Shrinkage</strong></td>
<td style="text-align: left;">Proportional; shrinks all coefficients,
but never to <em>exactly</em> zero.</td>
<td style="text-align: left;">Soft-thresholding; can force coefficients
to be <em>exactly</em> zero.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Feature Selection?</strong></td>
<td style="text-align: left;">No</td>
<td style="text-align: left;"><strong>Yes</strong>, this is its main
advantage.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Interpretability</strong></td>
<td style="text-align: left;">Less interpretable (keeps all <span
class="math inline">\(p\)</span> variables).</td>
<td style="text-align: left;">More interpretable (produces a â€œsparseâ€
model with fewer variables).</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Best Used Whenâ€¦</strong></td>
<td style="text-align: left;">â€¦most predictors are useful. (e.g., Slide
76: 45/45 relevant).</td>
<td style="text-align: left;">â€¦many predictors are â€œnoiseâ€ and only a
few are strong. (e.g., Slide 78: 2/45 relevant).</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Computation</strong></td>
<td style="text-align: left;">Has a simple, closed-form solution.</td>
<td style="text-align: left;">Requires numerical optimization (e.g.,
coordinate descent).</td>
</tr>
</tbody>
</table>
<h1 id="shrinkage-methods-ridge-lasso">9. Shrinkage Methods (Ridge &amp;
LASSO)</h1>
<h2 id="summary-of-shrinkage-methods-ridge-lasso">Summary of Shrinkage
Methods (Ridge &amp; LASSO)</h2>
<p>These slides introduce <strong>shrinkage methods</strong>, also known
as <strong>regularization</strong>, a technique used in regression (like
linear regression) to improve model performance. The main idea is to add
a <em>penalty</em> to the modelâ€™s loss function to â€œshrinkâ€ the size of
the coefficients. This helps to reduce model variance and prevent
overfitting, especially when you have many features.</p>
<p>The two main methods discussed are <strong>Ridge Regression</strong>
(<span class="math inline">\(L_2\)</span> penalty) and
<strong>LASSO</strong> (<span class="math inline">\(L_1\)</span>
penalty).</p>
<h2 id="key-mathematical-formulas-1">Key Mathematical Formulas</h2>
<ol type="1">
<li><p><strong>Standard Linear Model:</strong> The problem starts with
the standard linear regression model (from slide 1):</p>
<p><span class="math display">\[
\]</span>$$\mathbf{y} = \mathbf{X}\beta + \epsilon</p>
<p><span class="math display">\[
\]</span>$$ * <span class="math inline">\(\mathbf{y}\)</span> is the
<span class="math inline">\(n \times 1\)</span> vector of observed
outcomes.</p>
<ul>
<li><span class="math inline">\(\mathbf{X}\)</span> is the <span
class="math inline">\(n \times p\)</span> matrix of <span
class="math inline">\(p\)</span> predictor features for <span
class="math inline">\(n\)</span> observations.</li>
<li><span class="math inline">\(\beta\)</span> is the <span
class="math inline">\(p \times 1\)</span> vector of coefficients (what
we want to find).</li>
<li><span class="math inline">\(\epsilon\)</span> is the <span
class="math inline">\(n \times 1\)</span> vector of random errors.</li>
<li>The goal of standard â€œOrdinary Least Squaresâ€ (OLS) regression is to
find the <span class="math inline">\(\beta\)</span> that minimizes the
loss: <span class="math inline">\(\|\mathbf{X}\beta -
\mathbf{y}\|^2_2\)</span>.</li>
</ul></li>
<li><p><strong>LASSO (L1 Regularization):</strong> LASSO (Least Absolute
Shrinkage and Selection Operator) adds a penalty based on the
<em>absolute value</em> of the coefficients (the <span
class="math inline">\(L_1\)</span>-norm). This is the key formula from
slide 1:</p>
<p><span class="math display">\[
\]</span>$$\hat{\beta}(\lambda) \leftarrow \arg \min_{\beta} \left(
|\mathbf{X}\beta - \mathbf{y}|^2_2 + \lambda|\beta|_1 \right)</p>
<p><span class="math display">\[
\]</span>$$ * <span class="math inline">\(\|\beta\|_1 = \sum_{j=1}^{p}
|\beta_j|\)</span></p>
<ul>
<li><span class="math inline">\(\lambda\)</span> (lambda) is the
<strong>tuning parameter</strong> that controls the strength of the
penalty. A larger <span class="math inline">\(\lambda\)</span> means
more shrinkage.</li>
<li><strong>Key Property (Variable Selection):</strong> The <span
class="math inline">\(L_1\)</span> penalty can force some coefficients
(<span class="math inline">\(\beta_j\)</span>) to become <strong>exactly
zero</strong>. This means LASSO simultaneously performs <em>feature
selection</em> by automatically removing irrelevant predictors.</li>
<li><strong>Support (Slide 1):</strong> The question â€œCan it recover the
support of <span class="math inline">\(\beta\)</span>?â€ is asking if
LASSO can correctly identify the set of true non-zero coefficients
(defined as <span class="math inline">\(S := \{j : \beta_j \neq
0\}\)</span>).</li>
</ul></li>
<li><p><strong>Ridge Regression (L2 Regularization):</strong> Ridge
regression (mentioned on slide 2, shown on slide 3) adds a penalty based
on the <em>squared value</em> of the coefficients (the <span
class="math inline">\(L_2\)</span>-norm).</p>
<p><span class="math display">\[
\]</span>$$\hat{\beta}(\lambda) \leftarrow \arg \min_{\beta} \left(
|\mathbf{X}\beta - \mathbf{y}|^2_2 + \lambda|\beta|^2_2 \right)</p>
<p><span class="math display">\[
\]</span>$$ * <span class="math inline">\(\|\beta\|^2_2 = \sum_{j=1}^{p}
\beta_j^2\)</span></p>
<ul>
<li><strong>Key Property (Shrinkage):</strong> The <span
class="math inline">\(L_2\)</span> penalty <em>shrinks</em> coefficients
<em>towards</em> zero but <strong>never</strong> sets them to
<em>exactly</em> zero (unless <span class="math inline">\(\lambda =
\infty\)</span>). It is effective at handling multicollinearity.</li>
</ul></li>
</ol>
<h2 id="important-images-concepts">Important Images &amp; Concepts</h2>
<p>The most important images are the plots from slides 3 and 4. They
illustrate the two most critical concepts: <strong>how to choose <span
class="math inline">\(\lambda\)</span></strong> and <strong>what the
penalty does to the coefficients</strong>.</p>
<h3 id="tuning-parameter-selection-slides-3-4-left-plots">Tuning
Parameter Selection (Slides 3 &amp; 4, Left Plots)</h3>
<ul>
<li><strong>Problem:</strong> How do you find the <em>best</em> value
for <span class="math inline">\(\lambda\)</span>?</li>
<li><strong>Solution:</strong> <strong>Cross-Validation (CV)</strong>.
The slides show 10-fold CV.</li>
<li><strong>What the Plots Show:</strong> The left plots on slides 3 and
4 show the <strong>Cross-Validation Error</strong> (like MSE) for
different values of the penalty.
<ul>
<li>The x-axis represents the penalty strength (either <span
class="math inline">\(\lambda\)</span> itself or a related measure like
the shrinkage ratio <span
class="math inline">\(\|\hat{\beta}_\lambda\|_1 /
\|\hat{\beta}\|_1\)</span>).</li>
<li>The y-axis is the prediction error.</li>
<li>The curve is typically <strong>U-shaped</strong>. The vertical
dashed line marks the <strong>minimum</strong> of this curve. This
minimum point corresponds to the <strong>optimal <span
class="math inline">\(\lambda\)</span></strong>, which provides the best
balance between bias and variance, leading to the best-performing model
on unseen data.</li>
</ul></li>
</ul>
<h3 id="coefficient-paths-slides-3-4-right-plots">Coefficient Paths
(Slides 3 &amp; 4, Right Plots)</h3>
<p>These â€œtraceâ€ plots are crucial for understanding the difference
between Ridge and LASSO. They show how the value of each coefficient
(y-axis) changes as the penalty strength (x-axis) changes.</p>
<ul>
<li><strong>Slide 3 (Ridge):</strong> As <span
class="math inline">\(\lambda\)</span> increases (moving right), all
coefficient values are smoothly shrunk <em>towards</em> zero, but none
of them actually hit zero.</li>
<li><strong>Slide 4 (LASSO):</strong> As the penalty increases (moving
from right to left, as the ratio <span class="math inline">\(s\)</span>
goes from 1.0 to 0.0), you can see coefficients â€œdrop offâ€ and become
<strong>exactly zero</strong> one by one. The model with the optimal
<span class="math inline">\(\lambda\)</span> (vertical line) has
selected only a few non-zero coefficients (the pink and teal lines),
while all the grey lines have been set to zero. This is <em>feature
selection</em> in action.</li>
</ul>
<h2 id="key-discussion-points-slide-2">Key Discussion Points (Slide
2)</h2>
<ul>
<li><strong>Non-linear models:</strong> You can apply these methods to
non-linear models by first creating non-linear features (e.g., <span
class="math inline">\(x_1^2\)</span>, <span
class="math inline">\(x_2^2\)</span>, <span class="math inline">\(x_1
\cdot x_2\)</span>) and then feeding them into a LASSO or Ridge model.
The regularization will then select which of these linear <em>or</em>
non-linear terms are important.</li>
<li><strong>Correlated Features (Multicollinearity):</strong> The
question â€œIf <span class="math inline">\(x_j \approx x_k\)</span>, how
does LASSO behave?â€ is a key weakness of LASSO.
<ul>
<li><strong>LASSO:</strong> Tends to <em>arbitrarily</em> select one of
the correlated features and set the others to zero. This can make the
model unstable.</li>
<li><strong>Ridge:</strong> Tends to shrink the coefficients of
correlated features <em>together</em>, giving them similar (but smaller)
values.</li>
<li><strong>Elastic Net</strong> (not shown) is a hybrid of Ridge and
LASSO that is often used to get the best of both worlds: it can select
groups of correlated variables.</li>
</ul></li>
</ul>
<h2 id="python-code-understanding-using-scikit-learn">Python Code
Understanding (using <code>scikit-learn</code>)</h2>
<p>Here is how you would implement these concepts in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso, Ridge, LassoCV, RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Assume you have your data ---</span></span><br><span class="line"><span class="comment"># X: your feature matrix (e.g., shape 100, 20)</span></span><br><span class="line"><span class="comment"># y: your target vector (e.g., shape 100,)</span></span><br><span class="line"><span class="comment"># X, y = ... load your data ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. It&#x27;s crucial to scale your data before regularization</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Find the optimal lambda (alpha) using Cross-Validation</span></span><br><span class="line"><span class="comment"># scikit-learn uses &#x27;alpha&#x27; instead of &#x27;lambda&#x27; for the tuning parameter.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- For LASSO ---</span></span><br><span class="line"><span class="comment"># LassoCV automatically performs cross-validation (e.g., cv=10)</span></span><br><span class="line"><span class="comment"># to find the best alpha.</span></span><br><span class="line">lasso_cv_model = LassoCV(cv=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">lasso_cv_model.fit(X_scaled, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best alpha (lambda)</span></span><br><span class="line">best_alpha_lasso = lasso_cv_model.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal alpha (lambda) for LASSO: <span class="subst">&#123;best_alpha_lasso&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the final coefficients</span></span><br><span class="line">lasso_coeffs = lasso_cv_model.coef_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;LASSO coefficients: <span class="subst">&#123;lasso_coeffs&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># You will see that many of these are exactly 0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- For Ridge ---</span></span><br><span class="line"><span class="comment"># RidgeCV works similarly. It&#x27;s often good to test alphas on a log scale.</span></span><br><span class="line">ridge_alphas = np.logspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>) <span class="comment"># 100 values from 0.001 to 1000</span></span><br><span class="line">ridge_cv_model = RidgeCV(alphas=ridge_alphas, store_cv_values=<span class="literal">True</span>)</span><br><span class="line">ridge_cv_model.fit(X_scaled, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best alpha (lambda)</span></span><br><span class="line">best_alpha_ridge = ridge_cv_model.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal alpha (lambda) for Ridge: <span class="subst">&#123;best_alpha_ridge&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the final coefficients</span></span><br><span class="line">ridge_coeffs = ridge_cv_model.coef_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ridge coefficients: <span class="subst">&#123;ridge_coeffs&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># You will see these are small, but not exactly zero.</span></span><br></pre></td></tr></table></figure>
<h2 id="bias-variance-tradeoff">Bias-variance tradeoff</h2>
<h2 id="key-mathematical-formulas-concepts">Key Mathematical Formulas
&amp; Concepts</h2>
<h3 id="lasso-sign-consistency">LASSO: Sign Consistency</h3>
<p>This is the â€œidealâ€ scenario for LASSO. Sign consistency means that,
with enough data, the LASSO model not only selects the <em>correct</em>
set of features (it recovers the â€œsupportâ€ <span
class="math inline">\(S\)</span>) but also correctly identifies the
<em>sign</em> (positive or negative) of their coefficients.</p>
<ul>
<li><p><strong>The Goal (Slide 1):</strong></p>
<p><span class="math display">\[
\]</span>$$\text{sign}(\hat{\beta}(\lambda)) = \text{sign}(\beta)</p>
<p><span class="math display">\[
\]</span>$$This means the signs of our <em>estimated</em> coefficients
<span class="math inline">\(\hat{\beta}(\lambda)\)</span> match the
signs of the <em>true</em> underlying coefficients <span
class="math inline">\(\beta\)</span>.</p></li>
<li><p><strong>The â€œIrrepresentable Conditionâ€ (Slide 1):</strong> This
is the mathematical guarantee required for LASSO to achieve sign
consistency.</p>
<p><span class="math display">\[
\]</span>$$|\mathbf{X}_{S<sup>c}</sup>\top \mathbf{X}_S
(\mathbf{X}_S^\top \mathbf{X}<em>S)^{-1}
\text{sign}(\beta_S)|</em>\infty &lt; 1</p>
<p><span class="math display">\[
\]</span>$$ * <strong>Plain English:</strong> This formula is a complex
way of saying: <strong>The irrelevant features (<span
class="math inline">\(\mathbf{X}_{S^c}\)</span>) cannot be too strongly
correlated with the true, relevant features (<span
class="math inline">\(\mathbf{X}_S\)</span>).</strong></p>
<ul>
<li>If an irrelevant feature is very similar (highly correlated) to a
true feature, LASSO can get â€œconfusedâ€ and might pick the wrong one, or
its estimate will be unstable. This condition fails.</li>
</ul></li>
</ul>
<h3 id="ridge-regression-the-bias-variance-tradeoff">Ridge Regression:
The Bias-Variance Tradeoff</h3>
<ul>
<li><p><strong>The Formula (Slide 3):</strong></p>
<p><span class="math display">\[
\]</span>$$\hat{\beta}<em>{\text{ridge}}(\lambda) \leftarrow \arg
\min</em>{\beta} \left( |\mathbf{y} - \mathbf{X}\beta|^2 +
\lambda|\beta|^2 \right)</p>
<p><span class="math display">\[
\]</span>$$<em>(Note: This is the <span
class="math inline">\(L_2\)</span> penalty, so <span
class="math inline">\(\|\beta\|^2 = \sum
\beta_j^2\)</span>)</em></p></li>
<li><p><strong>The Problem it Solves: Collinearity (Slide 2)</strong>
When features are strongly correlated (e.g., <span
class="math inline">\(x_i \approx x_j\)</span>), regular methods
fail:</p>
<ul>
<li><strong>LSE (OLS):</strong> Fails because the matrix <span
class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> is
â€œnon-invertibleâ€ (or singular), so the math for the solution <span
class="math inline">\(\hat{\beta} = (\mathbf{X}^\top
\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\)</span> breaks down.</li>
<li><strong>LASSO:</strong> Fails because the <strong>Irrepresentable
Condition</strong> is violated. LASSO will tend to <em>arbitrarily</em>
pick one of the correlated features and set the others to zero.</li>
</ul></li>
<li><p><strong>The Ridge Solution (Slide 3):</strong></p>
<ol type="1">
<li><strong>Always has a solution:</strong> Adding the <span
class="math inline">\(\lambda\)</span> penalty makes the matrix math
work, even if <span class="math inline">\(\mathbf{X}^\top
\mathbf{X}\)</span> is non-invertible.</li>
<li><strong>Groups variables:</strong> This is the key takeaway. Instead
of arbitrarily picking one feature, <strong>Ridge tends to shrink the
coefficients of collinear variables <em>together</em></strong>.</li>
<li><strong>Bias-Variance Tradeoff:</strong> Ridge <em>introduces
bias</em> into the estimates (they are â€œwrongâ€ on purpose) to
<em>massively reduce variance</em> (they are more stable and less
sensitive to the specific training data). This trade-off usually leads
to a much lower overall error (Mean Squared Error).</li>
</ol></li>
</ul>
<h2 id="important-images-key-takeaways">Important Images &amp; Key
Takeaways</h2>
<ol type="1">
<li><p><strong>Slide 2 (Collinearity Failures):</strong> This is the
most important â€œproblemâ€ slide. It clearly explains <em>why</em> you
canâ€™t always use standard LSE or LASSO. The fact that all three methods
(LSE, LASSO, Forward Selection) fail with strong collinearity motivates
the need for Ridge.</p></li>
<li><p><strong>Slide 3 (Ridge Properties):</strong> This is the most
important â€œsolutionâ€ slide. The two most critical points are:</p>
<ul>
<li><code>Always unique solution for Î» &gt; 0</code></li>
<li><code>Collinear variables tend to be grouped!</code> (This is the
â€œfixâ€ for the problem on Slide 2).</li>
</ul></li>
</ol>
<h2 id="python-code-understanding-2">Python Code Understanding</h2>
<p>Letâ€™s demonstrate the <strong>key difference</strong> (Slide 3) in
how LASSO and Ridge handle collinear features.</p>
<p>We will create two features, <code>x1</code> and <code>x2</code>,
that are nearly identical.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso, Ridge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create a dataset with 2 strongly correlated features</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line"><span class="comment"># x1: a standard feature</span></span><br><span class="line">x1 = np.random.randn(n_samples)</span><br><span class="line"><span class="comment"># x2: almost identical to x1</span></span><br><span class="line">x2 = x1 + <span class="number">0.01</span> * np.random.randn(n_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine into our feature matrix X</span></span><br><span class="line">X = np.c_[x1, x2]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y: The target variable (let&#x27;s say y = 2*x1 + 2*x2)</span></span><br><span class="line">y = <span class="number">2</span> * x1 + <span class="number">2</span> * x2 + np.random.randn(n_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit LASSO (alpha is the same as lambda)</span></span><br><span class="line"><span class="comment"># We use a moderate alpha</span></span><br><span class="line">lasso_model = Lasso(alpha=<span class="number">1.0</span>)</span><br><span class="line">lasso_model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit Ridge (alpha is the same as lambda)</span></span><br><span class="line">ridge_model = Ridge(alpha=<span class="number">1.0</span>)</span><br><span class="line">ridge_model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Compare the coefficients</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- Results for Correlated Features ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;True Coefficients: [2.0, 2.0]&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;LASSO Coefficients: <span class="subst">&#123;np.<span class="built_in">round</span>(lasso_model.coef_, <span class="number">2</span>)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ridge Coefficients: <span class="subst">&#123;np.<span class="built_in">round</span>(ridge_model.coef_, <span class="number">2</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="example-output">Example Output:</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--- Results for Correlated Features ---</span><br><span class="line">True Coefficients: [2.0, 2.0]</span><br><span class="line">LASSO Coefficients: [3.89 0.  ]</span><br><span class="line">Ridge Coefficients: [1.95 1.94]</span><br></pre></td></tr></table></figure>
<h3 id="code-explanation">Code Explanation:</h3>
<ul>
<li><strong>LASSO:</strong> As predicted by the slides, LASSO failed to
find the true model. It <em>arbitrarily</em> picked <code>x1</code>,
gave it a large coefficient, and <strong>set <code>x2</code> to
zero</strong>. This is unstable and not what we wanted.</li>
<li><strong>Ridge:</strong> As predicted by Slide 3, Ridge handled the
collinearity perfectly. It identified that both <code>x1</code> and
<code>x2</code> were important and <strong>â€œgroupedâ€ them</strong> by
assigning them nearly identical, stable coefficients (1.95 and 1.94),
which are very close to the true values of 2.0.</li>
</ul>
<h1 id="elastic-net">10. Elastic Net</h1>
<h2 id="overall-summary">Overall Summary</h2>
<p>These slides introduce <strong>Elastic Net</strong>, a modern
regression method that solves the major weaknesses of its two
predecessors, <strong>Ridge</strong> and <strong>LASSO</strong>
regression.</p>
<ul>
<li><strong>Ridge</strong> is good for <strong>collinearity</strong>
(correlated features) but canâ€™t do <strong>variable selection</strong>
(it canâ€™t set any featureâ€™s coefficient to <em>exactly</em> zero).</li>
<li><strong>LASSO</strong> is good for <strong>variable
selection</strong> (it creates <em>sparse</em> models by setting
coefficients to zero) but behaves <strong>unstably</strong> when
features are correlated (it tends to randomly pick one and discard the
others).</li>
</ul>
<p><strong>Elastic Net</strong> combines the L1 penalty of LASSO and the
L2 penalty of Ridge. The result is a single, flexible model that:</p>
<ol type="1">
<li>Performs <strong>variable selection</strong> (like LASSO).</li>
<li>Handles <strong>correlated features</strong> stably by grouping them
together (like Ridge).</li>
<li>Can select more features than samples (<span class="math inline">\(p
&gt; n\)</span>), which LASSO cannot do.</li>
</ol>
<h3 id="slide-1-the-definition-and-formula-file-...020245.png">Slide 1:
The Definition and Formula (File: <code>...020245.png</code>)</h3>
<p>This slide explains <em>why</em> Elastic Net was created and defines
it <em>mathematically</em>.</p>
<ul>
<li><strong>The Problem:</strong> It states the exact trade-off:
<ul>
<li>â€œRidge regression can handle collinearity, but cannot perform
variable selection;â€</li>
<li>â€œLASSO can perform variable selection, but performs poorly when
collinearity;â€</li>
</ul></li>
<li><strong>The Solution (The Formula):</strong> The core of the method
is this optimization formula: <span
class="math display">\[\hat{\beta}_{eNet}(\lambda, \alpha) \leftarrow
\arg \min_{\beta} \left( \underbrace{\|\mathbf{y} -
\mathbf{X}\beta\|^2}_{\text{Loss}} + \lambda \left(
\underbrace{\alpha\|\beta\|_1}_{\text{L1 Penalty}} +
\underbrace{\frac{1-\alpha}{2}\|\beta\|_2^2}_{\text{L2 Penalty}} \right)
\right)\]</span></li>
<li><strong>Breaking Down the Formula:</strong>
<ul>
<li><strong><span class="math inline">\(\|\mathbf{y} -
\mathbf{X}\beta\|^2\)</span></strong>: This is the standard â€œResidual
Sum of Squaresâ€ (RSS). We want to find coefficients (<span
class="math inline">\(\beta\)</span>) that make the modelâ€™s predictions
(<span class="math inline">\(X\beta\)</span>) as close as possible to
the true values (<span class="math inline">\(y\)</span>).</li>
<li><strong><span class="math inline">\(\lambda\)</span>
(Lambda)</strong>: This is the <strong>master knob</strong> for
<em>total regularization strength</em>. A larger <span
class="math inline">\(\lambda\)</span> means a bigger penalty, which
â€œshrinksâ€ all coefficients more.</li>
<li><strong><span class="math inline">\(\alpha\)</span>
(Alpha)</strong>: This is the <strong>mixing parameter</strong> that
balances L1 and L2. This is the key innovation.
<ul>
<li><strong><span
class="math inline">\(\alpha\|\beta\|_1\)</span></strong>: This is the
<strong>L1 (LASSO)</strong> part. It forces weak coefficients to become
exactly zero, thus selecting variables.</li>
<li><strong><span
class="math inline">\(\frac{1-\alpha}{2}\|\beta\|_2^2\)</span></strong>:
This is the <strong>L2 (Ridge)</strong> part. It shrinks all
coefficients and, crucially, encourages correlated features to have
similar coefficients (the grouping effect).</li>
</ul></li>
</ul></li>
<li><strong>The Special Cases:</strong>
<ul>
<li>If <strong><span class="math inline">\(\alpha = 0\)</span></strong>,
the L1 term vanishes, and the model becomes pure <strong>Ridge
Regression</strong>.</li>
<li>If <strong><span class="math inline">\(\alpha = 1\)</span></strong>,
the L2 term vanishes, and the model becomes pure <strong>LASSO
Regression</strong>.</li>
<li>If <strong><span class="math inline">\(0 &lt; \alpha &lt;
1\)</span></strong>, you get <strong>Elastic Net</strong>, which
â€œencourages grouping of correlated variablesâ€ <em>and</em> â€œcan perform
variable selection.â€</li>
</ul></li>
</ul>
<h3
id="slide-2-the-intuition-and-the-grouping-effect-file-...020249.jpg">Slide
2: The Intuition and The Grouping Effect (File:
<code>...020249.jpg</code>)</h3>
<p>This slide gives you the <em>visual intuition</em> and the
<em>practical proof</em> of why Elastic Net works. It has two parts.</p>
<h4 id="part-1-the-three-graphs-geometric-intuition">Part 1: The Three
Graphs (Geometric Intuition)</h4>
<p>These graphs show the <em>constraint region</em> (the shaded shape)
for each penalty. The model tries to find the best coefficients (<span
class="math inline">\(\theta_{opt}\)</span>), and the final solution
(the green dot) is the first point where the cost function (the blue
ellipses) â€œtouchesâ€ the constraint region.</p>
<ul>
<li><strong>L1 Norm (LASSO):</strong> The region is a
<strong>diamond</strong>. Because of its <strong>sharp corners</strong>,
the ellipses are very likely to hit a corner first. At a corner, one of
the coefficients (e.g., <span class="math inline">\(\theta_1\)</span>)
is zero. This is a visual explanation of how LASSO creates
<strong>sparsity</strong> (variable selection).</li>
<li><strong>L2 Norm (Ridge):</strong> The region is a
<strong>circle</strong>. It has <strong>no corners</strong>. The
ellipses will hit a â€œsmoothâ€ point on the circle, shrinking both
coefficients (<span class="math inline">\(\theta_1\)</span> and <span
class="math inline">\(\theta_2\)</span>) but not setting either to zero.
This is <strong>weight sharing</strong>.</li>
<li><strong>L1 + L2 (Elastic Net):</strong> The region is a
<strong>â€œrounded squareâ€</strong>. Itâ€™s the perfect compromise.
<ul>
<li>It has â€œcornersâ€ (like LASSO) so it can still set coefficients to
zero.</li>
<li>It has â€œcurved edgesâ€ (like Ridge) so itâ€™s more stable and handles
correlated variables by finding a solution on an edge rather than a
single sharp corner.</li>
</ul></li>
</ul>
<h4 id="part-2-the-formula-the-grouping-effect">Part 2: The Formula (The
Grouping Effect)</h4>
<p>The text at the bottom explains Elastic Netâ€™s â€œgrouping effect.â€</p>
<ul>
<li><strong>The Implication:</strong> â€œIf <span
class="math inline">\(x_j \approx x_k\)</span>, then <span
class="math inline">\(\hat{\beta}_j \approx
\hat{\beta}_k\)</span>.â€</li>
<li><strong>Meaning:</strong> If two features (<span
class="math inline">\(x_j\)</span> and <span
class="math inline">\(x_k\)</span>) are highly correlated (their values
are very similar), Elastic Net will force their <em>coefficients</em>
(<span class="math inline">\(\hat{\beta}_j\)</span> and <span
class="math inline">\(\hat{\beta}_k\)</span>) to also be very
similar.</li>
<li><strong>Why this is good:</strong> This is the <em>opposite</em> of
LASSO. LASSO would be unstable and might arbitrarily set <span
class="math inline">\(\hat{\beta}_j\)</span> to a large value and <span
class="math inline">\(\hat{\beta}_k\)</span> to zero. Elastic Net
â€œgroupsâ€ them: it will either keep <em>both</em> in the model with
similar importance, or it will shrink <em>both</em> of them out of the
model together. This is a much more stable and realistic result.</li>
<li><strong>The Warning:</strong> â€œLASSO may be unstable in this case!â€
This directly highlights the problem that Elastic Net solves.</li>
</ul>
<h3 id="slide-3-the-feature-comparison-table-file-...020255.png">Slide
3: The Feature Comparison Table (File: <code>...020255.png</code>)</h3>
<p>This table is your â€œcheat sheetâ€ for choosing the right model. It
compares Ridge, LASSO, and Elastic Net on all their key properties.</p>
<ul>
<li><strong>Penalty:</strong> Shows the L2, L1, and combined
penalties.</li>
<li><strong>Sparsity:</strong> Can the model set coefficients to 0?
<ul>
<li>Ridge: <strong>No âŒ</strong></li>
<li>LASSO: <strong>Yes âœ…</strong></li>
<li>Elastic Net: <strong>Yes âœ…</strong></li>
</ul></li>
<li><strong>Variable Selection:</strong> This is a <em>crucial</em> row.
<ul>
<li>LASSO: <strong>Yes âœ…</strong>, BUT it has a major limitation: if
you have more features than samples (<span class="math inline">\(p &gt;
n\)</span>), LASSO can select <em>at most</em> <span
class="math inline">\(n\)</span> features.</li>
<li>Elastic Net: <strong>Yes âœ…</strong>, and it <strong>can select more
than <span class="math inline">\(n\)</span> variables</strong>. This
makes it the clear choice for â€œwideâ€ data problems (e.g., in genomics,
where <span class="math inline">\(p=20,000\)</span> features and <span
class="math inline">\(n=100\)</span> samples).</li>
</ul></li>
<li><strong>Grouping Effect:</strong> How does it handle correlated
features?
<ul>
<li>Ridge: <strong>Strong âœ…</strong></li>
<li>LASSO: <strong>Weak âŒ</strong> (it â€œpicks oneâ€)</li>
<li>Elastic Net: <strong>Strong âœ…</strong></li>
</ul></li>
<li><strong>Solution Uniqueness:</strong> Is the answer stable?
<ul>
<li>Ridge: <strong>Always âœ…</strong></li>
<li>LASSO: <strong>No âŒ</strong> (not if <span
class="math inline">\(X\)</span> is â€œrank-deficient,â€ e.g., <span
class="math inline">\(p &gt; n\)</span> or correlated features)</li>
<li>Elastic Net: <strong>Always âœ…</strong> (as long as <span
class="math inline">\(\alpha &lt; 1\)</span>, the Ridge component
guarantees a unique, stable solution).</li>
</ul></li>
<li><strong>Use Case:</strong> When should you use each?
<ul>
<li><strong>Ridge:</strong> For prediction, especially with
<strong>multicollinearity</strong>.</li>
<li><strong>LASSO:</strong> For <strong>interpretability</strong> and
creating <strong>sparse models</strong> (when you think only a few
features matter).</li>
<li><strong>Elastic Net:</strong> The best all-arounder. Use it for
<strong>correlated predictors</strong>, when <strong><span
class="math inline">\(p \gg n\)</span></strong>, or when you need both
<strong>sparsity + stability</strong>.</li>
</ul></li>
</ul>
<h3 id="code-understanding-python-scikit-learn">Code Understanding
(Python <code>scikit-learn</code>)</h3>
<p>When you use this in Python, be aware of a common confusion in the
parameter names:</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Concept (from your slides)</th>
<th style="text-align: left;"><code>scikit-learn</code> Parameter</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong><span
class="math inline">\(\lambda\)</span></strong> (Lambda)</td>
<td style="text-align: left;"><code>alpha</code></td>
<td style="text-align: left;">The <strong>overall strength</strong> of
regularization.</td>
</tr>
<tr>
<td style="text-align: left;"><strong><span
class="math inline">\(\alpha\)</span></strong> (Alpha)</td>
<td style="text-align: left;"><code>l1_ratio</code></td>
<td style="text-align: left;">The <strong>mixing parameter</strong>
between L1 and L2.</td>
</tr>
</tbody>
</table>
<p><strong>Example:</strong> An <code>l1_ratio</code> of <code>0</code>
is Ridge. An <code>l1_ratio</code> of <code>1</code> is LASSO. An
<code>l1_ratio</code> of <code>0.5</code> is a 50/50 mix.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet, ElasticNetCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Initialize a specific model</span></span><br><span class="line"><span class="comment"># This uses 0.5 for lambda (slide&#x27;s alpha) and 0.1 for lambda (slide&#x27;s lambda)</span></span><br><span class="line">model = ElasticNet(alpha=<span class="number">0.1</span>, l1_ratio=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. A much better way: Find the best parameters automatically</span></span><br><span class="line"><span class="comment"># This will test l1_ratios of 0.1, 0.5, and 0.9</span></span><br><span class="line"><span class="comment"># and automatically find the best &#x27;alpha&#x27; (strength) for each.</span></span><br><span class="line">cv_model = ElasticNetCV(</span><br><span class="line">    l1_ratio=[<span class="number">.1</span>, <span class="number">.5</span>, <span class="number">.9</span>],</span><br><span class="line">    cv=<span class="number">5</span>  <span class="comment"># 5-fold cross-validation</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit the model to your data (X_train, y_train)</span></span><br><span class="line"><span class="comment"># cv_model.fit(X_train, y_train)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. See the best parameters it found</span></span><br><span class="line"><span class="comment"># print(f&quot;Best l1_ratio (slide&#x27;s alpha): &#123;cv_model.l1_ratio_&#125;&quot;)</span></span><br><span class="line"><span class="comment"># print(f&quot;Best alpha (slide&#x27;s lambda): &#123;cv_model.alpha_&#125;&quot;)</span></span><br></pre></td></tr></table></figure>
<h1 id="high-dimensional-data-analysis">11. High-Dimensional Data
Analysis</h1>
<h2 id="the-core-problem-large-p-small-n">The Core Problem: Large <span
class="math inline">\(p\)</span>, Small <span
class="math inline">\(n\)</span></h2>
<p>The slides introduce the challenge of high-dimensional data, which is
defined by having <strong>many more features (predictors) <span
class="math inline">\(p\)</span> than observations (samples) <span
class="math inline">\(n\)</span></strong>. This is often written as
<strong><span class="math inline">\(p \gg n\)</span></strong>.</p>
<ul>
<li><strong>Example:</strong> Predicting blood pressure (the response
<span class="math inline">\(y\)</span>) using millions of genetic
markers (SNPs) as features <span class="math inline">\(X\)</span>, but
only having data from a few hundred patients.</li>
<li><strong>Troubles:</strong>
<ul>
<li><strong>Overfitting:</strong> Models become â€œtoo flexibleâ€ and learn
the noise in the training data, rather than the true underlying
pattern.</li>
<li><strong>Non-Unique Solution:</strong> When <span
class="math inline">\(p &gt; n\)</span>, the standard least squares
linear regression model doesnâ€™t even have a unique solution.</li>
<li><strong>Misleading Metrics:</strong> This leads to a common symptom:
a very small <strong>training error</strong> (or high <span
class="math inline">\(R^2\)</span>) but a very large <strong>test
error</strong>.</li>
</ul></li>
</ul>
<h2 id="most-important-image-the-overfitting-trap-figure-6.23">Most
Important Image: The Overfitting Trap (Figure 6.23)</h2>
<p>Figure 6.23 (from the first uploaded image) is the most critical
visual for understanding the <em>problem</em>. It shows what happens
when you add features (variables) that are <em>completely unrelated</em>
to the outcome.</p>
<ul>
<li><strong>Left Plot (RÂ²):</strong> The <span
class="math inline">\(R^2\)</span> on the training data increases
towards 1. This <em>looks</em> like a perfect fit.</li>
<li><strong>Center Plot (Training MSE):</strong> The Mean Squared Error
on the <em>training</em> data decreases to 0. This also <em>looks</em>
perfect.</li>
<li><strong>Right Plot (Test MSE):</strong> The Mean Squared Error on
the <em>test</em> data (new, unseen data) explodes. This reveals the
model is garbage and has just memorized the training set.</li>
</ul>
<p>âš ï¸ <strong>This is the key takeaway:</strong> In high dimensions,
<span class="math inline">\(R^2\)</span> and training MSE are
<strong>useless</strong> and <strong>misleading</strong> metrics for
model quality.</p>
<h2 id="the-solution-regularization-model-selection">The Solution:
Regularization &amp; Model Selection</h2>
<p>To combat overfitting, we must use <strong>less flexible
models</strong>. The main strategy is <strong>regularization</strong>
(also called shrinkage), which involves adding a penalty term to the
cost function to â€œshrinkâ€ the model coefficients (<span
class="math inline">\(\beta\)</span>).</p>
<h3 id="mathematical-formulas-python-code">Mathematical Formulas &amp;
Python Code ğŸ</h3>
<p>The standard <strong>Least Squares</strong> cost function you try to
minimize is: <span class="math display">\[\text{RSS} = \sum_{i=1}^n
\left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j\right)^2 \quad
\text{or} \quad \|y - X\beta\|^2_2\]</span> This fails when <span
class="math inline">\(p &gt; n\)</span>. The solutions modify this:</p>
<h4 id="a.-ridge-regression-l_2-penalty">A. Ridge Regression (<span
class="math inline">\(L_2\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> Shrinks all coefficients towards zero, but
never <em>to</em> zero. Itâ€™s good when many features are related to the
outcome.</li>
<li><strong>Math Formula:</strong> <span
class="math display">\[\text{Minimize: } \left( \|y - X\beta\|^2_2 +
\lambda \sum_{j=1}^p \beta_j^2 \right)\]</span>
<ul>
<li>The <span class="math inline">\(\lambda \sum_{j=1}^p
\beta_j^2\)</span> is the <strong><span
class="math inline">\(L_2\)</span> penalty</strong>.</li>
<li><span class="math inline">\(\lambda\)</span> (lambda) is a
<em>tuning parameter</em> that controls the penalty strength. A larger
<span class="math inline">\(\lambda\)</span> means more shrinkage.</li>
</ul></li>
<li><strong>Python (Scikit-learn):</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># alpha is the lambda (Î») tuning parameter</span></span><br><span class="line"><span class="comment"># We find the best alpha using cross-validation</span></span><br><span class="line">ridge_model = Ridge(alpha=<span class="number">1.0</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">ridge_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate using test error (e.g., MSE on test set)</span></span><br><span class="line"><span class="comment"># NOT with training R-squared</span></span><br><span class="line">test_score = ridge_model.score(X_test, y_test) </span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="b.-the-lasso-l_1-penalty">B. The Lasso (<span
class="math inline">\(L_1\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> This is a very important method. The <span
class="math inline">\(L_1\)</span> penalty can force coefficients to be
<strong>exactly zero</strong>. This means Lasso performs
<strong>automatic feature selection</strong>, creating a <em>sparse</em>
model.</li>
<li><strong>Math Formula:</strong> <span
class="math display">\[\text{Minimize: } \left( \|y - X\beta\|^2_2 +
\lambda \sum_{j=1}^p |\beta_j| \right)\]</span>
<ul>
<li>The <span class="math inline">\(\lambda \sum_{j=1}^p
|\beta_j|\)</span> is the <strong><span
class="math inline">\(L_1\)</span> penalty</strong>.</li>
<li>Again, <span class="math inline">\(\lambda\)</span> is the tuning
parameter.</li>
</ul></li>
<li><strong>Python (Scikit-learn):</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"></span><br><span class="line"><span class="comment"># alpha is the lambda (Î») tuning parameter</span></span><br><span class="line">lasso_model = Lasso(alpha=<span class="number">0.1</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">lasso_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The model automatically selects features</span></span><br><span class="line"><span class="comment"># Coefficients that are zero were &#x27;dropped&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(lasso_model.coef_) </span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="c.-other-methods">C. Other Methods</h4>
<p>The slides also mention:</p>
<ul>
<li><strong>Forward Stepwise Selection:</strong> A different approach
where you start with no features and add them one by one, picking the
one that improves the model most (based on a criterion like
cross-validation error).</li>
<li><strong>Principal Components Regression (PCR):</strong> A
dimensionality reduction technique.</li>
</ul>
<h2 id="the-curse-of-dimensionality-figure-6.24">The Curse of
Dimensionality (Figure 6.24)</h2>
<p>This example (Figures 6.24 and its description) shows a more subtle
problem.</p>
<ul>
<li><strong>Setup:</strong> A model with <span
class="math inline">\(n=100\)</span> observations and 20 <em>true</em>
features.</li>
<li><strong>Plots:</strong> They test Lasso by adding more and more
<em>irrelevant</em> features:
<ul>
<li><strong><span class="math inline">\(p=20\)</span> (Left):</strong>
Lasso performs well. The lowest test MSE is found with minimal
regularization.</li>
<li><strong><span class="math inline">\(p=50\)</span> (Center):</strong>
Lasso still works well, but it needs more regularization (a smaller
â€œDegrees of Freedomâ€) to filter out the 30 junk features.</li>
<li><strong><span class="math inline">\(p=2000\)</span>
(Right):</strong> This is the <strong>curse of dimensionality</strong>.
Even with a good method like Lasso, the 1,980 irrelevant features add so
much noise that the model <strong>performs poorly regardless</strong> of
the tuning parameter. The true signal is â€œlost in the noise.â€</li>
</ul></li>
</ul>
<h2 id="summary-cautions-for-p-n">Summary: Cautions for <span
class="math inline">\(p &gt; n\)</span></h2>
<p>The final slide gives the most important rules to follow:</p>
<ol type="1">
<li><strong>Beware Extreme Multicollinearity:</strong> When <span
class="math inline">\(p &gt; n\)</span>, your features are
mathematically guaranteed to be linearly related, which breaks standard
regression.</li>
<li><strong>Donâ€™t Overstate Results:</strong> A model you find (e.g.,
with Lasso) is just <em>one</em> of many potentially good models.</li>
<li><strong>ğŸš« DO NOT USE</strong> training <span
class="math inline">\(R^2\)</span>, <span
class="math inline">\(p\)</span>-values, or training MSE to justify your
model. As Figure 6.23 showed, they are misleading.</li>
<li><strong>âœ… DO USE</strong> <strong>test error</strong> and
<strong>cross-validation error</strong> to choose your model and assess
its performance.</li>
</ol>
<h2 id="the-core-problem-p-gg-n-the-troubles-slide">The Core Problem:
<span class="math inline">\(p \gg n\)</span> (The â€œTroublesâ€ Slide)</h2>
<p>This slide (filename: <code>...020259.png</code>) sets up the entire
problem. The issue isnâ€™t just â€œoverfittingâ€; itâ€™s a fundamental
mathematical breakdown of standard methods.</p>
<ul>
<li><strong>â€œLarge <span class="math inline">\(p\)</span> makes our
linear regression model too flexibleâ€</strong>: This is an
understatement. It leads to a problem called an <strong>underdetermined
system</strong>.</li>
<li><strong>â€œIf <span class="math inline">\(p &gt; n\)</span>, the LSE
is not even uniquely determinedâ€</strong>: This is the most important
technical point.
<ul>
<li><strong>Mathematical Reason:</strong> The standard solution for
Ordinary Least Squares (OLS) is <span class="math inline">\(\hat{\beta}
= (X^T X)^{-1} X^T y\)</span>.</li>
<li><span class="math inline">\(X\)</span> is the data matrix with <span
class="math inline">\(n\)</span> rows (observations) and <span
class="math inline">\(p\)</span> columns (features).</li>
<li>The matrix <span class="math inline">\(X^T X\)</span> has dimensions
<span class="math inline">\(p \times p\)</span>.</li>
<li>When <span class="math inline">\(p &gt; n\)</span>, the <span
class="math inline">\(X^T X\)</span> matrix is
<strong>singular</strong>, which means its determinant is zero and it
<strong>cannot be inverted</strong>. The <span
class="math inline">\((X^T X)^{-1}\)</span> term does not exist.</li>
<li><strong>â€œExtreme multicollinearityâ€</strong> (from slide
<code>...020744.png</code>) is the direct cause. When <span
class="math inline">\(p &gt; n\)</span>, the columns of <span
class="math inline">\(X\)</span> (the features) are <em>guaranteed</em>
to be linearly dependent. There are infinite combinations of the
features that can explain the data.</li>
</ul></li>
</ul>
<h2 id="the-simplest-example-n2-figure-6.22">The Simplest Example: <span
class="math inline">\(n=2\)</span> (Figure 6.22)</h2>
<p>This slide (filename: <code>...020728.png</code>) is the
<em>perfect</em> illustration of the â€œnot uniquely determinedâ€
problem.</p>
<ul>
<li><strong>Left Plot (Low-D):</strong> Many points (<span
class="math inline">\(n\)</span>), only two parameters (<span
class="math inline">\(p=2\)</span>: intercept <span
class="math inline">\(\beta_0\)</span> and slope <span
class="math inline">\(\beta_1\)</span>). The line is a â€œbest fitâ€ that
balances the errors. The training error (RSS) is non-zero.</li>
<li><strong>Right Plot (High-D):</strong> We have <span
class="math inline">\(n=2\)</span> observations and <span
class="math inline">\(p=2\)</span> parameters.
<ul>
<li>You have two equations (one for each point) and two unknowns (<span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>).</li>
<li>The model has <em>exactly</em> enough flexibility to pass
<em>perfectly</em> through both points.</li>
<li>The result is <strong>zero training error</strong>.</li>
<li>This â€œperfectâ€ fit is an illusion. If you got a <em>new</em> data
point, this line would almost certainly be a terrible predictor. This is
the essence of overfitting.</li>
</ul></li>
</ul>
<h2 id="the-consequence-misleading-metrics-figure-6.23">The Consequence:
Misleading Metrics (Figure 6.23)</h2>
<p>This slide (filename: <code>...020730.png</code>) scales up the
problem from <span class="math inline">\(n=2\)</span> to <span
class="math inline">\(n=20\)</span> and shows <em>why</em> you must be
cautious.</p>
<ul>
<li><strong>The Setup:</strong> <span
class="math inline">\(n=20\)</span> observations. We start with 1
feature and add more and more <em>irrelevant, junk</em> features.</li>
<li><strong>Left Plot (<span
class="math inline">\(R^2\)</span>):</strong> The <span
class="math inline">\(R^2\)</span> on the training data steadily
increases towards 1 as we add features. This is because, by pure chance,
each new junk feature can explain a tiny bit more of the noise in the
training set.</li>
<li><strong>Center Plot (Training MSE):</strong> The training error
drops to 0. This is the same as the <span
class="math inline">\(n=2\)</span> plot. Once the number of features
(<span class="math inline">\(p\)</span>) gets close to the number of
observations (<span class="math inline">\(n=20\)</span>), the model can
perfectly fit the 20 data points, even if the features are random
noise.</li>
<li><strong>Right Plot (Test MSE):</strong> This is the â€œtruth.â€ The
<em>actual</em> error on new, unseen data gets worse and worse. By
adding noise features, we are just â€œmemorizingâ€ the training set, and
our modelâ€™s ability to generalize is destroyed.</li>
<li><strong>Key Lesson:</strong> (from slide <code>...020744.png</code>)
This is why you must <strong>â€œAvoid usingâ€¦ <span
class="math inline">\(p\)</span>-values, <span
class="math inline">\(R^2\)</span>, or other traditional measures of
model on training as evidence of good fit.â€</strong> They are guaranteed
to lie to you when <span class="math inline">\(p &gt; n\)</span>.</li>
</ul>
<h2 id="the-solutions-the-deal-with-slide">The Solutions (The â€œDeal
withâ€¦â€ Slide)</h2>
<p>This slide (filename: <code>...020734.png</code>) lists the
strategies to fix this. The core idea is <strong>regularization</strong>
(or shrinkage). We add a â€œpenaltyâ€ to the cost function to stop the
<span class="math inline">\(\beta\)</span> coefficients from getting too
large or too numerous.</p>
<h4 id="a.-ridge-regression-l_2-penalty-1">A. Ridge Regression (<span
class="math inline">\(L_2\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> Keeps all <span
class="math inline">\(p\)</span> features, but shrinks their
coefficients. Itâ€™s excellent for handling multicollinearity.</li>
<li><strong>Math:</strong> <span class="math inline">\(\text{Minimize: }
\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j\right)^2 +
\lambda \sum_{j=1}^p \beta_j^2\)</span>
<ul>
<li>The first part is the standard RSS.</li>
<li>The <span class="math inline">\(\lambda \sum \beta_j^2\)</span> is
the <strong><span class="math inline">\(L_2\)</span> penalty</strong>.
It punishes large coefficient values.</li>
</ul></li>
<li><strong><span class="math inline">\(\lambda\)</span>
(Lambda):</strong> This is the <strong>tuning parameter</strong>.
<ul>
<li>If <span class="math inline">\(\lambda=0\)</span>, itâ€™s just OLS
(which fails).</li>
<li>If <span class="math inline">\(\lambda \to \infty\)</span>, all
<span class="math inline">\(\beta\)</span>â€™s are shrunk to 0.</li>
<li>The right <span class="math inline">\(\lambda\)</span> is chosen via
<strong>cross-validation</strong>.</li>
</ul></li>
</ul>
<h4 id="b.-the-lasso-l_1-penalty-1">B. The Lasso (<span
class="math inline">\(L_1\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> This is often preferred because it
performs <strong>automatic feature selection</strong>. It shrinks many
coefficients to be <strong>exactly zero</strong>.</li>
<li><strong>Math:</strong> <span class="math inline">\(\text{Minimize: }
\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j\right)^2 +
\lambda \sum_{j=1}^p |\beta_j|\)</span>
<ul>
<li>The <span class="math inline">\(\lambda \sum |\beta_j|\)</span> is
the <strong><span class="math inline">\(L_1\)</span> penalty</strong>.
This absolute value penalty is what allows coefficients to become
exactly 0.</li>
</ul></li>
<li><strong>Benefit:</strong> The final model is <em>sparse</em> (e.g.,
it might say â€œout of 2,000 features, only these 15 matterâ€).</li>
</ul>
<h4 id="c.-tuning-parameter-choice-the-real-work">C. Tuning Parameter
Choice (The <em>Real</em> Work)</h4>
<p>How do you pick the best <span
class="math inline">\(\lambda\)</span>? You must use the data you have.
The slides mention this and â€œcross validation errorâ€ (from
<code>...020744.png</code>).</p>
<ul>
<li><strong>Python Code (Scikit-learn):</strong> You donâ€™t just guess
<code>alpha</code> (which is <span
class="math inline">\(\lambda\)</span> in scikit-learn). You use a tool
like <code>LassoCV</code> or <code>GridSearchCV</code> to find the best
one. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a high-dimensional dataset</span></span><br><span class="line">X, y = make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">500</span>, n_informative=<span class="number">10</span>, noise=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># LassoCV automatically performs cross-validation to find the best alpha (lambda)</span></span><br><span class="line"><span class="comment"># cv=10 means 10-fold cross-validation</span></span><br><span class="line">lasso_cv_model = LassoCV(cv=<span class="number">10</span>, random_state=<span class="number">0</span>, max_iter=<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">lasso_cv_model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is the best lambda (alpha) it found:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best alpha (lambda): <span class="subst">&#123;lasso_cv_model.alpha_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can now see the coefficients</span></span><br><span class="line"><span class="comment"># Most of the 500 coefficients will be 0.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of non-zero features: <span class="subst">&#123;np.<span class="built_in">sum</span>(lasso_cv_model.coef_ != <span class="number">0</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="a-final-warning-the-curse-of-dimensionality-figure-6.24">A Final
Warning: The Curse of Dimensionality (Figure 6.24)</h2>
<p>This final set of slides (filenames: <code>...020738.png</code> and
<code>...020741.jpg</code>) provides a crucial, subtle warning:
<strong>Regularization is not magic.</strong></p>
<ul>
<li><strong>The Setup:</strong> <span
class="math inline">\(n=100\)</span> observations. There are <strong>20
real features</strong> that truly affect the response.</li>
<li><strong>The Experiment:</strong> They run Lasso three times, adding
more and more <em>noise</em> features:
<ul>
<li><strong>Left Plot (<span
class="math inline">\(p=20\)</span>):</strong> All 20 features are real.
The lowest test MSE is found with minimal regularization (high â€œDegrees
of Freedom,â€ meaning many non-zero coefficients). This makes sense; you
want to keep all 20 real features.</li>
<li><strong>Center Plot (<span
class="math inline">\(p=50\)</span>):</strong> Now we have 20 real
features + 30 noise features. Lasso still works! The best model is found
with more regularization (fewer â€œDegrees of Freedomâ€). Lasso
successfully â€œzeroed outâ€ many of the 30 noise features.</li>
<li><strong>Right Plot (<span
class="math inline">\(p=2000\)</span>):</strong> This is the
<strong>curse of dimensionality</strong>. We have 20 real features +
1980 noise features. The <em>noise</em> has completely overwhelmed the
<em>signal</em>. <strong>Lasso fails.</strong> The test MSE is high
<em>no matter what</em> tuning parameter you choose. The model cannot
distinguish the 20 real features from the 1980 junk ones.</li>
</ul></li>
</ul>
<p><strong>Final Takeaway:</strong> Even with advanced methods like
Lasso, if your <span class="math inline">\(p \gg n\)</span> problem is
<em>too</em> extreme (i.S. the signal-to-noise ratio is too low), it may
be impossible to build a good predictive model.</p>
<h2 id="the-goal-collaborative-filtering">The Goal: â€œCollaborative
Filteringâ€</h2>
<p>The first slide (<code>...021218.png</code>) uses the term
<strong>Collaborative Filtering</strong>. This is the key concept. The
model â€œcollaboratesâ€ by using the ratings of <em>all</em> users to fill
in the blanks for a <em>single</em> user.</p>
<ul>
<li><strong>How it works:</strong> The model assumes your â€œtasteâ€
(vector <span class="math inline">\(\mathbf{u}_i\)</span>) can be
described as a combination of <span class="math inline">\(r\)</span>
â€œlatent featuresâ€ (e.g., <span class="math inline">\(r=3\)</span>: %
action, % comedy, % drama). It <em>also</em> assumes each movie (vector
<span class="math inline">\(\mathbf{v}_j\)</span>) has a profile on
these same features.</li>
<li>Your predicted rating for a movie is the dot product of your taste
vector and the movieâ€™s feature vector.</li>
<li>The model finds the best â€œtasteâ€ vectors <span
class="math inline">\(\mathbf{U}\)</span> and â€œmovieâ€ vectors <span
class="math inline">\(\mathbf{V}\)</span> that explain all the known
ratings <em>simultaneously</em>. Itâ€™s collaborative because Leeâ€™s
ratings help define the features of â€œBullet Trainâ€ (<span
class="math inline">\(\mathbf{v}_2\)</span>), which in turn helps
predict Yangâ€™s rating for that same movie.</li>
</ul>
<h2 id="the-hard-problem-and-its-2-flavors">The Hard Problem (and its 2
Flavors)</h2>
<p>The second slide (<code>...021222.png</code>) presents the intuitive,
but computationally <em>very</em> hard, way to frame the problem.</p>
<h4 id="detail-1-noise-vs.-no-noise">Detail 1: Noise vs.Â No Noise</h4>
<p>The slide shows <span class="math inline">\(\mathbf{Y} = \mathbf{M} +
\mathbf{E}\)</span>. This is critical. * <span
class="math inline">\(\mathbf{M}\)</span> is the â€œtrue,â€ â€œclean,â€
underlying low-rank matrix of everyoneâ€™s â€œtrueâ€ preferences. * <span
class="math inline">\(\mathbf{E}\)</span> is a matrix of random noise.
(e.g., your true rating is 4.3, but you entered a 4; or you were in a
bad mood and rated a 3). * <span
class="math inline">\(\mathbf{Y}\)</span> is the <em>noisy data</em> we
actually observe.</p>
<p>Because of this noise, we donâ€™t expect to find a matrix <span
class="math inline">\(\mathbf{N}\)</span> that <em>perfectly</em>
matches our data. Instead, we try to find a low-rank <span
class="math inline">\(\mathbf{N}\)</span> that is <em>as close as
possible</em>. This leads to the formula: <span
class="math display">\[\underset{\text{rank}(\mathbf{N}) \le
r}{\text{minimize}} \quad \left\| \mathcal{P}_{\mathcal{O}}(\mathbf{Y} -
\mathbf{N}) \right\|_{\text{F}}^2\]</span> This says: â€œFind a matrix
<span class="math inline">\(\mathbf{N}\)</span> (of rank <span
class="math inline">\(r\)</span> or less) that minimizes the sum of
squared errors <em>only on the ratings we observed</em> (<span
class="math inline">\(\mathcal{O}\)</span>).â€</p>
<h4
id="detail-2-why-is-textrankmathbfn-le-r-a-non-convex-constraint">Detail
2: Why is <span class="math inline">\(\text{rank}(\mathbf{N}) \le
r\)</span> a â€œNon-convex constraintâ€?</h4>
<p>This is the â€œdifficult to optimizeâ€ part. A convex problem is
(simplistically) one with a single valley, making it easy to find the
single lowest point. A non-convex problem has many local valleys, and an
algorithm can get stuck in a â€œpretty goodâ€ valley instead of the â€œbestâ€
one.</p>
<p>The rank constraint is non-convex. For example, the average of two
rank-1 matrices is <em>not</em> necessarily a rank-1 matrix (it could be
rank-2). This lack of a â€œsmooth valleyâ€ property makes the problem
NP-hard.</p>
<h4 id="detail-3-the-number-of-parameters-rd_1-d_2">Detail 3: The Number
of Parameters: <span class="math inline">\(r(d_1 + d_2)\)</span></h4>
<p>The slide asks, â€œhow many entries are needed?â€ The answer is based on
the number of unknown parameters. * A rank-<span
class="math inline">\(r\)</span> matrix <span
class="math inline">\(\mathbf{M}\)</span> can be factored into <span
class="math inline">\(\mathbf{U}\)</span> (which is <span
class="math inline">\(d_1 \times r\)</span>) and <span
class="math inline">\(\mathbf{V}^T\)</span> (which is <span
class="math inline">\(r \times d_2\)</span>). * The number of entries in
<span class="math inline">\(\mathbf{U}\)</span> is <span
class="math inline">\(d_1 \times r\)</span>. * The number of entries in
<span class="math inline">\(\mathbf{V}\)</span> is <span
class="math inline">\(d_2 \times r\)</span>. * Total â€œunknownsâ€ to solve
for: <span class="math inline">\(d_1 r + d_2 r = r(d_1 + d_2)\)</span>.
* This means we must have <em>at least</em> <span
class="math inline">\(r(d_1 + d_2)\)</span> observed ratings to have any
hope of uniquely solving for <span
class="math inline">\(\mathbf{U}\)</span> and <span
class="math inline">\(\mathbf{V}\)</span>. If our number of observations
<span class="math inline">\(|\mathcal{O}|\)</span> is less than this,
the problem is hopelessly underdetermined.</p>
<h2 id="the-magic-solution-convex-relaxation">The â€œMagicâ€ Solution:
Convex Relaxation</h2>
<p>The final slide (<code>...021225.png</code>) presents the
groundbreaking solution from CandÃ¨s and Recht. This solution cleverly
<em>changes the problem</em> to one that is convex and solvable.</p>
<h4
id="detail-1-the-l1-norm-analogy-this-is-the-most-important-concept">Detail
1: The L1-Norm Analogy (This is the most important concept)</h4>
<p>This is the key to understanding <em>why</em> this works.</p>
<ul>
<li><strong>In Vectors (Lasso):</strong>
<ul>
<li><strong>Hard Problem:</strong> Find the <em>sparsest</em> vector
<span class="math inline">\(\beta\)</span> (fewest non-zeros). This is
<span class="math inline">\(L_0\)</span> norm, <span
class="math inline">\(\text{minimize } \|\beta\|_0\)</span>. This is
non-convex.</li>
<li><strong>Easy Problem:</strong> Minimize the <span
class="math inline">\(L_1\)</span> norm, <span
class="math inline">\(\text{minimize } \|\beta\|_1 = \sum
|\beta_j|\)</span>. This is convex, and itâ€™s a â€œrelaxationâ€ that
<em>also</em> produces sparse solutions.</li>
</ul></li>
<li><strong>In Matrices (Matrix Completion):</strong>
<ul>
<li><strong>Hard Problem:</strong> Find the <em>lowest-rank</em> matrix
<span class="math inline">\(\mathbf{X}\)</span>. Rank is the number of
non-zero singular values. This is <span
class="math inline">\(\text{minimize } \text{rank}(\mathbf{X})\)</span>.
This is non-convex.</li>
<li><strong>Easy Problem:</strong> Minimize the <strong>Nuclear
Norm</strong>, <span class="math inline">\(\text{minimize }
\|\mathbf{X}\|_* = \sum \sigma_i(\mathbf{X})\)</span> (where <span
class="math inline">\(\sigma_i\)</span> are the singular values). This
is convex, and itâ€™s the â€œmatrix equivalentâ€ of the <span
class="math inline">\(L_1\)</span> norm. Itâ€™s a relaxation that
<em>also</em> produces low-rank solutions.</li>
</ul></li>
</ul>
<h4 id="detail-2-noiseless-vs.-noisy-again">Detail 2: Noiseless
vs.Â Noisy (Again)</h4>
<p>Notice the <em>constraint</em> in this new problem: <span
class="math display">\[\text{Minimize } \quad \|\mathbf{X}\|_*\]</span>
<span class="math display">\[\text{Subject to } \quad X_{ij} = M_{ij},
\quad (i, j) \in \mathcal{O}\]</span></p>
<p>This formulation is for the <strong>noiseless</strong> case. It
assumes the <span class="math inline">\(M_{ij}\)</span> we observed are
<em>perfectly accurate</em>. It demands that our solution <span
class="math inline">\(\mathbf{X}\)</span> <em>exactly matches</em> the
known ratings. This is different from the optimization problem on the
previous slide, which just tried to get <em>close</em> to the noisy data
<span class="math inline">\(\mathbf{Y}\)</span>.</p>
<p>(In practice, you solve a noisy-aware version that combines both
ideas, but the slide shows the original, â€œexact completionâ€
problem.)</p>
<h4 id="detail-3-the-guarantee-what-the-math-at-the-bottom-means">Detail
3: The Guarantee (What the math at the bottom means)</h4>
<p><span class="math display">\[\text{If } \mathcal{O} \text{ is
randomly sampled and } |\mathcal{O}| \gg r(d_1+d_2)\log(d_1+d_2),
\text{... then the solution is unique and } \mathbf{M}
\text{...}\]</span></p>
<p>This is the punchline. The CandÃ¨s paper <em>proved</em> that if you
have <em>enough</em> (but still very few) <em>randomly</em> sampled
ratings, solving this easy convex problem (minimizing the nuclear norm)
will <em>magically give you the exact, true, low-rank matrix <span
class="math inline">\(\mathbf{M}\)</span></em>.</p>
<ul>
<li><strong><span class="math inline">\(|\mathcal{O}| \gg
r(d_1+d_2)\)</span></strong>: This part makes sense. We need <em>at
least</em> as many observations as our <span
class="math inline">\(r(d_1+d_2)\)</span> degrees of freedom.</li>
<li><strong><span class="math inline">\(\log(d_1+d_2)\)</span></strong>:
This â€œlogâ€ factor is the â€œpriceâ€ we pay for not knowing <em>where</em>
the information is. Itâ€™s an astonishingly small price.</li>
<li><strong>Example:</strong> For a 1,000,000 user x 10,000 movie matrix
(like Netflix) with <span class="math inline">\(r=10\)</span>, you donâ€™t
need <span class="math inline">\(\approx 10^{10}\)</span> ratings. You
need a number closer to <span class="math inline">\(10 \times (10^6 +
10^4) \times \log(\dots)\)</span>, which is <em>dramatically</em>
smaller. This is why this method is practical.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/06/5054C5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/06/5054C5/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-10-06 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-06T21:00:00+08:00">2025-10-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-10-19 22:11:02" itemprop="dateModified" datetime="2025-10-19T22:11:02+08:00">2025-10-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-5</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="resampling">1. Resampling</h1>
<p><strong>Resampling</strong> as a statistical tool to assess the
accuracy of models whose main goal is to estimate the <em>test
error</em> (a modelâ€™s performance on new, unseen data) because the
<em>training error</em> is overly optimistic due to overfitting.</p>
<p><strong>é‡é‡‡æ ·</strong>æ˜¯ä¸€ç§ç»Ÿè®¡å·¥å…·ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå…¶ä¸»è¦ç›®æ ‡æ˜¯ä¼°è®¡<em>æµ‹è¯•è¯¯å·®</em>ï¼ˆæ¨¡å‹åœ¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ä¸Šçš„è¡¨ç°ï¼‰ï¼Œå› ä¸ºç”±äºè¿‡æ‹Ÿåˆå¯¼è‡´<em>è®­ç»ƒè¯¯å·®</em>è¿‡äºä¹è§‚ã€‚</p>
<h2 id="key-concepts">Key Concepts</h2>
<ul>
<li><strong>Resampling:</strong> The process of repeatedly drawing
samples from a dataset. The two main types mentioned are
<strong>Cross-validation</strong> (to estimate model test error) and
<strong>Bootstrap</strong> (to quantify the uncertainty of estimates).
ä»æ•°æ®é›†ä¸­åå¤æŠ½å–æ ·æœ¬çš„è¿‡ç¨‹ã€‚ä¸»è¦æåˆ°çš„ä¸¤ç§ç±»å‹æ˜¯<strong>äº¤å‰éªŒè¯</strong>ï¼ˆç”¨äºä¼°è®¡æ¨¡å‹æµ‹è¯•è¯¯å·®ï¼‰å’Œ<strong>è‡ªä¸¾</strong>ï¼ˆç”¨äºé‡åŒ–ä¼°è®¡çš„ä¸ç¡®å®šæ€§ï¼‰ã€‚</li>
<li><strong>Data Splitting (Ideal Scenario):</strong> In a â€œdata-richâ€
situation, you split your data into three parts:
**åœ¨â€œæ•°æ®ä¸°å¯Œâ€çš„æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥å°†æ•°æ®æ‹†åˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼š
<ol type="1">
<li><strong>Training Data:</strong> Used to fit and train the parameters
of various models.ç”¨äºæ‹Ÿåˆå’Œè®­ç»ƒå„ç§æ¨¡å‹çš„å‚æ•°ã€‚</li>
<li><strong>Validation Data:</strong> Used to assess the trained models,
tune hyperparameters (e.g., choose the polynomial degree), and select
the <em>best</em> model. This helps prevent
overfitting.ç”¨äºè¯„ä¼°å·²è®­ç»ƒçš„æ¨¡å‹ã€è°ƒæ•´è¶…å‚æ•°ï¼ˆä¾‹å¦‚ï¼Œé€‰æ‹©å¤šé¡¹å¼çš„æ¬¡æ•°ï¼‰å¹¶é€‰æ‹©<em>æœ€ä½³</em>æ¨¡å‹ã€‚è¿™æœ‰åŠ©äºé˜²æ­¢è¿‡åº¦æ‹Ÿåˆã€‚</li>
<li><strong>Test Data:</strong> Used <em>only once</em> on the final,
selected model to get an unbiased estimate of its real-world
performance.
åœ¨æœ€ç»ˆé€‰å®šçš„æ¨¡å‹ä¸Šä»…ä½¿ç”¨ä¸€æ¬¡ï¼Œä»¥è·å¾—å…¶å®é™…æ€§èƒ½çš„æ— åä¼°è®¡ã€‚</li>
</ol></li>
<li><strong>Validation vs.Â Test Data:</strong> The slides emphasize this
difference (Slide 7). The <strong>validation set</strong> is part of the
model-building and selection process. The <strong>test set</strong> is
kept separate and is only used for the final report card after all
decisions are
made.<strong>éªŒè¯é›†</strong>æ˜¯æ¨¡å‹æ„å»ºå’Œé€‰æ‹©è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚<strong>æµ‹è¯•é›†</strong>æ˜¯ç‹¬ç«‹çš„ï¼Œä»…åœ¨æ‰€æœ‰å†³ç­–å®Œæˆåç”¨äºæœ€ç»ˆæŠ¥å‘Šã€‚</li>
</ul>
<h2 id="the-validation-set-approach">The Validation Set Approach</h2>
<p>This is the simplest cross-validation
method.è¿™æ˜¯æœ€ç®€å•çš„äº¤å‰éªŒè¯æ–¹æ³•ã€‚</p>
<ol type="1">
<li><strong>Split:</strong> The total dataset is randomly divided into
two parts: a <strong>training set</strong> and a <strong>validation
set</strong> (often a 50/50 or 70/30
split).å°†æ•´ä¸ªæ•°æ®é›†éšæœºåˆ†æˆä¸¤éƒ¨åˆ†ï¼š<strong>è®­ç»ƒé›†</strong>å’Œ<strong>éªŒè¯é›†</strong>ï¼ˆé€šå¸¸ä¸º
50/50 æˆ– 70/30 çš„æ¯”ä¾‹ï¼‰ã€‚</li>
<li><strong>Train:</strong> Various models are fit <em>only</em> on the
<strong>training
set</strong>.å„ç§æ¨¡å‹<em>ä»…</em>åœ¨<strong>è®­ç»ƒé›†</strong>ä¸Šè¿›è¡Œæ‹Ÿåˆã€‚</li>
<li><strong>Validate:</strong> The performance of each trained model is
evaluated using the <strong>validation set</strong>.
ä½¿ç”¨<strong>éªŒè¯é›†</strong>è¯„ä¼°æ¯ä¸ªè®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li><strong>Select:</strong> The model with the best performance (e.g.,
the lowest error) on the validation set is chosen as the final model.
é€‰æ‹©åœ¨éªŒè¯é›†ä¸Šæ€§èƒ½æœ€ä½³ï¼ˆä¾‹å¦‚ï¼Œè¯¯å·®æœ€å°ï¼‰çš„æ¨¡å‹ä½œä¸ºæœ€ç»ˆæ¨¡å‹ã€‚</li>
</ol>
<h3 id="important-image-schematic-slide-10">Important Image: Schematic
(Slide 10)</h3>
<p>This diagram clearly shows a set of <span
class="math inline">\(n\)</span> observations being randomly split into
a training set (blue, with observations 7, 22, 13) and a validation set
(beige, with observation 91). The model learns from the blue set and is
tested on the beige set. æ­¤å›¾æ¸…æ™°åœ°å±•ç¤ºäº†ä¸€ç»„ <span
class="math inline">\(n\)</span>
ä¸ªè§‚æµ‹å€¼è¢«éšæœºåˆ†æˆè®­ç»ƒé›†ï¼ˆè“è‰²ï¼Œè§‚æµ‹å€¼ç¼–å·ä¸º
7ã€22ã€13ï¼‰å’ŒéªŒè¯é›†ï¼ˆç±³è‰²ï¼Œè§‚æµ‹å€¼ç¼–å·ä¸º
91ï¼‰ã€‚æ¨¡å‹ä»è“è‰²æ•°æ®é›†è¿›è¡Œå­¦ä¹ ï¼Œå¹¶åœ¨ç±³è‰²æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚</p>
<h2 id="example-auto-data-formulas-code">Example: Auto Data (Formulas
&amp; Code)</h2>
<p>The slides use the <code>Auto</code> dataset to decide the best
polynomial degree to predict <code>mpg</code> from
<code>horsepower</code>.</p>
<h3 id="mathematical-models">Mathematical Models</h3>
<p>The models being compared are polynomials of different degrees. For
example:</p>
<ul>
<li><p><strong>Linear:</strong> <span class="math inline">\(mpg =
\beta_0 + \beta_1(horsepower)\)</span></p></li>
<li><p><strong>Quadratic:</strong> <span class="math inline">\(mpg =
\beta_0 + \beta_1(horsepower) + \beta_2(horsepower)^2\)</span></p></li>
<li><p><strong>Cubic:</strong> <span class="math inline">\(mpg = \beta_0
+ \beta_1(horsepower) + \beta_2(horsepower)^2 +
\beta_3(horsepower)^3\)</span></p></li>
<li><p><strong>çº¿æ€§</strong>ï¼š<span class="math inline">\(mpg = \beta_0
+ \beta_1(é©¬åŠ›)\)</span></p></li>
<li><p><strong>äºŒæ¬¡</strong>ï¼š<span class="math inline">\(mpg = \beta_0
+ \beta_1(é©¬åŠ›) + \beta_2(é©¬åŠ›)^2\)</span></p></li>
<li><p><strong>ä¸‰æ¬¡</strong>ï¼š<span class="math inline">\(mpg = \beta_0
+ \beta_1(é©¬åŠ›) + \beta_2(é©¬åŠ›)^2 + \beta_3(é©¬åŠ›)^3\)</span></p></li>
</ul>
<p>The performance metric used is the <strong>Mean Squared Error
(MSE)</strong> on the validation set:
ä½¿ç”¨çš„æ€§èƒ½æŒ‡æ ‡æ˜¯éªŒè¯é›†ä¸Šçš„<strong>å‡æ–¹è¯¯å·® (MSE)</strong>ï¼š <span
class="math display">\[MSE_{val} = \frac{1}{n_{val}} \sum_{i \in val}
(y_i - \hat{f}(x_i))^2\]</span> where <span
class="math inline">\(n_{val}\)</span> is the number of observations in
the validation set, <span class="math inline">\(y_i\)</span> is the true
<code>mpg</code> value, and <span
class="math inline">\(\hat{f}(x_i)\)</span> is the modelâ€™s prediction
for the <span class="math inline">\(i\)</span>-th observation in the
validation set. å…¶ä¸­ <span class="math inline">\(n_{val}\)</span>
æ˜¯éªŒè¯é›†ä¸­çš„è§‚æµ‹å€¼æ•°é‡ï¼Œ <span class="math inline">\(y_i\)</span>
æ˜¯çœŸå®çš„ <code>mpg</code> å€¼ï¼Œ<span
class="math inline">\(\hat{f}(x_i)\)</span> æ˜¯æ¨¡å‹å¯¹éªŒè¯é›†ä¸­ç¬¬ <span
class="math inline">\(i\)</span> ä¸ªè§‚æµ‹å€¼çš„é¢„æµ‹ã€‚ ### Important Image:
Polynomial Fits (Slide 8) å¤šé¡¹å¼æ‹Ÿåˆï¼ˆå¹»ç¯ç‰‡ 8ï¼‰</p>
<p>This plot is crucial. It shows the <code>Auto</code> data with linear
(red), quadratic (green), and cubic (blue) regression lines. * The
<strong>linear fit</strong> is clearly poor. * The <strong>quadratic and
cubic fits</strong> follow the dataâ€™s curve much better. * The inset box
shows the MSE calculated on the <em>full dataset</em> (this is training
MSE): * Linear MSE: ~26.42 * Quadratic MSE: ~21.60 * Cubic MSE: ~21.51
This suggests a non-linear fit is necessary, but it doesnâ€™t tell us
which one will generalize better.</p>
<p>è¿™å¼ å›¾è‡³å…³é‡è¦ã€‚å®ƒç”¨çº¿æ€§ï¼ˆçº¢è‰²ï¼‰ã€äºŒæ¬¡ï¼ˆç»¿è‰²ï¼‰å’Œä¸‰æ¬¡ï¼ˆè“è‰²ï¼‰å›å½’çº¿å±•ç¤ºäº†
<code>Auto</code> æ•°æ®ã€‚ * <strong>çº¿æ€§æ‹Ÿåˆ</strong> æ˜æ˜¾è¾ƒå·®ã€‚ *
<strong>äºŒæ¬¡å’Œä¸‰æ¬¡æ‹Ÿåˆ</strong> æ›´èƒ½è´´åˆæ•°æ®æ›²çº¿ã€‚ * æ’å›¾æ˜¾ç¤ºäº†åŸºäº
<em>å®Œæ•´æ•°æ®é›†</em> è®¡ç®—çš„å‡æ–¹è¯¯å·®ï¼ˆè¿™æ˜¯è®­ç»ƒå‡æ–¹è¯¯å·®ï¼‰ï¼š *
çº¿æ€§å‡æ–¹è¯¯å·®ï¼š~26.42 * äºŒæ¬¡å‡æ–¹è¯¯å·®ï¼š~21.60 * ä¸‰æ¬¡å‡æ–¹è¯¯å·®ï¼š~21.51
è¿™è¡¨æ˜éçº¿æ€§æ‹Ÿåˆæ˜¯å¿…è¦çš„ï¼Œä½†å®ƒå¹¶æ²¡æœ‰å‘Šè¯‰æˆ‘ä»¬å“ªç§æ‹Ÿåˆæ–¹å¼çš„æ³›åŒ–æ•ˆæœæ›´å¥½ã€‚
### Code Analysis</p>
<p>The slides show two different approaches in code:</p>
<p><strong>1. Python Code (Slide 9): Model Selection
Criteria</strong></p>
<ul>
<li><strong>What it does:</strong> This Python code (using
<code>pandas</code> and <code>statsmodels</code>) does <em>not</em>
implement the validation set approach. Instead, it fits polynomial
models (degrees 1 through 5) to the <em>entire</em> dataset.</li>
<li><strong>How it works:</strong> It calculates statistical criteria
like <strong>BIC</strong>, <strong>Mallowâ€™s <span
class="math inline">\(C_p\)</span></strong>, and <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>. These are mathematical
adjustments to the training error that <em>estimate</em> the test error
without needing a validation set.
<strong>å®ƒè®¡ç®—ç»Ÿè®¡æ ‡å‡†ï¼Œä¾‹å¦‚</strong>BIC<strong>ã€</strong>Mallow çš„
<span class="math inline">\(C_p\)</span>** å’Œ<strong>è°ƒæ•´åçš„ <span
class="math inline">\(R^2\)</span></strong>ã€‚è¿™äº›æ˜¯å¯¹è®­ç»ƒè¯¯å·®çš„æ•°å­¦è°ƒæ•´ï¼Œæ— éœ€éªŒè¯é›†å³å¯<em>ä¼°ç®—</em>æµ‹è¯•è¯¯å·®ã€‚</li>
<li><strong>Key line (logic):</strong> <code>sm.OLS(y, X).fit()</code>
is used to fit the model, and then metrics like <code>model.bic</code>
and <code>model.rsquared_adj</code> are extracted.</li>
<li><strong>Result:</strong> The table shows that the model with
<code>[horsepower, horsepower2]</code> (quadratic) has the lowest BIC
and <span class="math inline">\(C_p\)</span> values, suggesting itâ€™s the
best model according to these criteria.</li>
<li><strong>ç»“æœï¼š</strong>è¡¨æ ¼æ˜¾ç¤ºï¼Œå¸¦æœ‰
<code>[é©¬åŠ›, é©¬åŠ›2]</code>ï¼ˆäºŒæ¬¡å‡½æ•°ï¼‰çš„æ¨¡å‹å…·æœ‰æœ€ä½çš„ BIC å’Œ <span
class="math inline">\(C_p\)</span>
å€¼ï¼Œè¿™è¡¨æ˜æ ¹æ®è¿™äº›æ ‡å‡†ï¼Œå®ƒæ˜¯æœ€ä½³æ¨¡å‹ã€‚</li>
</ul>
<p><strong>2. R Code (Slides 14 &amp; 15): The Validation Set
Approach</strong></p>
<ul>
<li><strong>What it does:</strong> This R code <em>directly
implements</em> the validation set approach described on Slide 13.</li>
<li><strong>How it works:</strong>
<ol type="1">
<li><code>set.seed(...)</code>: Sets a random seed to make the split
reproducible.</li>
<li><code>train=sample(392, 196)</code>: Randomly selects 196 indices
(out of 392) to be the <strong>training set</strong>.</li>
<li><code>lm.fit=lm(mpg~poly(horsepower, 2), ..., subset=train)</code>:
Fits a quadratic model <em>only</em> using the <code>train</code>
data.</li>
<li><code>mean((mpg-predict(lm.fit,Auto))[-train]^2)</code>: This is the
key calculation.
<ul>
<li><code>predict(lm.fit, Auto)</code>: Predicts <code>mpg</code> for
<em>all</em> data.</li>
<li><code>[-train]</code>: Selects only the predictions for the
<strong>validation set</strong> (the data <em>not</em> in
<code>train</code>).</li>
<li><code>mean(...)</code>: Calculates the <strong>MSE on the validation
set</strong>.</li>
</ul></li>
</ol></li>
<li><strong>Result:</strong> The code is run three times with different
seeds (1, 2022, 1997).
<ul>
<li><strong>Seed 1:</strong> Quadratic MSE (18.71) is lowest.</li>
<li><strong>Seed 2022:</strong> Quadratic MSE (19.70) is lowest.</li>
<li><strong>Seed 1997:</strong> Quadratic MSE (19.08) is lowest.</li>
</ul></li>
<li><strong>Main Takeaway:</strong> In all random splits, the
<strong>quadratic model gives the lowest validation set MSE</strong>.
This provides evidence that the quadratic model is the best choice for
generalizing to new data. The fact that the MSE values change with each
seed also highlights a key <em>disadvantage</em> of this simple method:
the results can be variable depending on the random split.
<strong>ä¸»è¦ç»“è®º</strong>ï¼šåœ¨æ‰€æœ‰éšæœºæ‹†åˆ†ä¸­ï¼Œ**äºŒæ¬¡æ¨¡å‹çš„éªŒè¯é›† MSE
æœ€ä½ã€‚è¿™è¯æ˜äº†äºŒæ¬¡æ¨¡å‹æ˜¯æ¨å¹¿åˆ°æ–°æ•°æ®çš„æœ€ä½³é€‰æ‹©ã€‚MSE
å€¼éšæ¯ä¸ªç§å­å˜åŒ–çš„äº‹å®ä¹Ÿå‡¸æ˜¾äº†è¿™ç§ç®€å•æ–¹æ³•çš„ä¸€ä¸ªå…³é”®<em>ç¼ºç‚¹</em>ï¼šç»“æœå¯èƒ½ä¼šå› éšæœºæ‹†åˆ†è€Œå˜åŒ–ã€‚</li>
</ul>
<h1 id="the-validation-set-approach-éªŒè¯é›†æ–¹æ³•">2. The Validation Set
Approach éªŒè¯é›†æ–¹æ³•</h1>
<p>This method is a simple way to estimate a modelâ€™s performance on new,
unseen data (the â€œtest errorâ€).
è¿™ç§æ–¹æ³•æ˜¯ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ï¼ˆâ€œæµ‹è¯•è¯¯å·®â€ï¼‰ä¸Šçš„æ€§èƒ½ã€‚
The core idea is to <strong>randomly split</strong> your available data
into two parts:
å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†å¯ç”¨æ•°æ®<strong>éšæœºæ‹†åˆ†</strong>ä¸ºä¸¤éƒ¨åˆ†ï¼š 1.
<strong>Training Set:</strong> Used to fit (or â€œtrainâ€) your model.
ç”¨äºæ‹Ÿåˆï¼ˆæˆ–â€œè®­ç»ƒâ€ï¼‰æ¨¡å‹ã€‚ 2. <strong>Validation Set (or Test
Set):</strong> Used to evaluate the trained modelâ€™s performance. You
calculate the error (like Mean Squared Error) on this set.
ç”¨äºè¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹æ€§èƒ½ã€‚è®¡ç®—æ­¤é›†åˆçš„è¯¯å·®ï¼ˆä¾‹å¦‚å‡æ–¹è¯¯å·®ï¼‰ã€‚</p>
<h3 id="python-code-explained-slide-1">Python Code Explained (Slide
1)</h3>
<p>The first slide shows a Python example using the <code>Auto</code>
dataset to predict <code>mpg</code> from <code>horsepower</code>.</p>
<ol type="1">
<li><strong>Setup &amp; Data Loading:</strong>
<ul>
<li><code>import</code> statements load libraries like
<code>pandas</code> (for data),
<code>sklearn.model_selection.train_test_split</code> (the key function
for this method), and
<code>sklearn.linear_model.LinearRegression</code>.</li>
<li><code>Auto = pd.read_csv(...)</code> loads the data.</li>
<li><code>X = Auto['horsepower'].values</code> and
<code>y = Auto['mpg'].values</code> select the variables of
interest.</li>
</ul></li>
<li><strong>The Split:</strong>
<ul>
<li><code>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=007)</code></li>
<li>This is the <strong>most important line</strong> for this method. It
splits the data <code>X</code> and <code>y</code> into training and
testing (validation) sets.</li>
<li><code>train_size=0.5</code> means 50% of the data is for training
and 50% is for validation.</li>
<li><code>random_state=007</code> ensures the split is â€œrandomâ€ but
â€œreproducibleâ€ (using the same seed <code>007</code> will always produce
the same split).</li>
</ul></li>
<li><strong>Model Fitting &amp; Evaluation:</strong>
<ul>
<li>The code fits three different polynomial models, but it <strong>only
uses the training data</strong> (<code>X_train</code>,
<code>y_train</code>) to do so.</li>
<li><strong>Linear (Degree 1):</strong> A simple
<code>LinearRegression</code>.</li>
<li><strong>Quadratic (Degree 2):</strong> Uses
<code>PolynomialFeatures(2)</code> to create <span
class="math inline">\(x\)</span> and <span
class="math inline">\(x^2\)</span> terms, then fits a linear model to
them.</li>
<li><strong>Cubic (Degree 3):</strong> Uses
<code>PolynomialFeatures(3)</code> to create <span
class="math inline">\(x\)</span>, <span
class="math inline">\(x^2\)</span>, and <span
class="math inline">\(x^3\)</span> terms.</li>
<li>It then calculates the <strong>Mean Squared Error (MSE)</strong> for
all three models using the <strong>test data</strong>
(<code>X_test</code>, <code>y_test</code>).</li>
</ul></li>
<li><strong>Results (from the text on the slide):</strong>
<ul>
<li><strong>Linear MSE:</strong> <span class="math inline">\(\approx
23.3\)</span></li>
<li><strong>Quadratic MSE:</strong> <span class="math inline">\(\approx
19.4\)</span></li>
<li><strong>Cubic MSE:</strong> <span class="math inline">\(\approx
19.4\)</span></li>
<li><strong>Conclusion:</strong> The quadratic model gives a
significantly lower error than the linear model. The cubic model does
not offer any real improvement over the quadratic one.</li>
</ul>
<strong>ç»“æœï¼ˆæ¥è‡ªå¹»ç¯ç‰‡ä¸Šçš„æ–‡å­—ï¼‰ï¼š</strong>
<ul>
<li><strong>çº¿æ€§å‡æ–¹è¯¯å·®</strong>ï¼šçº¦ 23.3</li>
<li><strong>äºŒæ¬¡å‡æ–¹è¯¯å·®</strong>ï¼šçº¦ 19.4</li>
<li><strong>ä¸‰æ¬¡å‡æ–¹è¯¯å·®</strong>ï¼šçº¦ 19.4</li>
<li><strong>ç»“è®ºï¼š</strong>äºŒæ¬¡æ¨¡å‹çš„è¯¯å·®æ˜¾è‘—ä½äºçº¿æ€§æ¨¡å‹ã€‚ä¸‰æ¬¡æ¨¡å‹ä¸äºŒæ¬¡æ¨¡å‹ç›¸æ¯”å¹¶æ²¡æœ‰ä»»ä½•å®è´¨æ€§çš„æ”¹è¿›ã€‚</li>
</ul></li>
</ol>
<h3 id="key-images-the-problem-with-a-single-split">Key Images: The
Problem with a Single Split</h3>
<p>The most important images are on <strong>slide 9</strong> (labeled
â€œFigureâ€ and â€œPage 20â€).</p>
<ul>
<li><strong>Plot on the Left (Single Split):</strong> This graph shows
the validation MSE for polynomial degrees 1 through 10, based on the
<em>single random split</em> from the R code (slide 2). Just like the
Python example, it shows that the MSE drops sharply from degree 1 to 2,
and then stays relatively low. Based on this <em>one</em> chart, you
might pick degree 2 (quadratic) as the best model.</li>
</ul>
<p>**æ­¤å›¾æ˜¾ç¤ºäº†å¤šé¡¹å¼æ¬¡æ•°ä¸º 1 è‡³ 10 çš„éªŒè¯å‡æ–¹è¯¯å·®ï¼ŒåŸºäº R ä»£ç ï¼ˆå¹»ç¯ç‰‡
2ï¼‰ä¸­çš„<em>å•æ¬¡éšæœºåˆ†å‰²</em>ã€‚ä¸ Python ç¤ºä¾‹ä¸€æ ·ï¼Œå®ƒæ˜¾ç¤º MSE ä» 1 é˜¶åˆ° 2
é˜¶æ€¥å‰§ä¸‹é™ï¼Œç„¶åä¿æŒåœ¨ç›¸å¯¹è¾ƒä½çš„æ°´å¹³ã€‚åŸºäºè¿™å¼ <em>ä¸€</em>å›¾ï¼Œæ‚¨å¯èƒ½ä¼šé€‰æ‹©
2 é˜¶ï¼ˆäºŒæ¬¡ï¼‰ä½œä¸ºæœ€ä½³æ¨¡å‹ã€‚</p>
<ul>
<li><strong>Plot on the Right (Ten Splits):</strong> This is the
<strong>most critical plot</strong>. It shows the results of
<em>repeating the entire process 10 times</em>, each with a new random
split (from R code on slide 3).
<ul>
<li>You can see 10 different error curves.</li>
<li>While they all agree that degree 1 (linear) is bad, they <strong>do
not agree on the best model</strong>. Some curves suggest degree 2 is
best, others suggest 3, 4, or even 6.</li>
</ul>
<strong>è¿™æ˜¯</strong>æœ€å…³é”®çš„å›¾è¡¨**ã€‚å®ƒæ˜¾ç¤ºäº†<em>é‡å¤æ•´ä¸ªè¿‡ç¨‹ 10
æ¬¡</em>çš„ç»“æœï¼Œæ¯æ¬¡éƒ½ä½¿ç”¨æ–°çš„éšæœºåˆ†å‰²ï¼ˆæ¥è‡ªå¹»ç¯ç‰‡ 3 ä¸Šçš„ R ä»£ç ï¼‰ã€‚
<ul>
<li>æ‚¨å¯ä»¥çœ‹åˆ° 10 æ¡ä¸åŒçš„è¯¯å·®æ›²çº¿ã€‚</li>
<li>è™½ç„¶ä»–ä»¬éƒ½è®¤ä¸º 1
é˜¶ï¼ˆçº¿æ€§ï¼‰æ¨¡å‹ä¸å¥½ï¼Œä½†ä»–ä»¬<strong>å¯¹æœ€ä½³æ¨¡å‹çš„çœ‹æ³•å¹¶ä¸ä¸€è‡´</strong>ã€‚æœ‰äº›æ›²çº¿è¡¨æ˜
2 é˜¶æœ€ä½³ï¼Œè€Œå¦ä¸€äº›åˆ™è¡¨æ˜ 3 é˜¶ã€4 é˜¶ç”šè‡³ 6 é˜¶æœ€ä½³ã€‚</li>
</ul></li>
</ul>
<h3 id="summary-of-drawbacks-slides-7-8-9-23-25">Summary of Drawbacks
(Slides 7, 8, 9, 23, 25)</h3>
<p>The slides repeatedly emphasize the two main drawbacks of this simple
validation set approach:</p>
<ol type="1">
<li><p><strong>High Variability é«˜å˜å¼‚æ€§:</strong> The estimated test
MSE can be <strong>highly variable</strong>, depending on which
observations happen to land in the training set versus the validation
set. The plot with 10 curves (slide 9, right) proves this perfectly.
ä¼°è®¡çš„æµ‹è¯• MSE
å¯èƒ½<strong>é«˜åº¦å˜å¼‚</strong>ï¼Œå…·ä½“å–å†³äºå“ªäº›è§‚æµ‹å€¼æ°å¥½è½åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­ã€‚åŒ…å«
10 æ¡æ›²çº¿çš„å›¾è¡¨ï¼ˆå¹»ç¯ç‰‡ 9ï¼Œå³ä¾§ï¼‰å®Œç¾åœ°è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚</p></li>
<li><p><strong>Overestimation of Test Error é«˜ä¼°æµ‹è¯•è¯¯å·®:</strong></p>
<ul>
<li>The model is <strong>only trained on a subset</strong> (e.g., 50%)
of the available data. The validation data is â€œwastedâ€ and not used for
model building.</li>
<li>Statistical methods tend to perform worse when trained on fewer
observations.</li>
<li>Therefore, the model trained on just the training set is likely
<em>worse</em> than a model trained on the <em>entire</em> dataset.</li>
<li>This â€œworseâ€ model will have a <em>higher</em> error rate on the
validation set. This means the validation set MSE <strong>tends to
overestimate</strong> the true test error you would get from a model
trained on all your data.</li>
<li>è¯¥æ¨¡å‹<strong>ä»…åŸºäºå¯ç”¨æ•°æ®çš„å­é›†</strong>ï¼ˆä¾‹å¦‚
50%ï¼‰è¿›è¡Œè®­ç»ƒã€‚éªŒè¯æ•°æ®è¢«â€œæµªè´¹â€äº†ï¼Œå¹¶æœªç”¨äºæ¨¡å‹æ„å»ºã€‚</li>
<li>ç»Ÿè®¡æ–¹æ³•åœ¨è¾ƒå°‘çš„è§‚æµ‹å€¼ä¸Šè¿›è¡Œè®­ç»ƒæ—¶å¾€å¾€è¡¨ç°è¾ƒå·®ã€‚</li>
<li>å› æ­¤ï¼Œä»…åŸºäºè®­ç»ƒé›†è®­ç»ƒçš„æ¨¡å‹å¯èƒ½æ¯”åŸºäº<em>æ•´ä¸ª</em>æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹<em>æ›´å·®</em>ã€‚</li>
<li>è¿™ä¸ªâ€œæ›´å·®â€çš„æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„é”™è¯¯ç‡ä¼šæ›´é«˜ã€‚è¿™æ„å‘³ç€éªŒè¯é›†çš„ MSE
<strong>å€¾å‘äºé«˜ä¼°</strong>åŸºäºæ‰€æœ‰æ•°æ®è®­ç»ƒçš„æ¨¡å‹çš„çœŸå®æµ‹è¯•è¯¯å·®ã€‚</li>
</ul></li>
</ol>
<h2 id="cross-validation-the-solution-äº¤å‰éªŒè¯è§£å†³æ–¹æ¡ˆ">3.
Cross-Validation: The Solution äº¤å‰éªŒè¯ï¼šè§£å†³æ–¹æ¡ˆ</h2>
<p>The slides introduce <strong>Cross-Validation (CV)</strong> as the
method to overcome these drawbacks. The core idea is to use <em>all</em>
data points for both training and validation, just at different times.
<strong>äº¤å‰éªŒè¯
(CV)</strong>ï¼Œä»¥æ­¤æ¥å…‹æœè¿™äº›ç¼ºç‚¹ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†<em>æ‰€æœ‰</em>æ•°æ®ç‚¹ç”¨äºè®­ç»ƒå’ŒéªŒè¯ï¼Œåªæ˜¯ä½¿ç”¨çš„æ—¶é—´ä¸åŒã€‚</p>
<h3
id="leave-one-out-cross-validation-loocv-ç•™ä¸€æ³•äº¤å‰éªŒè¯-loocv">Leave-One-Out
Cross-Validation (LOOCV) ç•™ä¸€æ³•äº¤å‰éªŒè¯ (LOOCV)</h3>
<p>This is the first type of CV introduced (slide 10, page 26). For a
dataset with <span class="math inline">\(n\)</span> data points:</p>
<ol type="1">
<li><strong>Hold out</strong> the 1st data point (this is your
validation set).
<strong>ä¿ç•™</strong>ç¬¬ä¸€ä¸ªæ•°æ®ç‚¹ï¼ˆè¿™æ˜¯ä½ çš„éªŒè¯é›†ï¼‰ã€‚</li>
<li><strong>Train</strong> the model on the <em>other <span
class="math inline">\(n-1\)</span> data points</em>. ä½¿ç”¨<em>å…¶ä»– <span
class="math inline">\(n-1\)</span>
ä¸ªæ•°æ®ç‚¹</em><strong>è®­ç»ƒ</strong>æ¨¡å‹ã€‚</li>
<li><strong>Calculate</strong> the error (e.g., <span
class="math inline">\(\text{MSE}_1\)</span>) using only that 1st
held-out point. ä»…ä½¿ç”¨ç¬¬ä¸€ä¸ªä¿ç•™ç‚¹<strong>è®¡ç®—</strong>è¯¯å·®ï¼ˆä¾‹å¦‚ï¼Œ<span
class="math inline">\(\text{MSE}_1\)</span>ï¼‰ã€‚</li>
<li><strong>Repeat</strong> this <span class="math inline">\(n\)</span>
times, holding out the 2nd point, then the 3rd, and so on, until every
point has been used as the validation set exactly once.
<strong>é‡å¤</strong>æ­¤æ“ä½œ <span class="math inline">\(n\)</span>
æ¬¡ï¼Œä¿ç•™ç¬¬äºŒä¸ªç‚¹ï¼Œç„¶åæ˜¯ç¬¬ä¸‰ä¸ªç‚¹ï¼Œä¾æ­¤ç±»æ¨ï¼Œç›´åˆ°æ¯ä¸ªç‚¹éƒ½ä½œä¸ºéªŒè¯é›†ä½¿ç”¨ä¸€æ¬¡ã€‚</li>
<li>Your final test error estimate is the <strong>average of all <span
class="math inline">\(n\)</span> errors</strong>.
æœ€ç»ˆçš„æµ‹è¯•è¯¯å·®ä¼°è®¡æ˜¯<strong>æ‰€æœ‰ <span class="math inline">\(n\)</span>
ä¸ªè¯¯å·®çš„å¹³å‡å€¼</strong>ã€‚</li>
</ol>
<h3 id="key-formula-from-slide-10">Key Formula (from Slide 10)</h3>
<p>The formula for the <span class="math inline">\(n\)</span>-fold LOOCV
error estimate is: <span class="math inline">\(n\)</span> å€ LOOCV
è¯¯å·®ä¼°è®¡å…¬å¼ä¸ºï¼š <span class="math display">\[\text{CV}_{(n)} =
\frac{1}{n} \sum_{i=1}^{n} \text{MSE}_i\]</span></p>
<p>Where: * <span class="math inline">\(n\)</span> is the total number
of data points. æ˜¯æ•°æ®ç‚¹çš„æ€»æ•°ã€‚ * <span
class="math inline">\(\text{MSE}_i\)</span> is the Mean Squared Error
calculated on the <span class="math inline">\(i\)</span>-th data point
when it was held out. æ˜¯ä¿ç•™ç¬¬ <span class="math inline">\(i\)</span>
ä¸ªæ•°æ®ç‚¹æ—¶è®¡ç®—çš„å‡æ–¹è¯¯å·®ã€‚</p>
<h1 id="what-is-loocv-leave-one-out-cross-validation">3.What is LOOCV
(Leave-One-Out Cross Validation)</h1>
<p>Leave-One-Out Cross Validation (LOOCV) is a method for estimating the
test error of a model. For a dataset with <span
class="math inline">\(n\)</span> observations, you: ç•™ä¸€äº¤å‰éªŒè¯ (LOOCV)
æ˜¯ä¸€ç§ä¼°ç®—æ¨¡å‹æµ‹è¯•è¯¯å·®çš„æ–¹æ³•ã€‚å¯¹äºåŒ…å« <span
class="math inline">\(n\)</span> ä¸ªè§‚æµ‹å€¼çš„æ•°æ®é›†ï¼Œæ‚¨éœ€è¦ï¼š</p>
<ol type="1">
<li><strong>Fit the model <span class="math inline">\(n\)</span> times.
å¯¹æ¨¡å‹è¿›è¡Œ <span class="math inline">\(n\)</span> æ¬¡æ‹Ÿåˆ</strong></li>
<li>For each fit <span class="math inline">\(i\)</span> (from <span
class="math inline">\(1\)</span> to <span
class="math inline">\(n\)</span>), you train the model on all data
points <em>except</em> for observation <span
class="math inline">\(i\)</span>. å¯¹äºæ¯ä¸ªæ‹Ÿåˆ <span
class="math inline">\(i\)</span> ä¸ªæ ·æœ¬ï¼ˆä» <span
class="math inline">\(1\)</span> åˆ° <span
class="math inline">\(n\)</span>ï¼‰ï¼Œæ‚¨éœ€è¦åœ¨é™¤è§‚æµ‹å€¼ <span
class="math inline">\(i\)</span> ä¹‹å¤–çš„æ‰€æœ‰æ•°æ®ç‚¹ä¸Šè®­ç»ƒæ¨¡å‹ã€‚</li>
<li>You then use this trained model to make a prediction for the single
observation <span class="math inline">\(i\)</span> that was left out.
ç„¶åï¼Œæ‚¨éœ€è¦ä½¿ç”¨è¿™ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹å¯¹è¢«é—æ¼çš„å•ä¸ªè§‚æµ‹å€¼ <span
class="math inline">\(i\)</span> è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>The final LOOCV error is the average of the <span
class="math inline">\(n\)</span> prediction errors (typically the Mean
Squared Error, or MSE). æœ€ç»ˆçš„ LOOCV è¯¯å·®æ˜¯ <span
class="math inline">\(n\)</span>
ä¸ªé¢„æµ‹è¯¯å·®çš„å¹³å‡å€¼ï¼ˆé€šå¸¸ä¸ºå‡æ–¹è¯¯å·®ï¼Œç®€ç§° MSEï¼‰ã€‚</li>
</ol>
<p>This process is shown visually in the slide titled â€œLOOCVâ€ (slide
27), which is a key image for understanding the concept. <strong>Pros
&amp; Cons (from slide 28):</strong> * <strong>Pro:</strong> It has low
bias because the training set (<span class="math inline">\(n-1\)</span>
samples) is almost identical to the full dataset.ç”±äºè®­ç»ƒé›†ï¼ˆ<span
class="math inline">\(n-1\)</span>
ä¸ªæ ·æœ¬ï¼‰ä¸å®Œæ•´æ•°æ®é›†å‡ ä¹å®Œå…¨ç›¸åŒï¼Œå› æ­¤åå·®è¾ƒä½ã€‚ * <strong>Pro:</strong>
It produces a stable, non-random error estimate (unlike <span
class="math inline">\(k\)</span>-fold CV, which depends on the random
fold assignments). å®ƒèƒ½äº§ç”Ÿç¨³å®šçš„ééšæœºè¯¯å·®ä¼°è®¡ï¼ˆä¸åŒäº k
å€äº¤å‰éªŒè¯ï¼Œåè€…ä¾èµ–äºéšæœºæŠ˜å åˆ†é…ï¼‰ã€‚ * <strong>Con:</strong> It can be
extremely <strong>computationally expensive</strong>, as the model must
be refit <span class="math inline">\(n\)</span> times.
ç”±äºæ¨¡å‹å¿…é¡»é‡æ–°æ‹Ÿåˆ <span class="math inline">\(n\)</span>
æ¬¡ï¼Œè®¡ç®—æˆæœ¬æå…¶é«˜æ˜‚ã€‚ * <strong>Con:</strong> The <span
class="math inline">\(n\)</span> error estimates can be highly
correlated, which can sometimes lead to high variance in the final <span
class="math inline">\(CV\)</span> estimate. è¿™ <span
class="math inline">\(n\)</span> ä¸ªè¯¯å·®ä¼°è®¡å¯èƒ½é«˜åº¦ç›¸å…³ï¼Œæœ‰æ—¶ä¼šå¯¼è‡´æœ€ç»ˆ
<span class="math inline">\(CV\)</span> ä¼°è®¡å€¼å‡ºç°è¾ƒå¤§æ–¹å·®ã€‚</p>
<h2 id="key-mathematical-formulas">Key Mathematical Formulas</h2>
<p>The main challenge of LOOCV (being computationally expensive) has a
very efficient solution for linear models. LOOCV
çš„ä¸»è¦æŒ‘æˆ˜ï¼ˆè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼‰å¯¹äºçº¿æ€§æ¨¡å‹æ¥è¯´ï¼Œæœ‰ä¸€ä¸ªéå¸¸æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<h3 id="the-standard-slow-formula">1. The Standard (Slow) Formula</h3>
<p>As defined on slide 33, the LOOCV estimate of the MSE is:</p>
<p><span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n}
(y_i - \hat{y}_i^{(i)})^2\]</span></p>
<ul>
<li><span class="math inline">\(y_i\)</span> is the true value of the
<span class="math inline">\(i\)</span>-th observation. æ˜¯ç¬¬ <span
class="math inline">\(i\)</span> ä¸ªè§‚æµ‹å€¼çš„çœŸå®å€¼ã€‚</li>
<li><span class="math inline">\(\hat{y}_i^{(i)}\)</span> is the
predicted value for <span class="math inline">\(y_i\)</span> from a
model trained on all data <em>except</em> observation <span
class="math inline">\(i\)</span>. æ˜¯ä½¿ç”¨é™¤è§‚æµ‹å€¼ <span
class="math inline">\(i\)</span> ä¹‹å¤–çš„æ‰€æœ‰æ•°æ®è®­ç»ƒçš„æ¨¡å‹å¯¹ <span
class="math inline">\(y_i\)</span> çš„é¢„æµ‹å€¼ã€‚</li>
</ul>
<p>Calculating <span class="math inline">\(\hat{y}_i^{(i)}\)</span>
requires refitting the model <span class="math inline">\(n\)</span>
times. è®¡ç®— <span class="math inline">\(\hat{y}_i^{(i)}\)</span>
éœ€è¦é‡æ–°æ‹Ÿåˆæ¨¡å‹ <span class="math inline">\(n\)</span> æ¬¡ã€‚</p>
<h3 id="the-shortcut-fast-formula">2. The Shortcut (Fast) Formula</h3>
<p>Slide 34 provides a much simpler formula that <strong>only requires
fitting the model once</strong> on the <em>entire</em> dataset:
<em>åªéœ€å¯¹</em>æ•´ä¸ª*æ•°æ®é›†è¿›è¡Œä¸€æ¬¡æ¨¡å‹æ‹Ÿåˆ**ï¼š</p>
<p><span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n}
\left( \frac{y_i - \hat{y}_i}{1 - h_i} \right)^2\]</span></p>
<ul>
<li><span class="math inline">\(\hat{y}_i\)</span> is the prediction for
<span class="math inline">\(y_i\)</span> from the model trained on
<strong>all <span class="math inline">\(n\)</span> data points</strong>.
æ˜¯ä½¿ç”¨<strong>æ‰€æœ‰ <span class="math inline">\(n\)</span>
ä¸ªæ•°æ®ç‚¹</strong>è®­ç»ƒçš„æ¨¡å‹å¯¹ <span class="math inline">\(y_i\)</span>
çš„é¢„æµ‹å€¼ã€‚</li>
<li><span class="math inline">\(h_i\)</span> is the
<strong>leverage</strong> of the <span
class="math inline">\(i\)</span>-th observation. æ˜¯ç¬¬ <span
class="math inline">\(i\)</span>
ä¸ªè§‚æµ‹å€¼çš„<strong>æ æ†ç‡</strong>ã€‚</li>
</ul>
<h3 id="what-is-leverage-h_i">3. What is Leverage (<span
class="math inline">\(h_i\)</span>)?</h3>
<p>Slide 35 defines leverage:</p>
<ul>
<li><p><strong>Hat Matrix (<span
class="math inline">\(\mathbf{H}\)</span>):</strong> In a linear model,
the fitted values <span class="math inline">\(\hat{\mathbf{y}}\)</span>
are related to the true values <span
class="math inline">\(\mathbf{y}\)</span> by the hat matrix: <span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{H}\mathbf{y}\)</span>.</p></li>
<li><p><strong>Formula:</strong> The hat matrix is defined as <span
class="math inline">\(\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span>.</p></li>
<li><p><strong>Leverage (<span
class="math inline">\(h_i\)</span>):</strong> The leverage for the <span
class="math inline">\(i\)</span>-th observation is simply the <span
class="math inline">\(i\)</span>-th diagonal element of the hat matrix,
<span class="math inline">\(h_{ii}\)</span> (often just written as <span
class="math inline">\(h_i\)</span>).</p>
<ul>
<li><span class="math inline">\(h_i = \mathbf{x}_i^T
(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{x}_i\)</span></li>
</ul></li>
<li><p><strong>Meaning:</strong> Leverage measures how â€œinfluentialâ€ an
observationâ€™s <span class="math inline">\(x_i\)</span> value is in
determining its own predicted value <span
class="math inline">\(\hat{y}_i\)</span>. A high leverage score means
that point has a lot of influence on the modelâ€™s fit.</p></li>
<li><p><strong>å¸½å­çŸ©é˜µ (<span
class="math inline">\(\mathbf{H}\)</span>)ï¼š</strong>åœ¨çº¿æ€§æ¨¡å‹ä¸­ï¼Œæ‹Ÿåˆå€¼
<span class="math inline">\(\hat{\mathbf{y}}\)</span> ä¸çœŸå®å€¼ <span
class="math inline">\(\mathbf{y}\)</span> ä¹‹é—´å­˜åœ¨å¸½å­çŸ©é˜µå…³ç³»ï¼š<span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{H}\mathbf{y}\)</span>ã€‚</p></li>
<li><p><strong>å…¬å¼ï¼š</strong>å¸½å­çŸ©é˜µå®šä¹‰ä¸º <span
class="math inline">\(\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span>ã€‚</p></li>
<li><p><strong>æ æ†ç‡ (<span
class="math inline">\(h_i\)</span>)ï¼š</strong>ç¬¬ <span
class="math inline">\(i\)</span> ä¸ªè§‚æµ‹å€¼çš„æ æ†ç‡å°±æ˜¯å¸½å­çŸ©é˜µçš„ç¬¬ <span
class="math inline">\(i\)</span> ä¸ªå¯¹è§’çº¿å…ƒç´  <span
class="math inline">\(h_{ii}\)</span>ï¼ˆé€šå¸¸å†™ä¸º <span
class="math inline">\(h_i\)</span>ï¼‰ã€‚</p></li>
<li><p><span class="math inline">\(h_i = \mathbf{x}_i^T
(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{x}_i\)</span></p></li>
<li><p><strong>å«ä¹‰ï¼š</strong>æ æ†ç‡è¡¡é‡è§‚æµ‹å€¼çš„ <span
class="math inline">\(x_i\)</span> å€¼å¯¹å…¶è‡ªèº«é¢„æµ‹å€¼ <span
class="math inline">\(\hat{y}_i\)</span>
çš„â€œå½±å“åŠ›â€ã€‚æ æ†ç‡å¾—åˆ†é«˜æ„å‘³ç€è¯¥ç‚¹å¯¹æ¨¡å‹æ‹Ÿåˆæœ‰å¾ˆå¤§å½±å“ã€‚</p></li>
</ul>
<p>This shortcut formula is extremely important because it makes LOOCV
as fast to compute as a single model
fit.è¿™ä¸ªå¿«æ·å…¬å¼éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒä½¿å¾— LOOCV
çš„è®¡ç®—é€Ÿåº¦ä¸å•ä¸ªæ¨¡å‹æ‹Ÿåˆä¸€æ ·å¿«ã€‚</p>
<h2 id="python-code-explained-slide-29">Python Code Explained (Slide
29)</h2>
<p>This slide shows how to use LOOCV to select the best polynomial
degree for predicting <code>mpg</code> from <code>horsepower</code>.</p>
<ol type="1">
<li><strong>Imports:</strong> It imports standard libraries
(<code>pandas</code>, <code>matplotlib</code>) and key modules from
<code>sklearn</code>:
<ul>
<li><code>LinearRegression</code>: The model to be fit.</li>
<li><code>PolynomialFeatures</code>: A tool to create polynomial terms
(e.g., <span class="math inline">\(x, x^2, x^3\)</span>).</li>
<li><code>LeaveOneOut</code>: The LOOCV cross-validation strategy
object.</li>
<li><code>cross_val_score</code>: A function that automatically runs a
cross-validation test.</li>
</ul></li>
<li><strong>Setup:</strong>
<ul>
<li>It loads the <code>Auto.csv</code> data.</li>
<li>It defines <span class="math inline">\(X\)</span>
(<code>horsepower</code>) and <span class="math inline">\(y\)</span>
(<code>mpg</code>).</li>
<li>It creates a <code>LeaveOneOut</code> object:
<code>loo = LeaveOneOut()</code>.</li>
</ul></li>
<li><strong>Looping through Degrees:</strong>
<ul>
<li>The code loops <code>degree</code> from 1 to 10.</li>
<li><strong><code>make_pipeline</code>:</strong> For each degree, it
creates a <code>model</code> using <code>make_pipeline</code>. This
pipeline is a crucial concept:
<ul>
<li>It first runs <code>PolynomialFeatures(degree)</code> to transform
<span class="math inline">\(X\)</span> into <span
class="math inline">\([X, X^2, ..., X^{\text{degree}}]\)</span>.</li>
<li>It then feeds those features into <code>LinearRegression()</code> to
fit the model.</li>
</ul></li>
<li><strong><code>cross_val_score</code>:</strong> This is the most
important line.
<ul>
<li><code>scores = cross_val_score(model, X, y, cv=loo, scoring='neg_mean_squared_error')</code></li>
<li>This function automatically does the <em>entire</em> LOOCV process.
It takes the <code>model</code> (the pipeline), the data <span
class="math inline">\(X\)</span> and <span
class="math inline">\(y\)</span>, and the CV strategy
(<code>cv=loo</code>).</li>
<li><code>sklearn</code>â€™s <code>cross_val_score</code> uses the â€œfastâ€
leverage method internally for linear models, so it doesnâ€™t actually fit
the model <span class="math inline">\(n\)</span> times.</li>
<li>It uses <code>scoring='neg_mean_squared_error'</code> because the
<code>scoring</code> function assumes â€œhigher is better.â€ By calculating
the <em>negative</em> MSE, the best model will have the highest score
(i.e., closest to 0).</li>
</ul></li>
<li><strong>Storing Results:</strong> It calculates the mean of the
scores (which is the <span class="math inline">\(CV_{(n)}\)</span>) and
stores it.</li>
</ul></li>
<li><strong>Visualization:</strong>
<ul>
<li>The code then plots the final <code>cv_errors</code> (after flipping
the sign back to positive) against the <code>degree</code>.</li>
<li>The resulting plot (also on slide 32) shows the test MSE, allowing
you to visually pick the best degree (where the error is
minimized).</li>
<li>ç”Ÿæˆçš„å›¾ï¼ˆä¹Ÿåœ¨å¹»ç¯ç‰‡ 32 ä¸Šï¼‰æ˜¾ç¤ºäº†æµ‹è¯• MSEï¼Œè®©æ‚¨å¯ä»¥ç›´è§‚åœ°é€‰æ‹©æœ€ä½³
degreeï¼ˆè¯¯å·®æœ€å°åŒ–çš„ degreeï¼‰ã€‚</li>
</ul></li>
</ol>
<hr />
<h2 id="important-images">Important Images</h2>
<ul>
<li><p><strong>Slide 27 (<code>.../103628.png</code>):</strong> This is
the <strong>best conceptual image</strong>. It visually demonstrates how
LOOCV splits the data <span class="math inline">\(n\)</span> times, with
each observation getting one turn as the validation set.
<strong>è¿™æ˜¯</strong>æœ€ä½³æ¦‚å¿µå›¾**ã€‚å®ƒç›´è§‚åœ°å±•ç¤ºäº† LOOCV å¦‚ä½•å°†æ•°æ®æ‹†åˆ†
<span class="math inline">\(n\)</span>
æ¬¡ï¼Œæ¯ä¸ªè§‚å¯Ÿå€¼éƒ½ä¼šè¢«æ—‹è½¬ä¸€æ¬¡ä½œä¸ºéªŒè¯é›†ã€‚</p></li>
<li><p><strong>Slide 34 (<code>.../103711.png</code>):</strong> This
slide presents the <strong>most important formula</strong>: the â€œEasy
formulaâ€ or shortcut, <span class="math inline">\(CV_{(n)} = \frac{1}{n}
\sum (\frac{y_i - \hat{y}_i}{1 - h_i})^2\)</span>. This is the key
takeaway for <em>computing</em> LOOCV efficiently in linear models.
<strong>è¿™å¼ å¹»ç¯ç‰‡å±•ç¤ºäº†</strong>æœ€é‡è¦çš„å…¬å¼**ï¼šâ€œç®€å•å…¬å¼â€æˆ–ç®€ç§°ï¼Œ<span
class="math inline">\(CV_{(n)} = \frac{1}{n} \sum (\frac{y_i -
\hat{y}_i}{1 - h_i})^2\)</span>ã€‚è¿™æ˜¯åœ¨çº¿æ€§æ¨¡å‹ä¸­é«˜æ•ˆ<em>è®¡ç®—</em> LOOCV
çš„å…³é”®è¦ç‚¹ã€‚</p></li>
<li><p><strong>Slide 32 (<code>.../103701.jpg</code>):</strong> This is
the <strong>key results image</strong>. It contrasts the LOOCV error
curve (left) with the 10-fold CV error curves (right). It clearly shows
that LOOCV produces a single, stable error curve, while 10-fold CV
results vary slightly each time itâ€™s run due to the random data splits.
<strong>è¿™æ˜¯</strong>å…³é”®ç»“æœå›¾**ã€‚å®ƒå°† LOOCV è¯¯å·®æ›²çº¿ï¼ˆå·¦ï¼‰ä¸ 10 å€ CV
è¯¯å·®æ›²çº¿ï¼ˆå³ï¼‰è¿›è¡Œäº†å¯¹æ¯”ã€‚å®ƒæ¸…æ¥šåœ°è¡¨æ˜ï¼ŒLOOCV
äº§ç”Ÿäº†å•ä¸€ã€ç¨³å®šçš„è¯¯å·®æ›²çº¿ï¼Œè€Œç”±äºæ•°æ®åˆ†å‰²çš„éšæœºæ€§ï¼Œ10 å€ CV
çš„ç»“æœæ¯æ¬¡è¿è¡Œæ—¶éƒ½ä¼šç•¥æœ‰ä¸åŒã€‚</p></li>
</ul>
<h1 id="cross-validation-overview">4. Cross-Validation Overview</h1>
<p>These slides explain <strong>Cross-Validation (CV)</strong>, a method
used to estimate the test error of a model, helping to select the best
level of flexibility (e.g., the best polynomial degree). Itâ€™s an
improvement over a single validation set because it uses all the data
for both training and validation at different times.
è¿™æ˜¯ä¸€ç§ç”¨äºä¼°ç®—æ¨¡å‹æµ‹è¯•è¯¯å·®çš„æ–¹æ³•ï¼Œæœ‰åŠ©äºé€‰æ‹©æœ€ä½³çš„çµæ´»æ€§ï¼ˆä¾‹å¦‚ï¼Œæœ€ä½³å¤šé¡¹å¼æ¬¡æ•°ï¼‰ã€‚å®ƒæ¯”å•ä¸ªéªŒè¯é›†æœ‰æ‰€æ”¹è¿›ï¼Œå› ä¸ºå®ƒåœ¨ä¸åŒæ—¶é—´ä½¿ç”¨æ‰€æœ‰æ•°æ®è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ã€‚</p>
<p>The two main types discussed are <strong>K-fold CV</strong> and
<strong>Leave-One-Out CV (LOOCV)</strong>. ä¸»è¦è®¨è®ºçš„ä¸¤ç§ç±»å‹æ˜¯<strong>K
æŠ˜äº¤å‰éªŒè¯</strong>å’Œ<strong>ç•™ä¸€æ³•äº¤å‰éªŒè¯ (LOOCV)</strong>ã€‚</p>
<h2 id="k-fold-cross-validation-k-æŠ˜äº¤å‰éªŒè¯">K-Fold Cross-Validation K
æŠ˜äº¤å‰éªŒè¯</h2>
<p>This is the most common method.</p>
<h3 id="the-process">The Process</h3>
<p>As shown in the slides, the K-fold CV process is: 1.
<strong>Divide</strong> the dataset randomly into <span
class="math inline">\(K\)</span> non-overlapping groups (or â€œfoldsâ€),
usually of equal size. Common choices are <span
class="math inline">\(K=5\)</span> or <span
class="math inline">\(K=10\)</span>. å°†æ•°æ®é›†éšæœº<strong>åˆ’åˆ†</strong>ä¸º
<span class="math inline">\(K\)</span>
ä¸ªä¸é‡å çš„ç»„ï¼ˆæˆ–â€œæŠ˜â€ï¼‰ï¼Œé€šå¸¸å¤§å°ç›¸ç­‰ã€‚å¸¸è§çš„é€‰æ‹©æ˜¯ <span
class="math inline">\(K=5\)</span> æˆ– <span
class="math inline">\(K=10\)</span>ã€‚ 2. <strong>Iterate <span
class="math inline">\(K\)</span> times</strong>: In each iteration <span
class="math inline">\(i\)</span>, use the <span
class="math inline">\(i\)</span>-th fold as the <strong>validation
set</strong> and all other <span class="math inline">\(K-1\)</span>
folds combined as the <strong>training set</strong>. <strong>è¿­ä»£ <span
class="math inline">\(K\)</span> æ¬¡</strong>ï¼šåœ¨æ¯æ¬¡è¿­ä»£ <span
class="math inline">\(i\)</span> ä¸­ï¼Œä½¿ç”¨ç¬¬ <span
class="math inline">\(i\)</span>
ä¸ªæ ·æœ¬é›†ä½œä¸º<strong>éªŒè¯é›†</strong>ï¼Œå¹¶å°†æ‰€æœ‰å…¶ä»– <span
class="math inline">\(K-1\)</span>
ä¸ªæ ·æœ¬é›†åˆå¹¶ä½œä¸º<strong>è®­ç»ƒé›†</strong>ã€‚ 3. <strong>Calculate</strong>
the Mean Squared Error (<span class="math inline">\(MSE_i\)</span>) on
the validation fold. <strong>è®¡ç®—</strong>éªŒè¯é›†çš„å‡æ–¹è¯¯å·® (<span
class="math inline">\(MSE_i\)</span>)ã€‚ 4. <strong>Average</strong> all
<span class="math inline">\(K\)</span> error estimates to get the final
CV score. <strong>å¹³å‡</strong>æ‰€æœ‰ <span
class="math inline">\(K\)</span> ä¸ªè¯¯å·®ä¼°è®¡å€¼ï¼Œå¾—åˆ°æœ€ç»ˆçš„ CV åˆ†æ•°ã€‚ ###
Key Formula The final K-fold CV error estimate is the average of the
errors from each fold: æœ€ç»ˆçš„ K æŠ˜ CV
è¯¯å·®ä¼°è®¡å€¼æ˜¯æ¯ä¸ªæ ·æœ¬é›†è¯¯å·®çš„å¹³å‡å€¼ï¼š <span
class="math display">\[CV_{(K)} = \frac{1}{K} \sum_{i=1}^{K}
MSE_i\]</span></p>
<h3 id="important-image-the-concept">Important Image: The Concept</h3>
<p>The diagram in slide <code>104145.png</code> is the most important
for understanding the <em>concept</em> of K-fold CV. It shows a dataset
split into 5 folds (<span class="math inline">\(K=5\)</span>). The
process is repeated 5 times, with a different fold (in beige) held out
as the validation set in each run, while the rest (in blue) is used for
training. å®ƒå±•ç¤ºäº†ä¸€ä¸ªè¢«åˆ†æˆ 5 ä¸ªæ ·æœ¬é›† (<span
class="math inline">\(K=5\)</span>) çš„æ•°æ®é›†ã€‚è¯¥è¿‡ç¨‹é‡å¤ 5
æ¬¡ï¼Œæ¯æ¬¡è¿è¡Œéƒ½ä¼šä¿ç•™ä¸€ä¸ªä¸åŒçš„æŠ˜å ï¼ˆç±³è‰²ï¼‰ä½œä¸ºéªŒè¯é›†ï¼Œå…¶ä½™æŠ˜å ï¼ˆè“è‰²ï¼‰ç”¨äºè®­ç»ƒã€‚</p>
<h2 id="leave-one-out-cross-validation-loocv">Leave-One-Out
Cross-Validation (LOOCV)</h2>
<p>LOOCV is just a special case of K-fold CV where <strong><span
class="math inline">\(K = n\)</span></strong> (the total number of
observations). LOOCV åªæ˜¯ K æŠ˜äº¤å‰éªŒè¯çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œå…¶ä¸­ <strong><span
class="math inline">\(K = n\)</span></strong>ï¼ˆè§‚æµ‹å€¼æ€»æ•°ï¼‰ã€‚ * You
create <span class="math inline">\(n\)</span> â€œfolds,â€ each containing
just one data point. åˆ›å»º <span class="math inline">\(n\)</span>
ä¸ªâ€œæŠ˜å â€ï¼Œæ¯ä¸ªæŠ˜å ä»…åŒ…å«ä¸€ä¸ªæ•°æ®ç‚¹ã€‚ * You train the model <span
class="math inline">\(n\)</span> times, each time leaving out a
<em>single</em> different observation and then calculating the error for
that one point. å¯¹æ¨¡å‹è¿›è¡Œ <span class="math inline">\(n\)</span>
æ¬¡è®­ç»ƒï¼Œæ¯æ¬¡éƒ½çœç•¥ä¸€ä¸ªä¸åŒçš„è§‚æµ‹å€¼ï¼Œç„¶åè®¡ç®—è¯¥ç‚¹çš„è¯¯å·®ã€‚</p>
<h3 id="key-formulas">Key Formulas</h3>
<ol type="1">
<li><p><strong>Standard Definition:</strong> The LOOCV error is the
average of the <span class="math inline">\(n\)</span> squared errors:
<span class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N}
e_{[i]}^2\]</span> where <span class="math inline">\(e_{[i]} = y_i -
\hat{y}_{[i]}\)</span> is the prediction error for the <span
class="math inline">\(i\)</span>-th observation, calculated from a model
that was trained on <em>all data except</em> the <span
class="math inline">\(i\)</span>-th observation. This looks
computationally expensive. LOOCV è¯¯å·®æ˜¯ <span
class="math inline">\(n\)</span> ä¸ªå¹³æ–¹è¯¯å·®çš„å¹³å‡å€¼ï¼š <span
class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N}
e_{[i]}^2\]</span> å…¶ä¸­ <span class="math inline">\(e_{[i]} = y_i -
\hat{y}_{[i]}\)</span> æ˜¯ç¬¬ <span class="math inline">\(i\)</span>
ä¸ªè§‚æµ‹å€¼çš„é¢„æµ‹è¯¯å·®ï¼Œè¯¥è¯¯å·®ç”±ä¸€ä¸ªä½¿ç”¨é™¤ç¬¬ <span
class="math inline">\(i\)</span>
ä¸ªè§‚æµ‹å€¼ä»¥å¤–çš„æ‰€æœ‰æ•°æ®è®­ç»ƒçš„æ¨¡å‹è®¡ç®—å¾—å‡ºã€‚è¿™çœ‹èµ·æ¥è®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚</p></li>
<li><p><strong>Fast Computation (for Linear Regression):</strong> A key
point from the slides is that for linear regression, you donâ€™t need to
re-fit the model <span class="math inline">\(N\)</span> times. You can
fit the model <em>once</em> on all <span
class="math inline">\(N\)</span> data points and use the following
shortcut: <span class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N}
\left( \frac{e_i}{1 - h_i} \right)^2\]</span></p>
<ul>
<li><span class="math inline">\(e_i = y_i - \hat{y}_i\)</span> is the
standard residual (from the model fit on <em>all</em> data).</li>
<li><span class="math inline">\(h_i\)</span> is the <em>leverage
statistic</em> for the <span class="math inline">\(i\)</span>-th
observation (the <span class="math inline">\(i\)</span>-th diagonal
entry of the â€œhat matrixâ€ <span class="math inline">\(H\)</span>). This
makes LOOCV as fast to compute as a single model fit.
å¯¹äºçº¿æ€§å›å½’ï¼Œæ‚¨æ— éœ€é‡æ–°æ‹Ÿåˆæ¨¡å‹ <span class="math inline">\(N\)</span>
æ¬¡ã€‚æ‚¨å¯ä»¥å¯¹æ‰€æœ‰ <span class="math inline">\(N\)</span>
ä¸ªæ•°æ®ç‚¹<em>ä¸€æ¬¡æ€§</em>åœ°æ‹Ÿåˆæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹å¿«æ·æ–¹å¼ï¼š <span
class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N} \left(
\frac{e_i}{1 - h_i} \right)^2\]</span></li>
<li><span class="math inline">\(e_i = y_i - \hat{y}_i\)</span>
æ˜¯æ ‡å‡†æ®‹å·®ï¼ˆæ¥è‡ªå¯¹<em>æ‰€æœ‰</em>æ•°æ®çš„æ¨¡å‹æ‹Ÿåˆï¼‰ã€‚</li>
<li><span class="math inline">\(h_i\)</span> æ˜¯ç¬¬ <span
class="math inline">\(i\)</span> ä¸ªè§‚æµ‹å€¼ï¼ˆâ€œå¸½å­çŸ©é˜µâ€<span
class="math inline">\(H\)</span> çš„ç¬¬ <span
class="math inline">\(i\)</span> ä¸ªå¯¹è§’çº¿å…ƒç´ ï¼‰çš„<em>æ æ†ç»Ÿè®¡é‡</em>ã€‚
è¿™ä½¿å¾— LOOCV çš„è®¡ç®—é€Ÿåº¦ä¸å•æ¬¡æ¨¡å‹æ‹Ÿåˆä¸€æ ·å¿«ã€‚</li>
</ul></li>
</ol>
<h2 id="python-code-results">Python Code &amp; Results</h2>
<p>The Python code in slide <code>104156.jpg</code> shows how to use
10-fold CV to find the best polynomial degree for a model.</p>
<h3 id="code-understanding-slide-104156.jpg">Code Understanding (Slide
<code>104156.jpg</code>)</h3>
<p>Hereâ€™s a breakdown of the key <code>sklearn</code> parts:</p>
<ol type="1">
<li><strong><code>from sklearn.pipeline import make_pipeline</code></strong>:
This is used to chain steps. The pipeline
<code>make_pipeline(PolynomialFeatures(degree), LinearRegression())</code>
first creates polynomial features (like <span
class="math inline">\(x\)</span>, <span
class="math inline">\(x^2\)</span>, <span
class="math inline">\(x^3\)</span>) and then fits a linear model to
them.</li>
<li><strong><code>from sklearn.model_selection import KFold</code></strong>:
This object is used to define the <span
class="math inline">\(K\)</span>-fold split strategy.
<code>kf = KFold(n_splits=10, shuffle=True, random_state=1)</code>
creates a 10-fold splitter that shuffles the data first.</li>
<li><strong><code>from sklearn.model_selection import cross_val_score</code></strong>:
This is the most important function.
<ul>
<li><code>scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')</code></li>
<li>This one function does all the work: it takes the <code>model</code>
(the pipeline), the data <code>X</code> and <code>y</code>, and the CV
splitter <code>kf</code>. It automatically trains and evaluates the
model 10 times and returns an array of 10 scores (one for each
fold).</li>
<li><code>scoring='neg_mean_squared_error'</code> is used because
<code>cross_val_score</code> expects a <em>higher</em> score to be
<em>better</em>. Since we want to <em>minimize</em> MSE, we use
<em>negative</em> MSE.</li>
</ul></li>
<li><strong><code>avg_mse = -scores.mean()</code></strong>: The code
averages the 10 scores and flips the sign back to positive to get the
final CV (MSE) estimate for that polynomial degree.</li>
</ol>
<h3 id="important-image-the-results">Important Image: The Results</h3>
<p>The plots in slides <code>104156.jpg</code> (Python) and
<code>104224.png</code> (R) show the key result.</p>
<ul>
<li><strong>X-axis:</strong> Degree of Polynomial (model
complexity).å¤šé¡¹å¼çš„æ¬¡æ•°ï¼ˆæ¨¡å‹å¤æ‚åº¦ï¼‰ã€‚</li>
<li><strong>Y-axis:</strong> Estimated Test Error (CV Error /
MSE).ä¼°è®¡æµ‹è¯•è¯¯å·®ï¼ˆCV è¯¯å·® / MSEï¼‰ã€‚</li>
<li><strong>Interpretation:</strong> The plot shows a clear â€œUâ€ shape.
The error is high for degree 1 (a simple line), drops to its minimum at
<strong>degree 2</strong> (a quadratic <span class="math inline">\(ax^2
+ bx + c\)</span>), and then starts to rise again for higher degrees.
This rise indicates <strong>overfitting</strong>â€”the more complex models
are fitting the training dataâ€™s noise, leading to worse performance on
unseen validation data. è¯¥å›¾å‘ˆç°å‡ºæ¸…æ™°çš„â€œUâ€å½¢ã€‚1
æ¬¡ï¼ˆä¸€æ¡ç®€å•çš„ç›´çº¿ï¼‰æ—¶è¯¯å·®è¾ƒå¤§ï¼Œåœ¨<strong>2 æ¬¡</strong>ï¼ˆäºŒæ¬¡ <span
class="math inline">\(ax^2 + bx +
c\)</span>ï¼‰æ—¶é™è‡³æœ€å°ï¼Œç„¶åéšç€æ¬¡æ•°çš„å¢åŠ ï¼Œè¯¯å·®å†æ¬¡ä¸Šå‡ã€‚è¿™ç§ä¸Šå‡è¡¨æ˜<strong>è¿‡æ‹Ÿåˆ</strong>â€”â€”æ›´å¤æ‚çš„æ¨¡å‹ä¼šæ‹Ÿåˆè®­ç»ƒæ•°æ®çš„å™ªå£°ï¼Œå¯¼è‡´åœ¨æœªè§è¿‡çš„éªŒè¯æ•°æ®ä¸Šçš„æ€§èƒ½ä¸‹é™ã€‚</li>
<li><strong>Conclusion:</strong> The 10-fold CV analysis suggests that a
<strong>quadratic model (degree 2)</strong> is the best choice, as it
provides the lowest estimated test error. 10 å€ CV
åˆ†æè¡¨æ˜<strong>äºŒæ¬¡æ¨¡å‹ï¼ˆ2
æ¬¡ï¼‰</strong>æ˜¯æœ€ä½³é€‰æ‹©ï¼Œå› ä¸ºå®ƒæä¾›äº†æœ€ä½çš„ä¼°è®¡æµ‹è¯•è¯¯å·®ã€‚</li>
</ul>
<p>Letâ€™s dive into the details of that proof.</p>
<h2 id="detailed-summary-the-fast-computation-of-loocv-proof">Detailed
Summary: The â€œFast Computation of LOOCVâ€ Proof</h2>
<p>The most mathematically dense and important part of your slides is
the proof (spanning slides <code>104126.jpg</code>,
<code>104132.png</code>, and <code>104136.png</code>) that LOOCV, which
seems computationally very expensive, can be calculated quickly for
linear regression. LOOCV
è™½ç„¶è®¡ç®—æˆæœ¬çœ‹ä¼¼éå¸¸é«˜ï¼Œä½†å¯¹äºçº¿æ€§å›å½’æ¥è¯´ï¼Œå®ƒå¯ä»¥å¿«é€Ÿè®¡ç®—ã€‚ ### The
Goal</p>
<p>The goal is to prove that the LOOCV statistic, which is defined as:
<span class="math display">\[CV = \frac{1}{N} \sum_{i=1}^{N} e_{[i]}^2
\quad \text{where } e_{[i]} = y_i - \hat{y}_{[i]}\]</span> (Here, <span
class="math inline">\(\hat{y}_{[i]}\)</span> is the prediction for <span
class="math inline">\(y_i\)</span> from a model trained on all data
<em>except</em> point <span
class="math inline">\(i\)</span>).ï¼ˆå…¶ä¸­ï¼Œ<span
class="math inline">\(\hat{y}_{[i]}\)</span> è¡¨ç¤ºåŸºäºé™¤ç‚¹ <span
class="math inline">\(i\)</span> ä¹‹å¤–çš„æ‰€æœ‰æ•°æ®è®­ç»ƒçš„æ¨¡å‹å¯¹ <span
class="math inline">\(y_i\)</span> çš„é¢„æµ‹ï¼‰ã€‚</p>
<p>â€¦can be computed <em>without</em> re-fitting the model <span
class="math inline">\(N\)</span> times, using this â€œfastâ€ formula:
æ— éœ€é‡æ–°æ‹Ÿåˆæ¨¡å‹ <span class="math inline">\(N\)</span>
æ¬¡å³å¯è®¡ç®—ï¼Œä½¿ç”¨ä»¥ä¸‹â€œå¿«é€Ÿâ€å…¬å¼ï¼š <span class="math display">\[CV =
\frac{1}{N} \sum_{i=1}^{N} \left( \frac{e_i}{1 - h_i} \right)^2\]</span>
(Here, <span class="math inline">\(e_i\)</span> is the <em>standard</em>
residual and <span class="math inline">\(h_i\)</span> is the
<em>leverage</em>, both from a single model fit on <em>all</em>
data).</p>
<p>The entire proof boils down to showing one identity: <strong><span
class="math inline">\(e_{[i]} = e_i / (1 - h_i)\)</span></strong>.</p>
<h3 id="key-definitions-the-matrix-algebra-setup-çŸ©é˜µä»£æ•°è®¾ç½®">Key
Definitions (The Matrix Algebra Setup) ï¼ˆçŸ©é˜µä»£æ•°è®¾ç½®ï¼‰</h3>
<ul>
<li><strong>Model æ¨¡å‹:</strong> <span class="math inline">\(\mathbf{Y}
= \mathbf{X}\beta + \mathbf{e}\)</span></li>
<li><strong>Full Data Estimate å®Œæ•´æ•°æ®ä¼°è®¡ (<span
class="math inline">\(\hat{\beta}\)</span>):</strong> <span
class="math inline">\(\hat{\beta} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\)</span></li>
<li><strong>Hat Matrix å¸½å­çŸ©é˜µ (<span
class="math inline">\(\mathbf{H}\)</span>):</strong> <span
class="math inline">\(\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span></li>
<li><strong>Full Data Residual å®Œæ•´æ•°æ®æ®‹å·® (<span
class="math inline">\(e_i\)</span>):</strong> <span
class="math inline">\(e_i = y_i - \hat{y}_i = y_i -
\mathbf{x}_i^T\hat{\beta}\)</span></li>
<li><strong>Leverage (<span class="math inline">\(h_i\)</span>) æ æ†
(<span class="math inline">\(h_i\)</span>):</strong> The <span
class="math inline">\(i\)</span>-th diagonal element of <span
class="math inline">\(\mathbf{H}\)</span>. <span
class="math inline">\(h_i =
\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i\)</span></li>
<li><strong>Leave-One-Out Estimate (<span
class="math inline">\(\hat{\beta}_{[i]}\)</span>):</strong> <span
class="math inline">\(\hat{\beta}_{[i]} =
(\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1}\mathbf{X}_{[i]}^T\mathbf{Y}_{[i]}\)</span>
<ul>
<li><span class="math inline">\(\mathbf{X}_{[i]}\)</span> and <span
class="math inline">\(\mathbf{Y}_{[i]}\)</span> are the data with the
<span class="math inline">\(i\)</span>-th row removed.</li>
</ul></li>
<li><strong>LOOCV Residual LOOCV æ®‹å·® (<span
class="math inline">\(e_{[i]}\)</span>):</strong> <span
class="math inline">\(e_{[i]} = y_i -
\mathbf{x}_i^T\hat{\beta}_{[i]}\)</span></li>
</ul>
<h3 id="the-proof-step-by-step">The Proof Step-by-Step</h3>
<p>Here is the logic from your slides, broken down:</p>
<h4 id="step-1-relating-the-matrices-slide-104132.png">Step 1: Relating
the Matrices (Slide <code>104132.png</code>)</h4>
<p>The proofâ€™s â€œtrickâ€ is to relate the â€œfull dataâ€ matrix <span
class="math inline">\((\mathbf{X}^T\mathbf{X})\)</span> to the
â€œleave-one-outâ€ matrix <span
class="math inline">\((\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})\)</span>.
è¯æ˜çš„â€œæŠ€å·§â€æ˜¯å°†â€œå…¨æ•°æ®â€çŸ©é˜µ <span
class="math inline">\((\mathbf{X}^T\mathbf{X})\)</span> ä¸â€œç•™ä¸€æ³•â€çŸ©é˜µ
<span
class="math inline">\((\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})\)</span>
å…³è”èµ·æ¥ã€‚</p>
<ul>
<li><p>The full sum-of-squares matrix is just the leave-one-out matrix
<em>plus</em> the one observationâ€™s contribution:
å®Œæ•´çš„å¹³æ–¹å’ŒçŸ©é˜µå°±æ˜¯ç•™ä¸€æ³•çŸ©é˜µ<em>åŠ ä¸Š</em>ä¸€ä¸ªè§‚æµ‹å€¼çš„è´¡çŒ®ï¼š</p>
<p><span class="math display">\[\mathbf{X}^T\mathbf{X} =
\mathbf{X}_{[i]}^T\mathbf{X}_{[i]} +
\mathbf{x}_i\mathbf{x}_i^T\]</span></p></li>
<li><p>This means: <span
class="math inline">\(\mathbf{X}_{[i]}^T\mathbf{X}_{[i]} =
\mathbf{X}^T\mathbf{X} - \mathbf{x}_i\mathbf{x}_i^T\)</span></p></li>
</ul>
<h4 id="step-2-the-key-matrix-trick-slide-104132.png">Step 2: The Key
Matrix Trick (Slide <code>104132.png</code>)</h4>
<p>We need the inverse <span
class="math inline">\((\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1}\)</span>
to calculate <span class="math inline">\(\hat{\beta}_{[i]}\)</span>.
Finding this inverse directly is hard. Instead, we use the
<strong>Sherman-Morrison-Woodbury formula</strong>, which tells us how
to find the inverse of a matrix thatâ€™s been â€œupdatedâ€ (in this case, by
subtracting <span
class="math inline">\(\mathbf{x}_i\mathbf{x}_i^T\)</span>).</p>
<p>æˆ‘ä»¬éœ€è¦é€†<span
class="math inline">\((\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1}\)</span>
æ¥è®¡ç®— <span
class="math inline">\(\hat{\beta}_{[i]}\)</span>ã€‚ç›´æ¥æ±‚è¿™ä¸ªé€†çŸ©é˜µå¾ˆå›°éš¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨
<strong>Sherman-Morrison-Woodbury
å…¬å¼</strong>ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬å¦‚ä½•æ±‚ä¸€ä¸ªâ€œæ›´æ–°â€åçš„çŸ©é˜µçš„é€†çŸ©é˜µï¼ˆåœ¨æœ¬ä¾‹ä¸­ï¼Œæ˜¯é€šè¿‡å‡å»
<span class="math inline">\(\mathbf{x}_i\mathbf{x}_i^T\)</span>
æ¥å®ç°çš„ï¼‰ã€‚</p>
<p>The slide applies this formula to get: <span
class="math display">\[(\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1} =
(\mathbf{X}^T\mathbf{X})^{-1} +
\frac{(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}}{1
- h_i}\]</span> * This is the most complex step, but itâ€™s a standard
matrix identity. Itâ€™s crucial because it expresses the â€œleave-one-outâ€
inverse in terms of the â€œfull dataâ€ inverse <span
class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span>, which we
already have.</p>
<h4 id="step-3-finding-hatbeta_i-slide-104136.png">Step 3: Finding <span
class="math inline">\(\hat{\beta}_{[i]}\)</span> (Slide
<code>104136.png</code>)</h4>
<p>Now we can write a new formula for <span
class="math inline">\(\hat{\beta}_{[i]}\)</span> by substituting the
result from Step 2. We also note that <span
class="math inline">\(\mathbf{X}_{[i]}^T\mathbf{Y}_{[i]} =
\mathbf{X}^T\mathbf{Y} - \mathbf{x}_i y_i\)</span>.</p>
<p><span class="math display">\[\hat{\beta}_{[i]} =
(\mathbf{X}_{[i]}^T\mathbf{X}_{[i]})^{-1}
(\mathbf{X}_{[i]}^T\mathbf{Y}_{[i]})\]</span> <span
class="math display">\[\hat{\beta}_{[i]} = \left[
(\mathbf{X}^T\mathbf{X})^{-1} +
\frac{(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}}{1
- h_i} \right] (\mathbf{X}^T\mathbf{Y} - \mathbf{x}_i y_i)\]</span></p>
<p>The slide then shows the algebra to simplify this big expression.
When you expand and simplify everything, you get a much cleaner
result:</p>
<p><span class="math display">\[\hat{\beta}_{[i]} = \hat{\beta} -
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i \frac{e_i}{1 - h_i}\]</span> *
This is a beautiful result! It says the LOOCV coefficient vector is just
the <em>full</em> coefficient vector minus a small adjustment term
related to the <span class="math inline">\(i\)</span>-th observationâ€™s
residual (<span class="math inline">\(e_i\)</span>) and leverage (<span
class="math inline">\(h_i\)</span>). * è¿™æ˜¯ä¸€ä¸ªéå¸¸æ£’çš„ç»“æœï¼å®ƒè¡¨æ˜
LOOCV ç³»æ•°å‘é‡å°±æ˜¯<em>å®Œæ•´</em>çš„ç³»æ•°å‘é‡å‡å»ä¸€ä¸ªä¸ç¬¬ <span
class="math inline">\(i\)</span> ä¸ªè§‚æµ‹å€¼çš„æ®‹å·® (<span
class="math inline">\(e_i\)</span>) å’Œæ æ†ç‡ (<span
class="math inline">\(h_i\)</span>) ç›¸å…³çš„å°è°ƒæ•´é¡¹ã€‚</p>
<h4 id="step-4-finding-e_i-slide-104136.png">Step 4: Finding <span
class="math inline">\(e_{[i]}\)</span> (Slide
<code>104136.png</code>)</h4>
<p>This is the final step. We use the definition of <span
class="math inline">\(e_{[i]}\)</span> and the result from Step 3.
è¿™æ˜¯æœ€åä¸€æ­¥ã€‚æˆ‘ä»¬ä½¿ç”¨ <span class="math inline">\(e_{[i]}\)</span>
çš„å®šä¹‰å’Œæ­¥éª¤ 3 çš„ç»“æœã€‚</p>
<ul>
<li><strong>Start with the definition:</strong> <span
class="math inline">\(e_{[i]} = y_i -
\mathbf{x}_i^T\hat{\beta}_{[i]}\)</span></li>
<li><strong>Substitute <span
class="math inline">\(\hat{\beta}_{[i]}\)</span>:</strong> <span
class="math inline">\(e_{[i]} = y_i - \mathbf{x}_i^T \left[ \hat{\beta}
- (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i \frac{e_i}{1 - h_i}
\right]\)</span></li>
<li><strong>Distribute <span
class="math inline">\(\mathbf{x}_i^T\)</span>:</strong> <span
class="math inline">\(e_{[i]} = (y_i - \mathbf{x}_i^T\hat{\beta}) +
\left( \mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i \right)
\frac{e_i}{1 - h_i}\)</span></li>
<li><strong>Recognize the terms!</strong>
<ul>
<li>The first term is just the standard residual: <span
class="math inline">\((y_i - \mathbf{x}_i^T\hat{\beta}) =
e_i\)</span></li>
<li>The second term in parentheses is the definition of leverage: <span
class="math inline">\((\mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i)
= h_i\)</span></li>
</ul></li>
<li><strong>Substitute back:</strong> <span
class="math inline">\(e_{[i]} = e_i + h_i \left( \frac{e_i}{1 - h_i}
\right)\)</span></li>
<li><strong>Get a common denominator:</strong> <span
class="math inline">\(e_{[i]} = \frac{e_i(1 - h_i) + h_i e_i}{1 -
h_i}\)</span></li>
<li><strong>Simplify the numerator:</strong> <span
class="math inline">\(e_{[i]} = \frac{e_i - e_ih_i + e_ih_i}{1 -
h_i}\)</span></li>
</ul>
<p>This gives the final, simple relationship: <span
class="math display">\[e_{[i]} = \frac{e_i}{1 - h_i}\]</span></p>
<h3 id="conclusion">Conclusion</h3>
<p>By proving this identity, the slides show that to get all <span
class="math inline">\(N\)</span> of the â€œleave-one-outâ€ errors, you only
need to: 1. Fit <strong>one</strong> linear regression model on
<strong>all</strong> the data. 2. Calculate the standard residuals <span
class="math inline">\(e_i\)</span> and the leverage values <span
class="math inline">\(h_i\)</span> for all <span
class="math inline">\(N\)</span> points. 3. Apply the formula <span
class="math inline">\(e_i / (1 - h_i)\)</span> for each point.</p>
<p>This turns a procedure that looked like it would take <span
class="math inline">\(N\)</span> times the work into a procedure that
takes only <strong>1</strong> model fit. This is why LOOCV is a
practical and efficient method for linear regression.</p>
<p>é€šè¿‡è¯æ˜è¿™ä¸ªæ’ç­‰å¼ï¼Œå¹»ç¯ç‰‡æ˜¾ç¤ºï¼Œè¦è·å¾—æ‰€æœ‰ <span
class="math inline">\(N\)</span> ä¸ªâ€œç•™ä¸€æ³•â€è¯¯å·®ï¼Œæ‚¨åªéœ€ï¼š 1.
å¯¹<strong>æ‰€æœ‰</strong>æ•°æ®æ‹Ÿåˆ<strong>ä¸€ä¸ª</strong>çº¿æ€§å›å½’æ¨¡å‹ã€‚ 2.
è®¡ç®—æ‰€æœ‰ <span class="math inline">\(N\)</span> ä¸ªç‚¹çš„æ ‡å‡†æ®‹å·® <span
class="math inline">\(e_i\)</span> å’Œæ æ†å€¼ <span
class="math inline">\(h_i\)</span>ã€‚ 3. å¯¹æ¯ä¸ªç‚¹åº”ç”¨å…¬å¼ <span
class="math inline">\(e_i / (1 - h_i)\)</span>ã€‚</p>
<p>è¿™å°†ä¸€ä¸ªçœ‹ä¼¼éœ€è¦ <span class="math inline">\(N\)</span>
å€å·¥ä½œé‡çš„è¿‡ç¨‹å˜æˆäº†åªéœ€ <strong>1</strong>
æ¬¡æ¨¡å‹æ‹Ÿåˆçš„è¿‡ç¨‹ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ LOOCV
æ˜¯ä¸€ç§å®ç”¨ä¸”é«˜æ•ˆçš„çº¿æ€§å›å½’æ–¹æ³•ã€‚</p>
<h1 id="main-goal-of-cross-validation-äº¤å‰éªŒè¯çš„ä¸»è¦ç›®æ ‡">5. Main Goal
of Cross-Validation äº¤å‰éªŒè¯çš„ä¸»è¦ç›®æ ‡</h1>
<p>The central purpose of cross-validation is to <strong>estimate the
true test error</strong> of a machine learning model. This is crucial
for:</p>
<ol type="1">
<li><strong>Model Assessment:</strong> Evaluating how well a model will
perform on new, unseen data. è¯„ä¼°æ¨¡å‹åœ¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ä¸Šçš„è¡¨ç°ã€‚</li>
<li><strong>Model Selection:</strong> Choosing the best level of model
flexibility (e.g., the degree of a polynomial or the value of <span
class="math inline">\(K\)</span> in KNN) to avoid
<strong>overfitting</strong>.
é€‰æ‹©æœ€ä½³çš„æ¨¡å‹çµæ´»æ€§æ°´å¹³ï¼ˆä¾‹å¦‚ï¼Œå¤šé¡¹å¼çš„æ¬¡æ•°æˆ– KNN ä¸­çš„ <span
class="math inline">\(K\)</span>
å€¼ï¼‰ï¼Œä»¥é¿å…<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚</li>
</ol>
<p>As the slides show, <strong>training error</strong> (the error on the
data the model was trained on) consistently decreases as model
complexity increases. However, the <strong>test error</strong> follows a
U-shape: it first decreases (as the model learns the true signal) and
then increases (as the model starts fitting the noise, or
â€œoverfittingâ€). CV helps find the minimum point of this U-shaped test
error curve.
<strong>è®­ç»ƒè¯¯å·®</strong>ï¼ˆæ¨¡å‹è®­ç»ƒæ•°æ®çš„è¯¯å·®ï¼‰éšç€æ¨¡å‹å¤æ‚åº¦çš„å¢åŠ è€ŒæŒç»­ä¸‹é™ã€‚ç„¶è€Œï¼Œ<strong>æµ‹è¯•è¯¯å·®</strong>å‘ˆç°
U
å½¢ï¼šå®ƒå…ˆä¸‹é™ï¼ˆå½“æ¨¡å‹å­¦ä¹ çœŸå®ä¿¡å·æ—¶ï¼‰ï¼Œç„¶åä¸Šå‡ï¼ˆå½“æ¨¡å‹å¼€å§‹æ‹Ÿåˆå™ªå£°ï¼Œå³â€œè¿‡æ‹Ÿåˆâ€æ—¶ï¼‰ã€‚äº¤å‰éªŒè¯æœ‰åŠ©äºæ‰¾åˆ°è¿™æ¡
U å½¢æµ‹è¯•è¯¯å·®æ›²çº¿çš„æœ€å°å€¼ã€‚</p>
<h2 id="important-images-1">Important Images ğŸ–¼ï¸</h2>
<p>The most important image is on <strong>Slide 61</strong>.</p>
<p>These two plots perfectly illustrate the concept:</p>
<ul>
<li><strong>Blue Line (Training Error):</strong> Always goes down.</li>
<li><strong>Brown Line (True Test Error):</strong> Forms a â€œUâ€ shape.
This is what we <em>want</em> to find the minimum of, but itâ€™s unknown
in practice.</li>
<li><strong>Black Line (10-fold CV Error):</strong> This is our
<em>estimate</em> of the test error. Notice how closely it tracks the
brown line. The minimum of the CV curve (marked with an â€˜xâ€™) is very
close to the minimum of the true test error.</li>
</ul>
<p>This shows <em>why</em> CV works: it provides a reliable estimate to
guide our choice of model (e.g., polynomial degree 3-4 for logistic
regression, or <span class="math inline">\(K \approx 10\)</span> for
KNN).</p>
<ul>
<li><strong>è“çº¿ï¼ˆè®­ç»ƒè¯¯å·®ï¼‰ï¼š</strong>å§‹ç»ˆå‘ä¸‹ã€‚</li>
<li><strong>æ£•çº¿ï¼ˆçœŸå®æµ‹è¯•è¯¯å·®ï¼‰ï¼š</strong>å‘ˆâ€œUâ€å½¢ã€‚è¿™æ­£æ˜¯æˆ‘ä»¬<em>æƒ³è¦</em>æ‰¾åˆ°çš„æœ€å°å€¼ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­æ— æ³•ç¡®å®šã€‚</li>
<li><strong>é»‘çº¿ï¼ˆ10 å€ CV
è¯¯å·®ï¼‰ï¼š</strong>è¿™æ˜¯æˆ‘ä»¬å¯¹æµ‹è¯•è¯¯å·®çš„<em>ä¼°è®¡</em>ã€‚æ³¨æ„å®ƒä¸æ£•çº¿çš„å»åˆç¨‹åº¦ã€‚CV
æ›²çº¿çš„æœ€å°å€¼ï¼ˆæ ‡æœ‰â€œxâ€ï¼‰éå¸¸æ¥è¿‘çœŸå®æµ‹è¯•è¯¯å·®çš„æœ€å°å€¼ã€‚</li>
</ul>
<p>è¿™è¯´æ˜äº† CV
çš„<em>åŸå› </em>ï¼šå®ƒæä¾›äº†å¯é çš„ä¼°è®¡å€¼æ¥æŒ‡å¯¼æˆ‘ä»¬é€‰æ‹©æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œé€»è¾‘å›å½’çš„å¤šé¡¹å¼æ¬¡æ•°ä¸º
3-4ï¼ŒKNN çš„ <span class="math inline">\(K \approx 10\)</span>ï¼‰ã€‚</p>
<h2 id="key-formulas-for-classification">Key Formulas for
Classification</h2>
<p>For regression, we often use Mean Squared Error (MSE). For
classification, the slides introduce the <strong>classification error
rate</strong>.</p>
<p>For Leave-One-Out Cross-Validation (LOOCV), the error for a single
observation <span class="math inline">\(i\)</span> is: <span
class="math display">\[Err_i = I(y_i \neq \hat{y}_i^{(i)})\]</span> *
<span class="math inline">\(y_i\)</span> is the true label for
observation <span class="math inline">\(i\)</span>. * <span
class="math inline">\(\hat{y}_i^{(i)}\)</span> is the modelâ€™s prediction
for observation <span class="math inline">\(i\)</span> when the model
was trained on all <em>other</em> observations <em>except</em> <span
class="math inline">\(i\)</span>. * <span
class="math inline">\(I(\dots)\)</span> is an <strong>indicator
function</strong>: itâ€™s <span class="math inline">\(1\)</span> if the
condition is true (prediction is wrong) and <span
class="math inline">\(0\)</span> if false (prediction is correct).</p>
<p>The total <strong>CV error</strong> is simply the average of these
individual errors, which is the overall fraction of incorrect
classifications: <span class="math display">\[CV_{(n)} = \frac{1}{n}
\sum_{i=1}^{n} Err_i\]</span> The slides also show examples using
<strong>Log Loss</strong> (Slide 64), which is another common and
sensitive metric for classification. The logistic regression model
itself is defined by: <span class="math display">\[P(Y=1 | X) =
\frac{1}{1 + \exp(-\beta_0 - \beta_1 X_1 - \beta_2 X_2 -
\dots)}\]</span></p>
<p>å¯¹äºå›å½’ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨å‡æ–¹è¯¯å·®
(MSE)ã€‚å¯¹äºåˆ†ç±»ï¼Œå¹»ç¯ç‰‡ä»‹ç»äº†<strong>åˆ†ç±»é”™è¯¯ç‡</strong>ã€‚</p>
<p>å¯¹äºç•™ä¸€äº¤å‰éªŒè¯ (LOOCV)ï¼Œå•ä¸ªè§‚æµ‹å€¼ <span
class="math inline">\(i\)</span> çš„è¯¯å·®ä¸ºï¼š <span
class="math display">\[Err_i = I(y_i \neq \hat{y}_i^{(i)})\]</span> *
<span class="math inline">\(y_i\)</span> æ˜¯è§‚æµ‹å€¼ <span
class="math inline">\(i\)</span> çš„çœŸå®æ ‡ç­¾ã€‚ * <span
class="math inline">\(\hat{y}_i^{(i)}\)</span> æ˜¯æ¨¡å‹åœ¨é™¤ <span
class="math inline">\(i\)</span>
ä¹‹å¤–çš„æ‰€æœ‰å…¶ä»–è§‚æµ‹å€¼ä¸Šè¿›è¡Œè®­ç»ƒåï¼Œå¯¹è§‚æµ‹å€¼ <span
class="math inline">\(i\)</span> çš„é¢„æµ‹ã€‚ * <span
class="math inline">\(I(\dots)\)</span>
æ˜¯ä¸€ä¸ª<strong>æŒ‡ç¤ºå‡½æ•°</strong>ï¼šå¦‚æœæ¡ä»¶ä¸ºçœŸï¼ˆé¢„æµ‹é”™è¯¯ï¼‰ï¼Œåˆ™ä¸º <span
class="math inline">\(1\)</span>ï¼›å¦‚æœæ¡ä»¶ä¸ºå‡ï¼ˆé¢„æµ‹æ­£ç¡®ï¼‰ï¼Œåˆ™ä¸º <span
class="math inline">\(0\)</span>ã€‚</p>
<p>æ€»<strong>CVè¯¯å·®</strong>åªæ˜¯è¿™äº›å•ä¸ªè¯¯å·®çš„å¹³å‡å€¼ï¼Œä¹Ÿå°±æ˜¯é”™è¯¯åˆ†ç±»çš„æ€»ä½“æ¯”ä¾‹ï¼š
<span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n}
Err_i\]</span>
å¹»ç¯ç‰‡è¿˜å±•ç¤ºäº†ä½¿ç”¨<strong>å¯¹æ•°æŸå¤±</strong>ï¼ˆå¹»ç¯ç‰‡64ï¼‰çš„ç¤ºä¾‹ï¼Œè¿™æ˜¯å¦ä¸€ä¸ªå¸¸è§ä¸”æ•æ„Ÿçš„åˆ†ç±»æŒ‡æ ‡ã€‚é€»è¾‘å›å½’æ¨¡å‹æœ¬èº«çš„å®šä¹‰å¦‚ä¸‹ï¼š
<span class="math display">\[P(Y=1 | X) = \frac{1}{1 + \exp(-\beta_0 -
\beta_1 X_1 - \beta_2 X_2 - \dots)}\]</span></p>
<h2 id="python-code-explained">Python Code Explained ğŸ</h2>
<p>The slides provide two key Python examples. Both manually implement
K-fold cross-validation to show how it works.</p>
<h3 id="knn-regression-slide-52-knn-å›å½’">1. KNN Regression (Slide 52)
KNN å›å½’</h3>
<ul>
<li><strong>Goal:</strong> Find the best <code>n_neighbors</code> (K)
for a <code>KNeighborsRegressor</code>. ä¸º
<code>KNeighborsRegressor</code> æ‰¾åˆ°æœ€ä½³çš„ <code>n_neighbors</code>
(K)ã€‚</li>
<li><strong>Logic:</strong>
<ol type="1">
<li>It creates a <code>KFold</code> object to split the data into 10
folds (<code>n_splits=10</code>). åˆ›å»ºä¸€ä¸ª <code>KFold</code>
å¯¹è±¡ï¼Œå°†æ•°æ®æ‹†åˆ†æˆ 10 ä¸ªæŠ˜å ï¼ˆ<code>n_splits=10</code>ï¼‰ã€‚</li>
<li>It has an <strong>outer loop</strong> that iterates through
different values of <span class="math inline">\(K\)</span> (from 1 to
10). å®ƒæœ‰ä¸€ä¸ª <strong>å¤–å¾ªç¯</strong>ï¼Œè¿­ä»£ä¸åŒçš„ <span
class="math inline">\(K\)</span> å€¼ï¼ˆä» 1 åˆ° 10ï¼‰ã€‚</li>
<li>It has an <strong>inner loop</strong> that iterates through the 10
folds (<code>for train_index, test_index in kfold.split(X)</code>).
å®ƒæœ‰ä¸€ä¸ª <strong>å†…å¾ªç¯</strong>ï¼Œè¿­ä»£è¿™ 10
ä¸ªæŠ˜å ï¼ˆ<code>for train_index, test_index in kfold.split(X)</code>ï¼‰ã€‚</li>
<li><strong>Inside the inner loop:</strong>
<ul>
<li>It trains a <code>KNeighborsRegressor</code> on the 9 training folds
(<code>X_train</code>, <code>y_train</code>).</li>
<li>It makes predictions on the 1 held-out test fold
(<code>X_test</code>).</li>
<li>It calculates the mean squared error for that fold and stores
it.</li>
<li>åœ¨ 9 ä¸ªè®­ç»ƒæŠ˜å ï¼ˆ<code>X_train</code>, <code>y_train</code>ï¼‰ä¸Šè®­ç»ƒ
<code>KNeighborsRegressor</code>ã€‚</li>
<li>å®ƒå¯¹ç¬¬ä¸€ä¸ªä¿ç•™çš„æµ‹è¯•é›† (<code>X_test</code>) è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>å®ƒè®¡ç®—è¯¥é›†çš„å‡æ–¹è¯¯å·®å¹¶å­˜å‚¨ã€‚</li>
</ul></li>
<li><strong>After the inner loop:</strong> It averages the 10 error
scores (one from each fold) to get the final CV error for that specific
<span class="math inline">\(K\)</span>. å¯¹ 10
ä¸ªè¯¯å·®åˆ†æ•°ï¼ˆæ¯ä¸ªé›†ä¸€ä¸ªï¼‰æ±‚å¹³å‡å€¼ï¼Œå¾—åˆ°è¯¥ç‰¹å®š <span
class="math inline">\(K\)</span> çš„æœ€ç»ˆ CV è¯¯å·®ã€‚</li>
<li>The final plot shows this CV error vs.Â <span
class="math inline">\(K\)</span>, allowing us to pick the <span
class="math inline">\(K\)</span> with the lowest error. æœ€ç»ˆå›¾è¡¨æ˜¾ç¤ºäº†
CV è¯¯å·®ä¸ <span class="math inline">\(K\)</span>
çš„å…³ç³»ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿé€‰æ‹©è¯¯å·®æœ€å°çš„ <span
class="math inline">\(K\)</span>ã€‚</li>
</ol></li>
</ul>
<h3
id="logistic-regression-with-polynomials-slide-64-ä½¿ç”¨å¤šé¡¹å¼çš„é€»è¾‘å›å½’">2.
Logistic Regression with Polynomials (Slide 64)
ä½¿ç”¨å¤šé¡¹å¼çš„é€»è¾‘å›å½’</h3>
<ul>
<li><strong>Goal:</strong> Find the best <code>degree</code> for
<code>PolynomialFeatures</code> used with
<code>LogisticRegression</code>.</li>
<li><strong>Logic:</strong> This is very similar to the KNN example but
uses a different model and error metric.
<ol type="1">
<li>It sets up a 10-fold split (<code>kf = KFold(...)</code>).</li>
<li>An <strong>outer loop</strong> iterates through the
<code>degree</code> <span class="math inline">\(d\)</span> (from 1 to
10).</li>
<li>An <strong>inner loop</strong> iterates through the 10 folds.</li>
<li><strong>Inside the inner loop:</strong>
<ul>
<li>It creates <code>PolynomialFeatures</code> of degree <span
class="math inline">\(d\)</span>.</li>
<li>It transforms the 9 training folds (<code>X_train</code>) into
polynomial features (<code>X_train_poly</code>).</li>
<li>It trains a <code>LogisticRegression</code> model on
<code>X_train_poly</code>.</li>
<li>It transforms the 1 held-out test fold (<code>X_test</code>) using
the <em>same</em> polynomial transformer.</li>
<li>It calculates the <code>log_loss</code> on the test fold.</li>
</ul></li>
<li><strong>After the inner loop:</strong> It averages the 10
<code>log_loss</code> scores to get the final CV error for that
<code>degree</code>.</li>
<li>The plot shows CV error vs.Â degree, and the minimum is clearly at
<code>degree=3</code>.</li>
</ol></li>
</ul>
<h2 id="the-bias-variance-trade-off-in-cv-cv-ä¸­çš„åå·®-æ–¹å·®æƒè¡¡">The
Bias-Variance Trade-off in CV CV ä¸­çš„åå·®-æ–¹å·®æƒè¡¡</h2>
<p>This is a key theoretical point from <strong>Slide 54</strong> that
answers the questions on Slide 65. It compares LOOCV (<span
class="math inline">\(K=n\)</span>) with K-fold CV (<span
class="math inline">\(K=5\)</span> or <span
class="math inline">\(10\)</span>). è¿™æ˜¯<strong>å¹»ç¯ç‰‡
54</strong>ä¸­çš„ä¸€ä¸ªå…³é”®ç†è®ºç‚¹ï¼Œå®ƒå›ç­”äº†å¹»ç¯ç‰‡ 65 ä¸­çš„é—®é¢˜ã€‚å®ƒæ¯”è¾ƒäº†
LOOCVï¼ˆK=nï¼‰å’Œ K å€ CVï¼ˆK=5 æˆ– 10ï¼‰ã€‚</p>
<ul>
<li><strong>LOOCV (K=n):</strong>
<ul>
<li><strong>Bias:</strong> Very <strong>low</strong>. The model is
trained on <span class="math inline">\(n-1\)</span> samples, which is
almost the full dataset. The resulting error estimate is nearly unbiased
for the true test error. è¯¥æ¨¡å‹åŸºäº <span
class="math inline">\(n-1\)</span>
ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œè¿™å‡ ä¹æ˜¯æ•´ä¸ªæ•°æ®é›†ã€‚å¾—åˆ°çš„è¯¯å·®ä¼°è®¡å¯¹äºçœŸå®æµ‹è¯•è¯¯å·®å‡ ä¹æ²¡æœ‰åå·®ã€‚</li>
<li><strong>Variance:</strong> Very <strong>high</strong>. You are
training <span class="math inline">\(n\)</span> models that are
<em>almost identical</em> to each other (they only differ by one data
point). Averaging these highly correlated error estimates doesnâ€™t reduce
the variance much, making the CV estimate unstable.
éå¸¸<strong>é«˜</strong>ã€‚æ‚¨æ­£åœ¨è®­ç»ƒ <span
class="math inline">\(n\)</span>
ä¸ªå½¼æ­¤<em>å‡ ä¹ç›¸åŒ</em>çš„æ¨¡å‹ï¼ˆå®ƒä»¬ä»…ç›¸å·®ä¸€ä¸ªæ•°æ®ç‚¹ï¼‰ã€‚å¯¹è¿™äº›é«˜åº¦ç›¸å…³çš„è¯¯å·®ä¼°è®¡æ±‚å¹³å‡å€¼å¹¶ä¸èƒ½æ˜¾è‘—é™ä½æ–¹å·®ï¼Œä»è€Œå¯¼è‡´
CV ä¼°è®¡ä¸ç¨³å®šã€‚</li>
</ul></li>
<li><strong>K-Fold CV (K=5 or 10):</strong>
<ul>
<li><strong>Bias:</strong> Slightly <strong>higher</strong> than LOOCV.
The models are trained on, for example, 90% of the data. Since they are
trained on less data, they <em>might</em> perform slightly worse. This
means K-fold CV <strong>tends to slightly overestimate the true test
error</strong> (Slide 66).</li>
<li><strong>Variance:</strong> Much <strong>lower</strong> than LOOCV.
The 10 models are trained on more different â€œchunksâ€ of data (they
overlap less), so their error estimates are less correlated. Averaging
less-correlated estimates significantly reduces the overall
variance.</li>
</ul></li>
</ul>
<p><strong>Conclusion:</strong> We generally prefer <strong>10-fold
CV</strong> over LOOCV. It gives a much more stable (low-variance)
estimate of the test error, even if itâ€™s slightly more biased
(overestimating the error, which is a safe/conservative estimate).
æˆ‘ä»¬é€šå¸¸æ›´å–œæ¬¢<strong>10 å€äº¤å‰éªŒè¯</strong>è€Œä¸æ˜¯
LOOCVã€‚å®ƒèƒ½ç»™å‡ºæ›´ç¨³å®šï¼ˆä½æ–¹å·®ï¼‰çš„æµ‹è¯•è¯¯å·®ä¼°è®¡å€¼ï¼Œå³ä½¿å®ƒçš„åå·®ç•¥å¤§ï¼ˆé«˜ä¼°äº†è¯¯å·®ï¼Œè¿™æ˜¯ä¸€ä¸ªå®‰å…¨/ä¿å®ˆçš„ä¼°è®¡å€¼ï¼‰ã€‚</p>
<h2 id="the-core-problem-scenarios-slides-47-51">The Core Problem &amp;
Scenarios (Slides 47-51)</h2>
<p>These slides use three scenarios to show <em>why</em> we need
cross-validation (CV). The goal is to pick the right level of
<strong>model flexibility</strong> (e.g., the degree of a polynomial or
the complexity of a spline) to minimize the <strong>Test MSE</strong>
(Mean Squared Error), which we canâ€™t see in real life.
è¿™äº›å¹»ç¯ç‰‡ä½¿ç”¨äº†ä¸‰ç§åœºæ™¯æ¥è¯´æ˜ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦äº¤å‰éªŒè¯
(CV)ã€‚ç›®æ ‡æ˜¯é€‰æ‹©åˆé€‚çš„<strong>æ¨¡å‹çµæ´»æ€§</strong>ï¼ˆä¾‹å¦‚ï¼Œå¤šé¡¹å¼çš„æ¬¡æ•°æˆ–æ ·æ¡å‡½æ•°çš„å¤æ‚åº¦ï¼‰ï¼Œä»¥æœ€å°åŒ–<strong>æµ‹è¯•å‡æ–¹è¯¯å·®</strong>ï¼ˆMean
Squared Errorï¼‰ï¼Œè€Œè¿™åœ¨ç°å®ç”Ÿæ´»ä¸­æ˜¯æ— æ³•è§‚å¯Ÿåˆ°çš„ã€‚</p>
<ul>
<li><p><strong>The Curves (Slide 47):</strong> This slide is
central.</p>
<ul>
<li><p><strong>True Test MSE (Blue) çœŸå®æµ‹è¯•å‡æ–¹è¯¯å·®ï¼ˆè“è‰²ï¼‰:</strong>
This is the <em>real</em> error on new data. It has a
<strong>U-shape</strong>. Error is high for simple models (high bias),
drops as the model fits the data, and rises again for overly complex
models (high variance, or overfitting).
<strong>è¿™æ˜¯æ–°æ•°æ®çš„<em>çœŸå®</em>è¯¯å·®ã€‚å®ƒå‘ˆ</strong>U
å½¢**ã€‚å¯¹äºç®€å•æ¨¡å‹ï¼ˆé«˜åå·®ï¼‰ï¼Œè¯¯å·®è¾ƒé«˜ï¼›éšç€æ¨¡å‹æ‹Ÿåˆæ•°æ®çš„æ·±å…¥ï¼Œè¯¯å·®ä¼šä¸‹é™ï¼›å¯¹äºè¿‡äºå¤æ‚çš„æ¨¡å‹ï¼ˆé«˜æ–¹å·®æˆ–è¿‡æ‹Ÿåˆï¼‰ï¼Œè¯¯å·®ä¼šå†æ¬¡ä¸Šå‡ã€‚</p></li>
<li><p><strong>LOOCV (Black Dashed) &amp; 10-Fold CV (Orange)
LOOCVï¼ˆé»‘è‰²è™šçº¿ï¼‰å’Œ 10 å€ CVï¼ˆæ©™è‰²ï¼‰:</strong> These are our
<em>estimates</em> of the true test MSE. Notice how closely they track
the blue curve. The â€˜xâ€™ marks the minimum of the CV curve, which is our
<em>best guess</em> for the model with the minimum test MSE.
è¿™äº›æ˜¯æˆ‘ä»¬å¯¹çœŸå®æµ‹è¯• MSE
çš„<em>ä¼°è®¡</em>ã€‚è¯·æ³¨æ„å®ƒä»¬ä¸è“è‰²æ›²çº¿çš„å»åˆç¨‹åº¦ã€‚â€œxâ€æ ‡è®° CV
æ›²çº¿çš„æœ€å°å€¼ï¼Œè¿™æ˜¯æˆ‘ä»¬å¯¹å…·æœ‰æœ€å°æµ‹è¯• MSE
çš„æ¨¡å‹çš„<em>æœ€ä½³çŒœæµ‹</em>ã€‚</p></li>
</ul></li>
<li><p><strong>Scenario 1 (Slide 48):</strong> The true relationship is
non-linear. The right-hand plot shows that the test MSE (red curve) is
high for the simple linear model (blue square), but lower for the more
flexible smoothing splines (teal squares). CV helps us find the â€œsweet
spot.â€
çœŸå®çš„å…³ç³»æ˜¯éçº¿æ€§çš„ã€‚å³ä¾§å›¾è¡¨æ˜¾ç¤ºï¼Œå¯¹äºç®€å•çš„çº¿æ€§æ¨¡å‹ï¼ˆè“è‰²æ–¹å—ï¼‰ï¼Œæµ‹è¯•
MSEï¼ˆçº¢è‰²æ›²çº¿ï¼‰è¾ƒé«˜ï¼Œè€Œå¯¹äºæ›´çµæ´»çš„å¹³æ»‘æ ·æ¡å‡½æ•°ï¼ˆè“ç»¿è‰²æ–¹å—ï¼‰ï¼Œæµ‹è¯• MSE
è¾ƒä½ã€‚CV å¸®åŠ©æˆ‘ä»¬æ‰¾åˆ°â€œæœ€ä½³ç‚¹â€ã€‚</p></li>
<li><p><strong>Scenario 2 (Slide 49):</strong> The true relationship is
<strong>linear</strong>. Here, the test MSE (red curve) is
<em>lowest</em> for the simplest model (the linear one, blue square). CV
correctly identifies this, and its error estimate (blue square) is
lowest for that model.
çœŸå®çš„å…³ç³»æ˜¯<strong>çº¿æ€§</strong>çš„ã€‚åœ¨è¿™é‡Œï¼Œå¯¹äºæœ€ç®€å•çš„æ¨¡å‹ï¼ˆçº¿æ€§æ¨¡å‹ï¼Œè“è‰²æ–¹å—ï¼‰ï¼Œæµ‹è¯•
MSEï¼ˆçº¢è‰²æ›²çº¿ï¼‰<em>æœ€ä½</em>ã€‚CV
æ­£ç¡®åœ°è¯†åˆ«äº†è¿™ä¸€ç‚¹ï¼Œå¹¶ä¸”å…¶è¯¯å·®ä¼°è®¡ï¼ˆè“è‰²æ–¹å—ï¼‰æ˜¯è¯¥æ¨¡å‹ä¸­æœ€ä½çš„ã€‚</p></li>
<li><p><strong>Scenario 3 (Slide 50):</strong> The true relationship is
<strong>highly non-linear</strong>. The linear model (orange) is a very
poor fit. The test MSE (red curve) is minimized by the most flexible
model (teal square). CV again finds this.
çœŸå®çš„å…³ç³»æ˜¯<strong>é«˜åº¦éçº¿æ€§</strong>çš„ã€‚çº¿æ€§æ¨¡å‹ï¼ˆæ©™è‰²ï¼‰æ‹Ÿåˆåº¦å¾ˆå·®ã€‚æµ‹è¯•
MSEï¼ˆçº¢è‰²æ›²çº¿ï¼‰è¢«æœ€çµæ´»çš„æ¨¡å‹ï¼ˆè“ç»¿è‰²æ–¹å—ï¼‰æœ€å°åŒ–ã€‚CV
å†æ¬¡å‘ç°äº†è¿™ä¸€ç‚¹ã€‚</p></li>
<li><p><strong>Key Takeaway (Slide 51):</strong> We use CV to find the
<strong>tuning parameter</strong> (like polynomial degree) that
minimizes the test error. We care less about the <em>actual value</em>
of the CV error and more about <em>where its minimum is</em>. æˆ‘ä»¬ä½¿ç”¨
CV
æ¥æ‰¾åˆ°æœ€å°åŒ–æµ‹è¯•è¯¯å·®çš„<strong>è°ƒæ•´å‚æ•°</strong>ï¼ˆä¾‹å¦‚å¤šé¡¹å¼æ¬¡æ•°ï¼‰ã€‚æˆ‘ä»¬ä¸å¤ªå…³å¿ƒ
CV è¯¯å·®çš„<em>å®é™…å€¼</em>ï¼Œè€Œæ›´å…³å¿ƒ<em>å®ƒçš„æœ€å°å€¼</em>ã€‚</p></li>
</ul>
<h2 id="cv-for-classification-slides-55-61">CV for Classification
(Slides 55-61)</h2>
<p>This section shifts from regression (predicting a number, using MSE)
to classification (predicting a category, like â€œblueâ€ or â€œorangeâ€).
æœ¬èŠ‚ä»å›å½’ï¼ˆä½¿ç”¨ MSE
é¢„æµ‹æ•°å­—ï¼‰è½¬å‘åˆ†ç±»ï¼ˆé¢„æµ‹ç±»åˆ«ï¼Œä¾‹å¦‚â€œè“è‰²â€æˆ–â€œæ©™è‰²â€ï¼‰ã€‚</p>
<ul>
<li><strong>New Error Metric (Slide 55):</strong> We canâ€™t use MSE. A
natural choice is the <strong>classification error rate</strong>.
æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨ MSEã€‚ä¸€ä¸ªè‡ªç„¶çš„é€‰æ‹©æ˜¯<strong>åˆ†ç±»é”™è¯¯ç‡</strong>ã€‚
<ul>
<li><span class="math inline">\(Err_i = I(y_i \neq
\hat{y}_i^{(i)})\)</span></li>
<li>This is an <strong>indicator function</strong>: it is
<strong>1</strong> if the prediction for the <span
class="math inline">\(i\)</span>-th data point (when trained
<em>without</em> it) is wrong, and <strong>0</strong> if itâ€™s correct.
å¦‚æœå¯¹ç¬¬ <span class="math inline">\(i\)</span>
ä¸ªæ•°æ®ç‚¹çš„é¢„æµ‹ï¼ˆåœ¨æ²¡æœ‰å®ƒçš„æƒ…å†µä¸‹è®­ç»ƒæ—¶ï¼‰é”™è¯¯ï¼Œåˆ™ä¸º
<strong>1</strong>ï¼›å¦‚æœæ­£ç¡®ï¼Œåˆ™ä¸º <strong>0</strong>ã€‚</li>
<li>The final CV error is just the average of these 0s and 1s, giving
the total fraction of misclassified points: <span
class="math inline">\(CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n}
Err_i\)</span> æœ€ç»ˆçš„ CV è¯¯å·®å°±æ˜¯è¿™äº› 0 å’Œ 1
çš„å¹³å‡å€¼ï¼Œå³é”™è¯¯åˆ†ç±»ç‚¹çš„æ€»æ¯”ä¾‹ï¼š<span class="math inline">\(CV_{(n)} =
\frac{1}{n} \sum_{i=1}^{n} Err_i\)</span></li>
</ul></li>
<li><strong>The Example (Slides 56-61):</strong>
<ul>
<li><strong>Slides 56-58:</strong> We are shown a â€œtrueâ€ (but unknown)
non-linear boundary (purple dashed line) separating two classes. We then
try to <em>estimate</em> this boundary using logistic regression with
different polynomial degrees (degree 1, 2, 3, 4).
æˆ‘ä»¬çœ‹åˆ°äº†ä¸€æ¡â€œçœŸå®â€ï¼ˆä½†æœªçŸ¥ï¼‰çš„éçº¿æ€§è¾¹ç•Œï¼ˆç´«è‰²è™šçº¿ï¼‰ï¼Œå®ƒå°†ä¸¤ä¸ªç±»åˆ«åˆ†å¼€ã€‚ç„¶åï¼Œæˆ‘ä»¬å°è¯•ä½¿ç”¨ä¸åŒæ¬¡æ•°ï¼ˆ1ã€2ã€3ã€4
æ¬¡ï¼‰çš„é€»è¾‘å›å½’æ¥<em>ä¼°è®¡</em>è¿™æ¡è¾¹ç•Œã€‚</li>
<li><strong>Slides 59-60:</strong> This is a crucial point. In this
<em>simulated</em> example, we <em>do</em> know the true test error
rates. The true errors are [0.201, 0.197, <strong>0.160</strong>,
0.162]. The lowest error is for the 3rd-degree polynomial. But in a
real-world problem, <strong>we can never know these true
errors</strong>.
è¿™ä¸€ç‚¹è‡³å…³é‡è¦ã€‚åœ¨è¿™ä¸ª<em>æ¨¡æ‹Ÿ</em>ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬<em>ç¡®å®</em>çŸ¥é“çœŸå®çš„æµ‹è¯•é”™è¯¯ç‡ã€‚çœŸå®è¯¯å·®ä¸º
[0.201, 0.197, <strong>0.160</strong>,
0.162]ã€‚æœ€å°è¯¯å·®å‡ºç°åœ¨ä¸‰æ¬¡å¤šé¡¹å¼ä¸­ã€‚ä½†åœ¨å®é™…é—®é¢˜ä¸­ï¼Œ<strong>æˆ‘ä»¬æ°¸è¿œæ— æ³•çŸ¥é“è¿™äº›çœŸå®è¯¯å·®</strong>ã€‚</li>
<li><strong>Slide 61 (The Solution):</strong> This is the most important
image. It shows how CV <em>solves</em> the problem from slide 60.å±•ç¤ºäº†
CV å¦‚ä½•<em>è§£å†³</em>å¹»ç¯ç‰‡ 60 ä¸­çš„é—®é¢˜ã€‚
<ul>
<li><p><strong>Brown Curve (Test Error):</strong> This is the
<em>true</em> test error (from slide 59). We canâ€™t see this in practice.
Its minimum is at degree 3. è¿™æ˜¯<em>çœŸå®</em>çš„æµ‹è¯•è¯¯å·®ï¼ˆæ¥è‡ªå¹»ç¯ç‰‡
59ï¼‰ã€‚æˆ‘ä»¬åœ¨å®è·µä¸­çœ‹ä¸åˆ°å®ƒã€‚å®ƒçš„æœ€å°å€¼åœ¨ 3 æ¬¡æ–¹å¤„ã€‚</p></li>
<li><p><strong>Black Curve (10-fold CV Error):</strong> This is what we
<em>can</em> calculate. Itâ€™s our estimate of the test error.
<strong>Crucially, its minimum is also at degree 3.</strong></p></li>
<li><p><strong>é»‘è‰²æ›²çº¿ï¼ˆ10 å€ CV
è¯¯å·®ï¼‰ï¼š</strong>è¿™æ˜¯æˆ‘ä»¬<em>å¯ä»¥</em>è®¡ç®—å‡ºæ¥çš„ã€‚è¿™æ˜¯æˆ‘ä»¬å¯¹æµ‹è¯•è¯¯å·®çš„ä¼°è®¡ã€‚<strong>è‡³å…³é‡è¦çš„æ˜¯ï¼Œå®ƒçš„æœ€å°å€¼ä¹Ÿåœ¨
3 æ¬¡æ–¹å¤„ã€‚</strong></p></li>
<li><p>This proves that CV successfully found the best model (degree 3)
without ever seeing the <em>true</em> test error. The same logic is
shown for the KNN classifier on the right.</p></li>
<li><p>è¿™è¯æ˜ CV æˆåŠŸåœ°æ‰¾åˆ°äº†æœ€ä½³æ¨¡å‹ï¼ˆ3
æ¬¡æ–¹ï¼‰ï¼Œè€Œä»æœªçœ‹åˆ°<em>çœŸå®</em>çš„æµ‹è¯•è¯¯å·®ã€‚å³ä¾§çš„ KNN
åˆ†ç±»å™¨ä¹Ÿæ˜¾ç¤ºäº†ç›¸åŒçš„é€»è¾‘ã€‚</p></li>
</ul></li>
</ul></li>
</ul>
<h2 id="python-code-explained-slides-52-63-64">Python Code Explained
(Slides 52, 63, 64)</h2>
<p>The slides show how to <em>manually</em> implement K-fold CV. This is
great for understanding, even though libraries like
<code>GridSearchCV</code> can do this automatically.</p>
<ul>
<li><strong>KNN Regression (Slide 52):</strong>
<ol type="1">
<li><code>kfold = KFold(n_splits=10, ...)</code>: Creates an object that
knows how to split the data into 10 folds.</li>
<li><code>for n_k in neighbors:</code>: This is the <strong>outer
loop</strong> to test different <span class="math inline">\(K\)</span>
values (e.g., <span class="math inline">\(K\)</span>=1, 2, 3â€¦).</li>
<li><code>for train_index, test_index in kfold.split(X):</code>: This is
the <strong>inner loop</strong>. For a <em>single</em> <span
class="math inline">\(K\)</span>, it loops 10 times.</li>
<li>Inside the inner loop:
<ul>
<li>It splits the data into a 9-fold training set (<code>X_train</code>)
and a 1-fold test set (<code>X_test</code>).</li>
<li>It trains a <code>KNeighborsRegressor</code> on
<code>X_train</code>.</li>
<li>It makes predictions on <code>X_test</code> and calculates the error
(<code>mean_squared_error</code>).</li>
</ul></li>
<li><code>cv_errors.append(np.mean(mse_errors_k))</code>: After the
inner loop finishes 10 runs, it averages the 10 error scores for that
<span class="math inline">\(K\)</span> and stores it.</li>
<li>The final plot shows <code>cv_errors</code>
vs.Â <code>neighbors</code>, letting you pick the <span
class="math inline">\(K\)</span> with the lowest average error.</li>
</ol></li>
<li><strong>Logistic Regression Classification (Slides 63-64):</strong>
<ul>
<li>This code is almost identical, but with three key differences:
<ol type="1">
<li>The model is <code>LogisticRegression</code>.</li>
<li>It uses <code>PolynomialFeatures</code> to create new features
(<span class="math inline">\(X^2, X^3,\)</span> etc.) <em>inside</em>
the loop.</li>
<li>The error metric is <code>log_loss</code> (a common, more sensitive
metric than the simple 0/1 error rate).</li>
</ol></li>
<li>The plot on slide 64 shows the 10-fold CV error (using Log Loss)
vs.Â the Degree of the Polynomial. The minimum is clearly at
<strong>Degree = 3</strong>, matching the finding from slide 61.</li>
</ul></li>
</ul>
<h2 id="answering-the-key-questions-slides-54-65">Answering the Key
Questions (Slides 54 &amp; 65)</h2>
<p>Slide 65 asks two critical questions, which are answered directly by
the concepts on <strong>Slide 54 (Bias and variance
trade-off)</strong>.</p>
<h3 id="q1-how-does-k-affect-the-bias-and-variance-of-the-cv-error">Q1:
How does K affect the bias and variance of the CV error?</h3>
<p>This refers to <span class="math inline">\(K\)</span> in K-fold CV
(not to be confused with <span class="math inline">\(K\)</span> in KNN).
K å¦‚ä½•å½±å“ CV è¯¯å·®çš„åå·®å’Œæ–¹å·®ï¼Ÿ</p>
<ul>
<li><strong>Bias:</strong>
<ul>
<li><p><strong>LOOCV (K = n):</strong> This has <strong>very low
bias</strong>. The model is trained on <span
class="math inline">\(n-1\)</span> samples, which is <em>almost</em> the
full dataset. So, the error estimate <span
class="math inline">\(CV_{(n)}\)</span> is an almost-unbiased estimate
of the true test error. ** å®ƒçš„<strong>åå·®éå¸¸ä½</strong>ã€‚è¯¥æ¨¡å‹åŸºäº
<span class="math inline">\(n-1\)</span>
ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œè¿™å‡ ä¹æ˜¯æ•´ä¸ªæ•°æ®é›†ã€‚å› æ­¤ï¼Œè¯¯å·®ä¼°è®¡ <span
class="math inline">\(CV_{(n)}\)</span>
æ˜¯å¯¹çœŸå®æµ‹è¯•è¯¯å·®çš„å‡ ä¹æ— åä¼°è®¡ã€‚</p></li>
<li><p><strong>K-Fold (K &lt; n, e.g., K=10):</strong> This has
<strong>slightly higher bias</strong>. The models are trained on, for
example, 90% of the data. Because they are trained on less data, they
<em>might</em> perform slightly worse than a model trained on 100% of
the data. This â€œpessimismâ€ is the source of the bias.
<strong>åå·®ç•¥é«˜</strong>ã€‚ä¾‹å¦‚ï¼Œè¿™äº›æ¨¡å‹æ˜¯åŸºäº 90%
çš„æ•°æ®è¿›è¡Œè®­ç»ƒçš„ã€‚ç”±äºå®ƒä»¬åŸºäºè¾ƒå°‘çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤å®ƒä»¬çš„æ€§èƒ½<em>å¯èƒ½</em>ä¼šæ¯”åŸºäº
100% æ•°æ®è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹ç•¥å·®ã€‚è¿™ç§â€œæ‚²è§‚â€æ­£æ˜¯åå·®çš„æ ¹æºã€‚</p></li>
</ul></li>
<li><strong>Variance:</strong>
<ul>
<li><p><strong>LOOCV (K = n):</strong> This has <strong>very high
variance</strong>. You are training <span
class="math inline">\(n\)</span> models that are <em>almost
identical</em> (they only differ by one data point). Averaging <span
class="math inline">\(n\)</span> highly-correlated error estimates
doesnâ€™t reduce the variance much. This makes the final <span
class="math inline">\(CV_{(n)}\)</span> estimate unstable.
<strong>è¿™ç§æ¨¡å‹çš„æ–¹å·®</strong>éå¸¸é«˜**ã€‚æ‚¨æ­£åœ¨è®­ç»ƒ <span
class="math inline">\(n\)</span>
ä¸ª<em>å‡ ä¹ç›¸åŒ</em>çš„æ¨¡å‹ï¼ˆå®ƒä»¬åªæœ‰ä¸€ä¸ªæ•°æ®ç‚¹ä¸åŒï¼‰ã€‚å¯¹ <span
class="math inline">\(n\)</span>
ä¸ªé«˜åº¦ç›¸å…³çš„è¯¯å·®ä¼°è®¡å–å¹³å‡å€¼å¹¶ä¸èƒ½æ˜¾è‘—é™ä½æ–¹å·®ã€‚è¿™ä½¿å¾—æœ€ç»ˆçš„ <span
class="math inline">\(CV_{(n)}\)</span> ä¼°è®¡å€¼ä¸ç¨³å®šã€‚</p></li>
<li><p><strong>K-Fold (K &lt; n, e.g., K=10):</strong> This has
<strong>much lower variance</strong>. The 10 models are trained on more
different â€œchunksâ€ of data (they overlap less). Their error estimates
are less correlated, and averaging 10 less-correlated numbers gives a
much more stable (low-variance) final estimate.
<strong>è¿™ç§æ¨¡å‹çš„æ–¹å·®</strong>éå¸¸ä½**ã€‚è¿™ 10
ä¸ªæ¨¡å‹åŸºäºæ›´å¤šä¸åŒçš„æ•°æ®â€œå—â€è¿›è¡Œè®­ç»ƒï¼ˆå®ƒä»¬é‡å è¾ƒå°‘ï¼‰ã€‚å®ƒä»¬çš„è¯¯å·®ä¼°è®¡å€¼ç›¸å…³æ€§è¾ƒä½ï¼Œå¯¹
10
ä¸ªç›¸å…³æ€§è¾ƒä½çš„æ•°å–å¹³å‡å€¼å¯ä»¥å¾—åˆ°æ›´ç¨³å®šï¼ˆä½æ–¹å·®ï¼‰çš„æœ€ç»ˆä¼°è®¡å€¼ã€‚</p></li>
</ul></li>
</ul>
<p><strong>Conclusion (The Trade-off):</strong> We prefer <strong>K-fold
CV (K=5 or 10)</strong> over LOOCV. It gives a much more stable
(low-variance) estimate, and we are willing to accept a tiny increase in
bias to get it. æˆ‘ä»¬æ›´å–œæ¬¢<strong>K å€äº¤å‰éªŒè¯ï¼ˆK=5 æˆ–
10ï¼‰</strong>ï¼Œè€Œä¸æ˜¯å•å€äº¤å‰éªŒè¯ã€‚å®ƒèƒ½ç»™å‡ºæ›´ç¨³å®šï¼ˆä½æ–¹å·®ï¼‰çš„ä¼°è®¡å€¼ï¼Œå¹¶ä¸”æˆ‘ä»¬æ„¿æ„æ¥å—åå·®çš„è½»å¾®å¢åŠ æ¥è·å¾—å®ƒã€‚</p>
<h3
id="q2-does-cross-validation-over-estimate-or-under-estimate-the-true-test-error">Q2:
Does Cross Validation over-estimate or under-estimate the true test
error?</h3>
<p>äº¤å‰éªŒè¯ä¼šé«˜ä¼°è¿˜æ˜¯ä½ä¼°çœŸå®æµ‹è¯•è¯¯å·®ï¼Ÿ</p>
<p>Based on the bias discussion above:</p>
<p>Cross-validation (especially K-fold) generally <strong>over-estimates
the true test error</strong>. äº¤å‰éªŒè¯ï¼ˆå°¤å…¶æ˜¯ K
å€äº¤å‰éªŒè¯ï¼‰é€šå¸¸ä¼š<strong>é«˜ä¼°çœŸå®æµ‹è¯•è¯¯å·®</strong>ã€‚</p>
<p><strong>Reasoning:</strong> 1. The â€œtrue test errorâ€ is the error of
a model trained on the <em>entire dataset</em> (<span
class="math inline">\(n\)</span> samples). 2. K-fold CV trains its
models on <em>subsets</em> of the data (e.g., <span
class="math inline">\(n \times (K-1)/K\)</span> samples). 3. Since these
models are trained on <em>less</em> data, they are (on average) slightly
worse than the final model trained on all the data. 4. Because the CV
models are slightly worse, their error rates will be slightly
<em>higher</em>. 5. Therefore, the final CV error score is a slightly
â€œpessimisticâ€ or high estimate. This is considered a good thing, as itâ€™s
a <em>conservative</em> estimate of how our model will perform.
<strong>ç†ç”±ï¼š</strong> 1.
â€œçœŸå®æµ‹è¯•è¯¯å·®â€æ˜¯æŒ‡åœ¨<em>æ•´ä¸ªæ•°æ®é›†</em>ï¼ˆ<span
class="math inline">\(n\)</span> ä¸ªæ ·æœ¬ï¼‰ä¸Šè®­ç»ƒçš„æ¨¡å‹çš„è¯¯å·®ã€‚ 2. K
æŠ˜äº¤å‰éªŒè¯ (K-fold CV) åœ¨æ•°æ®<em>å­é›†</em>ä¸Šè®­ç»ƒå…¶æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œ<span
class="math inline">\(n \times (K-1)/K\)</span> ä¸ªæ ·æœ¬ï¼‰ã€‚ 3.
ç”±äºè¿™äº›æ¨¡å‹åŸºäº<em>è¾ƒå°‘</em>çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤å®ƒä»¬ï¼ˆå¹³å‡è€Œè¨€ï¼‰æ¯”åŸºäºæ‰€æœ‰æ•°æ®è®­ç»ƒçš„æœ€ç»ˆæ¨¡å‹ç•¥å·®ã€‚
4. ç”±äº CV æ¨¡å‹ç•¥å·®ï¼Œå…¶é”™è¯¯ç‡ä¼šç•¥é«˜<em>ã€‚ 5. å› æ­¤ï¼Œæœ€ç»ˆçš„ CV
é”™è¯¯ç‡æ˜¯ä¸€ä¸ªç•¥å¾®â€œæ‚²è§‚â€æˆ–åé«˜çš„ä¼°è®¡ã€‚è¿™è¢«è®¤ä¸ºæ˜¯ä¸€ä»¶å¥½äº‹ï¼Œå› ä¸ºå®ƒæ˜¯å¯¹æ¨¡å‹æ€§èƒ½çš„</em>ä¿å®ˆ*ä¼°è®¡ã€‚</p>
<h1 id="summary-of-bootstrap">6. Summary of Bootstrap</h1>
<p>Bootstrap is a <strong>resampling technique</strong> used to estimate
the <strong>uncertainty</strong> (like standard error or confidence
intervals) of a statistic. Its key idea is to <strong>treat your
original data sample as a proxy for the true population</strong>. It
then simulates the process of drawing new samples by instead
<strong>sampling <em>with replacement</em></strong> from your original
sample. Bootstrap
æ˜¯ä¸€ç§<strong>é‡é‡‡æ ·æŠ€æœ¯</strong>ï¼Œç”¨äºä¼°è®¡ç»Ÿè®¡æ•°æ®çš„<strong>ä¸ç¡®å®šæ€§</strong>ï¼ˆä¾‹å¦‚æ ‡å‡†è¯¯å·®æˆ–ç½®ä¿¡åŒºé—´ï¼‰ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯<strong>å°†åŸå§‹æ•°æ®æ ·æœ¬è§†ä¸ºçœŸå®æ€»ä½“çš„æ›¿ä»£æ ·æœ¬</strong>ã€‚ç„¶åï¼Œå®ƒé€šè¿‡ä»åŸå§‹æ ·æœ¬ä¸­è¿›è¡Œ<strong>æœ‰æ”¾å›çš„</strong>æŠ½æ ·æ¥æ¨¡æ‹ŸæŠ½å–æ–°æ ·æœ¬çš„è¿‡ç¨‹ã€‚</p>
<h3 id="the-problem">The Problem</h3>
<p>You have a single data sample (e.g., <span
class="math inline">\(n=100\)</span> people) and you calculate a
statistic, like the sample mean (<span
class="math inline">\(\bar{x}\)</span>) or a regression coefficient
(<span class="math inline">\(\hat{\beta}\)</span>). You want to know how
<em>accurate</em> this statistic is. How much would it vary if you could
repeat your experiment many times? This variation is measured by the
<strong>standard error (SE)</strong>. æ‚¨æœ‰ä¸€ä¸ªæ•°æ®æ ·æœ¬ï¼ˆä¾‹å¦‚ï¼Œ<span
class="math inline">\(n=100\)</span>
äººï¼‰ï¼Œå¹¶è®¡ç®—ä¸€ä¸ªç»Ÿè®¡æ•°æ®ï¼Œä¾‹å¦‚æ ·æœ¬å‡å€¼ (<span
class="math inline">\(\bar{x}\)</span>) æˆ–å›å½’ç³»æ•° (<span
class="math inline">\(\hat{\beta}\)</span>)ã€‚æ‚¨æƒ³çŸ¥é“è¿™ä¸ªç»Ÿè®¡æ•°æ®çš„<em>å‡†ç¡®åº¦</em>ã€‚å¦‚æœå¯ä»¥å¤šæ¬¡é‡å¤å®éªŒï¼Œå®ƒä¼šæœ‰å¤šå°‘å˜åŒ–ï¼Ÿè¿™ç§å˜åŒ–å¯ä»¥ç”¨<strong>æ ‡å‡†è¯¯å·®
(SE)</strong> æ¥è¡¡é‡ã€‚</p>
<h3 id="the-bootstrap-solution">The Bootstrap Solution</h3>
<p>Since you canâ€™t re-run the whole experiment, you <em>simulate</em> it
using the one sample you have.
ç”±äºæ‚¨æ— æ³•é‡æ–°è¿è¡Œæ•´ä¸ªå®éªŒï¼Œå› æ­¤æ‚¨å¯ä»¥ä½¿ç”¨ç°æœ‰çš„ä¸€ä¸ªæ ·æœ¬è¿›è¡Œâ€œæ¨¡æ‹Ÿâ€ã€‚</p>
<p><strong>The Process:</strong> 1. <strong>Original Sample (<span
class="math inline">\(Z\)</span>) åŸå§‹æ ·æœ¬ (<span
class="math inline">\(Z\)</span>):</strong> You have your one dataset
with <span class="math inline">\(n\)</span> observations. 2.
<strong>Bootstrap Sample (<span class="math inline">\(Z^{*1}\)</span>)
Bootstrap æ ·æœ¬ (<span class="math inline">\(Z^{*1}\)</span>):</strong>
Create a <em>new</em> dataset of size <span
class="math inline">\(n\)</span> by randomly pulling observations from
your original sample <em>with replacement</em>. (This means some
original observations will be picked multiple times, and some not at
all). 3. <strong>Calculate Statistic (<span
class="math inline">\(\hat{\theta}^{*1}\)</span>) è®¡ç®—ç»Ÿè®¡é‡ (<span
class="math inline">\(\hat{\theta}^{*1}\)</span>):</strong> Calculate
your statistic of interest (e.g., the mean, <span
class="math inline">\(\hat{\alpha}\)</span>, regression coefficients) on
this new bootstrap sample. 4. <strong>Repeat é‡å¤:</strong> Repeat steps
2 and 3 a large number of times (<span class="math inline">\(B\)</span>,
e.g., <span class="math inline">\(B=1000\)</span>). This gives you <span
class="math inline">\(B\)</span> bootstrap statistics: <span
class="math inline">\(\hat{\theta}^{*1}, \hat{\theta}^{*2}, ...,
\hat{\theta}^{*B}\)</span>. 5. <strong>Analyze the Bootstrap
Distribution åˆ†æè‡ªä¸¾åˆ†å¸ƒ:</strong> This collection of <span
class="math inline">\(B\)</span> statistics is your â€œbootstrap
distribution.â€ * <strong>Standard Error æ ‡å‡†è¯¯å·®:</strong> The
<strong>standard deviation</strong> of this bootstrap distribution is
your estimate of the <strong>standard error</strong> of your original
statistic. * <strong>Confidence Interval ç½®ä¿¡åŒºé—´:</strong> A 95%
confidence interval can be found by taking the <strong>2.5th and 97.5th
percentiles</strong> of this bootstrap distribution.</p>
<p><strong>Why use it?</strong> Itâ€™s powerful because it doesnâ€™t rely on
strong theoretical assumptions (like data being normally distributed).
It can be applied to almost <em>any</em> statistic, even very complex
ones (like the prediction from a KNN model), for which a simple
mathematical formula for standard error doesnâ€™t exist.
å®ƒéå¸¸å¼ºå¤§ï¼Œå› ä¸ºå®ƒä¸ä¾èµ–äºä¸¥æ ¼çš„ç†è®ºå‡è®¾ï¼ˆä¾‹å¦‚æ•°æ®æœä»æ­£æ€åˆ†å¸ƒï¼‰ã€‚å®ƒå‡ ä¹å¯ä»¥åº”ç”¨äº<em>ä»»ä½•</em>ç»Ÿè®¡æ•°æ®ï¼Œå³ä½¿æ˜¯éå¸¸å¤æ‚çš„ç»Ÿè®¡æ•°æ®ï¼ˆä¾‹å¦‚
KNN æ¨¡å‹çš„é¢„æµ‹ï¼‰ï¼Œå› ä¸ºè¿™äº›ç»Ÿè®¡æ•°æ®æ²¡æœ‰ç®€å•çš„æ ‡å‡†è¯¯å·®æ•°å­¦å…¬å¼ã€‚</p>
<h2 id="mathematical-understanding">Mathematical Understanding</h2>
<p>The core idea is to use the <strong>empirical distribution</strong>
(your sample) as an estimate for the true <strong>population
distribution</strong>.
å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨<strong>ç»éªŒåˆ†å¸ƒ</strong>ï¼ˆä½ çš„æ ·æœ¬ï¼‰æ¥ä¼°è®¡çœŸå®çš„<strong>æ€»ä½“åˆ†å¸ƒ</strong>ã€‚</p>
<h3 id="example-estimating-alpha">Example: Estimating <span
class="math inline">\(\alpha\)</span></h3>
<p>Your slides provide an example of finding the <span
class="math inline">\(\alpha\)</span> that minimizes the variance of a
portfolio, <span class="math inline">\(var(\alpha X +
(1-\alpha)Y)\)</span>. ç”¨äºè®¡ç®—ä½¿æŠ•èµ„ç»„åˆæ–¹å·®æœ€å°åŒ–çš„ <span
class="math inline">\(\alpha\)</span>ï¼Œå³ <span
class="math inline">\(var(\alpha X + (1-\alpha)Y)\)</span>ã€‚</p>
<ol type="1">
<li><p><strong>True Population Parameter (<span
class="math inline">\(\alpha\)</span>) çœŸå®æ€»ä½“å‚æ•° (<span
class="math inline">\(\alpha\)</span>):</strong> The <em>true</em> <span
class="math inline">\(\alpha\)</span> is a function of the
<em>population</em> variances and covariance: <em>çœŸå®</em> <span
class="math inline">\(\alpha\)</span>
æ˜¯<em>æ€»ä½“</em>æ–¹å·®å’Œåæ–¹å·®çš„å‡½æ•°ï¼š <span class="math display">\[\alpha
= \frac{\sigma_Y^2 - \sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 -
2\sigma_{XY}}\]</span> We can never know this value exactly unless we
know the entire population.
é™¤éæˆ‘ä»¬äº†è§£æ•´ä¸ªæ€»ä½“ï¼Œå¦åˆ™æˆ‘ä»¬æ°¸è¿œæ— æ³•å‡†ç¡®çŸ¥é“è¿™ä¸ªå€¼ã€‚</p></li>
<li><p><strong>Sample Statistic (<span
class="math inline">\(\hat{\alpha}\)</span>) æ ·æœ¬ç»Ÿè®¡é‡ (<span
class="math inline">\(\hat{\alpha}\)</span>):</strong> We
<em>estimate</em> <span class="math inline">\(\alpha\)</span> using our
sample, creating the statistic <span
class="math inline">\(\hat{\alpha}\)</span> by plugging in our
<em>sample</em> variances and covariance: æˆ‘ä»¬ä½¿ç”¨æ ·æœ¬<em>ä¼°è®¡</em>
<span
class="math inline">\(\alpha\)</span>ï¼Œé€šè¿‡ä»£å…¥<em>æ ·æœ¬</em>æ–¹å·®å’Œåæ–¹å·®æ¥åˆ›å»ºç»Ÿè®¡é‡
<span class="math inline">\(\hat{\alpha}\)</span>ï¼š <span
class="math display">\[\hat{\alpha} = \frac{\hat{\sigma}_Y^2 -
\hat{\sigma}_{XY}}{\hat{\sigma}_X^2 + \hat{\sigma}_Y^2 -
2\hat{\sigma}_{XY}}\]</span> This <span
class="math inline">\(\hat{\alpha}\)</span> is just <em>one number</em>
from our single sample. How confident are we in it? We need its standard
error, <span class="math inline">\(SE(\hat{\alpha})\)</span>. è¿™ä¸ª <span
class="math inline">\(\hat{\alpha}\)</span>
åªæ˜¯æˆ‘ä»¬å•ä¸ªæ ·æœ¬ä¸­çš„ä¸€ä¸ªæ•°å­—ã€‚æˆ‘ä»¬å¯¹å®ƒçš„ç½®ä¿¡åº¦æœ‰å¤šé«˜ï¼Ÿæˆ‘ä»¬éœ€è¦å®ƒçš„æ ‡å‡†è¯¯å·®ï¼Œ<span
class="math inline">\(SE(\hat{\alpha})\)</span>ã€‚</p></li>
<li><p><strong>Bootstrap Statistic (<span
class="math inline">\(\hat{\alpha}^*\)</span>) è‡ªä¸¾ç»Ÿè®¡é‡ (<span
class="math inline">\(\hat{\alpha}^*\)</span>):</strong> We apply the
bootstrap process:</p>
<ul>
<li>Create a bootstrap sample (by resampling with replacement).
åˆ›å»ºä¸€ä¸ªè‡ªä¸¾æ ·æœ¬ï¼ˆé€šè¿‡æ”¾å›é‡é‡‡æ ·ï¼‰ã€‚</li>
<li>Calculate <span class="math inline">\(\hat{\alpha}^*\)</span> using
the sample (co)variances of this <em>new bootstrap sample</em>.
ä½¿ç”¨è¿™ä¸ª<em>æ–°è‡ªä¸¾æ ·æœ¬</em>çš„æ ·æœ¬ï¼ˆåï¼‰æ–¹å·®è®¡ç®— <span
class="math inline">\(\hat{\alpha}^*\)</span>ã€‚</li>
<li>Repeat <span class="math inline">\(B\)</span> times to get <span
class="math inline">\(B\)</span> values: <span
class="math inline">\(\hat{\alpha}^{*1}, \hat{\alpha}^{*2}, ...,
\hat{\alpha}^{*B}\)</span>. é‡å¤ <span class="math inline">\(B\)</span>
æ¬¡ï¼Œå¾—åˆ° <span class="math inline">\(B\)</span> ä¸ªå€¼ï¼š<span
class="math inline">\(\hat{\alpha}^{*1}, \hat{\alpha}^{*2}, ...,
\hat{\alpha}^{*B}\)</span>ã€‚</li>
</ul></li>
<li><p><strong>Estimating the Standard Error ä¼°ç®—æ ‡å‡†è¯¯å·®:</strong> The
standard error of our original estimate <span
class="math inline">\(\hat{\alpha}\)</span> is <em>estimated</em> by the
standard deviation of all our bootstrap estimates: æˆ‘ä»¬åŸå§‹ä¼°è®¡å€¼ <span
class="math inline">\(\hat{\alpha}\)</span>
çš„æ ‡å‡†è¯¯å·®æ˜¯é€šè¿‡æ‰€æœ‰è‡ªä¸¾ä¼°è®¡å€¼çš„æ ‡å‡†å·®æ¥â€œä¼°ç®—â€çš„ï¼š <span
class="math display">\[SE_{boot}(\hat{\alpha}) = \sqrt{\frac{1}{B-1}
\sum_{j=1}^{B} (\hat{\alpha}^{*j} - \bar{\alpha}^*)^2}\]</span> where
<span class="math inline">\(\bar{\alpha}^*\)</span> is the average of
all <span class="math inline">\(B\)</span> bootstrap estimates. <span
class="math inline">\(\bar{\alpha}^*\)</span> æ˜¯æ‰€æœ‰ <span
class="math inline">\(B\)</span> ä¸ªè‡ªä¸¾ä¼°è®¡å€¼çš„å¹³å‡å€¼ã€‚</p></li>
</ol>
<p>The slides (p.Â 73, 77-78) show this visually. The â€œsampling from
populationâ€ histogram (left) is the <em>true</em> sampling distribution,
which we can only create in a simulation. The â€œBootstrapâ€ histogram
(right) is the bootstrap distribution created from <em>one</em> sample.
They look very similar, which shows the method works.
â€œä»æ€»ä½“æŠ½æ ·â€ç›´æ–¹å›¾ï¼ˆå·¦å›¾ï¼‰æ˜¯<em>çœŸå®</em>çš„æŠ½æ ·åˆ†å¸ƒï¼Œæˆ‘ä»¬åªèƒ½åœ¨æ¨¡æ‹Ÿä¸­åˆ›å»ºå®ƒã€‚â€œè‡ªä¸¾â€ç›´æ–¹å›¾ï¼ˆå³å›¾ï¼‰æ˜¯ä»<em>ä¸€ä¸ª</em>æ ·æœ¬åˆ›å»ºçš„è‡ªä¸¾åˆ†å¸ƒã€‚å®ƒä»¬çœ‹èµ·æ¥éå¸¸ç›¸ä¼¼ï¼Œè¿™è¡¨æ˜è¯¥æ–¹æ³•æœ‰æ•ˆã€‚</p>
<h2 id="code-analysis">Code Analysis</h2>
<h3 id="r-alpha-example-slides-75-77">R: <span
class="math inline">\(\alpha\)</span> Example (Slides 75 &amp; 77)</h3>
<ul>
<li><strong>Slide 75 (<code>The R code</code>): This is a SIMULATION,
not Bootstrap.</strong>
<ul>
<li><code>for(i in 1:m)&#123;...&#125;</code>: This loop runs <code>m=1000</code>
times.</li>
<li><code>returns &lt;- rmvnorm(...)</code>: <strong>Inside the
loop,</strong> it draws a <em>brand new sample</em> from the <em>true
population</em> every time.</li>
<li><code>alpha[i] &lt;- ...</code>: It calculates <span
class="math inline">\(\hat{\alpha}\)</span> for each new sample.</li>
<li><strong>Purpose:</strong> This code shows the <em>true sampling
distribution</em> of <span class="math inline">\(\hat{\alpha}\)</span>
(the â€œHistogram of alphaâ€). You can only do this if you know the true
population, as in a simulation.</li>
</ul></li>
<li><strong>Slide 77 (<code>The R code</code>): This IS
Bootstrap.</strong>
<ul>
<li><code>returns &lt;- rmvnorm(...)</code>: <strong>Outside the
loop,</strong> this is done <em>only once</em> to get <em>one</em>
original sample.</li>
<li><code>for(i in 1:B)&#123;...&#125;</code>: This is the bootstrap loop.</li>
<li><code>sample(1:nrow(returns), n, replace = T)</code>: <strong>This
is the key line.</strong> It randomly selects row numbers <em>with
replacement</em> from the <em>single</em> <code>returns</code>
dataset.</li>
<li><code>returns_boot &lt;- returns[sample(...), ]</code>: This creates
the bootstrap sample.</li>
<li><code>alpha_bootstrap[i] &lt;- ...</code>: It calculates <span
class="math inline">\(\hat{\alpha}^*\)</span> on the
<code>returns_boot</code> sample.</li>
<li><strong>Purpose:</strong> This code generates the <em>bootstrap
distribution</em> (the â€œBootstrapâ€ histogram on slide 78) to
<em>estimate</em> the true sampling distribution.</li>
</ul></li>
</ul>
<h3 id="r-linear-regression-example-slides-79-81">R: Linear Regression
Example (Slides 79 &amp; 81)</h3>
<ul>
<li><strong>Slide 79:</strong>
<ul>
<li><code>boot.fn &lt;- function(data, index)&#123; ... &#125;</code>: Defines a
function that the <code>boot</code> package needs. It takes data and an
<code>index</code> vector.</li>
<li><code>lm(mpg~horsepower, data=data, subset=index)</code>: This is
the core. It fits a linear model <em>only</em> on the data points
specified by the <code>index</code>. The <code>boot</code> function will
automatically supply this <code>index</code> as a
resampled-with-replacement vector.</li>
<li><code>boot(Auto, boot.fn, R=1000)</code>: This runs the bootstrap.
It calls <code>boot.fn</code> 1000 times, each time with a new resampled
<code>index</code>, and collects the coefficients.</li>
</ul></li>
<li><strong>Slide 81:</strong>
<ul>
<li><code>summary(lm(...))</code>: Shows the standard output. The â€œStd.
Errorâ€ column (e.g., 0.860, 0.006) is calculated using <em>mathematical
theory</em>.</li>
<li><code>boot.res</code>: Shows the bootstrap output. The â€œstd. errorâ€
column (e.g., 0.841, 0.007) is the <strong>standard deviation of the
1000 bootstrap estimates</strong>.</li>
<li><strong>Main Point:</strong> The standard errors from the bootstrap
are very close to the theoretical ones. This confirms the uncertainty.
If the model assumptions were violated, the bootstrap SE would be more
trustworthy.</li>
<li>The histograms show the bootstrap distributions for the intercept
(<code>t1*</code>) and the slope (<code>t2*</code>). The arrows show the
95% percentile confidence interval.</li>
</ul></li>
</ul>
<h3 id="python-knn-regression-example-slide-80">Python: KNN Regression
Example (Slide 80)</h3>
<p>This shows how to get a confidence interval for a <em>single
prediction</em>.</p>
<ul>
<li><code>for i in range(n_bootstraps):</code>: The bootstrap loop.</li>
<li><code>indices = np.random.choice(train_samples.shape[0], train_samples.shape[0], replace=True)</code>:
<strong>This is the key line</strong> in Python (like
<code>sample</code> in R). It gets a new set of indices with
replacement.</li>
<li><code>X_boot, y_boot = ...</code>: Creates the bootstrap
sample.</li>
<li><code>model.fit(X_boot, y_boot)</code>: A <em>new</em> KNN model is
trained on this bootstrap sample.</li>
<li><code>bootstrap_preds.append(model.predict(predict_point))</code>:
The model (trained on <span class="math inline">\(Z^{*i}\)</span>) makes
a prediction for the <em>same</em> fixed point. This is repeated 1000
times.</li>
<li><strong>Result:</strong> You get a <em>distribution of
predictions</em> for that one point. The 2.5th and 97.5th percentiles of
this distribution give you a 95% confidence interval <em>for that
specific prediction</em>. ä½ ä¼šå¾—åˆ°è¯¥ç‚¹çš„<em>é¢„æµ‹åˆ†å¸ƒ</em>ã€‚è¯¥åˆ†å¸ƒçš„ 2.5
å’Œ 97.5 ç™¾åˆ†ä½æ•°ä¸ºè¯¥ç‰¹å®šé¢„æµ‹æä¾›äº† 95% çš„ç½®ä¿¡åŒºé—´ã€‚</li>
</ul>
<h3 id="python-knn-on-auto-data-slide-82">Python: KNN on Auto data
(Slide 82)</h3>
<ul>
<li><strong>BE CAREFUL:</strong> This slide <strong>does NOT show
Bootstrap</strong>. It shows <strong>K-Fold Cross-Validation
(CV)</strong>.</li>
<li><strong>Purpose:</strong> The goal here is <em>not</em> to find
uncertainty. The goal is to find the <strong>best
hyperparameter</strong> (the best value for <span
class="math inline">\(k\)</span>, the number of neighbors).</li>
<li><strong>Method:</strong>
<ul>
<li><code>kf = KFold(n_splits=10)</code>: Splits the data into 10 chunks
(â€œfoldsâ€).</li>
<li><code>for train_index, test_index in kf.split(X):</code>: It loops
10 times. Each time, it trains on 9 chunks and tests on 1 chunk.</li>
</ul></li>
<li><strong>Key Difference for Exam:</strong>
<ul>
<li><strong>Bootstrap:</strong> Samples <em>with replacement</em> to
estimate <strong>uncertainty/standard error</strong>.</li>
<li><strong>Cross-Validation:</strong> Splits data <em>without
replacement</em> into <span class="math inline">\(K\)</span> folds to
estimate model <strong>performance/prediction error</strong> and tune
hyperparameters.</li>
<li><strong>è‡ªä¸¾æ³•</strong>ï¼šä½¿ç”¨<em>æœ‰æ”¾å›</em>çš„æ ·æœ¬æ¥ä¼°è®¡<strong>ä¸ç¡®å®šæ€§/æ ‡å‡†è¯¯å·®</strong>ã€‚</li>
<li><strong>äº¤å‰éªŒè¯</strong>ï¼šå°†æ•°æ®<em>æ— æ”¾å›</em>åœ°åˆ†æˆ <span
class="math inline">\(K\)</span>
ä»½ï¼Œä»¥ä¼°è®¡æ¨¡å‹<strong>æ€§èƒ½/é¢„æµ‹è¯¯å·®</strong>å¹¶è°ƒæ•´è¶…å‚æ•°ã€‚</li>
</ul></li>
</ul>
<h1
id="the-mathematical-theory-of-bootstrap-and-the-extension-to-cross-validation-cv.">7.
The mathematical theory of Bootstrap and the extension to
Cross-Validation (CV).</h1>
<h2 id="code-analysis-bootstrap-for-a-knn-prediction-slide-85">1. Code
Analysis: Bootstrap for a KNN Prediction (Slide 85)</h2>
<p>This Python code shows a different use of bootstrap: <strong>finding
the confidence interval for a single prediction</strong>, not for a
model coefficient.</p>
<ul>
<li><strong>Goal:</strong> To estimate the uncertainty of a KNN modelâ€™s
prediction for a <em>specific</em> new data point
(<code>predict_point</code>).</li>
<li><strong>Process:</strong>
<ol type="1">
<li><strong>Train Full Model:</strong> A KNN model (<code>knn</code>) is
first trained on the <em>entire</em> dataset. It makes one prediction
(<code>knpred</code>) for <code>predict_point</code>. This is our <span
class="math inline">\(\hat{f}(x_0)\)</span>.</li>
<li><strong>Bootstrap Loop
(<code>for i in range(n_bootstraps)</code>):</strong>
<ul>
<li><code>indices = np.random.choice(...)</code>: <strong>This is the
core bootstrap step.</strong> It creates a new list of indices by
sampling <em>with replacement</em> from the original data.</li>
<li><code>X_boot, y_boot = ...</code>: This creates the new bootstrap
dataset (<span class="math inline">\(Z^{*i}\)</span>).</li>
<li><code>km.fit(X_boot, y_boot)</code>: A <em>new</em> KNN model
(<code>km</code>) is trained <em>only</em> on this bootstrap
sample.</li>
<li><code>bootstrap_preds.append(km.predict(predict_point))</code>: This
newly trained model makes a prediction for the <em>same</em>
<code>predict_point</code>. This value is <span
class="math inline">\(\hat{f}^{*i}(x_0)\)</span>.</li>
</ul></li>
<li><strong>Analyze Distribution:</strong> After 1000 loops,
<code>bootstrap_preds</code> contains 1000 different predictions for the
same point.</li>
<li><strong>Confidence Interval:</strong>
<ul>
<li><code>np.percentile(bootstrap_preds, [2.5, 97.5])</code>: This finds
the 2.5th and 97.5th percentiles of the 1000 bootstrap predictions.</li>
<li>The resulting <code>[lower_bound, upper_bound]</code> (e.g.,
<code>[13.70, 15.70]</code>) forms the 95% confidence interval for the
prediction.</li>
</ul></li>
</ol></li>
<li><strong>Histogram Plot:</strong> The plot on the right visually
confirms this. It shows the distribution of the 1000 bootstrap
predictions, with the 95% confidence interval marked by the red dashed
lines.</li>
</ul>
<h2
id="mathematical-understanding-why-does-bootstrap-work-slides-87-88">2.
Mathematical Understanding: Why Does Bootstrap Work? (Slides 87-88)</h2>
<p>This is the theoretical justification for the entire method. Itâ€™s
based on an analogy. è¿™æ˜¯æ•´ä¸ªæ–¹æ³•çš„ç†è®ºä¾æ®ã€‚å®ƒåŸºäºä¸€ä¸ªç±»æ¯”ã€‚</p>
<h3 id="the-true-world-slide-87-top">The â€œTrueâ€ World (Slide 87,
Top)</h3>
<ul>
<li><p><strong>Population:</strong> There is a true, unknown population
distribution <span class="math inline">\(F\)</span>.
å­˜åœ¨ä¸€ä¸ªçœŸå®çš„ã€æœªçŸ¥çš„æ€»ä½“åˆ†å¸ƒ <span
class="math inline">\(F\)</span>ã€‚</p></li>
<li><p><strong>Parameter:</strong> We want to know a true parameter,
<span class="math inline">\(\theta\)</span>, which is a function of
<span class="math inline">\(F\)</span> (e.g., the true population mean).
æˆ‘ä»¬æƒ³çŸ¥é“ä¸€ä¸ªçœŸå®çš„å‚æ•° <span
class="math inline">\(\theta\)</span>ï¼Œå®ƒæ˜¯ <span
class="math inline">\(F\)</span>
çš„å‡½æ•°ï¼ˆä¾‹å¦‚ï¼ŒçœŸå®çš„æ€»ä½“å‡å€¼ï¼‰ã€‚</p></li>
<li><p><strong>Sample:</strong> We get <em>one</em> sample <span
class="math inline">\(X_1, ..., X_n\)</span> from <span
class="math inline">\(F\)</span>. æˆ‘ä»¬ä» <span
class="math inline">\(F\)</span> ä¸­è·å–<em>ä¸€ä¸ª</em>æ ·æœ¬ <span
class="math inline">\(X_1, ..., X_n\)</span>ã€‚</p></li>
<li><p><strong>Statistic:</strong> We calculate our best estimate <span
class="math inline">\(\hat{\theta}\)</span> from our sample. (e.g., the
sample mean <span class="math inline">\(\bar{x}\)</span>). <span
class="math inline">\(\hat{\theta}\)</span> is our proxy for <span
class="math inline">\(\theta\)</span>. æˆ‘ä»¬ä»æ ·æœ¬ä¸­è®¡ç®—å‡ºæœ€ä½³ä¼°è®¡å€¼
<span class="math inline">\(\hat{\theta}\)</span>ã€‚ï¼ˆä¾‹å¦‚ï¼Œæ ·æœ¬å‡å€¼
<span class="math inline">\(\bar{x}\)</span>ï¼‰ã€‚<span
class="math inline">\(\hat{\theta}\)</span> æ˜¯ <span
class="math inline">\(\theta\)</span> çš„æ›¿ä»£å€¼ã€‚</p></li>
<li><p><strong>The Problem:</strong> We want to know the accuracy of
<span class="math inline">\(\hat{\theta}\)</span>. How much would <span
class="math inline">\(\hat{\theta}\)</span> vary if we could draw many
samples? We want the <em>sampling distribution</em> of <span
class="math inline">\(\hat{\theta}\)</span> around <span
class="math inline">\(\theta\)</span>, specifically the distribution of
the error: <span class="math inline">\((\hat{\theta} - \theta)\)</span>.
æˆ‘ä»¬æƒ³çŸ¥é“ <span class="math inline">\(\hat{\theta}\)</span>
çš„å‡†ç¡®ç‡ã€‚å¦‚æœæˆ‘ä»¬å¯ä»¥æŠ½å–å¤šä¸ªæ ·æœ¬ï¼Œ<span
class="math inline">\(\hat{\theta}\)</span> ä¼šæœ‰å¤šå°‘å˜åŒ–ï¼Ÿæˆ‘ä»¬æƒ³è¦ <span
class="math inline">\(\hat{\theta}\)</span> å›´ç»• <span
class="math inline">\(\theta\)</span> çš„
<em>æŠ½æ ·åˆ†å¸ƒ</em>ï¼Œå…·ä½“æ¥è¯´æ˜¯è¯¯å·®çš„åˆ†å¸ƒï¼š<span
class="math inline">\((\hat{\theta} - \theta)\)</span>ã€‚</p></li>
<li><p><strong>CLT:</strong> The Central Limit Theorem states that <span
class="math inline">\(\sqrt{n}(\hat{\theta} - \theta)
\xrightarrow{\text{dist}} N(0, Var_F(\theta))\)</span>.</p></li>
<li><p><strong>ä¸­å¿ƒæé™å®šç†</strong>ï¼š<span
class="math inline">\(\sqrt{n}(\hat{\theta} - \theta)
\xrightarrow{\text{dist}} N(0, Var_F(\theta))\)</span>ã€‚</p></li>
<li><p><strong>The Catch:</strong> This is <strong>UNKNOWN</strong>
because we donâ€™t know <span
class="math inline">\(F\)</span>.è¿™æ˜¯<strong>æœªçŸ¥</strong>çš„ï¼Œå› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“
<span class="math inline">\(F\)</span>ã€‚</p></li>
</ul>
<h3 id="the-bootstrap-world-slide-87-bottom">The â€œBootstrapâ€ World
(Slide 87, Bottom)</h3>
<ul>
<li><strong>Population:</strong> We <em>pretend</em> our original sample
<em>is</em> the population. We call its distribution the â€œempirical
distribution,â€ <span class="math inline">\(\hat{F}_n\)</span>.
æˆ‘ä»¬<em>å‡è®¾</em>åŸå§‹æ ·æœ¬<em>å°±æ˜¯</em>æ€»ä½“ã€‚æˆ‘ä»¬ç§°å…¶åˆ†å¸ƒä¸ºâ€œç»éªŒåˆ†å¸ƒâ€ï¼Œå³
<span class="math inline">\(\hat{F}_n\)</span>ã€‚</li>
<li><strong>Parameter:</strong> In this new world, the â€œtrueâ€ parameter
is our original statistic, <span
class="math inline">\(\hat{\theta}\)</span> (which is a function of
<span class="math inline">\(\hat{F}_n\)</span>).
åœ¨è¿™ä¸ªæ–°ä¸–ç•Œä¸­ï¼Œâ€œçœŸå®â€å‚æ•°æ˜¯æˆ‘ä»¬åŸå§‹çš„ç»Ÿè®¡é‡ <span
class="math inline">\(\hat{\theta}\)</span>ï¼ˆå®ƒæ˜¯ <span
class="math inline">\(\hat{F}_n\)</span> çš„å‡½æ•°ï¼‰ã€‚</li>
<li><strong>Sample:</strong> We draw <em>many</em> bootstrap samples
<span class="math inline">\(X_1^*, ..., X_n^*\)</span> <em>from <span
class="math inline">\(\hat{F}_n\)</span></em> (i.e., sampling <em>with
replacement</em> from our original sample). æˆ‘ä»¬ä» <span
class="math inline">\(\hat{F}_n\)</span>* ä¸­æŠ½å– <em>è®¸å¤š</em> è‡ªä¸¾æ ·æœ¬
<span class="math inline">\(X_1^*, ...,
X_n^*\)</span>ï¼ˆå³ä»åŸå§‹æ ·æœ¬ä¸­è¿›è¡Œ <em>æœ‰æ”¾å›</em> æŠ½æ ·ï¼‰ã€‚</li>
<li><strong>Statistic:</strong> From each bootstrap sample, we calculate
a <em>bootstrap statistic</em>, <span
class="math inline">\(\hat{\theta}^*\)</span>.
ä»æ¯ä¸ªè‡ªä¸¾æ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—ä¸€ä¸ª <em>è‡ªä¸¾ç»Ÿè®¡é‡</em>ï¼Œå³ <span
class="math inline">\(\hat{\theta}^*\)</span>ã€‚</li>
<li><strong>The Solution:</strong> We can now <em>empirically</em> find
the distribution of <span class="math inline">\(\hat{\theta}^*\)</span>
around <span class="math inline">\(\hat{\theta}\)</span>. We look at the
distribution of the bootstrap error: <span
class="math inline">\((\hat{\theta}^* - \hat{\theta})\)</span>.
æˆ‘ä»¬ç°åœ¨å¯ä»¥ <em>å‡­ç»éªŒ</em> æ‰¾åˆ° <span
class="math inline">\(\hat{\theta}^*\)</span> å›´ç»• <span
class="math inline">\(\hat{\theta}\)</span>
çš„åˆ†å¸ƒã€‚æˆ‘ä»¬æ¥çœ‹çœ‹è‡ªä¸¾è¯¯å·®çš„åˆ†å¸ƒï¼š<span
class="math inline">\((\hat{\theta}^* - \hat{\theta})\)</span>ã€‚</li>
<li><strong>CLT:</strong> The CLT also states that <span
class="math inline">\(\sqrt{n}(\hat{\theta}^* - \hat{\theta})
\xrightarrow{\text{dist}} N(0, Var_{\hat{F}_n}(\theta))\)</span>.</li>
<li><strong>The Power:</strong> This distribution is
<strong>ESTIMABLE!</strong> We just run the bootstrap <span
class="math inline">\(B\)</span> times and we get <span
class="math inline">\(B\)</span> values of <span
class="math inline">\(\hat{\theta}^*\)</span>. We can then calculate
their variance, standard deviation, and percentiles directly.
è¿™ä¸ªåˆ†å¸ƒæ˜¯<strong>å¯ä¼°è®¡çš„ï¼</strong>æˆ‘ä»¬åªéœ€è¿è¡Œ <span
class="math inline">\(B\)</span> æ¬¡è‡ªä¸¾ç¨‹åºï¼Œå°±èƒ½å¾—åˆ° <span
class="math inline">\(B\)</span> ä¸ª <span
class="math inline">\(\hat{\theta}^*\)</span>
å€¼ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥ç›´æ¥è®¡ç®—å®ƒä»¬çš„æ–¹å·®ã€æ ‡å‡†å·®å’Œç™¾åˆ†ä½æ•°ã€‚</li>
</ul>
<h3 id="the-core-approximation-slide-88">The Core Approximation (Slide
88)</h3>
<p>The entire method relies on the assumption that <strong>the
(knowable) bootstrap distribution is a good approximation of the
(unknown) true sampling distribution.</strong>
æ•´ä¸ªæ–¹æ³•ä¾èµ–äºä»¥ä¸‹å‡è®¾ï¼š<strong>ï¼ˆå·²çŸ¥çš„ï¼‰è‡ªä¸¾åˆ†å¸ƒèƒ½å¤Ÿå¾ˆå¥½åœ°è¿‘ä¼¼ï¼ˆæœªçŸ¥çš„ï¼‰çœŸå®æŠ½æ ·åˆ†å¸ƒ</strong>ã€‚</p>
<p>The distribution of the <em>bootstrap error</em> approximates the
distribution of the <em>true error</em>.
<em>è‡ªä¸¾è¯¯å·®</em>çš„åˆ†å¸ƒè¿‘ä¼¼äº<em>çœŸå®è¯¯å·®</em>çš„åˆ†å¸ƒã€‚</p>
<p><span class="math display">\[\text{distribution of }
\sqrt{n}(\hat{\theta}^* - \hat{\theta}) \approx \text{distribution of }
\sqrt{n}(\hat{\theta} - \theta)\]</span></p>
<p>This is why: * The <strong>standard deviation</strong> of the <span
class="math inline">\(\hat{\theta}^*\)</span> values is our estimate for
the <strong>standard error</strong> of <span
class="math inline">\(\hat{\theta}\)</span>.
å€¼çš„<strong>æ ‡å‡†å·®</strong>æ˜¯æˆ‘ä»¬å¯¹ <span
class="math inline">\(\hat{\theta}\)</span>
çš„<strong>æ ‡å‡†è¯¯å·®</strong>çš„ä¼°è®¡å€¼ã€‚ * The <strong>percentiles</strong>
of the <span class="math inline">\(\hat{\theta}^*\)</span> distribution
(e.g., 2.5th and 97.5th) can be used to build a <strong>confidence
interval</strong> for the true parameter <span
class="math inline">\(\theta\)</span>.
åˆ†å¸ƒçš„<strong>ç™¾åˆ†ä½æ•°</strong>ï¼ˆä¾‹å¦‚ï¼Œç¬¬ 2.5 ä¸ªå’Œç¬¬ 97.5
ä¸ªï¼‰å¯ç”¨äºä¸ºçœŸå®å‚æ•° <span class="math inline">\(\theta\)</span>
å»ºç«‹<strong>ç½®ä¿¡åŒºé—´</strong>ã€‚</p>
<h2 id="extension-cross-validation-cv-analysis">3. Extension:
Cross-Validation (CV) Analysis</h2>
<h3 id="cv-for-hyperparameter-tuning-slide-84-è¶…å‚æ•°è°ƒä¼˜çš„-cv">CV for
Hyperparameter Tuning (Slide 84) è¶…å‚æ•°è°ƒä¼˜çš„ CV</h3>
<p>This plot is the <em>result</em> of the 10-fold CV code shown in the
previous set of slides (slide 82). * <strong>Purpose:</strong> To find
the optimal hyperparameter <span class="math inline">\(k\)</span>
(number of neighbors) for the KNN model. * <strong>X-axis:</strong>
Number of Neighbors (<span class="math inline">\(k\)</span>). *
<strong>Y-axis:</strong> CV Error (Mean Squared Error). *
<strong>Analysis:</strong> * <strong>Low <span
class="math inline">\(k\)</span> (e.g., <span class="math inline">\(k=1,
2\)</span>):</strong> High error. The model is too complex and
<strong>overfitting</strong> to the training data. * <strong>High <span
class="math inline">\(k\)</span> (e.g., <span
class="math inline">\(k&gt;40\)</span>):</strong> Error slowly
increases. The model is too simple and <strong>underfitting</strong>
(e.g., averaging too many neighbors). * <strong>Optimal <span
class="math inline">\(k\)</span>:</strong> The â€œsweet spotâ€ is at the
bottom of the â€œUâ€ shape, around <strong><span class="math inline">\(k
\approx 20-30\)</span></strong>, which gives the lowest CV error.</p>
<ul>
<li><strong>ç›®çš„</strong>ï¼šä¸º KNN æ¨¡å‹æ‰¾åˆ°æœ€ä¼˜è¶…å‚æ•° <span
class="math inline">\(k\)</span>ï¼ˆé‚»å±…æ•°ï¼‰ã€‚</li>
<li><strong>X è½´ï¼š</strong>é‚»å±…æ•° (<span
class="math inline">\(k\)</span>)ã€‚</li>
<li><strong>Y è½´ï¼š</strong>CV è¯¯å·®ï¼ˆå‡æ–¹è¯¯å·®ï¼‰ã€‚</li>
<li><strong>åˆ†æ</strong>ï¼š**</li>
<li><strong>ä½ <span class="math inline">\(k\)</span>ï¼ˆä¾‹å¦‚ï¼Œ<span
class="math inline">\(k=1,
2\)</span>ï¼‰ï¼š</strong>è¯¯å·®è¾ƒå¤§ã€‚æ¨¡å‹è¿‡äºå¤æ‚ï¼Œå¹¶ä¸”ä¸è®­ç»ƒæ•°æ®<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚</li>
<li><strong>é«˜ <span class="math inline">\(k\)</span>ï¼ˆä¾‹å¦‚ï¼Œ<span
class="math inline">\(k&gt;40\)</span>ï¼‰ï¼š</strong>è¯¯å·®ç¼“æ…¢å¢åŠ ã€‚æ¨¡å‹è¿‡äºç®€å•ä¸”<strong>æ¬ æ‹Ÿåˆ</strong>ï¼ˆä¾‹å¦‚ï¼Œå¯¹å¤ªå¤šé‚»å±…è¿›è¡Œå¹³å‡ï¼‰ã€‚</li>
<li><strong>æœ€ä¼˜ <span
class="math inline">\(k\)</span>ï¼š</strong>â€œæœ€ä½³ç‚¹â€ä½äºâ€œUâ€å½¢çš„åº•éƒ¨ï¼Œå¤§çº¦ä¸º<strong><span
class="math inline">\(k \approx 20-30\)</span></strong>ï¼Œæ­¤æ—¶ CV
è¯¯å·®æœ€ä½ã€‚</li>
</ul>
<h3 id="why-cv-over-estimates-test-error-slide-89">Why CV Over-Estimates
Test Error (Slide 89)</h3>
<p>This is a subtle but important theoretical point. * <strong>Our
Goal:</strong> We want to know the test error of our <em>final
model</em> (<span class="math inline">\(\hat{f}^{\text{full}}\)</span>),
which we will train on the <strong>full dataset</strong> (all <span
class="math inline">\(n\)</span> observations).
æˆ‘ä»¬æƒ³çŸ¥é“<em>æœ€ç»ˆæ¨¡å‹</em> (<span
class="math inline">\(\hat{f}^{\text{full}}\)</span>)
çš„æµ‹è¯•è¯¯å·®ï¼Œæˆ‘ä»¬å°†åœ¨<strong>å®Œæ•´æ•°æ®é›†</strong>ï¼ˆæ‰€æœ‰ <span
class="math inline">\(n\)</span> ä¸ªè§‚æµ‹å€¼ï¼‰ä¸Šè®­ç»ƒè¯¥æ¨¡å‹ã€‚ * <strong>What
CV Measures:</strong> <span class="math inline">\(k\)</span>-fold CV
does <em>not</em> test the final model. It tests <span
class="math inline">\(k\)</span> different models (<span
class="math inline">\(\hat{f}^{(k)}\)</span>), each trained on a
<em>smaller</em> dataset (of size <span
class="math inline">\(\frac{k-1}{k} \times n\)</span>). <span
class="math inline">\(k\)</span> å€ CV <em>ä¸</em>æµ‹è¯•æœ€ç»ˆæ¨¡å‹ã€‚å®ƒæµ‹è¯•äº†
<span class="math inline">\(k\)</span> ä¸ªä¸åŒçš„æ¨¡å‹ (<span
class="math inline">\(\hat{f}^{(k)}\)</span>)ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½åŸºäºä¸€ä¸ª<em>è¾ƒå°</em>çš„æ•°æ®é›†ï¼ˆå¤§å°ä¸º
<span class="math inline">\(\frac{k-1}{k} \times
n\)</span>ï¼‰è¿›è¡Œè®­ç»ƒã€‚</p>
<ul>
<li><strong>The Logic:</strong>
<ol type="1">
<li>Models trained on <em>less data</em> generally perform
<em>worse</em> than models trained on <em>more data</em>.
åŸºäº<em>è¾ƒå°‘æ•°æ®</em>è®­ç»ƒçš„æ¨¡å‹é€šå¸¸æ¯”åŸºäº<em>è¾ƒå¤šæ•°æ®</em>è®­ç»ƒçš„æ¨¡å‹è¡¨ç°<em>æ›´å·®</em>ã€‚</li>
<li>The CV error is the average error of models trained on <span
class="math inline">\(\frac{k-1}{k} n\)</span> observations. CV
è¯¯å·®æ˜¯ä½¿ç”¨ <span class="math inline">\(\frac{k-1}{k} n\)</span>
ä¸ªè§‚æµ‹å€¼è®­ç»ƒçš„æ¨¡å‹çš„å¹³å‡è¯¯å·®ã€‚</li>
<li>The â€œtrue test errorâ€ is the error of the model trained on <span
class="math inline">\(n\)</span> observations. â€œçœŸå®æµ‹è¯•è¯¯å·®â€æ˜¯ä½¿ç”¨
<span class="math inline">\(n\)</span> ä¸ªè§‚æµ‹å€¼è®­ç»ƒçš„æ¨¡å‹çš„è¯¯å·®ã€‚</li>
</ol></li>
<li><strong>Conclusion:</strong> Since the CV models are trained on
smaller datasets, they will, on average, have a slightly higher error
than the final model. Therefore, <strong>the CV error score is a
slightly <em>pessimistic</em> estimate (it over-estimates) the true test
error of the final model.</strong> ç”±äº CV
æ¨¡å‹æ˜¯åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼Œå› æ­¤å®ƒä»¬çš„å¹³å‡è¯¯å·®ä¼šç•¥é«˜äºæœ€ç»ˆæ¨¡å‹ã€‚å› æ­¤ï¼Œ<strong>CV
è¯¯å·®åˆ†æ•°æ˜¯ä¸€ä¸ªç•¥å¾®<em>æ‚²è§‚</em>çš„ä¼°è®¡ï¼ˆå®ƒé«˜ä¼°äº†ï¼‰æœ€ç»ˆæ¨¡å‹çš„çœŸå®æµ‹è¯•è¯¯å·®ã€‚</strong></li>
</ul>
<h3 id="correction-of-cv-error-slides-90-91">Correction of CV Error
(Slides 90-91)</h3>
<ul>
<li><p><strong>Theory (Slide 91):</strong> Advanced theory suggests the
expected test error <span class="math inline">\(R(n)\)</span> behaves
like <span class="math inline">\(R(n) = R^* + c/n\)</span>, where <span
class="math inline">\(R^*\)</span> is the irreducible error and <span
class="math inline">\(n\)</span> is the sample size. This formula
mathematically confirms that error <em>decreases</em> as sample size
<span class="math inline">\(n\)</span> <em>increases</em>.
é«˜çº§ç†è®ºè¡¨æ˜ï¼Œé¢„æœŸæµ‹è¯•è¯¯å·® <span class="math inline">\(R(n)\)</span>
çš„è¡Œä¸ºç±»ä¼¼äº <span class="math inline">\(R(n) = R^* + c/n\)</span>ï¼Œå…¶ä¸­
<span class="math inline">\(R^*\)</span> æ˜¯ä¸å¯çº¦è¯¯å·®ï¼Œ<span
class="math inline">\(n\)</span>
æ˜¯æ ·æœ¬é‡ã€‚è¯¥å…¬å¼ä»æ•°å­¦ä¸Šè¯å®äº†è¯¯å·®ä¼šéšç€æ ·æœ¬é‡ <span
class="math inline">\(n\)</span> çš„å¢åŠ è€Œ<em>å‡å°</em>ã€‚</p></li>
<li><p><strong>R Code (Slide 90):</strong> The <code>cv.glm</code>
function from the <code>boot</code> library automatically provides
this.</p>
<ul>
<li><code>cv.err$delta</code>: This output vector contains two
values.</li>
<li><code>[1] 24.23151</code> (Raw CV Error): This is the standard
Leave-One-Out CV (LOOCV) error.</li>
<li><code>[2] 24.23114</code> (Adjusted CV Error): This is a
bias-corrected estimate that accounts for the overestimation problem.
Itâ€™s slightly lower, representing a more accurate guess for the error of
the <em>final model</em> trained on all <span
class="math inline">\(n\)</span> data points.</li>
</ul></li>
</ul>
<p># The â€œCorrection of CV Errorâ€ extension.</p>
<h3 id="summary">Summary</h3>
<p>This section provides a deeper mathematical look at <em>why</em>
k-fold cross-validation (CV) slightly <strong>over-estimates</strong>
the true test error. æœ¬èŠ‚ä»æ•°å­¦è§’åº¦æ›´æ·±å…¥åœ°é˜è¿°äº† <em>ä¸ºä»€ä¹ˆ</em> k
æŠ˜äº¤å‰éªŒè¯ (CV) ä¼šç•¥å¾®<strong>é«˜ä¼°</strong>çœŸå®æµ‹è¯•è¯¯å·®ã€‚</p>
<ol type="1">
<li><p><strong>The Overestimation é«˜ä¼°:</strong> CV trains on <span
class="math inline">\(\frac{k-1}{k}\)</span> of the data, which is
<em>less</em> than the full dataset (size <span
class="math inline">\(n\)</span>). Models trained on less data are
generally <em>worse</em>. Therefore, the average error from CV (<span
class="math inline">\(CV_k\)</span>) is slightly <em>higher</em> (more
pessimistic) than the true error of the final model trained on all <span
class="math inline">\(n\)</span> data (<span
class="math inline">\(R(n)\)</span>). CV è®­ç»ƒçš„æ•°æ®ä¸º <span
class="math inline">\(\frac{k-1}{k}\)</span>ï¼Œå°äºå®Œæ•´æ•°æ®é›†ï¼ˆå¤§å°ä¸º
<span
class="math inline">\(n\)</span>ï¼‰ã€‚ä½¿ç”¨è¾ƒå°‘æ•°æ®è®­ç»ƒçš„æ¨¡å‹é€šå¸¸<em>æ›´å·®</em>ã€‚å› æ­¤ï¼ŒCV
çš„å¹³å‡è¯¯å·® (<span class="math inline">\(CV_k\)</span>)
ç•¥é«˜äºï¼ˆæ›´æ‚²è§‚åœ°ï¼‰åŸºäºæ‰€æœ‰ <span class="math inline">\(n\)</span>
ä¸ªæ•°æ®è®­ç»ƒçš„æœ€ç»ˆæ¨¡å‹çš„çœŸå®è¯¯å·® (<span
class="math inline">\(R(n)\)</span>)ã€‚</p></li>
<li><p><strong>A Simple Correction ç®€å•ä¿®æ­£:</strong> A mathematical
formula, <span class="math inline">\(\tilde{CV_k} = \frac{k-1}{k} \cdot
CV_k\)</span>, is proposed to â€œcorrectâ€ this overestimation.</p></li>
<li><p><strong>The Critical Flaw å…³é”®ç¼ºé™·:</strong> This correction is
derived <em>assuming</em> the <strong>irreducible error (<span
class="math inline">\(R^*\)</span>) is
zero</strong>.æ­¤ä¿®æ­£æ˜¯åœ¨<em>å‡è®¾</em><strong>ä¸å¯çº¦è¯¯å·® (<span
class="math inline">\(R^*\)</span>)
ä¸ºé›¶</strong>çš„æƒ…å†µä¸‹å¾—å‡ºçš„ã€‚</p></li>
<li><p><strong>The Takeaway è¦ç‚¹ (Code Analysis):</strong> The Python
code demonstrates a real-world scenario where there is noise
(<code>noise_std = 0.5</code>), meaning <span class="math inline">\(R^*
&gt; 0\)</span>. In this case, the <strong>simple correction
fails</strong>â€”it produces an error (0.217) that is <em>less
accurate</em> and further from the true error (0.272) than the
<strong>original raw CV error</strong> (0.271).</p></li>
</ol>
<p>Python
ä»£ç æ¼”ç¤ºäº†ä¸€ä¸ªå­˜åœ¨å™ªå£°ï¼ˆ<code>noise_std = 0.5</code>ï¼‰çš„çœŸå®åœºæ™¯ï¼Œå³
<span class="math inline">\(R^* &gt;
0\)</span>ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ<strong>ç®€å•ä¿®æ­£å¤±è´¥</strong>â€”â€”å®ƒäº§ç”Ÿçš„è¯¯å·®
(0.217) <em>ç²¾åº¦è¾ƒä½</em>ï¼Œå¹¶ä¸”ä¸çœŸå®è¯¯å·® (0.272) çš„è·ç¦»æ¯”<strong>åŸå§‹
CV è¯¯å·®</strong> (0.271) æ›´è¿œã€‚</p>
<p><strong>Exam Conclusion:</strong> For most real-world problems (which
have noise), the <strong>raw <span class="math inline">\(k\)</span>-fold
CV error is a better and more reliable estimate</strong> of the true
test error than the simple (and flawed) correction.
å¯¹äºå¤§å¤šæ•°å®é™…é—®é¢˜ï¼ˆåŒ…å«å™ªå£°ï¼‰ï¼Œ<strong>åŸå§‹ <span
class="math inline">\(k\)</span> å€ CV
è¯¯å·®æ¯”ç®€å•ï¼ˆä¸”æœ‰ç¼ºé™·çš„ï¼‰ä¿®æ­£æ–¹æ³•æ›´èƒ½å‡†ç¡®ã€å¯é åœ°ä¼°è®¡çœŸå®æµ‹è¯•è¯¯å·®</strong>ã€‚</p>
<h3 id="mathematical-understanding-1">Mathematical Understanding</h3>
<p>This section explains the theory of <em>why</em> <span
class="math inline">\(CV_k &gt; R(n)\)</span> and derives the simple
correction. æœ¬èŠ‚è§£é‡Šäº†ä¸ºä»€ä¹ˆ <span class="math inline">\(CV_k &gt;
R(n)\)</span>ï¼Œå¹¶æ¨å¯¼å‡ºç®€å•çš„ä¿®æ­£æ–¹æ³•ã€‚</p>
<ol type="1">
<li><p><strong>Assumed Error Behavior å‡è®¾è¯¯å·®è¡Œä¸º:</strong> We assume
the test error <span class="math inline">\(R(n)\)</span> for a model
trained on <span class="math inline">\(n\)</span> data points behaves
like: æˆ‘ä»¬å‡è®¾åŸºäº <span class="math inline">\(n\)</span>
ä¸ªæ•°æ®ç‚¹è®­ç»ƒçš„æ¨¡å‹çš„æµ‹è¯•è¯¯å·® <span class="math inline">\(R(n)\)</span>
çš„è¡Œä¸ºå¦‚ä¸‹ï¼š <span class="math display">\[R(n) = R^* +
\frac{c}{n}\]</span></p>
<ul>
<li><span class="math inline">\(R^*\)</span>: The <strong>irreducible
error</strong> (the â€œnoise floorâ€ you can never beat).
<strong>ä¸å¯çº¦è¯¯å·®</strong>ï¼ˆå³ä½ æ°¸è¿œæ— æ³•å…‹æœçš„â€œæœ¬åº•å™ªå£°â€ï¼‰ã€‚</li>
<li><span class="math inline">\(c/n\)</span>: The model variance, which
<em>decreases</em> as sample size <span class="math inline">\(n\)</span>
<em>increases</em>. æ¨¡å‹æ–¹å·®ï¼Œéšç€æ ·æœ¬é‡ <span
class="math inline">\(n\)</span> çš„å¢åŠ è€Œå‡å°ã€‚</li>
</ul></li>
<li><p><strong>Test Error vs.Â CV Error æµ‹è¯•è¯¯å·® vs.Â CV
è¯¯å·®:</strong></p>
<ul>
<li><strong>Test Error of Interest:</strong> This is the error of our
<em>final model</em> trained on all <span
class="math inline">\(n\)</span> points: <span
class="math display">\[R(n) = R^* + \frac{c}{n}\]</span></li>
<li><strong>æ„Ÿå…´è¶£çš„æµ‹è¯•è¯¯å·®</strong>ï¼šè¿™æ˜¯æˆ‘ä»¬åœ¨æ‰€æœ‰ <span
class="math inline">\(n\)</span>
ä¸ªç‚¹ä¸Šè®­ç»ƒçš„<em>æœ€ç»ˆæ¨¡å‹</em>çš„è¯¯å·®ï¼š</li>
<li><strong>k-fold CV Error:</strong> This is the average error of <span
class="math inline">\(k\)</span> models, each trained on a smaller
sample of size <span class="math inline">\(n&#39; =
(\frac{k-1}{k})n\)</span>.</li>
<li><strong>k å€ CV è¯¯å·®</strong>ï¼šè¿™æ˜¯ <span
class="math inline">\(k\)</span>
ä¸ªæ¨¡å‹çš„å¹³å‡è¯¯å·®ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„æ ·æœ¬ï¼ˆå¤§å°ä¸º <span
class="math inline">\(n&#39; = (\frac{k-1}{k})n\)</span>ï¼‰è¿›è¡Œè®­ç»ƒã€‚
<span class="math display">\[CV_k \approx R(n&#39;) =
R\left(\frac{k-1}{k}n\right) = R^* +
\frac{c}{\left(\frac{k-1}{k}\right)n} = R^* +
\frac{ck}{(k-1)n}\]</span></li>
</ul></li>
<li><p><strong>The Overestimation é«˜ä¼°:</strong> Letâ€™s compare <span
class="math inline">\(CV_k\)</span> and <span
class="math inline">\(R(n)\)</span>: <span class="math display">\[CV_k
\approx R^* + \left(\frac{k}{k-1}\right) \frac{c}{n}\]</span> <span
class="math display">\[R(n) = R^* + \left(\frac{k-1}{k-1}\right)
\frac{c}{n}\]</span> Since <span class="math inline">\(k &gt;
(k-1)\)</span>, the factor <span
class="math inline">\(\left(\frac{k}{k-1}\right)\)</span> is
<strong>greater than 1</strong>. This means the <span
class="math inline">\(CV_k\)</span> error term is larger than the <span
class="math inline">\(R(n)\)</span> error term. Thus: <strong><span
class="math inline">\(CV_k &gt; \text{Test error of interest }
R(n)\)</span></strong> ç”±äº <span class="math inline">\(k &gt;
(k-1)\)</span>ï¼Œå› å­ <span
class="math inline">\(\left(\frac{k}{k-1}\right)\)</span> <strong>å¤§äº
1</strong>ã€‚è¿™æ„å‘³ç€ <span class="math inline">\(CV_k\)</span>
è¯¯å·®é¡¹å¤§äº <span class="math inline">\(R(n)\)</span> è¯¯å·®é¡¹ã€‚å› æ­¤ï¼š
<strong><span class="math inline">\(CV_k &gt; \text{ç›®æ ‡æµ‹è¯•è¯¯å·® }
R(n)\)</span></strong></p></li>
<li><p><strong>Deriving the (Flawed) Correction
æ¨å¯¼ï¼ˆæœ‰ç¼ºé™·çš„ï¼‰ä¿®æ­£:</strong> This correction makes a <strong>strong
assumption: <span class="math inline">\(R^* \approx 0\)</span></strong>
(the model is perfectly specified, and there is no noise).
æ­¤ä¿®æ­£åŸºäºä¸€ä¸ª<strong>å¼ºå‡è®¾ï¼š<span class="math inline">\(R^* \approx
0\)</span></strong>ï¼ˆæ¨¡å‹å®Œå…¨æ­£ç¡®ï¼Œä¸”æ— å™ªå£°ï¼‰ã€‚</p>
<ul>
<li>If <span class="math inline">\(R^* = 0\)</span>, then <span
class="math inline">\(R(n) \approx \frac{c}{n}\)</span></li>
<li>If <span class="math inline">\(R^* = 0\)</span>, then <span
class="math inline">\(CV_k \approx \frac{ck}{(k-1)n}\)</span></li>
</ul>
<p>Now, look at the ratio between them: <span
class="math display">\[\frac{R(n)}{CV_k} \approx \frac{c/n}{ck/((k-1)n)}
= \frac{c}{n} \cdot \frac{(k-1)n}{ck} = \frac{k-1}{k}\]</span></p>
<p>This gives us the correction formula by isolating <span
class="math inline">\(R(n)\)</span>: é€šè¿‡åˆ†ç¦» <span
class="math inline">\(R(n)\)</span>ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ ¡æ­£å…¬å¼ï¼š <span
class="math display">\[R(n) \approx \left(\frac{k-1}{k}\right) \cdot
CV_k\]</span> This corrected version is denoted <span
class="math inline">\(\tilde{CV_k}\)</span>.è¿™ä¸ªæ ¡æ­£ç‰ˆæœ¬è¡¨ç¤ºä¸º <span
class="math inline">\(\tilde{CV_k}\)</span>ã€‚</p></li>
</ol>
<h3 id="code-analysis-slides-92-93">Code Analysis (Slides 92-93)</h3>
<p>The Python code is an experiment designed to <strong>test the
correction formula</strong>.</p>
<ul>
<li><p><strong>Goal:</strong> Compare the â€œRaw CV Errorâ€ (<span
class="math inline">\(CV_k\)</span>), the â€œCorrected CV Errorâ€ (<span
class="math inline">\(\tilde{CV_k}\)</span>), and the â€œTrue Test Errorâ€
(<span class="math inline">\(R(n)\)</span>) in a realistic
setting.</p></li>
<li><p><strong>Key Setup:</strong></p>
<ol type="1">
<li><code>def f(x)</code>: Defines the true, underlying function <span
class="math inline">\(y = x^2 + 15\sin(x)\)</span>.</li>
<li><code>noise_std = 0.5</code>: <strong>This is the most important
line.</strong> It adds significant random noise to the data. This
ensures that the <strong>irreducible error <span
class="math inline">\(R^*\)</span> is large and <span
class="math inline">\(R^* &gt; 0\)</span></strong>.</li>
<li><code>y = f(...) + np.random.normal(...)</code>: Creates the noisy
training data (the blue dots).</li>
</ol></li>
<li><p><strong>CV Calculation (Standard K-Fold):</strong></p>
<ul>
<li><code>kf = KFold(...)</code>: Sets up 5-fold CV (<span
class="math inline">\(k=5\)</span>).</li>
<li><code>for train_index, val_index in kf.split(x):</code>: This is the
standard loop. It trains on 4 folds and validates on 1 fold.</li>
<li><code>cv_error = np.mean(cv_mse_list)</code>: Calculates the
<strong>raw <span class="math inline">\(CV_5\)</span> error</strong>.
This is the first result (e.g., <strong>0.2715</strong>).</li>
</ul></li>
<li><p><strong>Correction Calculation:</strong></p>
<ul>
<li><code>correction_factor = (k_splits - 1) / k_splits</code>: This is
<span class="math inline">\(\frac{k-1}{k}\)</span>, which is <span
class="math inline">\(4/5 = 0.8\)</span>.</li>
<li><code>corrected_cv_error = correction_factor * cv_error</code>: This
applies the flawed formula from the math section (<span
class="math inline">\(0.2715 \times 0.8\)</span>). This is the second
result (e.g., <strong>0.2172</strong>).</li>
</ul></li>
<li><p><strong>â€œTrueâ€ Test Error Calculation:</strong></p>
<ul>
<li><code>knn.fit(x, y)</code>: Trains the <em>final model</em> on the
<em>entire</em> noisy dataset.</li>
<li><code>n_test = 1000</code>: Creates a <em>new, large</em> test set
to estimate the true error.</li>
<li><code>true_test_error = mean_squared_error(...)</code>: Calculates
the error of the final model on this new test set. This is our best
estimate of <span class="math inline">\(R(n)\)</span> (e.g.,
<strong>0.2725</strong>).</li>
</ul></li>
<li><p><strong>Analysis of Results (Slide 93):</strong></p>
<ul>
<li><strong>Raw 5-Fold CV MSE:</strong> 0.2715</li>
<li><strong>True test error:</strong> 0.2725</li>
<li><strong>Corrected 5-Fold CV MSE:</strong> 0.2172</li>
</ul>
<p>The <strong>Raw CV Error (0.2715) is an excellent estimate</strong>
of the True Test Error (0.2725). The <strong>Corrected Error (0.2172) is
much worse</strong>. This experiment <em>proves</em> that when noise
(<span class="math inline">\(R^*\)</span>) is present, the simple
correction formula should not be used.</p></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/01/5054C4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/01/5054C4/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-10-01 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-01T21:00:00+08:00">2025-10-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-10-18 23:00:24" itemprop="dateModified" datetime="2025-10-18T23:00:24+08:00">2025-10-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-4</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="what-is-classification">1. What is Classification?</h1>
<p>Classification is a type of <strong>supervised machine
learning</strong> where the goal is to predict a
<strong>categorical</strong> or qualitative response. Unlike regression
where you predict a continuous numerical value (like a price or
temperature), classification assigns an input to a specific category or
class.
åˆ†ç±»æ˜¯ä¸€ç§<strong>ç›‘ç£å¼æœºå™¨å­¦ä¹ </strong>ï¼Œå…¶ç›®æ ‡æ˜¯é¢„æµ‹<strong>åˆ†ç±»</strong>æˆ–å®šæ€§å“åº”ã€‚ä¸é¢„æµ‹è¿ç»­æ•°å€¼ï¼ˆä¾‹å¦‚ä»·æ ¼æˆ–æ¸©åº¦ï¼‰çš„å›å½’ä¸åŒï¼Œåˆ†ç±»å°†è¾“å…¥åˆ†é…åˆ°ç‰¹å®šçš„ç±»åˆ«æˆ–ç±»åˆ«ã€‚</p>
<p><strong>Key characteristics:</strong></p>
<ul>
<li><p><strong>Goal:</strong> Predict the class of a subject based on
input features.</p></li>
<li><p><strong>Output (Response):</strong> The output is a category,
such as â€˜Yesâ€™/â€˜Noâ€™, â€˜Spamâ€™/â€˜Not Spamâ€™, or
â€˜Highâ€™/â€˜Mediumâ€™/â€˜Lowâ€™.</p></li>
<li><p><strong>Applications:</strong> Common examples include email spam
detectors, medical diagnosis (e.g., virus carrier vs.Â non-carrier), and
fraud detection.</p>
<ul>
<li><strong>ç›®æ ‡</strong>ï¼šæ ¹æ®è¾“å…¥ç‰¹å¾é¢„æµ‹ä¸»é¢˜çš„ç±»åˆ«ã€‚</li>
<li><strong>è¾“å‡ºï¼ˆå“åº”ï¼‰ï¼š</strong>è¾“å‡ºæ˜¯ä¸€ä¸ªç±»åˆ«ï¼Œä¾‹å¦‚â€œæ˜¯â€/â€œå¦â€ã€â€œåƒåœ¾é‚®ä»¶â€/â€œéåƒåœ¾é‚®ä»¶â€æˆ–â€œé«˜â€/â€œä¸­â€/â€œä½â€ã€‚</li>
<li><strong>åº”ç”¨</strong>ï¼šå¸¸è§ç¤ºä¾‹åŒ…æ‹¬åƒåœ¾é‚®ä»¶æ£€æµ‹å™¨ã€åŒ»å­¦è¯Šæ–­ï¼ˆä¾‹å¦‚ï¼Œç—…æ¯’æºå¸¦è€…ä¸éç—…æ¯’æºå¸¦è€…ï¼‰å’Œæ¬ºè¯ˆæ£€æµ‹ã€‚
The example used in the slides is a credit card <strong>Default
dataset</strong>. The goal is to predict whether a customer will
<strong>default</strong> (â€˜Yesâ€™ or â€˜Noâ€™) on their payments based on
their monthly <strong>income</strong> and account
<strong>balance</strong>.</li>
</ul></li>
</ul>
<p>## Why Not Use Linear Regression?ä¸ºä»€ä¹ˆä¸ä½¿ç”¨çº¿æ€§å›å½’ï¼Ÿ</p>
<p>At first, it might seem possible to use linear regression for
classification. For a binary (two-class) problem like the default
dataset, you could code the outcomes as numbers, for example:</p>
<ul>
<li>Default = â€˜Noâ€™ =&gt; <span class="math inline">\(y = 0\)</span></li>
<li>Default = â€˜Yesâ€™ =&gt; <span class="math inline">\(y =
1\)</span></li>
</ul>
<p>You could then fit a standard linear regression model: <span
class="math inline">\(Y \approx \beta_0 + \beta_1 X\)</span>. In this
context, we would interpret the prediction <span
class="math inline">\(\hat{y}\)</span> as the <em>probability</em> of
default, so weâ€™d be modeling <span class="math inline">\(P(Y=1|X) =
\beta_0 + \beta_1 X\)</span>.</p>
<p>However, this approach has two major problems:
ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æœ‰ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š <strong>1. The Output Is Not a
Probability</strong> A linear model can produce outputs that are less
than 0 or greater than 1. This doesnâ€™t make sense for a probability,
which must always be between 0 and 1.</p>
<p>The image below is the most important one for understanding this
issue. The left plot shows a linear regression line fit to the 0/1
default data. You can see the line goes below 0 and would eventually go
above 1 for higher balances. The right plot shows a logistic regression
curve, which always stays between 0 and 1.</p>
<ul>
<li><strong>Left (Linear Regression):</strong> The straight blue line
predicts probabilities &lt; 0 for low balances.</li>
<li><strong>Right (Logistic Regression):</strong> The S-shaped blue
curve correctly constrains the probability output between 0 and 1.</li>
</ul>
<p><strong>2. It Doesnâ€™t Work for Multi-Class Problems</strong> If you
have more than two categories (e.g., â€˜mildâ€™, â€˜moderateâ€™, â€˜severeâ€™), you
might code them as 0, 1, and 2. A linear regression model would
incorrectly assume that the â€œdistanceâ€ between â€˜mildâ€™ and â€˜moderateâ€™ is
the same as the distance between â€˜moderateâ€™ and â€˜severeâ€™, which is
usually not a valid assumption.</p>
<p><strong>1. è¾“å‡ºä¸æ˜¯æ¦‚ç‡</strong> çº¿æ€§æ¨¡å‹å¯ä»¥äº§ç”Ÿå°äº 0 æˆ–å¤§äº 1
çš„è¾“å‡ºã€‚è¿™å¯¹äºæ¦‚ç‡æ¥è¯´æ¯«æ— æ„ä¹‰ï¼Œå› ä¸ºæ¦‚ç‡å¿…é¡»å§‹ç»ˆä»‹äº 0 å’Œ 1 ä¹‹é—´ã€‚</p>
<p>ä¸‹å›¾æ˜¯ç†è§£è¿™ä¸ªé—®é¢˜æœ€é‡è¦çš„å›¾ã€‚å·¦å›¾æ˜¾ç¤ºäº†ä¸ 0/1
é»˜è®¤æ•°æ®æ‹Ÿåˆçš„çº¿æ€§å›å½’çº¿ã€‚æ‚¨å¯ä»¥çœ‹åˆ°ï¼Œè¯¥çº¿ä½äº
0ï¼Œå¹¶ä¸”æœ€ç»ˆä¼šéšç€ä½™é¢çš„å¢åŠ è€Œé«˜äº
1ã€‚å³å›¾æ˜¾ç¤ºäº†é€»è¾‘å›å½’æ›²çº¿ï¼Œå®ƒå§‹ç»ˆä¿æŒåœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚</p>
<ul>
<li><strong>å·¦å›¾ï¼ˆçº¿æ€§å›å½’ï¼‰ï¼š</strong>è“è‰²ç›´çº¿é¢„æµ‹ä½ä½™é¢çš„æ¦‚ç‡å°äº
0ã€‚</li>
<li><strong>å³å›¾ï¼ˆé€»è¾‘å›å½’ï¼‰ï¼š</strong>S
å½¢è“è‰²æ›²çº¿æ­£ç¡®åœ°å°†æ¦‚ç‡è¾“å‡ºé™åˆ¶åœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚</li>
</ul>
<p><strong>2.å®ƒä¸é€‚ç”¨äºå¤šç±»åˆ«é—®é¢˜</strong>
å¦‚æœæ‚¨æœ‰ä¸¤ä¸ªä»¥ä¸Šçš„ç±»åˆ«ï¼ˆä¾‹å¦‚ï¼Œâ€œè½»åº¦â€ã€â€œä¸­åº¦â€ã€â€œé‡åº¦â€ï¼‰ï¼Œæ‚¨å¯èƒ½ä¼šå°†å®ƒä»¬ç¼–ç ä¸º
0ã€1 å’Œ
2ã€‚çº¿æ€§å›å½’æ¨¡å‹ä¼šé”™è¯¯åœ°å‡è®¾â€œè½»åº¦â€å’Œâ€œä¸­åº¦â€ä¹‹é—´çš„â€œè·ç¦»â€ä¸â€œä¸­åº¦â€å’Œâ€œé‡åº¦â€ä¹‹é—´çš„è·ç¦»ç›¸åŒï¼Œè¿™é€šå¸¸ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å‡è®¾ã€‚</p>
<p>## The Solution: Logistic Regression</p>
<p>Instead of modeling the response <span
class="math inline">\(y\)</span> directly, logistic regression models
the <strong>probability</strong> that <span
class="math inline">\(y\)</span> belongs to a particular class. To solve
the issue of the output not being a probability, it uses the
<strong>logistic function</strong> (also known as the sigmoid
function).</p>
<p>This function takes any real-valued input and squeezes it into an
output between 0 and 1.</p>
<p>The formula for the probability in a logistic regression model is:
<span class="math display">\[P(Y=1|X) = \frac{e^{\beta_0 + \beta_1 X}}{1
+ e^{\beta_0 + \beta_1 X}}\]</span> This S-shaped function, shown in the
right-hand plot above, ensures that the output is always a valid
probability. We can then set a threshold (e.g., 0.5) to make the final
class prediction. If <span class="math inline">\(P(Y=1|X) &gt;
0.5\)</span>, we predict â€˜Yesâ€™; otherwise, we predict â€˜Noâ€™.</p>
<p>## è§£å†³æ–¹æ¡ˆï¼šé€»è¾‘å›å½’</p>
<p>é€»è¾‘å›å½’ä¸æ˜¯ç›´æ¥å¯¹å“åº” <span class="math inline">\(y\)</span>
è¿›è¡Œå»ºæ¨¡ï¼Œè€Œæ˜¯å¯¹ <span class="math inline">\(y\)</span>
å±äºç‰¹å®šç±»åˆ«çš„<strong>æ¦‚ç‡</strong>è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¾“å‡ºä¸æ˜¯æ¦‚ç‡çš„é—®é¢˜ï¼Œå®ƒä½¿ç”¨äº†<strong>é€»è¾‘å‡½æ•°</strong>ï¼ˆä¹Ÿç§°ä¸º
S å‹å‡½æ•°ï¼‰ã€‚</p>
<p>æ­¤å‡½æ•°æ¥å—ä»»ä½•å®å€¼è¾“å…¥ï¼Œå¹¶å°†å…¶å‹ç¼©ä¸ºä»‹äº 0 å’Œ 1 ä¹‹é—´çš„è¾“å‡ºã€‚</p>
<p>é€»è¾‘å›å½’æ¨¡å‹ä¸­çš„æ¦‚ç‡å…¬å¼ä¸ºï¼š <span class="math display">\[P(Y=1|X) =
\frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}\]</span>
å¦‚ä¸Šå›¾å³ä¾§æ‰€ç¤ºï¼Œè¿™ä¸ª S
å½¢å‡½æ•°ç¡®ä¿è¾“å‡ºå§‹ç»ˆæ˜¯æœ‰æ•ˆæ¦‚ç‡ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ªé˜ˆå€¼ï¼ˆä¾‹å¦‚
0.5ï¼‰æ¥è¿›è¡Œæœ€ç»ˆçš„ç±»åˆ«é¢„æµ‹ã€‚å¦‚æœ <span class="math inline">\(P(Y=1|X)
&gt; 0.5\)</span>ï¼Œåˆ™é¢„æµ‹â€œæ˜¯â€ï¼›å¦åˆ™ï¼Œé¢„æµ‹â€œå¦â€ã€‚</p>
<p>## Data Visualization &amp; Code in Python</p>
<p>The slides use R to visualize the data. The boxplots are particularly
important because they show which variable is a better predictor.</p>
<ul>
<li><p><strong>Balance vs.Â Default:</strong> The boxplots for balance
show a clear difference. The median balance for those who default
(â€˜Yesâ€™) is much higher than for those who do not (â€˜Noâ€™). This suggests
<strong>balance is a strong predictor</strong>.</p></li>
<li><p><strong>Income vs.Â Default:</strong> The boxplots for income show
a lot of overlap. The median incomes for both groups are very similar.
This suggests <strong>income is a weak predictor</strong>.</p></li>
<li><p><strong>ä½™é¢
vs.Â è¿çº¦</strong>ï¼šä½™é¢çš„ç®±çº¿å›¾æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„å·®å¼‚ã€‚è¿çº¦è€…ï¼ˆâ€œæ˜¯â€ï¼‰çš„ä½™é¢ä¸­ä½æ•°è¿œé«˜äºæœªè¿çº¦è€…ï¼ˆâ€œå¦â€ï¼‰ã€‚è¿™è¡¨æ˜<strong>ä½™é¢æ˜¯ä¸€ä¸ªå¼ºæœ‰åŠ›çš„é¢„æµ‹æŒ‡æ ‡</strong>ã€‚</p></li>
<li><p><strong>æ”¶å…¥
vs.Â è¿çº¦</strong>ï¼šæ”¶å…¥çš„ç®±çº¿å›¾æ˜¾ç¤ºå‡ºå¾ˆå¤§çš„é‡å ã€‚ä¸¤ç»„çš„æ”¶å…¥ä¸­ä½æ•°éå¸¸ç›¸ä¼¼ã€‚è¿™è¡¨æ˜<strong>æ”¶å…¥æ˜¯ä¸€ä¸ªå¼±çš„é¢„æµ‹æŒ‡æ ‡</strong>ã€‚</p></li>
</ul>
<p>Hereâ€™s how you could perform similar analysis and modeling in Python
using <code>seaborn</code> and <code>scikit-learn</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;default_data.csv&#x27; has columns: &#x27;default&#x27; (Yes/No), &#x27;balance&#x27;, &#x27;income&#x27;</span></span><br><span class="line"><span class="comment"># You would load your data like this:</span></span><br><span class="line"><span class="comment"># df = pd.read_csv(&#x27;default_data.csv&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># For demonstration, let&#x27;s create some sample data</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;balance&#x27;</span>: [<span class="number">1200</span>, <span class="number">2100</span>, <span class="number">800</span>, <span class="number">1800</span>, <span class="number">500</span>, <span class="number">1600</span>, <span class="number">2200</span>, <span class="number">1900</span>],</span><br><span class="line">    <span class="string">&#x27;income&#x27;</span>: [<span class="number">45000</span>, <span class="number">60000</span>, <span class="number">30000</span>, <span class="number">55000</span>, <span class="number">25000</span>, <span class="number">48000</span>, <span class="number">70000</span>, <span class="number">65000</span>],</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>: [<span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. Data Visualization (like the slides) ---</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">14</span>, <span class="number">5</span>))</span><br><span class="line">fig.suptitle(<span class="string">&#x27;Predictor Analysis for Default&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boxplot for Balance</span></span><br><span class="line">sns.boxplot(ax=axes[<span class="number">0</span>], x=<span class="string">&#x27;default&#x27;</span>, y=<span class="string">&#x27;balance&#x27;</span>, data=df)</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&#x27;Balance vs. Default Status&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boxplot for Income</span></span><br><span class="line">sns.boxplot(ax=axes[<span class="number">1</span>], x=<span class="string">&#x27;default&#x27;</span>, y=<span class="string">&#x27;income&#x27;</span>, data=df)</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">&#x27;Income vs. Default Status&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. Logistic Regression Modeling ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert categorical &#x27;default&#x27; column to 0s and 1s</span></span><br><span class="line">df[<span class="string">&#x27;default_encoded&#x27;</span>] = df[<span class="string">&#x27;default&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x == <span class="string">&#x27;Yes&#x27;</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define features (X) and target (y)</span></span><br><span class="line">X = df[[<span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line">y = df[<span class="string">&#x27;default_encoded&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create and train the logistic regression model</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on new data</span></span><br><span class="line"><span class="comment"># For example, a person with a $2000 balance and $50,000 income</span></span><br><span class="line">new_customer = [[<span class="number">2000</span>, <span class="number">50000</span>]]</span><br><span class="line">predicted_prob = model.predict_proba(new_customer)</span><br><span class="line">prediction = model.predict(new_customer)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Customer data: Balance=2000, Income=50000&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Probability of No Default vs. Default: <span class="subst">&#123;predicted_prob&#125;</span>&quot;</span>) <span class="comment"># [[P(No), P(Yes)]]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Final Prediction (0=No, 1=Yes): <span class="subst">&#123;prediction&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="the-mathematical-foundation-of-logistic-regression">2. the
mathematical foundation of logistic regression</h1>
<p>This set of slides explains the mathematical foundation of logistic
regression, how its parameters are estimated using Maximum Likelihood
Estimation (MLE), and how an iterative algorithm called Newton-Raphson
is used to perform this estimation.</p>
<p>é€»è¾‘å›å½’çš„æ•°å­¦åŸºç¡€ã€å¦‚ä½•ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)
ä¼°è®¡å…¶å‚æ•°ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨åä¸º Newton-Raphson çš„è¿­ä»£ç®—æ³•è¿›è¡Œä¼°è®¡ã€‚</p>
<h2
id="the-logistic-regression-model-from-probabilities-to-log-oddsé€»è¾‘å›å½’æ¨¡å‹ä»æ¦‚ç‡åˆ°å¯¹æ•°å‡ ç‡">2.1
The Logistic Regression Model: From Probabilities to
Log-Oddsé€»è¾‘å›å½’æ¨¡å‹ï¼šä»æ¦‚ç‡åˆ°å¯¹æ•°å‡ ç‡</h2>
<p>The core of logistic regression is transforming a linear model into a
valid probability. This is done using the <strong>logistic
function</strong>, also known as the sigmoid function.
é€»è¾‘å›å½’çš„æ ¸å¿ƒæ˜¯å°†çº¿æ€§æ¨¡å‹è½¬æ¢ä¸ºæœ‰æ•ˆçš„æ¦‚ç‡ã€‚è¿™å¯ä»¥é€šè¿‡<strong>é€»è¾‘å‡½æ•°</strong>ï¼ˆä¹Ÿç§°ä¸º
S å‹å‡½æ•°ï¼‰æ¥å®ç°ã€‚ #### <strong>Key Mathematical Formulas</strong></p>
<ol type="1">
<li><p><strong>Probability of Class 1:</strong> The model assumes the
probability of an observation <span
class="math inline">\(\mathbf{x}\)</span> belonging to class 1 is given
by the sigmoid function: <span class="math display">\[
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-\beta^T \mathbf{x})} =
\frac{\exp(\beta^T \mathbf{x})}{1 + \exp(\beta^T \mathbf{x})}
\]</span> This function always outputs a value between 0 and 1, making
it perfect for modeling probabilities.</p></li>
<li><p><strong>Odds:</strong> The odds are the ratio of the probability
of an event happening to the probability of it not happening. <span
class="math display">\[
\text{Odds} = \frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})} = \exp(\beta^T
\mathbf{x})
\]</span></p></li>
<li><p><strong>Log-Odds (Logit):</strong> By taking the natural
logarithm of the odds, we get a linear relationship with the predictors.
This is called the <strong>logit transformation</strong>. <span
class="math display">\[
\text{logit}(P(y=1|\mathbf{x})) =
\log\left(\frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})}\right) = \beta^T
\mathbf{x}
\]</span> This final equation is the heart of the model. It states that
the log-odds of the outcome are a linear function of the predictors.
This provides a great interpretation: a one-unit increase in a predictor
<span class="math inline">\(x_j\)</span> changes the log-odds by <span
class="math inline">\(\beta_j\)</span>.</p></li>
<li><p><strong>ç±»åˆ« 1 çš„æ¦‚ç‡</strong>ï¼šè¯¥æ¨¡å‹å‡è®¾è§‚æµ‹å€¼ <span
class="math inline">\(\mathbf{x}\)</span> å±äºç±»åˆ« 1 çš„æ¦‚ç‡ç”± S
å‹å‡½æ•°ç»™å‡ºï¼š <span class="math display">\[
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-\beta^T \mathbf{x})} =
\frac{\exp(\beta^T \mathbf{x})}{1 + \exp(\beta^T \mathbf{x})}
\]</span> æ­¤å‡½æ•°çš„è¾“å‡ºå€¼å§‹ç»ˆä»‹äº 0 å’Œ 1
ä¹‹é—´ï¼Œéå¸¸é€‚åˆç”¨äºæ¦‚ç‡å»ºæ¨¡ã€‚</p></li>
<li><p><strong>å‡ ç‡</strong>ï¼š**å‡ ç‡æ˜¯äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ä¸ä¸å‘ç”Ÿçš„æ¦‚ç‡ä¹‹æ¯”ã€‚
<span class="math display">\[
\text{Odds} = \frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})} = \exp(\beta^T
\mathbf{x})
\]</span></p></li>
<li><p><strong>å¯¹æ•°æ¦‚ç‡
(Logit)</strong>ï¼šé€šè¿‡å¯¹æ¦‚ç‡å–è‡ªç„¶å¯¹æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ¦‚ç‡ä¸é¢„æµ‹å˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚è¿™è¢«ç§°ä¸º<strong>logit
å˜æ¢</strong>ã€‚ <span class="math display">\[
\text{logit}(P(y=1|\mathbf{x})) =
\log\left(\frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})}\right) = \beta^T
\mathbf{x}
\]</span>
æœ€åä¸€ä¸ªæ–¹ç¨‹æ˜¯æ¨¡å‹çš„æ ¸å¿ƒã€‚å®ƒæŒ‡å‡ºç»“æœçš„å¯¹æ•°æ¦‚ç‡æ˜¯é¢„æµ‹å˜é‡çš„çº¿æ€§å‡½æ•°ã€‚è¿™æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„è§£é‡Šï¼šé¢„æµ‹å˜é‡
<span class="math inline">\(x_j\)</span>
æ¯å¢åŠ ä¸€ä¸ªå•ä½ï¼Œå¯¹æ•°æ¦‚ç‡å°±ä¼šæ”¹å˜ <span
class="math inline">\(\beta_j\)</span>ã€‚</p></li>
</ol>
<h2
id="fitting-the-model-maximum-likelihood-estimation-mle-æ‹Ÿåˆæ¨¡å‹æœ€å¤§ä¼¼ç„¶ä¼°è®¡-mle">2.2
Fitting the Model: Maximum Likelihood Estimation (MLE)
æ‹Ÿåˆæ¨¡å‹ï¼šæœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)</h2>
<p>Unlike linear regression, which uses least squares to find the
best-fit line, logistic regression uses <strong>Maximum Likelihood
Estimation (MLE)</strong>. The goal of MLE is to find the parameter
values (the <span class="math inline">\(\beta\)</span> coefficients)
that maximize the probability of observing the actual data that we have.
ä¸ä½¿ç”¨æœ€å°äºŒä¹˜æ³•å¯»æ‰¾æœ€ä½³æ‹Ÿåˆçº¿çš„çº¿æ€§å›å½’ä¸åŒï¼Œé€»è¾‘å›å½’ä½¿ç”¨<strong>æœ€å¤§ä¼¼ç„¶ä¼°è®¡
(MLE)</strong>ã€‚MLE
çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä½¿è§‚æµ‹åˆ°å®é™…æ•°æ®çš„æ¦‚ç‡æœ€å¤§åŒ–çš„å‚æ•°å€¼ï¼ˆ<span
class="math inline">\(\beta\)</span> ç³»æ•°ï¼‰ã€‚</p>
<ol type="1">
<li><p><strong>Likelihood Function:</strong> This is the joint
probability of observing all the data points in our sample. Assuming
each observation is independent, itâ€™s the product of the individual
probabilities:
1.<strong>ä¼¼ç„¶å‡½æ•°</strong>ï¼šè¿™æ˜¯è§‚æµ‹åˆ°æ ·æœ¬ä¸­æ‰€æœ‰æ•°æ®ç‚¹çš„è”åˆæ¦‚ç‡ã€‚å‡è®¾æ¯ä¸ªè§‚æµ‹å€¼éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œå®ƒæ˜¯å„ä¸ªæ¦‚ç‡çš„ä¹˜ç§¯ï¼š
<span class="math display">\[
L(\beta) = \prod_{i=1}^{n} P(y_i|\mathbf{x}_i)
\]</span> A clever way to write this for a binary (0/1) outcome is:
<span class="math display">\[
L(\beta) = \prod_{i=1}^{n} \frac{\exp(y_i \beta^T \mathbf{x}_i)}{1 +
\exp(\beta^T \mathbf{x}_i)}
\]</span></p></li>
<li><p><strong>Log-Likelihood Function:</strong> Products are difficult
to work with mathematically, so we work with the logarithm of the
likelihood, which turns the product into a sum. Maximizing the
log-likelihood is the same as maximizing the likelihood.</p></li>
<li><p><strong>å¯¹æ•°ä¼¼ç„¶å‡½æ•°</strong>ï¼šä¹˜ç§¯åœ¨æ•°å­¦ä¸Šå¾ˆéš¾å¤„ç†ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ä¼¼ç„¶çš„å¯¹æ•°ï¼Œå°†ä¹˜ç§¯è½¬åŒ–ä¸ºå’Œã€‚æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ä¸æœ€å¤§åŒ–ä¼¼ç„¶ç›¸åŒã€‚
<span class="math display">\[
\ell(\beta) = \log(L(\beta)) = \sum_{i=1}^{n} \left[ y_i \beta^T
\mathbf{x}_i - \log(1 + \exp(\beta^T \mathbf{x}_i)) \right]
\]</span> <strong>Key Takeaway:</strong> The slides correctly state that
there is <strong>no explicit formula</strong> to solve for the <span
class="math inline">\(\hat{\beta}\)</span> that maximizes this function.
We must find it using a numerical optimization algorithm.
æ²¡æœ‰<strong>æ˜ç¡®çš„å…¬å¼</strong>æ¥æ±‚è§£æœ€å¤§åŒ–è¯¥å‡½æ•°çš„<span
class="math inline">\(\hat{\beta}\)</span>ã€‚æˆ‘ä»¬å¿…é¡»ä½¿ç”¨æ•°å€¼ä¼˜åŒ–ç®—æ³•æ¥æ‰¾åˆ°å®ƒã€‚</p></li>
</ol>
<h2 id="the-algorithm-newton-raphson-ç®—æ³•ç‰›é¡¿-æ‹‰å¤«æ£®ç®—æ³•">2.3 The
Algorithm: Newton-Raphson ç®—æ³•ï¼šç‰›é¡¿-æ‹‰å¤«æ£®ç®—æ³•</h2>
<p>The slides introduce the <strong>Newton-Raphson algorithm</strong> as
the method to find the optimal <span
class="math inline">\(\hat{\beta}\)</span>. Itâ€™s an efficient iterative
algorithm for finding the roots of a function (i.e., where <span
class="math inline">\(f(x)=0\)</span>).</p>
<p><strong>How does this apply to logistic regression?</strong> To
maximize the log-likelihood function <span
class="math inline">\(\ell(\beta)\)</span>, we need to find the point
where its derivative (gradient) is equal to zero. So, Newton-Raphson is
used to solve <span class="math inline">\(\frac{d\ell(\beta)}{d\beta} =
0\)</span>.</p>
<p>å®ƒæ˜¯ä¸€ç§é«˜æ•ˆçš„è¿­ä»£ç®—æ³•ï¼Œç”¨äºæ±‚å‡½æ•°çš„æ ¹ï¼ˆå³ï¼Œå½“<span
class="math inline">\(f(x)=0\)</span>æ—¶ï¼‰ã€‚</p>
<p><strong>è¿™å¦‚ä½•åº”ç”¨äºé€»è¾‘å›å½’ï¼Ÿ</strong> ä¸ºäº†æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶å‡½æ•° <span
class="math inline">\(\ell(\beta)\)</span>ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°å…¶å¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰ç­‰äºé›¶çš„ç‚¹ã€‚å› æ­¤ï¼Œç‰›é¡¿-æ‹‰å¤«æ£®æ³•ç”¨äºæ±‚è§£
<span class="math inline">\(\frac{d\ell(\beta)}{d\beta} =
0\)</span>ã€‚</p>
<h4 id="the-general-newton-raphson-method"><strong>The General
Newton-Raphson Method</strong></h4>
<p>The algorithm starts with an initial guess, <span
class="math inline">\(x^{old}\)</span>, and iteratively refines it using
the following update rule, which is based on a Taylor series
approximation: <span class="math display">\[
x^{new} = x^{old} - \frac{f(x^{old})}{f&#39;(x^{old})}
\]</span> where <span class="math inline">\(f&#39;(x)\)</span> is the
derivative of <span class="math inline">\(f(x)\)</span>. You repeat this
step until the value of <span class="math inline">\(x\)</span>
converges.</p>
<p>è¯¥ç®—æ³•ä»åˆå§‹ä¼°è®¡ <span class="math inline">\(x^{old}\)</span>
å¼€å§‹ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹åŸºäºæ³°å‹’çº§æ•°è¿‘ä¼¼çš„æ›´æ–°è§„åˆ™è¿­ä»£åœ°å¯¹å…¶è¿›è¡Œä¼˜åŒ–ï¼š <span
class="math display">\[
x^{new} = x^{old} - \frac{f(x^{old})}{f&#39;(x^{old})}
\]</span> å…¶ä¸­ <span class="math inline">\(f&#39;(x)\)</span> æ˜¯ <span
class="math inline">\(f(x)\)</span> çš„å¯¼æ•°ã€‚é‡å¤æ­¤æ­¥éª¤ï¼Œç›´åˆ° <span
class="math inline">\(x\)</span> çš„å€¼æ”¶æ•›ã€‚</p>
<h4
id="important-image-newton-raphson-example-x3---4-0"><strong>Important
Image: Newton-Raphson Example (<span class="math inline">\(x^3 - 4 =
0\)</span>)</strong></h4>
<p>[Image showing iterations of Newton-Raphson]</p>
<p>This slide is a great illustration of the algorithmâ€™s power. *
<strong>Goal:</strong> Find <span class="math inline">\(x\)</span> such
that <span class="math inline">\(f(x) = x^3 - 4 = 0\)</span>. *
<strong>Function:</strong> <span class="math inline">\(f(x) = x^3 -
4\)</span> * <strong>Derivative:</strong> <span
class="math inline">\(f&#39;(x) = 3x^2\)</span> * <strong>Update
Rule:</strong> <span class="math inline">\(x^{new} = x^{old} -
\frac{(x^{old})^3 - 4}{3(x^{old})^2}\)</span> Starting with a guess of
<span class="math inline">\(x^{old} = 2\)</span>, the algorithm
converges to the true answer (<span class="math inline">\(4^{1/3}
\approx 1.5874\)</span>) in just 4 steps.</p>
<ul>
<li><strong>ç›®æ ‡</strong>ï¼šæ‰¾åˆ° <span
class="math inline">\(x\)</span>ï¼Œä½¿å¾— <span class="math inline">\(f(x)
= x^3 - 4 = 0\)</span>ã€‚</li>
<li><strong>å‡½æ•°</strong>ï¼š<span class="math inline">\(f(x) = x^3 -
4\)</span></li>
<li><strong>å¯¼æ•°</strong>ï¼š<span class="math inline">\(f&#39;(x) =
3x^2\)</span></li>
<li><strong>æ›´æ–°è§„åˆ™</strong>ï¼š<span class="math inline">\(x^{new} =
x^{old} - \frac{(x^{old})^3 - 4}{3(x^{old})^2}\)</span> ä» <span
class="math inline">\(x^{old} = 2\)</span> çš„çŒœæµ‹å¼€å§‹ï¼Œè¯¥ç®—æ³•ä»…ç”¨ 4
æ­¥å°±æ”¶æ•›åˆ°çœŸå®ç­”æ¡ˆ (<span class="math inline">\(4^{1/3} \approx
1.5874\)</span>)ã€‚</li>
</ul>
<h4 id="code-understanding-python"><strong>Code Understanding
(Python)</strong></h4>
<p>The slides show Python code implementing Newton-Raphson. Letâ€™s break
down the key function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the function we want to find the root of</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) - x*x + <span class="number">3</span> * np.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define its derivative</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_prime</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) - <span class="number">2</span>*x + <span class="number">3</span> * np.cos(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Newton-Raphson method</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">newton_raphson</span>(<span class="params">x0, tol=<span class="number">1e-10</span>, max_iter=<span class="number">100</span></span>):</span><br><span class="line">    x = x0 <span class="comment"># Start with the initial guess</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        fx = f(x)      <span class="comment"># Calculate f(x_old)</span></span><br><span class="line">        fpx = f_prime(x) <span class="comment"># Calculate f&#x27;(x_old)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> fpx == <span class="number">0</span>: <span class="comment"># Cannot divide by zero</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Zero derivative. No solution found.&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is the core update rule</span></span><br><span class="line">        x_new = x - fx / fpx</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check if the change is small enough to stop</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(x_new - x) &lt; tol:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Converged to <span class="subst">&#123;x_new&#125;</span> after <span class="subst">&#123;i+<span class="number">1</span>&#125;</span> iterations.&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> x_new</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update x for the next iteration</span></span><br><span class="line">        x = x_new</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Exceeded maximum iterations. No solution found.&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial guess and execution</span></span><br><span class="line">x0 = <span class="number">0.5</span></span><br><span class="line">root = newton_raphson(x0)</span><br></pre></td></tr></table></figure>
<p>The slides show that with a good initial guess
(<code>x0 = 0.5</code>), the algorithm converges quickly. With a bad one
(<code>x0 = 50</code>), it still converges but takes many more steps.
This highlights the importance of the starting point. The slides also
show an implementation of <strong>Gradient Descent</strong>, another
popular optimization algorithm which uses the update rule
<code>x_new = x - learning_rate * gradient</code>.</p>
<h1
id="provide-a-great-case-study-on-logistic-regression-particularly-on-the-important-concept-of-confounding-variables.-heres-a-summary-covering-the-math-code-and-key-insights.">Provide
a great case study on logistic regression, particularly on the important
concept of confounding variables. Hereâ€™s a summary covering the math,
code, and key insights.</h1>
<ol start="3" type="1">
<li><h1 id="core-concept-logistic-regression-æ ¸å¿ƒæ¦‚å¿µé€»è¾‘å›å½’">Core
Concept: Logistic Regression ğŸ“ˆ # æ ¸å¿ƒæ¦‚å¿µï¼šé€»è¾‘å›å½’ ğŸ“ˆ</h1></li>
</ol>
<p>Logistic regression is a statistical method used for <strong>binary
classification</strong>, which means predicting an outcome that can only
be one of two things (e.g., Yes/No, True/False, 1/0).</p>
<p>In this example, the goal is to predict the probability that a
customer will <strong>default</strong> on a loan (Yes or No) based on
factors like their account <code>balance</code>, <code>income</code>,
and whether they are a <code>student</code>.</p>
<p>The core of logistic regression is the <strong>sigmoid (or logistic)
function</strong>, which takes any real-valued number and squishes it to
a value between 0 and 1, representing a probability.</p>
<p><span class="math display">\[
\hat{P}(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_p
X_p)}}
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{P}(Y=1|X)\)</span> is the predicted
probability of the outcome being â€œYesâ€ (e.g., default).</li>
<li><span class="math inline">\(\beta_0\)</span> is the intercept.</li>
<li><span class="math inline">\(\beta_1, ..., \beta_p\)</span> are the
coefficients for each input variable (<span class="math inline">\(X_1,
..., X_p\)</span>). The modelâ€™s job is to find the best values for these
<span class="math inline">\(\beta\)</span> coefficients.</li>
</ul>
<hr />
<p>é€»è¾‘å›å½’æ˜¯ä¸€ç§ç”¨äº<strong>äºŒå…ƒåˆ†ç±»</strong>çš„ç»Ÿè®¡æ–¹æ³•ï¼Œè¿™æ„å‘³ç€é¢„æµ‹ç»“æœåªèƒ½æ˜¯ä¸¤ç§æƒ…å†µä¹‹ä¸€ï¼ˆä¾‹å¦‚ï¼Œæ˜¯/å¦ã€çœŸ/å‡ã€1/0ï¼‰ã€‚</p>
<p>åœ¨æœ¬ä¾‹ä¸­ï¼Œç›®æ ‡æ˜¯æ ¹æ®å®¢æˆ·è´¦æˆ·â€œä½™é¢â€ã€â€œæ”¶å…¥â€ä»¥åŠæ˜¯å¦ä¸ºâ€œå­¦ç”Ÿâ€ç­‰å› ç´ ï¼Œé¢„æµ‹å®¢æˆ·<strong>æ‹–æ¬ </strong>è´·æ¬¾ï¼ˆæ˜¯æˆ–å¦ï¼‰çš„æ¦‚ç‡ã€‚</p>
<p>é€»è¾‘å›å½’çš„æ ¸å¿ƒæ˜¯<strong>Sigmoidï¼ˆæˆ–é€»è¾‘ï¼‰å‡½æ•°</strong>ï¼Œå®ƒå°†ä»»ä½•å®æ•°å‹ç¼©ä¸ºä»‹äº
0 å’Œ 1 ä¹‹é—´çš„å€¼ï¼Œä»¥è¡¨ç¤ºæ¦‚ç‡ã€‚</p>
<p><span class="math display">\[
\hat{P}(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_p
X_p)}}
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{P}(Y=1|X)\)</span>
æ˜¯ç»“æœä¸ºâ€œæ˜¯â€ï¼ˆä¾‹å¦‚ï¼Œé»˜è®¤ï¼‰çš„é¢„æµ‹æ¦‚ç‡ã€‚</li>
<li><span class="math inline">\(\beta_0\)</span> æ˜¯æˆªè·ã€‚</li>
<li><span class="math inline">\(\beta_1, ..., \beta_p\)</span>
æ˜¯æ¯ä¸ªè¾“å…¥å˜é‡ (<span class="math inline">\(X_1, ..., X_p\)</span>)
çš„ç³»æ•°ã€‚æ¨¡å‹çš„ä»»åŠ¡æ˜¯æ‰¾åˆ°è¿™äº› <span class="math inline">\(\beta\)</span>
ç³»æ•°çš„æœ€ä½³å€¼ã€‚</li>
</ul>
<h2 id="how-the-model-learns-mathematical-foundation">3.1 How the Model
â€œLearnsâ€ (Mathematical Foundation)</h2>
<p>The slides show that the modelâ€™s coefficients (<span
class="math inline">\(\beta\)</span>) are found using an algorithm like
<strong>Newton-Raphson</strong>. This is an iterative process to find
the values that <strong>maximize the log-likelihood function</strong>.
Think of this as finding the coefficient values that make the observed
data most
probable.è¿™æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œç”¨äºæŸ¥æ‰¾<strong>æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶å‡½æ•°</strong>çš„å€¼ã€‚å¯ä»¥å°†å…¶è§†ä¸ºæŸ¥æ‰¾ä½¿è§‚æµ‹æ•°æ®æ¦‚ç‡æœ€å¤§çš„ç³»æ•°å€¼ã€‚</p>
<p>The key slide for this is the one titled â€œNewton-Raphson Iterative
Algorithmâ€. It shows the formulas for: * The <strong>Gradient</strong>
(<span class="math inline">\(\nabla\ell\)</span>): The direction of the
steepest ascent of the log-likelihood function. * The
<strong>Hessian</strong> (<span class="math inline">\(H\)</span>): The
curvature of the log-likelihood function.</p>
<ul>
<li><strong>æ¢¯åº¦</strong> (<span
class="math inline">\(\nabla\ell\)</span>)ï¼šå¯¹æ•°ä¼¼ç„¶å‡½æ•°æœ€é™¡ä¸Šå‡çš„æ–¹å‘ã€‚</li>
<li><strong>é»‘æ£®çŸ©é˜µ</strong> (<span
class="math inline">\(H\)</span>)ï¼šå¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„æ›²ç‡ã€‚</li>
</ul>
<p>The updating rule is given by: <span class="math display">\[
\beta^{new} = \beta^{old} - H^{-1}\nabla\ell
\]</span> This formula is used repeatedly until the coefficient values
stop changing significantly, meaning the algorithm has converged to the
best fit. This process is also referred to as <strong>Iteratively
Reweighted Least Squares (IRLS)</strong>.
æ­¤å…¬å¼åå¤ä½¿ç”¨ï¼Œç›´åˆ°ç³»æ•°å€¼ä¸å†å‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œè¿™æ„å‘³ç€ç®—æ³•å·²æ”¶æ•›åˆ°æœ€ä½³æ‹Ÿåˆå€¼ã€‚æ­¤è¿‡ç¨‹ä¹Ÿç§°ä¸º<strong>è¿­ä»£é‡åŠ æƒæœ€å°äºŒä¹˜æ³•
(IRLS)</strong>ã€‚</p>
<hr />
<h2 id="the-puzzle-a-tale-of-two-models">3.2 The Puzzle: A Tale of Two
Models ğŸ•µï¸â€â™‚ï¸</h2>
<p>The most important story in these slides is how the effect of being a
student changes depending on the model. This is a classic example of a
<strong>confounding variable</strong>.</p>
<h4 id="model-1-simple-logistic-regression-default-vs.-student">Model 1:
Simple Logistic Regression (Default vs.Â Student)</h4>
<p>When predicting default using <em>only</em> student status, the model
is: <code>default ~ student</code></p>
<p>From the slides, the coefficients are: * Intercept (<span
class="math inline">\(\beta_0\)</span>): -3.5041 * student[Yes] (<span
class="math inline">\(\beta_1\)</span>): <strong>0.4049</strong>
(positive)</p>
<p>The equation for the log-odds is: <span class="math display">\[
\log\left(\frac{P(\text{default})}{1-P(\text{default})}\right) = -3.5041
+ 0.4049 \times (\text{is\_student})
\]</span></p>
<p><strong>Conclusion:</strong> The positive coefficient (0.4049)
suggests that <strong>students are more likely to default</strong> than
non-students. The slides calculate the probabilities: * <strong>Student
Default Probability:</strong> 4.31% * <strong>Non-Student Default
Probability:</strong> 2.92%</p>
<p>å­¦ç”Ÿèº«ä»½çš„å½±å“å¦‚ä½•æ ¹æ®æ¨¡å‹è€Œå˜åŒ–ã€‚è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„<strong>æ··æ‚å˜é‡</strong>çš„ä¾‹å­ã€‚</p>
<h4 id="æ¨¡å‹-1ç®€å•é€»è¾‘å›å½’è¿çº¦-vs.-å­¦ç”Ÿ">æ¨¡å‹ 1ï¼šç®€å•é€»è¾‘å›å½’ï¼ˆè¿çº¦
vs.Â å­¦ç”Ÿï¼‰</h4>
<p>ä»…ä½¿ç”¨å­¦ç”Ÿèº«ä»½é¢„æµ‹è¿çº¦æ—¶ï¼Œæ¨¡å‹ä¸ºï¼š <code>default ~ student</code></p>
<p>å¹»ç¯ç‰‡ä¸­æ˜¾ç¤ºçš„ç³»æ•°ä¸ºï¼š * æˆªè· (<span
class="math inline">\(\beta_0\)</span>): -3.5041 * å­¦ç”Ÿ[æ˜¯] (<span
class="math inline">\(\beta_1\)</span>):
<strong>0.4049</strong>ï¼ˆæ­£ï¼‰</p>
<p>å¯¹æ•°æ¦‚ç‡å…¬å¼ä¸ºï¼š <span class="math display">\[
\log\left(\frac{P(\text{default})}{1-P(\text{default})}\right) = -3.5041
+ 0.4049 \times (\text{is\_student})
\]</span></p>
<p><strong>ç»“è®º</strong>ï¼šæ­£ç³»æ•° (0.4049)
è¡¨æ˜<strong>å­¦ç”Ÿæ¯”éå­¦ç”Ÿæ›´æœ‰å¯èƒ½è¿çº¦</strong>ã€‚å¹»ç¯ç‰‡è®¡ç®—äº†ä»¥ä¸‹æ¦‚ç‡ï¼š *
<strong>å­¦ç”Ÿè¿çº¦æ¦‚ç‡</strong>ï¼š4.31% *
<strong>éå­¦ç”Ÿè¿çº¦æ¦‚ç‡</strong>ï¼š2.92%</p>
<h2
id="model-2-multiple-logistic-regression-default-vs.-all-variables-æ¨¡å‹-2å¤šå…ƒé€»è¾‘å›å½’è¿çº¦-vs.-æ‰€æœ‰å˜é‡">3.3
Model 2: Multiple Logistic Regression (Default vs.Â All Variables) æ¨¡å‹
2ï¼šå¤šå…ƒé€»è¾‘å›å½’ï¼ˆè¿çº¦ vs.Â æ‰€æœ‰å˜é‡ï¼‰</h2>
<p>When we add <code>balance</code> and <code>income</code> to the
model, it becomes: <code>default ~ student + balance + income</code></p>
<p>From the slides, the new coefficients are: * Intercept (<span
class="math inline">\(\beta_0\)</span>): -10.8690 * balance (<span
class="math inline">\(\beta_1\)</span>): 0.0057 * income (<span
class="math inline">\(\beta_2\)</span>): 0.0030 * student[Yes] (<span
class="math inline">\(\beta_3\)</span>): <strong>-0.6468</strong>
(negative)</p>
<p><strong>The Shocking Twist!</strong> The coefficient for
<code>student[Yes]</code> is now <strong>negative</strong>.</p>
<p><strong>Conclusion:</strong> When we control for balance and income,
<strong>students are actually <em>less</em> likely to default</strong>
than non-students with the same balance and income.</p>
<h4 id="why-the-change-the-confounding-variable-explained">Why the
Change? The Confounding Variable Explained</h4>
<p>The key insight, explained on the slide with multi-colored text
bubbles, is that <strong>students, on average, have higher credit card
balances</strong>.</p>
<ul>
<li>In the simple model, the <code>student</code> variable was
inadvertently capturing the risk associated with having a high
<code>balance</code>. The model mistakenly concluded â€œbeing a student
causes default.â€</li>
<li>In the multiple model, the <code>balance</code> variable properly
accounts for the risk from a high balance. With that effect isolated,
the <code>student</code> variable can show its true, underlying
relationship with default, which is negative.</li>
</ul>
<p>This demonstrates why itâ€™s crucial to consider multiple relevant
variables to avoid drawing incorrect conclusions. <strong>The most
important slides are the ones that present this paradox and its
explanation.</strong></p>
<p><strong>ä»¤äººéœ‡æƒŠçš„è½¬æŠ˜ï¼</strong> <code>student[Yes]</code>
çš„ç³»æ•°ç°åœ¨ä¸º<strong>è´Ÿ</strong>ã€‚</p>
<p><strong>ç»“è®ºï¼š</strong>å½“æˆ‘ä»¬æ§åˆ¶ä½™é¢å’Œæ”¶å…¥æ—¶ï¼Œ<strong>å­¦ç”Ÿå®é™…ä¸Šæ¯”å…·æœ‰ç›¸åŒä½™é¢å’Œæ”¶å…¥çš„éå­¦ç”Ÿæ›´<em>ä½</em>äºè¿çº¦</strong>ã€‚</p>
<h4 id="ä¸ºä»€ä¹ˆä¼šæœ‰å˜åŒ–æ··æ‚å˜é‡è§£é‡Š">ä¸ºä»€ä¹ˆä¼šæœ‰å˜åŒ–ï¼Ÿæ··æ‚å˜é‡è§£é‡Š</h4>
<p>å¹»ç¯ç‰‡ä¸Šç”¨å½©è‰²æ–‡å­—æ°”æ³¡è§£é‡Šäº†å…³é”®çš„è§è§£ï¼Œå³<strong>å­¦ç”Ÿå¹³å‡æ‹¥æœ‰æ›´é«˜çš„ä¿¡ç”¨å¡ä½™é¢</strong>ã€‚</p>
<ul>
<li>åœ¨ç®€å•æ¨¡å‹ä¸­ï¼Œâ€œå­¦ç”Ÿâ€å˜é‡æ— æ„ä¸­æ•æ‰åˆ°äº†é«˜ä½™é¢å¸¦æ¥çš„é£é™©ã€‚è¯¥æ¨¡å‹é”™è¯¯åœ°å¾—å‡ºäº†â€œå­¦ç”Ÿèº«ä»½å¯¼è‡´è¿çº¦â€çš„ç»“è®ºã€‚</li>
<li>åœ¨å¤šå…ƒæ¨¡å‹ä¸­ï¼Œâ€œä½™é¢â€å˜é‡æ­£ç¡®åœ°è§£é‡Šäº†é«˜ä½™é¢å¸¦æ¥çš„é£é™©ã€‚åœ¨åˆ†ç¦»å‡ºè¿™ä¸€å½±å“åï¼Œâ€œå­¦ç”Ÿâ€å˜é‡å¯ä»¥æ˜¾ç¤ºå…¶ä¸è¿çº¦ä¹‹é—´çœŸå®çš„æ½œåœ¨å…³ç³»ï¼Œå³è´Ÿç›¸å…³å…³ç³»ã€‚</li>
</ul>
<p>è¿™è¯´æ˜äº†ä¸ºä»€ä¹ˆè€ƒè™‘å¤šä¸ªç›¸å…³å˜é‡ä»¥é¿å…å¾—å‡ºé”™è¯¯ç»“è®ºè‡³å…³é‡è¦ã€‚</p>
<hr />
<h3 id="code-implementation-r-vs.-python">Code Implementation: R
vs.Â Python</h3>
<p>The slides use Râ€™s <code>glm()</code> (Generalized Linear Model)
function. Hereâ€™s how you would replicate this in Python.</p>
<h4 id="r-code-from-slides">R Code (from slides)</h4>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Simple Model</span></span><br><span class="line">glmod2 <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>default <span class="operator">~</span> student<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> family<span class="operator">=</span>binomial<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>glmod2<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model</span></span><br><span class="line">glmod3 <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>default <span class="operator">~</span> .<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> family<span class="operator">=</span>binomial<span class="punctuation">)</span> <span class="comment"># &#x27;.&#x27; means all other variables</span></span><br><span class="line">summary<span class="punctuation">(</span>glmod3<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<h4 id="python-equivalent">Python Equivalent</h4>
<p>We can use two popular libraries: <code>statsmodels</code> (which
gives R-style summaries) and <code>scikit-learn</code> (the standard for
machine learning).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Default&#x27; is a pandas DataFrame with columns:</span></span><br><span class="line"><span class="comment"># &#x27;default&#x27; (0/1), &#x27;student&#x27; (0/1), &#x27;balance&#x27;, &#x27;income&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Using statsmodels (recommended for interpretation) ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data</span></span><br><span class="line"><span class="comment"># For statsmodels, we need to manually add the intercept</span></span><br><span class="line">X_simple = Default[[<span class="string">&#x27;student&#x27;</span>]]</span><br><span class="line">X_simple = sm.add_constant(X_simple)</span><br><span class="line">y = Default[<span class="string">&#x27;default&#x27;</span>]</span><br><span class="line"></span><br><span class="line">X_multiple = Default[[<span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line">X_multiple = sm.add_constant(X_multiple)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simple Model: default ~ student</span></span><br><span class="line">model_simple = sm.Logit(y, X_simple).fit()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- Simple Model ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_simple.summary())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model: default ~ student + balance + income</span></span><br><span class="line">model_multiple = sm.Logit(y, X_multiple).fit()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Multiple Model ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_multiple.summary())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Using scikit-learn (recommended for prediction tasks) ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data (scikit-learn adds intercept by default)</span></span><br><span class="line">X_simple_sk = Default[[<span class="string">&#x27;student&#x27;</span>]]</span><br><span class="line">y_sk = Default[<span class="string">&#x27;default&#x27;</span>]</span><br><span class="line"></span><br><span class="line">X_multiple_sk = Default[[<span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simple Model</span></span><br><span class="line">clf_simple = LogisticRegression().fit(X_simple_sk, y_sk)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nSimple Model Intercept (scikit-learn): <span class="subst">&#123;clf_simple.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Simple Model Coefficient (scikit-learn): <span class="subst">&#123;clf_simple.coef_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model</span></span><br><span class="line">clf_multiple = LogisticRegression().fit(X_multiple_sk, y_sk)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nMultiple Model Intercept (scikit-learn): <span class="subst">&#123;clf_multiple.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Multiple Model Coefficients (scikit-learn): <span class="subst">&#123;clf_multiple.coef_&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1
id="making-predictions-and-the-decision-boundary-è¿›è¡Œé¢„æµ‹å’Œå†³ç­–è¾¹ç•Œ">4
Making Predictions and the Decision Boundary ğŸ¯è¿›è¡Œé¢„æµ‹å’Œå†³ç­–è¾¹ç•Œ</h1>
<p>Once the model is trained (i.e., we have the coefficients <span
class="math inline">\(\hat{\beta}\)</span>), we can make predictions.
ä¸€æ—¦æ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆå³ï¼Œæˆ‘ä»¬æœ‰äº†ç³»æ•° <span
class="math inline">\(\hat{\beta}\)</span>ï¼‰ï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œé¢„æµ‹äº†ã€‚ ##
Math Behind Predictions</p>
<p>The model outputs the <strong>log-odds</strong>, which can be
converted into a probability. A key concept is the <strong>decision
boundary</strong>, which is the threshold where the model is uncertain
(probability = 50%).
æ¨¡å‹è¾“å‡º<strong>å¯¹æ•°æ¦‚ç‡</strong>ï¼Œå®ƒå¯ä»¥è½¬æ¢ä¸ºæ¦‚ç‡ã€‚ä¸€ä¸ªå…³é”®æ¦‚å¿µæ˜¯<strong>å†³ç­–è¾¹ç•Œ</strong>ï¼Œå®ƒæ˜¯æ¨¡å‹ä¸ç¡®å®šçš„é˜ˆå€¼ï¼ˆæ¦‚ç‡
= 50%ï¼‰ã€‚</p>
<ol type="1">
<li><p><strong>The Estimated Odds</strong>: The core output of the
linear part of the model is the exponential of the linear equation,
which gives the odds of the outcome being â€˜Yesâ€™ (or 1).
<strong>ä¼°è®¡æ¦‚ç‡</strong>ï¼šæ¨¡å‹çº¿æ€§éƒ¨åˆ†çš„æ ¸å¿ƒè¾“å‡ºæ˜¯çº¿æ€§æ–¹ç¨‹çš„æŒ‡æ•°ï¼Œå®ƒç»™å‡ºäº†ç»“æœä¸ºâ€œæ˜¯â€ï¼ˆæˆ–
1ï¼‰çš„æ¦‚ç‡ã€‚</p>
<p><span class="math display">\[
\]</span>$$\frac{\hat{P}(y=1|\mathbf{x}_0)}{\hat{P}(y=0|\mathbf{x}_0)} =
\exp(\hat{\beta}^\top \mathbf{x}_0)</p>
<p><span class="math display">\[
\]</span><span class="math display">\[
\]</span></p></li>
<li><p><strong>The Decision Rule</strong>: We classify a new observation
<span class="math inline">\(\mathbf{x}_0\)</span> by comparing its
predicted odds to a threshold <span
class="math inline">\(\delta\)</span>.
<strong>å†³ç­–è§„åˆ™</strong>ï¼šæˆ‘ä»¬é€šè¿‡æ¯”è¾ƒæ–°è§‚æµ‹å€¼ <span
class="math inline">\(\mathbf{x}_0\)</span> çš„é¢„æµ‹æ¦‚ç‡ä¸é˜ˆå€¼ <span
class="math inline">\(\delta\)</span> æ¥å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚</p>
<ul>
<li>Predict <span class="math inline">\(y=1\)</span> if <span
class="math inline">\(\exp(\hat{\beta}^\top \mathbf{x}_0) &gt;
\delta\)</span></li>
<li>Predict <span class="math inline">\(y=0\)</span> if <span
class="math inline">\(\exp(\hat{\beta}^\top \mathbf{x}_0) &lt;
\delta\)</span> A common default is <span
class="math inline">\(\delta=1\)</span>, which means we predict â€˜Yesâ€™ if
the probability is greater than 0.5.</li>
</ul></li>
<li><p><strong>The Linear Boundary</strong>: The decision boundary
itself is where the odds are exactly equal to the threshold. By taking
the logarithm, we see that this boundary is a <strong>linear
equation</strong>. This is why logistic regression is called a
<strong>linear classifier</strong>.
<strong>çº¿æ€§è¾¹ç•Œ</strong>ï¼šå†³ç­–è¾¹ç•Œæœ¬èº«å°±æ˜¯æ¦‚ç‡æ°å¥½ç­‰äºé˜ˆå€¼çš„åœ°æ–¹ã€‚å–å¯¹æ•°åï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸ªè¾¹ç•Œæ˜¯ä¸€ä¸ª<strong>çº¿æ€§æ–¹ç¨‹</strong>ã€‚è¿™å°±æ˜¯é€»è¾‘å›å½’è¢«ç§°ä¸º<strong>çº¿æ€§åˆ†ç±»å™¨</strong>çš„åŸå› ã€‚
<span class="math display">\[
\]</span>$$\hat{\beta}^\top \mathbf{x} = \log(\delta)</p>
<p><span class="math display">\[
\]</span>$$For <span class="math inline">\(\delta=1\)</span>, the
boundary is simply <span class="math inline">\(\hat{\beta}^\top
\mathbf{x} = 0\)</span>.</p></li>
</ol>
<p>This concept is visualized perfectly in the slide titled â€œLinear
Classifier,â€ which shows a straight line neatly separating two classes
of data points.
é¢˜ä¸ºâ€œçº¿æ€§åˆ†ç±»å™¨â€çš„å¹»ç¯ç‰‡å®Œç¾åœ°å±•ç¤ºäº†è¿™ä¸€æ¦‚å¿µï¼Œå®ƒå±•ç¤ºäº†ä¸€æ¡ç›´çº¿ï¼Œå°†ä¸¤ç±»æ•°æ®ç‚¹å·§å¦™åœ°åˆ†éš”å¼€æ¥ã€‚</p>
<h2 id="visualizing-the-confounding-effect">Visualizing the Confounding
Effect</h2>
<p>The most important image in this set is <strong>Figure 4.3</strong>,
as it visually explains the confounding puzzle from the first set of
slides.</p>
<ul>
<li><strong>Right Panel (Boxplots)</strong>: This shows that
<strong>students (Yes) tend to have higher credit card balances</strong>
than non-students (No). This is the source of the confounding.</li>
<li><strong>Left Panel (Default Rates)</strong>:
<ul>
<li>The <strong>dashed lines</strong> show the <em>overall</em> default
rates. The orange line (students) is higher than the blue line
(non-students). This matches our simple model
(<code>default ~ student</code>).</li>
<li>The <strong>solid S-shaped curves</strong> show the probability of
default as a function of balance. For any <em>given</em> balance, the
blue curve (non-students) is slightly higher than the orange curve
(students). This means that <strong>at the same level of debt, students
are <em>less</em> likely to default</strong>. This matches our multiple
regression model
(<code>default ~ student + balance + income</code>).</li>
</ul></li>
</ul>
<p>This single figure brilliantly illustrates how a variable can appear
to have one effect in isolation but the opposite effect when controlling
for a confounding factor. *
<strong>å³ä¾§é¢æ¿ï¼ˆç®±çº¿å›¾ï¼‰</strong>ï¼šè¿™è¡¨æ˜<strong>å­¦ç”Ÿï¼ˆæ˜¯ï¼‰çš„ä¿¡ç”¨å¡ä½™é¢å¾€å¾€é«˜äºéå­¦ç”Ÿï¼ˆå¦ï¼‰ã€‚è¿™å°±æ˜¯æ··æ‚æ•ˆåº”çš„æ ¹æºã€‚
* </strong>å·¦å›¾ï¼ˆè¿çº¦ç‡ï¼‰<strong>ï¼š *
</strong>è™šçº¿<strong>æ˜¾ç¤º<em>æ€»ä½“</em>è¿çº¦ç‡ã€‚æ©™è‰²çº¿ï¼ˆå­¦ç”Ÿï¼‰é«˜äºè“è‰²çº¿ï¼ˆéå­¦ç”Ÿï¼‰ã€‚è¿™ä¸æˆ‘ä»¬çš„ç®€å•æ¨¡å‹ï¼ˆâ€œè¿çº¦
~ å­¦ç”Ÿâ€ï¼‰ç›¸ç¬¦ã€‚ * </strong>S
å½¢å®çº¿<strong>æ˜¾ç¤ºè¿çº¦æ¦‚ç‡ä¸ä½™é¢çš„å…³ç³»ã€‚å¯¹äºä»»ä½•<em>ç»™å®š</em>çš„ä½™é¢ï¼Œè“è‰²æ›²çº¿ï¼ˆéå­¦ç”Ÿï¼‰ç•¥é«˜äºæ©™è‰²æ›²çº¿ï¼ˆå­¦ç”Ÿï¼‰ã€‚è¿™æ„å‘³ç€</strong>åœ¨ç›¸åŒçš„å€ºåŠ¡æ°´å¹³ä¸‹ï¼Œå­¦ç”Ÿè¿çº¦çš„å¯èƒ½æ€§<em>è¾ƒå°</em>ã€‚è¿™ä¸æˆ‘ä»¬çš„å¤šå…ƒå›å½’æ¨¡å‹ï¼ˆâ€œè¿çº¦
~ å­¦ç”Ÿ + ä½™é¢ + æ”¶å…¥â€ï¼‰ç›¸ç¬¦ã€‚</p>
<p>è¿™å¼ å›¾å·§å¦™åœ°è¯´æ˜äº†ä¸ºä»€ä¹ˆä¸€ä¸ªå˜é‡åœ¨å•ç‹¬ä½¿ç”¨æ—¶ä¼¼ä¹ä¼šäº§ç”Ÿä¸€ç§å½±å“ï¼Œä½†åœ¨æ§åˆ¶æ··æ‚å› ç´ åå´ä¼šäº§ç”Ÿç›¸åçš„å½±å“ã€‚</p>
<h2 id="an-important-edge-case-perfect-separation">An Important Edge
Case: Perfect Separation âš ï¸</h2>
<p>What happens if the data can be perfectly separated by a straight
line? å¦‚æœæ•°æ®å¯ä»¥ç”¨ä¸€æ¡ç›´çº¿å®Œç¾åˆ†ç¦»ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ</p>
<p>One might think this is the ideal scenario, but it causes a problem
for the logistic regression algorithm. The model will try to find
coefficients that make the probabilities for each class as close to 1
and 0 as possible. To do this, the magnitude of the coefficients (<span
class="math inline">\(\hat{\beta}\)</span>) must grow infinitely large.
äººä»¬å¯èƒ½è®¤ä¸ºè¿™æ˜¯ç†æƒ³æƒ…å†µï¼Œä½†å®ƒä¼šç»™é€»è¾‘å›å½’ç®—æ³•å¸¦æ¥é—®é¢˜ã€‚æ¨¡å‹ä¼šå°è¯•æ‰¾åˆ°ä½¿æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡å°½å¯èƒ½æ¥è¿‘
1 å’Œ 0 çš„ç³»æ•°ã€‚ä¸ºæ­¤ï¼Œç³»æ•° (<span
class="math inline">\(\hat{\beta}\)</span>) çš„å¤§å°å¿…é¡»æ— é™å¤§ã€‚</p>
<p>The slide â€œNon-convergence for perfectly separated caseâ€ demonstrates
this:</p>
<ul>
<li><p><strong>The Code</strong>: It generates two distinct,
non-overlapping clusters of data points using Pythonâ€™s
<code>scikit-learn</code>.</p></li>
<li><p><strong>Parameter Estimates Graph</strong>: It shows the
<code>Intercept</code>, <code>Coefficient 1</code>, and
<code>Coefficient 2</code> values increasing or decreasing without limit
as the algorithm runs through more iterations. They never converge to a
stable value.</p></li>
<li><p><strong>Decision Boundary Graph</strong>: The decision boundary
itself might look reasonable, but the underlying coefficients are
unstable.</p></li>
<li><p><strong>ä»£ç </strong>ï¼šå®ƒä½¿ç”¨ Python çš„ <code>scikit-learn</code>
ç”Ÿæˆä¸¤ä¸ªä¸åŒçš„ã€ä¸é‡å çš„æ•°æ®ç‚¹èšç±»ã€‚</p></li>
<li><p><strong>å‚æ•°ä¼°è®¡å›¾</strong>ï¼šå®ƒæ˜¾ç¤ºâ€œæˆªè·â€ã€â€œç³»æ•° 1â€å’Œâ€œç³»æ•°
2â€çš„å€¼éšç€ç®—æ³•è¿­ä»£æ¬¡æ•°çš„å¢åŠ æˆ–å‡å°‘è€Œæ— é™å¢å¤§æˆ–å‡å°ã€‚å®ƒä»¬æ°¸è¿œä¸ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªç¨³å®šçš„å€¼ã€‚</p></li>
<li><p><strong>å†³ç­–è¾¹ç•Œå›¾</strong>ï¼šå†³ç­–è¾¹ç•Œæœ¬èº«å¯èƒ½çœ‹èµ·æ¥åˆç†ï¼Œä½†åº•å±‚ç³»æ•°æ˜¯ä¸ç¨³å®šçš„ã€‚</p></li>
</ul>
<p><strong>Key Takeaway</strong>: If your logistic regression model
fails to converge, the first thing you should check for is perfect
separation in your training data.
<strong>å…³é”®è¦ç‚¹</strong>ï¼šå¦‚æœæ‚¨çš„é€»è¾‘å›å½’æ¨¡å‹æœªèƒ½æ”¶æ•›ï¼Œæ‚¨åº”è¯¥æ£€æŸ¥çš„ç¬¬ä¸€ä»¶äº‹å°±æ˜¯è®­ç»ƒæ•°æ®æ˜¯å¦å®Œç¾åˆ†ç¦»ã€‚</p>
<h2 id="code-understanding">Code Understanding</h2>
<p>The slides provide useful code snippets in both R and Python.</p>
<h2 id="r-code-plotting-predictions">R Code (Plotting Predictions)</h2>
<p>This code generates the plot with the two S-shaped curves (one for
students, one for non-students) showing the probability of default as
balance increases.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Create a data frame for prediction with a range of balances</span></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># One version for students, one for non-students</span></span><br><span class="line">Default.st <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>balance<span class="operator">=</span>seq<span class="punctuation">(</span><span class="number">500</span><span class="punctuation">,</span> <span class="number">2500</span><span class="punctuation">,</span> by<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> student<span class="operator">=</span><span class="string">&quot;Yes&quot;</span><span class="punctuation">)</span></span><br><span class="line">Default.nonst <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>balance<span class="operator">=</span>seq<span class="punctuation">(</span><span class="number">500</span><span class="punctuation">,</span> <span class="number">2500</span><span class="punctuation">,</span> by<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> student<span class="operator">=</span><span class="string">&quot;No&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Use the trained multiple regression model (glmod3) to predict probabilities</span></span><br><span class="line">pred.st <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>glmod3<span class="punctuation">,</span> Default.st<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line">pred.nonst <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>glmod3<span class="punctuation">,</span> Default.nonst<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Plot the results</span></span><br><span class="line">plot<span class="punctuation">(</span>Default.st<span class="operator">$</span>balance<span class="punctuation">,</span> pred.st<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;l&quot;</span><span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;red&quot;</span><span class="punctuation">,</span> ...<span class="punctuation">)</span> <span class="operator">/</span><span class="operator">/</span> Students</span><br><span class="line">lines<span class="punctuation">(</span>Default.nonst<span class="operator">$</span>balance<span class="punctuation">,</span> pred.nonst<span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;blue&quot;</span><span class="punctuation">,</span> ...<span class="punctuation">)</span> <span class="operator">/</span><span class="operator">/</span> Non<span class="operator">-</span>students</span><br></pre></td></tr></table></figure>
<h4 id="python-code-visualizing-the-decision-boundary">Python Code
(Visualizing the Decision Boundary)</h4>
<p>This Python code uses <code>scikit-learn</code> and
<code>matplotlib</code> to create the plot showing the linear decision
boundary.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Generate synthetic data with two classes</span></span><br><span class="line">X, y = make_classification(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Initialize and fit the logistic regression model</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create a mesh grid of points to make predictions over the entire plot area</span></span><br><span class="line">xx, yy = np.meshgrid(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Predict the probability for each point on the grid</span></span><br><span class="line">probs = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Plot the decision boundary where the probability is 0.5</span></span><br><span class="line">plt.contour(xx, yy, probs.reshape(xx.shape), levels=[<span class="number">0.5</span>], ...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. Scatter plot the actual data points</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, ...)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="other-important-remarks">Other Important Remarks</h3>
<p>The â€œRemarksâ€ slide briefly mentions some key extensions:</p>
<ul>
<li><p><strong>Probit Model</strong>: An alternative to logistic
regression that uses the cumulative distribution function (CDF) of the
standard normal distribution instead of the sigmoid function. The
results are often very similar.</p></li>
<li><p><strong>Softmax Regression</strong>: An extension of logistic
regression used for multi-class classification (when there are more than
two possible outcomes).</p></li>
<li><p><strong>Probit
æ¨¡å‹</strong>ï¼šé€»è¾‘å›å½’çš„æ›¿ä»£æ–¹æ³•ï¼Œå®ƒä½¿ç”¨æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
(CDF) ä»£æ›¿ S å‹å‡½æ•°ã€‚ç»“æœé€šå¸¸éå¸¸ç›¸ä¼¼ã€‚</p></li>
<li><p><strong>Softmax
å›å½’</strong>ï¼šé€»è¾‘å›å½’çš„æ‰©å±•ï¼Œç”¨äºå¤šç±»åˆ†ç±»ï¼ˆå½“å­˜åœ¨ä¸¤ä¸ªä»¥ä¸Šå¯èƒ½ç»“æœæ—¶ï¼‰ã€‚</p></li>
</ul>
<h1
id="here-is-a-summary-of-the-slides-on-linear-discriminant-analysis-lda-including-the-key-mathematical-formulas-visual-explanations-and-how-to-implement-it-in-python.">5.
Here is a summary of the slides on Linear Discriminant Analysis (LDA),
including the key mathematical formulas, visual explanations, and how to
implement it in Python.</h1>
<h2
id="the-main-idea-classification-using-probabilities-ä½¿ç”¨æ¦‚ç‡è¿›è¡Œåˆ†ç±»">The
Main Idea: Classification Using Probabilities ä½¿ç”¨æ¦‚ç‡è¿›è¡Œåˆ†ç±»</h2>
<p>Linear Discriminant Analysis (LDA) is a classification method. For a
given input <strong>x</strong>, it calculates the probability that
<strong>x</strong> belongs to each class and then assigns
<strong>x</strong> to the class with the <strong>highest
probability</strong>.</p>
<p>It does this using <strong>Bayesâ€™ Theorem</strong>, which provides a
formula for the posterior probability <span class="math inline">\(P(Y=k
| X=x)\)</span>, or the probability that the class is <span
class="math inline">\(k\)</span> given the input <span
class="math inline">\(x\)</span>. çº¿æ€§åˆ¤åˆ«åˆ†æ (LDA)
æ˜¯ä¸€ç§åˆ†ç±»æ–¹æ³•ã€‚å¯¹äºç»™å®šçš„è¾“å…¥ <strong>x</strong>ï¼Œå®ƒè®¡ç®—
<strong>x</strong> å±äºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œç„¶åå°† <strong>x</strong>
åˆ†é…ç»™<strong>æ¦‚ç‡æœ€é«˜</strong>çš„ç±»åˆ«ã€‚</p>
<p>å®ƒä½¿ç”¨<strong>è´å¶æ–¯å®šç†</strong>æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¯¥å®šç†æä¾›äº†åéªŒæ¦‚ç‡
<span class="math inline">\(P(Y=k | X=x)\)</span> çš„å…¬å¼ï¼Œå³ç»™å®šè¾“å…¥
<span class="math inline">\(x\)</span>ï¼Œè¯¥ç±»åˆ«å±äº <span
class="math inline">\(k\)</span> çš„æ¦‚ç‡ã€‚ <span class="math display">\[
p_k(x) = P(Y=k|X=x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)}
\]</span></p>
<ul>
<li><span class="math inline">\(p_k(x)\)</span> is the <strong>posterior
probability</strong> we want to maximize.</li>
<li><span class="math inline">\(\pi_k = P(Y=k)\)</span> is the
<strong>prior probability</strong> of class <span
class="math inline">\(k\)</span> (how common the class is overall).</li>
<li><span class="math inline">\(f_k(x) = f(x|Y=k)\)</span> is the
<strong>class-conditional probability density function</strong> of
observing input <span class="math inline">\(x\)</span> if it belongs to
class <span class="math inline">\(k\)</span>.</li>
</ul>
<p>To classify a new observation <span class="math inline">\(x\)</span>,
we simply find the class <span class="math inline">\(k\)</span> that
makes <span class="math inline">\(p_k(x)\)</span> the largest.
ä¸ºäº†å¯¹æ–°çš„è§‚å¯Ÿå€¼ <span class="math inline">\(x\)</span>
è¿›è¡Œåˆ†ç±»ï¼Œæˆ‘ä»¬åªéœ€æ‰¾åˆ°ä½¿ <span class="math inline">\(p_k(x)\)</span>
æœ€å¤§çš„ç±»åˆ« <span class="math inline">\(k\)</span> å³å¯ã€‚</p>
<hr />
<h2 id="key-assumptions-of-lda">Key Assumptions of LDA</h2>
<p>LDAâ€™s power comes from a specific, simplifying assumption about the
dataâ€™s distribution. LDA
çš„å¼ºå¤§ä¹‹å¤„åœ¨äºå®ƒå¯¹æ•°æ®åˆ†å¸ƒè¿›è¡Œäº†ç‰¹å®šçš„ç®€åŒ–å‡è®¾ã€‚</p>
<ol type="1">
<li><p><strong>Gaussian Distribution:</strong> LDA assumes that the data
within each class <span class="math inline">\(k\)</span> follows a
p-dimensional multivariate normal (or Gaussian) distribution, denoted as
<span class="math inline">\(X|Y=k \sim \mathcal{N}(\mu_k,
\Sigma)\)</span>.</p></li>
<li><p><strong>Common Covariance:</strong> A crucial assumption is that
all classes share the <strong>same covariance matrix</strong> <span
class="math inline">\(\Sigma\)</span>. This means that while the classes
may have different centers (means, <span
class="math inline">\(\mu_k\)</span>), their shape and orientation
(covariance, <span class="math inline">\(\Sigma\)</span>) are
identical.</p></li>
<li><p><strong>é«˜æ–¯åˆ†å¸ƒ</strong>ï¼šLDA å‡è®¾æ¯ä¸ªç±» <span
class="math inline">\(k\)</span> ä¸­çš„æ•°æ®æœä» p
ç»´å¤šå…ƒæ­£æ€ï¼ˆæˆ–é«˜æ–¯ï¼‰åˆ†å¸ƒï¼Œè¡¨ç¤ºä¸º <span class="math inline">\(X|Y=k \sim
\mathcal{N}(\mu_k, \Sigma)\)</span>ã€‚</p></li>
<li><p><strong>å…±åŒåæ–¹å·®</strong>ï¼šä¸€ä¸ªå…³é”®å‡è®¾æ˜¯æ‰€æœ‰ç±»å…±äº«<strong>ç›¸åŒçš„åæ–¹å·®çŸ©é˜µ</strong>
<span
class="math inline">\(\Sigma\)</span>ã€‚è¿™æ„å‘³ç€è™½ç„¶ç±»å¯èƒ½å…·æœ‰ä¸åŒçš„ä¸­å¿ƒï¼ˆå‡å€¼ï¼Œ<span
class="math inline">\(\mu_k\)</span>ï¼‰ï¼Œä½†å®ƒä»¬çš„å½¢çŠ¶å’Œæ–¹å‘ï¼ˆåæ–¹å·®ï¼Œ<span
class="math inline">\(\Sigma\)</span>ï¼‰æ˜¯ç›¸åŒçš„ã€‚</p></li>
</ol>
<p>The probability density function for a class <span
class="math inline">\(k\)</span> is: <span class="math display">\[
f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp \left( -\frac{1}{2}(x
- \mu_k)^T \Sigma^{-1} (x - \mu_k) \right)
\]</span></p>
<p>The image above (from your slide â€œKnowing normal distributionâ€)
illustrates this. The two â€œbellsâ€ have different centers (different
<span class="math inline">\(\mu_k\)</span>) but similar shapes. The one
on the right is â€œtilted,â€ indicating correlation between variables,
which is captured in the shared covariance matrix <span
class="math inline">\(\Sigma\)</span>.
ä¸Šå›¾ï¼ˆæ‘˜è‡ªå¹»ç¯ç‰‡â€œäº†è§£æ­£æ€åˆ†å¸ƒâ€ï¼‰è¯´æ˜äº†è¿™ä¸€ç‚¹ã€‚ä¸¤ä¸ªâ€œé’Ÿâ€å½¢çš„ä¸­å¿ƒä¸åŒï¼ˆ<span
class="math inline">\(\mu_k\)</span>
ä¸åŒï¼‰ï¼Œä½†å½¢çŠ¶ç›¸ä¼¼ã€‚å³è¾¹çš„é’Ÿå½¢â€œå€¾æ–œâ€ï¼Œè¡¨ç¤ºå˜é‡ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œè¿™ä½“ç°åœ¨å…±äº«åæ–¹å·®çŸ©é˜µ
<span class="math inline">\(\Sigma\)</span> ä¸­ã€‚</p>
<hr />
<h2 id="the-math-behind-lda-the-discriminant-function-åˆ¤åˆ«å‡½æ•°">The Math
Behind LDA: The Discriminant Function åˆ¤åˆ«å‡½æ•°</h2>
<p>Since we only need to find the class <span
class="math inline">\(k\)</span> that maximizes the posterior
probability <span class="math inline">\(p_k(x)\)</span>, we can simplify
the math. The denominator in Bayesâ€™ theorem is the same for all classes,
so we only need to maximize the numerator: <span
class="math inline">\(\pi_k f_k(x)\)</span>.
ç”±äºæˆ‘ä»¬åªéœ€è¦æ‰¾åˆ°ä½¿åéªŒæ¦‚ç‡ <span class="math inline">\(p_k(x)\)</span>
æœ€å¤§åŒ–çš„ç±»åˆ« <span
class="math inline">\(k\)</span>ï¼Œå› æ­¤å¯ä»¥ç®€åŒ–æ•°å­¦è®¡ç®—ã€‚è´å¶æ–¯å®šç†ä¸­çš„åˆ†æ¯å¯¹äºæ‰€æœ‰ç±»åˆ«éƒ½æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦æœ€å¤§åŒ–åˆ†å­ï¼š<span
class="math inline">\(\pi_k f_k(x)\)</span>ã€‚ Taking the logarithm
(which doesnâ€™t change which class is maximal) and removing constant
terms gives us the <strong>linear discriminant function</strong>, <span
class="math inline">\(\delta_k(x)\)</span>:
å–å¯¹æ•°ï¼ˆè¿™ä¸ä¼šæ”¹å˜å“ªä¸ªç±»åˆ«æ˜¯æœ€å¤§å€¼ï¼‰å¹¶ç§»é™¤å¸¸æ•°é¡¹ï¼Œå¾—åˆ°<strong>çº¿æ€§åˆ¤åˆ«å‡½æ•°</strong>ï¼Œ<span
class="math inline">\(\delta_k(x)\)</span>ï¼š</p>
<p><span class="math display">\[
\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1}
\mu_k + \log(\pi_k)
\]</span></p>
<p>This function is <strong>linear</strong> in <span
class="math inline">\(x\)</span>, which is why the method is called
<em>Linear</em> Discriminant Analysis. The decision boundary between any
two classes, say class <span class="math inline">\(k\)</span> and class
<span class="math inline">\(l\)</span>, is the set of points where <span
class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>, which defines
a linear hyperplane. è¯¥å‡½æ•°å…³äº <span class="math inline">\(x\)</span>
æ˜¯<strong>çº¿æ€§</strong>çš„ï¼Œå› æ­¤è¯¥æ–¹æ³•è¢«ç§°ä¸º<em>çº¿æ€§</em>åˆ¤åˆ«åˆ†æã€‚ä»»æ„ä¸¤ä¸ªç±»åˆ«ï¼ˆä¾‹å¦‚ç±»åˆ«
<span class="math inline">\(k\)</span> å’Œç±»åˆ« <span
class="math inline">\(l\)</span>ï¼‰ä¹‹é—´çš„å†³ç­–è¾¹ç•Œæ˜¯æ»¡è¶³ <span
class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>
çš„ç‚¹çš„é›†åˆï¼Œè¿™å®šä¹‰äº†ä¸€ä¸ªçº¿æ€§è¶…å¹³é¢ã€‚</p>
<p>The image above (from your â€œGraph of LDAâ€ slide) is very important. *
<strong>Left:</strong> The ellipses show the true 95% probability
contours for three Gaussian classes. The dashed lines are the ideal
Bayes decision boundaries, which are perfectly linear because the
assumption of common covariance holds. * <strong>Right:</strong> This
shows a sample of data points drawn from those distributions. The solid
lines are the LDA decision boundaries calculated from the sample. They
are a very good estimate of the ideal boundaries. ä¸Šå›¾ï¼ˆæ¥è‡ªæ‚¨çš„â€œLDA
å›¾â€å¹»ç¯ç‰‡ï¼‰éå¸¸é‡è¦ã€‚ *
<strong>å·¦å›¾ï¼š</strong>æ¤­åœ†æ˜¾ç¤ºäº†ä¸‰ä¸ªé«˜æ–¯ç±»åˆ«çš„çœŸå® 95%
æ¦‚ç‡è½®å»“ã€‚è™šçº¿æ˜¯ç†æƒ³çš„è´å¶æ–¯å†³ç­–è¾¹ç•Œï¼Œç”±äºå…±åŒåæ–¹å·®å‡è®¾æˆç«‹ï¼Œå› æ­¤å®ƒä»¬æ˜¯å®Œç¾çš„çº¿æ€§ã€‚
*
<strong>å³å›¾ï¼š</strong>è¿™æ˜¾ç¤ºäº†ä»è¿™äº›åˆ†å¸ƒä¸­æŠ½å–çš„æ•°æ®ç‚¹æ ·æœ¬ã€‚å®çº¿æ˜¯æ ¹æ®æ ·æœ¬è®¡ç®—å‡ºçš„
LDA å†³ç­–è¾¹ç•Œã€‚å®ƒä»¬æ˜¯å¯¹ç†æƒ³è¾¹ç•Œçš„éå¸¸å¥½çš„ä¼°è®¡ã€‚ ***</p>
<h2
id="practical-implementation-estimating-the-parameters-å®é™…åº”ç”¨ä¼°è®¡å‚æ•°">Practical
Implementation: Estimating the Parameters å®é™…åº”ç”¨ï¼šä¼°è®¡å‚æ•°</h2>
<p>In a real-world scenario, we donâ€™t know the true parameters (<span
class="math inline">\(\mu_k\)</span>, <span
class="math inline">\(\Sigma\)</span>, <span
class="math inline">\(\pi_k\)</span>). Instead, we
<strong>estimate</strong> them from our training data (<span
class="math inline">\(n\)</span> total samples, with <span
class="math inline">\(n_k\)</span> samples in class <span
class="math inline">\(k\)</span>).
åœ¨å®é™…åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬ä¸çŸ¥é“çœŸæ­£çš„å‚æ•°ï¼ˆ<span
class="math inline">\(\mu_k\)</span>ã€<span
class="math inline">\(\Sigma\)</span>ã€<span
class="math inline">\(\pi_k\)</span>ï¼‰ã€‚ç›¸åï¼Œæˆ‘ä»¬æ ¹æ®è®­ç»ƒæ•°æ®ï¼ˆ<span
class="math inline">\(n\)</span> ä¸ªæ ·æœ¬ï¼Œ<span
class="math inline">\(n_k\)</span> ä¸ªæ ·æœ¬å±äº <span
class="math inline">\(k\)</span> ç±»ï¼‰æ¥<strong>ä¼°è®¡</strong>å®ƒä»¬ã€‚</p>
<ul>
<li><strong>Prior Probability (<span
class="math inline">\(\hat{\pi}_k\)</span>):</strong> The proportion of
training samples in class <span class="math inline">\(k\)</span>. <span
class="math display">\[\hat{\pi}_k = \frac{n_k}{n}\]</span></li>
<li><strong>Class Mean (<span
class="math inline">\(\hat{\mu}_k\)</span>):</strong> The average of the
training samples in class <span class="math inline">\(k\)</span>. <span
class="math display">\[\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i=k}
x_i\]</span></li>
<li><strong>Common Covariance (<span
class="math inline">\(\hat{\Sigma}\)</span>):</strong> A weighted
average of the sample covariance matrices for each class. This is often
called the â€œpooledâ€ covariance. <span
class="math display">\[\hat{\Sigma} = \frac{1}{n-K} \sum_{k=1}^{K}
\sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T\]</span></li>
<li><strong>å…ˆéªŒæ¦‚ç‡ (<span
class="math inline">\(\hat{\pi}_k\)</span>)ï¼š</strong>è®­ç»ƒæ ·æœ¬åœ¨ <span
class="math inline">\(k\)</span> ç±»ä¸­çš„æ¯”ä¾‹ã€‚ <span
class="math display">\[\hat{\pi}_k = \frac{n_k}{n}\]</span></li>
<li><strong>ç±»åˆ«å‡å€¼ (<span
class="math inline">\(\hat{\mu}_k\)</span>)ï¼š</strong>è®­ç»ƒæ ·æœ¬åœ¨ <span
class="math inline">\(k\)</span> ç±»ä¸­çš„å¹³å‡å€¼ã€‚ <span
class="math display">\[\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i=k}
x_i\]</span></li>
<li><strong>å…¬å…±åæ–¹å·® (<span
class="math inline">\(\hat{\Sigma}\)</span>)ï¼š</strong>æ¯ä¸ªç±»çš„æ ·æœ¬åæ–¹å·®çŸ©é˜µçš„åŠ æƒå¹³å‡å€¼ã€‚è¿™é€šå¸¸è¢«ç§°ä¸ºâ€œåˆå¹¶â€åæ–¹å·®ã€‚
<span class="math display">\[\hat{\Sigma} = \frac{1}{n-K} \sum_{k=1}^{K}
\sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T\]</span></li>
</ul>
<p>We then plug these estimates into the discriminant function to get
<span class="math inline">\(\hat{\delta}_k(x)\)</span> and classify a
new observation <span class="math inline">\(x\)</span> to the class with
the largest score. ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›ä¼°è®¡å€¼ä»£å…¥åˆ¤åˆ«å‡½æ•°ï¼Œå¾—åˆ° <span
class="math inline">\(\hat{\delta}_k(x)\)</span>ï¼Œå¹¶å°†æ–°çš„è§‚æµ‹å€¼ <span
class="math inline">\(x\)</span> å½’ç±»åˆ°å¾—åˆ†æœ€é«˜çš„ç±»åˆ«ã€‚ ***</p>
<h2 id="evaluating-performance">Evaluating Performance</h2>
<p>After training the model, we evaluate its performance using a
<strong>confusion matrix</strong>.
è®­ç»ƒæ¨¡å‹åï¼Œæˆ‘ä»¬ä½¿ç”¨<strong>æ··æ·†çŸ©é˜µ</strong>æ¥è¯„ä¼°å…¶æ€§èƒ½ã€‚</p>
<p>This matrix shows the true classes versus the predicted classes. *
<strong>Diagonal elements</strong> (9644, 81) are correct predictions. *
<strong>Off-diagonal elements</strong> (23, 252) are errors.
è¯¥çŸ©é˜µæ˜¾ç¤ºäº†çœŸå®ç±»åˆ«ä¸é¢„æµ‹ç±»åˆ«çš„å¯¹æ¯”ã€‚ * <strong>å¯¹è§’çº¿å…ƒç´ </strong>
(9644, 81) è¡¨ç¤ºæ­£ç¡®é¢„æµ‹ã€‚ * <strong>éå¯¹è§’çº¿å…ƒç´ </strong> (23, 252)
è¡¨ç¤ºé”™è¯¯é¢„æµ‹ã€‚</p>
<p>From this matrix, we can calculate key metrics: * <strong>Overall
Error Rate:</strong> Total incorrect predictions / Total predictions. *
Example: <span class="math inline">\((252 + 23) / 10000 =
2.75\%\)</span> * <strong>Sensitivity (True Positive Rate):</strong>
Correctly predicted positives / Total actual positives. It answers: â€œOf
all the people who actually defaulted, what fraction did we catch?â€ *
Example: <span class="math inline">\(81 / 333 = 24.3\%\)</span>. The
sensitivity is <span class="math inline">\(1 - 75.7\% = 24.3\%\)</span>.
* <strong>Specificity (True Negative Rate):</strong> Correctly predicted
negatives / Total actual negatives. It answers: â€œOf all the people who
did not default, what fraction did we correctly identify?â€ * Example:
<span class="math inline">\(9644 / 9667 = 99.8\%\)</span>. The
specificity is <span class="math inline">\(1 - 0.24\% =
99.8\%\)</span>.</p>
<p>The example in your slides shows a high error rate for â€œdefaultâ€
people (75.7%) because the classes are <strong>unbalanced</strong>â€”there
are far fewer defaulters. This highlights the importance of looking at
class-specific metrics, not just the overall error rate.</p>
<hr />
<h2 id="python-code-understanding">Python Code Understanding</h2>
<p>In Python, you can easily implement LDA using the
<code>scikit-learn</code> library. The code conceptually mirrors the
steps we discussed.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume you have your data X (features) and y (labels)</span></span><br><span class="line"><span class="comment"># X = features (e.g., balance, income)</span></span><br><span class="line"><span class="comment"># y = labels (e.g., 0 for &#x27;no-default&#x27;, 1 for &#x27;default&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create an instance of the LDA model</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit the model to the training data</span></span><br><span class="line"><span class="comment"># This is where the model calculates the estimates:</span></span><br><span class="line"><span class="comment">#  - Prior probabilities (pi_k)</span></span><br><span class="line"><span class="comment">#  - Class means (mu_k)</span></span><br><span class="line"><span class="comment">#  - Pooled covariance matrix (Sigma)</span></span><br><span class="line">lda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Make predictions on new, unseen data</span></span><br><span class="line">predictions = lda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Evaluate the model&#x27;s performance</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, predictions))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, predictions))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>LinearDiscriminantAnalysis()</code> creates the classifier
object.</li>
<li><code>lda.fit(X_train, y_train)</code> is the core training step
where the model learns the <span
class="math inline">\(\hat{\pi}_k\)</span>, <span
class="math inline">\(\hat{\mu}_k\)</span>, and <span
class="math inline">\(\hat{\Sigma}\)</span> parameters from the
data.</li>
<li><code>lda.predict(X_test)</code> uses the learned discriminant
function <span class="math inline">\(\hat{\delta}_k(x)\)</span> to
classify each sample in the test set.</li>
<li><code>confusion_matrix</code> and <code>classification_report</code>
are tools to evaluate the results, just like in the slides.</li>
</ul>
<h1
id="here-is-a-summary-of-the-provided-slides-on-linear-discriminant-analysis-lda-focusing-on-mathematical-concepts-python-code-interpretation-and-key-visuals.">6.
Here is a summary of the provided slides on Linear Discriminant Analysis
(LDA), focusing on mathematical concepts, Python code interpretation,
and key visuals.</h1>
<h2 id="core-concept-lda-for-classification">Core Concept: LDA for
Classification</h2>
<p>Linear Discriminant Analysis (LDA) is a classification method that
models the probability that an observation belongs to a certain class.
It works by finding a linear combination of features that best separates
two or more classes.</p>
<p>The decision is based on <strong>Bayesâ€™ theorem</strong>. For a given
observation with features <span class="math inline">\(X=x\)</span>, LDA
calculates the <strong>posterior probability</strong>, <span
class="math inline">\(p_k(x) = Pr(Y=k|X=x)\)</span>, for each class
<span class="math inline">\(k\)</span>. This is the probability that the
observation belongs to class <span class="math inline">\(k\)</span>
given its features. çº¿æ€§åˆ¤åˆ«åˆ†æ (LDA)
æ˜¯ä¸€ç§åˆ†ç±»æ–¹æ³•ï¼Œå®ƒå¯¹è§‚æµ‹å€¼å±äºæŸä¸ªç±»åˆ«çš„æ¦‚ç‡è¿›è¡Œå»ºæ¨¡ã€‚å®ƒçš„å·¥ä½œåŸç†æ˜¯æ‰¾åˆ°èƒ½å¤Ÿæœ€å¥½åœ°åŒºåˆ†ä¸¤ä¸ªæˆ–å¤šä¸ªç±»åˆ«çš„ç‰¹å¾çš„çº¿æ€§ç»„åˆã€‚</p>
<p>è¯¥å†³ç­–åŸºäº<strong>è´å¶æ–¯å®šç†</strong>ã€‚å¯¹äºç‰¹å¾ä¸º <span
class="math inline">\(X=x\)</span> çš„ç»™å®šè§‚æµ‹å€¼ï¼ŒLDA ä¼šè®¡ç®—æ¯ä¸ªç±»åˆ«
<span class="math inline">\(k\)</span>
çš„<strong>åéªŒæ¦‚ç‡</strong>ï¼Œ<span class="math inline">\(p_k(x) =
Pr(Y=k|X=x)\)</span>ã€‚è¿™æ˜¯ç»™å®šè§‚æµ‹å€¼çš„ç‰¹å¾åï¼Œè¯¥è§‚æµ‹å€¼å±äºç±»åˆ« <span
class="math inline">\(k\)</span> çš„æ¦‚ç‡ã€‚</p>
<p>By default, the Bayes classifier assigns an observation to the class
with the highest posterior probability. For a binary (two-class) problem
like â€˜Yesâ€™ vs.Â â€˜Noâ€™, this means:
é»˜è®¤æƒ…å†µä¸‹ï¼Œè´å¶æ–¯åˆ†ç±»å™¨å°†è§‚æµ‹å€¼åˆ†é…ç»™åéªŒæ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ã€‚å¯¹äºåƒâ€œæ˜¯â€ä¸â€œå¦â€è¿™æ ·çš„äºŒåˆ†ç±»é—®é¢˜ï¼Œè¿™æ„å‘³ç€ï¼š</p>
<ul>
<li>Assign to â€˜Yesâ€™ if <span class="math inline">\(Pr(Y=\text{Yes}|X=x)
&gt; 0.5\)</span></li>
<li>Assign to â€˜Noâ€™ otherwise</li>
</ul>
<h2 id="modifying-the-decision-threshold">Modifying the Decision
Threshold</h2>
<p>The default 0.5 threshold isnâ€™t always optimal. In many real-world
scenarios, the cost of one type of error is much higher than another.
For example, in credit card default prediction: é»˜è®¤çš„ 0.5
é˜ˆå€¼å¹¶éæ€»æ˜¯æœ€ä¼˜çš„ã€‚åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­ï¼Œä¸€ç§é”™è¯¯çš„ä»£ä»·è¿œé«˜äºå¦ä¸€ç§ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¿¡ç”¨å¡è¿çº¦é¢„æµ‹ä¸­ï¼š</p>
<ul>
<li><strong>False Negative:</strong> Incorrectly classifying a person
who will default as someone who wonâ€™t. (The bank loses money).</li>
<li><strong>False Positive:</strong> Incorrectly classifying a person
who wonâ€™t default as someone who will. (The bank loses a potential
customer).</li>
</ul>
<p>A bank might decide that missing a defaulter is much worse than
denying a good customer. To catch more potential defaulters, they can
<strong>lower the probability threshold</strong>.
é“¶è¡Œå¯èƒ½ä¼šè®¤ä¸ºé”™è¿‡ä¸€ä¸ªè¿çº¦è€…æ¯”æ‹’ç»ä¸€ä¸ªä¼˜è´¨å®¢æˆ·æ›´ç³Ÿç³•ã€‚ä¸ºäº†æ•æ‰æ›´å¤šæ½œåœ¨çš„è¿çº¦è€…ï¼Œä»–ä»¬å¯ä»¥<strong>é™ä½æ¦‚ç‡é˜ˆå€¼</strong>ã€‚</p>
<p>A modified rule could be: <span class="math display">\[
Pr(\text{default}=\text{Yes}|X=x) &gt; 0.2
\]</span> This makes the model more â€œsensitiveâ€ to flagging potential
defaulters, even at the cost of misclassifying more non-defaulters.
é™ä½é˜ˆå€¼<strong>ä¼šæé«˜æ•æ„Ÿåº¦</strong>ï¼Œä½†<strong>ä¼šé™ä½ç‰¹å¼‚æ€§</strong>ã€‚</p>
<p>This decision leads to a <strong>trade-off</strong> between two key
performance metrics: * <strong>Sensitivity (True Positive
Rate):</strong> The ability to correctly identify positive cases. (e.g.,
<code>Correctly identified defaulters / Total actual defaulters</code>).
* <strong>Specificity (True Negative Rate):</strong> The ability to
correctly identify negative cases. (e.g.,
<code>Correctly identified non-defaulters / Total actual non-defaulters</code>).</p>
<p>è¿™ä¸€å†³ç­–ä¼šå¯¼è‡´ä¸¤ä¸ªå…³é”®ç»©æ•ˆæŒ‡æ ‡ä¹‹é—´çš„<strong>æƒè¡¡</strong>ï¼š *
<strong>æ•æ„Ÿåº¦ï¼ˆçœŸé˜³æ€§ç‡ï¼‰ï¼š</strong>æ­£ç¡®è¯†åˆ«é˜³æ€§æ¡ˆä¾‹çš„èƒ½åŠ›ã€‚ï¼ˆä¾‹å¦‚ï¼Œâ€œæ­£ç¡®è¯†åˆ«çš„è¿çº¦è€…/å®é™…è¿çº¦è€…æ€»æ•°â€ï¼‰ã€‚
*
<strong>ç‰¹å¼‚æ€§ï¼ˆçœŸé˜´æ€§ç‡ï¼‰ï¼š</strong>æ­£ç¡®è¯†åˆ«é˜´æ€§æ¡ˆä¾‹çš„èƒ½åŠ›ã€‚ï¼ˆä¾‹å¦‚ï¼Œâ€œæ­£ç¡®è¯†åˆ«çš„éè¿çº¦è€…/å®é™…éè¿çº¦è€…æ€»æ•°â€ï¼‰ã€‚</p>
<p>Lowering the threshold <strong>increases sensitivity</strong> but
<strong>decreases specificity</strong>. ## Python Code Explained</p>
<p>The slides show how to implement and adjust LDA using Pythonâ€™s
<code>scikit-learn</code> library.</p>
<h2 id="basic-lda-implementation">Basic LDA Implementation</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the necessary library</span></span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize and train the LDA model</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line">lda_train = lda.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions using the default 0.5 threshold</span></span><br><span class="line">y_pred = lda.predict(X)</span><br></pre></td></tr></table></figure>
<p>This code trains an LDA model and makes predictions using the
standard 50% probability boundary.</p>
<h2 id="adjusting-the-prediction-threshold">Adjusting the Prediction
Threshold</h2>
<p>To use a custom threshold (e.g., 0.2), you donâ€™t use the
<code>.predict()</code> method. Instead, you get the class probabilities
with <code>.predict_proba()</code> and apply the threshold manually.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Get the probabilities for each class</span></span><br><span class="line"><span class="comment"># lda.predict_proba(X) returns an array like [[P(No), P(Yes)], ...]</span></span><br><span class="line"><span class="comment"># We select the second column [:, 1] for the &#x27;Yes&#x27; class probability</span></span><br><span class="line">lda_probs = lda.predict_proba(X)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Define a custom threshold</span></span><br><span class="line">threshold = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Apply the threshold to get new predictions</span></span><br><span class="line"><span class="comment"># This creates a boolean array (True where prob &gt; 0.2, else False)</span></span><br><span class="line"><span class="comment"># We then convert True/False to &#x27;Yes&#x27;/&#x27;No&#x27; labels</span></span><br><span class="line">lda_pred1 = np.where(lda_probs &gt; threshold, <span class="string">&quot;Yes&quot;</span>, <span class="string">&quot;No&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>This is the core technique for tuning the classifierâ€™s behavior to
meet specific business needs, as demonstrated on slides 55 and 56 for
both LDA and Logistic Regression.</p>
<h2 id="important-images-to-understand">Important Images to
Understand</h2>
<ol type="1">
<li><strong>Confusion Matrix (Slide 49):</strong> This table is crucial.
It breaks down the modelâ€™s predictions into True Positives, True
Negatives, False Positives, and False Negatives. All key metrics like
error rate, sensitivity, and specificity are calculated from this
matrix. <strong>æ··æ·†çŸ©é˜µï¼ˆå¹»ç¯ç‰‡
49ï¼‰ï¼š</strong>è¿™å¼ è¡¨è‡³å…³é‡è¦ã€‚å®ƒå°†æ¨¡å‹çš„é¢„æµ‹åˆ†è§£ä¸ºçœŸé˜³æ€§ã€çœŸé˜´æ€§ã€å‡é˜³æ€§å’Œå‡é˜´æ€§ã€‚æ‰€æœ‰å…³é”®æŒ‡æ ‡ï¼Œä¾‹å¦‚é”™è¯¯ç‡ã€çµæ•åº¦å’Œç‰¹å¼‚æ€§ï¼Œéƒ½åŸºäºæ­¤çŸ©é˜µè®¡ç®—å¾—å‡ºã€‚</li>
<li><strong>LDA Decision Boundaries (Slide 51):</strong> This plot
provides a powerful visual intuition. It shows the data points for two
classes and the decision boundary line. The different parallel lines
show how changing the threshold from 0.5 to 0.1 or 0.9 shifts the
boundary, making the model classify more or fewer points into the
minority class. <strong>LDA å†³ç­–è¾¹ç•Œï¼ˆå¹»ç¯ç‰‡
51ï¼‰ï¼š</strong>è¿™å¼ å›¾æä¾›äº†å¼ºå¤§çš„è§†è§‰ç›´è§‚æ€§ã€‚å®ƒå±•ç¤ºäº†ä¸¤ä¸ªç±»åˆ«çš„æ•°æ®ç‚¹å’Œå†³ç­–è¾¹ç•Œçº¿ã€‚ä¸åŒçš„å¹³è¡Œçº¿æ˜¾ç¤ºäº†å°†é˜ˆå€¼ä»
0.5 æ›´æ”¹ä¸º 0.1 æˆ– 0.9
æ—¶è¾¹ç•Œå¦‚ä½•ç§»åŠ¨ï¼Œä»è€Œä½¿æ¨¡å‹å°†æ›´å¤šæˆ–æ›´å°‘çš„ç‚¹å½’å…¥å°‘æ•°ç±»</li>
<li><strong>Error Rate Tradeoff Curve (Slide 53):</strong> This graph is
the most important for understanding the business implication of
changing the threshold. It clearly shows that as the threshold changes,
the error rate for one class goes down while the error rate for the
other goes up. The overall error is minimized at a certain point, but
that may not be the optimal point from a business perspective.
<strong>é”™è¯¯ç‡æƒè¡¡æ›²çº¿ï¼ˆå¹»ç¯ç‰‡
53ï¼‰ï¼š</strong>è¿™å¼ å›¾å¯¹äºç†è§£æ›´æ”¹é˜ˆå€¼çš„ä¸šåŠ¡å«ä¹‰è‡³å…³é‡è¦ã€‚å®ƒæ¸…æ¥šåœ°è¡¨æ˜ï¼Œéšç€é˜ˆå€¼çš„å˜åŒ–ï¼Œä¸€ä¸ªç±»åˆ«çš„é”™è¯¯ç‡ä¸‹é™ï¼Œè€Œå¦ä¸€ä¸ªç±»åˆ«çš„é”™è¯¯ç‡ä¸Šå‡ã€‚æ€»ä½“è¯¯å·®åœ¨æŸä¸ªç‚¹è¾¾åˆ°æœ€å°ï¼Œä½†ä»ä¸šåŠ¡è§’åº¦æ¥çœ‹ï¼Œè¿™å¯èƒ½å¹¶éæœ€ä½³ç‚¹ã€‚</li>
<li><strong>ROC Curve (Slides 54 &amp; 55):</strong> The Receiver
Operating Characteristic (ROC) curve plots Sensitivity vs.Â (1 -
Specificity) for <em>all possible thresholds</em>. An ideal classifier
has a curve that â€œhugsâ€ the top-left corner, indicating high sensitivity
and high specificity. Itâ€™s a standard way to visualize and compare the
overall performance of different classifiers. <strong>ROC æ›²çº¿ï¼ˆå¹»ç¯ç‰‡
54 å’Œ 55ï¼‰ï¼š</strong> æ¥æ”¶è€…æ“ä½œç‰¹æ€§ (ROC)
æ›²çº¿ç»˜åˆ¶äº†<em>æ‰€æœ‰å¯èƒ½é˜ˆå€¼</em>çš„çµæ•åº¦ä¸ï¼ˆ1 -
ç‰¹å¼‚æ€§ï¼‰çš„å…³ç³»ã€‚ç†æƒ³çš„åˆ†ç±»å™¨æ›²çº¿â€œç´§è´´â€å·¦ä¸Šè§’ï¼Œè¡¨ç¤ºé«˜çµæ•åº¦å’Œé«˜ç‰¹å¼‚æ€§ã€‚è¿™æ˜¯å¯è§†åŒ–å’Œæ¯”è¾ƒä¸åŒåˆ†ç±»å™¨æ•´ä½“æ€§èƒ½çš„æ ‡å‡†æ–¹æ³•ã€‚</li>
</ol>
<h1
id="here-is-a-summary-of-the-provided-slides-on-linear-and-quadratic-discriminant-analysis-including-the-key-formulas-python-code-equivalents-and-explanations-of-the-important-concepts.">7.
Here is a summary of the provided slides on Linear and Quadratic
Discriminant Analysis, including the key formulas, Python code
equivalents, and explanations of the important concepts.</h1>
<h2 id="key-goal-classification"><strong>Key Goal:
Classification</strong></h2>
<p>Both <strong>Linear Discriminant Analysis (LDA)</strong> and
<strong>Quadratic Discriminant Analysis (QDA)</strong> are
classification algorithms. Their main goal is to find a decision
boundary to separate different classes (e.g., â€œdefaultâ€ vs.Â â€œnot
defaultâ€) in the data. <strong>çº¿æ€§åˆ¤åˆ«åˆ†æ (LDA)</strong> å’Œ
<strong>äºŒæ¬¡åˆ¤åˆ«åˆ†æ (QDA)</strong>
éƒ½æ˜¯åˆ†ç±»ç®—æ³•ã€‚å®ƒä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªå†³ç­–è¾¹ç•Œæ¥åŒºåˆ†æ•°æ®ä¸­çš„ä¸åŒç±»åˆ«ï¼ˆä¾‹å¦‚ï¼Œâ€œé»˜è®¤â€ä¸â€œéé»˜è®¤â€ï¼‰ã€‚</p>
<h3 id="linear-discriminant-analysis-lda">## Linear Discriminant
Analysis (LDA)</h3>
<p>LDA creates a <strong>linear</strong> decision boundary between
classes. LDA åœ¨ç±»åˆ«ä¹‹é—´åˆ›å»º<strong>çº¿æ€§</strong>å†³ç­–è¾¹ç•Œã€‚</p>
<h4 id="core-idea-fishers-interpretation"><strong>Core Idea (Fisherâ€™s
Interpretation)</strong></h4>
<p>Imagine you have data points for different classes in a 3D space.
Fisherâ€™s idea is to find the best angle to shine a â€œflashlightâ€ on the
data to project its shadow onto a 2D wall (or a 1D line). The â€œbestâ€
projection is the one where the shadows of the different classes are
<strong>as far apart from each other as possible</strong>, while the
shadows within each class are <strong>as tightly packed as
possible</strong>. æƒ³è±¡ä¸€ä¸‹ï¼Œä½ åœ¨ä¸‰ç»´ç©ºé—´ä¸­æ‹¥æœ‰ä¸åŒç±»åˆ«çš„æ•°æ®ç‚¹ã€‚Fisher
çš„æ€æƒ³æ˜¯æ‰¾åˆ°æœ€ä½³è§’åº¦ï¼Œç”¨â€œæ‰‹ç”µç­’â€ç…§å°„æ•°æ®ï¼Œå°†å…¶é˜´å½±æŠ•å°„åˆ°äºŒç»´å¢™å£ï¼ˆæˆ–ä¸€ç»´çº¿ä¸Šï¼‰ã€‚
â€œæœ€ä½³â€æŠ•å½±æ˜¯ä¸åŒç±»åˆ«çš„é˜´å½±<strong>å½¼æ­¤ä¹‹é—´å°½å¯èƒ½è¿œ</strong>ï¼Œè€Œæ¯ä¸ªç±»åˆ«å†…çš„é˜´å½±<strong>å°½å¯èƒ½ç´§å¯†</strong>çš„æŠ•å½±ã€‚</p>
<ul>
<li><strong>Maximize:</strong> The distance between the means of the
projected classes (Between-Class Variance).
æŠ•å½±ç±»åˆ«å‡å€¼ä¹‹é—´çš„è·ç¦»ï¼ˆç±»é—´æ–¹å·®ï¼‰ã€‚</li>
<li><strong>Minimize:</strong> The spread or variance within each
projected class (Within-Class Variance).
æ¯ä¸ªæŠ•å½±ç±»åˆ«å†…çš„æ‰©æ•£æˆ–æ–¹å·®ï¼ˆç±»å†…æ–¹å·®ï¼‰ã€‚ This is the most important
image for understanding the intuition behind LDA. It shows how
projecting the data onto a specific line (defined by vector
<code>w</code>) can make the two classes clearly separable.
è¿™æ˜¯ç†è§£LDAèƒŒåç›´è§‰çš„æœ€é‡è¦å›¾åƒã€‚å®ƒå±•ç¤ºäº†å¦‚ä½•å°†æ•°æ®æŠ•å½±åˆ°ç‰¹å®šç›´çº¿ï¼ˆç”±å‘é‡â€œwâ€å®šä¹‰ï¼‰ä¸Šï¼Œä»è€Œä½¿ä¸¤ä¸ªç±»åˆ«æ¸…æ™°å¯åˆ†ã€‚</li>
</ul>
<h4 id="key-mathematical-formulas"><strong>Key Mathematical
Formulas</strong></h4>
<p>To achieve this, LDA maximizes a ratio called the <strong>Rayleigh
quotient</strong>. LDAæœ€å¤§åŒ–ä¸€ä¸ªç§°ä¸º<strong>ç‘åˆ©å•†</strong>çš„æ¯”ç‡ã€‚</p>
<ol type="1">
<li><strong>Within-Class Covariance (<span
class="math inline">\(\hat{\Sigma}_W\)</span>)</strong>: Measures the
spread of data <em>inside</em> each class. <strong>ç±»å†…åæ–¹å·® (<span
class="math inline">\(\hat{\Sigma}_W\)</span>)</strong>ï¼šè¡¡é‡æ¯ä¸ªç±»åˆ«<em>å†…éƒ¨</em>æ•°æ®çš„æ‰©æ•£ç¨‹åº¦ã€‚
<span class="math display">\[\hat{\Sigma}_W = \frac{1}{n-K}
\sum_{k=1}^{K} \sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i -
\hat{\mu}_k)^\top\]</span></li>
<li><strong>Between-Class Covariance (<span
class="math inline">\(\hat{\Sigma}_B\)</span>)</strong>: Measures the
spread <em>between</em> the means of different classes.
<strong>ç±»é—´åæ–¹å·® (<span
class="math inline">\(\hat{\Sigma}_B\)</span>)</strong>ï¼šè¡¡é‡ä¸åŒç±»åˆ«å‡å€¼<em>ä¹‹é—´</em>çš„å·®å¼‚ã€‚
<span class="math display">\[\hat{\Sigma}_B = \sum_{k=1}^{K} n_k
(\hat{\mu}_k - \hat{\mu})(\hat{\mu}_k - \hat{\mu})^\top\]</span></li>
<li><strong>Objective Function</strong>: Find the projection vector
<span class="math inline">\(w\)</span> that maximizes the ratio of
between-class variance to within-class variance.
<strong>ç›®æ ‡å‡½æ•°</strong>ï¼šæ‰¾åˆ°æŠ•å½±å‘é‡ <span
class="math inline">\(w\)</span>ï¼Œä½¿ç±»é—´æ–¹å·®ä¸ç±»å†…æ–¹å·®ä¹‹æ¯”æœ€å¤§åŒ–ã€‚ <span
class="math display">\[\max_w \frac{w^\top \hat{\Sigma}_B w}{w^\top
\hat{\Sigma}_W w}\]</span></li>
</ol>
<h4 id="ldas-main-assumption"><strong>LDAâ€™s Main
Assumption</strong></h4>
<p>The key assumption of LDA is that all classes share the <strong>same
covariance matrix (<span
class="math inline">\(\Sigma\)</span>)</strong>. They can have different
means (<span class="math inline">\(\mu_k\)</span>), but their spread and
orientation must be identical. This assumption is what results in a
linear decision boundary. LDA
çš„å…³é”®å‡è®¾æ˜¯æ‰€æœ‰ç±»åˆ«å…±äº«<strong>ç›¸åŒçš„åæ–¹å·®çŸ©é˜µ (<span
class="math inline">\(\Sigma\)</span>)</strong>ã€‚å®ƒä»¬å¯ä»¥å…·æœ‰ä¸åŒçš„å‡å€¼
(<span
class="math inline">\(\mu_k\)</span>)ï¼Œä½†å®ƒä»¬çš„æ•£åº¦å’Œæ–¹å‘å¿…é¡»ç›¸åŒã€‚æ­£æ˜¯è¿™ä¸€å‡è®¾å¯¼è‡´äº†çº¿æ€§å†³ç­–è¾¹ç•Œã€‚</p>
<h3 id="quadratic-discriminant-analysis-qda">## Quadratic Discriminant
Analysis (QDA)</h3>
<p>QDA is a more flexible extension of LDA that creates a
<strong>quadratic</strong> (curved) decision boundary. QDA æ˜¯ LDA
çš„æ›´çµæ´»çš„æ‰©å±•ï¼Œå®ƒåˆ›å»ºäº†<strong>äºŒæ¬¡</strong>ï¼ˆæ›²çº¿ï¼‰å†³ç­–è¾¹ç•Œã€‚ ####
<strong>Core Idea &amp; Key Assumption</strong></p>
<p>QDA starts with the same principles as LDA but drops the key
assumption. QDA assumes that <strong>each class has its own unique
covariance matrix (<span
class="math inline">\(\Sigma_k\)</span>)</strong>. QDA çš„åŸç†ä¸ LDA
ç›¸åŒï¼Œä½†æ”¾å¼ƒäº†å…³é”®å‡è®¾ã€‚QDA å‡è®¾<strong>æ¯ä¸ªç±»åˆ«éƒ½æœ‰è‡ªå·±ç‹¬ç‰¹çš„åæ–¹å·®çŸ©é˜µ
(<span class="math inline">\(\Sigma_k\)</span>)</strong>ã€‚</p>
<p>This means each class can have its own spread, shape, and
orientation. This additional flexibility allows for a more complex,
curved decision boundary.
è¿™æ„å‘³ç€æ¯ä¸ªç±»åˆ«å¯ä»¥æ‹¥æœ‰è‡ªå·±çš„æ•£åº¦ã€å½¢çŠ¶å’Œæ–¹å‘ã€‚è¿™ç§é¢å¤–çš„çµæ´»æ€§ä½¿å¾—å†³ç­–è¾¹ç•Œæ›´åŠ å¤æ‚ã€æ›²çº¿åŒ–ã€‚</p>
<h4 id="key-mathematical-formula"><strong>Key Mathematical
Formula</strong></h4>
<p>The classification is made using a discrimination function, <span
class="math inline">\(\delta_k(x)\)</span>. We assign a data point <span
class="math inline">\(x\)</span> to the class <span
class="math inline">\(k\)</span> for which <span
class="math inline">\(\delta_k(x)\)</span> is largest. The function for
QDA is: <span class="math display">\[\delta_k(x) = -\frac{1}{2}(x -
\mu_k)^\top \Sigma_k^{-1}(x - \mu_k) - \frac{1}{2}\log(|\Sigma_k|) +
\log \pi_k\]</span> The term containing <span
class="math inline">\(x^\top \Sigma_k^{-1} x\)</span> makes this
function a <strong>quadratic</strong> function of <span
class="math inline">\(x\)</span>.</p>
<h3 id="lda-vs.-qda-the-trade-off">## LDA vs.Â QDA: The Trade-Off</h3>
<p>The choice between LDA and QDA is a classic <strong>bias-variance
trade-off</strong>. åœ¨ LDA å’Œ QDA
ä¹‹é—´è¿›è¡Œé€‰æ‹©æ˜¯å…¸å‹çš„<strong>åå·®-æ–¹å·®æƒè¡¡</strong>ã€‚</p>
<ul>
<li><p><strong>Use LDA when:</strong></p>
<ul>
<li>The assumption of a common covariance matrix is reasonable (the
classes have similar shapes).</li>
<li>You have a small amount of training data, as LDA is less prone to
overfitting.</li>
<li>Simplicity is preferred. LDA is less flexible (high bias) but has
lower variance.</li>
<li>å‡è®¾å…±åŒåæ–¹å·®çŸ©é˜µæ˜¯åˆç†çš„ï¼ˆç±»åˆ«å…·æœ‰ç›¸ä¼¼çš„å½¢çŠ¶ï¼‰ã€‚</li>
<li>è®­ç»ƒæ•°æ®é‡è¾ƒå°‘ï¼Œå› ä¸º LDA ä¸æ˜“è¿‡æ‹Ÿåˆã€‚</li>
<li>ç®€æ´æ˜¯é¦–é€‰ã€‚LDA çµæ´»æ€§è¾ƒå·®ï¼ˆåå·®è¾ƒå¤§ï¼‰ï¼Œä½†æ–¹å·®è¾ƒå°ã€‚</li>
</ul></li>
<li><p><strong>Use QDA when:</strong></p>
<ul>
<li>The classes have clearly different shapes and spreads (different
covariance matrices).</li>
<li>You have a large amount of training data to properly estimate the
separate covariance matrices for each class.</li>
<li>QDA is more flexible (low bias) but can have high variance, meaning
it might overfit on smaller datasets.</li>
<li>ç±»åˆ«å…·æœ‰æ˜æ˜¾ä¸åŒçš„å½¢çŠ¶å’Œåˆ†å¸ƒï¼ˆä¸åŒçš„åæ–¹å·®çŸ©é˜µï¼‰ã€‚</li>
<li>æ‹¥æœ‰å¤§é‡è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥æ­£ç¡®ä¼°è®¡æ¯ä¸ªç±»åˆ«çš„ç‹¬ç«‹åæ–¹å·®çŸ©é˜µã€‚</li>
<li>QDA
æ›´çµæ´»ï¼ˆåå·®è¾ƒå°ï¼‰ï¼Œä½†æ–¹å·®è¾ƒå¤§ï¼Œè¿™æ„å‘³ç€å®ƒå¯èƒ½åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆã€‚
<strong>Rule of Thumb:</strong> If the class variances are equal or
close, LDA is better. Otherwise, QDA is better.
<strong>ç»éªŒæ³•åˆ™ï¼š</strong> å¦‚æœç±»åˆ«æ–¹å·®ç›¸ç­‰æˆ–æ¥è¿‘ï¼Œåˆ™ LDA
æ›´ä½³ã€‚å¦åˆ™ï¼ŒQDA æ›´å¥½ã€‚</li>
</ul></li>
</ul>
<h3 id="code-understanding-python-equivalent">## Code Understanding
(Python Equivalent)</h3>
<p>The slides show code in R. Hereâ€™s how you would perform LDA and
evaluate it in Python using the popular <code>scikit-learn</code>
library.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, accuracy_score, roc_curve, auc</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;df&#x27; is your DataFrame with features and a &#x27;target&#x27; column</span></span><br><span class="line"><span class="comment"># X = df.drop(&#x27;target&#x27;, axis=1)</span></span><br><span class="line"><span class="comment"># y = df[&#x27;target&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit an LDA model (equivalent to lda() in R)</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line">lda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Make predictions (equivalent to predict() in R)</span></span><br><span class="line">y_pred_lda = lda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># To fit a QDA model, the process is identical:</span></span><br><span class="line"><span class="comment"># qda = QuadraticDiscriminantAnalysis()</span></span><br><span class="line"><span class="comment"># qda.fit(X_train, y_train)</span></span><br><span class="line"><span class="comment"># y_pred_qda = qda.predict(X_test)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Create a confusion matrix (equivalent to table())</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LDA Confusion Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, y_pred_lda))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Plot the ROC Curve (equivalent to the R code for ROC)</span></span><br><span class="line"><span class="comment"># Get prediction probabilities for the positive class</span></span><br><span class="line">y_pred_proba = lda.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate ROC curve points</span></span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate Area Under the Curve (AUC)</span></span><br><span class="line">roc_auc = auc(fpr, tpr)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(fpr, tpr, color=<span class="string">&#x27;blue&#x27;</span>, lw=<span class="number">2</span>, label=<span class="string">f&#x27;LDA ROC curve (area = <span class="subst">&#123;roc_auc:<span class="number">.2</span>f&#125;</span>)&#x27;</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">&#x27;gray&#x27;</span>, lw=<span class="number">2</span>, linestyle=<span class="string">&#x27;--&#x27;</span>) <span class="comment"># Random guess line</span></span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate (1 - Specificity)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate (Sensitivity)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Receiver Operating Characteristic (ROC) Curve&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="understanding-the-roc-curve"><strong>Understanding the ROC
Curve</strong></h4>
<p>The <strong>ROC Curve</strong> is another important image. It helps
you visualize a classifierâ€™s performance across all possible
classification thresholds. <strong>ROC æ›²çº¿</strong>
æ˜¯å¦ä¸€ä¸ªé‡è¦çš„å›¾åƒã€‚å®ƒå¯ä»¥å¸®åŠ©æ‚¨ç›´è§‚åœ°äº†è§£åˆ†ç±»å™¨åœ¨æ‰€æœ‰å¯èƒ½çš„åˆ†ç±»é˜ˆå€¼ä¸‹çš„æ€§èƒ½ã€‚</p>
<ul>
<li>The <strong>Y-axis</strong> is the <strong>True Positive
Rate</strong> (Sensitivity): â€œOf all the actual positives, how many did
we correctly identify?â€</li>
<li>The <strong>X-axis</strong> is the <strong>False Positive
Rate</strong>: â€œOf all the actual negatives, how many did we incorrectly
label as positive?â€</li>
<li>A perfect classifier would have a curve that goes straight up to the
top-left corner (100% TPR, 0% FPR). The diagonal line represents a
random guess. The <strong>Area Under the Curve (AUC)</strong> summarizes
the modelâ€™s performance; a value closer to 1.0 is better.</li>
<li><strong>Y è½´</strong>
è¡¨ç¤º<strong>çœŸé˜³æ€§ç‡</strong>ï¼ˆæ•æ„Ÿåº¦ï¼‰ï¼šâ€œåœ¨æ‰€æœ‰å®é™…çš„é˜³æ€§æ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬æ­£ç¡®è¯†åˆ«äº†å¤šå°‘ä¸ªï¼Ÿâ€</li>
<li><strong>X è½´</strong>
è¡¨ç¤º<strong>å‡é˜³æ€§ç‡</strong>ï¼šâ€œåœ¨æ‰€æœ‰å®é™…çš„é˜´æ€§æ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬é”™è¯¯åœ°å°†å¤šå°‘ä¸ªæ ‡è®°ä¸ºé˜³æ€§ï¼Ÿâ€</li>
<li>ä¸€ä¸ªå®Œç¾çš„åˆ†ç±»å™¨åº”è¯¥æœ‰ä¸€æ¡ç›´çº¿ä¸Šå‡åˆ°å·¦ä¸Šè§’çš„æ›²çº¿ï¼ˆçœŸé˜³æ€§ç‡
100%ï¼Œå‡é˜³æ€§ç‡ 0%ï¼‰ã€‚å¯¹è§’çº¿è¡¨ç¤ºéšæœºçŒœæµ‹ã€‚<strong>æ›²çº¿ä¸‹é¢ç§¯
(AUC)</strong> æ¦‚æ‹¬äº†æ¨¡å‹çš„æ€§èƒ½ï¼›è¯¥å€¼è¶Šæ¥è¿‘ 1.0 è¶Šå¥½ã€‚</li>
</ul>
<h1
id="here-is-a-summary-of-the-provided-slides-on-quadratic-discriminant-analysis-qda-including-the-key-formulas-code-explanations-with-python-equivalents-and-a-guide-to-the-most-important-images.">8.
Here is a summary of the provided slides on Quadratic Discriminant
Analysis (QDA), including the key formulas, code explanations with
Python equivalents, and a guide to the most important images.</h1>
<h3 id="core-concept-qda-vs.-lda">## Core Concept: QDA vs.Â LDA</h3>
<p>The main difference between <strong>Linear Discriminant Analysis
(LDA)</strong> and <strong>Quadratic Discriminant Analysis
(QDA)</strong> lies in their assumptions about the data.
<strong>çº¿æ€§åˆ¤åˆ«åˆ†æ (LDA)</strong> å’Œ <strong>äºŒæ¬¡åˆ¤åˆ«åˆ†æ
(QDA)</strong> çš„ä¸»è¦åŒºåˆ«åœ¨äºå®ƒä»¬å¯¹æ•°æ®çš„å‡è®¾ã€‚ * <strong>LDA</strong>
assumes that all classes share the <strong>same covariance
matrix</strong> (<span class="math inline">\(\Sigma\)</span>). It models
each class as a normal distribution with a different mean (<span
class="math inline">\(\mu_k\)</span>) but the same shape and
orientation. This results in a <em>linear</em> decision boundary between
classes. å‡è®¾æ‰€æœ‰ç±»åˆ«å…±äº«<strong>ç›¸åŒçš„åæ–¹å·®çŸ©é˜µ</strong> (<span
class="math inline">\(\Sigma\)</span>)ã€‚å®ƒå°†æ¯ä¸ªç±»åˆ«å»ºæ¨¡ä¸ºå‡å€¼ä¸åŒ
(<span class="math inline">\(\mu_k\)</span>)
ä½†å½¢çŠ¶å’Œæ–¹å‘ç›¸åŒçš„æ­£æ€åˆ†å¸ƒã€‚è¿™ä¼šå¯¼è‡´ç±»åˆ«ä¹‹é—´å‡ºç° <em>çº¿æ€§</em>
å†³ç­–è¾¹ç•Œã€‚ * <strong>QDA</strong> is more flexible. It assumes that each
class <span class="math inline">\(k\)</span> has its <strong>own,
separate covariance matrix</strong> (<span
class="math inline">\(\Sigma_k\)</span>). This allows each classâ€™s
distribution to have a unique shape, size, and orientation. This
flexibility results in a <em>quadratic</em> decision boundary (like a
parabola, hyperbola, or ellipse). æ›´çµæ´»ã€‚å®ƒå‡è®¾æ¯ä¸ªç±»åˆ« <span
class="math inline">\(k\)</span> éƒ½æœ‰å…¶<strong>ç‹¬ç«‹çš„åæ–¹å·®çŸ©é˜µ</strong>
(<span
class="math inline">\(\Sigma_k\)</span>)ã€‚è¿™ä½¿å¾—æ¯ä¸ªç±»åˆ«çš„åˆ†å¸ƒå…·æœ‰ç‹¬ç‰¹çš„å½¢çŠ¶ã€å¤§å°å’Œæ–¹å‘ã€‚è¿™ç§çµæ´»æ€§å¯¼è‡´äº†<em>äºŒæ¬¡</em>å†³ç­–è¾¹ç•Œï¼ˆç±»ä¼¼äºæŠ›ç‰©çº¿ã€åŒæ›²çº¿æˆ–æ¤­åœ†ï¼‰ã€‚
<strong>Analogy</strong> ğŸ’¡: Imagine youâ€™re drawing boundaries around
different clusters of stars. LDA gives you only straight lines to
separate the clusters. QDA gives you curved lines (circles, ellipses),
which can create a much better fit if the clusters themselves are
elliptical and point in different directions.
æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£åœ¨å›´ç»•ä¸åŒçš„æ˜Ÿå›¢ç»˜åˆ¶è¾¹ç•Œã€‚LDA åªæä¾›ç›´çº¿æ¥åˆ†éš”æ˜Ÿå›¢ã€‚QDA
æä¾›æ›²çº¿ï¼ˆåœ†å½¢ã€æ¤­åœ†å½¢ï¼‰ï¼Œå¦‚æœæ˜Ÿå›¢æœ¬èº«æ˜¯æ¤­åœ†å½¢ä¸”æŒ‡å‘ä¸åŒçš„æ–¹å‘ï¼Œåˆ™å¯ä»¥äº§ç”Ÿæ›´å¥½çš„æ‹Ÿåˆæ•ˆæœã€‚</p>
<h3 id="the-math-behind-qda">## The Math Behind QDA</h3>
<p>QDA classifies a new observation <span
class="math inline">\(x\)</span> to the class <span
class="math inline">\(k\)</span> that has the highest discriminant
score, <span class="math inline">\(\delta_k(x)\)</span>. The formula for
this score is what makes the boundary quadratic. QDA å°†æ–°çš„è§‚æµ‹å€¼ <span
class="math inline">\(x\)</span> å½’ç±»åˆ°å…·æœ‰æœ€é«˜åˆ¤åˆ«åˆ†æ•° <span
class="math inline">\(\delta_k(x)\)</span> çš„ç±» <span
class="math inline">\(k\)</span>
ä¸­ã€‚è¯¥åˆ†æ•°çš„å…¬å¼ä½¿å¾—è¾¹ç•Œå…·æœ‰äºŒæ¬¡é¡¹ã€‚</p>
<p>The discriminant function for class <span
class="math inline">\(k\)</span> is: <span
class="math display">\[\delta_k(x) = -\frac{1}{2}(x - \mu_k)^T
\Sigma_k^{-1}(x - \mu_k) - \frac{1}{2}\log(|\Sigma_k|) +
\log(\pi_k)\]</span></p>
<p>Letâ€™s break it down:</p>
<ul>
<li><span class="math inline">\((x - \mu_k)^T \Sigma_k^{-1}(x -
\mu_k)\)</span>: This is a quadratic term (since it involves <span
class="math inline">\(x^T \Sigma_k^{-1} x\)</span>). It measures the
squared Mahalanobis distance from <span class="math inline">\(x\)</span>
to the class mean <span class="math inline">\(\mu_k\)</span>, scaled by
that classâ€™s specific covariance <span
class="math inline">\(\Sigma_k\)</span>.</li>
<li><span class="math inline">\(\log(|\Sigma_k|)\)</span>: A term that
penalizes classes with larger variance.</li>
<li><span class="math inline">\(\log(\pi_k)\)</span>: The prior
probability of class <span class="math inline">\(k\)</span>. This is our
initial belief about how likely class <span
class="math inline">\(k\)</span> is, before seeing the data.
<ul>
<li><span class="math inline">\((x - \mu_k)^T \Sigma_k^{-1}(x -
\mu_k)\)</span>ï¼šè¿™æ˜¯ä¸€ä¸ªäºŒæ¬¡é¡¹ï¼ˆå› ä¸ºå®ƒæ¶‰åŠ <span
class="math inline">\(x^T \Sigma_k^{-1} x\)</span>ï¼‰ã€‚å®ƒæµ‹é‡ä» <span
class="math inline">\(x\)</span> åˆ°ç±»å‡å€¼ <span
class="math inline">\(\mu_k\)</span>
çš„å¹³æ–¹é©¬æ°è·ç¦»ï¼Œå¹¶æ ¹æ®è¯¥ç±»çš„ç‰¹å®šåæ–¹å·® <span
class="math inline">\(\Sigma_k\)</span> è¿›è¡Œç¼©æ”¾ã€‚</li>
<li><span
class="math inline">\(\log(|\Sigma_k|)\)</span>ï¼šç”¨äºæƒ©ç½šæ–¹å·®è¾ƒå¤§çš„ç±»çš„é¡¹ã€‚</li>
<li><span class="math inline">\(\log(\pi_k)\)</span>ï¼šç±» <span
class="math inline">\(k\)</span> çš„å…ˆéªŒæ¦‚ç‡ã€‚è¿™æ˜¯æˆ‘ä»¬åœ¨çœ‹åˆ°æ•°æ®ä¹‹å‰å¯¹ç±»
<span class="math inline">\(k\)</span> å¯èƒ½æ€§çš„åˆå§‹ä¿¡å¿µã€‚ Because each
class <span class="math inline">\(k\)</span> has its own <span
class="math inline">\(\Sigma_k\)</span>, the quadratic term doesnâ€™t
cancel out when comparing scores between classes, leading to a quadratic
boundary. ç”±äºæ¯ä¸ªç±» <span class="math inline">\(k\)</span> éƒ½æœ‰å…¶è‡ªå·±çš„
<span
class="math inline">\(\Sigma_k\)</span>ï¼Œå› æ­¤åœ¨æ¯”è¾ƒç±»ä¹‹é—´çš„åˆ†æ•°æ—¶ï¼ŒäºŒæ¬¡é¡¹ä¸ä¼šæŠµæ¶ˆï¼Œä»è€Œå¯¼è‡´äºŒæ¬¡è¾¹ç•Œã€‚
<strong>Key Trade-off</strong>:</li>
</ul></li>
<li>If the class variances (<span
class="math inline">\(\Sigma_k\)</span>) are truly different,
<strong>QDA is better</strong>.</li>
<li>If the class variances are similar, <strong>LDA is often
better</strong> because itâ€™s less flexible and less likely to overfit,
especially with a small number of training samples.</li>
<li>å¦‚æœç±»æ–¹å·® (<span class="math inline">\(\Sigma_k\)</span>)
ç¡®å®ä¸åŒï¼Œ<strong>QDA æ›´å¥½</strong>ã€‚</li>
<li>å¦‚æœç±»æ–¹å·®ç›¸ä¼¼ï¼Œ<strong>LDA
é€šå¸¸æ›´å¥½</strong>ï¼Œå› ä¸ºå®ƒçš„çµæ´»æ€§è¾ƒå·®ï¼Œå¹¶ä¸”ä¸å¤ªå¯èƒ½è¿‡æ‹Ÿåˆï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ ·æœ¬æ•°é‡è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚</li>
</ul>
<h3 id="code-implementation-r-and-python">## Code Implementation: R and
Python</h3>
<p>The slides provide R code for fitting a QDA model and evaluating it.
Below is an explanation of the R code and its equivalent in Python using
the popular <code>scikit-learn</code> library.</p>
<h4 id="r-code-from-the-slides">R Code (from the slides)</h4>
<p>The code uses the <code>MASS</code> library for QDA and the
<code>ROCR</code> library for evaluation.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ######## QDA ##########</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Fit the model on the training data</span></span><br><span class="line"><span class="comment"># This formula `Default~.` means &quot;predict &#x27;Default&#x27; using all other variables&quot;.</span></span><br><span class="line">qda.fit.mod2 <span class="operator">&lt;-</span> qda<span class="punctuation">(</span>Default<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> subset<span class="operator">=</span>train.ids<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Make predictions on the test data</span></span><br><span class="line"><span class="comment"># We are interested in the posterior probabilities for the ROC curve</span></span><br><span class="line">qda.fit.pred3 <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>qda.fit.mod2<span class="punctuation">,</span> Default_test<span class="punctuation">)</span><span class="operator">$</span>posterior<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Evaluate using ROC and AUC</span></span><br><span class="line"><span class="comment"># &#x27;prediction&#x27; and &#x27;performance&#x27; are functions from the ROCR library</span></span><br><span class="line">perf <span class="operator">&lt;-</span> performance<span class="punctuation">(</span>prediction<span class="punctuation">(</span>qda.fit.pred3<span class="punctuation">,</span> Default_test<span class="operator">$</span>Default<span class="punctuation">)</span><span class="punctuation">,</span> <span class="string">&quot;auc&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Get the AUC value</span></span><br><span class="line">auc_value <span class="operator">&lt;-</span> perf<span class="operator">@</span>y.values<span class="punctuation">[[</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">]</span></span><br><span class="line"><span class="comment"># Result from slide: 0.9638683</span></span><br></pre></td></tr></table></figure>
<h4 id="python-equivalent-scikit-learn">Python Equivalent
(<code>scikit-learn</code>)</h4>
<p>Hereâ€™s how you would perform the same steps in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> QuadraticDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score, roc_curve</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Default&#x27; is your DataFrame and &#x27;default&#x27; is the target column</span></span><br><span class="line"><span class="comment"># (preprocessing &#x27;student&#x27; and &#x27;default&#x27; columns to numbers)</span></span><br><span class="line"><span class="comment"># Default[&#x27;default_num&#x27;] = Default[&#x27;default&#x27;].apply(lambda x: 1 if x == &#x27;Yes&#x27; else 0)</span></span><br><span class="line"><span class="comment"># X = Default[[&#x27;balance&#x27;, &#x27;income&#x27;, ...]]</span></span><br><span class="line"><span class="comment"># y = Default[&#x27;default_num&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Initialize and fit the QDA model</span></span><br><span class="line">qda = QuadraticDiscriminantAnalysis()</span><br><span class="line">qda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Predict probabilities on the test set</span></span><br><span class="line"><span class="comment"># We need the probability of the positive class (&#x27;Yes&#x27;) for the AUC calculation</span></span><br><span class="line">y_pred_proba = qda.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Calculate the AUC score</span></span><br><span class="line">auc_score = roc_auc_score(y_test, y_pred_proba)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;AUC Score for QDA: <span class="subst">&#123;auc_score:<span class="number">.7</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also plot the ROC curve</span></span><br><span class="line"><span class="comment"># fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)</span></span><br><span class="line"><span class="comment"># plt.plot(fpr, tpr)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure>
<h3 id="model-evaluation-roc-and-auc">## Model Evaluation: ROC and
AUC</h3>
<p>The slides correctly emphasize using the <strong>ROC curve</strong>
and the <strong>Area Under the Curve (AUC)</strong> to compare model
performance.</p>
<ul>
<li><p><strong>ROC Curve (Receiver Operating Characteristic)</strong>:
This plot shows how well a model can distinguish between two classes. It
plots the <strong>True Positive Rate</strong> (y-axis) against the
<strong>False Positive Rate</strong> (x-axis) at all possible
classification thresholds. A better model has a curve that is closer to
the top-left corner.</p></li>
<li><p><strong>AUC (Area Under the Curve)</strong>: This is a single
number that summarizes the entire ROC curve.</p>
<ul>
<li><strong>AUC = 1</strong>: Perfect classifier.</li>
<li><strong>AUC = 0.5</strong>: A useless classifier (equivalent to
random guessing).</li>
<li><strong>AUC &gt; 0.7</strong>: Generally considered an acceptable
model.</li>
</ul></li>
<li><p><strong>ROC
æ›²çº¿ï¼ˆæ¥æ”¶è€…æ“ä½œç‰¹å¾ï¼‰</strong>ï¼šæ­¤å›¾æ˜¾ç¤ºäº†æ¨¡å‹åŒºåˆ†ä¸¤ä¸ªç±»åˆ«çš„èƒ½åŠ›ã€‚å®ƒç»˜åˆ¶äº†æ‰€æœ‰å¯èƒ½çš„åˆ†ç±»é˜ˆå€¼ä¸‹çš„
<strong>çœŸé˜³æ€§ç‡</strong>ï¼ˆy è½´ï¼‰ä¸ <strong>å‡é˜³æ€§ç‡</strong>ï¼ˆx
è½´ï¼‰çš„å¯¹æ¯”å›¾ã€‚æ›´å¥½çš„æ¨¡å‹çš„æ›²çº¿è¶Šé è¿‘å·¦ä¸Šè§’ï¼Œæ•ˆæœå°±è¶Šå¥½ã€‚</p>
<ul>
<li><p><strong>AUCï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰</strong>ï¼šè¿™æ˜¯ä¸€ä¸ªæ¦‚æ‹¬æ•´ä¸ª ROC
æ›²çº¿çš„æ•°å€¼ã€‚</p></li>
<li><p><strong>AUC = 1</strong>ï¼šå®Œç¾çš„åˆ†ç±»å™¨ã€‚</p></li>
<li><p><strong>AUC =
0.5</strong>ï¼šæ— ç”¨çš„åˆ†ç±»å™¨ï¼ˆç›¸å½“äºéšæœºçŒœæµ‹ï¼‰ã€‚</p></li>
<li><p><strong>AUC &gt;
0.7</strong>ï¼šé€šå¸¸è¢«è®¤ä¸ºæ˜¯å¯æ¥å—çš„æ¨¡å‹ã€‚</p></li>
</ul></li>
</ul>
<p>The slides show that for the <code>Default</code> dataset,
<strong>LDAâ€™s AUC (0.9647) was slightly higher than QDAâ€™s
(0.9639)</strong>. This suggests that the assumption of a common
covariance matrix (LDA) was a slightly better fit for this particular
test set, possibly because QDAâ€™s extra flexibility wasnâ€™t needed and it
may have slightly overfit the training data.
è¿™è¡¨æ˜ï¼Œå¯¹äºè¿™ä¸ªç‰¹å®šçš„æµ‹è¯•é›†ï¼Œå…¬å…±åæ–¹å·®çŸ©é˜µ (LDA)
çš„å‡è®¾æ‹Ÿåˆåº¦ç•¥é«˜ï¼Œå¯èƒ½æ˜¯å› ä¸º QDA
çš„é¢å¤–çµæ´»æ€§å¹¶éå¿…éœ€ï¼Œå¹¶ä¸”å¯èƒ½å¯¹è®­ç»ƒæ•°æ®ç•¥å¾®è¿‡æ‹Ÿåˆã€‚</p>
<h3 id="key-takeaways-and-important-images">## Key Takeaways and
Important Images</h3>
<h3
id="heres-a-ranking-of-the-most-important-visual-aids-in-your-slides">Hereâ€™s
a ranking of the most important visual aids in your slides:</h3>
<ol type="1">
<li><p><strong>Slide 68/69 (Model Assumption &amp; Formula)</strong>:
These are the <strong>most critical</strong> slides. They present the
core theoretical difference between LDA and QDA and provide the
mathematical foundation (the discriminant function formula).
Understanding these is key to understanding QDA.</p></li>
<li><p><strong>Slide 73 (ROC Comparison)</strong>: This is the most
important image for <strong>practical evaluation</strong>. It visually
compares the performance of LDA and QDA side-by-side, making it easy to
see which one performs better on this specific dataset. The concept of
AUC is introduced here as the method for comparison.</p></li>
<li><p><strong>Slide 71 (Decision Boundaries with Different
Thresholds)</strong>: This is an excellent conceptual image. It shows
how the quadratic decision boundary (the curved lines) separates the
data points. It also illustrates how changing the probability threshold
(from 0.1 to 0.5 to 0.9) shifts the boundary, trading off between
precision and recall.</p></li>
</ol>
<p>Of course. Here is a summary of the remaining slides, which compare
QDA to other popular classification models like Logistic Regression and
K-Nearest Neighbors (KNN).</p>
<hr />
<h3 id="visualizing-the-core-trade-off-lda-vs.-qda">Visualizing the Core
Trade-off: LDA vs.Â QDA</h3>
<p>This is the most important concept in these slides. The choice
between LDA and QDA depends entirely on the underlying structure of your
data.</p>
<p>The slide shows two scenarios: 1. <strong>Left Plot (<span
class="math inline">\(\Sigma_1 = \Sigma_2\)</span>):</strong> When the
true covariance matrices of the classes are the same, the optimal
decision boundary (the Bayes classifier) is a straight line. LDA, which
assumes equal covariances, creates a linear boundary that approximates
this optimal boundary very well. QDAâ€™s flexible, curved boundary is
unnecessarily complex and might overfit the training data. <strong>In
this case, LDA is better.</strong> 2. <strong>Right Plot (<span
class="math inline">\(\Sigma_1 \neq \Sigma_2\)</span>):</strong> When
the true covariance matrices are different, the optimal decision
boundary is a curve. QDAâ€™s quadratic model can capture this
non-linearity much better than LDAâ€™s rigid linear model. <strong>In this
case, QDA is better.</strong></p>
<p>This perfectly illustrates the <strong>bias-variance
tradeoff</strong>. LDA has higher bias (itâ€™s less flexible) but lower
variance. QDA has lower bias (itâ€™s more flexible) but higher
variance.</p>
<hr />
<h3 id="comparing-performance-on-the-default-dataset">Comparing
Performance on the â€œDefaultâ€ Dataset</h3>
<p>The slides compare four different models on the same classification
task. Letâ€™s look at their performance using the <strong>Area Under the
Curve (AUC)</strong>, where a higher score is better.</p>
<ul>
<li><strong>LDA AUC:</strong> 0.9647</li>
<li><strong>QDA AUC:</strong> 0.9639</li>
<li><strong>Logistic Regression AUC:</strong> 0.9645</li>
<li><strong>K-Nearest Neighbors (KNN):</strong> The plot shows test
error vs.Â K. The error is lowest around K=4, but itâ€™s not directly
converted to an AUC score in the slides.</li>
</ul>
<p>Interestingly, for this particular dataset, LDA, QDA, and Logistic
Regression perform almost identically. This suggests that the decision
boundary for this problem is likely very close to linear, meaning the
extra flexibility of QDA isnâ€™t providing much benefit.</p>
<hr />
<h3 id="pros-and-cons-which-model-to-choose">Pros and Cons: Which Model
to Choose?</h3>
<p>The final slide asks for a comparison of the models. Hereâ€™s a summary
of their key characteristics:</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Decision Boundary</th>
<th style="text-align: left;">Key Pro</th>
<th style="text-align: left;">Key Con</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Logistic Regression</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">Highly interpretable, no strong
assumptions about data distribution.</td>
<td style="text-align: left;">Inflexible; cannot capture non-linear
relationships.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Linear Discriminant Analysis
(LDA)</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">More stable than Logistic Regression when
classes are well-separated.</td>
<td style="text-align: left;">Assumes data is normally distributed with
equal covariance matrices for all classes.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Quadratic Discriminant Analysis
(QDA)</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Quadratic (Curved)</td>
<td style="text-align: left;">More flexible than LDA; can model
non-linear boundaries.</td>
<td style="text-align: left;">Requires more data to estimate parameters
and is more prone to overfitting. Assumes normality.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>K-Nearest Neighbors
(KNN)</strong></td>
<td style="text-align: left;">Non-Parametric</td>
<td style="text-align: left;">Highly Non-linear</td>
<td style="text-align: left;">Extremely flexible; makes no assumptions
about the dataâ€™s distribution.</td>
<td style="text-align: left;">Can be slow on large datasets and suffers
from the â€œcurse of dimensionality.â€ Less interpretable.</td>
</tr>
</tbody>
</table>
<h4 id="summary-of-the-comparison">Summary of the Comparison:</h4>
<ul>
<li><strong>Linear Models (Logistic Regression &amp; LDA):</strong>
Choose these for simplicity, interpretability, and when you believe the
relationship between predictors and the class is linear. LDA often
outperforms Logistic Regression if its normality assumptions are
met.</li>
<li><strong>Non-Linear Models (QDA &amp; KNN):</strong> Choose these
when the decision boundary is likely more complex. QDA is a good middle
ground, offering more flexibility than LDA without being as completely
data-driven as KNN. KNN is the most flexible but requires careful tuning
of the parameter K to avoid overfitting or underfitting.</li>
</ul>
<h1
id="here-is-a-more-detailed-slide-by-slide-analysis-of-the-presentation.">9.
Here is a more detailed, slide-by-slide analysis of the
presentation.</h1>
<h2 id="four-classification-methods-comparison-by-simulation">4.6 Four
Classification Methods: Comparison by Simulation</h2>
<p>This section (slides 81-87) introduces four classification methods
and systematically compares their performance on six different simulated
datasets. The goal is to see which method works best under different
conditions (e.g., linear vs.Â non-linear boundaries, normal
vs.Â non-normal data).</p>
<p>The four methods being compared are: * <strong>Logistic
Regression:</strong> A linear method that models the log-odds as a
linear function of the predictors. * <strong>Linear Discriminant
Analysis (LDA):</strong> Another linear method. It also assumes a linear
decision boundary but makes stronger assumptions than logistic
regression (e.g., that data within each class is normally distributed
with a common covariance matrix). * <strong>Quadratic Discriminant
Analysis (QDA):</strong> A non-linear method. It assumes the log-odds
are a <em>quadratic</em> function, which creates a more flexible, curved
decision boundary. It assumes data within each class is normally
distributed, but <em>without</em> a common covariance matrix. *
<strong>K-Nearest Neighbors (KNN):</strong> A non-parametric, highly
flexible method. Two versions are tested: * <strong>KNN-1 (<span
class="math inline">\(K=1\)</span>):</strong> A very flexible (high
variance) model. * <strong>KNN-CV:</strong> A tuned model where the best
<span class="math inline">\(K\)</span> is chosen via
cross-validation.</p>
<p>æ¯”è¾ƒçš„å››ç§æ–¹æ³•æ˜¯ï¼š *
<strong>é€»è¾‘å›å½’</strong>ï¼šä¸€ç§å°†å¯¹æ•°æ¦‚ç‡å»ºæ¨¡ä¸ºé¢„æµ‹å˜é‡çº¿æ€§å‡½æ•°çš„çº¿æ€§æ–¹æ³•ã€‚
* <strong>çº¿æ€§åˆ¤åˆ«åˆ†æ
(LDA)</strong>ï¼šå¦ä¸€ç§çº¿æ€§æ–¹æ³•ã€‚å®ƒä¹Ÿå‡è®¾çº¿æ€§å†³ç­–è¾¹ç•Œï¼Œä½†æ¯”é€»è¾‘å›å½’åšå‡ºæ›´å¼ºçš„å‡è®¾ï¼ˆä¾‹å¦‚ï¼Œæ¯ä¸ªç±»ä¸­çš„æ•°æ®å‘ˆæ­£æ€åˆ†å¸ƒï¼Œä¸”å…·æœ‰å…±åŒçš„åæ–¹å·®çŸ©é˜µï¼‰ã€‚
* <strong>äºŒæ¬¡åˆ¤åˆ«åˆ†æ
(QDA)</strong>ï¼šä¸€ç§éçº¿æ€§æ–¹æ³•ã€‚å®ƒå‡è®¾å¯¹æ•°æ¦‚ç‡ä¸º<em>äºŒæ¬¡</em>å‡½æ•°ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªæ›´çµæ´»ã€æ›´å¼¯æ›²çš„å†³ç­–è¾¹ç•Œã€‚å®ƒå‡è®¾æ¯ä¸ªç±»ä¸­çš„æ•°æ®æœä»æ­£æ€åˆ†å¸ƒï¼Œä½†<em>æ²¡æœ‰</em>å…±åŒçš„åæ–¹å·®çŸ©é˜µã€‚
* <strong>Kæœ€è¿‘é‚»
(KNN)</strong>ï¼šä¸€ç§éå‚æ•°åŒ–ã€é«˜åº¦çµæ´»çš„æ–¹æ³•ã€‚æµ‹è¯•äº†ä¸¤ä¸ªç‰ˆæœ¬ï¼š *
<strong>KNN-1 (<span
class="math inline">\(K=1\)</span>)</strong>ï¼šä¸€ä¸ªéå¸¸çµæ´»ï¼ˆé«˜æ–¹å·®ï¼‰çš„æ¨¡å‹ã€‚
*
<strong>KNN-CV</strong>ï¼šä¸€ä¸ªç»è¿‡è°ƒæ•´çš„æ¨¡å‹ï¼Œé€šè¿‡äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³çš„<span
class="math inline">\(K\)</span>ã€‚</p>
<h3 id="analysis-of-simulation-scenarios">Analysis of Simulation
Scenarios</h3>
<p>The performance is measured by the <strong>test error rate</strong>
(lower is better), shown in the boxplots for each scenario.
æ€§èƒ½é€šè¿‡<strong>æµ‹è¯•é”™è¯¯ç‡</strong>ï¼ˆè¶Šä½è¶Šå¥½ï¼‰æ¥è¡¡é‡ï¼Œæ¯ä¸ªåœºæ™¯çš„ç®±çº¿å›¾éƒ½æ˜¾ç¤ºäº†è¯¥é”™è¯¯ç‡ã€‚</p>
<ul>
<li><strong>Scenario 1 (Slide 82):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>linear</strong> decision boundary.
Data is <strong>normally distributed</strong> with <em>uncorrelated</em>
predictors.</li>
<li><strong>Result:</strong> <strong>LDA and Logistic Regression perform
best</strong>. Their test error rates are low and similar. This is
expected, as the setup perfectly matches their core assumption (linear
boundary). QDA is slightly worse because its extra flexibility (being
quadratic) is unnecessary. KNN-1 is the worst, as its high flexibility
leads to high variance (overfitting).</li>
<li><strong>ç»“æœï¼š</strong> <strong>LDA
å’Œé€»è¾‘å›å½’è¡¨ç°æœ€ä½³</strong>ã€‚å®ƒä»¬çš„æµ‹è¯•é”™è¯¯ç‡è¾ƒä½ä¸”ç›¸ä¼¼ã€‚è¿™æ˜¯æ„æ–™ä¹‹ä¸­çš„ï¼Œå› ä¸ºè®¾ç½®å®Œå…¨ç¬¦åˆå®ƒä»¬çš„æ ¸å¿ƒå‡è®¾ï¼ˆçº¿æ€§è¾¹ç•Œï¼‰ã€‚QDA
ç•¥å·®ï¼Œå› ä¸ºå…¶é¢å¤–çš„çµæ´»æ€§ï¼ˆäºŒæ¬¡æ–¹ï¼‰æ˜¯ä¸å¿…è¦çš„ã€‚KNN-1
æœ€å·®ï¼Œå› ä¸ºå…¶é«˜çµæ´»æ€§å¯¼è‡´æ–¹å·®è¾ƒå¤§ï¼ˆè¿‡æ‹Ÿåˆï¼‰ã€‚</li>
</ul></li>
<li><strong>Scenario 2 (Slide 83):</strong>
<ul>
<li><strong>Setup:</strong> Same as Scenario 1 (<strong>linear</strong>
boundary, <strong>normal</strong> data), but now the two predictors have
a <strong>correlation of 0.5</strong>.</li>
<li><strong>Result:</strong> <strong>Almost no change</strong> from
Scenario 1. <strong>LDA and Logistic Regression are still the
best</strong>. This shows that these linear methods are robust to
correlation between predictors.</li>
<li><strong>ç»“æœï¼š</strong>ä¸åœºæ™¯ 1
ç›¸æ¯”<strong>å‡ ä¹æ²¡æœ‰å˜åŒ–</strong>ã€‚<strong>LDA
å’Œé€»è¾‘å›å½’ä»ç„¶æ˜¯æœ€ä½³</strong>ã€‚è¿™è¡¨æ˜è¿™äº›çº¿æ€§æ–¹æ³•å¯¹é¢„æµ‹å› å­ä¹‹é—´çš„ç›¸å…³æ€§å…·æœ‰é²æ£’æ€§ã€‚</li>
</ul></li>
<li><strong>Scenario 3 (Slide 84):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>linear</strong> decision boundary,
but the data is drawn from a <strong>t-distribution</strong> (which is
non-normal and has â€œheavy tails,â€ or more extreme outliers).</li>
<li><strong>Result:</strong> <strong>Logistic Regression is the clear
winner</strong>. LDAâ€™s performance gets worse because its assumption of
<em>normality</em> is violated by the t-distribution. QDAâ€™s performance
deteriorates significantly due to the non-normality. This highlights a
key difference: logistic regression is more robust to violations of the
normality assumption.</li>
<li><strong>ç»“æœï¼š</strong>é€»è¾‘å›å½’æ˜æ˜¾èƒœå‡º**ã€‚LDA çš„æ€§èƒ½ä¼šå˜å·®ï¼Œå› ä¸º t
åˆ†å¸ƒè¿åäº†å…¶æ­£æ€æ€§å‡è®¾ã€‚QDA
çš„æ€§èƒ½ç”±äºéæ­£æ€æ€§è€Œæ˜¾è‘—ä¸‹é™ã€‚è¿™å‡¸æ˜¾äº†ä¸€ä¸ªå…³é”®åŒºåˆ«ï¼šé€»è¾‘å›å½’å¯¹è¿åæ­£æ€æ€§å‡è®¾çš„æƒ…å†µæ›´ç¨³å¥ã€‚</li>
</ul></li>
<li><strong>Scenario 4 (Slide 85):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>quadratic</strong> decision
boundary. Data is <strong>normally distributed</strong> with different
correlations in each class.</li>
<li><strong>Result:</strong> <strong>QDA is the clear winner</strong> by
a large margin. This setup perfectly matches QDAâ€™s assumption (quadratic
boundary from normal data with different covariance structures). All
other methods (LDA, Logistic, KNN) are linear or not flexible enough, so
they perform poorly.</li>
<li><strong>ç»“æœï¼š</strong>QDA æ˜æ˜¾èƒœå‡º**ï¼Œä¸”é¥é¥é¢†å…ˆã€‚æ­¤è®¾ç½®å®Œå…¨ç¬¦åˆ
QDA
çš„å‡è®¾ï¼ˆæ¥è‡ªå…·æœ‰ä¸åŒåæ–¹å·®ç»“æ„çš„æ­£æ€æ•°æ®çš„äºŒæ¬¡è¾¹ç•Œï¼‰ã€‚æ‰€æœ‰å…¶ä»–æ–¹æ³•ï¼ˆLDAã€Logisticã€KNNï¼‰éƒ½æ˜¯çº¿æ€§çš„æˆ–ä¸å¤Ÿçµæ´»ï¼Œå› æ­¤æ€§èƒ½ä¸ä½³ã€‚</li>
</ul></li>
<li><strong>Scenario 5 (Slide 86):</strong>
<ul>
<li><strong>Setup:</strong> Another <strong>quadratic</strong> boundary,
but generated in a different way (using a logistic function of quadratic
terms).</li>
<li><strong>Result:</strong> <strong>QDA performs best again</strong>,
closely followed by the flexible <strong>KNN-CV</strong>. The linear
methods (LDA, Logistic) have poor performance because they cannot
capture the curve.</li>
<li><strong>ç»“æœï¼šQDA
å†æ¬¡è¡¨ç°æœ€ä½³</strong>ï¼Œç´§éšå…¶åçš„æ˜¯çµæ´»çš„<strong>KNN-CV</strong>ã€‚çº¿æ€§æ–¹æ³•ï¼ˆLDAã€Logisticï¼‰æ€§èƒ½è¾ƒå·®ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•æ•æ‰æ›²çº¿ã€‚</li>
</ul></li>
<li><strong>Scenario 6 (Slide 87):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>complex, non-linear</strong>
decision boundary (more complex than a simple quadratic curve).</li>
<li><strong>Result:</strong> The <strong>flexible KNN-CV method is the
winner</strong>. Its non-parametric nature allows it to approximate the
complex shape. QDA is not flexible <em>enough</em> and performs worse.
This slide highlights the bias-variance trade-off: the overly simple
KNN-1 is the worst, but the <em>tuned</em> KNN-CV is the best.</li>
<li><strong>ç»“æœï¼š</strong>çµæ´»çš„ KNN-CV
æ–¹æ³•èƒœå‡º**ã€‚å…¶éå‚æ•°ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿè¿‘ä¼¼å¤æ‚çš„å½¢çŠ¶ã€‚ QDA
ä¸å¤Ÿçµæ´»ï¼Œæ€§èƒ½è¾ƒå·®ã€‚è¿™å¼ å¹»ç¯ç‰‡é‡ç‚¹ä»‹ç»äº†åå·®-æ–¹å·®æƒè¡¡ï¼šè¿‡äºç®€å•çš„ KNN-1
æœ€å·®ï¼Œè€Œ <em>è°ƒæ•´åçš„</em> KNN-CV æœ€å¥½ã€‚</li>
</ul></li>
</ul>
<h2 id="r-example-on-smarket-data">4.7 R Example on Smarket Data</h2>
<p>This section (slides 88-93) applies Logistic Regression and LDA to
the <code>Smarket</code> dataset from the <code>ISLR</code> package to
predict the stock marketâ€™s <code>Direction</code> (Up or Down).
æœ¬èŠ‚ï¼ˆå¹»ç¯ç‰‡ 88-93ï¼‰å°†é€»è¾‘å›å½’å’Œ LDA
åº”ç”¨äºâ€œISLRâ€åŒ…ä¸­çš„â€œSmarketâ€æ•°æ®é›†ï¼Œä»¥é¢„æµ‹è‚¡å¸‚çš„â€œæ–¹å‘â€ï¼ˆä¸Šæ¶¨æˆ–ä¸‹è·Œï¼‰ã€‚
### Data Preparation (Slides 88, 89, 90)</p>
<ol type="1">
<li><strong>Load Data:</strong> The <code>ISLR</code> library is loaded,
and the <code>Smarket</code> dataset is explored. It contains daily
percentage returns (<code>Lag1</code>â€¦<code>Lag5</code> for the previous
5 days, <code>Today</code>), <code>Volume</code>, and the
<code>Year</code>.</li>
<li><strong>Explore Data:</strong> A correlation matrix
(<code>cor(Smarket[,-9])</code>) is computed, and a plot of
<code>Volume</code> over time is generated.</li>
<li><strong>Split Data:</strong> The data is split into a training set
(Years 2001-2004) and a test set (Year 2005).
<ul>
<li><code>train &lt;- (Year&lt;2005)</code></li>
<li><code>Smarket.2005 &lt;- Smarket[!train,]</code></li>
<li><code>Direction.2005 &lt;- Direction[!train]</code></li>
<li>The test set has 252 observations.</li>
</ul></li>
<li><strong>åŠ è½½æ•°æ®</strong>ï¼šåŠ è½½â€œISLRâ€åº“ï¼Œå¹¶æ¢ç´¢â€œSmarketâ€æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¯æ—¥ç™¾åˆ†æ¯”æ”¶ç›Šç‡ï¼ˆå‰
5 å¤©çš„â€œLag1â€â€¦â€œLag5â€ï¼Œâ€œä»Šæ—¥â€ï¼‰ã€â€œæˆäº¤é‡â€å’Œâ€œå¹´ä»½â€ã€‚</li>
<li><strong>æ¢ç´¢æ•°æ®</strong>ï¼šè®¡ç®—ç›¸å…³çŸ©é˜µ
(<code>cor(Smarket[,-9])</code>)ï¼Œå¹¶ç”Ÿæˆâ€œæˆäº¤é‡â€éšæ—¶é—´å˜åŒ–çš„å›¾è¡¨ã€‚</li>
<li><strong>æ‹†åˆ†æ•°æ®</strong>ï¼šå°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆå¹´ä»½
2001-2004ï¼‰å’Œæµ‹è¯•é›†ï¼ˆå¹´ä»½ 2005ï¼‰ã€‚
<ul>
<li><code>train &lt;- (Year&lt;2005)</code></li>
<li><code>Smarket.2005 &lt;- Smarket[!train,]</code></li>
<li><code>Direction.2005 &lt;- Direction[!train]</code></li>
<li>æµ‹è¯•é›†åŒ…å« 252 ä¸ªè§‚æµ‹å€¼ã€‚</li>
</ul></li>
</ol>
<h3 id="model-1-logistic-regression-all-predictors-slide-90">Model 1:
Logistic Regression (All Predictors) (Slide 90)</h3>
<ul>
<li><strong>Model:</strong> A logistic regression model is fit on the
training data using <em>all</em> predictors.
<ul>
<li><code>glm.fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data=Smarket, family=binomial, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> The model is used to predict the
direction for the 2005 test data.
<ul>
<li><code>glm.probs &lt;- predict(glm.fit, Smarket.2005, type="response")</code></li>
<li>A threshold of 0.5 is used to classify: if <span
class="math inline">\(P(\text{Up}) &gt; 0.5\)</span>, predict â€œUpâ€.</li>
</ul></li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.5198 (or <strong>48.0%
accuracy</strong>).</li>
<li><strong>Conclusion:</strong> This is â€œnot good!â€â€”itâ€™s worse than
flipping a coin. This suggests the model is either too complex or the
predictors are not useful.</li>
</ul></li>
</ul>
<h3 id="model-2-logistic-regression-lag1-lag2-slide-91">Model 2:
Logistic Regression (Lag1 &amp; Lag2) (Slide 91)</h3>
<ul>
<li><strong>Model:</strong> Based on the poor results, a simpler model
is tried, using only <code>Lag1</code> and <code>Lag2</code>.
<ul>
<li><code>glm.fit &lt;- glm(Direction ~ Lag1 + Lag2, data=Smarket, family=binomial, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> Predictions are made on the 2005 test
set.</li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.4404 (or <strong>55.95%
accuracy</strong>). This is an improvement.</li>
<li><strong>Confusion Matrix:</strong> | | True Down | True Up | | :â€” |
:â€” | :â€” | | <strong>Pred Down</strong> | 77 | 69 | | <strong>Pred
Up</strong> | 35 | 71 |</li>
<li><strong>ROC and AUC:</strong> The ROC (Receiver Operating
Characteristic) curve is plotted, and the AUC (Area Under the Curve) is
calculated.</li>
<li><strong>AUC Value:</strong> <strong>0.5584</strong>. This is very
close to 0.5 (which represents a random-chance model), indicating that
the model has very weak predictive power, even though its accuracy is
above 50%.</li>
</ul></li>
</ul>
<h3 id="model-3-lda-lag1-lag2-slide-92">Model 3: LDA (Lag1 &amp; Lag2)
(Slide 92)</h3>
<ul>
<li><strong>Model:</strong> LDA is now performed using the same setup:
<code>Lag1</code> and <code>Lag2</code> as predictors, trained on the
2001-2004 data.
<ul>
<li><code>library(MASS)</code></li>
<li><code>lda.fit &lt;- lda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> Predictions are made on the 2005 test
set.
<ul>
<li><code>lda.pred &lt;- predict(lda.fit, Smarket.2005)</code></li>
</ul></li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.4404 (or <strong>55.95%
accuracy</strong>).</li>
<li><strong>Confusion Matrix:</strong> | | True Down | True Up | | :â€” |
:â€” | :â€” | | <strong>Pred Down</strong> | 77 | 69 | | <strong>Pred
Up</strong> | 35 | 71 |</li>
<li><strong>Observation:</strong> The confusion matrix and accuracy are
<em>identical</em> to the logistic regression model.</li>
</ul></li>
</ul>
<h3 id="final-comparison-slide-93">Final Comparison (Slide 93)</h3>
<ul>
<li><strong>ROC and AUC for LDA:</strong> The ROC curve for the LDA
model is plotted.</li>
<li><strong>AUC Value:</strong> <strong>0.5584</strong>.</li>
<li><strong>Main Conclusion:</strong> As highlighted in the green box,
<strong>â€œLDA has identical performance as Logistic regression!â€</strong>
In this specific practical example, using these two predictors, both
linear methods produce the exact same confusion matrix, the same
accuracy (56%), and the same AUC (0.558). This reinforces the
theoretical idea that both are fitting a linear boundary.</li>
</ul>
<h3 id="æœ€ç»ˆæ¯”è¾ƒå¹»ç¯ç‰‡-93">æœ€ç»ˆæ¯”è¾ƒï¼ˆå¹»ç¯ç‰‡ 93ï¼‰</h3>
<ul>
<li><strong>LDA çš„ ROC å’Œ AUCï¼š</strong>ç»˜åˆ¶äº† LDA æ¨¡å‹çš„ ROC
æ›²çº¿ã€‚</li>
<li><strong>AUC å€¼ï¼š</strong>0.5584**ã€‚</li>
<li><strong>ä¸»è¦ç»“è®ºï¼š</strong>å¦‚ç»¿è‰²æ–¹æ¡†æ‰€ç¤ºï¼Œâ€œLDA çš„æ€§èƒ½ä¸ Logistic
å›å½’ç›¸åŒï¼â€**
åœ¨è¿™ä¸ªå…·ä½“çš„å®é™…ç¤ºä¾‹ä¸­ï¼Œä½¿ç”¨è¿™ä¸¤ä¸ªé¢„æµ‹å˜é‡ï¼Œä¸¤ç§çº¿æ€§æ–¹æ³•éƒ½äº§ç”Ÿäº†å®Œå…¨ç›¸åŒçš„æ··æ·†çŸ©é˜µã€ç›¸åŒçš„å‡†ç¡®ç‡ï¼ˆ56%ï¼‰å’Œç›¸åŒçš„
AUCï¼ˆ0.558ï¼‰ã€‚è¿™å¼ºåŒ–äº†ä¸¤è€…å‡æ‹Ÿåˆçº¿æ€§è¾¹ç•Œçš„ç†è®ºè§‚ç‚¹ã€‚</li>
</ul>
<h2 id="r-example-on-smarket-data-continued">4.7 R Example on Smarket
Data (Continued)</h2>
<p>The previous slides showed that Logistic Regression and Linear
Discriminant Analysis (LDA) had <strong>identical performance</strong>
on the Smarket dataset (using <code>Lag1</code> and <code>Lag2</code>),
both achieving 56% test accuracy and an AUC of 0.558. The analysis now
tests a more flexible method, QDA.</p>
<h3 id="model-3-qda-lag1-lag2-slides-94-95">Model 3: QDA (Lag1 &amp;
Lag2) (Slides 94-95)</h3>
<ul>
<li><strong>Model:</strong> A Quadratic Discriminant Analysis (QDA)
model is fit on the same training data (2001-2004) using only the
<code>Lag1</code> and <code>Lag2</code> predictors.
<ul>
<li><code>qda.fit &lt;- qda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> The model is used to predict the market
direction for the 2005 test set.</li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Accuracy:</strong> The model achieves a test accuracy
of <strong>0.5992 (or 60%)</strong>.</li>
<li><strong>AUC:</strong> The Area Under the Curve (AUC) for the QDA
model is <strong>0.562</strong>.</li>
</ul></li>
<li><strong>Conclusion:</strong> As the slide highlights, <strong>â€œQDA
has better test performance than LDA and Logistic
regression!â€</strong></li>
</ul>
<h3 id="smarket-example-summary">Smarket Example Summary</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Model Type</th>
<th style="text-align: left;">Test Accuracy</th>
<th style="text-align: left;">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Logistic Regression</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">~56%</td>
<td style="text-align: left;">0.558</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LDA</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">~56%</td>
<td style="text-align: left;">0.558</td>
</tr>
<tr>
<td style="text-align: left;"><strong>QDA</strong></td>
<td style="text-align: left;">Quadratic</td>
<td style="text-align: left;"><strong>~60%</strong></td>
<td style="text-align: left;"><strong>0.562</strong></td>
</tr>
</tbody>
</table>
<p>This practical example reinforces the lessons from the simulations
(Section 4.6). The two linear methods (LDA, Logistic) had identical
performance. The more flexible, non-linear QDA model performed better,
suggesting that the true decision boundary between â€œUpâ€ and â€œDownâ€
(based on <code>Lag1</code> and <code>Lag2</code>) is not perfectly
linear.</p>
<h2 id="kernel-lda">4.8 Kernel LDA</h2>
<p>This new section introduces an even more advanced non-linear method,
Kernel LDA.</p>
<h3 id="the-problem-linear-inseparability-slide-97">The Problem: Linear
Inseparability (Slide 97)</h3>
<p>The section starts with a clear visual example. A dataset of two
concentric circles (a â€œdonutâ€ shape) is <strong>linearly
inseparable</strong>. It is impossible to draw a single straight line to
separate the inner (purple) class from the outer (yellow) class.</p>
<h3 id="the-solution-the-kernel-trick-slides-97-99">The Solution: The
Kernel Trick (Slides 97, 99)</h3>
<ol type="1">
<li><strong>Nonlinear Transformation:</strong> The data is â€œliftedâ€ into
a higher-dimensional <em>feature space</em> using a <strong>nonlinear
transformation</strong>, <span class="math inline">\(x \mapsto
\phi(x)\)</span>. In the example on the slide, the 2D data is
transformed, and in this new space, the two classes <em>become</em>
<strong>linearly separable</strong>.</li>
<li><strong>The â€œKernel Trickâ€:</strong> The main idea (from slide 99)
is that we donâ€™t need to explicitly compute this complex transformation
<span class="math inline">\(\phi(x)\)</span>. LDA (based on Fisherâ€™s
approach) only requires inner products of the data points. The â€œkernel
trickâ€ allows us to replace the inner product in the high-dimensional
feature space (<span class="math inline">\(x_i^T x_j\)</span>) with a
simple <strong>kernel function</strong>, <span
class="math inline">\(k(x_i, x_j)\)</span>, computed in the original,
low-dimensional space.
<ul>
<li>An example of such a kernel is the <strong>Gaussian (RBF)
kernel</strong>: <span class="math inline">\(k(x_i, x_j) \propto
e^{-\|x_i - x_j\|^2 / \sigma^2}\)</span>.</li>
</ul></li>
</ol>
<h3 id="academic-foundations-slide-98">Academic Foundations (Slide
98)</h3>
<p>This method is based on foundational academic papers that generalized
linear methods using kernels: * <strong>Fisher discriminant analysis
with kernels</strong> (Mika, 1999) * <strong>Generalized Discriminant
Analysis Using a Kernel Approach</strong> (Baudat, 2000) *
<strong>Kernel principal component analysis</strong> (SchÃ¶lkopf,
1997)</p>
<p>In short, Kernel LDA is an extension of LDA that uses the kernel
trick to find a linear boundary in a high-dimensional feature space,
which corresponds to a highly non-linear boundary in the original
space.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/27/QM9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/27/QM9/" class="post-title-link" itemprop="url">QM9 Dataset</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-27 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-27T21:00:00+08:00">2025-09-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-29 03:57:13" itemprop="dateModified" datetime="2025-09-29T03:57:13+08:00">2025-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/dataset/" itemprop="url" rel="index"><span itemprop="name">dataset</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="qm9-æ•°æ®é›†çš„xyzæ ¼å¼è¯¦è§£">1. QM9 æ•°æ®é›†çš„XYZæ ¼å¼è¯¦è§£</h3>
<p>è¿™ä¸ªæ•°æ®é›†ä½¿ç”¨çš„ â€œXYZ-likeâ€
æ ¼å¼æ˜¯ä¸€ç§<strong>æ‰©å±•çš„ã€éæ ‡å‡†çš„XYZæ ¼å¼</strong>ã€‚</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 43%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr>
<th>è¡Œå·</th>
<th>å†…å®¹</th>
<th>è§£é‡Š</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ç¬¬ 1 è¡Œ</strong></td>
<td><code>na</code></td>
<td>ä¸€ä¸ªæ•´æ•°ï¼Œä»£è¡¨åˆ†å­ä¸­çš„åŸå­æ€»æ•°ã€‚</td>
</tr>
<tr>
<td><strong>ç¬¬ 2 è¡Œ</strong></td>
<td><code>Properties 1-17</code></td>
<td>åŒ…å«17ä¸ªç†åŒ–æ€§è´¨çš„æ•°å€¼ï¼Œç”¨åˆ¶è¡¨ç¬¦æˆ–ç©ºæ ¼åˆ†éš”ã€‚</td>
</tr>
<tr>
<td><strong>ç¬¬ 3 åˆ° na+2 è¡Œ</strong></td>
<td><code>Element  x  y  z  charge</code></td>
<td>æ¯è¡Œä»£è¡¨ä¸€ä¸ªåŸå­ã€‚ä¾æ¬¡æ˜¯ï¼šå…ƒç´ ç¬¦å·ã€x/y/zåæ ‡ï¼ˆå•ä½ï¼šåŸƒï¼‰ã€Mullikenéƒ¨åˆ†ç”µè·ï¼ˆå•ä½ï¼šeï¼‰ã€‚</td>
</tr>
<tr>
<td><strong>ç¬¬ na+3 è¡Œ</strong></td>
<td><code>Frequencies</code></td>
<td>åˆ†å­çš„æŒ¯åŠ¨é¢‘ç‡ï¼ˆ3na-5æˆ–3na-6ä¸ªï¼‰ã€‚</td>
</tr>
<tr>
<td><strong>ç¬¬ na+4 è¡Œ</strong></td>
<td><code>SMILES_GDB9   SMILES_relaxed</code></td>
<td>æ¥è‡ªGDB9çš„SMILESå­—ç¬¦ä¸²å’Œå¼›è±«åçš„å‡ ä½•æ„å‹çš„SMILESå­—ç¬¦ä¸²ã€‚</td>
</tr>
<tr>
<td><strong>ç¬¬ na+5 è¡Œ</strong></td>
<td><code>InChI_GDB9    InChI_relaxed</code></td>
<td>å¯¹åº”çš„InChIå­—ç¬¦ä¸²ã€‚</td>
</tr>
</tbody>
</table>
<p><strong>ä¸æ ‡å‡†XYZæ ¼å¼å¯¹æ¯”ï¼š</strong> *
<strong>æ ‡å‡†æ ¼å¼</strong>åªæœ‰ç¬¬1è¡Œï¼ˆåŸå­æ•°ï¼‰ã€ç¬¬2è¡Œï¼ˆæ³¨é‡Šï¼‰å’Œåç»­çš„åŸå­åæ ‡è¡Œï¼ˆä»…å«å…ƒç´ å’Œxyzåæ ‡ï¼‰ã€‚
*
<strong>QM9æ ¼å¼</strong>åœ¨ç¬¬2è¡Œæ’å…¥äº†å¤§é‡å±æ€§æ•°æ®ï¼Œåœ¨åŸå­åæ ‡è¡Œå¢åŠ äº†ç”µè·åˆ—ï¼Œå¹¶åœ¨æ–‡ä»¶æœ«å°¾é™„åŠ äº†é¢‘ç‡ã€SMILESå’ŒInChIä¿¡æ¯ã€‚</p>
<h3 id="readme">2. readme</h3>
<ol type="1">
<li><strong>æ•°æ®é›†æ ¸å¿ƒå†…å®¹</strong>:
<ul>
<li>å®ƒåŒ…å«äº†<strong>133,885ä¸ª</strong>å°å‹æœ‰æœºåˆ†å­ï¼ˆç”±H, C, N, O,
Få…ƒç´ ç»„æˆï¼‰çš„é‡å­åŒ–å­¦è®¡ç®—æ•°æ®ã€‚</li>
<li>æ‰€æœ‰åˆ†å­çš„å‡ ä½•æ„å‹éƒ½ç»è¿‡äº†<strong>DFT/B3LYP/6-31G(2df,p)</strong>æ°´å¹³çš„ä¼˜åŒ–ã€‚</li>
<li><code>dsC7O2H10nsd.xyz.tar.bz2</code>æ˜¯è¯¥æ•°æ®é›†çš„ä¸€ä¸ªå­é›†ï¼Œä¸“é—¨åŒ…å«<strong>6,095ä¸ªCâ‚‡Hâ‚â‚€Oâ‚‚çš„åŒåˆ†å¼‚æ„ä½“</strong>ï¼Œå…¶èƒ½é‡å­¦æ€§è´¨åœ¨æ›´é«˜ç²¾åº¦çš„<strong>G4MP2</strong>ç†è®ºæ°´å¹³ä¸‹è®¡ç®—ã€‚</li>
</ul></li>
<li><strong>æ–‡ä»¶ç»“æ„ä¸æ ¼å¼</strong>:
<ul>
<li>æ˜ç¡®æŒ‡å‡ºæ¯ä¸ªåˆ†å­å­˜å‚¨åœ¨å•ç‹¬çš„<code>.xyz</code>æ–‡ä»¶ä¸­ï¼Œå¹¶è¯¦ç»†æè¿°äº†ä¸Šè¿°çš„<strong>éæ ‡å‡†XYZæ‰©å±•æ ¼å¼</strong>ã€‚</li>
<li>è¯¦ç»†åˆ—å‡ºäº†è®°å½•åœ¨æ–‡ä»¶ç¬¬2è¡Œçš„<strong>17ç§ç†åŒ–æ€§è´¨</strong>ï¼ŒåŒ…æ‹¬è½¬åŠ¨å¸¸æ•°(A,
B,
C)ã€å¶æçŸ©(mu)ã€HOMO/LUMOèƒ½çº§ã€é›¶ç‚¹æŒ¯åŠ¨èƒ½(zpve)ã€å†…èƒ½(U)ã€ç„“(H)å’Œå‰å¸ƒæ–¯è‡ªç”±èƒ½(G)ç­‰ã€‚</li>
</ul></li>
<li><strong>æ•°æ®æ¥æºä¸è®¡ç®—æ–¹æ³•</strong>:
<ul>
<li>æ•°æ®æºäº<strong>GDB-9</strong>åŒ–å­¦æ•°æ®åº“ã€‚</li>
<li>ä¸»è¦ä½¿ç”¨äº†ä¸¤ç§é‡å­åŒ–å­¦ç†è®ºæ°´å¹³ï¼š<strong>B3LYP</strong>ç”¨äºå¤§éƒ¨åˆ†å±æ€§è®¡ç®—ï¼Œ<strong>G4MP2</strong>ç”¨äºCâ‚‡Hâ‚â‚€Oâ‚‚å­é›†çš„èƒ½é‡è®¡ç®—ã€‚</li>
</ul></li>
<li><strong>å¼•ç”¨è¦æ±‚</strong>:
<ul>
<li>æ–‡ä»¶æ˜ç¡®è¦æ±‚ï¼Œå¦‚æœä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œéœ€è¦å¼•ç”¨Raghunathan
Ramakrishnanç­‰äººåœ¨2014å¹´å‘è¡¨äºã€ŠScientific Dataã€‹çš„è®ºæ–‡ã€‚</li>
</ul></li>
<li><strong>å…¶ä»–ä¿¡æ¯</strong>:
<ul>
<li>æä¾›äº†ä¸€äº›é¢å¤–æ–‡ä»¶ï¼ˆå¦‚<code>validation.txt</code>,
<code>uncharacterized.txt</code>ï¼‰çš„è¯´æ˜ã€‚</li>
<li>æåˆ°äº†æ•°æ®é›†ä¸­æœ‰å°‘æ•°å‡ ä¸ªåˆ†å­åœ¨å‡ ä½•ä¼˜åŒ–æ—¶éš¾ä»¥æ”¶æ•›ã€‚</li>
</ul></li>
</ol>
<h3 id="å¯è§†åŒ–">3. å¯è§†åŒ–</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import ase.io</span><br><span class="line">import nglview as nv</span><br><span class="line">import io</span><br><span class="line"></span><br><span class="line">def parse_qm9_xyz(file_path):</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    Parses a QM9 extended XYZ file and returns a standard XYZ string.</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    with open(file_path, <span class="string">&#x27;r&#x27;</span>) as f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First line is the number of atoms</span></span><br><span class="line">    num_atoms = int(lines[0].strip())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The next line is properties (skip it)</span></span><br><span class="line">    <span class="comment"># The next num_atoms lines are the coordinates</span></span><br><span class="line">    coord_lines = lines[2:2+num_atoms]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Rebuild a standard XYZ format string in memory</span></span><br><span class="line">    standard_xyz = f<span class="string">&quot;&#123;num_atoms&#125;\n&quot;</span></span><br><span class="line">    standard_xyz += <span class="string">&quot;Comment line\n&quot;</span> <span class="comment"># Add a standard comment line</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> coord_lines:</span><br><span class="line">        parts = line.split()</span><br><span class="line">        <span class="comment"># Keep only the element and the x, y, z coordinates</span></span><br><span class="line">        standard_xyz += f<span class="string">&quot;&#123;parts[0]&#125; &#123;parts[1]&#125; &#123;parts[2]&#125; &#123;parts[3]&#125;\n&quot;</span></span><br><span class="line">        </span><br><span class="line">    <span class="built_in">return</span> standard_xyz</span><br><span class="line"></span><br><span class="line"><span class="comment"># Path to your data file</span></span><br><span class="line">file_path = <span class="string">&quot;/root/QM9/QM9/Data_for_6095_constitutional_isomers_of_C7H10O2.xyz/dsC7O2H10nsd_0001.xyz&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Parse the special file format into a standard XYZ string</span></span><br><span class="line">standard_xyz_data = parse_qm9_xyz(file_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. ASE reads the standard XYZ data from the string variable</span></span><br><span class="line"><span class="comment">#    We use io.StringIO to make the string behave like a file</span></span><br><span class="line">atoms = ase.io.read(io.StringIO(standard_xyz_data), format=<span class="string">&quot;xyz&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create the nglview visualization widget</span></span><br><span class="line">view = nv.show_ase(atoms)</span><br><span class="line">view.add_ball_and_stick()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the widget in the notebook output</span></span><br><span class="line">view</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol type="1">
<li><strong>å®šä¹‰è§£æå‡½æ•° <code>parse_qm9_xyz</code></strong>:
<ul>
<li><strong>ç›®çš„</strong>:
å°†è¿™ä¸ªå‡½æ•°ä½œä¸ºä¸“é—¨å¤„ç†QM9ç‰¹æ®Šæ ¼å¼çš„å·¥å…·ã€‚ä»£ç ä¸»ä½“æ¸…æ™°ï¼Œæ˜“äºå¤ç”¨ã€‚</li>
<li><strong>è¯»å–æ–‡ä»¶</strong>: <code>with open(...)</code>
å®‰å…¨åœ°æ‰“å¼€æ–‡ä»¶ï¼Œå¹¶ç”¨ <code>f.readlines()</code>
å°†æ–‡ä»¶æ‰€æœ‰è¡Œä¸€æ¬¡æ€§è¯»å…¥ä¸€ä¸ªåˆ—è¡¨ <code>lines</code> ä¸­ã€‚</li>
<li><strong>æå–åŸå­æ•°é‡</strong>:
<code>num_atoms = int(lines[0].strip())</code>
è¯»å–ç¬¬ä¸€è¡Œï¼ˆ<code>lines[0]</code>ï¼‰ï¼Œå»é™¤å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼ï¼ˆ<code>.strip()</code>ï¼‰ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ•´æ•°ã€‚è¿™æ˜¯æ„å»ºæ ‡å‡†XYZæ ¼å¼çš„å¿…è¦ä¿¡æ¯ã€‚</li>
<li><strong>æå–åæ ‡ä¿¡æ¯</strong>:
<code>coord_lines = lines[2:2+num_atoms]</code>
æ ‡ä¿¡æ¯ä»ç¬¬3è¡Œå¼€å§‹ï¼ˆç´¢å¼•ä¸º2ï¼‰ï¼ŒæŒç»­<code>num_atoms</code>è¡Œã€‚é€šè¿‡åˆ—è¡¨åˆ‡ç‰‡ï¼Œç²¾ç¡®åœ°æå–å‡ºæ‰€æœ‰åŒ…å«åŸå­åæ ‡çš„è¡Œï¼Œè·³è¿‡äº†ç¬¬2è¡Œçš„å±æ€§ä¿¡æ¯ã€‚</li>
<li><strong>æ„å»ºæ ‡å‡†XYZæ ¼å¼å­—ç¬¦ä¸²</strong>:
<ul>
<li>åˆ›å»ºä¸€ä¸ªåä¸º <code>standard_xyz</code> çš„æ–°å­—ç¬¦ä¸²ã€‚</li>
<li>é¦–å…ˆï¼Œå°†åŸå­æ•°é‡å’Œæ¢è¡Œç¬¦å†™å…¥ã€‚</li>
<li>ç„¶åï¼Œæ·»åŠ ä¸€è¡Œæ ‡å‡†çš„æ³¨é‡Šï¼ˆâ€œComment
lineâ€ï¼‰ï¼Œè¿™æ˜¯æ ‡å‡†XYZæ ¼å¼æ‰€è¦æ±‚çš„ã€‚</li>
<li>æœ€åï¼Œéå†åˆšåˆšæå–çš„ <code>coord_lines</code> åˆ—è¡¨ã€‚å¯¹äºæ¯ä¸€è¡Œï¼Œä½¿ç”¨
<code>.split()</code>
å°†å…¶æ‹†åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼ˆä¾‹å¦‚ï¼š<code>['C', 'x', 'y', 'z', 'charge']</code>ï¼‰ã€‚åªå–å‰å››éƒ¨åˆ†ï¼ˆå…ƒç´ ç¬¦å·å’Œxyzåæ ‡ï¼‰ï¼Œå¹¶é‡æ–°ç»„åˆæˆæ–°çš„ä¸€è¡Œï¼Œ<strong>ä»è€Œä¸¢å¼ƒäº†æœ«å°¾çš„Mullikenç”µè·æ•°æ®</strong>ã€‚</li>
</ul></li>
<li><strong>è¿”å›ç»“æœ</strong>:
å‡½æ•°è¿”å›ä¸€ä¸ªåŒ…å«äº†æ ‡å‡†XYZæ ¼å¼æ•°æ®çš„ã€å¹²å‡€çš„å­—ç¬¦ä¸²ã€‚</li>
</ul></li>
<li><strong>ä¸»ç¨‹åºæ‰§è¡Œæµç¨‹</strong>:
<ul>
<li><strong>è°ƒç”¨å‡½æ•°</strong>:
<code>standard_xyz_data = parse_qm9_xyz(file_path)</code>
è°ƒç”¨ä¸Šé¢çš„å‡½æ•°ï¼Œå®Œæˆä»æ–‡ä»¶åˆ°æ ‡å‡†æ ¼å¼å­—ç¬¦ä¸²çš„è½¬æ¢ã€‚</li>
<li><strong>åœ¨å†…å­˜ä¸­è¯»å–</strong>:
<code>ase.io.read(io.StringIO(standard_xyz_data), format="xyz")</code>
è¿™ä¸€æ­¥éå¸¸é«˜æ•ˆã€‚<code>io.StringIO</code> å°†æˆ‘ä»¬çš„å­—ç¬¦ä¸²å˜é‡
<code>standard_xyz_data</code>
æ¨¡æ‹Ÿæˆä¸€ä¸ªå†…å­˜ä¸­çš„æ–‡æœ¬æ–‡ä»¶ã€‚è¿™æ ·ï¼Œ<code>ase.io.read</code>
å°±å¯ä»¥ç›´æ¥è¯»å–å®ƒï¼Œè€Œæ— éœ€å…ˆå°†æ¸…æ´—åçš„æ•°æ®å†™å…¥ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶å†è¯»å–ï¼ŒèŠ‚çœäº†ç£ç›˜I/Oæ“ä½œã€‚</li>
<li><strong>å¯è§†åŒ–</strong>: æ¥ä¸‹æ¥çš„ä»£ç  (<code>nv.show_ase</code>ç­‰)
å°±å’Œæœ€åˆçš„è®¾æƒ³ä¸€æ ·äº†ï¼Œå› ä¸ºæ­¤æ—¶ <code>atoms</code>
å¯¹è±¡å·²ç»æ˜¯é€šè¿‡æ ‡å‡†ã€å¹²å‡€çš„æ•°æ®æˆåŠŸåˆ›å»ºçš„äº†ã€‚</li>
</ul></li>
</ol>
<p><img src="/imgs/QM9/C7O2H10/C7O2H10.png" alt="C7O2H10"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/27/fusionnetwork/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/27/fusionnetwork/" class="post-title-link" itemprop="url">FusionProt - è®ºæ–‡é˜…è¯»</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-27 11:00:00" itemprop="dateCreated datePublished" datetime="2025-09-27T11:00:00+08:00">2025-09-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-29 03:56:14" itemprop="dateModified" datetime="2025-09-29T03:56:14+08:00">2025-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Reading/" itemprop="url" rel="index"><span itemprop="name">Paper Reading</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Fusing Sequence and Structural Information for Unified Protein
Representation Learning</p>
<p><a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=imcinaOHod">FusionProt</a></p>
<h2 id="è›‹ç™½è´¨è¡¨ç¤ºå­¦ä¹ ">1 è›‹ç™½è´¨è¡¨ç¤ºå­¦ä¹ ï¼š</h2>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>FusionProt :å¯å­¦ä¹ èåˆ
tokenå’Œè¿­ä»£åŒå‘ä¿¡æ¯äº¤æ¢ï¼Œå®ç°åºåˆ—ä¸ç»“æ„çš„åŠ¨æ€ååŒå­¦ä¹ ï¼Œè€Œéé™æ€æ‹¼æ¥ã€‚</p>
<h2 id="ä¸€ç»´1dæ°¨åŸºé…¸åºåˆ—å’Œä¸‰ç»´3dç©ºé—´ç»“æ„">2.
ä¸€ç»´ï¼ˆ1Dï¼‰æ°¨åŸºé…¸åºåˆ—å’Œä¸‰ç»´ï¼ˆ3Dï¼‰ç©ºé—´ç»“æ„ï¼š</h2>
<ul>
<li><p><strong>å•æ¨¡æ€ä¾èµ–:</strong>
ProteinBERTã€ESM-2ä»…åŸºäºåºåˆ—</p></li>
<li><p><strong>é™æ€èåˆç¼ºé™· :</strong>ESM-GearNetã€SaProt
ç»“åˆåºåˆ—ä¸ç»“æ„ï¼Œä½†é‡‡ç”¨ â€œå•å‘ / ä¸€æ¬¡æ€§èåˆâ€</p></li>
</ul>
<p>å¥½çš„ï¼Œå®Œå…¨æ²¡æœ‰é—®é¢˜ã€‚è¿™æ˜¯å¯¹ <code>FusionNetwork</code>
æ¨¡å‹æ¶æ„ä»£ç çš„ä¸­æ–‡å¤è¿°åˆ†æã€‚</p>
<h2 id="æ¨¡å‹æ€»ä½“">3. æ¨¡å‹æ€»ä½“</h2>
<p><img src="/imgs/fusionProt/FusionProt.png" alt="fusion">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@R.register(<span class="params"><span class="string">&quot;models.FusionNetwork&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FusionNetwork</span>(nn.Module, core.Configurable):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sequence_model, structure_model, fusion=<span class="string">&quot;series&quot;</span>, cross_dim=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(FusionNetwork, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.sequence_model = sequence_model</span><br><span class="line">        <span class="variable language_">self</span>.structure_model = structure_model</span><br><span class="line">        <span class="variable language_">self</span>.output_dim = sequence_model.output_dim + structure_model.output_dim</span><br><span class="line">        <span class="variable language_">self</span>.inject_step = <span class="number">5</span>   <span class="comment"># (sequence_layers / structure_layers) layers</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong><code>class FusionNetwork(...)</code></strong>:
å®šä¹‰äº†æ¨¡å‹ç±»ï¼Œå®ƒç»§æ‰¿è‡ª PyTorch çš„åŸºç¡€æ¨¡å— <code>nn.Module</code>ã€‚</li>
<li><strong><code>__init__(...)</code></strong>:
æ„é€ å‡½æ•°ï¼Œæ¥æ”¶å·²ç»åˆå§‹åŒ–å¥½çš„ <code>sequence_model</code> å’Œ
<code>structure_model</code> ä½œä¸ºè¾“å…¥ã€‚</li>
<li><strong><code>self.output_dim</code></strong>:
å®šä¹‰äº†æ¨¡å‹æœ€ç»ˆè¾“å‡ºç‰¹å¾çš„ç»´åº¦ã€‚å› ä¸ºæœ€åä¼šå°†ä¸¤ä¸ªæ¨¡å‹çš„ç‰¹å¾æ‹¼æ¥èµ·æ¥ï¼Œæ‰€ä»¥æ˜¯ä¸¤è€…è¾“å‡ºç»´åº¦ä¹‹å’Œã€‚</li>
<li><strong><code>self.inject_step = 5</code></strong>:å®šä¹‰äº†ä¿¡æ¯â€œæ³¨å…¥â€æˆ–â€œäº¤æµâ€çš„é¢‘ç‡ã€‚è¿™é‡Œè®¾ç½®ä¸º
5ï¼Œæ„å‘³ç€<strong>æ¯ç»è¿‡åºåˆ—æ¨¡å‹çš„ 5
å±‚ï¼Œå°±ä¼šè¿›è¡Œä¸€æ¬¡ä¿¡æ¯äº¤æ¢</strong>ã€‚</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Structure embeddings layer</span></span><br><span class="line">raw_input_dim = <span class="number">21</span>  <span class="comment"># amino acid tokens</span></span><br><span class="line"><span class="variable language_">self</span>.structure_embed_linear = nn.Linear(raw_input_dim, structure_model.input_dim)</span><br><span class="line"><span class="variable language_">self</span>.embedding_batch_norm = nn.BatchNorm1d(structure_model.input_dim)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>self.structure_embed_linear</code></strong>:
ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç”¨äºå°†åŸå§‹çš„ç»“æ„è¾“å…¥ï¼ˆæ¯”å¦‚ 21
ç§æ°¨åŸºé…¸çš„ç‹¬çƒ­ç¼–ç ï¼‰è½¬æ¢ä¸ºç»“æ„æ¨¡å‹ï¼ˆGNNï¼‰æ‰€æœŸæœ›çš„è¾“å…¥ç»´åº¦ã€‚</li>
<li><strong><code>self.embedding_batch_norm</code></strong>:
æ‰¹å½’ä¸€åŒ–å±‚ï¼Œç”¨äºç¨³å®šç»“æ„åµŒå…¥å±‚çš„è®­ç»ƒè¿‡ç¨‹ã€‚</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normal Initialization of the 3D structure token</span></span><br><span class="line">structure_token = nn.Parameter(torch.Tensor(structure_model.input_dim).unsqueeze(<span class="number">0</span>))</span><br><span class="line">nn.init.normal_(structure_token, mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>)</span><br><span class="line"><span class="variable language_">self</span>.structure_token = nn.Parameter(structure_token.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>self.structure_token</code></strong>: ä¸€ä¸ªå¯å­¦ä¹ çš„å‘é‡
(<code>nn.Parameter</code>)ã€‚è¿™ä¸ªâ€œä»¤ç‰Œâ€ä¸ä»£è¡¨ä»»ä½•çœŸå®çš„åŸå­æˆ–æ°¨åŸºé…¸ï¼Œè€Œæ˜¯ä¸€ä¸ªæŠ½è±¡çš„è½½ä½“ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®ƒå°†<strong>å­¦ä¹ å¦‚ä½•ç¼–ç å’Œè¡¨ç¤ºæ•´ä¸ªè›‹ç™½è´¨çš„å…¨å±€
3D ç»“æ„ä¿¡æ¯</strong>ã€‚å®ƒå°±åƒä¸€ä¸ªä¿¡æ¯ä¿¡ä½¿ã€‚</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Linear Transformation between structure to sequential spaces</span></span><br><span class="line"><span class="variable language_">self</span>.structure_linears = nn.ModuleList([...])</span><br><span class="line"><span class="variable language_">self</span>.seq_linears = nn.ModuleList([...])</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>self.structure_linears</code> /
<code>self.seq_linears</code></strong>:
åºåˆ—æ¨¡å‹å’Œç»“æ„æ¨¡å‹å†…éƒ¨å¤„ç†çš„ç‰¹å¾å‘é‡ç»´åº¦å¯èƒ½ä¸åŒã€‚å½“â€œ3D
ä»¤ç‰Œâ€éœ€è¦åœ¨ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´ä¼ é€’æ—¶ï¼Œè¿™äº›çº¿æ€§å±‚è´Ÿè´£å°†å®ƒçš„è¡¨ç¤ºä»ä¸€ä¸ªæ¨¡å‹çš„ç‰¹å¾ç©ºé—´è½¬æ¢åˆ°å¦ä¸€ä¸ªæ¨¡å‹çš„ç‰¹å¾ç©ºé—´ã€‚</li>
</ul>
<h2 id="å‰å‘">4. å‰å‘</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, graph, <span class="built_in">input</span>, all_loss=<span class="literal">None</span>, metric=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># Build a new protein graph with the 3D token (the lase node)</span></span><br><span class="line">    new_graph = <span class="variable language_">self</span>.build_protein_graph_with_3d_token(graph)</span><br></pre></td></tr></table></figure>
<ul>
<li>é¦–å…ˆè°ƒç”¨è¾…åŠ©å‡½æ•°ï¼Œå°†è¾“å…¥çš„è›‹ç™½è´¨å›¾è°±è¿›è¡Œæ”¹é€ ï¼šä¸ºå›¾è°±å¢åŠ ä¸€ä¸ªä»£è¡¨â€œ3D
ä»¤ç‰Œâ€çš„æ–°èŠ‚ç‚¹ï¼Œå¹¶å°†è¿™ä¸ªæ–°èŠ‚ç‚¹ä¸å›¾ä¸­æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹è¿æ¥èµ·æ¥ã€‚</li>
</ul>
<h5 id="åºåˆ—æ¨¡å‹çš„åˆå§‹åŒ–"><strong>åºåˆ—æ¨¡å‹çš„åˆå§‹åŒ–</strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequence (ESM) model initialization</span></span><br><span class="line">sequence_input = <span class="variable language_">self</span>.sequence_model.mapping[graph.residue_type]</span><br><span class="line">sequence_input[sequence_input == -<span class="number">1</span>] = graph.residue_type[sequence_input == -<span class="number">1</span>]</span><br><span class="line">size = graph.num_residues</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check if sequence size is not bigger than max seq length</span></span><br><span class="line"><span class="keyword">if</span> (size &gt; <span class="variable language_">self</span>.sequence_model.max_input_length).<span class="built_in">any</span>():</span><br><span class="line">    starts = size.cumsum(<span class="number">0</span>) - size</span><br><span class="line">    size = size.clamp(<span class="built_in">max</span>=<span class="variable language_">self</span>.sequence_model.max_input_length)</span><br><span class="line">    ends = starts + size</span><br><span class="line">    mask = functional.multi_slice_mask(starts, ends, graph.num_residues)</span><br><span class="line">    sequence_input = sequence_input[mask]</span><br><span class="line">    graph = graph.subresidue(mask)</span><br><span class="line">size_ext = size</span><br><span class="line"></span><br><span class="line"><span class="comment"># BOS == CLS</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.alphabet.prepend_bos:</span><br><span class="line">    bos = torch.ones(graph.batch_size, dtype=torch.long, device=<span class="variable language_">self</span>.sequence_model.device) * <span class="variable language_">self</span>.sequence_model.alphabet.cls_idx</span><br><span class="line">    sequence_input, size_ext = functional._extend(bos, torch.ones_like(size_ext), sequence_input, size_ext)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.alphabet.append_eos:</span><br><span class="line">    eos = torch.ones(graph.batch_size, dtype=torch.long, device=<span class="variable language_">self</span>.sequence_model.device) * <span class="variable language_">self</span>.sequence_model.alphabet.eos_idx</span><br><span class="line">    sequence_input, size_ext = functional._extend(sequence_input, size_ext, eos, torch.ones_like(size_ext))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Padding</span></span><br><span class="line">tokens = functional.variadic_to_padded(sequence_input, size_ext, value=<span class="variable language_">self</span>.sequence_model.alphabet.padding_idx)[<span class="number">0</span>]</span><br><span class="line">repr_layers = [<span class="variable language_">self</span>.sequence_model.repr_layer]</span><br><span class="line"><span class="keyword">assert</span> tokens.ndim == <span class="number">2</span></span><br><span class="line">padding_mask = tokens.eq(<span class="variable language_">self</span>.sequence_model.model.padding_idx)  <span class="comment"># B, T</span></span><br></pre></td></tr></table></figure>
<ul>
<li>åºåˆ—æ•°æ®è¿›è¡Œ Transformer æ¨¡å‹ï¼ˆå¦‚ ESMï¼‰æ‰€éœ€çš„æ ‡å‡†é¢„å¤„ç†ã€‚</li>
<li>åŒ…æ‹¬æ·»åŠ åºåˆ—å¼€å§‹ï¼ˆBOSï¼‰å’Œç»“æŸï¼ˆEOSï¼‰æ ‡è®°ï¼Œä»¥åŠå°†æ‰€æœ‰åºåˆ—å¡«å……ï¼ˆPaddingï¼‰åˆ°ç›¸åŒé•¿åº¦ï¼Œä»¥ä¾¿è¿›è¡Œæ‰¹å¤„ç†ã€‚</li>
</ul>
<h5 id="æ¨¡å‹åˆå§‹åŒ–ä¸åˆæ¬¡èåˆ"><strong>æ¨¡å‹åˆå§‹åŒ–ä¸åˆæ¬¡èåˆ</strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequence embedding layer</span></span><br><span class="line">x = <span class="variable language_">self</span>.sequence_model.model.embed_scale * <span class="variable language_">self</span>.sequence_model.model.embed_tokens(tokens)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.model.token_dropout:</span><br><span class="line">    x.masked_fill_((tokens == <span class="variable language_">self</span>.sequence_model.model.mask_idx).unsqueeze(-<span class="number">1</span>), <span class="number">0.0</span>)</span><br><span class="line">    <span class="comment"># x: B x T x C</span></span><br><span class="line">    mask_ratio_train = <span class="number">0.15</span> * <span class="number">0.8</span></span><br><span class="line">    src_lengths = (~padding_mask).<span class="built_in">sum</span>(-<span class="number">1</span>)</span><br><span class="line">    mask_ratio_observed = (tokens == <span class="variable language_">self</span>.sequence_model.model.mask_idx).<span class="built_in">sum</span>(-<span class="number">1</span>).to(x.dtype) / src_lengths</span><br><span class="line">    x = x * (<span class="number">1</span> - mask_ratio_train) / (<span class="number">1</span> - mask_ratio_observed)[:, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Structure model initialization</span></span><br><span class="line">structure_hiddens = []</span><br><span class="line">batch_size = graph.batch_size</span><br><span class="line">structure_embedding = <span class="variable language_">self</span>.embedding_batch_norm(<span class="variable language_">self</span>.structure_embed_linear(<span class="built_in">input</span>))</span><br><span class="line">structure_token_batched = <span class="variable language_">self</span>.structure_token.unsqueeze(<span class="number">0</span>).expand(batch_size, -<span class="number">1</span>)</span><br><span class="line">structure_input = torch.cat([structure_embedding.squeeze(<span class="number">1</span>), structure_token_batched], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the 3D token representation</span></span><br><span class="line">structure_token_expanded = <span class="variable language_">self</span>.structure_token.unsqueeze(<span class="number">0</span>).expand(x.size(<span class="number">0</span>), -<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">x = torch.cat((x[:, :-<span class="number">1</span>], structure_token_expanded, x[:, -<span class="number">1</span>:]), dim=<span class="number">1</span>)</span><br><span class="line">padding_mask = torch.cat([padding_mask[:, :-<span class="number">1</span>],</span><br><span class="line">                          torch.zeros(padding_mask.size(<span class="number">0</span>), <span class="number">1</span>).to(padding_mask), padding_mask[:, -<span class="number">1</span>:]], dim=<span class="number">1</span>)</span><br><span class="line">size_ext += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> padding_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    x = x * (<span class="number">1</span> - padding_mask.unsqueeze(-<span class="number">1</span>).type_as(x))</span><br><span class="line"></span><br><span class="line">repr_layers = <span class="built_in">set</span>(repr_layers)</span><br><span class="line">hidden_representations = &#123;&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="number">0</span> <span class="keyword">in</span> repr_layers:</span><br><span class="line">    hidden_representations[<span class="number">0</span>] = x</span><br><span class="line"></span><br><span class="line"><span class="comment"># (B, T, E) =&gt; (T, B, E)</span></span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> padding_mask.<span class="built_in">any</span>():</span><br><span class="line">    padding_mask = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>å°† 3D ä»¤ç‰Œæ’å…¥åºåˆ—ã€‚</strong>
<ol type="1">
<li>ä¸ºåºåˆ—æ•°æ®ç”Ÿæˆåˆå§‹çš„è¯åµŒå…¥è¡¨ç¤º <code>x</code>ã€‚</li>
<li>å°† <code>self.structure_token</code> çš„åˆå§‹çŠ¶æ€æ’å…¥åˆ°åºåˆ—åµŒå…¥
<code>x</code> ä¸­ï¼Œé€šå¸¸æ˜¯æ”¾åœ¨åºåˆ—ç»“æŸæ ‡è®°ï¼ˆEOSï¼‰ä¹‹å‰ã€‚</li>
<li>åºåˆ—æ¨¡å‹çœ‹åˆ°çš„è¾“å…¥åºåˆ—å˜æˆäº†
<code>[BOS, æ®‹åŸº1, æ®‹åŸº2, ..., æ®‹åŸºN, **3Dä»¤ç‰Œ**, EOS]</code>
çš„å½¢å¼ã€‚</li>
</ol></li>
</ul>
<h5 id="èåˆå¾ªç¯"><strong>èåˆå¾ªç¯ </strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> seq_layer_idx, seq_layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.sequence_model.model.layers):</span><br><span class="line">    x, attn = seq_layer(</span><br><span class="line">        x,</span><br><span class="line">        self_attn_padding_mask=padding_mask,</span><br><span class="line">        need_head_weights=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> (seq_layer_idx + <span class="number">1</span>) <span class="keyword">in</span> repr_layers:</span><br><span class="line">        hidden_representations[seq_layer_idx + <span class="number">1</span>] = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>æ¨¡å‹å¼€å§‹é€å±‚éå†åºåˆ—æ¨¡å‹çš„æ‰€æœ‰å±‚ï¼ˆä¾‹å¦‚ Transformer
çš„ç¼–ç å™¨å±‚ï¼‰ã€‚<code>x</code> åœ¨æ¯ä¸€å±‚éƒ½ä¼šè¢«æ›´æ–°ã€‚</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> seq_layer_idx &gt; <span class="number">0</span> <span class="keyword">and</span> seq_layer_idx % <span class="variable language_">self</span>.inject_step == <span class="number">0</span>:</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ä¿¡æ¯æ³¨å…¥ç‚¹</strong>ï¼šæ¯å½“å±‚æ•°çš„ç´¢å¼•èƒ½è¢«
<code>inject_step</code> (å³ 5) æ•´é™¤æ—¶ï¼Œå°±è§¦å‘ä¸€æ¬¡ä¿¡æ¯äº¤æ¢ã€‚</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. ä»åºåˆ—ä¸­æå– 3D ä»¤ç‰Œçš„è¡¨ç¤º</span></span><br><span class="line"><span class="keyword">if</span> structure_layer_index == <span class="number">0</span>:</span><br><span class="line">    structure_input = torch.cat((structure_input[:-<span class="number">1</span> * batch_size],  x[-<span class="number">2</span>, :, :]), dim=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    structure_input = torch.cat((structure_input[:-<span class="number">1</span> * batch_size],</span><br><span class="line">                                 <span class="variable language_">self</span>.seq_linears[structure_layer_index](x[-<span class="number">2</span>, :, :])), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. ç”¨ç»“æ„æ¨¡å‹çš„ä¸€å±‚æ¥å¤„ç†</span></span><br><span class="line">hidden = <span class="variable language_">self</span>.structure_model.layers[structure_layer_index](new_graph, structure_input)</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.structure_model.short_cut <span class="keyword">and</span> hidden.shape == structure_input.shape:</span><br><span class="line">    hidden = hidden + structure_input</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.structure_model.batch_norm:</span><br><span class="line">    hidden = <span class="variable language_">self</span>.structure_model.batch_norms[structure_layer_index](hidden)</span><br><span class="line"></span><br><span class="line">structure_hiddens.append(hidden)</span><br><span class="line">structure_input = hidden</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. å°†æ›´æ–°åçš„ 3D ä»¤ç‰Œè¡¨ç¤ºæ’å›åºåˆ—</span></span><br><span class="line">updated_structure_token = <span class="variable language_">self</span>.structure_linears[...](structure_input[-<span class="number">1</span> * batch_size:])</span><br><span class="line">x = torch.cat((x[:-<span class="number">2</span>, :, :], updated_structure_token.unsqueeze(<span class="number">0</span>), x[-<span class="number">1</span>:, :, :]), dim=<span class="number">0</span>)</span><br><span class="line">structure_layer_index += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ä¿¡æ¯æµç¨‹</strong>ï¼š
<ol type="1">
<li><strong>ä»åºåˆ—åˆ°ç»“æ„</strong>ï¼šæ¨¡å‹ä»åºåˆ—è¡¨ç¤º <code>x</code>
ä¸­æå–å‡ºâ€œ3D
ä»¤ç‰Œâ€çš„æœ€æ–°å‘é‡ã€‚è¿™ä¸ªå‘é‡æ­¤æ—¶å·²ç»å¸æ”¶äº†å‰é¢å‡ å±‚åºåˆ—æ¨¡å‹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ç„¶åï¼Œé€šè¿‡ï¼ˆ<code>seq_linears</code>ï¼‰å°†å…¶è½¬æ¢åï¼Œæ›´æ–°åˆ°ç»“æ„æ¨¡å‹çš„è¾“å…¥ä¸­ã€‚</li>
<li><strong>ç»“æ„ä¿¡æ¯å¤„ç†</strong>ï¼šè¿è¡Œä¸€å±‚ç»“æ„æ¨¡å‹ï¼ˆGNNï¼‰ã€‚GNN
æ ¹æ®å›¾çš„è¿æ¥å…³ç³»æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹çš„è¡¨ç¤ºï¼Œå½“ç„¶ä¹ŸåŒ…æ‹¬â€œ3D
ä»¤ç‰Œâ€è¿™ä¸ªç‰¹æ®ŠèŠ‚ç‚¹ã€‚</li>
<li><strong>ä»ç»“æ„åˆ°åºåˆ—</strong>ï¼šä» GNN çš„è¾“å‡ºä¸­ï¼Œå†æ¬¡æå–å‡ºâ€œ3D
ä»¤ç‰Œâ€çš„å‘é‡ã€‚è¿™ä¸ªå‘é‡åŒ…å«æ›´æ–°åçš„ç»“æ„ä¿¡æ¯ã€‚å†é€šè¿‡ï¼ˆ<code>structure_linears</code>ï¼‰è½¬æ¢åï¼ŒæŠŠå®ƒ<strong>æ’å›</strong>åˆ°åºåˆ—è¡¨ç¤º
<code>x</code> ä¸­ï¼Œæ›¿æ¢æ‰æ—§çš„ç‰ˆæœ¬ã€‚</li>
</ol></li>
</ul>
<p>è¿™ä¸ªå¾ªç¯ä¸æ–­é‡å¤ã€‚</p>
<h5 id="è¾“å‡º"><strong>è¾“å‡º</strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Structural Output</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.structure_model.concat_hidden:</span><br><span class="line">    structure_node_feature = torch.cat(structure_hiddens, dim=-<span class="number">1</span>)[:-<span class="number">1</span> * batch_size]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    structure_node_feature = structure_hiddens[-<span class="number">1</span>][:-<span class="number">1</span> * batch_size]</span><br><span class="line"></span><br><span class="line">structure_graph_feature = <span class="variable language_">self</span>.structure_model.readout(graph, structure_node_feature)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sequence Output</span></span><br><span class="line">x = <span class="variable language_">self</span>.sequence_model.model.emb_layer_norm_after(x)</span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># (T, B, E) =&gt; (B, T, E)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># last hidden representation should have layer norm applied</span></span><br><span class="line"><span class="keyword">if</span> (seq_layer_idx + <span class="number">1</span>) <span class="keyword">in</span> repr_layers:</span><br><span class="line">    hidden_representations[seq_layer_idx + <span class="number">1</span>] = x</span><br><span class="line">x = <span class="variable language_">self</span>.sequence_model.model.lm_head(x)</span><br><span class="line"></span><br><span class="line">output = &#123;<span class="string">&quot;logits&quot;</span>: x, <span class="string">&quot;representations&quot;</span>: hidden_representations&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sequence (ESM) model outputs</span></span><br><span class="line">residue_feature = output[<span class="string">&quot;representations&quot;</span>][<span class="variable language_">self</span>.sequence_model.repr_layer]</span><br><span class="line">residue_feature = functional.padded_to_variadic(residue_feature, size_ext)</span><br><span class="line">starts = size_ext.cumsum(<span class="number">0</span>) - size_ext</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.alphabet.prepend_bos:</span><br><span class="line">    starts = starts + <span class="number">1</span></span><br><span class="line">ends = starts + size</span><br><span class="line">mask = functional.multi_slice_mask(starts, ends, <span class="built_in">len</span>(residue_feature))</span><br><span class="line">residue_feature = residue_feature[mask]</span><br><span class="line">graph_feature = <span class="variable language_">self</span>.sequence_model.readout(graph, residue_feature)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine both models outputs</span></span><br><span class="line">node_feature = torch.cat(...)</span><br><span class="line">graph_feature = torch.cat(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> &#123;<span class="string">&quot;graph_feature&quot;</span>: graph_feature, <span class="string">&quot;node_feature&quot;</span>: node_feature&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>æå–è¾“å‡º</strong>ï¼šå¾ªç¯ç»“æŸåï¼Œåˆ†åˆ«ä»ä¸¤ä¸ªæ¨¡å‹ä¸­æå–æœ€ç»ˆçš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li><strong>è¯»å‡ºï¼ˆReadoutï¼‰</strong>ï¼šä½¿ç”¨ä¸€ä¸ªâ€œè¯»å‡ºå‡½æ•°â€ï¼ˆå¦‚æ±‚å’Œæˆ–å¹³å‡ï¼‰å°†èŠ‚ç‚¹çº§åˆ«çš„ç‰¹å¾èšåˆæˆä¸€ä¸ªä»£è¡¨æ•´ä¸ªè›‹ç™½è´¨çš„å›¾çº§åˆ«ç‰¹å¾ã€‚</li>
<li><strong>æœ€ç»ˆç»„åˆ</strong>ï¼šå°†æ¥è‡ªåºåˆ—æ¨¡å‹å’Œç»“æ„æ¨¡å‹çš„èŠ‚ç‚¹ç‰¹å¾ï¼ˆ<code>node_feature</code>ï¼‰å’Œå›¾ç‰¹å¾ï¼ˆ<code>graph_feature</code>ï¼‰åˆ†åˆ«æ‹¼æ¥ï¼ˆconcatenateï¼‰èµ·æ¥ã€‚</li>
<li><strong>è¿”å›ç»“æœ</strong>ï¼šè¿”å›ä¸€ä¸ªåŒ…å«ç»„åˆåç‰¹å¾çš„å­—å…¸ï¼Œå¯ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åŠŸèƒ½é¢„æµ‹ã€å±æ€§å›å½’ç­‰ï¼‰ã€‚</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/26/5120C4-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/26/5120C4-2/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>
              

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-26 21:00:00 / ä¿®æ”¹æ—¶é—´ï¼š20:48:45" itemprop="dateCreated datePublished" datetime="2025-09-26T21:00:00+08:00">2025-09-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - è®¡ç®—èƒ½æºææ–™å’Œç”µå­ç»“æ„æ¨¡æ‹Ÿ Lecture-4</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="monte-carlo-mc-method">1 Monte Carlo (MC) Method:</h2>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>This whiteboard provides a concise but detailed overview of two
important and related simulation techniques in computational physics and
chemistry: the Metropolis Monte Carlo (MC) method and Hamiltonian (or
Hybrid) Monte Carlo (HMC). Here is a detailed breakdown of the concepts
presented.</p>
<h3 id="metropolis-monte-carlo-mc-method">1. Metropolis Monte Carlo (MC)
Method</h3>
<p>The heading â€œMetropolis MC methodâ€ introduces a foundational
algorithm in statistical mechanics. Metropolis Monte Carlo is a method
used to generate a sequence of states for a system, allowing for the
calculation of average properties. å·¦ä¸Šè§’çš„è¿™ä¸€éƒ¨åˆ†ä»‹ç»äº†åŸºç¡€çš„
<strong>Metropolis Monte Carlo</strong>
ç®—æ³•ã€‚å®ƒæ˜¯ä¸€ç§ç”ŸæˆçŠ¶æ€åºåˆ—çš„æ–¹æ³•ï¼Œä½¿å¾—å¤„äºä»»ä½•çŠ¶æ€çš„æ¦‚ç‡éƒ½ç¬¦åˆæœŸæœ›çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆåœ¨ç‰©ç†å­¦ä¸­é€šå¸¸æ˜¯ç»å°”å…¹æ›¼åˆ†å¸ƒï¼‰ã€‚</p>
<ul>
<li><strong>Conceptual Diagram:</strong> The small box with numbered
sites (0-5) and an arrow showing a move from state 0 to 2, and then to
3, illustrates a â€œrandom walk.â€ In Metropolis MC, the system transitions
from one state to another by making small, random changes.
å°æ–¹æ¡†ä¸­æ ‡æœ‰ç¼–å·çš„ä½ç‚¹ï¼ˆ0-5ï¼‰ï¼Œç®­å¤´è¡¨ç¤ºä»çŠ¶æ€ 0 åˆ°çŠ¶æ€ 2ï¼Œå†åˆ°çŠ¶æ€ 3
çš„ç§»åŠ¨ï¼Œä»£è¡¨â€œéšæœºæ¸¸èµ°â€ã€‚åœ¨ Metropolis MC
ä¸­ï¼Œç³»ç»Ÿé€šè¿‡è¿›è¡Œå¾®å°çš„éšæœºå˜åŒ–ä»ä¸€ä¸ªçŠ¶æ€è¿‡æ¸¡åˆ°å¦ä¸€ä¸ªçŠ¶æ€ã€‚</li>
<li><strong>Random Number Generation:</strong> The notation
<code>rand t \in (0,1)</code> indicates the use of a random number <span
class="math inline">\(t\)</span> drawn from a uniform distribution
between 0 and 1. This is a core component of the algorithm, used to
decide whether to accept or reject a proposed new state. ç¬¦å·
<code>rand t \in (0,1)</code> è¡¨ç¤ºä½¿ç”¨ä» 0 åˆ° 1
ä¹‹é—´çš„å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–çš„éšæœºæ•° <span
class="math inline">\(t\)</span>ã€‚è¿™æ˜¯ç®—æ³•çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œç”¨äºå†³å®šæ˜¯å¦æ¥å—æˆ–æ‹’ç»æè®®çš„æ–°çŠ¶æ€ã€‚</li>
<li><strong>Detailed Balance Condition:</strong> The equation <span
class="math inline">\(P_o T(o \to n) = P_n T(n \to o)\)</span> is the
principle of detailed balance. It states that in a system at
equilibrium, the probability of being in an old state (<span
class="math inline">\(o\)</span>) and transitioning to a new state
(<span class="math inline">\(n\)</span>) is equal to the probability of
being in the new state and transitioning back to the old one. This
condition is crucial because it ensures that the simulation will
eventually sample states according to their correct thermodynamic
probabilities (the Boltzmann distribution). æ–¹ç¨‹ <span
class="math inline">\(P_o T(o \to n) = P_n T(n \to o)\)</span>
æ˜¯è¯¦ç»†å¹³è¡¡çš„åŸç†ã€‚å®ƒæŒ‡å‡ºï¼Œåœ¨å¹³è¡¡ç³»ç»Ÿä¸­ï¼Œå¤„äºæ—§çŠ¶æ€ (<span
class="math inline">\(o\)</span>) å¹¶è½¬å˜ä¸ºæ–°çŠ¶æ€ (<span
class="math inline">\(n\)</span>)
çš„æ¦‚ç‡ç­‰äºå¤„äºæ–°çŠ¶æ€å¹¶è½¬å˜å›æ—§çŠ¶æ€çš„æ¦‚ç‡ã€‚æ­¤æ¡ä»¶è‡³å…³â€‹â€‹é‡è¦ï¼Œå› ä¸ºå®ƒç¡®ä¿æ¨¡æ‹Ÿæœ€ç»ˆå°†æ ¹æ®æ­£ç¡®çš„çƒ­åŠ›å­¦æ¦‚ç‡ï¼ˆç»å°”å…¹æ›¼åˆ†å¸ƒï¼‰å¯¹çŠ¶æ€è¿›è¡Œé‡‡æ ·ã€‚</li>
<li><strong>Acceptance Rate:</strong> The note <code>\sim 30\%?</code>
likely refers to the target <strong>acceptance rate</strong> for an
efficient Metropolis MC simulation. If new states are accepted too often
or too rarely, the exploration of the systemâ€™s possible configurations
is inefficient. While the famous optimal acceptance rate for certain
high-dimensional problems is around 23.4%, a range of 20-50% is often
considered effective. æ³¨é‡Šâ€œ30%ï¼Ÿâ€æŒ‡çš„æ˜¯é«˜æ•ˆ Metropolis
è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿçš„ç›®æ ‡<strong>æ¥å—ç‡</strong>ã€‚å¦‚æœæ–°çŠ¶æ€æ¥å—è¿‡äºé¢‘ç¹æˆ–è¿‡äºç¨€å°‘ï¼Œç³»ç»Ÿå¯¹å¯èƒ½é…ç½®çš„æ¢ç´¢å°±ä¼šå˜å¾—ä½æ•ˆã€‚è™½ç„¶æŸäº›é«˜ç»´é—®é¢˜çš„æœ€ä½³æ¥å—ç‡çº¦ä¸º
23.4%ï¼Œä½†é€šå¸¸è®¤ä¸º 20-50% çš„èŒƒå›´æ˜¯æœ‰æ•ˆçš„ã€‚</li>
</ul>
<h3 id="hamiltonian-hybrid-monte-carlo-hmc">2. Hamiltonian / Hybrid
Monte Carlo (HMC)</h3>
<p>The second topic, â€œHamiltonian/Hybrid MC (HMC),â€ is a more advanced
Monte Carlo method that uses principles from classical mechanics to
propose new states more intelligently than the simple random-walk
approach of the standard Metropolis method. This often leads to a much
higher acceptance rate and more efficient exploration of the state
space. ç¬¬äºŒä¸ªä¸»é¢˜â€œå“ˆå¯†é¡¿/æ··åˆè’™ç‰¹å¡ç½—
(HMC)â€æ˜¯ä¸€ç§æ›´å…ˆè¿›çš„è’™ç‰¹å¡ç½—æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ç»å…¸åŠ›å­¦åŸç†ï¼Œæ¯”æ ‡å‡† Metropolis
æ–¹æ³•ä¸­ç®€å•çš„éšæœºæ¸¸èµ°æ–¹æ³•æ›´æ™ºèƒ½åœ°æå‡ºæ–°çŠ¶æ€ã€‚è¿™é€šå¸¸ä¼šå¸¦æ¥æ›´é«˜çš„æ¥å—ç‡å’Œæ›´é«˜æ•ˆçš„çŠ¶æ€ç©ºé—´æ¢ç´¢ã€‚</p>
<p>The whiteboard outlines a four-step HMC algorithm:</p>
<p><strong>Step 1: Randomize Velocities</strong> The first step is to
randomize the velocities: <span class="math inline">\(\vec{v}_i \sim
\mathcal{N}(0, k_B T)\)</span>. ç¬¬ä¸€æ­¥æ˜¯éšæœºåŒ–é€Ÿåº¦ï¼š<span
class="math inline">\(\vec{v}_i \sim \mathcal{N}(0, k_B T)\)</span>ã€‚ *
This step introduces momentum into the system. For each particle <span
class="math inline">\(i\)</span>, a velocity vector <span
class="math inline">\(\vec{v}_i\)</span> is randomly drawn from a normal
(Gaussian) distribution with a mean of 0 and a variance related to the
temperature <span class="math inline">\(T\)</span> and the Boltzmann
constant <span class="math inline">\(k_B\)</span>.
æ­¤æ­¥éª¤å°†åŠ¨é‡å¼•å…¥ç³»ç»Ÿã€‚å¯¹äºæ¯ä¸ªç²’å­ <span
class="math inline">\(i\)</span>ï¼Œé€Ÿåº¦çŸ¢é‡ <span
class="math inline">\(\vec{v}_i\)</span>
ä¼šéšæœºåœ°ä»æ­£æ€ï¼ˆé«˜æ–¯ï¼‰åˆ†å¸ƒä¸­æŠ½å–ï¼Œè¯¥åˆ†å¸ƒçš„å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸æ¸©åº¦ <span
class="math inline">\(T\)</span> å’Œç»å°”å…¹æ›¼å¸¸æ•° <span
class="math inline">\(k_B\)</span> ç›¸å…³ã€‚ * The full formula for this
probability distribution, <span
class="math inline">\(f(\vec{v})\)</span>, is the
<strong>Maxwell-Boltzmann distribution</strong>, which is written out
further down the board. è¯¥æ¦‚ç‡åˆ†å¸ƒçš„å®Œæ•´å…¬å¼ <span
class="math inline">\(f(\vec{v})\)</span>
æ˜¯<strong>éº¦å…‹æ–¯éŸ¦-ç»å°”å…¹æ›¼åˆ†å¸ƒ</strong>ã€‚</p>
<p><strong>Step 2: Molecular Dynamics (MD) Integration</strong> The
board notes this as <code>t=0 \to h \text&#123; or &#125; mh</code>
<code>MD</code> and mentions the <code>Verlet</code> algorithm.</p>
<ul>
<li>This is the â€œHamiltonian dynamicsâ€ part of the algorithm. Starting
from the current positions and the newly randomized velocities, the
systemâ€™s trajectory is calculated for a short period of time (<span
class="math inline">\(h\)</span> or <span
class="math inline">\(mh\)</span>) using Molecular Dynamics (MD).
è¿™æ˜¯ç®—æ³•çš„â€œå“ˆå¯†é¡¿åŠ¨åŠ›å­¦â€éƒ¨åˆ†ã€‚ä»å½“å‰ä½ç½®å’Œæ–°éšæœºåŒ–çš„é€Ÿåº¦å¼€å§‹ï¼Œä½¿ç”¨åˆ†å­åŠ¨åŠ›å­¦
(MD) è®¡ç®—ç³»ç»Ÿåœ¨çŸ­æ—¶é—´å†…ï¼ˆ<span class="math inline">\(h\)</span> æˆ– <span
class="math inline">\(mh\)</span>ï¼‰çš„è½¨è¿¹ã€‚</li>
<li>The name <strong>Verlet</strong> refers to the Verlet integration
algorithm, a numerical method used to solve Newtonâ€™s equations of
motion. It is popular in MD simulations because it is time-reversible
and conserves energy well over long simulations. æŒ‡çš„æ˜¯ Verlet
ç§¯åˆ†ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ±‚è§£ç‰›é¡¿è¿åŠ¨æ–¹ç¨‹çš„æ•°å€¼æ–¹æ³•ã€‚å®ƒåœ¨ MD
æ¨¡æ‹Ÿä¸­å¾ˆå—æ¬¢è¿ï¼Œå› ä¸ºå®ƒå…·æœ‰æ—¶é—´å¯é€†æ€§ï¼Œå¹¶ä¸”åœ¨é•¿æ—¶é—´æ¨¡æ‹Ÿä¸­èƒ½é‡å®ˆæ’æ•ˆæœè‰¯å¥½ã€‚</li>
</ul>
<p><strong>Step 3: Calculate Total Energy</strong> The third step is to
<code>calculate total energy</code>: <span class="math inline">\(E_n =
K_n + V_n\)</span>. ç¬¬ä¸‰æ­¥æ˜¯â€œè®¡ç®—æ€»èƒ½é‡â€ï¼š<span
class="math inline">\(E_n = K_n + V_n\)</span>ã€‚ * After the MD
trajectory, the system is in a new state <span
class="math inline">\(n\)</span>. The total energy of this new state,
<span class="math inline">\(E_n\)</span>, is calculated as the sum of
its kinetic energy (<span class="math inline">\(K_n\)</span>, from the
velocities) and its potential energy (<span
class="math inline">\(V_n\)</span>, from the positions). MD
è½¨è¿¹ä¹‹åï¼Œç³»ç»Ÿå¤„äºæ–°çŠ¶æ€ <span
class="math inline">\(n\)</span>ã€‚æ–°çŠ¶æ€çš„æ€»èƒ½é‡ <span
class="math inline">\(E_n\)</span> ç­‰äºå…¶åŠ¨èƒ½ (<span
class="math inline">\(K_n\)</span>ï¼Œç”±é€Ÿåº¦è®¡ç®—å¾—å‡ºï¼‰å’ŒåŠ¿èƒ½ (<span
class="math inline">\(V_n\)</span>ï¼Œç”±ä½ç½®è®¡ç®—å¾—å‡º)ä¹‹å’Œã€‚</p>
<p><strong>Step 4: Acceptance Test</strong> The final step is the
acceptance criterion: <span class="math inline">\(\text{acc}(o \to n) =
\min(1, e^{-\beta(E_n - E_o)})\)</span>. æœ€åä¸€æ­¥æ˜¯éªŒæ”¶æ ‡å‡†ï¼š<span
class="math inline">\(\text{acc}(o \to n) = \min(1, e^{-\beta(E_n -
E_o)})\)</span>ã€‚ * This is the Metropolis acceptance criterion. The
algorithm decides whether to accept the new state <span
class="math inline">\(n\)</span> or reject it and stay in the old state
<span class="math inline">\(o\)</span>. è¿™æ˜¯ Metropolis
éªŒæ”¶æ ‡å‡†ã€‚ç®—æ³•å†³å®šæ˜¯æ¥å—æ–°çŠ¶æ€ <span class="math inline">\(n\)</span>
è¿˜æ˜¯æ‹’ç»å®ƒå¹¶ä¿æŒæ—§çŠ¶æ€ <span class="math inline">\(o\)</span>ã€‚ * The
probability of acceptance depends on the change in total energy (<span
class="math inline">\(E_n - E_o\)</span>). If the new energy is lower,
the move is always accepted. If the new energy is higher, it might still
be accepted with a probability <span class="math inline">\(e^{-\beta(E_n
- E_o)}\)</span>, where <span class="math inline">\(\beta = 1/(k_B
T)\)</span>. This allows the system to escape from local energy minima.
éªŒæ”¶æ¦‚ç‡å–å†³äºæ€»èƒ½é‡çš„å˜åŒ– (<span class="math inline">\(E_n -
E_o\)</span>)ã€‚å¦‚æœæ–°èƒ½é‡è¾ƒä½ï¼Œåˆ™å§‹ç»ˆæ¥å—è¯¥ç§»åŠ¨ã€‚å¦‚æœæ–°çš„èƒ½é‡æ›´é«˜ï¼Œå®ƒä»ç„¶å¯èƒ½ä»¥æ¦‚ç‡
<span class="math inline">\(e^{-\beta(E_n - E_o)}\)</span> è¢«æ¥å—ï¼Œå…¶ä¸­
<span class="math inline">\(\beta = 1/(k_B
T)\)</span>ã€‚è¿™ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿæ‘†è„±å±€éƒ¨èƒ½é‡æœ€å°å€¼ã€‚</p>
<h3 id="key-formulas-and-notations">Key Formulas and Notations</h3>
<ul>
<li><p><strong>Maxwell-Boltzmann
Distributionéº¦å…‹æ–¯éŸ¦-ç»å°”å…¹æ›¼åˆ†å¸ƒ:</strong> The formula for the velocity
distribution is given as: <span class="math inline">\(f(\vec{v}) =
\left(\frac{m}{2\pi k_B T}\right)^{3/2} \exp\left(-\frac{m v^2}{2 k_B
T}\right)\)</span> This gives the probability density for a particle of
mass <span class="math inline">\(m\)</span> to have a velocity <span
class="math inline">\(\vec{v}\)</span> at a given temperature <span
class="math inline">\(T\)</span>.è´¨é‡ä¸º <span
class="math inline">\(m\)</span> çš„ç²’å­é€Ÿåº¦ä¸º çš„æ¦‚ç‡å¯†åº¦</p></li>
<li><p><strong>Energy Conservation and Acceptance Rate:</strong> The
notes <span class="math inline">\(E_n \approx E_o\)</span> and <span
class="math inline">\(75\%\)</span> highlight a key advantage of HMC.
Because the Verlet integrator approximately conserves energy, the final
energy <span class="math inline">\(E_n\)</span> after the MD trajectory
is usually very close to the initial energy <span
class="math inline">\(E_o\)</span>. This means the term <span
class="math inline">\((E_n - E_o)\)</span> is small, and the acceptance
probability is high. The <span class="math inline">\(75\%\)</span>
indicates a typical or target acceptance rate for HMC, which is
significantly higher than for standard Metropolis MC. æ³¨é‡Š <span
class="math inline">\(E_n \approx E_o\)</span> å’Œ <span
class="math inline">\(75\%\)</span> å‡¸æ˜¾äº† HMC çš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿ã€‚ç”±äº
Verlet ç§¯åˆ†å™¨è¿‘ä¼¼åœ°å®ˆæ’èƒ½é‡ï¼ŒMD è½¨è¿¹åçš„æœ€ç»ˆèƒ½é‡ <span
class="math inline">\(E_n\)</span> é€šå¸¸éå¸¸æ¥è¿‘åˆå§‹èƒ½é‡ <span
class="math inline">\(E_o\)</span>ã€‚è¿™æ„å‘³ç€ <span
class="math inline">\((E_n - E_o)\)</span> é¡¹å¾ˆå°ï¼Œæ¥å—æ¦‚ç‡å¾ˆé«˜ã€‚<span
class="math inline">\(75\%\)</span> è¡¨ç¤º HMC
çš„å…¸å‹æˆ–ç›®æ ‡æ¥å—ç‡ï¼Œæ˜æ˜¾é«˜äºæ ‡å‡† Metropolis MCã€‚</p></li>
<li><p><strong>Hamiltonian Operator:</strong> The symbol <span
class="math inline">\(\hat{H}\)</span> written on the adjacent board
represents the Hamiltonian operator, which gives the total energy of the
system. The note <code>Î” Adiabatic</code> suggests that the MD evolution
is ideally an adiabatic process (no heat exchange), during which the
total energy (the Hamiltonian) is conserved. ç›¸é‚»æ¿ä¸Šçš„ç¬¦å· <span
class="math inline">\(\hat{H}\)</span>
ä»£è¡¨å“ˆå¯†é¡¿ç®—ç¬¦ï¼Œå®ƒç»™å‡ºäº†ç³»ç»Ÿçš„æ€»èƒ½é‡ã€‚æ³¨é‡Šâ€œÎ” Adiabaticâ€è¡¨æ˜ MD
æ¼”åŒ–åœ¨ç†æƒ³æƒ…å†µä¸‹æ˜¯ä¸€ä¸ªç»çƒ­è¿‡ç¨‹ï¼ˆæ— çƒ­äº¤æ¢ï¼‰ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­æ€»èƒ½é‡ï¼ˆå“ˆå¯†é¡¿é‡ï¼‰å®ˆæ’ã€‚</p></li>
</ul>
<p>This whiteboard displays the fundamental equation of quantum
chemistry: the time-dependent SchrÃ¶dinger equation, along with the
detailed breakdown of the molecular Hamiltonian operator. This equation
is the starting point for almost all <em>ab initio</em>
(first-principles) quantum mechanical calculations of molecular systems.
è¿™å—ç™½æ¿å±•ç¤ºäº†é‡å­åŒ–å­¦çš„åŸºæœ¬æ–¹ç¨‹ï¼šå«æ—¶è–›å®šè°”æ–¹ç¨‹ï¼Œä»¥åŠåˆ†å­å“ˆå¯†é¡¿ç®—ç¬¦çš„è¯¦ç»†åˆ†è§£ã€‚è¯¥æ–¹ç¨‹æ˜¯å‡ ä¹æ‰€æœ‰åˆ†å­ç³»ç»Ÿ<em>ä»å¤´ç®—</em>ï¼ˆç¬¬ä¸€æ€§åŸç†ï¼‰é‡å­åŠ›å­¦è®¡ç®—çš„èµ·ç‚¹ã€‚</p>
<h3 id="the-time-dependent-schrÃ¶dinger-equation">3. The Time-Dependent
SchrÃ¶dinger Equation</h3>
<p>At the top of the board, the fundamental equation governing the
evolution of a quantum mechanical system is presented:
ç™½æ¿é¡¶éƒ¨æ˜¾ç¤ºäº†æ§åˆ¶é‡å­åŠ›å­¦ç³»ç»Ÿæ¼”åŒ–çš„åŸºæœ¬æ–¹ç¨‹ï¼š <span
class="math inline">\(i\hbar \frac{\partial \Psi}{\partial t} =
\hat{\mathcal{H}} \Psi\)</span></p>
<ul>
<li><p><strong><span class="math inline">\(\Psi\)</span> (Psi)</strong>
is the <strong>wave function</strong> of the system. It contains all the
information that can be known about the system (e.g., the positions and
momenta of all particles).
æ˜¯ç³»ç»Ÿçš„<strong>æ³¢å‡½æ•°</strong>ã€‚å®ƒåŒ…å«äº†å…³äºç³»ç»Ÿçš„æ‰€æœ‰å·²çŸ¥ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œæ‰€æœ‰ç²’å­çš„ä½ç½®å’ŒåŠ¨é‡ï¼‰ã€‚</p></li>
<li><p><strong><span
class="math inline">\(\hat{\mathcal{H}}\)</span></strong> is the
<strong>Hamiltonian operator</strong>, which represents the total energy
of the system.
æ˜¯<strong>å“ˆå¯†é¡¿ç®—ç¬¦</strong>ï¼Œè¡¨ç¤ºç³»ç»Ÿçš„æ€»èƒ½é‡ã€‚</p></li>
<li><p><strong><span class="math inline">\(i\)</span></strong>
æ˜¯è™šæ•°å•ä½ã€‚</p></li>
<li><p><strong><span class="math inline">\(i\)</span></strong> is the
imaginary unit.</p></li>
<li><p><strong><span class="math inline">\(\hbar\)</span></strong> is
the <strong>reduced Planck
constant</strong>.æ˜¯<strong>çº¦åŒ–æ™®æœ—å…‹å¸¸æ•°</strong>ã€‚</p></li>
<li><p><strong><span class="math inline">\(\frac{\partial \Psi}{\partial
t}\)</span></strong> represents how the wave function changes over
time.è¡¨ç¤ºæ³¢å‡½æ•°éšæ—¶é—´çš„å˜åŒ–ã€‚</p></li>
</ul>
<p>This equation states that the time evolution of the quantum state is
dictated by the systemâ€™s total energy operator, the Hamiltonian. The
note â€œÎ” Adiabatic processâ€ likely connects to the context of the
Born-Oppenheimer approximation, where the electronic SchrÃ¶dinger
equation is solved for fixed nuclear positions, assuming the electrons
adjust adiabatically (instantaneously) to the motion of the nuclei.
è¯¥æ–¹ç¨‹è¡¨æ˜ï¼Œé‡å­æ€çš„æ—¶é—´æ¼”åŒ–ç”±ç³»ç»Ÿçš„æ€»èƒ½é‡ç®—ç¬¦â€”â€”å“ˆå¯†é¡¿ç®—ç¬¦å†³å®šã€‚æ³¨é‡Šâ€œÎ”ç»çƒ­è¿‡ç¨‹â€ä¸ç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼ç›¸å…³ï¼Œåœ¨è¯¥è¿‘ä¼¼ä¸­ï¼Œç”µå­è–›å®šè°”æ–¹ç¨‹æ˜¯é’ˆå¯¹å›ºå®šåŸå­æ ¸ä½ç½®æ±‚è§£çš„ï¼Œå‡è®¾ç”µå­ä»¥ç»çƒ­æ–¹å¼ï¼ˆç¬æ—¶ï¼‰è°ƒæ•´ä»¥é€‚åº”åŸå­æ ¸çš„è¿åŠ¨ã€‚</p>
<h3 id="the-full-molecular-hamiltonian-hatmathcalh">4. The Full
Molecular Hamiltonian (<span
class="math inline">\(\hat{\mathcal{H}}\)</span>)</h3>
<p>The main part of the whiteboard is the detailed expression for the
non-relativistic, time-independent molecular Hamiltonian. It is the sum
of the kinetic and potential energies of all the nuclei and electrons in
the system. The equation can be broken down into five distinct terms:
ç™½æ¿çš„ä¸»è¦éƒ¨åˆ†æ˜¯éç›¸å¯¹è®ºæ€§ã€æ—¶é—´æ— å…³çš„åˆ†å­å“ˆå¯†é¡¿é‡çš„è¯¦ç»†è¡¨è¾¾å¼ã€‚å®ƒæ˜¯ç³»ç»Ÿä¸­æ‰€æœ‰åŸå­æ ¸å’Œç”µå­çš„åŠ¨èƒ½å’ŒåŠ¿èƒ½ä¹‹å’Œã€‚</p>
<p>è¯¥æ–¹ç¨‹å¯ä»¥åˆ†è§£ä¸ºäº”ä¸ªä¸åŒçš„é¡¹ï¼š</p>
<p><span class="math inline">\(\hat{\mathcal{H}} = -\sum_{I=1}^{P}
\frac{\hbar^2}{2M_I}\nabla_I^2 - \sum_{i=1}^{N}
\frac{\hbar^2}{2m}\nabla_i^2 + \frac{e^2}{2}\sum_{I=1}^{P}\sum_{J \neq
I}^{P} \frac{Z_I Z_J}{|\vec{R}_I - \vec{R}_J|} +
\frac{e^2}{2}\sum_{i=1}^{N}\sum_{j \neq i}^{N} \frac{1}{|\vec{r}_i -
\vec{r}_j|} - e^2\sum_{I=1}^{P}\sum_{i=1}^{N} \frac{Z_I}{|\vec{R}_I -
\vec{r}_i|}\)</span></p>
<p>Letâ€™s analyze each component:</p>
<p><strong>A. Kinetic Energy Terms åŠ¨èƒ½é¡¹</strong></p>
<ol type="1">
<li><strong>Kinetic Energy of the Nuclei åŸå­æ ¸çš„åŠ¨èƒ½:</strong> <span
class="math inline">\(-\sum_{I=1}^{P}
\frac{\hbar^2}{2M_I}\nabla_I^2\)</span> This term is the sum of the
kinetic energy operators for all the nuclei in the
system.æ­¤é¡¹æ˜¯ç³»ç»Ÿä¸­æ‰€æœ‰åŸå­æ ¸çš„åŠ¨èƒ½ç®—ç¬¦ä¹‹å’Œã€‚
<ul>
<li>The sum is over all nuclei, indexed by <span
class="math inline">\(I\)</span> from 1 to <span
class="math inline">\(P\)</span>.è¯¥å’Œæ¶µç›–æ‰€æœ‰åŸå­æ ¸ï¼Œç´¢å¼•ä¸º <span
class="math inline">\(I\)</span>ï¼Œä» 1 åˆ° <span
class="math inline">\(P\)</span>ã€‚</li>
<li><span class="math inline">\(M_I\)</span> is the mass of nucleus
<span class="math inline">\(I\)</span>.æ˜¯åŸå­æ ¸ <span
class="math inline">\(I\)</span> çš„è´¨é‡ã€‚</li>
<li><span class="math inline">\(\nabla_I^2\)</span> is the Laplacian
operator, which involves the second spatial derivatives with respect to
the coordinates of nucleus <span
class="math inline">\(I\)</span>.æ˜¯æ‹‰æ™®æ‹‰æ–¯ç®—ç¬¦ï¼Œå®ƒæ¶‰åŠåŸå­æ ¸ <span
class="math inline">\(I\)</span> åæ ‡çš„äºŒé˜¶ç©ºé—´å¯¼æ•°ã€‚</li>
</ul></li>
<li><strong>Kinetic Energy of the Electrons ç”µå­çš„åŠ¨èƒ½:</strong> <span
class="math inline">\(-\sum_{i=1}^{N}
\frac{\hbar^2}{2m}\nabla_i^2\)</span> This is the corresponding sum of
the kinetic energy operators for all the
electrons.è¿™æ˜¯æ‰€æœ‰ç”µå­çš„åŠ¨èƒ½ç®—ç¬¦çš„å¯¹åº”å’Œã€‚
<ul>
<li>The sum is over all electrons, indexed by <span
class="math inline">\(i\)</span> from 1 to <span
class="math inline">\(N\)</span>.è¯¥å’Œæ˜¯é’ˆå¯¹æ‰€æœ‰ç”µå­çš„ï¼Œç´¢å¼•ä¸º <span
class="math inline">\(i\)</span>ï¼Œä» 1 åˆ° <span
class="math inline">\(N\)</span>ã€‚</li>
<li><span class="math inline">\(m\)</span> is the mass of an
electron.æ˜¯ç”µå­çš„è´¨é‡ã€‚</li>
<li><span class="math inline">\(\nabla_i^2\)</span> is the Laplacian
operator with respect to the coordinates of electron <span
class="math inline">\(i\)</span>.æ˜¯å…³äºç”µå­ <span
class="math inline">\(i\)</span> åæ ‡çš„æ‹‰æ™®æ‹‰æ–¯ç®—ç¬¦ã€‚</li>
</ul></li>
</ol>
<p><strong>B. Potential Energy Terms (Electrostatic Interactions)
åŠ¿èƒ½é¡¹ï¼ˆé™ç”µç›¸äº’ä½œç”¨ï¼‰</strong></p>
<ol start="3" type="1">
<li><strong>Nuclear-Nuclear Repulsion æ ¸é—´æ’æ–¥åŠ›:</strong> <span
class="math inline">\(+\frac{e^2}{2}\sum_{I=1}^{P}\sum_{J \neq I}^{P}
\frac{Z_I Z_J}{|\vec{R}_I - \vec{R}_J|}\)</span> This term represents
the potential energy from the electrostatic (Coulomb) repulsion between
all pairs of positively charged
nuclei.è¯¥é¡¹è¡¨ç¤ºæ‰€æœ‰å¸¦æ­£ç”µåŸå­æ ¸å¯¹ä¹‹é—´é™ç”µï¼ˆåº“ä»‘ï¼‰æ’æ–¥åŠ›äº§ç”Ÿçš„åŠ¿èƒ½ã€‚
<ul>
<li>The double summation runs over all unique pairs of nuclei (<span
class="math inline">\(I, J\)</span>).å¯¹æ‰€æœ‰å”¯ä¸€çš„åŸå­æ ¸å¯¹ (<span
class="math inline">\(I, J\)</span>) è¿›è¡ŒåŒé‡æ±‚å’Œã€‚</li>
<li><span class="math inline">\(Z_I\)</span> is the atomic number (i.e.,
the charge) of nucleus <span class="math inline">\(I\)</span>.æ˜¯åŸå­æ ¸
<span class="math inline">\(I\)</span> çš„åŸå­åºæ•°ï¼ˆå³ç”µè·ï¼‰ã€‚</li>
<li><span class="math inline">\(\vec{R}_I\)</span> is the position
vector of nucleus <span class="math inline">\(I\)</span>.æ˜¯åŸå­æ ¸ <span
class="math inline">\(I\)</span> çš„ä½ç½®çŸ¢é‡ã€‚</li>
<li><span class="math inline">\(e\)</span> is the elementary
charge.æ˜¯åŸºæœ¬ç”µè·ã€‚</li>
</ul></li>
<li><strong>Electron-Electron Repulsion ç”µå­é—´æ’æ–¥åŠ›:</strong> <span
class="math inline">\(+\frac{e^2}{2}\sum_{i=1}^{N}\sum_{j \neq i}^{N}
\frac{1}{|\vec{r}_i - \vec{r}_j|}\)</span> This term represents the
potential energy from the electrostatic repulsion between all pairs of
negatively charged
electrons.è¯¥é¡¹è¡¨ç¤ºæ‰€æœ‰å¸¦è´Ÿç”µçš„ç”µå­å¯¹ä¹‹é—´é™ç”µæ’æ–¥çš„åŠ¿èƒ½ã€‚
<ul>
<li>The double summation runs over all unique pairs of electrons (<span
class="math inline">\(i, j\)</span>).å¯¹æ‰€æœ‰ä¸åŒçš„ç”µå­å¯¹ (<span
class="math inline">\(i, j\)</span>) è¿›è¡ŒåŒé‡æ±‚å’Œã€‚</li>
<li><span class="math inline">\(\vec{r}_i\)</span> is the position
vector of electron <span class="math inline">\(i\)</span>.æ˜¯ç”µå­ <span
class="math inline">\(i\)</span> çš„ä½ç½®çŸ¢é‡ã€‚</li>
</ul></li>
<li><strong>Nuclear-Electron Attraction æ ¸-ç”µå­å¼•åŠ›:</strong> <span
class="math inline">\(-e^2\sum_{I=1}^{P}\sum_{i=1}^{N}
\frac{Z_I}{|\vec{R}_I - \vec{r}_i|}\)</span> This final term represents
the potential energy from the electrostatic attraction between the
nuclei and the electrons.è¿™æœ€åä¸€é¡¹è¡¨ç¤ºåŸå­æ ¸å’Œç”µå­ä¹‹é—´é™ç”µå¼•åŠ›çš„åŠ¿èƒ½ã€‚
<ul>
<li>The summation runs over all nuclei and all
electrons.è¯¥æ±‚å’Œé€‚ç”¨äºæ‰€æœ‰åŸå­æ ¸å’Œæ‰€æœ‰ç”µå­ã€‚</li>
</ul></li>
</ol>
<h3 id="notations-and-conventions">5. Notations and Conventions</h3>
<ul>
<li><strong>Atomic Units:</strong> The note <span
class="math inline">\(\frac{1}{4\pi\epsilon_0} = k = 1\)</span> is a key
indicator of the convention being used. This sets the Coulomb constant
to 1, which is a hallmark of <strong>Hartree atomic units</strong>. In
this system, the elementary charge (<span
class="math inline">\(e\)</span>), electron mass (<span
class="math inline">\(m\)</span>), and reduced Planck constant (<span
class="math inline">\(\hbar\)</span>) are also set to 1. This simplifies
the Hamiltonian significantly, removing the physical constants and
making the equations easier to work with computationally.
æ˜¯æ‰€ç”¨çº¦å®šçš„å…³é”®æŒ‡æ ‡ã€‚è¿™å°†åº“ä»‘å¸¸æ•°è®¾ç½®ä¸º 1ï¼Œè¿™æ˜¯<strong>Hartree
åŸå­å•ä½</strong>çš„æ ‡å¿—ã€‚åœ¨è¿™ä¸ªç³»ç»Ÿä¸­ï¼ŒåŸºæœ¬ç”µè· (<span
class="math inline">\(e\)</span>)ã€ç”µå­è´¨é‡ (<span
class="math inline">\(m\)</span>) å’Œâ€‹â€‹çº¦åŒ–æ™®æœ—å…‹å¸¸æ•° (<span
class="math inline">\(\hbar\)</span>) ä¹Ÿè®¾ä¸º
1ã€‚è¿™æ˜¾è‘—ç®€åŒ–äº†å“ˆå¯†é¡¿é‡ï¼Œæ¶ˆé™¤äº†ç‰©ç†å¸¸æ•°ï¼Œä½¿æ–¹ç¨‹æ›´æ˜“äºè®¡ç®—ã€‚</li>
<li><strong>Interaction Terms:</strong> The notations <span
class="math inline">\(\{i, j\}\)</span>, <span
class="math inline">\(\{i, j, k\}\)</span>, etc., refer to the
â€œmany-bodyâ€ problem. The Hamiltonian contains two-body terms
(interactions between pairs of particles), and solving the SchrÃ¶dinger
equation exactly is extremely difficult because the motion of every
particle is correlated with every other particle. Computational methods
are designed to approximate these interactions. ç¬¦å· <span
class="math inline">\(\{i, j\}\)</span>ã€<span
class="math inline">\(\{i, j, k\}\)</span>
ç­‰æŒ‡çš„æ˜¯â€œå¤šä½“â€é—®é¢˜ã€‚å“ˆå¯†é¡¿é‡åŒ…å«äºŒä½“é¡¹ï¼ˆç²’å­å¯¹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼‰ï¼Œè€Œç²¾ç¡®æ±‚è§£è–›å®šè°”æ–¹ç¨‹æå…¶å›°éš¾ï¼Œå› ä¸ºæ¯ä¸ªç²’å­çš„è¿åŠ¨éƒ½ä¸å…¶ä»–ç²’å­ç›¸å…³ã€‚è®¡ç®—æ–¹æ³•æ—¨åœ¨è¿‘ä¼¼è¿™äº›ç›¸äº’ä½œç”¨ã€‚</li>
</ul>
<p>This whiteboard presents the mathematical foundation for
<strong>non-adiabatic molecular dynamics</strong>, a sophisticated
method in theoretical chemistry and physics used to simulate processes
where the Born-Oppenheimer approximation breaks down. This typically
occurs in photochemistry, electron transfer reactions, and when
molecules interact with intense laser fields.
è¿™å—ç™½æ¿å±•ç¤ºäº†<strong>éç»çƒ­åˆ†å­åŠ¨åŠ›å­¦</strong>çš„æ•°å­¦åŸºç¡€ï¼Œè¿™æ˜¯ç†è®ºåŒ–å­¦å’Œç‰©ç†å­¦ä¸­ä¸€ç§å¤æ‚çš„æ–¹æ³•ï¼Œç”¨äºæ¨¡æ‹Ÿç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼å¤±æ•ˆçš„è¿‡ç¨‹ã€‚è¿™é€šå¸¸å‘ç”Ÿåœ¨å…‰åŒ–å­¦ã€ç”µå­è½¬ç§»ååº”ä»¥åŠåˆ†å­ä¸å¼ºæ¿€å…‰åœºç›¸äº’ä½œç”¨æ—¶ã€‚</p>
<h3
id="topic-non-adiabatic-molecular-dynamics-md-éç»çƒ­åˆ†å­åŠ¨åŠ›å­¦-md">6.
Topic: Non-Adiabatic Molecular Dynamics (MD) éç»çƒ­åˆ†å­åŠ¨åŠ›å­¦ (MD)</h3>
<p>The title â€œÎ” non-adiabatic MDâ€ indicates that the topic moves beyond
the standard Born-Oppenheimer approximation. In this approximation, it
is assumed that the light electrons adjust instantaneously to the motion
of the heavy nuclei, allowing the system to be described by a single
potential energy surface. Non-adiabatic methods, by contrast, account
for the quantum mechanical coupling between multiple electronic
states.</p>
<p>æ ‡é¢˜â€œÎ” éç»çƒ­
MDâ€è¡¨æ˜è¯¥ä¸»é¢˜è¶…è¶Šäº†æ ‡å‡†çš„ç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼ã€‚åœ¨è¯¥è¿‘ä¼¼ä¸­ï¼Œå‡è®¾è½»ç”µå­ä¼šæ ¹æ®é‡åŸå­æ ¸çš„è¿åŠ¨è¿›è¡Œç¬æ—¶è°ƒæ•´ï¼Œä»è€Œä½¿ç³»ç»Ÿå¯ä»¥ç”¨å•ä¸ªåŠ¿èƒ½é¢æ¥æè¿°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œéç»çƒ­æ–¹æ³•åˆ™è€ƒè™‘äº†å¤šä¸ªç”µå­æ€ä¹‹é—´çš„é‡å­åŠ›å­¦è€¦åˆã€‚</p>
<h3 id="the-born-huang-ansatz-ç»æ©-é»„æ‹Ÿè®¾">7. The Born-Huang Ansatz
ç»æ©-é»„æ‹Ÿè®¾</h3>
<p>The starting point for this method is the â€œansatzâ€ (an educated guess
for the form of the solution). This is the Born-Huang expansion for the
total molecular wave function, <span
class="math inline">\(\Psi\)</span>.
è¯¥æ–¹æ³•çš„èµ·ç‚¹æ˜¯â€œæ‹Ÿè®¾â€ï¼ˆå¯¹è§£å½¢å¼çš„åˆç†çŒœæµ‹ï¼‰ã€‚è¿™æ˜¯åˆ†å­æ€»æ³¢å‡½æ•° <span
class="math inline">\(\Psi\)</span> çš„ç»æ©-é»„å±•å¼€å¼ã€‚</p>
<p><span class="math inline">\(\Psi(\vec{R}, \vec{r}, t) = \sum_{n}
\Theta_n(\vec{R}, t) \Phi_n(\vec{R}, \vec{r})\)</span></p>
<ul>
<li><p><strong><span class="math inline">\(\Psi(\vec{R}, \vec{r},
t)\)</span></strong> is the total wave function for the entire molecule.
It depends on the coordinates of all nuclei (<span
class="math inline">\(\vec{R}\)</span>), all electrons (<span
class="math inline">\(\vec{r}\)</span>), and time (<span
class="math inline">\(t\)</span>).
æ˜¯æ•´ä¸ªåˆ†å­çš„æ€»æ³¢å‡½æ•°ã€‚å®ƒå–å†³äºæ‰€æœ‰åŸå­æ ¸ (<span
class="math inline">\(\vec{R}\)</span>)ã€æ‰€æœ‰ç”µå­ (<span
class="math inline">\(\vec{r}\)</span>) å’Œæ—¶é—´ (<span
class="math inline">\(t\)</span>) çš„åæ ‡ã€‚</p></li>
<li><p><strong><span class="math inline">\(\Phi_n(\vec{R},
\vec{r})\)</span></strong> are the <strong>electronic wave
functions</strong>. They are the solutions to the electronic SchrÃ¶dinger
equation for a fixed nuclear geometry <span
class="math inline">\(\vec{R}\)</span> and form a complete basis set.
The index <span class="math inline">\(n\)</span> labels the electronic
state (e.g., ground state, first excited state, etc.).
å®ƒä»¬æ˜¯ç»™å®šåŸå­æ ¸å‡ ä½•æ„å‹ <span class="math inline">\(\vec{R}\)</span>
çš„ç”µå­è–›å®šè°”æ–¹ç¨‹çš„è§£ï¼Œå¹¶æ„æˆä¸€ä¸ªå®Œæ•´çš„åŸºç»„ã€‚ä¸‹æ ‡ <span
class="math inline">\(n\)</span>
æ ‡è®°ç”µå­æ€ï¼ˆä¾‹å¦‚ï¼ŒåŸºæ€ã€ç¬¬ä¸€æ¿€å‘æ€ç­‰ï¼‰ã€‚</p></li>
<li><p><strong><span class="math inline">\(\Theta_n(\vec{R},
t)\)</span></strong> are the <strong>nuclear wave functions</strong>.
Each <span class="math inline">\(\Theta_n\)</span> describes the motion
of the nuclei on the potential energy surface of the corresponding
electronic state, <span class="math inline">\(\Phi_n\)</span>.
Crucially, they depend on time. æ˜¯<strong>æ ¸æ³¢å‡½æ•°</strong>ã€‚æ¯ä¸ª <span
class="math inline">\(\Theta_n\)</span> æè¿°åŸå­æ ¸åœ¨ç›¸åº”ç”µå­æ€ <span
class="math inline">\(\Phi_n\)</span>
åŠ¿èƒ½é¢ä¸Šçš„è¿åŠ¨ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œå®ƒä»¬ä¾èµ–äºæ—¶é—´ã€‚</p></li>
</ul>
<p>This ansatz expresses the total molecular state as a superposition of
electronic states, where the coefficients of the superposition are the
nuclear wave functions.
è¯¥æ‹Ÿè®¾å°†æ€»åˆ†å­æ€è¡¨ç¤ºä¸ºç”µå­æ€çš„å åŠ ï¼Œå…¶ä¸­å åŠ çš„ç³»æ•°æ˜¯æ ¸æ³¢å‡½æ•°ã€‚</p>
<h3 id="the-partitioned-molecular-hamiltonian-åˆ†å‰²åˆ†å­å“ˆå¯†é¡¿é‡">8. The
Partitioned Molecular Hamiltonian åˆ†å‰²åˆ†å­å“ˆå¯†é¡¿é‡</h3>
<p>The total molecular Hamiltonian, <span
class="math inline">\(\hat{\mathcal{H}}\)</span>, is partitioned into
terms that act on the nuclei and electrons separately. æ€»åˆ†å­å“ˆå¯†é¡¿é‡
<span class="math inline">\(\hat{\mathcal{H}}\)</span>
è¢«åˆ†å‰²æˆåˆ†åˆ«ä½œç”¨äºåŸå­æ ¸å’Œç”µå­çš„é¡¹ã€‚</p>
<p><span class="math inline">\(\hat{\mathcal{H}} = -\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2 + \hat{\mathcal{H}}_e +
\hat{V}_{nn}\)</span></p>
<ul>
<li><p><strong><span class="math inline">\(-\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2\)</span></strong>: This is the kinetic
energy operator for the nuclei, often denoted as <span
class="math inline">\(\hat{T}_n\)</span>.è¿™æ˜¯åŸå­æ ¸çš„åŠ¨èƒ½ç®—ç¬¦ï¼Œé€šå¸¸è¡¨ç¤ºä¸º
<span class="math inline">\(\hat{T}_n\)</span>ã€‚</p></li>
<li><p><strong><span
class="math inline">\(\hat{\mathcal{H}}_e\)</span></strong>: This is the
<strong>electronic Hamiltonian</strong>, which includes the kinetic
energy of the electrons and the potential energy of electron-electron
and electron-nuclear interactions.
è¿™æ˜¯<strong>ç”µå­å“ˆå¯†é¡¿é‡</strong>ï¼ŒåŒ…å«ç”µå­çš„åŠ¨èƒ½ä»¥åŠç”µå­-ç”µå­å’Œç”µå­-æ ¸ç›¸äº’ä½œç”¨çš„åŠ¿èƒ½ã€‚</p></li>
<li><p><strong><span
class="math inline">\(\hat{V}_{nn}\)</span></strong>: This is the
potential energy operator for <strong>nuclear-nuclear
repulsion</strong>.è¿™æ˜¯<strong>æ ¸-æ ¸æ’æ–¥</strong>çš„åŠ¿èƒ½ç®—ç¬¦ã€‚</p></li>
</ul>
<h3 id="the-electronic-schrÃ¶dinger-equation-ç”µå­è–›å®šè°”æ–¹ç¨‹">9. The
Electronic SchrÃ¶dinger Equation ç”µå­è–›å®šè°”æ–¹ç¨‹</h3>
<p>The electronic basis functions, <span
class="math inline">\(\Phi_n\)</span>, are defined as the eigenfunctions
of the electronic Hamiltonian (plus the nuclear repulsion term) for a
fixed nuclear configuration <span
class="math inline">\(\vec{R}\)</span>. ç”µå­åŸºå‡½æ•° <span
class="math inline">\(\Phi_n\)</span> å®šä¹‰ä¸ºå¯¹äºå›ºå®šçš„æ ¸æ„å‹ <span
class="math inline">\(\vec{R}\)</span>ï¼Œç”µå­å“ˆå¯†é¡¿é‡ï¼ˆåŠ ä¸Šæ ¸æ’æ–¥é¡¹ï¼‰çš„æœ¬å¾å‡½æ•°ã€‚</p>
<p><span class="math inline">\((\hat{\mathcal{H}}_e + \hat{V}_{nn})
\Phi_n(\vec{R}, \vec{r}) = E_n(\vec{R}) \Phi_n(\vec{R},
\vec{r})\)</span></p>
<ul>
<li><strong><span class="math inline">\(E_n(\vec{R})\)</span></strong>
are the eigenvalues, which are the <strong>potential energy surfaces
(PES)</strong>. Each electronic state <span
class="math inline">\(n\)</span> has its own PES, which dictates the
forces acting on the nuclei when the molecule is in that electronic
state. æ˜¯ç‰¹å¾å€¼ï¼Œå³<strong>åŠ¿èƒ½é¢ (PES)</strong>ã€‚æ¯ä¸ªç”µå­æ€ <span
class="math inline">\(n\)</span>
éƒ½æœ‰å…¶è‡ªèº«çš„åŠ¿èƒ½é¢ï¼Œå®ƒå†³å®šäº†åˆ†å­å¤„äºè¯¥ç”µå­æ€æ—¶ä½œç”¨äºåŸå­æ ¸çš„åŠ›ã€‚</li>
</ul>
<h3
id="deriving-the-equations-of-motion-for-the-nuclei-æ¨å¯¼åŸå­æ ¸è¿åŠ¨æ–¹ç¨‹">10.
Deriving the Equations of Motion for the Nuclei æ¨å¯¼åŸå­æ ¸è¿åŠ¨æ–¹ç¨‹</h3>
<p>The final part of the whiteboard begins the derivation of the
time-dependent SchrÃ¶dinger equation for the nuclear wave functions,
<span class="math inline">\(\Theta_k\)</span>. The process starts with
the full time-dependent SchrÃ¶dinger equation, <span
class="math inline">\(i\hbar \frac{\partial \Psi}{\partial t} =
\hat{\mathcal{H}} \Psi\)</span>. To find the equation for a specific
nuclear wave function <span class="math inline">\(\Theta_k\)</span>,
this main equation is projected onto the corresponding electronic basis
state <span class="math inline">\(\Phi_k\)</span>.
ç™½æ¿çš„æœ€åä¸€éƒ¨åˆ†å¼€å§‹æ¨å¯¼åŸå­æ ¸æ³¢å‡½æ•° <span
class="math inline">\(\Theta_k\)</span>
çš„å«æ—¶è–›å®šè°”æ–¹ç¨‹ã€‚è¯¥è¿‡ç¨‹ä»å®Œæ•´çš„å«æ—¶è–›å®šè°”æ–¹ç¨‹ <span
class="math inline">\(i\hbar \frac{\partial \Psi}{\partial t} =
\hat{\mathcal{H}} \Psi\)</span> å¼€å§‹ã€‚ä¸ºäº†æ‰¾åˆ°ç‰¹å®šåŸå­æ ¸æ³¢å‡½æ•° <span
class="math inline">\(\Theta_k\)</span>
çš„æ–¹ç¨‹ï¼Œéœ€è¦å°†è¿™ä¸ªä¸»æ–¹ç¨‹æŠ•å½±åˆ°ç›¸åº”çš„ç”µå­åŸºæ€ <span
class="math inline">\(\Phi_k\)</span> ä¸Šã€‚</p>
<p>This is done by multiplying from the left by the complex conjugate of
the electronic wave function, <span
class="math inline">\(\Phi_k^*\)</span>, and integrating over all
electronic coordinates, <span class="math inline">\(d\vec{r}\)</span>.
å¯ä»¥é€šè¿‡ä»å·¦è¾¹ä¹˜ä»¥ç”µå­æ³¢å‡½æ•° <span
class="math inline">\(\Phi_k^*\)</span> çš„å¤å…±è½­ï¼Œç„¶ååœ¨æ‰€æœ‰ç”µå­åæ ‡
<span class="math inline">\(d\vec{r}\)</span> ä¸Šç§¯åˆ†æ¥å®ç°ã€‚</p>
<p><span class="math inline">\(\int \Phi_k^* i\hbar
\frac{\partial}{\partial t} \Psi \,d\vec{r} = \int \Phi_k^*
\hat{\mathcal{H}} \Psi \,d\vec{r}\)</span></p>
<p>The board then shows the result of substituting the Born-Huang ansatz
for <span class="math inline">\(\Psi\)</span> and the partitioned
Hamiltonian for <span class="math inline">\(\hat{\mathcal{H}}\)</span>
into this projected equation: ç„¶åï¼Œé»‘æ¿æ˜¾ç¤ºå°† Born-Huang æ‹Ÿè®¾å¼ä»£å…¥
<span
class="math inline">\(\Psi\)</span>ï¼Œå°†åˆ†å—å“ˆå¯†é¡¿é‡ä»£å…¥ä»¥ä¸‹æŠ•å½±æ–¹ç¨‹çš„ç»“æœï¼š</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k(\vec{R}, t) = \int \Phi_k^* \left( -\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2 + \hat{\mathcal{H}}_e + \hat{V}_{nn}
\right) \sum_n \Theta_n \Phi_n \,d\vec{r}\)</span></p>
<ul>
<li><p><strong>Left Hand Side</strong>: The left side of the projection
has been simplified. Because the electronic basis functions <span
class="math inline">\(\Phi_n\)</span> form an orthonormal set (<span
class="math inline">\(\int \Phi_k^* \Phi_n d\vec{r} =
\delta_{kn}\)</span>), the sum collapses to a single term for <span
class="math inline">\(n=k\)</span>. æŠ•å½±å·¦ä¾§å·²ç®€åŒ–ã€‚ç”±äºç”µå­åŸºå‡½æ•° <span
class="math inline">\(\Phi_n\)</span> æ„æˆä¸€ä¸ªæ­£äº¤é›† (<span
class="math inline">\(\int \Phi_k^* \Phi_n d\vec{r} =
\delta_{kn}\)</span>ï¼Œå› æ­¤å½“ <span class="math inline">\(n=k\)</span>
æ—¶ï¼Œå’Œå°†æŠ˜å ä¸ºä¸€ä¸ªé¡¹ã€‚</p></li>
<li><p><strong>Right Hand Side</strong>: This complex integral is the
core of non-adiabatic dynamics. When the nuclear kinetic energy
operator, <span class="math inline">\(\nabla_I^2\)</span>, acts on the
product <span class="math inline">\(\Theta_n \Phi_n\)</span>, it acts on
both functions (via the product rule). The terms that arise from <span
class="math inline">\(\nabla_I\)</span> acting on the electronic wave
functions <span class="math inline">\(\Phi_n\)</span> are known as
<strong>non-adiabatic coupling terms</strong>. These terms are
responsible for enabling transitions between different electronic
potential energy surfaces, which is the essence of non-adiabatic
dynamics. è¿™ä¸ªå¤ç§¯åˆ†æ˜¯éç»çƒ­åŠ¨åŠ›å­¦çš„æ ¸å¿ƒã€‚å½“æ ¸åŠ¨èƒ½ç®—ç¬¦ <span
class="math inline">\(\nabla_I^2\)</span> ä½œç”¨äºä¹˜ç§¯ <span
class="math inline">\(\Theta_n \Phi_n\)</span>
æ—¶ï¼Œå®ƒä¼šä½œç”¨äºè¿™ä¸¤ä¸ªå‡½æ•°ï¼ˆé€šè¿‡ä¹˜ç§¯è§„åˆ™ï¼‰ã€‚ç”± <span
class="math inline">\(\nabla_I\)</span> ä½œç”¨äºç”µå­æ³¢å‡½æ•° <span
class="math inline">\(\Phi_n\)</span>
è€Œäº§ç”Ÿçš„é¡¹ç§°ä¸º<strong>éç»çƒ­è€¦åˆé¡¹</strong>ã€‚è¿™äº›æœ¯è¯­è´Ÿè´£å®ç°ä¸åŒç”µå­åŠ¿èƒ½é¢ä¹‹é—´çš„è½¬å˜ï¼Œè¿™æ˜¯éç»çƒ­åŠ¨åŠ›å­¦çš„æœ¬è´¨ã€‚</p></li>
</ul>
<p>This whiteboard continues the mathematical derivation for
non-adiabatic molecular dynamics started in the previous image. It
focuses on expanding the nuclear kinetic energy term to reveal the
crucial couplings between different electronic
states.è¿™å—ç™½æ¿å»¶ç»­äº†ä¸Šä¸€å¼ å›¾ç‰‡ä¸­éç»çƒ­åˆ†å­åŠ¨åŠ›å­¦çš„æ•°å­¦æ¨å¯¼ã€‚å®ƒç€é‡äºæ‰©å±•æ ¸åŠ¨èƒ½é¡¹ï¼Œä»¥æ­ç¤ºä¸åŒç”µå­æ€ä¹‹é—´çš„å…³é”®è€¦åˆã€‚</p>
<h3
id="starting-point-the-projected-schrÃ¶dinger-equation-èµ·ç‚¹æŠ•å½±è–›å®šè°”æ–¹ç¨‹">11.
Starting Point: The Projected SchrÃ¶dinger Equation
èµ·ç‚¹ï¼šæŠ•å½±è–›å®šè°”æ–¹ç¨‹</h3>
<p>The derivation picks up from the equation for the time evolution of
the nuclear wave function, <span
class="math inline">\(\Theta_k\)</span>. The right-hand side of this
equation is being evaluated. æ¨å¯¼è¿‡ç¨‹å–è‡ªæ ¸æ³¢å‡½æ•° <span
class="math inline">\(\Theta_k\)</span>
çš„æ—¶é—´æ¼”åŒ–æ–¹ç¨‹ã€‚è¯¥æ–¹ç¨‹çš„å³è¾¹æ­£åœ¨æ±‚å€¼ã€‚</p>
<p><span class="math inline">\(= \int \Phi_k^* \left( -\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2 \right) \sum_n \Theta_n \Phi_n \,d\vec{r}
+ E_k \Theta_k\)</span></p>
<p>This equation separates the total energy into two parts
è¯¥æ–¹ç¨‹å°†æ€»èƒ½é‡åˆ†ä¸ºä¸¤éƒ¨åˆ† : * The first term is the contribution from the
<strong>nuclear kinetic energy operator</strong>, <span
class="math inline">\(-\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2\)</span>.
ç¬¬ä¸€é¡¹æ˜¯<strong>æ ¸åŠ¨èƒ½ç®—ç¬¦</strong>çš„è´¡çŒ® * The second term, <span
class="math inline">\(E_k \Theta_k\)</span>, is the contribution from
the <strong>potential energy</strong>. This term arises from the action
of the electronic Hamiltonian part <span
class="math inline">\((\hat{\mathcal{H}}_e + \hat{V}_{nn})\)</span> on
the basis functions. Due to the orthonormality of the electronic
wavefunctions (<span class="math inline">\(\int \Phi_k^* \Phi_n
\,d\vec{r} = \delta_{kn}\)</span>), the sum over <span
class="math inline">\(n\)</span> collapses to a single term for the
potential energy. ç¬¬äºŒé¡¹ï¼Œ<span class="math inline">\(E_k
\Theta_k\)</span>ï¼Œæ˜¯<strong>åŠ¿èƒ½</strong>çš„è´¡çŒ®ã€‚è¿™ä¸€é¡¹æºäºç”µå­å“ˆå¯†é¡¿é‡éƒ¨åˆ†
<span class="math inline">\((\hat{\mathcal{H}}_e +
\hat{V}_{nn})\)</span> å¯¹åŸºå‡½æ•°çš„ä½œç”¨ã€‚ç”±äºç”µå­æ³¢å‡½æ•°ï¼ˆ<span
class="math inline">\(\int \Phi_k^* \Phi_n \,d\vec{r} =
\delta_{kn}\)</span>ï¼‰çš„æ­£äº¤æ€§ï¼Œ<span
class="math inline">\(n\)</span>é¡¹çš„å’Œä¼šåç¼©ä¸ºåŠ¿èƒ½çš„ä¸€é¡¹ã€‚</p>
<p>The challenge, and the core of the physics, lies in evaluating the
first term, as the nuclear derivative <span
class="math inline">\(\nabla_I\)</span> acts on <em>both</em> the
nuclear wave function <span class="math inline">\(\Theta_n\)</span> and
the electronic wave function <span
class="math inline">\(\Phi_n\)</span>.
éš¾ç‚¹åœ¨äºï¼Œä¹Ÿæ˜¯ç‰©ç†çš„æ ¸å¿ƒåœ¨äºå¦‚ä½•è®¡ç®—ç¬¬ä¸€é¡¹ï¼Œå› ä¸ºæ ¸å¯¼æ•° <span
class="math inline">\(\nabla_I\)</span> åŒæ—¶ä½œç”¨äºæ ¸æ³¢å‡½æ•° <span
class="math inline">\(\Theta_n\)</span> å’Œç”µå­æ³¢å‡½æ•° <span
class="math inline">\(\Phi_n\)</span>ã€‚</p>
<h3
id="applying-the-product-rule-for-the-laplacian-åº”ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ä¹˜ç§¯è§„åˆ™">12.
Applying the Product Rule for the Laplacian
åº”ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ä¹˜ç§¯è§„åˆ™</h3>
<p>To expand the kinetic energy term, the product rule for the Laplacian
operator acting on two functions (A and B) is used. The board writes
this rule as: ä¸ºäº†å±•å¼€åŠ¨èƒ½é¡¹ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†æ‹‰æ™®æ‹‰æ–¯ç®—å­ä½œç”¨äºä¸¤ä¸ªå‡½æ•°ï¼ˆA å’Œ
Bï¼‰çš„ä¹˜ç§¯è§„åˆ™ã€‚æ£‹ç›˜ä¸Šå°†è¿™æ¡è§„åˆ™å†™æˆï¼š <span
class="math inline">\(\nabla^2(AB) = (\nabla^2 A)B + 2(\nabla
A)\cdot(\nabla B) + A(\nabla^2 B)\)</span></p>
<p>In our case, <span class="math inline">\(A = \Theta_n(\vec{R},
t)\)</span> and <span class="math inline">\(B = \Phi_n(\vec{R},
\vec{r})\)</span>. The derivative <span
class="math inline">\(\nabla_I\)</span> is with respect to the nuclear
coordinates <span class="math inline">\(\vec{R}_I\)</span>.
åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œ<span class="math inline">\(A = \Theta_n(\vec{R},
t)\)</span>ï¼Œ<span class="math inline">\(B = \Phi_n(\vec{R},
\vec{r})\)</span>ã€‚å¯¼æ•° <span class="math inline">\(\nabla_I\)</span>
æ˜¯å…³äºåŸå­æ ¸åæ ‡ <span class="math inline">\(\vec{R}_I\)</span> çš„ã€‚</p>
<h3 id="expanding-the-kinetic-energy-term-å±•å¼€åŠ¨èƒ½é¡¹">13. Expanding the
Kinetic Energy Term å±•å¼€åŠ¨èƒ½é¡¹</h3>
<p>Applying this rule, the integral containing the kinetic energy
operator is expanded: åº”ç”¨æ­¤è§„åˆ™ï¼Œå±•å¼€åŒ…å«åŠ¨èƒ½ç®—ç¬¦çš„ç§¯åˆ†ï¼š <span
class="math inline">\(= -\sum_I \frac{\hbar^2}{2M_I} \int \Phi_k^*
\sum_n \left( (\nabla_I^2 \Theta_n)\Phi_n + 2(\nabla_I
\Theta_n)\cdot(\nabla_I \Phi_n) + \Theta_n(\nabla_I^2 \Phi_n) \right)
d\vec{r} + E_k \Theta_k\)</span></p>
<p>This step explicitly shows how the nuclear kinetic energy operator
gives rise to three distinct types of
terms.æ­¤æ­¥éª¤æ˜ç¡®å±•ç¤ºäº†æ ¸åŠ¨èƒ½ç®—ç¬¦å¦‚ä½•äº§ç”Ÿä¸‰ç§ä¸åŒç±»å‹çš„é¡¹ã€‚</p>
<h3
id="final-result-and-identification-of-coupling-terms-æœ€ç»ˆç»“æœåŠè€¦åˆé¡¹çš„è¯†åˆ«">14.
Final Result and Identification of Coupling Terms
æœ€ç»ˆç»“æœåŠè€¦åˆé¡¹çš„è¯†åˆ«</h3>
<p>The final step is to take the integral over the electronic
coordinates (<span class="math inline">\(d\vec{r}\)</span>) and
rearrange the terms. The expression is simplified by again using the
orthonormality of the electronic wave functions, <span
class="math inline">\(\int \Phi_k^* \Phi_n \, d\vec{r} =
\delta_{kn}\)</span>. æœ€åä¸€æ­¥æ˜¯å¯¹ç”µå­åæ ‡ (<span
class="math inline">\(d\vec{r}\)</span>)
è¿›è¡Œç§¯åˆ†ï¼Œå¹¶é‡æ–°æ’åˆ—å„é¡¹ã€‚å†æ¬¡åˆ©ç”¨ç”µå­æ³¢å‡½æ•°çš„æ­£äº¤æ€§ç®€åŒ–è¡¨è¾¾å¼ï¼Œ<span
class="math inline">\(\int \Phi_k^* \Phi_n \, d\vec{r} =
\delta_{kn}\)</span>ã€‚</p>
<p><span class="math inline">\(= -\sum_I \frac{\hbar^2}{2M_I} \left(
\nabla_I^2 \Theta_k + \sum_n 2 \left( \int \Phi_k^* \nabla_I \Phi_n \,
d\vec{r} \right) \cdot \nabla_I \Theta_n + \sum_n \left( \int \Phi_k^*
\nabla_I^2 \Phi_n \, d\vec{r} \right) \Theta_n \right) + E_k
\Theta_k\)</span></p>
<p>This final equation is profound. It represents the time-independent
SchrÃ¶dinger equation for the nuclear wave function <span
class="math inline">\(\Theta_k\)</span>, but it is coupled to all other
nuclear wave functions <span class="math inline">\(\Theta_n\)</span>.
Letâ€™s break down the key terms within the parentheses:
æœ€åä¸€ä¸ªæ–¹ç¨‹æ„ä¹‰æ·±è¿œã€‚å®ƒä»£è¡¨äº†æ ¸æ³¢å‡½æ•° <span
class="math inline">\(\Theta_k\)</span>
çš„ä¸æ—¶é—´æ— å…³çš„è–›å®šè°”æ–¹ç¨‹ï¼Œä½†å®ƒä¸æ‰€æœ‰å…¶ä»–æ ¸æ³¢å‡½æ•° <span
class="math inline">\(\Theta_n\)</span>
è€¦åˆã€‚è®©æˆ‘ä»¬åˆ†è§£ä¸€ä¸‹æ‹¬å·å†…çš„å…³é”®é¡¹ï¼š</p>
<ul>
<li><p><strong><span class="math inline">\(\nabla_I^2
\Theta_k\)</span></strong>: This is the standard kinetic energy term for
the nuclei moving on the potential energy surface of state <span
class="math inline">\(k\)</span>. This is the only term that would
remain in the simple Born-Oppenheimer (adiabatic) approximation.
è¿™æ˜¯åŸå­æ ¸åœ¨åŠ¿èƒ½é¢ <span class="math inline">\(k\)</span>
ä¸Šè¿åŠ¨çš„æ ‡å‡†åŠ¨èƒ½é¡¹ã€‚è¿™æ˜¯åœ¨ç®€å•çš„
Born-Oppenheimerï¼ˆç»çƒ­ï¼‰è¿‘ä¼¼ä¸­å”¯ä¸€ä¿ç•™çš„é¡¹ã€‚</p></li>
<li><p><strong><span class="math inline">\(\left( \int \Phi_k^* \nabla_I
\Phi_n \, d\vec{r} \right)\)</span></strong>: This is the
<strong>first-derivative non-adiabatic coupling term (NACT)</strong>,
often called the derivative coupling. This vector quantity determines
the strength of the coupling between electronic states <span
class="math inline">\(k\)</span> and <span
class="math inline">\(n\)</span> due to the velocity of the nuclei. It
is the primary term responsible for enabling transitions between
different potential energy surfaces. è¿™æ˜¯<strong>ä¸€é˜¶å¯¼æ•°éç»çƒ­è€¦åˆé¡¹
(NACT)</strong>ï¼Œé€šå¸¸ç§°ä¸ºå¯¼æ•°è€¦åˆã€‚è¯¥çŸ¢é‡å†³å®šäº†ç”±äºåŸå­æ ¸é€Ÿåº¦è€Œå¯¼è‡´çš„ç”µå­æ€
<span class="math inline">\(k\)</span> å’Œ <span
class="math inline">\(n\)</span>
ä¹‹é—´è€¦åˆçš„å¼ºåº¦ã€‚å®ƒæ˜¯å®ç°ä¸åŒåŠ¿èƒ½é¢ä¹‹é—´è·ƒè¿çš„ä¸»è¦é¡¹ã€‚</p></li>
<li><p><strong><span class="math inline">\(\left( \int \Phi_k^*
\nabla_I^2 \Phi_n \, d\vec{r} \right)\)</span></strong>: This is the
<strong>second-derivative non-adiabatic coupling term</strong>, a scalar
quantity. While often smaller than the first-derivative term, it is also
part of the complete description of non-adiabatic effects.
æ˜¯<strong>äºŒé˜¶å¯¼æ•°éç»çƒ­è€¦åˆé¡¹</strong>ï¼Œä¸€ä¸ªæ ‡é‡ã€‚è™½ç„¶å®ƒé€šå¸¸å°äºä¸€é˜¶å¯¼æ•°é¡¹ï¼Œä½†å®ƒä¹Ÿæ˜¯éç»çƒ­æ•ˆåº”å®Œæ•´æè¿°çš„ä¸€éƒ¨åˆ†ã€‚</p></li>
</ul>
<p>In summary, this derivation shows mathematically how the motion of
the nuclei (via the <span class="math inline">\(\nabla_I\)</span>
operator) can induce quantum mechanical transitions between different
electronic states (<span class="math inline">\(\Phi_k \leftrightarrow
\Phi_n\)</span>). The strength of these transitions is governed by the
non-adiabatic coupling terms, which depend on how the electronic wave
functions change as the nuclear geometry changes.
æ€»ä¹‹ï¼Œè¯¥æ¨å¯¼ä»æ•°å­¦ä¸Šå±•ç¤ºäº†åŸå­æ ¸çš„è¿åŠ¨ï¼ˆé€šè¿‡ <span
class="math inline">\(\nabla_I\)</span>
ç®—ç¬¦ï¼‰å¦‚ä½•è¯±å¯¼ä¸åŒç”µå­æ€ä¹‹é—´çš„é‡å­åŠ›å­¦è·ƒè¿ï¼ˆ<span
class="math inline">\(\Phi_k \leftrightarrow
\Phi_n\)</span>ï¼‰ã€‚è¿™äº›è·ƒè¿çš„å¼ºåº¦ç”±éç»çƒ­è€¦åˆé¡¹æ§åˆ¶ï¼Œè€Œéç»çƒ­è€¦åˆé¡¹åˆå–å†³äºç”µå­æ³¢å‡½æ•°å¦‚ä½•éšåŸå­æ ¸å‡ ä½•ç»“æ„çš„å˜åŒ–è€Œå˜åŒ–ã€‚</p>
<p>This whiteboard concludes the derivation of the equations for
non-adiabatic molecular dynamics by defining the coupling operator and
then showing how different levels of approximationâ€”specifically the
Born-Huang and the more restrictive Born-Oppenheimer
approximationsâ€”arise from neglecting certain coupling terms.
è¿™å—ç™½æ¿é€šè¿‡å®šä¹‰è€¦åˆç®—ç¬¦ï¼Œå¹¶å±•ç¤ºä¸åŒç¨‹åº¦çš„è¿‘ä¼¼â€”â€”ç‰¹åˆ«æ˜¯ Born-Huang
è¿‘ä¼¼å’Œæ›´ä¸¥æ ¼çš„ Born-Oppenheimer
è¿‘ä¼¼â€”â€”æ˜¯å¦‚ä½•é€šè¿‡å¿½ç•¥æŸäº›è€¦åˆé¡¹è€Œäº§ç”Ÿçš„ï¼Œä»è€Œæ¨å¯¼å‡ºéç»çƒ­åˆ†å­åŠ¨åŠ›å­¦æ–¹ç¨‹çš„ã€‚</p>
<h3
id="definition-of-the-non-adiabatic-coupling-operator-éç»çƒ­è€¦åˆç®—ç¬¦çš„å®šä¹‰">15.
Definition of the Non-Adiabatic Coupling Operator
éç»çƒ­è€¦åˆç®—ç¬¦çš„å®šä¹‰</h3>
<p>The whiteboard begins by collecting all the non-adiabatic coupling
terms derived previously into a single operator, <span
class="math inline">\(C_{kn}\)</span>.
ç™½æ¿é¦–å…ˆå°†ä¹‹å‰æ¨å¯¼çš„æ‰€æœ‰éç»çƒ­è€¦åˆé¡¹åˆå¹¶ä¸ºä¸€ä¸ªç®—ç¬¦ <span
class="math inline">\(C_{kn}\)</span>ã€‚</p>
<p>Let <span class="math inline">\(C_{kn} = -\sum_{I}
\frac{\hbar^2}{2M_I} \left( 2 \left( \int \Phi_k^* \nabla_I \Phi_n \,
d\vec{r} \right) \cdot \nabla_I + \left( \int \Phi_k^* \nabla_I^2 \Phi_n
\, d\vec{r} \right) \right)\)</span></p>
<ul>
<li>This operator, <span class="math inline">\(C_{kn}\)</span>,
represents the total effect of the coupling between electronic state
<span class="math inline">\(k\)</span> and electronic state <span
class="math inline">\(n\)</span>, which is induced by the kinetic energy
of the nuclei. æ­¤ç®—ç¬¦ <span class="math inline">\(C_{kn}\)</span>
è¡¨ç¤ºç”±åŸå­æ ¸åŠ¨èƒ½å¼•èµ·çš„ç”µå­æ€ <span class="math inline">\(k\)</span>
å’Œç”µå­æ€ <span class="math inline">\(n\)</span> ä¹‹é—´è€¦åˆçš„æ€»æ•ˆåº”ã€‚</li>
<li>The operator acts on the nuclear wave function that follows it in
the full equation. The <span class="math inline">\(\nabla_I\)</span>
term acts as a derivative on that wave function.
è¯¥ç®—ç¬¦ä½œç”¨äºå®Œæ•´æ–¹ç¨‹ä¸­è·Ÿéšå®ƒçš„æ ¸æ³¢å‡½æ•°ã€‚<span
class="math inline">\(\nabla_I\)</span> é¡¹å……å½“è¯¥æ³¢å‡½æ•°çš„å¯¼æ•°ã€‚</li>
</ul>
<h3 id="the-coupled-equations-of-motion-è€¦åˆè¿åŠ¨æ–¹ç¨‹">16. The Coupled
Equations of Motion è€¦åˆè¿åŠ¨æ–¹ç¨‹</h3>
<p>Using this compact definition, the full set of coupled time-dependent
SchrÃ¶dinger equations for the nuclear wave functions can be written as:
åŸºäºæ­¤ç®€æ´å®šä¹‰ï¼Œæ ¸æ³¢å‡½æ•°çš„å®Œæ•´è€¦åˆå«æ—¶è–›å®šè°”æ–¹ç¨‹ç»„å¯ä»¥å†™æˆï¼š</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k \right)
\Theta_k + \sum_n C_{kn} \Theta_n\)</span></p>
<p>This is the central result. It shows that the time evolution of the
nuclear wave function on a given potential energy surface <span
class="math inline">\(k\)</span> (described by <span
class="math inline">\(\Theta_k\)</span>) depends on two things:
è¿™æ˜¯æ ¸å¿ƒç»“è®ºã€‚å®ƒè¡¨æ˜ï¼Œæ ¸æ³¢å‡½æ•°åœ¨ç»™å®šåŠ¿èƒ½é¢ <span
class="math inline">\(k\)</span>ï¼ˆç”¨ <span
class="math inline">\(\Theta_k\)</span>
æè¿°ï¼‰ä¸Šçš„æ—¶é—´æ¼”åŒ–å–å†³äºä¸¤ä¸ªå› ç´ ï¼š 1. The motion on its own surface,
governed by its kinetic energy and the potential <span
class="math inline">\(E_k\)</span>. å…¶è‡ªèº«è¡¨é¢ä¸Šçš„è¿åŠ¨ï¼Œç”±å…¶åŠ¨èƒ½å’ŒåŠ¿èƒ½
<span class="math inline">\(E_k\)</span> æ§åˆ¶ã€‚ 2. The influence of the
nuclear wave functions on <em>all other</em> electronic surfaces (<span
class="math inline">\(\Theta_n\)</span>), mediated by the coupling
operators <span class="math inline">\(C_{kn}\)</span>.
æ ¸æ³¢å‡½æ•°å¯¹<em>æ‰€æœ‰å…¶ä»–</em>ç”µå­è¡¨é¢ï¼ˆ<span
class="math inline">\(\Theta_n\)</span>ï¼‰çš„å½±å“ï¼Œç”±è€¦åˆç®—ç¬¦ <span
class="math inline">\(C_{kn}\)</span> ä»‹å¯¼ã€‚</p>
<h3 id="the-born-huang-approximation-ç»æ©-é»„è¿‘ä¼¼">17. The Born-Huang
Approximation ç»æ©-é»„è¿‘ä¼¼</h3>
<p>The first and most crucial approximation is introduced to simplify
this complex set of coupled equations.
ä¸ºäº†ç®€åŒ–è¿™ç»„å¤æ‚çš„è€¦åˆæ–¹ç¨‹ï¼Œå¼•å…¥äº†ç¬¬ä¸€ä¸ªä¹Ÿæ˜¯æœ€é‡è¦çš„è¿‘ä¼¼ã€‚</p>
<p><strong>If <span class="math inline">\(C_{kn} = 0\)</span> for <span
class="math inline">\(k \neq n\)</span> (Born-Huang
approximation)</strong></p>
<p>This approximation assumes that the <strong>off-diagonal</strong>
coupling terms, which are responsible for transitions between different
electronic states, are negligible. However, it retains the
<strong>diagonal</strong> coupling term (<span
class="math inline">\(C_{kk}\)</span>). This leads to a simplified,
uncoupled equation:
è¯¥è¿‘ä¼¼å‡è®¾å¯¼è‡´ä¸åŒç”µå­æ€ä¹‹é—´è·ƒè¿çš„<strong>éå¯¹è§’</strong>è€¦åˆé¡¹å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚ç„¶è€Œï¼Œå®ƒä¿ç•™äº†<strong>å¯¹è§’</strong>è€¦åˆé¡¹ï¼ˆ<span
class="math inline">\(C_{kk}\)</span>ï¼‰ã€‚è¿™å¯ä»¥å¾—åˆ°ä¸€ä¸ªç®€åŒ–çš„éè€¦åˆæ–¹ç¨‹ï¼š</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k +
C_{kk} \right) \Theta_k\)</span></p>
<p>Substituting the definition of <span
class="math inline">\(C_{kk}\)</span>: ä»£å…¥ <span
class="math inline">\(C_{kk}\)</span> çš„å®šä¹‰ï¼š</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k -
\sum_I \frac{\hbar^2}{2M_I} \left( 2 \left( \int \Phi_k^* \nabla_I
\Phi_k \, d\vec{r} \right) \cdot \nabla_I + \int \Phi_k^* \nabla_I^2
\Phi_k \, d\vec{r} \right) \right) \Theta_k\)</span></p>
<p>The term <span class="math inline">\(C_{kk}\)</span> is known as the
<strong>diagonal Born-Oppenheimer correction (DBOC)</strong>. It
represents a small correction to the potential energy surface <span
class="math inline">\(E_k\)</span> that arises from the fact that the
electrons do not adjust perfectly and instantaneously to the nuclear
motion, even within the same electronic state. <span
class="math inline">\(C_{kk}\)</span>
é¡¹è¢«ç§°ä¸º<strong>å¯¹è§’ç»æ©-å¥¥æœ¬æµ·é»˜ä¿®æ­£ (DBOC)</strong>ã€‚å®ƒè¡¨ç¤ºå¯¹åŠ¿èƒ½é¢
<span class="math inline">\(E_k\)</span>
çš„å¾®å°ä¿®æ­£ï¼Œå…¶åŸå› æ˜¯å³ä½¿åœ¨ç›¸åŒçš„ç”µå­æ€ä¸‹ï¼Œç”µå­ä¹Ÿæ— æ³•å®Œç¾ä¸”å³æ—¶åœ°é€‚åº”æ ¸è¿åŠ¨ã€‚</p>
<ul>
<li><strong>Note on Real Wavefunctions å…³äºå®æ³¢å‡½æ•°çš„æ³¨é‡Š</strong>: The
board shows that for real wavefunctions, the first-derivative part of
the diagonal correction vanishes: <span class="math inline">\(\int
\Phi_k \nabla_I \Phi_k \, d\vec{r} = 0\)</span>. This is because the
integral is related to the gradient of the normalization condition,
<span class="math inline">\(\nabla_I \int \Phi_k^2 \, d\vec{r} =
\nabla_I(1) = 0\)</span>, which expands to <span
class="math inline">\(2\int \Phi_k \nabla_I \Phi_k \, d\vec{r} =
0\)</span>. é»‘æ¿æ˜¾ç¤ºï¼Œå¯¹äºå®æ³¢å‡½æ•°ï¼Œå¯¹è§’ä¿®æ­£çš„ä¸€é˜¶å¯¼æ•°éƒ¨åˆ†ä¸ºé›¶ï¼š<span
class="math inline">\(\int \Phi_k \nabla_I \Phi_k \, d\vec{r} =
0\)</span>ã€‚è¿™æ˜¯å› ä¸ºç§¯åˆ†ä¸å½’ä¸€åŒ–æ¡ä»¶çš„æ¢¯åº¦æœ‰å…³ï¼Œ<span
class="math inline">\(\nabla_I \int \Phi_k^2 \, d\vec{r} = \nabla_I(1) =
0\)</span>ï¼Œå…¶å±•å¼€ä¸º <span class="math inline">\(2\int \Phi_k \nabla_I
\Phi_k \, d\vec{r} = 0\)</span>ã€‚</li>
</ul>
<h3 id="the-born-oppenheimer-approximation-ç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼">18. The
Born-Oppenheimer Approximation ç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼</h3>
<p>The final and most widely used approximation is the Born-Oppenheimer
approximation. It is more restrictive than the Born-Huang approximation.
æœ€åä¸€ç§ä¹Ÿæ˜¯æœ€å¹¿æ³›ä½¿ç”¨çš„è¿‘ä¼¼æ–¹æ³•æ˜¯ç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼ã€‚å®ƒæ¯”ç»æ©-é»„è¿‘ä¼¼æ›´å…·é™åˆ¶æ€§ã€‚</p>
<p><strong>If <span class="math inline">\(C_{kk} = 0\)</span>
(Born-Oppenheimer approximation) è‹¥<span class="math inline">\(C_{kk} =
0\)</span>ï¼ˆç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼ï¼‰</strong></p>
<p>This assumes that the diagonal correction term is also negligible. By
setting all <span class="math inline">\(C_{kn}=0\)</span> (both diagonal
and off-diagonal), the equations become completely decoupled, and the
nuclear motion evolves independently on each potential energy surface.
è¿™å‡è®¾å¯¹è§’ä¿®æ­£é¡¹ä¹Ÿå¯å¿½ç•¥ä¸è®¡ã€‚é€šè¿‡ä»¤æ‰€æœ‰<span
class="math inline">\(C_{kn}=0\)</span>ï¼ˆåŒ…æ‹¬å¯¹è§’å’Œéå¯¹è§’ï¼‰ï¼Œæ–¹ç¨‹ç»„å®Œå…¨è§£è€¦ï¼ŒåŸå­æ ¸è¿åŠ¨åœ¨æ¯ä¸ªåŠ¿èƒ½é¢ä¸Šç‹¬ç«‹æ¼”åŒ–ã€‚</p>
<p>The result is the standard <strong>time-dependent SchrÃ¶dinger
equation for the nuclei</strong>:
ç”±æ­¤å¯å¾—æ ‡å‡†çš„<strong>åŸå­æ ¸çš„å«æ—¶è–›å®šè°”æ–¹ç¨‹</strong>ï¼š</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k \right)
\Theta_k\)</span></p>
<p>This equation is the foundation of most of quantum chemistry. It
states that the nuclei move on a static potential energy surface <span
class="math inline">\(E_k(\vec{R})\)</span> provided by the electrons,
without any possibility of transitioning to other electronic states or
having the surface be corrected by their own motion.</p>
<p>è¯¥æ–¹ç¨‹æ˜¯å¤§å¤šæ•°é‡å­åŒ–å­¦çš„åŸºç¡€ã€‚åŸå­æ ¸åœ¨ç”±ç”µå­æä¾›çš„é™æ€åŠ¿èƒ½é¢ <span
class="math inline">\(E_k(\vec{R})\)</span>
ä¸Šè¿åŠ¨ï¼Œä¸å­˜åœ¨è·ƒè¿åˆ°å…¶ä»–ç”µå­æ€æˆ–å› è‡ªèº«è¿åŠ¨è€Œä¿®æ­£åŠ¿èƒ½é¢çš„å¯èƒ½æ€§ã€‚</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/18/img_assert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/18/img_assert/" class="post-title-link" itemprop="url">BLOGS - IMG Assert</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-18 10:00:00" itemprop="dateCreated datePublished" datetime="2025-09-18T10:00:00+08:00">2025-09-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-19 19:24:51" itemprop="dateModified" datetime="2025-09-19T19:24:51+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">æŠ€æœ¯</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="é—®é¢˜ä¸»è¦ä¸ºäº†å›¾åƒä¸æ˜¾ç¤ºé—®é¢˜">ã€é—®é¢˜ã€‘ä¸»è¦ä¸ºäº†å›¾åƒä¸æ˜¾ç¤ºé—®é¢˜</h2>
<h3 id="step1æ ¹ç›®å½•ä¸­çš„é…ç½®æ–‡ä»¶">Step1:æ ¹ç›®å½•ä¸­çš„é…ç½®æ–‡ä»¶</h3>
<h3 id="step2å°†-markdown-è¡Œæ›¿æ¢ä¸ºhtml-ä»£ç ">Step2:å°† Markdown
è¡Œæ›¿æ¢ä¸ºHTML ä»£ç </h3>
<h3 id="step3è®¾ç½®ä¸‹æ–¹æ·»åŠ root">Step3:è®¾ç½®ä¸‹æ–¹æ·»åŠ ROOT</h3>
<h3
id="step4ä¸éœ€è¦æ­¤æ’ä»¶ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å¸è½½æ’ä»¶">Step4:ä¸éœ€è¦æ­¤æ’ä»¶ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å¸è½½æ’ä»¶ï¼š</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment"># URL</span></span><br><span class="line"><span class="comment">## Set your site url here. For example, if you use GitHub Page, set url as &#x27;https://username.github.io/project&#x27;</span></span><br><span class="line">$ url: https://TianyaoBlogs.github.io/</span><br><span class="line"></span><br><span class="line">$ root: /</span><br><span class="line"></span><br><span class="line">$ permalink: :year/:month/:day/:title/</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ &lt;img src=<span class="string">&quot;/imgs/5054C3/General_linear_regression_model.png&quot;</span> alt=<span class="string">&quot;A diagram of the general linear regression model&quot;</span>&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm uninstall hexo-asset-image</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/17/5120C3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/17/5120C3/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W3-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-17 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-17T21:00:00+08:00">2025-09-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-19 20:28:09" itemprop="dateModified" datetime="2025-09-19T20:28:09+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - è®¡ç®—èƒ½æºææ–™å’Œç”µå­ç»“æ„æ¨¡æ‹Ÿ Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="radial-distribution-function">1 radial distribution
function:</h2>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>This whiteboard explains the process of calculating the
<strong>radial distribution function</strong>, often denoted as <span
class="math inline">\(g(r)\)</span>, to analyze the atomic structure of
a material, which is referred to here as a â€œfilmâ€.
æœ¬ç™½æ¿è§£é‡Šäº†è®¡ç®—<strong>å¾„å‘åˆ†å¸ƒå‡½æ•°</strong>ï¼ˆé€šå¸¸è¡¨ç¤ºä¸º <span
class="math inline">\(g(r)\)</span>ï¼‰çš„è¿‡ç¨‹ï¼Œç”¨äºåˆ†æææ–™ï¼ˆæœ¬æ–‡ä¸­ç§°ä¸ºâ€œè–„è†œâ€ï¼‰çš„åŸå­ç»“æ„ã€‚</p>
<p>In simple terms, the radial distribution function tells you the
probability of finding an atom at a certain distance from another
reference atom. Itâ€™s a powerful way to see the local structure in a
disordered system like a liquid or an amorphous solid.</p>
<p>ç®€å•æ¥è¯´ï¼Œå¾„å‘åˆ†å¸ƒå‡½æ•°è¡¨ç¤ºåœ¨è·ç¦»å¦ä¸€ä¸ªå‚è€ƒåŸå­ä¸€å®šè·ç¦»å¤„æ‰¾åˆ°ä¸€ä¸ªåŸå­çš„æ¦‚ç‡ã€‚å®ƒæ˜¯è§‚å¯Ÿæ— åºç³»ç»Ÿï¼ˆä¾‹å¦‚æ¶²ä½“æˆ–éæ™¶æ€å›ºä½“ï¼‰å±€éƒ¨ç»“æ„çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<h3 id="core-concept-radial-distribution-function-å¾„å‘åˆ†å¸ƒå‡½æ•°">## Core
Concept: Radial Distribution Function å¾„å‘åˆ†å¸ƒå‡½æ•°</h3>
<p>The main goal is to compute the radial distribution function, <span
class="math inline">\(g(r)\)</span>, which is defined as the ratio of
the actual number of atoms found in a thin shell at a distance <span
class="math inline">\(r\)</span> to the number of atoms youâ€™d expect to
find if the material were an ideal gas (completely random).
ä¸»è¦ç›®æ ‡æ˜¯è®¡ç®—å¾„å‘åˆ†å¸ƒå‡½æ•° <span
class="math inline">\(g(r)\)</span>ï¼Œå…¶å®šä¹‰ä¸ºåœ¨è·ç¦» <span
class="math inline">\(r\)</span>
çš„è–„å£³å±‚ä¸­å®é™…å‘ç°çš„åŸå­æ•°ä¸ææ–™ä¸ºç†æƒ³æ°”ä½“ï¼ˆå®Œå…¨éšæœºï¼‰æ—¶é¢„æœŸå‘ç°çš„åŸå­æ•°ä¹‹æ¯”ã€‚</p>
<p>The formula is expressed as: <span class="math display">\[g(r)dr =
\frac{n(r)}{\text{ideal gas}}\]</span></p>
<ul>
<li><strong><span class="math inline">\(n(r)\)</span></strong>:
Represents the average number of atoms found in a thin spherical shell
between a distance <span class="math inline">\(r\)</span> and <span
class="math inline">\(r+dr\)</span> from a central atom.
è¡¨ç¤ºè·ç¦»ä¸­å¿ƒåŸå­ <span class="math inline">\(r\)</span> åˆ° <span
class="math inline">\(r+dr\)</span> ä¹‹é—´çš„è–„çƒå£³ä¸­åŸå­çš„å¹³å‡æ•°é‡ã€‚</li>
<li><strong>ideal gas</strong>: Represents the number of atoms you would
expect in that same shell if the atoms were distributed completely
randomly with the same average density (<span
class="math inline">\(\rho\)</span>). The volume of this shell is
approximately <span class="math inline">\(4\pi r^2
dr\)</span>.è¡¨ç¤ºå¦‚æœåŸå­å®Œå…¨éšæœºåˆ†å¸ƒä¸”å¹³å‡å¯†åº¦ (<span
class="math inline">\(\rho\)</span>)
ç›¸åŒï¼Œåˆ™è¯¥çƒå£³ä¸­åŸå­çš„æ•°é‡ã€‚è¯¥çƒå£³çš„ä½“ç§¯çº¦ä¸º <span
class="math inline">\(4\pi r^2 dr\)</span>ã€‚</li>
</ul>
<p>A peak in the <span class="math inline">\(g(r)\)</span> plot
indicates a high probability of finding neighboring atoms at that
specific distance, revealing the materialâ€™s structural shells (e.g.,
nearest neighbors, second-nearest neighbors, etc.).<span
class="math inline">\(g(r)\)</span>
å›¾ä¸­çš„å³°å€¼è¡¨ç¤ºåœ¨è¯¥ç‰¹å®šè·ç¦»å¤„æ‰¾åˆ°ç›¸é‚»åŸå­çš„æ¦‚ç‡å¾ˆé«˜ï¼Œä»è€Œæ­ç¤ºäº†ææ–™çš„ç»“æ„å£³ï¼ˆä¾‹å¦‚ï¼Œæœ€è¿‘é‚»ã€æ¬¡è¿‘é‚»ç­‰ï¼‰ã€‚</p>
<h3 id="calculation-method">## Calculation Method</h3>
<p>The board outlines a two-step averaging process to get a
statistically meaningful result from simulation data (a â€œfilmâ€ at 20
frames per second).</p>
<ol type="1">
<li><p><strong>Average over atoms:</strong> In a single frame (a
snapshot in time), you pick one atom as the center. Then, you count how
many other atoms (<span class="math inline">\(n(r)\)</span>) are in
concentric spherical shells around it. This process is repeated,
treating each atom in the frame as the center, and the results are
averaged.</p></li>
<li><p><strong>Average over frames:</strong> The entire process
described above is repeated for multiple frames from the simulation or
video. This time-averaging ensures that the final result represents the
typical structure of the material over time, smoothing out random
fluctuations.</p></li>
</ol>
<p>The board notes â€œdx = bin width 0.01Ã…â€, which is a practical detail
for the calculation. To create a histogram, the distance <code>r</code>
is divided into small segments (bins) of 0.01 angstroms.</p>
<h3 id="connection-to-experiments">## Connection to Experiments</h3>
<p>Finally, the whiteboard mentions <strong>â€œframe X-ray
scatteringâ€</strong>. This is a crucial point because it connects this
computational analysis to real-world experiments. Experimental
techniques like X-ray or neutron scattering can be used to measure a
quantity called the structure factor, <span
class="math inline">\(S(q)\)</span>, which is directly related to the
radial distribution function <span class="math inline">\(g(r)\)</span>
through a mathematical operation called a Fourier transform. This allows
scientists to directly compare the structure produced in their
simulations with the structure of a real material measured in a lab.
æœ€åï¼Œç™½æ¿ä¸Šæåˆ°äº†<strong>â€œå¸§ X
å°„çº¿æ•£å°„â€</strong>ã€‚è¿™ä¸€ç‚¹è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå°†è®¡ç®—åˆ†æä¸å®é™…å®éªŒè”ç³»èµ·æ¥ã€‚Xå°„çº¿æˆ–ä¸­å­æ•£å°„ç­‰å®éªŒæŠ€æœ¯å¯ä»¥ç”¨æ¥æµ‹é‡ä¸€ä¸ªç§°ä¸ºç»“æ„å› å­<span
class="math inline">\(S(q)\)</span>çš„é‡ï¼Œè¯¥é‡é€šè¿‡å‚…é‡Œå¶å˜æ¢çš„æ•°å­¦è¿ç®—ä¸å¾„å‘åˆ†å¸ƒå‡½æ•°<span
class="math inline">\(g(r)\)</span>ç›´æ¥ç›¸å…³ã€‚è¿™ä½¿å¾—ç§‘å­¦å®¶èƒ½å¤Ÿç›´æ¥å°†æ¨¡æ‹Ÿä¸­äº§ç”Ÿçš„ç»“æ„ä¸å®éªŒå®¤æµ‹é‡çš„çœŸå®ææ–™ç»“æ„è¿›è¡Œæ¯”è¾ƒã€‚</p>
<p>The board correctly links <span class="math inline">\(g(r)\)</span>
to X-ray scattering experiments. The quantity measured in these
experiments is the <strong>static structure factor</strong>, <span
class="math inline">\(S(q)\)</span>, which describes how the material
scatters radiation. The relationship between the two is a Fourier
transform: è¯¥æ¿æ­£ç¡®åœ°å°†<span
class="math inline">\(g(r)\)</span>ä¸Xå°„çº¿æ•£å°„å®éªŒè”ç³»èµ·æ¥ã€‚è¿™äº›å®éªŒä¸­æµ‹é‡çš„é‡æ˜¯<strong>é™æ€ç»“æ„å› å­</strong><span
class="math inline">\(S(q)\)</span>ï¼Œå®ƒæè¿°äº†ææ–™å¦‚ä½•æ•£å°„è¾å°„ã€‚ä¸¤è€…ä¹‹é—´çš„å…³ç³»æ˜¯å‚…é‡Œå¶å˜æ¢ï¼š
<span class="math display">\[S(q) = 1 + 4 \pi \rho \int_0^\infty [g(r) -
1] r^2 \frac{\sin(qr)}{qr} dr\]</span> This equation is crucial because
it bridges the gap between computer simulations (which calculate <span
class="math inline">\(g(r)\)</span>) and physical experiments (which
measure <span class="math inline">\(S(q)\)</span>).
è¿™ä¸ªæ–¹ç¨‹è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¼¥åˆäº†è®¡ç®—æœºæ¨¡æ‹Ÿï¼ˆè®¡ç®— <span
class="math inline">\(g(r)\)</span>ï¼‰å’Œç‰©ç†å®éªŒï¼ˆæµ‹é‡ <span
class="math inline">\(S(q)\)</span>ï¼‰ä¹‹é—´çš„å·®è·ã€‚</p>
<h3
id="the-gaussian-distribution-probability-of-particle-position-é«˜æ–¯åˆ†å¸ƒç²’å­ä½ç½®çš„æ¦‚ç‡">##
2. The Gaussian Distribution: Probability of Particle Position
é«˜æ–¯åˆ†å¸ƒï¼šç²’å­ä½ç½®çš„æ¦‚ç‡</h3>
<p>The board starts with the formula for a one-dimensional
<strong>Gaussian (or normal) distribution</strong>:
ç™½æ¿é¦–å…ˆå±•ç¤ºçš„æ˜¯ä¸€ç»´<strong>é«˜æ–¯ï¼ˆæˆ–æ­£æ€ï¼‰åˆ†å¸ƒ</strong>çš„å…¬å¼ï¼š</p>
<p><span class="math display">\[f(x | \mu, \sigma^2) =
\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</span></p>
<p>This equation describes the probability of finding a particle at a
specific position <code>x</code> after a certain amount of time has
passed. * <strong><span class="math inline">\(\mu\)</span> (mu)</strong>
is the <strong>mean</strong> or average position. For a simple diffusion
process starting at the origin, the particles spread out symmetrically,
so the average position remains at the origin (<span
class="math inline">\(\mu = 0\)</span>). * <strong><span
class="math inline">\(\sigma^2\)</span> (sigma squared)</strong> is the
<strong>variance</strong>, which measures how spread out the particles
are from the mean position. A larger variance means the particles have,
on average, traveled farther from the starting point.
è¿™ä¸ªæ–¹ç¨‹æè¿°äº†ç»è¿‡ä¸€å®šæ—¶é—´åï¼Œåœ¨ç‰¹å®šä½ç½®â€œxâ€æ‰¾åˆ°ç²’å­çš„æ¦‚ç‡ã€‚ *
<strong><span class="math inline">\(\mu\)</span> (mu)</strong>
æ˜¯<strong>å¹³å‡å€¼</strong>æˆ–å¹³å‡ä½ç½®ã€‚å¯¹äºä»åŸç‚¹å¼€å§‹çš„ç®€å•æ‰©æ•£è¿‡ç¨‹ï¼Œç²’å­å¯¹ç§°æ‰©æ•£ï¼Œå› æ­¤å¹³å‡ä½ç½®ä¿æŒåœ¨åŸç‚¹ï¼ˆ<span
class="math inline">\(\mu = 0\)</span>ï¼‰ã€‚ * <strong><span
class="math inline">\(\sigma^2\)</span>ï¼ˆsigma å¹³æ–¹ï¼‰</strong>
æ˜¯<strong>æ–¹å·®</strong>ï¼Œç”¨â€‹â€‹äºè¡¡é‡ç²’å­ä¸å¹³å‡ä½ç½®çš„æ‰©æ•£ç¨‹åº¦ã€‚æ–¹å·®è¶Šå¤§ï¼Œæ„å‘³ç€ç²’å­å¹³å‡è·ç¦»èµ·ç‚¹è¶Šè¿œã€‚</p>
<p>The note â€œBlack-Scholesâ€ is a side reference. The Black-Scholes
model, famous in financial mathematics for pricing options, uses similar
mathematical principles based on Brownian motion to model the random
fluctuations of stock prices. â€œBlack-Scholesâ€æ³¨é‡Šä»…ä¾›å‚è€ƒã€‚Black-Scholes
æ¨¡å‹åœ¨é‡‘èæ•°å­¦ä¸­ä»¥æœŸæƒå®šä»·è€Œé—»åï¼Œå®ƒä½¿ç”¨åŸºäºå¸ƒæœ—è¿åŠ¨çš„ç±»ä¼¼æ•°å­¦åŸç†æ¥æ¨¡æ‹Ÿè‚¡ç¥¨ä»·æ ¼çš„éšæœºæ³¢åŠ¨ã€‚</p>
<h3
id="mean-squared-displacement-msd-quantifying-the-spread-å‡æ–¹ä½ç§»-msdé‡åŒ–æ‰©æ•£">##
3. Mean Squared Displacement (MSD): Quantifying the Spread å‡æ–¹ä½ç§»
(MSD)ï¼šé‡åŒ–æ‰©æ•£</h3>
<p>The core of the board is dedicated to the <strong>Mean Squared
Displacement (MSD)</strong>. This is the primary tool used to measure
how far, on average, particles have moved over a time interval
<code>t</code>. æœ¬ç‰ˆå—çš„æ ¸å¿ƒå†…å®¹æ˜¯<strong>å‡æ–¹ä½ç§»
(MSD)</strong>ã€‚è¿™æ˜¯ç”¨äºæµ‹é‡ç²’å­åœ¨æ—¶é—´é—´éš”â€œtâ€å†…å¹³å‡ç§»åŠ¨è·ç¦»çš„ä¸»è¦å·¥å…·ã€‚</p>
<p>The variance <span class="math inline">\(\sigma^2\)</span> is
formally defined as the average of the squared deviations from the mean:
<span class="math display">\[\sigma^2 = \langle x^2(t) \rangle - \langle
x(t) \rangle^2\]</span> * <span class="math inline">\(\langle x(t)
\rangle\)</span> is the average displacement. As mentioned, for simple
diffusion, <span class="math inline">\(\langle x(t) \rangle =
0\)</span>. * <span class="math inline">\(\langle x^2(t)
\rangle\)</span> is the average of the <em>square</em> of the
displacement. æ–¹å·®<span
class="math inline">\(\sigma^2\)</span>çš„æ­£å¼å®šä¹‰ä¸ºä¸å¹³å‡å€¼åå·®å¹³æ–¹çš„å¹³å‡å€¼ï¼š
<span class="math display">\[\sigma^2 = \langle x^2(t) \rangle - \langle
x(t) \rangle^2\]</span> * <span class="math inline">\(\langle x(t)
\rangle\)</span>æ˜¯å¹³å‡ä½ç§»ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œå¯¹äºç®€å•æ‰©æ•£ï¼Œ<span
class="math inline">\(\langle x(t) \rangle = 0\)</span>ã€‚ * <span
class="math inline">\(\langle x^2(t)
\rangle\)</span>æ˜¯ä½ç§»<em>å¹³æ–¹</em>çš„å¹³å‡å€¼ã€‚</p>
<p>Since <span class="math inline">\(\langle x(t) \rangle = 0\)</span>,
the variance is simply equal to the MSD: <span
class="math display">\[\sigma^2 = \langle x^2(t) \rangle\]</span> ç”±äº
<span class="math inline">\(\langle x(t) \rangle =
0\)</span>ï¼Œæ–¹å·®ç­‰äºå‡æ–¹å·® (MSD)ï¼š <span class="math display">\[\sigma^2
= \langle x^2(t) \rangle\]</span></p>
<p>The crucial insight for a diffusive process is that the <strong>MSD
grows linearly with time</strong>. The rate of this growth is determined
by the <strong>diffusion coefficient, D</strong>. The board shows this
relationship for different dimensions: æ‰©æ•£è¿‡ç¨‹çš„å…³é”®åœ¨äº<strong>MSD
éšæ—¶é—´çº¿æ€§å¢é•¿</strong>ã€‚å…¶å¢é•¿ç‡ç”±<strong>æ‰©æ•£ç³»æ•°
D</strong>å†³å®šã€‚æ£‹ç›˜æ˜¾ç¤ºäº†ä¸åŒç»´åº¦ä¸‹çš„è¿™ç§å…³ç³»ï¼š</p>
<ul>
<li><strong>1D:</strong> <span class="math inline">\(\langle x^2(t)
\rangle = 2Dt\)</span> (Movement along a line) ï¼ˆæ²¿ç›´çº¿è¿åŠ¨ï¼‰</li>
<li><strong>2D:</strong> The board has a slight typo or ambiguity with
<span class="math inline">\(\langle z^2(t) \rangle = 2Dt\)</span>. For
2D motion in the x-y plane, the total MSD would be <span
class="math inline">\(\langle r^2(t) \rangle = \langle x^2(t) \rangle +
\langle y^2(t) \rangle = 4Dt\)</span>. The note on the board might be
referring to just one component of motion. **æ£‹ç›˜ä¸Šçš„ <span
class="math inline">\(\langle z^2(t) \rangle = 2Dt\)</span>
å­˜åœ¨è½»å¾®æ‹¼å†™é”™è¯¯æˆ–æ­§ä¹‰ã€‚å¯¹äº x-y å¹³é¢ä¸Šçš„äºŒç»´è¿åŠ¨ï¼Œæ€»å¹³å‡æ•£å°„å·® (MSD) ä¸º
<span class="math inline">\(\langle r^2(t) \rangle = \langle x^2(t)
\rangle + \langle y^2(t) \rangle =
4Dt\)</span>ã€‚é»‘æ¿ä¸Šçš„æ³¨é‡Šå¯èƒ½ä»…æŒ‡è¿åŠ¨çš„ä¸€ä¸ªåˆ†é‡ã€‚</li>
<li><strong>3D:</strong> <span class="math inline">\(\langle r^2(t)
\rangle = \langle |\vec{r}(t) - \vec{r}(0)|^2 \rangle = 6Dt\)</span>
(Movement in 3D space, which is the most common case in molecular
simulations) ï¼ˆä¸‰ç»´ç©ºé—´ä¸­çš„è¿åŠ¨ï¼Œè¿™æ˜¯åˆ†å­æ¨¡æ‹Ÿä¸­æœ€å¸¸è§çš„æƒ…å†µï¼‰ Here,
<span class="math inline">\(\vec{r}(t)\)</span> is the position vector
of a particle at time <code>t</code>. The quantity <span
class="math inline">\(\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle\)</span> is the average of the squared distance a particle has
traveled from its initial position <span
class="math inline">\(\vec{r}(0)\)</span>. è¿™é‡Œï¼Œ<span
class="math inline">\(\vec{r}(t)\)</span> æ˜¯ç²’å­åœ¨æ—¶é—´ <code>t</code>
çš„ä½ç½®çŸ¢é‡ã€‚ <span class="math inline">\(\langle |\vec{r}(t) -
\vec{r}(0)|^2 \rangle\)</span> æ˜¯ç²’å­ä»å…¶åˆå§‹ä½ç½® <span
class="math inline">\(\vec{r}(0)\)</span> è¡Œè¿›è·ç¦»çš„å¹³æ–¹å¹³å‡å€¼ã€‚</li>
</ul>
<h3
id="the-einstein-relation-connecting-microscopic-motion-to-a-macroscopic-property-çˆ±å› æ–¯å¦å…³ç³»å°†å¾®è§‚è¿åŠ¨ä¸å®è§‚ç‰¹æ€§è”ç³»èµ·æ¥">##
4. The Einstein Relation: Connecting Microscopic Motion to a Macroscopic
Property çˆ±å› æ–¯å¦å…³ç³»ï¼šå°†å¾®è§‚è¿åŠ¨ä¸å®è§‚ç‰¹æ€§è”ç³»èµ·æ¥</h3>
<p>Finally, the board presents the famous <strong>Einstein
relation</strong>, which rearranges the 3D MSD equation to solve for the
diffusion coefficient <code>D</code>:</p>
<p><span class="math display">\[D = \lim_{t \to \infty} \frac{\langle
|\vec{r}(t) - \vec{r}(0)|^2 \rangle}{6t}\]</span></p>
<p>This is a cornerstone equation in statistical mechanics. It provides
a practical way to calculate a macroscopic propertyâ€”the
<strong>diffusion coefficient <code>D</code></strong>â€”from the
microscopic movements of individual particles observed in a computer
simulation.
è¿™æ˜¯ç»Ÿè®¡åŠ›å­¦ä¸­çš„ä¸€ä¸ªåŸºçŸ³æ–¹ç¨‹ã€‚å®ƒæä¾›äº†ä¸€ç§å®ç”¨çš„æ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—æœºæ¨¡æ‹Ÿä¸­è§‚å¯Ÿåˆ°çš„å•ä¸ªç²’å­çš„å¾®è§‚è¿åŠ¨æ¥è®¡ç®—å®è§‚å±æ€§â€”â€”æ‰©æ•£ç³»æ•°â€œDâ€ã€‚</p>
<p>In practice, one would: 1. Run a simulation of particles.
è¿è¡Œç²’å­æ¨¡æ‹Ÿã€‚ 2. Track the position of each particle over time.
è·Ÿè¸ªæ¯ä¸ªç²’å­éšæ—¶é—´çš„ä½ç½®ã€‚ 3. Calculate the squared displacement <span
class="math inline">\(|\vec{r}(t) - \vec{r}(0)|^2\)</span> for each
particle at various time intervals <code>t</code>.
è®¡ç®—æ¯ä¸ªç²’å­åœ¨ä¸åŒæ—¶é—´é—´éš”â€œtâ€çš„ä½ç§»å¹³æ–¹<span
class="math inline">\(|\vec{r}(t) - \vec{r}(0)|^2\)</span>ã€‚ 4. Average
this value over all particles to get the MSD, <span
class="math inline">\(\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle\)</span>. å¯¹æ‰€æœ‰ç²’å­å–å¹³å‡å€¼ï¼Œå¾—åˆ°å‡æ–¹å·®ï¼ˆMSDï¼‰ï¼Œå³<span
class="math inline">\(\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle\)</span>ã€‚ 5. Plot the MSD as a function of time.
å°†MSDç»˜åˆ¶æˆæ—¶é—´å‡½æ•°ã€‚ 6. The slope of this line, divided by 6, gives the
diffusion coefficient <code>D</code>. The <code>lim tâ†’âˆ</code> indicates
that this linear relationship is most accurate for long time scales,
after initial transient effects have died down.
è¿™æ¡ç›´çº¿çš„æ–œç‡é™¤ä»¥6ï¼Œå³æ‰©æ•£ç³»æ•°â€œDâ€ã€‚â€œlim
tâ†’âˆâ€è¡¨æ˜ï¼Œåœ¨åˆå§‹ç¬æ€æ•ˆåº”æ¶ˆé€€åï¼Œè¿™ç§çº¿æ€§å…³ç³»åœ¨é•¿æ—¶é—´å°ºåº¦ä¸Šæœ€ä¸ºå‡†ç¡®ã€‚</p>
<h3 id="right-board-green-kubo-relations">## 5. Right Board: Green-Kubo
Relations</h3>
<p>This board introduces a more advanced and powerful method to
calculate transport coefficients like the diffusion coefficient, known
as the <strong>Green-Kubo relations</strong>.
æœ¬é¢æ¿ä»‹ç»äº†ä¸€ç§æ›´å…ˆè¿›ã€æ›´å¼ºå¤§çš„æ–¹æ³•æ¥è®¡ç®—æ‰©æ•£ç³»æ•°ç­‰ä¼ è¾“ç³»æ•°ï¼Œå³<strong>Green-Kubo
å…³ç³»</strong>ã€‚</p>
<h4 id="velocity-autocorrelation-function-vacf-é€Ÿåº¦è‡ªç›¸å…³å‡½æ•°-vacf">###
<strong>Velocity Autocorrelation Function (VACF)</strong> é€Ÿåº¦è‡ªç›¸å…³å‡½æ•°
(VACF)</h4>
<p>The key idea is to look at how a particleâ€™s velocity at one point in
time is related to its velocity at a later time. This is measured by the
<strong>Velocity Autocorrelation Function (VACF)</strong>: <span
class="math display">\[C_{vv}(t) = \langle \vec{v}(t&#39;) \cdot
\vec{v}(t&#39; + t) \rangle\]</span> This function tells us how long a
particle â€œremembersâ€ its velocity. For a typical liquid, the velocity is
quickly randomized by collisions, so the VACF decays to zero rapidly.
å…¶æ ¸å¿ƒæ€æƒ³æ˜¯è€ƒå¯Ÿç²’å­åœ¨æŸä¸€æ—¶é—´ç‚¹çš„é€Ÿåº¦ä¸å…¶åœ¨ä¹‹åæ—¶é—´ç‚¹çš„é€Ÿåº¦ä¹‹é—´çš„å…³ç³»ã€‚è¿™å¯ä»¥é€šè¿‡<strong>é€Ÿåº¦è‡ªç›¸å…³å‡½æ•°
(VACF)</strong>æ¥æµ‹é‡ï¼š <span class="math display">\[C_{vv}(t) = \langle
\vec{v}(t&#39;) \cdot \vec{v}(t&#39; + t) \rangle\]</span>
æ­¤å‡½æ•°å‘Šè¯‰æˆ‘ä»¬ç²’å­â€œè®°ä½â€å…¶é€Ÿåº¦çš„æ—¶é—´ã€‚å¯¹äºå…¸å‹çš„æ¶²ä½“ï¼Œé€Ÿåº¦ä¼šå› ç¢°æ’è€Œè¿…é€ŸéšæœºåŒ–ï¼Œå› æ­¤
VACF ä¼šè¿…é€Ÿè¡°å‡ä¸ºé›¶ã€‚</p>
<h4 id="connecting-msd-and-vacf">### <strong>Connecting MSD and
VACF</strong></h4>
<p>The board shows the mathematical link between the MSD and the VACF.
Starting with the definition of position as the integral of velocity,
<span class="math inline">\(\vec{r}(t) = \int_0^t \vec{v}(t&#39;)
dt&#39;\)</span>, one can show that the MSD is a double integral of the
VACF. The board writes this as: <span class="math display">\[\langle
x^2(t) \rangle = \left\langle \left( \int_0^t v(t&#39;) dt&#39; \right)
\left( \int_0^t v(t&#39;&#39;) dt&#39;&#39; \right) \right\rangle =
\int_0^t dt&#39; \int_0^t dt&#39;&#39; \langle v(t&#39;) v(t&#39;&#39;)
\rangle\]</span> This shows that the two pictures of motionâ€”the
particleâ€™s displacement (MSD) and its velocity fluctuations (VACF)â€”are
deeply connected. è¯¥é¢æ¿å±•ç¤ºäº† MSD å’Œ VACF
ä¹‹é—´çš„æ•°å­¦è”ç³»ã€‚ä»ä½ç½®å®šä¹‰ä¸ºé€Ÿåº¦çš„ç§¯åˆ†å¼€å§‹ï¼Œ<span
class="math inline">\(\vec{r}(t) = \int_0^t \vec{v}(t&#39;)
dt&#39;\)</span>ï¼Œå¯ä»¥è¯æ˜ MSD æ˜¯ VACF çš„äºŒé‡ç§¯åˆ†ã€‚é»‘æ¿ä¸Šå†™ç€ï¼š <span
class="math display">\[\langle x^2(t) \rangle = \left\langle \left(
\int_0^t v(t&#39;) dt&#39; \right) \left( \int_0^t v(t&#39;&#39;)
dt&#39;&#39; \right) \right\rangle = \int_0^t dt&#39; \int_0^t
dt&#39;&#39; \langle v(t&#39;) v(t&#39;&#39;) \rangle\]</span>
è¿™è¡¨æ˜ï¼Œç²’å­è¿åŠ¨çš„ä¸¤å¹…å›¾åƒâ€”â€”ç²’å­çš„ä½ç§»ï¼ˆMSDï¼‰å’Œé€Ÿåº¦æ¶¨è½ï¼ˆVACFï¼‰â€”â€”ä¹‹é—´å­˜åœ¨ç€æ·±åˆ»çš„è”ç³»ã€‚</p>
<h4 id="the-green-kubo-formula-for-diffusion-æ‰©æ•£çš„æ ¼æ—-ä¹…ä¿å…¬å¼">###
<strong>The Green-Kubo Formula for Diffusion
æ‰©æ•£çš„æ ¼æ—-ä¹…ä¿å…¬å¼</strong></h4>
<p>By combining the Einstein relation with the integral of the VACF, one
arrives at the Green-Kubo formula for the diffusion coefficient: <span
class="math display">\[D = \frac{1}{3} \int_0^\infty \langle \vec{v}(0)
\cdot \vec{v}(t) \rangle dt\]</span> This incredible result states that
the <strong>macroscopic</strong> property of diffusion (<span
class="math inline">\(D\)</span>) is determined by the integral of the
<strong>microscopic</strong> velocity correlations. Itâ€™s often a more
efficient way to compute <span class="math inline">\(D\)</span> in
simulations than calculating the long-time limit of the MSD.
å°†çˆ±å› æ–¯å¦å…³ç³»ä¸VACFç§¯åˆ†ç›¸ç»“åˆï¼Œå¯ä»¥å¾—åˆ°æ‰©æ•£ç³»æ•°çš„æ ¼æ—-ä¹…ä¿å…¬å¼ï¼š <span
class="math display">\[D = \frac{1}{3} \int_0^\infty \langle \vec{v}(0)
\cdot \vec{v}(t) \rangle dt\]</span>
è¿™ä¸ªä»¤äººéš¾ä»¥ç½®ä¿¡çš„ç»“æœè¡¨æ˜ï¼Œæ‰©æ•£çš„<strong>å®è§‚</strong>ç‰¹æ€§ï¼ˆ<span
class="math inline">\(D\)</span>ï¼‰ç”±<strong>å¾®è§‚</strong>é€Ÿåº¦å…³è”çš„ç§¯åˆ†å†³å®šã€‚åœ¨æ¨¡æ‹Ÿä¸­ï¼Œè¿™é€šå¸¸æ˜¯è®¡ç®—<span
class="math inline">\(D\)</span>æ¯”è®¡ç®—MSDçš„é•¿æœŸæé™æ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚</p>
<h3 id="the-grand-narrative-from-micro-to-macro-å®å¤§å™äº‹ä»å¾®è§‚åˆ°å®è§‚">##
6. The Grand Narrative: From Micro to Macro å®å¤§å™äº‹ï¼šä»å¾®è§‚åˆ°å®è§‚</h3>
<p>The previous whiteboards gave us two ways to calculate the
<strong>diffusion constant, D</strong>, from the microscopic random walk
of individual atoms:
ä¹‹å‰çš„ç™½æ¿æä¾›äº†ä¸¤ç§ä»å•ä¸ªåŸå­çš„å¾®è§‚éšæœºæ¸¸åŠ¨è®¡ç®—<strong>æ‰©æ•£å¸¸æ•°
D</strong>çš„æ–¹æ³•ï¼š 1. <strong>Einstein Relation:</strong> From the
long-term slope of the Mean Squared Displacement (MSD). æ ¹æ®å‡æ–¹ä½ç§»
(MSD) çš„é•¿æœŸæ–œç‡ã€‚ 2. <strong>Green-Kubo Relation:</strong> From the
integral of the Velocity Autocorrelation Function (VACF).
æ ¹æ®é€Ÿåº¦è‡ªç›¸å…³å‡½æ•° (VACF) çš„ç§¯åˆ†ã€‚</p>
<p>This new whiteboard shows how that single microscopic parameter,
<code>D</code>, governs the large-scale, observable process of diffusion
described by <strong>Fickâ€™s Laws</strong> and the <strong>Diffusion
Equation</strong>. è¿™å—æ–°çš„ç™½æ¿å±•ç¤ºäº†å•ä¸ªå¾®è§‚å‚æ•° <code>D</code>
å¦‚ä½•æ§åˆ¶<strong>è²å…‹å®šå¾‹</strong>å’Œ<strong>æ‰©æ•£æ–¹ç¨‹</strong>æ‰€æè¿°çš„å¤§è§„æ¨¡å¯è§‚æµ‹æ‰©æ•£è¿‡ç¨‹ã€‚</p>
<h3 id="the-starting-point-a-liquids-structure-èµ·ç‚¹æ¶²ä½“çš„ç»“æ„">## 1. The
Starting Point: A Liquidâ€™s Structure èµ·ç‚¹ï¼šæ¶²ä½“çš„ç»“æ„</h3>
<p>The plot on the top left is the <strong>Radial Distribution Function,
<span class="math inline">\(g(r)\)</span></strong>, which we discussed
in detail from the first whiteboard. å·¦ä¸Šè§’çš„å›¾æ˜¯<strong>å¾„å‘åˆ†å¸ƒå‡½æ•°
<span
class="math inline">\(g(r)\)</span></strong>ï¼Œæˆ‘ä»¬åœ¨ç¬¬ä¸€ä¸ªç™½æ¿ä¸Šè¯¦ç»†è®¨è®ºè¿‡å®ƒã€‚</p>
<ul>
<li><strong>The Plot:</strong> It shows the characteristic structure of
a liquid. The peaks are labeled â€œ1stâ€, â€œ2ndâ€, and â€œ3rdâ€, corresponding
to the first, second, and third <strong>solvation shells</strong>
(layers of neighboring atoms).
å®ƒæ˜¾ç¤ºäº†æ¶²ä½“çš„ç‰¹å¾ç»“æ„ã€‚å³°åˆ†åˆ«æ ‡è®°ä¸ºâ€œç¬¬ä¸€â€ã€â€œç¬¬äºŒâ€å’Œâ€œç¬¬ä¸‰â€ï¼Œåˆ†åˆ«å¯¹åº”äºç¬¬ä¸€ã€ç¬¬äºŒå’Œç¬¬ä¸‰<strong>æº¶å‰‚åŒ–å£³å±‚</strong>ï¼ˆç›¸é‚»åŸå­å±‚ï¼‰ã€‚</li>
<li><strong>The Limit:</strong> The note <code>lim râ†’âˆ g(r) = 1</code>
confirms that at large distances, the liquid has no long-range order, as
expected.æ³¨é‡Šâ€œlim râ†’âˆ g(r) =
1â€è¯å®äº†åœ¨è¿œè·ç¦»ä¸‹ï¼Œæ¶²ä½“æ²¡æœ‰é•¿ç¨‹æœ‰åºï¼Œè¿™ä¸é¢„æœŸä¸€è‡´ã€‚</li>
<li><strong>System Parameters:</strong> The values <code>T = 0.71</code>
and <code>Ï = 0.844</code> are the temperature and density of the
simulated system (likely in reduced or â€œLennard-Jonesâ€ units) for which
this <span class="math inline">\(g(r)\)</span> was calculated. å€¼â€œT =
0.71â€å’Œâ€œÏ =
0.844â€åˆ†åˆ«æ˜¯æ¨¡æ‹Ÿç³»ç»Ÿçš„æ¸©åº¦å’Œå¯†åº¦ï¼ˆå¯èƒ½é‡‡ç”¨çº¦åŒ–æˆ–â€œLennard-Jonesâ€å•ä½ï¼‰ï¼Œç”¨äºè®¡ç®—æ­¤
<span class="math inline">\(g(r)\)</span>ã€‚</li>
</ul>
<p>This section sets the stage: we are looking at the dynamics within a
system that has this specific liquid-like structure.
æœ¬èŠ‚å¥ å®šäº†åŸºç¡€ï¼šæˆ‘ä»¬å°†ç ”ç©¶å…·æœ‰ç‰¹å®šç±»æ¶²ä½“ç»“æ„çš„ç³»ç»Ÿå†…çš„åŠ¨åŠ›å­¦ã€‚</p>
<h3 id="the-macroscopic-laws-of-diffusion-å®è§‚æ‰©æ•£å®šå¾‹">## 2. The
Macroscopic Laws of Diffusion å®è§‚æ‰©æ•£å®šå¾‹</h3>
<p>The bottom-left and top-right sections introduce the continuum
equations that describe how concentration changes in space and time.
å·¦ä¸‹è§’å’Œå³ä¸Šè§’éƒ¨åˆ†ä»‹ç»äº†æè¿°æµ“åº¦éšç©ºé—´å’Œæ—¶é—´å˜åŒ–çš„è¿ç»­æ–¹ç¨‹ã€‚å·¦ä¸‹è§’å’Œå³ä¸Šè§’éƒ¨åˆ†ä»‹ç»äº†æè¿°æµ“åº¦éšç©ºé—´å’Œæ—¶é—´å˜åŒ–çš„è¿ç»­æ–¹ç¨‹ã€‚</p>
<h4 id="ficks-first-law-è²å…‹ç¬¬ä¸€å®šå¾‹">### <strong>Fickâ€™s First Law
è²å…‹ç¬¬ä¸€å®šå¾‹</strong></h4>
<p><span class="math display">\[\vec{J} = -D \nabla C\]</span> This is
Fickâ€™s first law of diffusion. It states that there is a
<strong>flux</strong> of particles (<span
class="math inline">\(\vec{J}\)</span>), meaning a net flow. This flow
is directed from high concentration to low concentration (hence the
minus sign) and its magnitude is proportional to the
<strong>concentration gradient</strong> (<span
class="math inline">\(\nabla C\)</span>).
è¿™æ˜¯è²å…‹ç¬¬ä¸€æ‰©æ•£å®šå¾‹ã€‚å®ƒæŒ‡å‡ºå­˜åœ¨ç²’å­çš„<strong>é€šé‡</strong> (<span
class="math inline">\(\vec{J}\)</span>)ï¼Œå³å‡€æµé‡ã€‚è¯¥æµé‡ä»é«˜æµ“åº¦æµå‘ä½æµ“åº¦ï¼ˆå› æ­¤å¸¦æœ‰è´Ÿå·ï¼‰ï¼Œå…¶å¤§å°ä¸<strong>æµ“åº¦æ¢¯åº¦</strong>
(<span class="math inline">\(\nabla C\)</span>) æˆæ­£æ¯”ã€‚</p>
<p><strong>The Crucial Link:</strong> The proportionality constant is
<strong>D</strong>, the very same <strong>diffusion constant</strong> we
calculated from the microscopic random walk (MSD/VACF). This is the key
connection: the collective result of countless individual random walks
is a predictable net flow of particles.
æ¯”ä¾‹å¸¸æ•°æ˜¯<strong>D</strong>ï¼Œä¸æˆ‘ä»¬æ ¹æ®å¾®è§‚éšæœºæ¸¸èµ° (MSD/VACF)
è®¡ç®—å‡ºçš„<strong>æ‰©æ•£å¸¸æ•°</strong>å®Œå…¨ç›¸åŒã€‚è¿™æ˜¯å…³é”®çš„è”ç³»ï¼šæ— æ•°ä¸ªä½“éšæœºæ¸¸åŠ¨çš„é›†åˆç»“æœæ˜¯å¯é¢„æµ‹çš„ç²’å­å‡€æµã€‚</p>
<h4
id="the-diffusion-equation-ficks-second-law-æ‰©æ•£æ–¹ç¨‹è²å…‹ç¬¬äºŒå®šå¾‹">###
<strong>The Diffusion Equation (Fickâ€™s Second Law)
æ‰©æ•£æ–¹ç¨‹ï¼ˆè²å…‹ç¬¬äºŒå®šå¾‹ï¼‰</strong></h4>
<p><span class="math display">\[\frac{\partial C(\vec{r},t)}{\partial t}
= D \nabla^2 C(\vec{r},t)\]</span> This is the <strong>diffusion
equation</strong>, one of the most important equations in physics and
chemistry (also called the heat equation, as noted). Itâ€™s derived from
Fickâ€™s first law and the principle of mass conservation (<span
class="math inline">\(\frac{\partial C}{\partial t} + \nabla \cdot
\vec{J} = 0\)</span>). Itâ€™s a differential equation that tells you
exactly how the concentration at any point, <span
class="math inline">\(C(\vec{r},t)\)</span>, will change over time.
è¿™å°±æ˜¯<strong>æ‰©æ•£æ–¹ç¨‹</strong>ï¼Œå®ƒæ˜¯ç‰©ç†å­¦å’ŒåŒ–å­¦ä¸­æœ€é‡è¦çš„æ–¹ç¨‹ä¹‹ä¸€ï¼ˆä¹Ÿç§°ä¸ºçƒ­æ–¹ç¨‹ï¼‰ã€‚å®ƒæºäºè²å…‹ç¬¬ä¸€å®šå¾‹å’Œè´¨é‡å®ˆæ’å®šå¾‹ï¼ˆ<span
class="math inline">\(\frac{\partial C}{\partial t} + \nabla \cdot
\vec{J} = 0\)</span>ï¼‰ã€‚å®ƒæ˜¯ä¸€ä¸ªå¾®åˆ†æ–¹ç¨‹ï¼Œå¯ä»¥ç²¾ç¡®åœ°å‘Šè¯‰ä½ ä»»æ„ä¸€ç‚¹çš„æµ“åº¦
<span class="math inline">\(C(\vec{r},t)\)</span> éšæ—¶é—´çš„å˜åŒ–ã€‚</p>
<h3
id="the-solution-connecting-back-to-the-random-walk-ä¸éšæœºæ¸¸åŠ¨è”ç³»èµ·æ¥">##
3. The Solution: Connecting Back to the Random Walk
ä¸éšæœºæ¸¸åŠ¨è”ç³»èµ·æ¥</h3>
<p>This is the most beautiful part. The board shows the solution to the
diffusion equation for a very specific scenario, linking the macroscopic
equation directly back to the microscopic random walk.
é»‘æ¿ä¸Šå±•ç¤ºäº†ä¸€ä¸ªéå¸¸å…·ä½“åœºæ™¯ä¸‹æ‰©æ•£æ–¹ç¨‹çš„è§£ï¼Œå°†å®è§‚æ–¹ç¨‹ç›´æ¥ä¸å¾®è§‚éšæœºæ¸¸åŠ¨è”ç³»èµ·æ¥ã€‚</p>
<h4 id="the-initial-condition-åˆå§‹æ¡ä»¶">### <strong>The Initial
Condition åˆå§‹æ¡ä»¶</strong></h4>
<p>The problem is set up by assuming all particles start at a single
point at time zero: <span class="math display">\[C(\vec{r}, 0) =
\delta(\vec{r})\]</span> This is a <strong>Dirac delta
function</strong>, representing an infinitely concentrated point source
at the origin. é—®é¢˜å‡è®¾æ‰€æœ‰ç²’å­åœ¨æ—¶é—´é›¶ç‚¹å¤„ä»ä¸€ä¸ªç‚¹å¼€å§‹ï¼š <span
class="math display">\[C(\vec{r}, 0) = \delta(\vec{r})\]</span>
è¿™æ˜¯ä¸€ä¸ª<strong>ç‹„æ‹‰å…‹å‡½æ•°</strong>ï¼Œè¡¨ç¤ºä¸€ä¸ªåœ¨åŸç‚¹å¤„æ— é™é›†ä¸­çš„ç‚¹æºã€‚</p>
<h4 id="the-fundamental-solution-greens-function-åŸºæœ¬è§£æ ¼æ—å‡½æ•°">###
<strong>The Fundamental Solution (Greenâ€™s Function)
åŸºæœ¬è§£ï¼ˆæ ¼æ—å‡½æ•°ï¼‰</strong></h4>
<p>The solution to the diffusion equation with this starting condition
is called the <strong>fundamental solution</strong> or <strong>Greenâ€™s
function</strong>. For one dimension, it is: <span
class="math display">\[C(x,t) = \frac{1}{\sqrt{4\pi Dt}}
\exp\left(-\frac{x^2}{4Dt}\right)\]</span></p>
<p><strong>The â€œAha!â€ Moment:</strong> This is a <strong>Gaussian
distribution</strong>. Letâ€™s compare it to the formula from the second
whiteboard: * The mean is <span class="math inline">\(\mu=0\)</span>.
å‡å€¼ä¸º <span class="math inline">\(\mu=0\)</span>ã€‚ * The variance is
<span class="math inline">\(\sigma^2 = 2Dt\)</span>. æ–¹å·®ä¸º <span
class="math inline">\(\sigma^2 = 2Dt\)</span>ã€‚</p>
<p>This is an incredible result. The macroscopic diffusion equation
predicts that a concentration pulse will spread out over time, and the
shape of the concentration profile will be a Gaussian curve. The width
of this curve, measured by its variance <span
class="math inline">\(\sigma^2\)</span>, is <strong>exactly the Mean
Squared Displacement, <span class="math inline">\(\langle x^2(t)
\rangle\)</span>, of the individual random-walking particles.</strong>
å®è§‚æ‰©æ•£æ–¹ç¨‹é¢„æµ‹æµ“åº¦è„‰å†²ä¼šéšæ—¶é—´æ‰©æ•£ï¼Œæµ“åº¦åˆ†å¸ƒçš„å½¢çŠ¶å°†æ˜¯é«˜æ–¯æ›²çº¿ã€‚è¿™æ¡æ›²çº¿çš„å®½åº¦ï¼Œç”¨å…¶æ–¹å·®
<span class="math inline">\(\sigma^2\)</span>
æ¥è¡¡é‡ï¼Œ<strong>æ°å¥½æ˜¯å•ä¸ªéšæœºæ¸¸åŠ¨ç²’å­çš„å‡æ–¹ä½ç§» <span
class="math inline">\(\langle x^2(t) \rangle\)</span>ã€‚</strong></p>
<p>This perfectly unites the two perspectives: * <strong>Microscopicå¾®è§‚
(Board 2):</strong> Particles undergo a random walk, and their average
squared displacement from the origin grows as <span
class="math inline">\(\langle x^2(t) \rangle = 2Dt\)</span>.
ç²’å­è¿›è¡Œéšæœºæ¸¸åŠ¨ï¼Œå®ƒä»¬ç›¸å¯¹äºåŸç‚¹çš„å¹³å‡å¹³æ–¹ä½ç§»éšç€ <span
class="math inline">\(\langle x^2(t) \rangle = 2Dt\)</span>
çš„å¢é•¿è€Œå¢é•¿ã€‚ * <strong>Macroscopicå®è§‚ (This Board):</strong> A
collection of these particles, described by a continuum concentration
<code>C</code>, spreads out in a Gaussian profile whose variance is
<span class="math inline">\(\sigma^2 = 2Dt\)</span>.
è¿™äº›ç²’å­çš„é›†åˆï¼Œç”¨è¿ç»­æµ“åº¦â€œCâ€æ¥æè¿°ï¼Œå‘ˆæ–¹å·®ä¸º <span
class="math inline">\(\sigma^2 = 2Dt\)</span> çš„é«˜æ–¯åˆ†å¸ƒã€‚</p>
<p>The two pictures are mathematically identical.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/17/5120C3-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/17/5120C3-2/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W3-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-17 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-17T21:00:00+08:00">2025-09-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-21 05:21:00" itemprop="dateModified" datetime="2025-09-21T05:21:00+08:00">2025-09-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - è®¡ç®—èƒ½æºææ–™å’Œç”µå­ç»“æ„æ¨¡æ‹Ÿ Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="radial-distribution-function-rdfé™æ€ç»“æ„">1 radial distribution
function RDFé™æ€ç»“æ„:</h2>
<ul>
<li><strong>å†…å®¹</strong>: This whiteboard serves as an excellent
summary, pulling together all the key concepts weâ€™ve discussed into a
single, cohesive picture. Letâ€™s connect everything on this slide to our
detailed conversation.</li>
</ul>
<h3 id="rdf-the-static-structure-rdfé™æ€ç»“æ„">1. RDF: The Static
Structure RDFé™æ€ç»“æ„</h3>
<p>On the top left, you see <strong>RDF (Radial Distribution
Function)</strong>.</p>
<ul>
<li><strong>The Plots:</strong> The board shows the familiar <span
class="math inline">\(g(r)\)</span> plot with its characteristic peaks
for a liquid. Below it is a plot of the interatomic potential energy,
<span class="math inline">\(V(r)\)</span>. This addition is very
insightful! It shows <em>why</em> the first peak in <span
class="math inline">\(g(r)\)</span> exists: it corresponds to the
minimum energy distance (<span class="math inline">\(\sigma\)</span>)
where particles are most stable and likely to be found.
ç™½æ¿å±•ç¤ºäº†æˆ‘ä»¬ç†Ÿæ‚‰çš„<span
class="math inline">\(g(r)\)</span>å›¾ï¼Œå®ƒå¸¦æœ‰æ¶²ä½“çš„ç‰¹å¾å³°ã€‚ä¸‹æ–¹æ˜¯åŸå­é—´åŠ¿èƒ½<span
class="math inline">\(V(r)\)</span>çš„å›¾ã€‚è¿™ä¸ªè¡¥å……éå¸¸æœ‰è§åœ°ï¼å®ƒè§£é‡Šäº†ä¸ºä»€ä¹ˆ
<span class="math inline">\(g(r)\)</span>
ä¸­çš„ç¬¬ä¸€ä¸ªå³°å€¼å­˜åœ¨ï¼šå®ƒå¯¹åº”äºç²’å­æœ€ç¨³å®šä¸”æœ€æœ‰å¯èƒ½è¢«å‘ç°çš„æœ€å°èƒ½é‡è·ç¦»
(<span class="math inline">\(\sigma\)</span>)ã€‚</li>
<li><strong>Connection:</strong> This section summarizes our first
discussion. Itâ€™s the starting point for our analysisâ€”a static snapshot
of the materialâ€™s average atomic arrangement before we consider how the
atoms move.
æœ¬èŠ‚æ€»ç»“äº†æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªè®¨è®ºã€‚è¿™æ˜¯æˆ‘ä»¬åˆ†æçš„èµ·ç‚¹â€”â€”åœ¨æˆ‘ä»¬è€ƒè™‘åŸå­å¦‚ä½•è¿åŠ¨ä¹‹å‰ï¼Œå®ƒæ˜¯ææ–™å¹³å‡åŸå­æ’åˆ—çš„é™æ€å¿«ç…§ã€‚</li>
</ul>
<h3
id="msd-and-the-einstein-relation-the-displacement-picture-å‡æ–¹ä½ç§»-msd-å’Œçˆ±å› æ–¯å¦å…³ç³»ä½ç§»å›¾åƒ">2.
MSD and The Einstein Relation: The Displacement Picture å‡æ–¹ä½ç§» (MSD)
å’Œçˆ±å› æ–¯å¦å…³ç³»ï¼šä½ç§»å›¾åƒ</h3>
<p>The board then moves to dynamics, presenting two methods to calculate
the <strong>diffusion constant, D</strong>. The first is the
<strong>Einstein relation</strong>. ä¸¤ç§è®¡ç®—<strong>æ‰©æ•£å¸¸æ•°
D</strong>çš„æ–¹æ³•ã€‚ç¬¬ä¸€ç§æ˜¯<strong>çˆ±å› æ–¯å¦å…³ç³»</strong>ã€‚</p>
<ul>
<li><strong>The Formula:</strong> It correctly states that the Mean
Squared Displacement (MSD), <span class="math inline">\(\langle r^2
\rangle\)</span>, is equal to <span class="math inline">\(6Dt\)</span>
in three dimensions. It then rearranges this to solve for <span
class="math inline">\(D\)</span>: å®ƒæ­£ç¡®åœ°æŒ‡å‡ºäº†å‡æ–¹ä½ç§» (MSD)ï¼Œ<span
class="math inline">\(\langle r^2 \rangle\)</span>ï¼Œåœ¨ä¸‰ç»´ç©ºé—´ä¸­ç­‰äº
<span class="math inline">\(6Dt\)</span>ã€‚ç„¶åé‡æ–°æ’åˆ—è¯¥å…¬å¼ä»¥æ±‚è§£ <span
class="math inline">\(D\)</span>ï¼š <span class="math display">\[D =
\lim_{t\to\infty} \frac{\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle}{6t}\]</span></li>
<li><strong>The Diagram:</strong> The central diagram beautifully
illustrates the concept. It shows a particle in a simulation box (with
â€œN=108â€ likely being the number of particles simulated) moving from an
initial position <span class="math inline">\(\vec{r}_i(0)\)</span> to a
final position <span class="math inline">\(\vec{r}_i(t_j)\)</span>. The
MSD is the average of the square of this displacement over all particles
and many time origins. The graph labeled â€œMSDâ€ shows how you would plot
this data and find the slope (â€œfittingâ€) to calculate <span
class="math inline">\(D\)</span>.
ä¸­é—´çš„å›¾è¡¨å®Œç¾åœ°é˜é‡Šäº†è¿™ä¸ªæ¦‚å¿µã€‚å®ƒå±•ç¤ºäº†ä¸€ä¸ªç²’å­åœ¨æ¨¡æ‹Ÿæ¡†ä¸­ï¼ˆâ€œN=108â€
å¯èƒ½æ˜¯æ¨¡æ‹Ÿç²’å­çš„æ•°é‡ï¼‰ä»åˆå§‹ä½ç½® <span
class="math inline">\(\vec{r}_i(0)\)</span> ç§»åŠ¨åˆ°æœ€ç»ˆä½ç½® <span
class="math inline">\(\vec{r}_i(t_j)\)</span>ã€‚MSD
æ˜¯è¯¥ä½ç§»å¹³æ–¹åœ¨æ‰€æœ‰ç²’å­å’Œå¤šä¸ªæ—¶é—´åŸç‚¹ä¸Šçš„å¹³å‡å€¼ã€‚æ ‡æœ‰â€œMSDâ€çš„å›¾è¡¨æ˜¾ç¤ºäº†å¦‚ä½•ç»˜åˆ¶è¿™äº›æ•°æ®å¹¶æ‰¾åˆ°æ–œç‡ï¼ˆâ€œæ‹Ÿåˆâ€ï¼‰æ¥è®¡ç®—
<span class="math inline">\(D\)</span>ã€‚</li>
<li><strong>Connection:</strong> This is a perfect summary of the
â€œDisplacement Pictureâ€ we analyzed on the second whiteboard. Itâ€™s the
most intuitive way to think about diffusion: how far particles spread
out over
time.è¿™å®Œç¾åœ°æ€»ç»“äº†æˆ‘ä»¬åœ¨ç¬¬äºŒä¸ªç™½æ¿ä¸Šåˆ†æçš„â€œä½ç§»å›¾â€ã€‚è¿™æ˜¯æ€è€ƒæ‰©æ•£æœ€ç›´è§‚çš„æ–¹å¼ï¼šç²’å­éšæ—¶é—´æ‰©æ•£çš„è·ç¦»ã€‚</li>
</ul>
<h3
id="the-green-kubo-relation-the-fluctuation-picture-æ ¼æ—-ä¹…ä¿å…³ç³»æ¶¨è½å›¾">3.
The Green-Kubo Relation: The Fluctuation Picture
æ ¼æ—-ä¹…ä¿å…³ç³»ï¼šæ¶¨è½å›¾</h3>
<p>Finally, the board presents the more advanced but often more
practical method: the <strong>Green-Kubo relation</strong>.</p>
<ul>
<li><strong>The Equations:</strong> This section displays the two key
equations from our last discussion:
<ol type="1">
<li>The MSD as the double integral of the Velocity Autocorrelation
Function (VACF). é€Ÿåº¦è‡ªç›¸å…³å‡½æ•° (VACF) çš„äºŒé‡ç§¯åˆ†çš„å‡æ–¹å·® (MSD)ã€‚</li>
<li>The crucial derivative step: <span
class="math inline">\(\frac{d\langle x^2(t)\rangle}{dt} = 2 \int_0^t
dt&#39;&#39; \langle V_x(t) V_x(t&#39;&#39;) \rangle\)</span>.
å…³é”®çš„å¯¼æ•°æ­¥éª¤ï¼š<span class="math inline">\(\frac{d\langle
x^2(t)\rangle}{dt} = 2 \int_0^t dt&#39;&#39; \langle V_x(t)
V_x(t&#39;&#39;) \rangle\)</span>ã€‚</li>
</ol></li>
<li><strong>The Diagram:</strong> The small diagram of a square with
axes <span class="math inline">\(t&#39;\)</span> and <span
class="math inline">\(t&#39;&#39;\)</span> visually represents the
two-dimensional domain of integration for the double integral.
ä¸€ä¸ªå¸¦æœ‰è½´ <span class="math inline">\(t&#39;\)</span> å’Œ <span
class="math inline">\(t&#39;&#39;\)</span>
çš„å°æ­£æ–¹å½¢å›¾ç›´è§‚åœ°è¡¨ç¤ºäº†äºŒé‡ç§¯åˆ†çš„äºŒç»´ç§¯åˆ†åŸŸã€‚</li>
<li><strong>Connection:</strong> This summarizes the â€œFluctuation
Picture.â€ It shows the mathematical heart of the derivation that proves
the Einstein and Green-Kubo methods are equivalent. As we concluded,
this method is often numerically superior because it involves
integrating a rapidly decaying function (the VACF) rather than finding
the slope of a noisy, unbounded function (the MSD).
è¿™æ¦‚æ‹¬äº†â€œæ¶¨è½å›¾â€ã€‚å®ƒå±•ç¤ºäº†è¯æ˜çˆ±å› æ–¯å¦æ–¹æ³•å’Œæ ¼æ—-ä¹…ä¿æ–¹æ³•ç­‰ä»·çš„æ¨å¯¼è¿‡ç¨‹çš„æ•°å­¦æ ¸å¿ƒã€‚æ­£å¦‚æˆ‘ä»¬æ€»ç»“çš„é‚£æ ·ï¼Œè¿™ç§æ–¹æ³•é€šå¸¸åœ¨æ•°å€¼ä¸Šæ›´èƒœä¸€ç­¹ï¼Œå› ä¸ºå®ƒæ¶‰åŠå¯¹å¿«é€Ÿè¡°å‡å‡½æ•°ï¼ˆVACFï¼‰è¿›è¡Œç§¯åˆ†ï¼Œè€Œä¸æ˜¯æ±‚å™ªå£°æ— ç•Œå‡½æ•°ï¼ˆMSDï¼‰çš„æ–œç‡ã€‚</li>
</ul>
<p>In essence, this single whiteboard is a complete roadmap for
analyzing diffusion in a molecular simulation. It shows how to first
characterize the materialâ€™s <strong>structure</strong> (<span
class="math inline">\(g(r)\)</span>) and then how to compute its key
dynamic propertyâ€”the <strong>diffusion constant
<code>D</code></strong>â€”using two powerful, interconnected methods.
æœ¬è´¨ä¸Šï¼Œè¿™å—ç™½æ¿å°±æ˜¯åˆ†å­æ¨¡æ‹Ÿä¸­åˆ†ææ‰©æ•£çš„å®Œæ•´è·¯çº¿å›¾ã€‚å®ƒå±•ç¤ºäº†å¦‚ä½•é¦–å…ˆè¡¨å¾ææ–™çš„<strong>ç»“æ„</strong>ï¼ˆ<span
class="math inline">\(g(r)\)</span>ï¼‰ï¼Œç„¶åå¦‚ä½•ä½¿ç”¨ä¸¤ç§å¼ºå¤§ä¸”ç›¸äº’å…³è”çš„æ–¹æ³•è®¡ç®—å…¶å…³é”®çš„åŠ¨æ€ç‰¹æ€§â€”â€”<strong>æ‰©æ•£å¸¸æ•°
<code>D</code></strong>ã€‚</p>
<p>This whiteboard beautifully concludes the derivation of the
Green-Kubo relation, showing the final formulas and how they are used in
practice. It provides the punchline to the mathematical story weâ€™ve been
following.</p>
<p>Letâ€™s break down the details.</p>
<h3 id="finalizing-the-derivation">4. Finalizing the Derivation</h3>
<p>The top lines of the board show the final step in connecting the Mean
Squared Displacement (MSD) to the Velocity Autocorrelation Function
(VACF).</p>
<p><span class="math display">\[\lim_{t\to\infty} \frac{d\langle x^2
\rangle}{dt} = 2 \int_0^\infty d\tau \langle V_x(0) V_x(\tau)
\rangle\]</span></p>
<ul>
<li><strong>The Left Side:</strong> As we know from the <strong>Einstein
relation</strong>, the long-time limit of the derivative of the 1D MSD,
<span class="math inline">\(\lim_{t\to\infty} \frac{d\langle x^2
\rangle}{dt}\)</span>, is simply equal to <strong><span
class="math inline">\(2D\)</span></strong>.</li>
<li><strong>The Right Side:</strong> This is the result of the
mathematical derivation from the previous slide. It shows that this same
quantity is also equal to twice the total integral of the VACF.</li>
</ul>
<p>By equating these two, we can solve for the diffusion coefficient,
<code>D</code>.</p>
<h3 id="the-velocity-autocorrelation-function-vacf">5. The Velocity
Autocorrelation Function (VACF)</h3>
<p>The board explicitly names the key quantity here:</p>
<p><span class="math display">\[\Phi(\tau) = \langle V_x(0) V_x(\tau)
\rangle\]</span></p>
<p>This is the <strong>â€œVelocity autocorrelation functionâ€</strong>
(abbreviated as VAF on the board), which weâ€™ve denoted as VACF. The
variable has been changed from <code>t</code> to <code>Ï„</code> (tau) to
represent a â€œtime lagâ€ or interval, which is common notation.</p>
<ul>
<li><strong>The Plot:</strong> The graph on the board shows a typical
plot of the VACF, <span class="math inline">\(\Phi(\tau)\)</span>,
versus the time lag <span class="math inline">\(\tau\)</span>.
<ul>
<li>It starts at a maximum positive value at <span
class="math inline">\(\tau=0\)</span> (when the velocity is perfectly
correlated with itself).</li>
<li>It rapidly decays towards zero as the particle undergoes collisions
that randomize its velocity.</li>
</ul></li>
<li><strong>The Integral:</strong> The shaded area under this curve
represents the value of the integral <span
class="math inline">\(\int_0^\infty \Phi(\tau) d\tau\)</span>. The
Green-Kubo formula states that the diffusion coefficient is directly
proportional to this area.</li>
</ul>
<h3 id="the-green-kubo-formulas-for-the-diffusion-coefficient">6. The
Green-Kubo Formulas for the Diffusion Coefficient</h3>
<p>After canceling the factor of 2, the board presents the final,
practical formulas for <code>D</code>.</p>
<ul>
<li><strong>In 1 Dimension:</strong> <span class="math display">\[D =
\int_0^\infty d\tau \langle V_x(0) V_x(\tau) \rangle\]</span></li>
<li><strong>In 3 Dimensions:</strong> This is the more general and
useful formula. <span class="math display">\[D = \frac{1}{3}
\int_0^\infty d\tau \langle \vec{v}(0) \cdot \vec{v}(\tau)
\rangle\]</span> There are two important changes for 3D:
<ol type="1">
<li>We use the full <strong>velocity vectors</strong> and their dot
product, <span class="math inline">\(\vec{v}(0) \cdot
\vec{v}(\tau)\)</span>, to capture motion in all directions.</li>
<li>We divide by <strong>3</strong> to get the average contribution to
diffusion in any one direction (x, y, or z).</li>
</ol></li>
</ul>
<h3 id="practical-calculation-in-a-simulation">7. Practical Calculation
in a Simulation</h3>
<p>The last formula on the board shows how this is implemented in a
computer simulation with a finite number of atoms.</p>
<p><span class="math display">\[D = \frac{1}{3N} \int_0^\infty d\tau
\sum_{i=1}^{N} \langle \vec{v}_i(0) \cdot \vec{v}_i(\tau)
\rangle\]</span></p>
<ul>
<li><strong><span
class="math inline">\(\sum_{i=1}^{N}\)</span></strong>: This
<strong>summation</strong> symbol indicates that you must compute the
VACF for <em>each individual atom</em> (from atom <code>i=1</code> to
atom <code>N</code>).</li>
<li><strong><span class="math inline">\(\frac{1}{N}\)</span></strong>:
You then <strong>average</strong> the results over all <code>N</code>
atoms in your simulation box.</li>
<li><strong><span class="math inline">\(\langle \dots
\rangle\)</span></strong>: The angle brackets here still imply an
additional average over multiple different starting times
(<code>t=0</code>) to get good statistics.</li>
</ul>
<p>This formula is the practical recipe: to get the diffusion
coefficient, you track the velocity of every atom, calculate each oneâ€™s
VACF, average them together, and then integrate the result over
time.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/16/5054C3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/16/5054C3/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-16 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-16T21:00:00+08:00">2025-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-19 19:24:11" itemprop="dateModified" datetime="2025-09-19T19:24:11+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="general-linear-regression-model.">1. General linear regression
model.</h1>
<p><img src="/imgs/5054C3/General_linear_regression_model.png" alt="Diagram of a linear regression model">
## 1.1 general linear regression model - <strong>å†…å®¹</strong>:
<strong>general linear regression model</strong>.</p>
<p>the fundamental equation:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1x_{i1} + \dots +
\beta_px_{ip} + \epsilon_i\]</span></p>
<p>And it correctly identifies the main goal: to <strong>estimate the
parameters</strong> (the coefficients <span
class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span>) from
data so we can make predictions on new data.</p>
<p>æ ¸å¿ƒç›®æ ‡ï¼šé€šè¿‡æ•°æ®<strong>ä¼°è®¡å‚æ•°</strong>ï¼ˆå³ç³»æ•° <span
class="math inline">\(\beta_0, \beta_1, \dots,
\beta_p\)</span>ï¼‰ï¼Œä»è€Œå¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚</p>
<h2
id="how-we-actually-find-the-best-values-for-the-Î²-coefficients-parameter-estimation">1.2
How we actually find the best values for the <span
class="math inline">\(Î²\)</span> coefficients (parameter
estimation)?:</h2>
<ul>
<li><strong>å†…å®¹</strong>: We find the best values for the <span
class="math inline">\(\beta\)</span> coefficients by finding the values
that <strong>minimize the overall error</strong> of the model. The most
common and fundamental method for this is called <strong>Ordinary Least
Squares (OLS)</strong>.</li>
</ul>
<h3
id="the-main-method-ordinary-least-squares-ols-æ™®é€šæœ€å°äºŒä¹˜æ³•-ols">##
The Main Method: Ordinary Least Squares (OLS) æ™®é€šæœ€å°äºŒä¹˜æ³• (OLS)</h3>
<p>The core idea of OLS is to find the line (or hyperplane in multiple
dimensions) that is as close as possible to all the data points
simultaneously. OLS
çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ‰¾åˆ°ä¸€æ¡å°½å¯èƒ½åŒæ—¶æ¥è¿‘æ‰€æœ‰æ•°æ®ç‚¹çš„ç›´çº¿ï¼ˆæˆ–å¤šç»´è¶…å¹³é¢ï¼‰ã€‚</p>
<h4 id="define-the-error-residuals-è¯¯å·®">1. Define the Error (Residuals)
è¯¯å·®</h4>
<p>First, we need to define what â€œerrorâ€ means. For any single data
point, the error is the difference between the actual value (<span
class="math inline">\(y_i\)</span>) and the value predicted by our model
(<span class="math inline">\(\hat{y}_i\)</span>). This difference is
called the <strong>residual</strong>.
é¦–å…ˆï¼Œéœ€è¦å®šä¹‰â€œè¯¯å·®â€çš„å«ä¹‰ã€‚å¯¹äºä»»ä½•å•ä¸ªæ•°æ®ç‚¹ï¼Œè¯¯å·®æ˜¯å®é™…å€¼ (<span
class="math inline">\(y_i\)</span>) ä¸æ¨¡å‹é¢„æµ‹å€¼ (<span
class="math inline">\(\hat{y}_i\)</span>)
ä¹‹é—´çš„å·®å€¼ã€‚è¿™ä¸ªå·®å€¼ç§°ä¸º<strong>æ®‹å·®</strong>ã€‚</p>
<p><strong>Residual</strong> = Actual Value - Predicted Value
<strong>æ®‹å·®</strong> = å®é™…å€¼ - é¢„æµ‹å€¼ <span class="math display">\[e_i
= y_i - \hat{y}_i\]</span></p>
<p>You can visualize residuals as the vertical distance from each data
point to the regression line.
å¯ä»¥å°†æ®‹å·®å¯è§†åŒ–ä¸ºæ¯ä¸ªæ•°æ®ç‚¹åˆ°å›å½’çº¿çš„å‚ç›´è·ç¦»ã€‚</p>
<h4
id="the-cost-function-sum-of-squared-residuals-æˆæœ¬å‡½æ•°æ®‹å·®å¹³æ–¹å’Œ">2.
The Cost Function: Sum of Squared Residuals æˆæœ¬å‡½æ•°ï¼šæ®‹å·®å¹³æ–¹å’Œ</h4>
<p>We want to make all these residuals as small as possible. We canâ€™t
just add them up, because some are positive and some are negative, and
they would cancel each other out.
æ‰€æœ‰æ®‹å·®å°½å¯èƒ½å°ã€‚ä¸èƒ½ç®€å•åœ°å°†å®ƒä»¬ç›¸åŠ ï¼Œå› ä¸ºæœ‰äº›æ˜¯æ­£æ•°ï¼Œæœ‰äº›æ˜¯è´Ÿæ•°ï¼Œå®ƒä»¬ä¼šç›¸äº’æŠµæ¶ˆã€‚</p>
<p>So, we square each residual (which makes them all positive) and then
sum them up. This gives us the <strong>Sum of Squared Residuals
(SSR)</strong>, which is our â€œcost function.â€
å› æ­¤ï¼Œå°†æ¯ä¸ªæ®‹å·®æ±‚å¹³æ–¹ï¼ˆä½¿å®ƒä»¬éƒ½ä¸ºæ­£æ•°ï¼‰ï¼Œç„¶åå°†å®ƒä»¬ç›¸åŠ ã€‚è¿™å°±å¾—åˆ°äº†<strong>æ®‹å·®å¹³æ–¹å’Œ
(SSR)</strong>ï¼Œä¹Ÿå°±æ˜¯â€œæˆæœ¬å‡½æ•°â€ã€‚</p>
<p><span class="math display">\[SSR = \sum_{i=1}^{n} e_i^2 =
\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span></p>
<p>The goal of OLS is simple: <strong>find the values of <span
class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span> that
make this SSR value as small as possible.</strong></p>
<h4
id="solving-for-the-coefficients-the-normal-equation-æ±‚è§£ç³»æ•°æ­£æ€æ–¹ç¨‹">3.
Solving for the Coefficients: The Normal Equation
æ±‚è§£ç³»æ•°ï¼šæ­£æ€æ–¹ç¨‹</h4>
<p>For linear regression, calculus provides a direct, exact solution to
this minimization problem. By taking the derivative of the SSR function
with respect to each <span class="math inline">\(\beta\)</span>
coefficient and setting it to zero, we can solve for the optimal values.
å¯¹äºçº¿æ€§å›å½’ï¼Œå¾®ç§¯åˆ†ä¸ºè¿™ä¸ªæœ€å°åŒ–é—®é¢˜æä¾›äº†ç›´æ¥ã€ç²¾ç¡®çš„è§£ã€‚é€šè¿‡å¯¹ SSR
å‡½æ•°çš„æ¯ä¸ª <span class="math inline">\(\beta\)</span>
ç³»æ•°æ±‚å¯¼å¹¶å°†å…¶è®¾ä¸ºé›¶ï¼Œå°±å¯ä»¥æ±‚è§£å‡ºæœ€ä¼˜å€¼ã€‚</p>
<p>This process results in a formula known as the <strong>Normal
Equation</strong>, which can be expressed cleanly using matrix algebra:
è¿™ä¸ªè¿‡ç¨‹ä¼šå¾—åˆ°ä¸€ä¸ªç§°ä¸º<strong>æ­£æ€æ–¹ç¨‹</strong>çš„å…¬å¼ï¼Œå®ƒå¯ä»¥ç”¨çŸ©é˜µä»£æ•°æ¸…æ™°åœ°è¡¨ç¤ºå‡ºæ¥ï¼š</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the
vector of our estimated coefficients.ä¼°è®¡ç³»æ•°çš„å‘é‡ã€‚</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is a matrix where
each row is an observation and each column is a feature (with an added
column of 1s for the intercept <span
class="math inline">\(\beta_0\)</span>).å…¶ä¸­æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªè§‚æµ‹å€¼ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªç‰¹å¾ï¼ˆæˆªè·
<span class="math inline">\(\beta_0\)</span> å¢åŠ äº†ä¸€åˆ—å…¨ä¸º 1
çš„å€¼ï¼‰ã€‚</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of the
actual response values.å®é™…å“åº”å€¼çš„å‘é‡ã€‚</li>
</ul>
<p>Statistical software and programming libraries (like Scikit-learn in
Python) use this equation (or more computationally stable versions of
it) to find the best coefficients for you instantly.</p>
<h3 id="an-alternative-method-gradient-descent-æ¢¯åº¦ä¸‹é™">## An
Alternative Method: Gradient Descent æ¢¯åº¦ä¸‹é™</h3>
<p>While the Normal Equation gives a direct answer, it can be very slow
if you have a massive number of features (e.g., hundreds of thousands).
An alternative, iterative method used across machine learning is
<strong>Gradient Descent</strong>.</p>
<p><strong>The Intuition:</strong> Imagine the SSR cost function is a
big valley. Your initial (random) <span
class="math inline">\(\beta\)</span> coefficients place you somewhere on
the slope of this valley.</p>
<ol type="1">
<li><strong>Check the slope</strong> (the gradient) at your current
position. <strong>æ£€æŸ¥æ‚¨å½“å‰ä½ç½®çš„æ–œç‡</strong>ï¼ˆæ¢¯åº¦ï¼‰ã€‚</li>
<li><strong>Take a small step</strong> in the steepest <em>downhill</em>
direction. <strong>æœæœ€é™¡çš„<em>ä¸‹å¡</em>æ–¹å‘</strong>è¿ˆå‡ºä¸€å°æ­¥**ã€‚</li>
<li><strong>Repeat.</strong> You keep taking steps downhill until you
reach the bottom of the valley. The bottom of the valley represents the
minimum SSR, and your coordinates at that point are the optimal <span
class="math inline">\(\beta\)</span> coefficients.
<strong>é‡å¤</strong>ã€‚æ‚¨ç»§ç»­å‘ä¸‹èµ°ï¼Œç›´åˆ°åˆ°è¾¾å±±è°·åº•éƒ¨ã€‚è°·åº•ä»£è¡¨æœ€å°SSRï¼Œè¯¥ç‚¹çš„åæ ‡å³ä¸ºæœ€ä¼˜<span
class="math inline">\(\beta\)</span>ç³»æ•°ã€‚</li>
</ol>
<p>The size of each â€œstepâ€ you take is controlled by a parameter called
the <strong>learning rate</strong>. Gradient Descent is the foundational
optimization algorithm for many complex models, including neural
networks.
æ¯æ¬¡â€œæ­¥è¿›â€çš„å¤§å°ç”±ä¸€ä¸ªç§°ä¸º<strong>å­¦ä¹ ç‡</strong>çš„å‚æ•°æ§åˆ¶ã€‚æ¢¯åº¦ä¸‹é™æ˜¯è®¸å¤šå¤æ‚æ¨¡å‹ï¼ˆåŒ…æ‹¬ç¥ç»ç½‘ç»œï¼‰çš„åŸºç¡€ä¼˜åŒ–ç®—æ³•ã€‚</p>
<h3 id="summary-ols-vs.-gradient-descent">## Summary: OLS vs.Â Gradient
Descent</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Ordinary Least Squares (OLS)</th>
<th style="text-align: left;">Gradient Descent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>How it works</strong></td>
<td style="text-align: left;">Direct calculation using the Normal
Equation.</td>
<td style="text-align: left;">Iterative; takes steps to minimize
error.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Pros</strong></td>
<td style="text-align: left;">Provides an exact, optimal solution. No
parameters to tune.</td>
<td style="text-align: left;">More efficient for very large datasets.
Very versatile.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Cons</strong></td>
<td style="text-align: left;">Can be computationally expensive with many
features.</td>
<td style="text-align: left;">Requires choosing a learning rate. May not
find the exact minimum.</td>
</tr>
</tbody>
</table>
<h1 id="simple-linear-regression">2. Simple Linear Regression</h1>
<p><img src="/imgs/5054C3/Simple_Linear_Regression.png" alt="Simple_Linear_Regression"></p>
<h2 id="simple-linear-regression-1">2.1 Simple Linear Regression</h2>
<ul>
<li><strong>å†…å®¹</strong>: <strong>Simple Linear Regression:</strong> a
special case of the general model you showed earlier where you only have
<strong>one</strong> predictor variable (<span
class="math inline">\(p=1\)</span>).</li>
</ul>
<h3 id="the-model-and-the-goal-æ¨¡å‹å’Œç›®æ ‡">## The Model and the Goal
æ¨¡å‹å’Œç›®æ ‡</h3>
<p>Sets up the simplified equation for a line: <span
class="math display">\[y_i = \beta_0 + \beta_1x_i + \epsilon_i\]</span>
* <span class="math inline">\(y_i\)</span> is the outcome you want to
predict.è¦é¢„æµ‹çš„ç»“æœã€‚ * <span class="math inline">\(x_i\)</span> is
your single input feature or covariate.å•ä¸ªè¾“å…¥ç‰¹å¾æˆ–åå˜é‡ã€‚ * <span
class="math inline">\(\beta_1\)</span> is the <strong>slope</strong> of
the line. It tells you how much <span class="math inline">\(y\)</span>
is expected to increase for a one-unit increase in <span
class="math inline">\(x\)</span>.è¡¨ç¤º <span
class="math inline">\(x\)</span> æ¯å¢åŠ ä¸€ä¸ªå•ä½ï¼Œ<span
class="math inline">\(y\)</span> é¢„è®¡ä¼šå¢åŠ å¤šå°‘ã€‚ * <span
class="math inline">\(\beta_0\)</span> is the
<strong>intercept</strong>. Itâ€™s the predicted value of <span
class="math inline">\(y\)</span> when <span
class="math inline">\(x\)</span> is zero.å½“ <span
class="math inline">\(x\)</span> ä¸ºé›¶æ—¶ <span
class="math inline">\(y\)</span> çš„é¢„æµ‹å€¼ã€‚ * <span
class="math inline">\(\epsilon_i\)</span> is the random error
term.æ˜¯éšæœºè¯¯å·®é¡¹ã€‚</p>
<p>The goal, stated as â€œMinimize the sum of squares of err,â€ is exactly
the <strong>Ordinary Least Squares (OLS)</strong> method we just
discussed. Itâ€™s written here as: <span class="math display">\[\min_{a,b}
\sum_{i=1}^{n} (y_i - a - bx_i)^2\]</span> This is just a different way
of writing the same thing, where they use <code>a</code> for the
intercept (<span class="math inline">\(\beta_0\)</span>) and
<code>b</code> for the slope (<span
class="math inline">\(\beta_1\)</span>). Youâ€™re trying to find the
specific values of the slope and intercept that make the sum of all the
squared errors as small as possible.
ç›®æ ‡ï¼Œå³â€œæœ€å°åŒ–è¯¯å·®å¹³æ–¹å’Œâ€ï¼Œæ­£æ˜¯<strong>æ™®é€šæœ€å°äºŒä¹˜æ³•
(OLS)</strong>ã€‚ï¼š <span class="math display">\[\min_{a,b}
\sum_{i=1}^{n} (y_i - a - bx_i)^2\]</span> è¿™æ˜¯å¦ä¸€ç§å†™æ³•ï¼Œå…¶ä¸­ç”¨
<code>a</code> è¡¨ç¤ºæˆªè· (<span
class="math inline">\(\beta_0\)</span>)ï¼Œ<code>b</code> è¡¨ç¤ºæ–œç‡ (<span
class="math inline">\(\beta_1\)</span>)ã€‚å°è¯•æ‰¾åˆ°æ–œç‡å’Œæˆªè·çš„å…·ä½“å€¼ï¼Œä½¿å¾—æ‰€æœ‰å¹³æ–¹è¯¯å·®ä¹‹å’Œå°½å¯èƒ½å°ã€‚</p>
<h3 id="the-solution-the-estimator-formulas-è§£å†³æ–¹æ¡ˆä¼°è®¡å…¬å¼">## The
Solution: The Estimator Formulas è§£å†³æ–¹æ¡ˆï¼šä¼°è®¡å…¬å¼</h3>
<p>The most important part of this slide is the
<strong>solution</strong>. For the simple case with only one variable,
you donâ€™t need complex matrix algebra (the Normal Equation). Instead,
the minimization problem can be solved with these two straightforward
formulas:
å¯¹äºåªæœ‰ä¸€ä¸ªå˜é‡çš„ç®€å•æƒ…å†µï¼Œä¸éœ€è¦å¤æ‚çš„çŸ©é˜µä»£æ•°ï¼ˆæ­£æ€æ–¹ç¨‹ï¼‰ã€‚ç›¸åï¼Œæœ€å°åŒ–é—®é¢˜å¯ä»¥ç”¨ä»¥ä¸‹ä¸¤ä¸ªç®€å•çš„å…¬å¼æ¥è§£å†³ï¼š</p>
<h4 id="the-slope-hatbeta_1">1. The Slope: <span
class="math inline">\(\hat{\beta}_1\)</span></h4>
<p><span class="math display">\[\hat{\beta}_1 = \frac{\sum_{i=1}^{n}
(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i -
\bar{x})^2}\]</span> * <strong>Intuition:</strong> This formula might
look complex, but itâ€™s actually very intuitive. * The numerator, <span
class="math inline">\(\sum(x_i - \bar{x})(y_i - \bar{y})\)</span>, is
closely related to the <strong>covariance</strong> between X and Y. It
measures whether X and Y tend to move in the same direction (positive
slope) or in opposite directions (negative slope). ä¸ X å’Œ Y
ä¹‹é—´çš„<strong>åæ–¹å·®</strong>å¯†åˆ‡ç›¸å…³ã€‚å®ƒè¡¡é‡ X å’Œ Y
æ˜¯å€¾å‘äºæœç›¸åŒæ–¹å‘ï¼ˆæ­£æ–œç‡ï¼‰è¿˜æ˜¯æœç›¸åæ–¹å‘ï¼ˆè´Ÿæ–œç‡ï¼‰ç§»åŠ¨ã€‚ * The
denominator, <span class="math inline">\(\sum(x_i - \bar{x})^2\)</span>,
is related to the <strong>variance</strong> of X. It measures how much X
varies on its own. å®ƒè¡¡é‡ X è‡ªèº«çš„å˜åŒ–é‡ã€‚ * <strong>In short, the slope
is a measure of how X and Y vary together, scaled by how much X varies
by itself.</strong> æ–œç‡è¡¡é‡çš„æ˜¯ X å’Œ Y å…±åŒå˜åŒ–çš„ç¨‹åº¦ï¼Œå¹¶ä»¥ X
è‡ªèº«çš„å˜åŒ–é‡ä¸ºæ ‡åº¦ã€‚</p>
<h4 id="the-intercept-hatbeta_0-æˆªè·">2. The Intercept: <span
class="math inline">\(\hat{\beta}_0\)</span> æˆªè·</h4>
<p><span class="math display">\[\hat{\beta}_0 = \bar{y} -
\hat{\beta}_1\bar{x}\]</span> * <strong>Intuition:</strong> This formula
is even simpler and has a wonderful geometric meaning. It ensures that
the <strong>line of best fit always passes through the â€œcenter of massâ€
of the data</strong>, which is the point of averages <span
class="math inline">\((\bar{x}, \bar{y})\)</span>.
å®ƒç¡®ä¿<strong>æœ€ä½³æ‹Ÿåˆçº¿å§‹ç»ˆç©¿è¿‡æ•°æ®çš„â€œè´¨å¿ƒâ€</strong>ï¼Œå³å¹³å‡å€¼ <span
class="math inline">\((\bar{x}, \bar{y})\)</span> çš„ç‚¹ã€‚è®¡ç®—å‡ºæœ€ä½³æ–œç‡
(<span class="math inline">\(\hat{\beta}_1\)</span>)
åï¼Œå°±å¯ä»¥å°†å…¶ä»£å…¥æ­¤å…¬å¼ã€‚ç„¶åï¼Œå¯ä»¥è°ƒæ•´æˆªè· (<span
class="math inline">\(\hat{\beta}_0\)</span>)ï¼Œä½¿ç›´çº¿å®Œç¾åœ°å›´ç»•æ•°æ®äº‘çš„ä¸­å¿ƒç‚¹æ—‹è½¬ã€‚
* Once youâ€™ve calculated the best slope (<span
class="math inline">\(\hat{\beta}_1\)</span>), you can plug it into this
formula. You then adjust the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) so that the line pivots
perfectly around the central point of your data cloud.</p>
<p>In summary, this slide provides the precise, closed-form formulas to
calculate the slope and intercept for the line of best fit in a simple
linear regression model.</p>
<h1 id="statistical-inference">3. Statistical Inference</h1>
<p><img src="/imgs/5054C3/Statistical_Inference1.png" alt="Statistical_Inference1">
<img src="/imgs/5054C3/Statistical_Inference2.png" alt="Statistical_Inference2">
## 3.1 Statistical Inference - <strong>å†…å®¹</strong>:
<strong>Statistical Inference:</strong> These two slides are deeply
connected and explain how we go from just <em>calculating</em> the
coefficients to understanding how <em>accurate</em> and
<em>reliable</em> they are.
è§£é‡Šäº†æˆ‘ä»¬å¦‚ä½•ä»ä»…ä»…<em>è®¡ç®—</em>ç³»æ•°åˆ°ç†è§£å®ƒä»¬çš„<em>å‡†ç¡®æ€§</em>å’Œ<em>å¯é æ€§</em>ã€‚</p>
<h3 id="the-core-problem-quantifying-uncertainty-é‡åŒ–ä¸ç¡®å®šæ€§">## The
Core Problem: Quantifying Uncertainty é‡åŒ–ä¸ç¡®å®šæ€§</h3>
<p>The second slide poses the fundamental questions: * â€œHow accurate are
<span class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span>?â€å‡†ç¡®æ€§å¦‚ä½•ï¼Ÿ * â€œWhat are
the distributions of <span class="math inline">\(\hat{\beta}_0\)</span>
and <span class="math inline">\(\hat{\beta}_1\)</span>?â€åˆ†å¸ƒæ˜¯ä»€ä¹ˆï¼Ÿ</p>
<p>The reason we ask this is that our estimated coefficients (<span
class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>) were
calculated from a <strong>specific sample of data</strong>. If we
collected a different random sample from the same population, we would
get slightly different estimates.ä¼°è®¡çš„ç³»æ•° (<span
class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>)
æ˜¯æ ¹æ®<strong>ç‰¹å®šçš„æ•°æ®æ ·æœ¬</strong>è®¡ç®—å‡ºæ¥çš„ã€‚å¦‚æœæˆ‘ä»¬ä»åŒä¸€æ€»ä½“ä¸­éšæœºæŠ½å–ä¸åŒçš„æ ·æœ¬ï¼Œæˆ‘ä»¬å¾—åˆ°çš„ä¼°è®¡å€¼ä¼šç•¥æœ‰ä¸åŒã€‚</p>
<p>The goal of statistical inference is to use the estimates from our
single sample to make conclusions about the <strong>true, unknown
population parameters</strong> (<span class="math inline">\(\beta_0,
\beta_1\)</span>) and to quantify our uncertainty about
them.ç»Ÿè®¡æ¨æ–­çš„ç›®æ ‡æ˜¯åˆ©ç”¨å•ä¸ªæ ·æœ¬çš„ä¼°è®¡å€¼å¾—å‡ºå…³äº<strong>çœŸå®ã€æœªçŸ¥çš„æ€»ä½“å‚æ•°</strong>ï¼ˆ<span
class="math inline">\(\beta_0,
\beta_1\)</span>ï¼‰çš„ç»“è®ºï¼Œå¹¶é‡åŒ–å¯¹è¿™äº›å‚æ•°çš„ä¸ç¡®å®šæ€§ã€‚</p>
<h3
id="the-key-assumption-that-makes-it-possible-å®ç°è¿™ä¸€ç›®æ ‡çš„å…³é”®å‡è®¾">##
The Key Assumption That Makes It Possible å®ç°è¿™ä¸€ç›®æ ‡çš„å…³é”®å‡è®¾</h3>
<p>To figure out the distribution of our estimates, we must make an
assumption about the distribution of the errors. This is the most
important assumption in linear regression for inference:
ä¸ºäº†ç¡®å®šä¼°è®¡å€¼çš„åˆ†å¸ƒï¼Œå¿…é¡»å¯¹è¯¯å·®çš„åˆ†å¸ƒåšå‡ºå‡è®¾ã€‚è¿™æ˜¯çº¿æ€§å›å½’æ¨æ–­ä¸­æœ€é‡è¦çš„å‡è®¾ï¼š
<strong>Assumption:</strong> <span class="math inline">\(\epsilon_i
\stackrel{\text{i.i.d.}}{\sim} N(0, \sigma^2)\)</span></p>
<p>This means we assume the random error terms are: * <strong>Normally
distributed</strong> (<span class="math inline">\(N\)</span>).*
<strong>æ­£æ€åˆ†å¸ƒ</strong>ï¼ˆ<span class="math inline">\(N\)</span>ï¼‰ã€‚ *
Have a mean of <strong>zero</strong> (our model is correct on average).*
å‡å€¼ä¸º<strong>é›¶</strong>ï¼ˆæ¨¡å‹å¹³å‡è€Œè¨€æ˜¯æ­£ç¡®çš„ï¼‰ã€‚ * Have a constant
variance <strong><span class="math inline">\(\sigma^2\)</span></strong>
(homoscedasticity).* æ–¹å·®ä¸ºå¸¸æ•°<strong><span
class="math inline">\(\sigma^2\)</span></strong>ï¼ˆæ–¹å·®é½æ€§ï¼‰ã€‚ * Are
<strong>independent and identically distributed</strong> (i.i.d.),
meaning each error is independent of the others.*
æ˜¯<strong>ç‹¬ç«‹åŒåˆ†å¸ƒ</strong>ï¼ˆi.i.d.ï¼‰çš„ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªè¯¯å·®éƒ½ç‹¬ç«‹äºå…¶ä»–è¯¯å·®ã€‚</p>
<p><strong>Why is this important?</strong> Because our coefficients
<span class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span> are calculated as weighted
sums of the <span class="math inline">\(y_i\)</span> values, and the
<span class="math inline">\(y_i\)</span> values depend on the errors
<span class="math inline">\(\epsilon_i\)</span>. This assumption about
the errors allows us to prove that our estimated coefficients themselves
are also normally distributed. ç³»æ•° <span
class="math inline">\(\hat{\beta}_0\)</span> å’Œ <span
class="math inline">\(\hat{\beta}_1\)</span> æ˜¯é€šè¿‡ <span
class="math inline">\(y_i\)</span> å€¼çš„åŠ æƒå’Œè®¡ç®—çš„ï¼Œè€Œ <span
class="math inline">\(y_i\)</span> å€¼å–å†³äºè¯¯å·® <span
class="math inline">\(\epsilon_i\)</span>ã€‚è¿™ä¸ªå…³äºè¯¯å·®çš„å‡è®¾ä½¿èƒ½å¤Ÿè¯æ˜ä¼°è®¡çš„ç³»æ•°æœ¬èº«ä¹Ÿæœä»æ­£æ€åˆ†å¸ƒã€‚</p>
<h3
id="the-solution-the-theorem-and-the-t-distribution-å®šç†å’Œ-t-åˆ†å¸ƒ">##
The Solution: The Theorem and the t-distribution å®šç†å’Œ t åˆ†å¸ƒ</h3>
<p>The first slide provides the central theorem that allows us to
perform inference. It tells us exactly how to standardize our estimated
coefficients so they follow a known distribution.
ç¬¬ä¸€å¼ å¹»ç¯ç‰‡æä¾›äº†è¿›è¡Œæ¨æ–­çš„æ ¸å¿ƒå®šç†ã€‚å®ƒå‡†ç¡®åœ°å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•å¯¹ä¼°è®¡çš„ç³»æ•°è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä½¿å…¶æœä»å·²çŸ¥çš„åˆ†å¸ƒã€‚</p>
<h4 id="the-standard-error-s.e.-æ ‡å‡†è¯¯å·®-s.e.">1. The Standard Error
(s.e.) æ ‡å‡†è¯¯å·® (s.e.)</h4>
<p>First, look at the denominators in the red dotted boxes. These are
the <strong>standard errors</strong> of the coefficients,
<code>s.e.($\hat&#123;\beta&#125;_1$)</code> and
<code>s.e.($\hat&#123;\beta&#125;_0$)</code>.
ç¬¬ä¸€å¼ å¹»ç¯ç‰‡æä¾›äº†è¿›è¡Œæ¨æ–­çš„æ ¸å¿ƒå®šç†ã€‚å®ƒå‡†ç¡®åœ°å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•å¯¹ä¼°è®¡çš„ç³»æ•°è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä½¿å…¶æœä»å·²çŸ¥çš„åˆ†å¸ƒã€‚</p>
<ul>
<li><strong>What it is:</strong> The standard error is the estimated
<strong>standard deviation of the coefficientâ€™s sampling
distribution</strong>. In simpler terms, itâ€™s a measure of the average
amount by which our estimate <span
class="math inline">\(\hat{\beta}_1\)</span> would differ from the true
<span class="math inline">\(\beta_1\)</span> if we were to repeat the
experiment many times.
æ ‡å‡†è¯¯å·®æ˜¯ç³»æ•°æŠ½æ ·åˆ†å¸ƒçš„<strong>æ ‡å‡†å·®</strong>ä¼°è®¡å€¼ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒè¡¡é‡çš„æ˜¯å¦‚æœæˆ‘ä»¬é‡å¤å®éªŒå¤šæ¬¡ï¼Œæˆ‘ä»¬ä¼°è®¡çš„
<span class="math inline">\(\hat{\beta}_1\)</span> ä¸çœŸå®çš„ <span
class="math inline">\(\beta_1\)</span> ä¹‹é—´çš„å¹³å‡å·®å¼‚ã€‚</li>
<li><strong>A smaller standard error means a more precise and reliable
estimate.</strong>
<strong>æ ‡å‡†è¯¯å·®è¶Šå°ï¼Œä¼°è®¡å€¼è¶Šç²¾ç¡®å¯é ã€‚</strong></li>
</ul>
<h4 id="the-t-statistic-t-ç»Ÿè®¡é‡">2. The t-statistic t ç»Ÿè®¡é‡</h4>
<p>The theorem shows two fractions that form a
<strong>t-statistic</strong>. The general structure for this is:
è¯¥å®šç†å±•ç¤ºäº†ä¸¤ä¸ªæ„æˆ<strong>t ç»Ÿè®¡é‡</strong>çš„åˆ†æ•°ã€‚å…¶ä¸€èˆ¬ç»“æ„å¦‚ä¸‹ï¼š
<span class="math display">\[t = \frac{\text{ (Sample Estimate - True
Value) }}{\text{ Standard Error of the Estimate }}\]</span></p>
<p>For <span class="math inline">\(\beta_1\)</span>, this is: <span
class="math inline">\(\frac{\hat{\beta}_1 -
\beta_1}{\text{s.e.}(\hat{\beta}_1)}\)</span>.</p>
<p>The key insight is that this specific quantity follows a
<strong>Studentâ€™s t-distribution</strong> with <strong><span
class="math inline">\(n-2\)</span> degrees of freedom</strong>.
å…³é”®åœ¨äºï¼Œè¿™ä¸ªç‰¹å®šé‡æœä»<strong>å­¦ç”Ÿ t
åˆ†å¸ƒ</strong>ï¼Œå…¶è‡ªç”±åº¦ä¸º<strong><span
class="math inline">\(n-2\)</span>ã€‚ * </strong>Studentâ€™s
t-distribution:** This is a probability distribution that looks very
similar to the normal distribution but has slightly â€œheavierâ€ tails. We
use it instead of the normal distribution because we had to
<em>estimate</em> the standard deviation of the errors (<code>s</code>
in the formula), which adds extra uncertainty.
è¿™æ˜¯ä¸€ç§æ¦‚ç‡åˆ†å¸ƒï¼Œä¸æ­£æ€åˆ†å¸ƒéå¸¸ç›¸ä¼¼ï¼Œä½†å°¾éƒ¨ç•¥é‡ã€‚ä½¿ç”¨å®ƒæ¥ä»£æ›¿æ­£æ€åˆ†å¸ƒï¼Œæ˜¯å› ä¸ºå¿…é¡»<em>ä¼°è®¡</em>è¯¯å·®çš„æ ‡å‡†å·®ï¼ˆå…¬å¼ä¸­çš„
<code>s</code>ï¼‰ï¼Œè¿™ä¼šå¢åŠ é¢å¤–çš„ä¸ç¡®å®šæ€§ã€‚ * <strong>Degrees of Freedom
(n-2):</strong> We start with <code>n</code> data points, but we lose
two degrees of freedom because we used the data to estimate two
parameters: <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>. ä» <code>n</code>
ä¸ªæ•°æ®ç‚¹å¼€å§‹ï¼Œä½†ç”±äºç”¨è¿™äº›æ•°æ®ä¼°è®¡äº†ä¸¤ä¸ªå‚æ•°ï¼š<span
class="math inline">\(\beta_0\)</span> å’Œ <span
class="math inline">\(\beta_1\)</span>ï¼Œå› æ­¤æŸå¤±äº†ä¸¤ä¸ªè‡ªç”±åº¦ã€‚ #### 3.
Estimating the Error Variance (<span
class="math inline">\(s^2\)</span>)ä¼°è®¡è¯¯å·®æ–¹å·® (<span
class="math inline">\(s^2\)</span>) To calculate the standard errors, we
need a value for <code>s</code>, which is our estimate of the true error
standard deviation <span class="math inline">\(\sigma\)</span>. This is
calculated from the <strong>Residual Sum of Squares (RSS)</strong>.
ä¸ºäº†è®¡ç®—æ ‡å‡†è¯¯å·®ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª <code>s</code> çš„å€¼ï¼Œå®ƒæ˜¯å¯¹çœŸå®è¯¯å·®æ ‡å‡†å·®
<span class="math inline">\(\sigma\)</span>
çš„ä¼°è®¡å€¼ã€‚è¯¥å€¼ç”±<strong>æ®‹å·®å¹³æ–¹å’Œ (RSS)</strong> è®¡ç®—å¾—å‡ºã€‚ *
<strong>RSS:</strong> First, we calculate the RSS = <span
class="math inline">\(\sum(y_i - \hat{y}_i)^2\)</span>, which is the sum
of all the squared errors.* <strong>RSS</strong>ï¼šé¦–å…ˆï¼Œè®¡ç®— RSS = <span
class="math inline">\(\sum(y_i -
\hat{y}_i)^2\)</span>ï¼Œå³æ‰€æœ‰å¹³æ–¹è¯¯å·®ä¹‹å’Œã€‚ * <strong><span
class="math inline">\(s^2\)</span>:</strong> Then, we find the estimate
of the error variance: <span class="math inline">\(s^2 = \text{RSS} /
(n-2)\)</span>. We divide by <span class="math inline">\(n-2\)</span> to
get an unbiased estimate. * <strong><span
class="math inline">\(s^2\)</span></strong>ï¼šç„¶åï¼Œè®¡ç®—è¯¯å·®æ–¹å·®çš„ä¼°è®¡å€¼ï¼š<span
class="math inline">\(s^2 = \text{RSS} / (n-2)\)</span>ã€‚æˆ‘ä»¬å°†å…¶é™¤ä»¥
<span class="math inline">\(n-2\)</span> å³å¯å¾—åˆ°æ— åä¼°è®¡å€¼ã€‚ *
<code>s</code> is simply the square root of <span
class="math inline">\(s^2\)</span>. This <code>s</code> is the value
used in the standard error formulas.* <code>s</code> å°±æ˜¯ <span
class="math inline">\(s^2\)</span> çš„å¹³æ–¹æ ¹ã€‚è¿™ä¸ª <code>s</code>
æ˜¯æ ‡å‡†è¯¯å·®å…¬å¼ä¸­ä½¿ç”¨çš„å€¼ã€‚</p>
<h3 id="what-this-allows-us-to-do-the-practical-use">## What This Allows
Us To Do (The Practical Use)</h3>
<p>Because we know the exact distribution of our t-statistic, we can now
achieve our goal of quantifying uncertainty: å› ä¸ºçŸ¥é“ t
ç»Ÿè®¡é‡çš„ç²¾ç¡®åˆ†å¸ƒï¼Œæ‰€ä»¥ç°åœ¨å¯ä»¥å®ç°é‡åŒ–ä¸ç¡®å®šæ€§çš„ç›®æ ‡ï¼š</p>
<ol type="1">
<li><strong>Hypothesis Testing:</strong> We can test if a predictor is
actually useful. The most common test is for the null hypothesis <span
class="math inline">\(H_0: \beta_1 = 0\)</span>. If we can prove the
observed <span class="math inline">\(\hat{\beta}_1\)</span> is very
unlikely to occur if the true <span
class="math inline">\(\beta_1\)</span> were zero, we can conclude there
is a statistically significant relationship between <span
class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span>.
å¯ä»¥æ£€éªŒä¸€ä¸ªé¢„æµ‹å˜é‡æ˜¯å¦çœŸçš„æœ‰ç”¨ã€‚æœ€å¸¸è§çš„æ£€éªŒæ˜¯é›¶å‡è®¾ <span
class="math inline">\(H_0: \beta_1 = 0\)</span>ã€‚å¦‚æœèƒ½è¯æ˜ï¼Œå½“çœŸå®çš„
<span class="math inline">\(\beta_1\)</span> ä¸ºé›¶æ—¶ï¼Œè§‚æµ‹åˆ°çš„ <span
class="math inline">\(\hat{\beta}_1\)</span>
ä¸å¤ªå¯èƒ½å‘ç”Ÿï¼Œé‚£ä¹ˆå°±å¯ä»¥å¾—å‡ºç»“è®ºï¼Œ<span class="math inline">\(x\)</span>
å’Œ <span class="math inline">\(y\)</span>
ä¹‹é—´å­˜åœ¨ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—å…³ç³»ã€‚</li>
<li><strong>Confidence Intervals:</strong> We can construct a range of
plausible values for the true coefficient. For example, we can calculate
a 95% confidence interval for <span
class="math inline">\(\beta_1\)</span>. This gives us a range where we
are 95% confident the true value of <span
class="math inline">\(\beta_1\)</span> lies.
å¯ä»¥ä¸ºçœŸå®ç³»æ•°æ„å»ºä¸€ç³»åˆ—åˆç†çš„å€¼ã€‚</li>
</ol>
<h1 id="multiple-linear-regression">4. Multiple Linear Regression</h1>
<p><img src="/imgs/5054C3/Multiple_Linear Regression1.png" alt="Multiple_Linear Regression1">
<img src="/imgs/5054C3/Multiple_Linear Regression2.png" alt="Multiple_Linear Regression2">
## 4.1 Multiple Linear Regression - <strong>å†…å®¹</strong>:
<strong>Multiple Linear Regression:</strong></p>
<p>Hereâ€™s a detailed breakdown that connects both slides.</p>
<h3
id="the-model-from-one-to-many-predictors-ä»å•é¢„æµ‹å˜é‡åˆ°å¤šé¢„æµ‹å˜é‡">##
The Model: From One to Many Predictors ä»å•é¢„æµ‹å˜é‡åˆ°å¤šé¢„æµ‹å˜é‡</h3>
<p>The first slide introduces the <strong>Multiple Linear Regression
model</strong>. This is a direct extension of the simple model, but
instead of using just one predictor variable, we use multiple (<span
class="math inline">\(p\)</span>) predictors to explain our response
variable.
å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹æ˜¯ç®€å•æ¨¡å‹çš„ç›´æ¥æ‰©å±•ï¼Œä½†ä¸æ˜¯åªä½¿ç”¨ä¸€ä¸ªé¢„æµ‹å˜é‡ï¼Œè€Œæ˜¯ä½¿ç”¨å¤šä¸ªï¼ˆ<span
class="math inline">\(p\)</span>ï¼‰é¢„æµ‹å˜é‡æ¥è§£é‡Šå“åº”å˜é‡ã€‚</p>
<p>The general formula is: <span class="math display">\[y_i = \beta_0 +
\beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip} +
\epsilon_i\]</span></p>
<h4 id="key-change-in-interpretation">Key Change in Interpretation</h4>
<p>This is the most important new concept. In simple regression, <span
class="math inline">\(\beta_1\)</span> was just the slope. In multiple
regression, each coefficient has a more nuanced meaning:
åœ¨ç®€å•å›å½’ä¸­ï¼Œ<span class="math inline">\(\beta_1\)</span>
åªæ˜¯æ–œç‡ã€‚åœ¨å¤šå…ƒå›å½’ä¸­ï¼Œæ¯ä¸ªç³»æ•°éƒ½æœ‰æ›´å¾®å¦™çš„å«ä¹‰ï¼š</p>
<p><strong><span class="math inline">\(\beta_j\)</span> is the average
change in <span class="math inline">\(y\)</span> for a one-unit increase
in <span class="math inline">\(x_j\)</span>, while holding all other
predictors constant.</strong></p>
<p>This is incredibly powerful. Using the advertising example from your
slide: * <span class="math inline">\(y_i = \beta_0 +
\beta_1(\text{TV}_i) + \beta_2(\text{Radio}_i) +
\beta_3(\text{Newspaper}_i) + \epsilon_i\)</span> * <span
class="math inline">\(\beta_1\)</span> represents the effect of TV
advertising on sales, <strong>after controlling for</strong> the amount
spent on Radio and Newspaper ads. This allows you to isolate the unique
contribution of each advertising
channel.è¡¨ç¤ºåœ¨<strong>æ§åˆ¶</strong>å¹¿æ’­å’ŒæŠ¥çº¸å¹¿å‘Šæ”¯å‡ºåï¼Œç”µè§†å¹¿å‘Šå¯¹é”€å”®é¢çš„å½±å“ã€‚è¿™å¯ä»¥è®©æ‚¨åŒºåˆ†æ¯ä¸ªå¹¿å‘Šæ¸ é“çš„ç‹¬ç‰¹è´¡çŒ®ã€‚</p>
<h3 id="the-solution-deriving-the-normal-equation-æ¨å¯¼æ­£æ€æ–¹ç¨‹">## The
Solution: Deriving the Normal Equation æ¨å¯¼æ­£æ€æ–¹ç¨‹</h3>
<p>The second slide shows the mathematical process for finding the best
coefficients (<span class="math inline">\(\beta_0, \beta_1, \dots,
\beta_p\)</span>) using the <strong>Ordinary Least Squares
(OLS)</strong> method. Itâ€™s essentially a condensed derivation of the
<strong>Normal Equation</strong>. ä½¿ç”¨<strong>æ™®é€šæœ€å°äºŒä¹˜æ³•
(OLS)</strong> å¯»æ‰¾æœ€ä½³ç³»æ•° (<span class="math inline">\(\beta_0,
\beta_1, \dots, \beta_p\)</span>)
çš„æ•°å­¦è¿‡ç¨‹ã€‚å®ƒæœ¬è´¨ä¸Šæ˜¯<strong>æ­£æ€æ–¹ç¨‹</strong>çš„ç®€åŒ–æ¨å¯¼ã€‚</p>
<h4 id="the-goal-minimizing-the-sum-of-squares-æœ€å°åŒ–å¹³æ–¹å’Œ">1. The
Goal: Minimizing the Sum of Squares æœ€å°åŒ–å¹³æ–¹å’Œ</h4>
<p>Just like before, our goal is to minimize the sum of the squared
errors (or residuals): ç›®æ ‡æ˜¯æœ€å°åŒ–å¹³æ–¹è¯¯å·®ï¼ˆæˆ–æ®‹å·®ï¼‰ä¹‹å’Œã€‚</p>
<ul>
<li><strong>Scalar Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_{i1} -
\beta_2x_{i2} - \beta_3x_{i3})^2\)</span>
<ul>
<li>This is easy to read but gets very long with more variables.
ä»£ç æ˜“äºé˜…è¯»ï¼Œä½†å˜é‡è¶Šå¤šï¼Œä»£ç è¶Šé•¿ã€‚</li>
</ul></li>
<li><strong>Vector Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \boldsymbol{\beta}^T
\mathbf{x}_i)^2\)</span>
<ul>
<li>This is a more compact and powerful way to write the same thing
using linear algebra, where <span
class="math inline">\(\boldsymbol{\beta}^T \mathbf{x}_i\)</span> is the
dot product that calculates the entire predicted value <span
class="math inline">\(\hat{y}_i\)</span>.
è¿™æ˜¯ä¸€ç§æ›´ç®€æ´ã€æ›´å¼ºå¤§çš„çº¿æ€§ä»£æ•°è¡¨ç¤ºæ–¹æ³•ï¼Œå…¶ä¸­ <span
class="math inline">\(\boldsymbol{\beta}^T \mathbf{x}_i\)</span>
æ˜¯è®¡ç®—æ•´ä¸ªé¢„æµ‹å€¼ <span class="math inline">\(\hat{y}_i\)</span>
çš„ç‚¹ç§¯ã€‚</li>
</ul></li>
</ul>
<h4
id="the-method-using-calculus-to-find-the-minimum-ä½¿ç”¨å¾®ç§¯åˆ†æ±‚æœ€å°å€¼">2.
The Method: Using Calculus to Find the Minimum ä½¿ç”¨å¾®ç§¯åˆ†æ±‚æœ€å°å€¼</h4>
<p>To find the set of <span class="math inline">\(\beta\)</span> values
that results in the lowest possible error, we use calculus.</p>
<ul>
<li><p><strong>The Derivative (Gradient):</strong> Since our error
function depends on multiple <span class="math inline">\(\beta\)</span>
coefficients, we canâ€™t take a simple derivative. Instead, we take the
<strong>gradient</strong>, which is a vector of partial derivatives (one
for each coefficient). This tells us the â€œslopeâ€ of the error function
in every direction. å¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰ è¯¯å·®å‡½æ•°ä¾èµ–äºå¤šä¸ª <span
class="math inline">\(\beta\)</span>
ç³»æ•°ï¼Œå› æ­¤æˆ‘ä»¬ä¸èƒ½ç®€å•åœ°æ±‚å¯¼æ•°ã€‚ç›¸åï¼Œé‡‡ç”¨<strong>æ¢¯åº¦</strong>ï¼Œå®ƒæ˜¯ä¸€ä¸ªç”±åå¯¼æ•°ç»„æˆçš„å‘é‡ï¼ˆæ¯ä¸ªç³»æ•°å¯¹åº”ä¸€ä¸ªåå¯¼æ•°ï¼‰ã€‚è¿™å‘Šè¯‰è¯¯å·®å‡½æ•°åœ¨å„ä¸ªæ–¹å‘ä¸Šçš„â€œæ–œç‡â€ã€‚</p></li>
<li><p><strong>Setting the Gradient to Zero:</strong> The minimum of a
function occurs where its slope is zero (the very bottom of the error
â€œvalleyâ€). The slide shows the result of taking this gradient and
setting it to
zero.å‡½æ•°çš„æœ€å°å€¼å‡ºç°åœ¨å…¶æ–œç‡ä¸ºé›¶çš„åœ°æ–¹ï¼ˆå³è¯¯å·®â€œè°·åº•â€çš„æœ€ä½ç‚¹ï¼‰ã€‚å¹»ç¯ç‰‡å±•ç¤ºäº†å–æ­¤æ¢¯åº¦å¹¶å°†å…¶è®¾ä¸ºé›¶çš„ç»“æœã€‚</p></li>
</ul>
<p>The equation shown on the slide: <span class="math display">\[2
\sum_{i=1}^{n} (\boldsymbol{\beta}^T \mathbf{x}_i - y_i)\mathbf{x}_i^T =
0\]</span> â€¦is the result of this calculus step. The goal is now to
algebraically rearrange this equation to solve for <span
class="math inline">\(\boldsymbol{\beta}\)</span>.
æ˜¯è¿™ä¸€å¾®ç§¯åˆ†æ­¥éª¤çš„ç»“æœã€‚ç°åœ¨çš„ç›®æ ‡æ˜¯ç”¨ä»£æ•°æ–¹æ³•é‡æ–°æ’åˆ—è¿™ä¸ªæ–¹ç¨‹ï¼Œä»¥æ±‚è§£
<span class="math inline">\(\boldsymbol{\beta}\)</span>ã€‚</p>
<h4 id="the-result-the-normal-equation-æ­£åˆ™æ–¹ç¨‹">3. The Result: The
Normal Equation æ­£åˆ™æ–¹ç¨‹</h4>
<p>After rearranging the equation from the previous step and expressing
the sums in their full matrix form, we arrive at a clean and beautiful
solution. While the slide doesnâ€™t show the final step, the result of
â€œSetting the gradient zero and solve <span
class="math inline">\(\beta\)</span>â€ is the <strong>Normal
Equation</strong>:
é‡æ–°æ’åˆ—ä¸Šä¸€æ­¥ä¸­çš„æ–¹ç¨‹ï¼Œå¹¶å°†å’Œè¡¨ç¤ºä¸ºå®Œæ•´çš„çŸ©é˜µå½¢å¼åï¼Œå¾—åˆ°äº†ä¸€ä¸ªç®€æ´ç¾è§‚çš„è§£ã€‚è™½ç„¶å¹»ç¯ç‰‡æ²¡æœ‰å±•ç¤ºæœ€åä¸€æ­¥ï¼Œâ€œè®¾ç½®æ¢¯åº¦é›¶ç‚¹å¹¶æ±‚è§£
<span class="math inline">\(\beta\)</span>â€
çš„ç»“æœå°±æ˜¯<strong>æ­£æ€æ–¹ç¨‹</strong>ï¼š</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the
vector of our optimal coefficient estimates.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is the â€œdesign
matrixâ€ where each row is an observation and each column is a predictor
variable. <span class="math inline">\(\mathbf{X}\)</span>
æ˜¯â€œè®¾è®¡çŸ©é˜µâ€ï¼Œå…¶ä¸­æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªè§‚æµ‹å€¼ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªé¢„æµ‹å˜é‡ã€‚</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of our
response variable. <span class="math inline">\(\mathbf{y}\)</span>
æ˜¯æˆ‘ä»¬çš„å“åº”å˜é‡çš„å‘é‡ã€‚</li>
</ul>
<p>This single equation is the general solution for finding the OLS
coefficients for <strong>any</strong> linear regression model, no matter
how many predictors you have. This is what statistical software
calculates for you under the hood.
æ— è®ºæœ‰å¤šå°‘ä¸ªé¢„æµ‹å˜é‡ï¼Œè¿™ä¸ªç®€å•çš„æ–¹ç¨‹éƒ½æ˜¯<strong>ä»»ä½•</strong>çº¿æ€§å›å½’æ¨¡å‹ä¸­
OLS ç³»æ•°çš„é€šè§£ã€‚</p>
<h1 id="matrix-notatio">5. matrix notatio</h1>
<p><img src="/imgs/5054C3/matrix_notatio.png"></p>
<ul>
<li><strong>å†…å®¹</strong>: This slide introduces the <strong>matrix
notation</strong> for multiple linear regression, which is a powerful
way to represent the entire system of equations in a compact form. This
notation isnâ€™t just for tidinessâ€”itâ€™s the foundation for how the
solutions are derived and calculated in software.</li>
</ul>
<p>å¤šå…ƒçº¿æ€§å›å½’çš„<strong>çŸ©é˜µç¬¦å·</strong>ï¼Œè¿™æ˜¯ä¸€ç§ä»¥ç´§å‡‘å½¢å¼è¡¨ç¤ºæ•´ä¸ªæ–¹ç¨‹ç»„çš„æœ‰æ•ˆæ–¹æ³•ã€‚è¿™ç§ç¬¦å·ä¸ä»…ä»…æ˜¯ä¸ºäº†ç®€æ´ï¼Œå®ƒè¿˜æ˜¯è½¯ä»¶ä¸­æ¨å¯¼å’Œè®¡ç®—è§£çš„åŸºç¡€ã€‚
Here is a more detailed breakdown.</p>
<h3 id="why-use-matrix-notation">## Why Use Matrix Notation?</h3>
<p>Imagine you have 10,000 observations (<span
class="math inline">\(n=10,000\)</span>) and 5 predictor variables
(<span class="math inline">\(p=5\)</span>). Writing out the model
equation for each observation would be impossible: <span
class="math inline">\(y_1 = \beta_0 + \beta_1x_{11} + \dots +
\beta_5x_{15} + \epsilon_1\)</span> <span class="math inline">\(y_2 =
\beta_0 + \beta_1x_{21} + \dots + \beta_5x_{25} + \epsilon_2\)</span>
â€¦and so on for 10,000 lines.</p>
<p>å‡è®¾ä½ æœ‰ 10,000 ä¸ªè§‚æµ‹å€¼ï¼ˆn=10,000ï¼‰å’Œ 5
ä¸ªé¢„æµ‹å˜é‡ï¼ˆp=5ï¼‰ã€‚ä¸ºæ¯ä¸ªè§‚æµ‹å€¼å†™å‡ºæ¨¡å‹æ–¹ç¨‹æ˜¯ä¸å¯èƒ½çš„ï¼š <span
class="math inline">\(y_1 = \beta_0 + \beta_1x_{11} + \dots +
\beta_5x_{15} + \epsilon_1\)</span> <span class="math inline">\(y_2 =
\beta_0 + \beta_1x_{21} + \dots + \beta_5x_{25} + \epsilon_2\)</span>
â€¦â€¦ä»¥æ­¤ç±»æ¨ï¼Œç›´åˆ° 10,000 è¡Œã€‚ Matrix notation allows us to consolidate
this entire system into a single, elegant
equation:çŸ©é˜µç¬¦å·ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†æ•´ä¸ªç³»ç»Ÿåˆå¹¶æˆä¸€ä¸ªç®€æ´çš„æ–¹ç¨‹ï¼š <span
class="math display">\[\mathbf{y} = \mathbf{X}\boldsymbol{\beta} +
\boldsymbol{\epsilon}\]</span> Letâ€™s break down each component shown on
your slide.</p>
<h3 id="the-components-explained">## The Components Explained</h3>
<h4 id="the-design-matrix-mathbfx-è®¾è®¡çŸ©é˜µ">1. The Design Matrix: <span
class="math inline">\(\mathbf{X}\)</span> è®¾è®¡çŸ©é˜µ</h4>
<p><span class="math display">\[\mathbf{X} = \begin{pmatrix} 1 &amp;
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\ 1 &amp; x_{21} &amp;
x_{22} &amp; \cdots &amp; x_{2p} \\ \vdots &amp; \vdots &amp; \vdots
&amp; \ddots &amp; \vdots \\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots
&amp; x_{np} \end{pmatrix}\]</span> This is the most important matrix.
It contains all of your predictor variable
data.è¿™æ˜¯æœ€é‡è¦çš„çŸ©é˜µã€‚å®ƒåŒ…å«æ‰€æœ‰é¢„æµ‹å˜é‡æ•°æ®ã€‚ * <strong>Rows:</strong>
Each row represents a single observation (e.g., a person, a company, a
day). There are <strong>n</strong>
rows.æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªè§‚å¯Ÿå€¼ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªäººã€ä¸€å®¶å…¬å¸ã€ä¸€å¤©ï¼‰ã€‚å…±æœ‰
<strong>n</strong> è¡Œã€‚ * <strong>Columns:</strong> Each column
represents a predictor variable. There are <strong>p</strong> predictor
columns, plus one special column.æ¯åˆ—ä»£è¡¨ä¸€ä¸ªé¢„æµ‹å˜é‡ã€‚å…±æœ‰
<strong>p</strong> ä¸ªé¢„æµ‹åˆ—ï¼Œå¤–åŠ ä¸€ä¸ªç‰¹æ®Šåˆ—ã€‚ * <strong>The Column of
Ones:</strong> This is a crucial detail. This first column of all ones
is a placeholder for the <strong>intercept term (<span
class="math inline">\(\beta_0\)</span>)</strong>. When you perform
matrix multiplication, this <code>1</code> gets multiplied by <span
class="math inline">\(\beta_0\)</span>, ensuring the intercept is
included in the model for every single observation.
è¿™æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦çš„ç»†èŠ‚ã€‚ç¬¬ä¸€åˆ—ï¼ˆå…¨ 1ï¼‰æ˜¯<strong>æˆªè·é¡¹ (<span
class="math inline">\(\beta_0\)</span>)</strong>
çš„å ä½ç¬¦ã€‚æ‰§è¡ŒçŸ©é˜µä¹˜æ³•æ—¶ï¼Œè¿™ä¸ª <code>1</code> ä¼šä¹˜ä»¥ <span
class="math inline">\(\beta_0\)</span>ï¼Œä»¥ç¡®ä¿æˆªè·åŒ…å«åœ¨æ¨¡å‹ä¸­ï¼Œé€‚ç”¨äºæ¯ä¸ªè§‚æµ‹å€¼ã€‚</p>
<h4 id="the-coefficient-vector-boldsymbolbeta-ç³»æ•°å‘é‡">2. The
Coefficient Vector: <span
class="math inline">\(\boldsymbol{\beta}\)</span> ç³»æ•°å‘é‡</h4>
<p><span class="math display">\[\boldsymbol{\beta} = \begin{pmatrix}
\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix}\]</span> This is a
column vector that contains all the model parametersâ€”the unknown values
we want to estimate. The goal of linear regression is to find the
numerical values for this vector.</p>
<h4 id="the-response-vector-mathbfy-å“åº”å‘é‡">3. The Response Vector:
<span class="math inline">\(\mathbf{y}\)</span> å“åº”å‘é‡</h4>
<p><span class="math display">\[\mathbf{y} = \begin{pmatrix} y_1 \\
\vdots \\ y_n \end{pmatrix}\]</span> This is a column vector containing
all the observed outcomes you are trying to predict (e.g., sales, test
scores, stock prices).</p>
<h4 id="the-error-vector-boldsymbolepsilon-è¯¯å·®å‘é‡">4. The Error
Vector: <span class="math inline">\(\boldsymbol{\epsilon}\)</span>
è¯¯å·®å‘é‡</h4>
<p><span class="math display">\[\boldsymbol{\epsilon} = \begin{pmatrix}
\epsilon_1 \\ \vdots \\ \epsilon_n \end{pmatrix}\]</span> This column
vector bundles together all the individual, unobserved random errors. It
represents the portion of <strong>y</strong> that our model cannot
explain with <strong>X</strong>.</p>
<h3 id="putting-it-all-together">## Putting It All Together</h3>
<p>When you write the equation <span class="math inline">\(\mathbf{y} =
\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span>, you are
actually representing the entire system of individual equations.</p>
<p>Letâ€™s look at the multiplication <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>: <span
class="math display">\[\begin{pmatrix} 1 &amp; x_{11} &amp; \dots &amp;
x_{1p} \\ 1 &amp; x_{21} &amp; \dots &amp; x_{2p} \\ \vdots &amp; \vdots
&amp; \ddots &amp; \vdots \\ 1 &amp; x_{n1} &amp; \dots &amp; x_{np}
\end{pmatrix} \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
\end{pmatrix} = \begin{pmatrix} 1\cdot\beta_0 + x_{11}\cdot\beta_1 +
\dots + x_{1p}\cdot\beta_p \\ 1\cdot\beta_0 + x_{21}\cdot\beta_1 + \dots
+ x_{2p}\cdot\beta_p \\ \vdots \\ 1\cdot\beta_0 + x_{n1}\cdot\beta_1 +
\dots + x_{np}\cdot\beta_p \end{pmatrix}\]</span> As you can see, the
result of this multiplication is a single column vector where each row
is the â€œpredictorâ€ part of the regression equation for that observation.
æ­¤ä¹˜æ³•çš„ç»“æœæ˜¯ä¸€ä¸ªå•åˆ—å‘é‡ï¼Œå…¶ä¸­æ¯ä¸€è¡Œéƒ½æ˜¯è¯¥è§‚æµ‹å€¼çš„å›å½’æ–¹ç¨‹çš„â€œé¢„æµ‹å˜é‡â€éƒ¨åˆ†ã€‚</p>
<p>By setting this equal to <span class="math inline">\(\mathbf{y} -
\boldsymbol{\epsilon}\)</span>, you perfectly recreate the entire set of
<code>n</code> equations in one clean statement. This compact form is
what allows us to easily derive and compute the <strong>Normal
Equation</strong> solution: <span
class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>.è¿™ç§ç´§å‡‘å½¢å¼ä½¿æˆ‘ä»¬èƒ½å¤Ÿè½»æ¾æ¨å¯¼å’Œè®¡ç®—<strong>æ­£æ€æ–¹ç¨‹</strong>çš„è§£</p>
<h1
id="the-core-mathematical-conclusion-of-ordinary-least-squares-ols">6.
the core mathematical conclusion of Ordinary Least Squares (OLS)</h1>
<p><img src="/imgs/5054C3/OLS1.png">
<img src="/imgs/5054C3/OLS2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>: Of course. These slides present the core
mathematical conclusion of Ordinary Least Squares (OLS) and a key
geometric property that explains <em>why</em> this solution works.
å±•ç¤ºäº†æ™®é€šæœ€å°äºŒä¹˜æ³• (OLS)
çš„æ ¸å¿ƒæ•°å­¦ç»“è®ºï¼Œä»¥åŠä¸€ä¸ªå…³é”®çš„å‡ ä½•æ€§è´¨ï¼Œè§£é‡Šäº†è¯¥è§£å†³æ–¹æ¡ˆ<em>ä¸ºä½•</em>æœ‰æ•ˆã€‚
Letâ€™s break down the concepts and the calculation processes in
detail.</li>
</ul>
<hr />
<h3 id="part-1-the-objective-and-the-solution-slide-1-æœ€å°åŒ–å‡ ä½•è·ç¦»">##
Part 1: The Objective and the Solution (Slide 1) æœ€å°åŒ–å‡ ä½•è·ç¦»</h3>
<p>This slide summarizes the entire OLS problem and its solution in the
language of matrix algebra.</p>
<h4 id="the-concept-minimizing-geometric-distance"><strong>The Concept:
Minimizing Geometric Distance</strong></h4>
<p>â€œæœ€å°äºŒä¹˜å‡†åˆ™â€æ˜¯æˆ‘ä»¬æ¨¡å‹çš„ç›®æ ‡ã€‚ The â€œleast squares criterionâ€ is the
objective of our model. The slide shows it in two equivalent forms:</p>
<ol type="1">
<li><strong>Summation Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_{i1} -
\dots - \beta_px_{ip})^2\)</span> This is the sum of the squared
differences between the actual values (<span
class="math inline">\(y_i\)</span>) and the predicted values. è¿™æ˜¯å®é™…å€¼
(<span class="math inline">\(y_i\)</span>) ä¸é¢„æµ‹å€¼ä¹‹å·®çš„å¹³æ–¹å’Œã€‚</li>
<li><strong>Matrix Form:</strong> <span
class="math inline">\(||\mathbf{y} - \mathbf{X}\boldsymbol{\beta}||^2 =
(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} -
\mathbf{X}\boldsymbol{\beta})\)</span> This is the more powerful way to
view the problem. Think of <span
class="math inline">\(\mathbf{y}\)</span> (the vector of all actual
outcomes) and <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> (the vector
of all predicted outcomes) as two points in an n-dimensional space. The
expression <span class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{\beta}||^2\)</span> represents the <strong>squared
Euclidean distance</strong> between these two points. å°† <span
class="math inline">\(\mathbf{y}\)</span>ï¼ˆæ‰€æœ‰å®é™…ç»“æœçš„å‘é‡ï¼‰å’Œ <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>ï¼ˆæ‰€æœ‰é¢„æµ‹ç»“æœçš„å‘é‡ï¼‰è§†ä¸º
n ç»´ç©ºé—´ä¸­çš„ä¸¤ä¸ªç‚¹ã€‚è¡¨è¾¾å¼ <span class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{\beta}||^2\)</span>
è¡¨ç¤ºè¿™ä¸¤ç‚¹ä¹‹é—´çš„<strong>å¹³æ–¹æ¬§æ°è·ç¦»</strong>ã€‚ Therefore, the OLS
problem is a geometric one: <strong>Find the coefficient vector <span
class="math inline">\(\boldsymbol{\beta}\)</span> that makes the
predicted values vector <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> as close as
possible to the actual values vector <span
class="math inline">\(\mathbf{y}\)</span>.</strong> å› æ­¤ï¼ŒOLS
é—®é¢˜æ˜¯ä¸€ä¸ªå‡ ä½•é—®é¢˜ï¼š<strong>æ‰¾åˆ°ä¸€ä¸ªç³»æ•°å‘é‡ <span
class="math inline">\(\boldsymbol{\beta}\)</span>ï¼Œä½¿é¢„æµ‹å€¼å‘é‡ <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>
å°½å¯èƒ½æ¥è¿‘å®é™…å€¼å‘é‡ <span
class="math inline">\(\mathbf{y}\)</span>ã€‚</strong></li>
</ol>
<h4
id="the-solution-the-least-squares-estimator-lseæœ€å°äºŒä¹˜ä¼°è®¡å™¨-lse"><strong>The
Solution: The Least Squares Estimator (LSE)</strong>æœ€å°äºŒä¹˜ä¼°è®¡å™¨
(LSE)</h4>
<p>The slide provides the direct solution to this minimization problem,
which is the <strong>Normal
Equation</strong>:æ­¤æœ€å°åŒ–é—®é¢˜çš„ç›´æ¥è§£ï¼Œå³<strong>æ­£æ€æ–¹ç¨‹</strong>ï¼š</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<p>This formula gives you the exact vector of coefficients <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that minimizes
the squared distance. We get this formula by taking the gradient (the
multidimensional version of a derivative) of the distance function with
respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>,
setting it to zero, and solving, as hinted at in your previous slides.
ç»™å‡ºäº†ä½¿å¹³æ–¹è·ç¦»æœ€å°åŒ–çš„ç²¾ç¡®ç³»æ•°å‘é‡ é€šè¿‡å–è·ç¦»å‡½æ•°å…³äº <span
class="math inline">\(\boldsymbol{\beta}\)</span>
çš„æ¢¯åº¦ï¼ˆå¯¼æ•°çš„å¤šç»´ç‰ˆæœ¬ï¼‰ï¼Œå°†å…¶è®¾ä¸ºé›¶ï¼Œç„¶åæ±‚è§£ï¼Œå³å¯å¾—åˆ°æ­¤å…¬å¼ã€‚
Finally, the slide defines: * <strong>Fitted values:</strong> <span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span> (The vector of predictions
using our optimal coefficients). æ‹Ÿåˆå€¼ * <strong>Residuals:</strong>
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span> (The vector of errors, representing the
difference between actuals and
predictions).è¯¯å·®å‘é‡ï¼Œè¡¨ç¤ºå®é™…å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„å·®å¼‚</p>
<h3
id="part-2-the-geometric-property-and-proofs-slide-2å‡ ä½•æ€§è´¨åŠè¯æ˜">##
Part 2: The Geometric Property and Proofs (Slide 2)å‡ ä½•æ€§è´¨åŠè¯æ˜</h3>
<p>This slide explains a beautiful and fundamental property of the least
squares solution:
<strong>orthogonality</strong>.è§£é‡Šäº†æœ€å°äºŒä¹˜è§£çš„ä¸€ä¸ªç¾å¦™è€ŒåŸºæœ¬çš„æ€§è´¨ï¼š<strong>æ­£äº¤æ€§</strong>ã€‚</p>
<h4 id="the-concept-orthogonality-of-residualsæ®‹å·®çš„æ­£äº¤æ€§"><strong>The
Concept: Orthogonality of Residuals</strong>æ®‹å·®çš„æ­£äº¤æ€§</h4>
<p>The main idea is that the residual vector <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> is
<strong>orthogonal</strong> (perpendicular) to every predictor variable
in your model. ä¸»è¦æ€æƒ³æ˜¯æ®‹å·®å‘é‡ <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>
ä¸æ¨¡å‹ä¸­çš„æ¯ä¸ªé¢„æµ‹å˜é‡<strong>æ­£äº¤</strong>ï¼ˆå‚ç›´ï¼‰ã€‚</p>
<ul>
<li><p><strong>Geometric Intuition:</strong> Think of the columns of
your matrix <span class="math inline">\(\mathbf{X}\)</span> (i.e., your
predictors and the intercept) as defining a flat surface, or a
â€œhyperplane,â€ in a high-dimensional space. Your actual data vector <span
class="math inline">\(\mathbf{y}\)</span> exists somewhere in this
space, likely not on the hyperplane. The OLS process finds the point on
that hyperplane, <span class="math inline">\(\hat{\mathbf{y}}\)</span>,
that is closest to <span class="math inline">\(\mathbf{y}\)</span>. The
shortest line from a point to a plane is always one that is
<strong>perpendicular</strong> to the plane. The residual vector, <span
class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>, <em>is</em> that line. å°†çŸ©é˜µ <span
class="math inline">\(\mathbf{X}\)</span>
çš„åˆ—ï¼ˆå³é¢„æµ‹å˜é‡å’Œæˆªè·ï¼‰æƒ³è±¡æˆåœ¨é«˜ç»´ç©ºé—´ä¸­å®šä¹‰ä¸€ä¸ªå¹³é¢æˆ–â€œè¶…å¹³é¢â€ã€‚å®é™…æ•°æ®å‘é‡
<span class="math inline">\(\mathbf{y}\)</span>
å­˜åœ¨äºè¯¥ç©ºé—´çš„æŸä¸ªä½ç½®ï¼Œå¯èƒ½ä¸åœ¨è¶…å¹³é¢ä¸Šã€‚OLS è¿‡ç¨‹ä¼šåœ¨è¯¥è¶…å¹³é¢ <span
class="math inline">\(\hat{\mathbf{y}}\)</span> ä¸Šæ‰¾åˆ°ä¸ <span
class="math inline">\(\mathbf{y}\)</span>
æœ€æ¥è¿‘çš„ç‚¹ã€‚ä»ä¸€ä¸ªç‚¹åˆ°ä¸€ä¸ªå¹³é¢çš„æœ€çŸ­çº¿å§‹ç»ˆæ˜¯ä¸è¯¥å¹³é¢<strong>å‚ç›´</strong>çš„çº¿ã€‚æ®‹å·®å‘é‡
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span> å°±æ˜¯è¿™æ¡ç›´çº¿ã€‚</p></li>
<li><p><strong>Mathematical Statement:</strong> This geometric property
is stated as <span class="math inline">\(\mathbf{X}^T
\hat{\boldsymbol{\epsilon}} = \mathbf{0}\)</span>. This equation means
that the dot product of the residual vector with every column of <span
class="math inline">\(\mathbf{X}\)</span> is zero, which is the
mathematical definition of orthogonality. è¯¥ç­‰å¼æ„å‘³ç€æ®‹å·®å‘é‡ä¸ <span
class="math inline">\(\mathbf{X}\)</span>
æ¯ä¸€åˆ—çš„ç‚¹ç§¯éƒ½ä¸ºé›¶ï¼Œè¿™æ­£æ˜¯æ­£äº¤æ€§çš„æ•°å­¦å®šä¹‰ã€‚</p></li>
</ul>
<h4 id="the-calculation-process-the-proofs"><strong>The Calculation
Process (The Proofs)</strong></h4>
<p><strong>1. Proof of Orthogonality:</strong> The slide shows a
step-by-step calculation to prove that <span
class="math inline">\(\mathbf{X}^T \hat{\boldsymbol{\epsilon}}\)</span>
is indeed zero. * <strong>Step 1:</strong> Start with the expression to
be proven: <span class="math inline">\(\mathbf{X}^T
\hat{\boldsymbol{\epsilon}}\)</span> ä»å¾…è¯æ˜çš„è¡¨è¾¾å¼å¼€å§‹ï¼š *
<strong>Step 2:</strong> Substitute the definition of the residual,
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\mathbf{X}^T (\mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}})\]</span> ä»£å…¥æ®‹å·®çš„å®šä¹‰ *
<strong>Step 3:</strong> Distribute the <span
class="math inline">\(\mathbf{X}^T\)</span>: <span
class="math display">\[\mathbf{X}^T \mathbf{y} -
\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}}\]</span><em>åˆ†é… </em>
<strong>Step 4:</strong> Substitute the Normal Equation for <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\mathbf{X}^T \mathbf{y} - \mathbf{X}^T\mathbf{X}
[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}]\]</span> *
<strong>Step 5:</strong> The key step is the cancellation. A matrix
<span class="math inline">\((\mathbf{X}^T\mathbf{X})\)</span> multiplied
by its inverse <span
class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span> equals the
identity matrix <span class="math inline">\(\mathbf{I}\)</span>, which
acts like the number 1 in multiplication. <span
class="math display">\[\mathbf{X}^T \mathbf{y} - \mathbf{I}
\mathbf{X}^T\mathbf{y} = \mathbf{X}^T \mathbf{y} -
\mathbf{X}^T\mathbf{y} = \mathbf{0}\]</span> å…³é”®æ­¥éª¤æ˜¯æ¶ˆå»ã€‚ This
completes the proof, showing that the orthogonality property is a direct
consequence of the Normal Equation solution.</p>
<p><strong>2. Proof of LSE:</strong> This is a more abstract proof
showing that our <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> truly gives the
minimum possible error. It uses the orthogonality property and the
Pythagorean theorem for vectors. It essentially shows that for any other
possible coefficient vector <span
class="math inline">\(\boldsymbol{v}\)</span>, the error <span
class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{v}||^2\)</span> will always be greater than or
equal to the error from our LSE, <span
class="math inline">\(||\mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}}||^2\)</span>.</p>
<h1 id="geometric-interpretation">7.geometric interpretation</h1>
<p><img src="/imgs/5054C3/geometric_interpretation1.png">
<img src="/imgs/5054C3/geometric_interpretation2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>These two slides together provide a powerful geometric interpretation
of how Ordinary Least Squares (OLS) works, centered on the concepts of
<strong>orthogonality</strong> and <strong>projection</strong>.
ä»¥<strong>æ­£äº¤æ€§</strong>å’Œ<strong>æŠ•å½±</strong>çš„æ¦‚å¿µä¸ºä¸­å¿ƒï¼Œä»å‡ ä½•è§’åº¦æœ‰åŠ›åœ°è¯ é‡Šäº†æ™®é€šæœ€å°äºŒä¹˜æ³•
(OLS) çš„å·¥ä½œåŸç†ã€‚</p>
<p>Hereâ€™s a detailed summary of the concepts and the processes they
describe.</p>
<h3 id="summary">## Summary</h3>
<p>These slides explain that the process of finding the â€œbest fitâ€ line
in regression is geometrically equivalent to <strong>projecting</strong>
the actual data vector (<span class="math inline">\(\mathbf{y}\)</span>)
onto a hyperplane defined by the predictor variables (<span
class="math inline">\(\mathbf{X}\)</span>). This projection splits the
actual data into two perpendicular components:
è§£é‡Šäº†å›å½’åˆ†æä¸­å¯»æ‰¾â€œæœ€ä½³æ‹Ÿåˆâ€ç›´çº¿çš„è¿‡ç¨‹ï¼Œå…¶å‡ ä½•æ„ä¹‰ç­‰åŒäºå°†å®é™…æ•°æ®å‘é‡
(<span class="math inline">\(\mathbf{y}\)</span>)
<strong>æŠ•å½±</strong>åˆ°ç”±é¢„æµ‹å˜é‡ (<span
class="math inline">\(\mathbf{X}\)</span>)
å®šä¹‰çš„è¶…å¹³é¢ä¸Šã€‚æ­¤æŠ•å½±å°†å®é™…æ•°æ®æ‹†åˆ†ä¸ºä¸¤ä¸ªå‚ç›´åˆ†é‡ï¼š</p>
<ol type="1">
<li><strong>The Fitted Values (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>):</strong> The part of
the data that is perfectly explained by the model (the projection).
æ•°æ®ä¸­èƒ½å¤Ÿè¢«æ¨¡å‹å®Œç¾è§£é‡Šçš„éƒ¨åˆ†ï¼ˆæŠ•å½±ï¼‰ã€‚</li>
<li><strong>The Residuals (<span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>):</strong>
The part of the data that is unexplained (the error), which is
perpendicular to the explained part.
æ•°æ®ä¸­æ— æ³•è§£é‡Šçš„éƒ¨åˆ†ï¼ˆè¯¯å·®ï¼‰ï¼Œå®ƒä¸è¢«è§£é‡Šéƒ¨åˆ†å‚ç›´ã€‚ A special tool called
the <strong>projection matrix (H)</strong>, or â€œhat matrix,â€ is
introduced as the operator that performs this projection.
å¼•å…¥ä¸€ä¸ªç§°ä¸º<strong>æŠ•å½±çŸ©é˜µ
(H)</strong>ï¼ˆæˆ–ç§°â€œå¸½å­çŸ©é˜µâ€ï¼‰çš„ç‰¹æ®Šå·¥å…·ï¼Œä½œä¸ºæ‰§è¡Œæ­¤æŠ•å½±çš„è¿ç®—ç¬¦ã€‚</li>
</ol>
<h3 id="concepts-and-process-explained-in-detail">## Concepts and
Process Explained in Detail</h3>
<h4 id="the-fitted-values-as-a-linear-combination-æ‹Ÿåˆå€¼ä½œä¸ºçº¿æ€§ç»„åˆ">1.
The Fitted Values as a Linear Combination æ‹Ÿåˆå€¼ä½œä¸ºçº¿æ€§ç»„åˆ</h4>
<p>The first slide starts by stating that the fitted value vector <span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span> is a <strong>linear
combination</strong> of the columns of <span
class="math inline">\(\mathbf{X}\)</span> (your predictors).</p>
<ul>
<li><p><strong>Concept:</strong> This means that the vector of fitted
values, <span class="math inline">\(\hat{\mathbf{y}}\)</span>, must lie
within the geometric space (a line, plane, or hyperplane) spanned by
your predictor variables. The model is incapable of producing a
prediction that does not live in this space. è¿™æ„å‘³ç€æ‹Ÿåˆå€¼å‘é‡ <span
class="math inline">\(\hat{\mathbf{y}}\)</span>
å¿…é¡»ä½äºé¢„æµ‹å˜é‡æ‰€æ„æˆçš„å‡ ä½•ç©ºé—´ï¼ˆç›´çº¿ã€å¹³é¢æˆ–è¶…å¹³é¢ï¼‰å†…ã€‚æ¨¡å‹æ— æ³•ç”Ÿæˆä¸å­˜åœ¨äºæ­¤ç©ºé—´çš„é¢„æµ‹ã€‚
#### 2. The Projection Matrix (The â€œHat Matrixâ€) æŠ•å½±çŸ©é˜µï¼ˆâ€œå¸½å­çŸ©é˜µâ€ï¼‰
The second slide introduces the tool that makes this projection happen:
the <strong>projection matrix</strong>, also called the <strong>hat
matrix</strong>, <strong>H</strong>.</p></li>
<li><p><strong>Definition:</strong> <span
class="math inline">\(\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span></p></li>
<li><p><strong>Process:</strong> This matrix has a special job. When you
multiply it by any vector (like our data vector <span
class="math inline">\(\mathbf{y}\)</span>), it projects that vector onto
the space spanned by the columns of <span
class="math inline">\(\mathbf{X}\)</span>. We can see this by starting
with our definition of fitted values and substituting the normal
equation solution for <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\hat{\mathbf{y}} =
\mathbf{X}\hat{\boldsymbol{\beta}} =
\mathbf{X}[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}] =
[\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T]\mathbf{y}\]</span>
This shows that: <span class="math display">\[\hat{\mathbf{y}} =
\mathbf{H}\mathbf{y}\]</span> This is why <strong>H</strong> is
nicknamed the <strong>hat matrix</strong>â€”it â€œputs the hatâ€ on <span
class="math inline">\(\mathbf{y}\)</span>.
è¿™ä¸ªçŸ©é˜µæœ‰å…¶ç‰¹æ®Šçš„ç”¨é€”ã€‚å½“ä½ å°†å®ƒä¹˜ä»¥ä»»ä½•å‘é‡ï¼ˆä¾‹å¦‚æˆ‘ä»¬çš„æ•°æ®å‘é‡ <span
class="math inline">\(\mathbf{y}\)</span>ï¼‰æ—¶ï¼Œå®ƒä¼šå°†è¯¥å‘é‡æŠ•å½±åˆ°ç”±
<span class="math inline">\(\mathbf{X}\)</span> çš„åˆ—æ‰€è·¨è¶Šçš„ç©ºé—´ä¸Šã€‚
#### 3. The Orthogonality of Fitted Values and Residuals
æ‹Ÿåˆå€¼å’Œæ®‹å·®çš„æ­£äº¤æ€§ This is the central concept of the first slide and
a fundamental property of least squares.</p></li>
<li><p><strong>Concept:</strong> The fitted value vector (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>) and the residual vector
(<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>) are <strong>orthogonal</strong>
(perpendicular) to each other.</p></li>
<li><p><strong>Mathematical Statement:</strong> Their dot product is
zero: <span class="math inline">\(\hat{\mathbf{y}}^T(\mathbf{y} -
\hat{\mathbf{y}}) = 0\)</span>.</p></li>
<li><p><strong>Geometric Intuition:</strong> This means the vectors
<span class="math inline">\(\mathbf{y}\)</span>, <span
class="math inline">\(\hat{\mathbf{y}}\)</span>, and <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> form a
<strong>right-angled triangle</strong> in n-dimensional space. The
actual data vector <span class="math inline">\(\mathbf{y}\)</span> is
the hypotenuse, while the modelâ€™s prediction <span
class="math inline">\(\hat{\mathbf{y}}\)</span> and the error <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> are the two
perpendicular legs. è¿™æ„å‘³ç€å‘é‡ <span
class="math inline">\(\mathbf{y}\)</span>ã€<span
class="math inline">\(\hat{\mathbf{y}}\)</span> å’Œ <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> åœ¨ n
ç»´ç©ºé—´ä¸­æ„æˆä¸€ä¸ª<strong>ç›´è§’ä¸‰è§’å½¢</strong>ã€‚å®é™…æ•°æ®å‘é‡ <span
class="math inline">\(\mathbf{y}\)</span> æ˜¯æ–œè¾¹ï¼Œè€Œæ¨¡å‹çš„é¢„æµ‹å€¼ <span
class="math inline">\(\hat{\mathbf{y}}\)</span> å’Œè¯¯å·®å€¼ <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>
æ˜¯ä¸¤æ¡å‚ç›´è¾¹ã€‚</p></li>
</ul>
<h4 id="the-pythagorean-theorem-of-least-squares">4. The Pythagorean
Theorem of Least Squares</h4>
<p>The orthogonality relationship directly implies the Pythagorean
theorem.</p>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(||\mathbf{y}||^2 = ||\hat{\mathbf{y}}||^2 +
||\mathbf{y} - \hat{\mathbf{y}}||^2\)</span></li>
<li><strong>Concept:</strong> This is one of the most important
equations in statistics, as it partitions the total variance in the
data. è¿™æ˜¯ç»Ÿè®¡å­¦ä¸­æœ€é‡è¦çš„æ–¹ç¨‹ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒå¯ä»¥åˆ†å‰²æ•°æ®ä¸­çš„æ€»æ–¹å·®ã€‚
<ul>
<li><span class="math inline">\(||\mathbf{y}||^2\)</span> is
proportional to the <strong>Total Sum of Squares (TSS):</strong> The
total variation of the response variable around its
mean.å“åº”å˜é‡å›´ç»•å…¶å‡å€¼çš„æ€»å˜å¼‚ã€‚</li>
<li><span class="math inline">\(||\hat{\mathbf{y}}||^2\)</span> is
proportional to the <strong>Explained Sum of Squares (ESS):</strong> The
portion of the total variation that is explained by your regression
model.å›å½’æ¨¡å‹å¯ä»¥è§£é‡Šçš„æ€»å˜å¼‚éƒ¨åˆ†ã€‚</li>
<li><span class="math inline">\(||\mathbf{y} -
\hat{\mathbf{y}}||^2\)</span> is the <strong>Residual Sum of Squares
(RSS):</strong> The portion of the total variation that is left
unexplained (the error).æ€»å˜å¼‚ä¸­æœªè§£é‡Šçš„éƒ¨åˆ†ï¼ˆå³è¯¯å·®ï¼‰ã€‚</li>
</ul></li>
</ul>
<p>This relationship, <strong>Total Variation = Explained Variation +
Unexplained Variation</strong>, is the foundation for calculating
metrics like <strong>R-squared (<span
class="math inline">\(R^2\)</span>)</strong>, which measures the
goodness of fit of your model. <strong>æ€»å˜å¼‚ = è§£é‡Šå˜å¼‚ +
æœªè§£é‡Šå˜å¼‚</strong>ï¼Œæ˜¯è®¡ç®—<strong>R å¹³æ–¹ (<span
class="math inline">\(R^2\)</span>)</strong>
ç­‰æŒ‡æ ‡çš„åŸºç¡€ï¼Œè¯¥æŒ‡æ ‡ç”¨äºè¡¡é‡æ¨¡å‹çš„æ‹Ÿåˆä¼˜åº¦ã€‚</p>
<h4 id="residuals-and-the-identity-matrix-æ®‹å·®å’Œå•ä½çŸ©é˜µ">5. Residuals
and the Identity Matrix æ®‹å·®å’Œå•ä½çŸ©é˜µ</h4>
<p>Finally, the second slide shows that just as <strong>H</strong>
projects onto the â€œmodel space,â€ a related matrix projects onto the
â€œerror space.â€ æœ€åï¼Œç¬¬äºŒå¼ å¹»ç¯ç‰‡æ˜¾ç¤ºï¼Œæ­£å¦‚<strong>H</strong>
æŠ•å½±åˆ°â€œæ¨¡å‹ç©ºé—´â€ä¸€æ ·ï¼Œç›¸å…³çŸ©é˜µä¹Ÿä¼šæŠ•å½±åˆ°â€œè¯¯å·®ç©ºé—´â€ã€‚ *
<strong>Process:</strong> We can express the residuals using the hat
matrix: <span class="math display">\[\hat{\boldsymbol{\epsilon}} =
\mathbf{y} - \hat{\mathbf{y}} = \mathbf{y} - \mathbf{H}\mathbf{y} =
(\mathbf{I} - \mathbf{H})\mathbf{y}\]</span> The matrix <span
class="math inline">\((\mathbf{I} - \mathbf{H})\)</span> is also a
projection matrix. It takes the original data vector <span
class="math inline">\(\mathbf{y}\)</span> and projects it onto the space
that is orthogonal to all of your predictors, giving you the residual
vector directly.</p>
<h1
id="visualization-of-ordinary-least-squares-ols-regression">8.visualization
of Ordinary Least Squares (OLS) regression</h1>
<p><img src="/imgs/5054C3/visualization_of_Ordinary_Least_Squares_(OLS)_regression.png"></p>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>This slide provides an excellent geometric visualization of whatâ€™s
happening â€œunder the hoodâ€ in Ordinary Least Squares (OLS) regression.
It translates the algebraic formulas into a more intuitive spatial
concept. è¿™å¼ å¹»ç¯ç‰‡ä»¥å‡ºè‰²çš„å‡ ä½•å¯è§†åŒ–æ–¹å¼å±•ç°äº†æ™®é€šæœ€å°äºŒä¹˜ (OLS)
å›å½’çš„â€œå¹•åâ€æœºåˆ¶ã€‚å®ƒå°†ä»£æ•°å…¬å¼è½¬åŒ–ä¸ºæ›´ç›´è§‚çš„ç©ºé—´æ¦‚å¿µã€‚</p>
<h3 id="summary-1">## Summary</h3>
<p>The image shows that the process of finding the least squares
estimates is geometrically equivalent to taking the <strong>actual
outcome vector</strong> (<span
class="math inline">\(\mathbf{y}\)</span>) and finding its
<strong>orthogonal projection</strong> (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>) onto a
<strong>hyperplane</strong> formed by the predictor variables (<span
class="math inline">\(\mathbf{x}_1\)</span> and <span
class="math inline">\(\mathbf{x}_2\)</span>). The projection <span
class="math inline">\(\hat{\mathbf{y}}\)</span> is the vector of fitted
values, representing the closest possible approximation of the real data
that the model can achieve.</p>
<p>è¯¥å›¾æ˜¾ç¤ºï¼Œå¯»æ‰¾æœ€å°äºŒä¹˜ä¼°è®¡å€¼çš„è¿‡ç¨‹åœ¨å‡ ä½•ä¸Šç­‰åŒäºå°†<strong>å®é™…ç»“æœå‘é‡</strong>
(<span class="math inline">\(\mathbf{y}\)</span>)
æ±‚å‡ºå…¶<strong>æ­£äº¤æŠ•å½±</strong> (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>) åˆ°ç”±é¢„æµ‹å˜é‡ (<span
class="math inline">\(\mathbf{x}_1\)</span> å’Œ <span
class="math inline">\(\mathbf{x}_2\)</span>
æ„æˆçš„<strong>è¶…å¹³é¢</strong>ä¸Šã€‚æŠ•å½± <span
class="math inline">\(\hat{\mathbf{y}}\)</span>
æ˜¯æ‹Ÿåˆå€¼çš„å‘é‡ï¼Œè¡¨ç¤ºæ¨¡å‹èƒ½å¤Ÿè¾¾åˆ°çš„ä¸çœŸå®æ•°æ®æœ€æ¥è¿‘çš„è¿‘ä¼¼å€¼ã€‚</p>
<h3 id="the-concepts-explained-spatiallyç©ºé—´æ¦‚å¿µè§£é‡Š">## The Concepts
Explained Spatiallyç©ºé—´æ¦‚å¿µè§£é‡Š</h3>
<p>Letâ€™s break down each element of the diagram and its meaning:</p>
<h4 id="the-space-itself-ç©ºé—´æœ¬èº«">1. The Space Itself ç©ºé—´æœ¬èº«</h4>
<ul>
<li><strong>Concept:</strong> We are not in a simple 2D or 3D graph
where axes are X and Y. Instead, we are in an <strong>n-dimensional
space</strong>, where <strong>n is the number of observations</strong>
in your dataset. Each axis in this space corresponds to one observation
(e.g., one person, one day).
æˆ‘ä»¬å¹¶éèº«å¤„ä¸€ä¸ªç®€å•çš„äºŒç»´æˆ–ä¸‰ç»´å›¾å½¢ä¸­ï¼Œå…¶ä¸­åæ ‡è½´ä¸º X å’Œ
Yã€‚ç›¸åï¼Œæˆ‘ä»¬èº«å¤„ä¸€ä¸ª <strong>n ç»´ç©ºé—´</strong>ï¼Œå…¶ä¸­ <strong>n
æ˜¯æ•°æ®é›†ä¸­çš„è§‚æµ‹å€¼æ•°é‡</strong>ã€‚æ­¤ç©ºé—´ä¸­çš„æ¯ä¸ªè½´å¯¹åº”ä¸€ä¸ªè§‚æµ‹å€¼ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªäººï¼Œä¸€å¤©ï¼‰ã€‚</li>
<li><strong>Meaning:</strong> A vector like <strong>y</strong> or
<strong>xâ‚</strong> is a single point in this high-dimensional space.
For example, if you have 50 data points, <strong>y</strong> is a vector
pointing to a specific location in a 50-dimensional space. åƒ
<strong>y</strong> æˆ– <strong>xâ‚</strong>
è¿™æ ·çš„å‘é‡æ˜¯è¿™ä¸ªé«˜ç»´ç©ºé—´ä¸­çš„å•ä¸ªç‚¹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ 50
ä¸ªæ•°æ®ç‚¹ï¼Œ<strong>y</strong> å°±æ˜¯æŒ‡å‘ 50 ç»´ç©ºé—´ä¸­ç‰¹å®šä½ç½®çš„å‘é‡ã€‚</li>
</ul>
<h4
id="the-predictor-hyperplane-the-yellow-surfaceé¢„æµ‹å˜é‡è¶…å¹³é¢é»„è‰²è¡¨é¢">2.
The Predictor Hyperplane (The Yellow
Surface)é¢„æµ‹å˜é‡è¶…å¹³é¢ï¼ˆé»„è‰²è¡¨é¢ï¼‰</h4>
<ul>
<li><strong>Concept:</strong> The vectors for your predictor variables,
<strong>xâ‚</strong> and <strong>xâ‚‚</strong>, define a flat surface. If
you had only one predictor, this would be a line. With two, itâ€™s a
plane. With more, itâ€™s a <strong>hyperplane</strong>.é¢„æµ‹å˜é‡çš„å‘é‡
<strong>xâ‚</strong> å’Œ <strong>xâ‚‚</strong>
å®šä¹‰äº†ä¸€ä¸ªå¹³é¢ã€‚å¦‚æœåªæœ‰ä¸€ä¸ªé¢„æµ‹å˜é‡ï¼Œå®ƒå°±æ˜¯ä¸€æ¡çº¿ã€‚å¦‚æœæœ‰ä¸¤ä¸ªï¼Œå®ƒå°±æ˜¯ä¸€ä¸ªå¹³é¢ã€‚å¦‚æœæœ‰æ›´å¤šçš„é¢„æµ‹å˜é‡ï¼Œå®ƒå°±æ˜¯ä¸€ä¸ª<strong>è¶…å¹³é¢</strong>ã€‚</li>
<li><strong>Meaning:</strong> This yellow plane represents the
<strong>â€œworld of possible predictionsâ€</strong> that your model is
allowed to make. Any linear combination of your predictorsâ€”which is what
a linear regression model calculatesâ€”will result in a vector that lies
<em>somewhere</em> on this surface.
è¿™ä¸ªé»„è‰²å¹³é¢ä»£è¡¨ä½ çš„æ¨¡å‹å¯ä»¥åšå‡ºçš„<strong>â€œå¯èƒ½é¢„æµ‹çš„ä¸–ç•Œâ€</strong>ã€‚ä»»ä½•é¢„æµ‹å˜é‡çš„çº¿æ€§ç»„åˆï¼ˆä¹Ÿå°±æ˜¯çº¿æ€§å›å½’æ¨¡å‹è®¡ç®—çš„ç»“æœï¼‰éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªä½äºè¿™ä¸ªå¹³é¢<em>æŸå¤„</em>çš„å‘é‡ã€‚
#### 3. The Actual Outcome Vector (y)å®é™…ç»“æœå‘é‡ (y)</li>
<li><strong>Concept:</strong> The red vector <strong>y</strong>
represents your actual, observed data. Itâ€™s a single point in the
n-dimensional space. çº¢è‰²å‘é‡ <strong>y</strong>
ä»£è¡¨ä½ å®é™…è§‚å¯Ÿåˆ°çš„æ•°æ®ã€‚å®ƒæ˜¯ n ç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ã€‚</li>
<li><strong>Meaning:</strong> Critically, this vector usually does
<strong>not</strong> lie on the predictor hyperplane. If it did, your
model would be a perfect fit with zero error. The fact that itâ€™s â€œoff
the planeâ€ represents the real-world noise and variation that the model
cannot fully capture.
è‡³å…³é‡è¦çš„æ˜¯ï¼Œè¿™ä¸ªå‘é‡é€šå¸¸<strong>ä¸</strong>ä½äºé¢„æµ‹å˜é‡è¶…å¹³é¢ä¸Šã€‚å¦‚æœå®ƒä½äºè¶…å¹³é¢ä¸Šï¼Œä½ çš„æ¨¡å‹å°†å®Œç¾æ‹Ÿåˆï¼Œè¯¯å·®ä¸ºé›¶ã€‚å®ƒâ€œåç¦»å¹³é¢â€ä»£è¡¨äº†æ¨¡å‹æ— æ³•å®Œå…¨æ•æ‰åˆ°çš„çœŸå®ä¸–ç•Œçš„å™ªå£°å’Œå˜åŒ–ã€‚</li>
</ul>
<h4 id="the-fitted-value-vector-Å·æ‹Ÿåˆå€¼å‘é‡-Å·">4. The Fitted Value
Vector (Å·)æ‹Ÿåˆå€¼å‘é‡ (Å·)</h4>
<ul>
<li><strong>Concept:</strong> Since <strong>y</strong> is not on the
plane, we find the point on the plane that is <strong>geometrically
closest</strong> to <strong>y</strong>. This closest point is found by
dropping a perpendicular line from <strong>y</strong> to the plane. The
point where it lands is the <strong>orthogonal projection</strong>,
labeled <strong>Å·</strong> (y-hat). ç”±äº <strong>y</strong>
ä¸åœ¨å¹³é¢ä¸Šï¼Œå› æ­¤æˆ‘ä»¬åœ¨å¹³é¢ä¸Šæ‰¾åˆ°ä¸ <strong>y</strong>
<strong>å‡ ä½•ä¸Šæœ€æ¥è¿‘çš„ç‚¹ã€‚è¿™ä¸ªæœ€æ¥è¿‘ç‚¹æ˜¯é€šè¿‡ä» </strong>y**
åˆ°å¹³é¢åšä¸€æ¡å‚ç›´çº¿æ‰¾åˆ°çš„ã€‚å‚ç›´çº¿æ‰€åœ¨çš„ç‚¹å°±æ˜¯<strong>æ­£äº¤æŠ•å½±</strong>ï¼Œæ ‡è®°ä¸º
<strong>Å·</strong> (y-hat)ã€‚</li>
<li><strong>Meaning:</strong> <strong>Å· is the vector of your modelâ€™s
fitted values.</strong> It is the â€œbestâ€ possible approximation of the
real data that can be created using the given predictors because it
minimizes the distance (and therefore the squared error) between the
actual data (<strong>y</strong>) and the modelâ€™s prediction. <strong>Å·
æ˜¯æ¨¡å‹æ‹Ÿåˆå€¼çš„å‘é‡ã€‚</strong>å®ƒæ˜¯ä½¿ç”¨ç»™å®šé¢„æµ‹å˜é‡å¯ä»¥åˆ›å»ºçš„å¯¹çœŸå®æ•°æ®çš„â€œæœ€ä½³â€è¿‘ä¼¼å€¼ï¼Œå› ä¸ºå®ƒæœ€å°åŒ–äº†å®é™…æ•°æ®
(<strong>y</strong>)
ä¸æ¨¡å‹é¢„æµ‹å€¼ä¹‹é—´çš„è·ç¦»ï¼ˆä»è€Œæœ€å°åŒ–äº†å¹³æ–¹è¯¯å·®ï¼‰ã€‚</li>
</ul>
<h4 id="the-residual-vector-the-dashed-lineæ®‹å·®å‘é‡è™šçº¿">5. The Residual
Vector (The Dashed Line)æ®‹å·®å‘é‡ï¼ˆè™šçº¿ï¼‰</h4>
<ul>
<li><strong>Concept:</strong> The dashed line connecting the tip of
<strong>y</strong> to the tip of <strong>Å·</strong> is the
<strong>residual vector</strong> (<span
class="math inline">\(\boldsymbol{\epsilon} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>). Its length is the shortest possible distance
from <strong>y</strong> to the hyperplane.
è¿æ¥<strong>y</strong>é¡¶ç‚¹å’Œ<strong>Å·</strong>é¡¶ç‚¹çš„è™šçº¿æ˜¯<strong>æ®‹å·®å‘é‡</strong>
(<span class="math inline">\(\boldsymbol{\epsilon} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>)ã€‚å…¶é•¿åº¦æ˜¯ä»<strong>y</strong>åˆ°è¶…å¹³é¢çš„æœ€çŸ­å¯èƒ½è·ç¦»ã€‚</li>
<li><strong>Meaning:</strong> This vector represents the
<strong>error</strong> of the modelâ€”the part of the actual data that is
left over after accounting for the predictors. The right-angle symbol
(â””) is the most important part of the diagram, as it shows this error is
<strong>orthogonal</strong> (perpendicular) to the prediction and to all
the predictors. This visualizes the core property that the modelâ€™s
errors are uncorrelated with the predictors.
è¿æ¥<strong>y</strong>é¡¶ç‚¹å’Œ<strong>Å·</strong>é¡¶ç‚¹çš„è™šçº¿æ˜¯<strong>æ®‹å·®å‘é‡</strong>
(<span class="math inline">\(\boldsymbol{\epsilon} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>)ã€‚å…¶é•¿åº¦æ˜¯ä»<strong>y</strong>åˆ°è¶…å¹³é¢çš„æœ€çŸ­å¯èƒ½è·ç¦»ã€‚</li>
</ul>
<h1 id="singular-value-decomposition-svd-å¥‡å¼‚å€¼åˆ†è§£-svd">9.Singular
Value Decomposition (SVD) å¥‡å¼‚å€¼åˆ†è§£ (SVD)</h1>
<p><img src="/imgs/5054C3/SVD1.png">
<img src="/imgs/5054C3/SVD2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>These slides delve into the more advanced linear algebra behind the
projection matrix (<strong>H</strong>), explaining its fundamental
properties and offering a new way to construct it using <strong>Singular
Value Decomposition (SVD)</strong>. æ¢è®¨äº†æŠ•å½±çŸ©é˜µ (<strong>H</strong>)
èƒŒåæ›´é«˜çº§çš„çº¿æ€§ä»£æ•°ï¼Œè§£é‡Šäº†å®ƒçš„åŸºæœ¬æ€§è´¨ï¼Œå¹¶æä¾›äº†ä¸€ç§ä½¿ç”¨<strong>å¥‡å¼‚å€¼åˆ†è§£
(SVD)</strong> æ„é€ å®ƒçš„æ–°æ–¹æ³•ã€‚</p>
<h3 id="summary-2">## Summary</h3>
<p>These slides show that the <strong>projection matrix (H)</strong>,
which is central to least squares, has two key mathematical properties:
itâ€™s <strong>symmetric</strong> and <strong>idempotent</strong>
(projecting twice is the same as projecting once). These properties
dictate that its eigenvalues must be either 1 or 0. Singular Value
Decomposition (SVD) of the data matrix <strong>X</strong> provides an
elegant and numerically stable way to express <strong>H</strong> as
<strong>UUáµ€</strong>, which makes these fundamental properties easier to
understand and prove. è¿™äº›å¹»ç¯ç‰‡å±•ç¤ºäº†<strong>æŠ•å½±çŸ©é˜µ
(H)</strong>ï¼ˆæœ€å°äºŒä¹˜æ³•çš„æ ¸å¿ƒï¼‰çš„ä¸¤ä¸ªå…³é”®æ•°å­¦æ€§è´¨ï¼š<strong>å¯¹ç§°</strong>å’Œ<strong>å¹‚ç­‰</strong>ï¼ˆæŠ•å½±ä¸¤æ¬¡ç­‰äºæŠ•å½±ä¸€æ¬¡ï¼‰ã€‚è¿™äº›æ€§è´¨å†³å®šäº†å®ƒçš„ç‰¹å¾å€¼å¿…é¡»ä¸º
1 æˆ– 0ã€‚æ•°æ®çŸ©é˜µ <strong>X</strong> çš„å¥‡å¼‚å€¼åˆ†è§£ (SVD)
æä¾›äº†ä¸€ç§ä¼˜é›…ä¸”æ•°å€¼ç¨³å®šçš„æ–¹å¼ï¼Œå°†<strong>H</strong> è¡¨ç¤ºä¸º
<strong>UUáµ€</strong>ï¼Œè¿™ä½¿å¾—è¿™äº›åŸºæœ¬æ€§è´¨æ›´å®¹æ˜“ç†è§£å’Œè¯æ˜ã€‚</p>
<h3 id="concepts-and-process-explained-in-detail-1">## Concepts and
Process Explained in Detail</h3>
<h4 id="singular-value-decomposition-svd">1. Singular Value
Decomposition (SVD)</h4>
<p>The first slide introduces SVD, a powerful method for factoring any
matrix.</p>
<ul>
<li><strong>Concept:</strong> SVD breaks down your data matrix
<strong>X</strong> into three simpler matrices: <strong>X =
UDVáµ€</strong>. Think of this as revealing the fundamental structure of
your data.SVD å°†æ•°æ®çŸ©é˜µ <strong>X</strong>
åˆ†è§£ä¸ºä¸‰ä¸ªæ›´ç®€å•çš„çŸ©é˜µï¼š<strong>X =
UDVáµ€</strong>ã€‚è¿™å¯ä»¥ç†è§£ä¸ºæ­ç¤ºæ•°æ®çš„åŸºæœ¬ç»“æ„ã€‚
<ul>
<li><strong>U:</strong> An <strong>orthogonal matrix</strong> whose
columns form a perfect, orthonormal basis for the space spanned by your
predictors (the column space of <strong>X</strong>). These columns
represent the principal directions of your dataâ€™s
space.ä¸€ä¸ª<strong>æ­£äº¤çŸ©é˜µ</strong>ï¼Œå…¶åˆ—æ„æˆé¢„æµ‹å˜é‡æ‰€å ç©ºé—´ï¼ˆ<strong>X</strong>
çš„åˆ—ç©ºé—´ï¼‰çš„å®Œç¾æ­£äº¤åŸºã€‚è¿™äº›åˆ—ä»£è¡¨æ•°æ®ç©ºé—´çš„ä¸»æ–¹å‘ã€‚</li>
<li><strong>D:</strong> A <strong>diagonal matrix</strong> containing
the â€œsingular values,â€ which measure the importance or magnitude of each
of these principal
directions.ä¸€ä¸ª<strong>å¯¹è§’çŸ©é˜µ</strong>ï¼ŒåŒ…å«â€œå¥‡å¼‚å€¼â€ï¼Œç”¨äºè¡¡é‡æ¯ä¸ªä¸»æ–¹å‘çš„é‡è¦æ€§æˆ–å¤§å°ã€‚</li>
<li><strong>V:</strong> Another <strong>orthogonal
matrix</strong>.å¦ä¸€ä¸ª<strong>æ­£äº¤çŸ©é˜µ</strong>ã€‚</li>
</ul></li>
<li><strong>Process (How SVD simplifies the Projection Matrix) SVD
å¦‚ä½•ç®€åŒ–æŠ•å½±çŸ©é˜µ:</strong> The main takeaway from this slide is the new,
simpler formula for the hat matrix: <span
class="math display">\[\mathbf{H} = \mathbf{UU}^T\]</span> This result
is derived by substituting <strong>X = UDVáµ€</strong> into the original,
more complex formula for <strong>H</strong>: <span
class="math display">\[\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\]</span> When you
perform this substitution and use the fact that for orthogonal matrices
<strong>U</strong> and <strong>V</strong>, we have <strong>Uáµ€U =
I</strong> and <strong>Váµ€V = I</strong>, the <strong>D</strong> and
<strong>V</strong> matrices completely cancel out, leaving the
beautifully simple form <strong>H = UUáµ€</strong>. This tells us that
projection is fundamentally about the basis vectors (<strong>U</strong>)
of the predictor space. æ‰§è¡Œæ­¤ä»£å…¥å¹¶åˆ©ç”¨æ­£äº¤çŸ©é˜µ <strong>U</strong> å’Œ
<strong>V</strong> çš„å…¬å¼ï¼Œå³ <strong>Uáµ€U = I</strong> å’Œ <strong>Váµ€V =
I</strong>ï¼Œ<strong>D</strong> å’Œ <strong>V</strong>
çŸ©é˜µå®Œå…¨æŠµæ¶ˆï¼Œå‰©ä¸‹ç®€æ´çš„å½¢å¼ <strong>H =
UUáµ€</strong>ã€‚è¿™å‘Šè¯‰æˆ‘ä»¬ï¼ŒæŠ•å½±æœ¬è´¨ä¸Šæ˜¯å…³äºé¢„æµ‹ç©ºé—´çš„åŸºå‘é‡ï¼ˆ<strong>U</strong>ï¼‰çš„ã€‚</li>
</ul>
<h4 id="the-properties-of-the-projection-matrix-h-æŠ•å½±çŸ©é˜µ-h-çš„æ€§è´¨">2.
The Properties of the Projection Matrix (H) æŠ•å½±çŸ©é˜µ (H) çš„æ€§è´¨</h4>
<p>The second slide describes the essential nature of any projection
matrix.</p>
<ul>
<li><p><strong>Symmetric (H = Háµ€):</strong> This property ensures that
the projection is orthogonal (i.e., it finds the closest point by moving
perpendicularly).
æ­¤æ€§è´¨ç¡®ä¿æŠ•å½±æ˜¯æ­£äº¤çš„ï¼ˆå³ï¼Œå®ƒé€šè¿‡å‚ç›´ç§»åŠ¨æ‰¾åˆ°æœ€è¿‘çš„ç‚¹ï¼‰ã€‚</p></li>
<li><p><strong>Idempotent (HÂ² = H):</strong> This is the most intuitive
property of a projection. è¿™æ˜¯æŠ•å½±æœ€ç›´è§‚çš„æ€§è´¨ã€‚</p>
<ul>
<li><strong>Concept:</strong> â€œDoing it twice is the same as doing it
once.â€ â€œä¸¤æ¬¡å’Œä¸€æ¬¡ç›¸åŒã€‚â€</li>
<li><strong>Geometric Meaning:</strong> Imagine you project a point onto
a flat tabletop. That projected point is now <em>on the table</em>. If
you try to project it onto the table <em>again</em>, it doesnâ€™t move.
The projection of a projection is just the projection itself.
Mathematically, this is <strong>H(Hv) = Hv</strong>, which simplifies to
<strong>HÂ² = H</strong>.
æƒ³è±¡ä¸€ä¸‹ï¼Œä½ å°†ä¸€ä¸ªç‚¹æŠ•å½±åˆ°å¹³å¦çš„æ¡Œé¢ä¸Šã€‚è¿™ä¸ªæŠ•å½±ç‚¹ç°åœ¨<em>åœ¨æ¡Œå­ä¸Š</em>ã€‚å¦‚æœä½ å°è¯•<em>å†æ¬¡</em>å°†å®ƒæŠ•å½±åˆ°æ¡Œå­ä¸Šï¼Œå®ƒä¸ä¼šç§»åŠ¨ã€‚æŠ•å½±çš„æŠ•å½±å°±æ˜¯æŠ•å½±æœ¬èº«ã€‚ä»æ•°å­¦ä¸Šè®²ï¼Œè¿™æ˜¯<strong>H(Hv)
= Hv</strong>ï¼Œç®€åŒ–ä¸º<strong>HÂ² = H</strong>ã€‚</li>
</ul></li>
</ul>
<h4 id="eigenvalues-and-eigenspaces-ç‰¹å¾å€¼å’Œç‰¹å¾ç©ºé—´">3. Eigenvalues and
Eigenspaces ç‰¹å¾å€¼å’Œç‰¹å¾ç©ºé—´</h4>
<p>The idempotency property has a profound consequence for the matrixâ€™s
eigenvalues.</p>
<ul>
<li><strong>Concept:</strong> The eigenvalues of <strong>H</strong> can
only be <strong>1 or 0</strong>.
<strong>H</strong>çš„ç‰¹å¾å€¼åªèƒ½æ˜¯<strong>1</strong>æˆ–0**ã€‚</li>
<li><strong>Process (The Proof):</strong>
<ol type="1">
<li>Let <strong>v</strong> be an eigenvector of <strong>H</strong> with
eigenvalue <span class="math inline">\(\lambda\)</span>. By definition,
<strong>Hv</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>.
è®¾<strong>v</strong>æ˜¯<strong>H</strong>çš„ç‰¹å¾å‘é‡ï¼Œå…¶ç‰¹å¾å€¼ä¸º<span
class="math inline">\(\lambda\)</span>ã€‚æ ¹æ®å®šä¹‰ï¼Œ<strong>Hv</strong> =
<span class="math inline">\(\lambda\)</span><strong>v</strong>ã€‚</li>
<li>If we apply <strong>H</strong> again, we get <strong>H(Hv)</strong>
= <strong>H</strong>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda\)</span>(<strong>Hv</strong>) = <span
class="math inline">\(\lambda\)</span>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>.
å¦‚æœæˆ‘ä»¬å†æ¬¡åº”ç”¨<strong>H</strong>ï¼Œæˆ‘ä»¬å¾—åˆ°<strong>H(Hv)</strong> =
<strong>H</strong>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda\)</span>(<strong>Hv</strong>) = <span
class="math inline">\(\lambda\)</span>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>ã€‚</li>
<li>So, we have <strong>HÂ²v</strong> = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>.
å› æ­¤ï¼Œæˆ‘ä»¬æœ‰<strong>HÂ²v</strong> = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>ã€‚</li>
<li>But since <strong>H</strong> is idempotent, <strong>HÂ² = H</strong>,
which means <strong>HÂ²v = Hv</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>.
ä½†ç”±äº<strong>H</strong>æ˜¯å¹‚ç­‰çš„ï¼Œ<strong>HÂ² =
H</strong>ï¼Œè¿™æ„å‘³ç€<strong>HÂ²v = Hv</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>ã€‚</li>
<li>Therefore, we must have <span
class="math inline">\(\lambda^2\)</span><strong>v</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>, which means
<span class="math inline">\(\lambda^2 = \lambda\)</span>. The only two
numbers in existence that satisfy this equation are <strong>0 and
1</strong>. å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»æœ‰<span
class="math inline">\(\lambda^2\)</span><strong>v</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>ï¼Œè¿™æ„å‘³ç€<span
class="math inline">\(\lambda^2 =
\lambda\)</span>ã€‚æ»¡è¶³æ­¤ç­‰å¼çš„ä»…æœ‰ä¸¤ä¸ªæ•°å­—æ˜¯<strong>0</strong>å’Œ<strong>1</strong>ã€‚</li>
</ol></li>
<li><strong>Connecting Eigenvalues to the Model
å°†ç‰¹å¾å€¼è¿æ¥åˆ°æ¨¡å‹:</strong>
<ul>
<li><p><strong>Eigenvalue = 1:</strong> The eigenvectors associated with
an eigenvalue of 1 are the vectors that <strong>do not change</strong>
when projected. This is only possible if they were already in the space
being projected onto. Therefore, the space <code>Lâ‚</code> is the
<strong>column space of X</strong>â€”the â€œmodel space.â€ <strong>H</strong>
is the projection onto this space. <strong>ä¸ç‰¹å¾å€¼ä¸º 1
ç›¸å…³è”çš„ç‰¹å¾å‘é‡æ˜¯æŠ•å½±æ—¶</strong>ä¸ä¼šæ”¹å˜<strong>çš„å‘é‡ã€‚åªæœ‰å½“å®ƒä»¬å·²ç»å­˜åœ¨äºæŠ•å½±åˆ°çš„ç©ºé—´ä¸­æ—¶ï¼Œè¿™ç§æƒ…å†µæ‰æœ‰å¯èƒ½å‘ç”Ÿã€‚å› æ­¤ï¼Œç©ºé—´
<code>Lâ‚</code> æ˜¯ X çš„</strong>åˆ—ç©ºé—´<strong>â€”â€”â€œæ¨¡å‹ç©ºé—´â€ã€‚</strong>H**
æ˜¯åˆ°è¯¥ç©ºé—´çš„æŠ•å½±ã€‚</p></li>
<li><p><strong>Eigenvalue = 0:</strong> The eigenvectors associated with
an eigenvalue of 0 are the vectors that get sent to the zero vector when
projected. This happens to vectors that are <strong>orthogonal</strong>
to the projection space. Therefore, the space <code>Lâ‚€</code> is the
<strong>orthogonal â€œerrorâ€ space</strong>. The matrix <strong>I -
H</strong> is the projection onto this space.</p></li>
</ul>
<strong>ä¸ç‰¹å¾å€¼ä¸º 0
ç›¸å…³è”çš„ç‰¹å¾å‘é‡æ˜¯æŠ•å½±æ—¶è¢«å‘é€åˆ°é›¶å‘é‡çš„å‘é‡ã€‚è¿™ç§æƒ…å†µå‘ç”Ÿåœ¨ä¸æŠ•å½±ç©ºé—´</strong>æ­£äº¤<strong>çš„å‘é‡ä¸Šã€‚å› æ­¤ï¼Œç©ºé—´
<code>Lâ‚€</code> æ˜¯</strong>æ­£äº¤â€œè¯¯å·®â€ç©ºé—´<strong>ã€‚çŸ©é˜µ </strong>I - H**
æ˜¯åˆ°è¯¥ç©ºé—´çš„æŠ•å½±ã€‚</li>
</ul>
<h1 id="statistical-inference-1">10.statistical inference</h1>
<p><img src="/imgs/5054C3/statistical_inference_in_linear_regression1.png">
<img src="/imgs/5054C3/statistical_inference_in_linear_regression2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>These slides cover the theoretical backbone of statistical inference
in linear regression. They explain the necessary assumptions and the
resulting probability distributions of our estimates, which is what
allows us to perform hypothesis tests and create confidence
intervals.</p>
<p>è¿™äº›å¹»ç¯ç‰‡æ¶µç›–äº†çº¿æ€§å›å½’ä¸­ç»Ÿè®¡æ¨æ–­çš„ç†è®ºåŸºç¡€ã€‚å®ƒä»¬è§£é‡Šäº†å¿…è¦çš„å‡è®¾ä»¥åŠç”±æ­¤å¾—å‡ºçš„ä¼°è®¡æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¿›è¡Œå‡è®¾æ£€éªŒå¹¶åˆ›å»ºç½®ä¿¡åŒºé—´ã€‚</p>
<h3 id="summary-3">## Summary</h3>
<p>These slides lay out the statistical assumptions required for the
Least Squares Estimator (LSE). The core idea is that if we assume the
errors are independent and normally distributed, we can then prove that:
è¿™äº›å¹»ç¯ç‰‡åˆ—å‡ºäº†æœ€å°äºŒä¹˜ä¼°è®¡é‡ (LSE)
æ‰€éœ€çš„ç»Ÿè®¡å‡è®¾ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å‡è®¾è¯¯å·®æ˜¯ç‹¬ç«‹çš„ä¸”æœä»æ­£æ€åˆ†å¸ƒï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è¯æ˜ï¼š</p>
<ol type="1">
<li><p>Our estimated coefficients (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) also follow a
<strong>Normal distribution</strong> (or a
<strong>t-distribution</strong> when standardized). æˆ‘ä»¬çš„ä¼°è®¡ç³»æ•°
(<span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>)
ä¹Ÿæœä»<strong>æ­£æ€åˆ†å¸ƒ</strong>ï¼ˆæ ‡å‡†åŒ–åæœä»<strong>t
åˆ†å¸ƒ</strong>ï¼‰ã€‚</p></li>
<li><p>Our summed-up squared errors (RSS) follow a <strong>Chi-squared
distribution</strong>. æˆ‘ä»¬çš„å¹³æ–¹è¯¯å·®æ€»å’Œ (RSS)
æœä»<strong>å¡æ–¹åˆ†å¸ƒ</strong>ã€‚</p></li>
<li><p>A specific ratio of the explained variance to the unexplained
variance follows an <strong>F-distribution</strong>, which is used to
test the overall significance of the model.
è§£é‡Šæ–¹å·®ä¸æœªè§£é‡Šæ–¹å·®çš„ç‰¹å®šæ¯”ç‡æœä»<strong>F
åˆ†å¸ƒ</strong>ï¼Œè¯¥åˆ†å¸ƒç”¨äºæ£€éªŒæ¨¡å‹çš„æ•´ä½“æ˜¾è‘—æ€§ã€‚</p></li>
</ol>
<p>These known distributions are the foundation for all statistical
inference in linear
models.è¿™äº›å·²çŸ¥çš„åˆ†å¸ƒæ˜¯çº¿æ€§æ¨¡å‹ä¸­æ‰€æœ‰ç»Ÿè®¡æ¨æ–­çš„åŸºç¡€ã€‚</p>
<h3 id="deeper-dive-into-concepts-and-processes">## Deeper Dive into
Concepts and Processes</h3>
<h4 id="the-model-assumptions-the-foundation-æ¨¡å‹å‡è®¾åŸºç¡€">1. The Model
Assumptions (The Foundation) æ¨¡å‹å‡è®¾ï¼ˆåŸºç¡€ï¼‰</h4>
<p>The first slide states the two assumptions that are critical for
everything that follows. Without them, we canâ€™t make claims about the
statistical properties of our estimates.
ç¬¬ä¸€å¼ å¹»ç¯ç‰‡é˜è¿°äº†å¯¹åç»­æ‰€æœ‰å†…å®¹éƒ½è‡³å…³é‡è¦çš„ä¸¤ä¸ªå‡è®¾ã€‚æ²¡æœ‰å®ƒä»¬ï¼Œæˆ‘ä»¬å°±æ— æ³•æ–­è¨€ä¼°è®¡å€¼çš„ç»Ÿè®¡ç‰¹æ€§ã€‚</p>
<ul>
<li><strong>Assumption 1: <span class="math inline">\(\epsilon_i \sim
N(0, \sigma^2)\)</span></strong>
<ul>
<li><strong>Concept:</strong> This assumes that the error terms (the
part of <code>y</code> that the model canâ€™t explain) are drawn from a
normal (bell-curve) distribution with a mean of zero and a constant
variance <span class="math inline">\(\sigma^2\)</span>.
**å‡è®¾è¯¯å·®é¡¹ï¼ˆæ¨¡å‹æ— æ³•è§£é‡Šçš„ y
å€¼éƒ¨åˆ†ï¼‰æœä»æ­£æ€ï¼ˆé’Ÿå½¢æ›²çº¿ï¼‰åˆ†å¸ƒï¼Œè¯¥åˆ†å¸ƒçš„å‡å€¼ä¸ºé›¶ï¼Œæ–¹å·®ä¸ºå¸¸æ•° <span
class="math inline">\(\sigma^2\)</span>ã€‚</li>
<li><strong>Meaning in Plain English:</strong>
<ul>
<li><strong>Mean of 0:</strong> The model is â€œcorrect on average.â€ The
errors are not systematically positive or negative.
**æ¨¡å‹â€œå¹³å‡æ­£ç¡®â€ã€‚è¯¯å·®å¹¶éç³»ç»Ÿåœ°ä¸ºæ­£æˆ–è´Ÿã€‚</li>
<li><strong>Normal Distribution:</strong> Small errors are more likely
than large errors. This is a common assumption for random noise.
**å°è¯¯å·®æ¯”å¤§è¯¯å·®æ›´æœ‰å¯èƒ½å‡ºç°ã€‚è¿™æ˜¯éšæœºå™ªå£°çš„å¸¸è§å‡è®¾ã€‚</li>
<li><strong>Constant Variance (<span
class="math inline">\(\sigma^2\)</span>):</strong> The amount of random
scatter around the regression line is the same at all levels of the
predictor variables. This is called <strong>homoscedasticity</strong>.
å›å½’çº¿å‘¨å›´çš„éšæœºæ•£åº¦åœ¨é¢„æµ‹å˜é‡çš„å„ä¸ªæ°´å¹³ä¸Šéƒ½æ˜¯ç›¸åŒçš„ã€‚è¿™è¢«ç§°ä¸º<strong>åŒæ–¹å·®æ€§</strong>ã€‚</li>
</ul></li>
</ul></li>
<li><strong>Assumption 2: Observations are independent</strong>
è§‚æµ‹å€¼æ˜¯ç‹¬ç«‹çš„**
<ul>
<li><strong>Concept:</strong> Each data point <span
class="math inline">\((x_i, y_i)\)</span> is an independent piece of
information. The value of the error for one observation gives no
information about the error for another observation. æ¯ä¸ªæ•°æ®ç‚¹ <span
class="math inline">\((x_i, y_i)\)</span>
éƒ½æ˜¯ä¸€æ¡ç‹¬ç«‹çš„ä¿¡æ¯ã€‚ä¸€ä¸ªè§‚æµ‹å€¼çš„è¯¯å·®å€¼å¹¶ä¸èƒ½åæ˜ å¦ä¸€ä¸ªè§‚æµ‹å€¼çš„è¯¯å·®ã€‚</li>
<li><strong>Meaning:</strong> This is often true for cross-sectional
data (e.g., a random sample of people) but can be violated in
time-series data where todayâ€™s error might be correlated with
yesterdayâ€™s.
è¿™é€šå¸¸é€‚ç”¨äºæ¨ªæˆªé¢æ•°æ®ï¼ˆä¾‹å¦‚ï¼ŒéšæœºæŠ½æ ·çš„äººç¾¤ï¼‰ï¼Œä½†åœ¨æ—¶é—´åºåˆ—æ•°æ®ä¸­å¯èƒ½ä¸æˆç«‹ï¼Œå› ä¸ºä»Šå¤©çš„è¯¯å·®å¯èƒ½ä¸æ˜¨å¤©çš„è¯¯å·®ç›¸å…³ã€‚</li>
</ul></li>
</ul>
<h4
id="the-distribution-of-the-coefficients-theorem-of-lse-ç³»æ•°åˆ†å¸ƒæœ€å°äºŒä¹˜æ³•å®šç†">2.
The Distribution of the Coefficients (Theorem of LSE)
ç³»æ•°åˆ†å¸ƒï¼ˆæœ€å°äºŒä¹˜æ³•å®šç†ï¼‰</h4>
<p>This is the most important result for understanding the accuracy of
our individual predictors.</p>
<ul>
<li><strong>Concept 1: The Sampling Distribution of <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></strong>
<ul>
<li><p><strong>Formula:</strong> <span
class="math inline">\(\hat{\boldsymbol{\beta}} \sim
N(\boldsymbol{\beta},
\sigma^2(\mathbf{X}^T\mathbf{X})^{-1})\)</span></p></li>
<li><p><strong>Meaning:</strong> If you were to take many different
random samples from the population and calculate the coefficients <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> for each sample,
the distribution of those coefficients would be a multivariate normal
distribution. **å¦‚æœä»æ€»ä½“ä¸­éšæœºæŠ½å–è®¸å¤šä¸åŒçš„æ ·æœ¬ï¼Œå¹¶è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç³»æ•°
<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>ï¼Œåˆ™è¿™äº›ç³»æ•°çš„åˆ†å¸ƒå°†æœä»å¤šå…ƒæ­£æ€åˆ†å¸ƒã€‚</p>
<ul>
<li>The center of this distribution is the <strong>true population
coefficient vector <span
class="math inline">\(\boldsymbol{\beta}\)</span></strong>. This means
our estimator is <strong>unbiased</strong>â€”on average, it finds the
right answer. è¯¥åˆ†å¸ƒçš„ä¸­å¿ƒæ˜¯<strong>çœŸå®çš„æ€»ä½“ç³»æ•°å‘é‡ <span
class="math inline">\(\boldsymbol{\beta}\)</span></strong>ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬çš„ä¼°è®¡å™¨æ˜¯<strong>æ— åçš„</strong>â€”â€”å¹³å‡è€Œè¨€ï¼Œå®ƒèƒ½å¤Ÿæ‰¾åˆ°æ­£ç¡®çš„ç­”æ¡ˆã€‚</li>
<li>The â€œspreadâ€ of this distribution is its variance-covariance matrix,
<span
class="math inline">\(\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\)</span>.
This tells us the uncertainty in our estimates.
è¯¥åˆ†å¸ƒçš„â€œæ•£åº¦â€æ˜¯å…¶æ–¹å·®-åæ–¹å·®çŸ©é˜µ</li>
</ul></li>
</ul></li>
<li><strong>Concept 2: The t-statistic</strong> t ç»Ÿè®¡é‡
<ul>
<li><strong>Formula:</strong> The standardized coefficient, <span
class="math inline">\(\frac{\hat{\beta}_j -
\beta_j}{\text{s.e.}(\hat{\beta}_j)}\)</span>, follows a
<strong>t-distribution</strong> with <strong><span
class="math inline">\(n-p-1\)</span> degrees of freedom</strong>.</li>
<li><strong>Process &amp; Meaning:</strong> In the real world, we donâ€™t
know the true error variance <span
class="math inline">\(\sigma^2\)</span>. We have to estimate it using
our sample data, which gives us <span
class="math inline">\(s^2\)</span>. Because we are using an
<em>estimate</em> of the variance, we introduce extra uncertainty. The
t-distribution is like a normal distribution but with slightly â€œfatterâ€
tails to account for this additional uncertainty. The degrees of
freedom, <span class="math inline">\(n-p-1\)</span>, reflect the number
of data points (<code>n</code>) minus the number of parameters we had to
estimate (<code>p</code> slopes + 1 intercept). This is the basis for
t-tests and confidence intervals for each coefficient.
åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œæˆ‘ä»¬ä¸çŸ¥é“çœŸå®çš„è¯¯å·®æ–¹å·® <span
class="math inline">\(\sigma^2\)</span>ã€‚æˆ‘ä»¬å¿…é¡»ä½¿ç”¨æ ·æœ¬æ•°æ®æ¥ä¼°è®¡å®ƒï¼Œä»è€Œå¾—åˆ°
<span
class="math inline">\(s^2\)</span>ã€‚ç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯æ–¹å·®çš„<em>ä¼°è®¡å€¼</em>ï¼Œå› æ­¤å¼•å…¥äº†é¢å¤–çš„ä¸ç¡®å®šæ€§ã€‚
t åˆ†å¸ƒç±»ä¼¼äºæ­£æ€åˆ†å¸ƒï¼Œä½†å°¾éƒ¨ç•¥å¾®â€œä¸°æ»¡â€ï¼Œä»¥è§£é‡Šè¿™ç§é¢å¤–çš„ä¸ç¡®å®šæ€§ã€‚è‡ªç”±åº¦
<span class="math inline">\(n-p-1\)</span>
è¡¨ç¤ºæ•°æ®ç‚¹çš„æ•°é‡ï¼ˆ<code>n</code>ï¼‰å‡å»æˆ‘ä»¬éœ€è¦ä¼°è®¡çš„å‚æ•°æ•°é‡ï¼ˆ<code>p</code>
ä¸ªæ–œç‡ + 1 ä¸ªæˆªè·ï¼‰ã€‚è¿™æ˜¯ t æ£€éªŒå’Œæ¯ä¸ªç³»æ•°ç½®ä¿¡åŒºé—´çš„åŸºç¡€ã€‚</li>
</ul></li>
</ul>
<h4
id="the-distribution-of-the-error-theorem-of-residual-è¯¯å·®åˆ†å¸ƒæ®‹å·®å®šç†">3.
The Distribution of the Error (Theorem of Residual)
è¯¯å·®åˆ†å¸ƒï¼ˆæ®‹å·®å®šç†ï¼‰</h4>
<p>This theorem helps us understand the properties of our modelâ€™s
overall error.</p>
<ul>
<li><p><strong>Concept:</strong> The <strong>Residual Sum of Squares
(RSS)</strong>, when scaled by the true variance, follows a
<strong>Chi-squared (<span class="math inline">\(\chi^2\)</span>)
distribution</strong> with <span class="math inline">\(n-p-1\)</span>
degrees of freedom. <strong>æ®‹å·®å¹³æ–¹å’Œ (RSS)</strong>
ç»çœŸå®æ–¹å·®ç¼©æ”¾åï¼Œæœä»è‡ªç”±åº¦ä¸º <span
class="math inline">\(n-p-1\)</span> çš„<strong>å¡æ–¹ (<span
class="math inline">\(\chi^2\)</span>) åˆ†å¸ƒ</strong>ã€‚</p></li>
<li><p><strong>Process &amp; Meaning:</strong> The Chi-squared
distribution often arises when dealing with sums of squared normal
variables. This theorem provides a formal probability distribution for
our total model error. Its most important consequence is that it allows
us to prove that:
**å¡æ–¹åˆ†å¸ƒé€šå¸¸ç”¨äºå¤„ç†æ­£æ€å˜é‡çš„å¹³æ–¹å’Œã€‚è¯¥å®šç†ä¸ºæˆ‘ä»¬æ¨¡å‹çš„æ€»ä½“è¯¯å·®æä¾›äº†ä¸€ä¸ªæ­£å¼çš„æ¦‚ç‡åˆ†å¸ƒã€‚å®ƒæœ€é‡è¦çš„æ¨è®ºæ˜¯ï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿè¯æ˜ï¼š</p>
<p><span class="math display">\[s^2 = \text{RSS}/(n - p - 1)\]</span> is
an <strong>unbiased estimate</strong> of the true error variance <span
class="math inline">\(\sigma^2\)</span>. This <span
class="math inline">\(s^2\)</span> is a critical ingredient for
calculating the standard errors of our coefficients. <span
class="math display">\[s^2 = \text{RSS}/(n - p - 1)\]</span>
æ˜¯çœŸå®è¯¯å·®æ–¹å·® <span class="math inline">\(\sigma^2\)</span>
çš„<strong>æ— åä¼°è®¡</strong>ã€‚è¿™ä¸ª <span
class="math inline">\(s^2\)</span>
æ˜¯è®¡ç®—ç³»æ•°æ ‡å‡†è¯¯å·®çš„å…³é”®å› ç´ ã€‚</p></li>
</ul>
<h4 id="the-f-distribution-and-the-overall-model-test">4. The
F-Distribution and the Overall Model Test</h4>
<p>This final theorem combines our findings about the coefficients and
the residuals.</p>
<ul>
<li><p><strong>Concept:</strong> The F-statistic, which is essentially a
ratio of the variance explained by the model to the variance left
unexplained, follows an <strong>F-distribution</strong>. F
ç»Ÿè®¡é‡æœ¬è´¨ä¸Šæ˜¯æ¨¡å‹è§£é‡Šçš„æ–¹å·®ä¸æœªè§£é‡Šæ–¹å·®çš„æ¯”ç‡ï¼Œæœä» F åˆ†å¸ƒã€‚</p></li>
<li><p><strong>Process &amp; Meaning:</strong> This result relies on the
fact that our coefficient estimates (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) are independent
of our total error (RSS). The F-distribution is used for the
<strong>F-test of overall significance</strong>. This test checks the
null hypothesis that <em>all</em> of your slope coefficients are
simultaneously zero (<span class="math inline">\(\beta_1 = \beta_2 =
\dots = \beta_p = 0\)</span>). If the F-test gives a small p-value, you
can conclude that your model, as a whole, is statistically significant
and provides a better fit than a model with no predictors. å¦‚æœ F
æ£€éªŒå¾—å‡ºçš„ p
å€¼è¾ƒå°ï¼Œåˆ™å¯ä»¥å¾—å‡ºç»“è®ºï¼Œæ‚¨çš„æ¨¡å‹æ•´ä½“ä¸Šå…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ï¼Œå¹¶ä¸”æ¯”æ²¡æœ‰é¢„æµ‹å› å­çš„æ¨¡å‹æ‹Ÿåˆæ•ˆæœæ›´å¥½ã€‚</p></li>
</ul>
<h1 id="construct-different-types-of-intervals">11.construct different
types of intervals</h1>
<p><img src="/imgs/5054C3/construct_different_types_of_intervals1.png">
<img src="/imgs/5054C3/construct_different_types_of_intervals2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>These slides explain how to use the statistical properties of the
least squares estimates to construct different types of intervals, which
are essential for quantifying the uncertainty in your modelâ€™s
predictions and parameters.
è¿™äº›å¹»ç¯ç‰‡è§£é‡Šäº†å¦‚ä½•åˆ©ç”¨æœ€å°äºŒä¹˜ä¼°è®¡çš„ç»Ÿè®¡ç‰¹æ€§æ¥æ„å»ºä¸åŒç±»å‹çš„åŒºé—´ï¼Œè¿™å¯¹äºé‡åŒ–æ¨¡å‹é¢„æµ‹å’Œå‚æ•°ä¸­çš„ä¸ç¡®å®šæ€§è‡³å…³é‡è¦ã€‚</p>
<h3 id="summary-4">Summary</h3>
<p>These slides show how to calculate three distinct types of intervals
in linear regression, each answering a different question about
uncertainty:
å±•ç¤ºäº†å¦‚ä½•è®¡ç®—çº¿æ€§å›å½’ä¸­ä¸‰ç§ä¸åŒç±»å‹çš„åŒºé—´ï¼Œæ¯ç§åŒºé—´åˆ†åˆ«å›ç­”äº†å…³äºä¸ç¡®å®šæ€§çš„ä¸åŒé—®é¢˜ï¼š</p>
<ol type="1">
<li><strong>Confidence Interval for a Parameter (<span
class="math inline">\(\beta_j\)</span>):</strong> Provides a plausible
range for a single, true unknown coefficient in the model.
ä¸ºæ¨¡å‹ä¸­å•ä¸ªçœŸå®æœªçŸ¥ç³»æ•°æä¾›ä¸€ä¸ªåˆç†çš„èŒƒå›´ã€‚</li>
<li><strong>Confidence Interval for the Mean Response:</strong> Provides
a plausible range for the <em>average</em> outcome for a given set of
predictor values.
ä¸ºç»™å®šä¸€ç»„é¢„æµ‹å˜é‡å€¼çš„<em>å¹³å‡</em>ç»“æœæä¾›ä¸€ä¸ªåˆç†çš„èŒƒå›´ã€‚</li>
<li><strong>Prediction Interval:</strong> Provides a plausible range for
a <em>single future</em> outcome for a given set of predictor values.
This interval is always wider than the confidence interval for the mean
response because it must also account for individual random error.
ä¸ºç»™å®šä¸€ç»„é¢„æµ‹å˜é‡å€¼çš„<em>å•ä¸ªæœªæ¥</em>ç»“æœæä¾›ä¸€ä¸ªåˆç†çš„èŒƒå›´ã€‚è¯¥åŒºé—´å§‹ç»ˆæ¯”å¹³å‡å“åº”çš„ç½®ä¿¡åŒºé—´æ›´å®½ï¼Œå› ä¸ºå®ƒè¿˜å¿…é¡»è€ƒè™‘å•ä¸ªéšæœºè¯¯å·®ã€‚</li>
</ol>
<h3 id="deeper-dive-into-concepts-and-processes-1">Deeper Dive into
Concepts and Processes</h3>
<h4
id="confidence-interval-for-a-single-parameter-å•ä¸ªå‚æ•°çš„ç½®ä¿¡åŒºé—´">1.
Confidence Interval for a Single Parameter å•ä¸ªå‚æ•°çš„ç½®ä¿¡åŒºé—´</h4>
<p>This interval addresses the uncertainty around one specific
coefficient, like the slope for your most important predictor.
æ­¤åŒºé—´ç”¨äºè§£å†³å›´ç»•æŸä¸ªç‰¹å®šç³»æ•°çš„ä¸ç¡®å®šæ€§ï¼Œä¾‹å¦‚æœ€é‡è¦çš„é¢„æµ‹å› å­çš„æ–œç‡ã€‚</p>
<ul>
<li><strong>The Question It Answers:</strong> â€œIâ€™ve calculated a slope
of <span class="math inline">\(\hat{\beta}_1 = 10.5\)</span>. How sure
am I about this number? What is a plausible range for the <em>true</em>
population slope?â€ æˆ‘è®¡ç®—å‡ºäº†æ–œç‡ä¸º <span
class="math inline">\(\hat{\beta}_1 =
10.5\)</span>ã€‚æˆ‘å¯¹è¿™ä¸ªæ•°å­—æœ‰å¤šç¡®å®šï¼Ÿ<em>çœŸå®</em>æ€»ä½“æ–œç‡çš„åˆç†èŒƒå›´æ˜¯å¤šå°‘ï¼Ÿâ€</li>
<li><strong>The Formula:</strong> <span
class="math inline">\(\hat{\beta}_j \pm t_{n-p-1}(\alpha/2) s
\sqrt{c_{jj}}\)</span>
<ul>
<li><strong><span class="math inline">\(\hat{\beta}_j\)</span></strong>:
This is your best point estimate for the coefficient, taken directly
from the model output. è¿™æ˜¯è¯¥ç³»æ•°çš„æœ€ä½³ç‚¹ä¼°è®¡å€¼ï¼Œç›´æ¥å–è‡ªæ¨¡å‹è¾“å‡ºã€‚</li>
<li><strong><span
class="math inline">\(t_{n-p-1}(\alpha/2)\)</span></strong>: This is the
<strong>critical value</strong> from a t-distribution. Itâ€™s a multiplier
that sets the width of the interval based on your desired confidence
level (e.g., for 95% confidence, <span
class="math inline">\(\alpha=0.05\)</span>). è¿™æ˜¯ t
åˆ†å¸ƒçš„<strong>ä¸´ç•Œå€¼</strong>ã€‚å®ƒæ˜¯ä¸€ä¸ªä¹˜æ•°ï¼Œæ ¹æ®æ‚¨æ‰€éœ€çš„ç½®ä¿¡æ°´å¹³è®¾ç½®åŒºé—´å®½åº¦ï¼ˆä¾‹å¦‚ï¼Œå¯¹äº
95% çš„ç½®ä¿¡åº¦ï¼Œ<span class="math inline">\(\alpha=0.05\)</span>ï¼‰ã€‚</li>
<li><strong><span class="math inline">\(s
\sqrt{c_{jj}}\)</span></strong>: This whole term is the <strong>standard
error</strong> of the coefficient <span
class="math inline">\(\hat{\beta}_j\)</span>. It measures the precision
of your estimate. A smaller standard error means a narrower, more
precise interval. è¿™æ•´ä¸ªé¡¹æ˜¯ç³»æ•° <span
class="math inline">\(\hat{\beta}_j\)</span>
çš„<strong>æ ‡å‡†è¯¯å·®</strong>ã€‚å®ƒè¡¡é‡æ‚¨ä¼°è®¡çš„ç²¾åº¦ã€‚æ ‡å‡†è¯¯å·®è¶Šå°ï¼ŒåŒºé—´è¶Šçª„ï¼Œç²¾åº¦è¶Šé«˜ã€‚</li>
</ul></li>
</ul>
<h4 id="confidence-interval-for-the-mean-response-å¹³å‡å“åº”çš„ç½®ä¿¡åŒºé—´">2.
Confidence Interval for the Mean Response å¹³å‡å“åº”çš„ç½®ä¿¡åŒºé—´</h4>
<p>This interval addresses the uncertainty about the location of the
regression line itself. è¿™ä¸ªåŒºé—´è§£å†³äº†å›å½’çº¿æœ¬èº«ä½ç½®çš„ä¸ç¡®å®šæ€§ã€‚</p>
<ul>
<li><strong>The Question It Answers:</strong> â€œFor a house with 3
bedrooms and 2 bathrooms, what is the plausible range for the
<em>average</em> selling price of <em>all such houses</em>?â€
<strong>å®ƒå›ç­”çš„é—®é¢˜</strong>ï¼šâ€œå¯¹äºä¸€æ ‹æœ‰ 3 é—´å§å®¤å’Œ 2
é—´æµ´å®¤çš„æˆ¿å­ï¼Œ<em>æ‰€æœ‰æ­¤ç±»æˆ¿å±‹</em>çš„<em>å¹³å‡</em>å”®ä»·çš„åˆç†èŒƒå›´æ˜¯å¤šå°‘ï¼Ÿâ€</li>
<li><strong>The Formula:</strong> <span
class="math inline">\(\hat{\boldsymbol{\beta}}^T \mathbf{x} \pm
t_{n-p-1}(\alpha/2)s\sqrt{\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}}\)</span>
<ul>
<li><strong><span class="math inline">\(\hat{\boldsymbol{\beta}}^T
\mathbf{x}\)</span></strong>: This is your point prediction, <span
class="math inline">\(\hat{y}\)</span>, for the given input vector
<strong>x</strong>. è¿™æ˜¯ç»™å®šè¾“å…¥å‘é‡ <strong>x</strong> çš„ç‚¹é¢„æµ‹ <span
class="math inline">\(\hat{y}\)</span>ã€‚</li>
<li><strong><span
class="math inline">\(s\sqrt{\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}}\)</span></strong>:
This is the standard error of the mean response. Its value depends on
how far the input vector <strong>x</strong> is from the center of the
data. This means the confidence interval is narrowest near the average
of your data and gets wider as you move toward the extremes.
è¿™æ˜¯å¹³å‡å“åº”çš„æ ‡å‡†è¯¯å·®ã€‚å…¶å€¼å–å†³äºè¾“å…¥å‘é‡ <strong>x</strong>
è·ç¦»æ•°æ®ä¸­å¿ƒçš„è·ç¦»ã€‚è¿™æ„å‘³ç€ç½®ä¿¡åŒºé—´åœ¨æ•°æ®å¹³å‡å€¼é™„è¿‘æœ€çª„ï¼Œå¹¶ä¸”éšç€æ¥è¿‘æå€¼è€Œå˜å®½ã€‚</li>
</ul></li>
</ul>
<h4
id="prediction-interval-for-an-individual-response-å•ä¸ªå“åº”çš„é¢„æµ‹åŒºé—´">3.
Prediction Interval for an Individual Response å•ä¸ªå“åº”çš„é¢„æµ‹åŒºé—´</h4>
<p>This is the most comprehensive interval and is often the most useful
for making real-world predictions.
è¿™æ˜¯æœ€å…¨é¢çš„åŒºé—´ï¼Œé€šå¸¸å¯¹äºè¿›è¡Œå®é™…é¢„æµ‹æœ€æœ‰ç”¨ã€‚</p>
<ul>
<li><strong>The Question It Answers:</strong> â€œI want to predict the
selling price for <em>one specific house</em> that has 3 bedrooms and 2
bathrooms. What is a plausible price range for this <em>single
house</em>?â€</li>
<li><strong>The Formula:</strong> <span
class="math inline">\(\hat{\boldsymbol{\beta}}^T \mathbf{x} \pm
t_{n-p-1}(\alpha/2)s\sqrt{1 +
\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}}\)</span></li>
<li><strong>The Key Difference:</strong> Notice the formula is identical
to the one above, except for the <strong><code>1 + ...</code></strong>
inside the square root. This â€œ1â€ is critically important. It accounts
for the second source of uncertainty.
<strong>è¯·æ³¨æ„ï¼Œè¯¥å…¬å¼ä¸ä¸Šé¢çš„å…¬å¼å®Œå…¨ç›¸åŒï¼Œåªæ˜¯å¹³æ–¹æ ¹ä¸­çš„</strong><code>1 + ...</code>**ä¸åŒã€‚è¿™ä¸ªâ€œ1â€è‡³å…³é‡è¦ã€‚å®ƒè§£é‡Šäº†ç¬¬äºŒä¸ªä¸ç¡®å®šæ€§æ¥æºã€‚
<ol type="1">
<li><strong>Uncertainty in the model:</strong> We are not perfectly
certain about the true location of the regression line. This is captured
by the <span
class="math inline">\(\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}\)</span>
term. **æˆ‘ä»¬æ— æ³•å®Œå…¨ç¡®å®šå›å½’çº¿çš„çœŸå®ä½ç½®ã€‚è¿™å¯ä»¥é€šè¿‡ <span
class="math inline">\(\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}\)</span>
é¡¹æ¥æ•æ‰ã€‚</li>
<li><strong>Uncertainty in the individual data point:</strong> Even if
we knew the true regression line perfectly, individual data points would
still be scattered around it due to random error (<span
class="math inline">\(\epsilon\)</span>). The â€œ1â€ in the formula
accounts for this irreducible, random error of a single observation.
å³ä½¿æˆ‘ä»¬å®Œå…¨äº†è§£çœŸå®çš„å›å½’çº¿ï¼Œç”±äºéšæœºè¯¯å·® (<span
class="math inline">\(\epsilon\)</span>)ï¼Œå•ä¸ªæ•°æ®ç‚¹ä»ç„¶ä¼šæ•£å¸ƒåœ¨å…¶å‘¨å›´ã€‚å…¬å¼ä¸­çš„â€œ1â€è§£é‡Šäº†å•ä¸ªè§‚æµ‹å€¼ä¸­è¿™ç§ä¸å¯çº¦çš„éšæœºè¯¯å·®ã€‚</li>
</ol></li>
</ul>
<p>Because it accounts for both types of uncertainty, the
<strong>prediction interval is always wider than the confidence interval
for the mean</strong>.
ç”±äºå®ƒåŒæ—¶è€ƒè™‘äº†ä¸¤ç§ä¸ç¡®å®šæ€§ï¼Œå› æ­¤<strong>é¢„æµ‹åŒºé—´</strong>æ€»æ˜¯æ¯”å‡å€¼çš„ç½®ä¿¡åŒºé—´æ›´å®½ã€‚</p>
<h4 id="the-core-difference-an-analogy-ä¸€ä¸ªç±»æ¯”">The Core Difference: An
Analogy ä¸€ä¸ªç±»æ¯”</h4>
<ul>
<li><p><strong>Confidence Interval (Mean) å‡å€¼ç½®ä¿¡åŒºé—´:</strong> Like
predicting the <strong>average</strong> arrival time for a specific
flight that runs every day. After observing it for a year, you can
predict the average very accurately (e.g., 10:05 AM Â± 2 minutes).
å°±åƒé¢„æµ‹æ¯å¤©ç‰¹å®šèˆªç­çš„<strong>å¹³å‡</strong>åˆ°è¾¾æ—¶é—´ã€‚ç»è¿‡ä¸€å¹´çš„è§‚å¯Ÿï¼Œæ‚¨å¯ä»¥éå¸¸å‡†ç¡®åœ°é¢„æµ‹å¹³å‡å€¼ï¼ˆä¾‹å¦‚ï¼Œä¸Šåˆ
10:05 Â± 2 åˆ†é’Ÿï¼‰ã€‚</p></li>
<li><p><strong>Prediction Interval (Individual) ä¸ªä½“é¢„æµ‹åŒºé—´:</strong>
Like predicting the arrival time for that same flight on <strong>one
specific day</strong> next week. You have to account for the uncertainty
in the average <em>plus</em> the potential for random, one-time events
like weather or air traffic delays. Your prediction must be wider to be
safe (e.g., 10:05 AM Â± 15 minutes).
å°±åƒé¢„æµ‹åŒä¸€èˆªç­ä¸‹å‘¨<strong>æŸä¸€å¤©</strong>çš„åˆ°è¾¾æ—¶é—´ã€‚æ‚¨å¿…é¡»è€ƒè™‘å¹³å‡å€¼çš„ä¸ç¡®å®šæ€§ï¼Œä»¥åŠ*å¯èƒ½å‡ºç°çš„éšæœºã€ä¸€æ¬¡æ€§äº‹ä»¶ï¼Œä¾‹å¦‚å¤©æ°”æˆ–ç©ºä¸­äº¤é€šå»¶è¯¯ã€‚æ‚¨çš„é¢„æµ‹èŒƒå›´å¿…é¡»æ›´å¹¿æ‰èƒ½å®‰å…¨ï¼ˆä¾‹å¦‚ï¼Œä¸Šåˆ
10:05 Â± 15 åˆ†é’Ÿï¼‰ã€‚</p></li>
</ul>
<h1 id="construct-different-types-of-intervals-1">12.construct different
types of intervals</h1>
<p><img src="/imgs/5054C3/ANOVA1.png">
<img src="/imgs/5054C3/ANOVA2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>These slides explain <strong>Analysis of Variance (ANOVA)</strong>, a
method used in regression to break down the total variability in your
data to test if your model is statistically significant as a
whole.è¿™äº›å¹»ç¯ç‰‡è®²è§£äº†<strong>æ–¹å·®åˆ†æ
(ANOVA)</strong>ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå›å½’åˆ†æçš„æ–¹æ³•ï¼Œç”¨äºåˆ†è§£æ•°æ®ä¸­çš„æ€»å˜å¼‚æ€§ï¼Œä»¥æ£€éªŒæ¨¡å‹æ•´ä½“æ˜¯å¦å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ã€‚</p>
<h3 id="summary-5">Summary</h3>
<p>The core idea is to decompose the total variation in the response
variable (<strong>Total Sum of Squares, SS_total</strong>) into two
parts: the variation that is explained by your regression model
(<strong>Regression Sum of Squares, SS_reg</strong>) and the variation
that is left unexplained (<strong>Error Sum of Squares,
SS_error</strong>).
å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†å“åº”å˜é‡çš„æ€»å˜å¼‚ï¼ˆ<strong>æ€»å¹³æ–¹å’Œï¼ŒSS_total</strong>ï¼‰åˆ†è§£ä¸ºä¸¤éƒ¨åˆ†ï¼šå›å½’æ¨¡å‹å¯ä»¥è§£é‡Šçš„å˜å¼‚ï¼ˆ<strong>å›å½’å¹³æ–¹å’Œï¼ŒSS_reg</strong>ï¼‰å’Œæœªè§£é‡Šçš„å˜å¼‚ï¼ˆ<strong>è¯¯å·®å¹³æ–¹å’Œï¼ŒSS_error</strong>ï¼‰ã€‚</p>
<p>By comparing the size of the explained variation to the unexplained
variation using an <strong>F-statistic</strong>, we can formally test
the hypothesis that our model has predictive power. This entire process
is neatly organized in an <strong>ANOVA table</strong>.
é€šè¿‡ä½¿ç”¨<strong>F
ç»Ÿè®¡é‡</strong>æ¯”è¾ƒå·²è§£é‡Šå˜å¼‚ä¸æœªè§£é‡Šå˜å¼‚çš„å¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥æ­£å¼æ£€éªŒæ¨¡å‹å…·æœ‰é¢„æµ‹èƒ½åŠ›çš„å‡è®¾ã€‚æ•´ä¸ªè¿‡ç¨‹éƒ½æ•´é½åœ°ç»„ç»‡åœ¨<strong>æ–¹å·®åˆ†æè¡¨</strong>ä¸­ã€‚</p>
<h3 id="deeper-dive-into-concepts-and-connections">Deeper Dive into
Concepts and Connections</h3>
<h4
id="the-decomposition-of-variances-the-core-equation-æ–¹å·®åˆ†è§£æ ¸å¿ƒæ–¹ç¨‹">1.
The Decomposition of Variances (The Core Equation)
æ–¹å·®åˆ†è§£ï¼ˆæ ¸å¿ƒæ–¹ç¨‹ï¼‰</h4>
<p>The first slide starts with the fundamental equation of ANOVA, which
stems directly from the geometric properties of least squares:
ç¬¬ä¸€å¼ å¹»ç¯ç‰‡ä»¥æ–¹å·®åˆ†æçš„åŸºæœ¬æ–¹ç¨‹å¼€å¤´ï¼Œè¯¥æ–¹ç¨‹ç›´æ¥æºäºæœ€å°äºŒä¹˜çš„å‡ ä½•æ€§è´¨ï¼š</p>
<p><span class="math display">\[SS_{total} = SS_{reg} +
SS_{error}\]</span></p>
<ul>
<li><strong>SS_total (Total Sum of Squares):</strong> <span
class="math inline">\(\sum(y_i - \bar{y})^2\)</span>
<ul>
<li><strong>Concept:</strong> This measures the <strong>total
variation</strong> in your response variable, <code>y</code>. Imagine
you didnâ€™t have a model and your only prediction for any <code>y</code>
was its overall average, <code>È³</code>. SS_total is the total squared
error of this simple â€œmean-onlyâ€ model. It represents the total amount
of variation you are trying to explain.
è¿™æµ‹é‡çš„æ˜¯å“åº”å˜é‡â€œyâ€çš„<strong>æ€»å˜å¼‚</strong>ã€‚å‡è®¾ä½ æ²¡æœ‰æ¨¡å‹ï¼Œä½ å¯¹ä»»ä½•â€œyâ€çš„å”¯ä¸€é¢„æµ‹æ˜¯å®ƒçš„æ•´ä½“å¹³å‡å€¼â€œÈ³â€ã€‚SS_total
æ˜¯è¿™ä¸ªç®€å•çš„â€œä»…å‡å€¼â€æ¨¡å‹çš„æ€»å¹³æ–¹è¯¯å·®ã€‚å®ƒä»£è¡¨äº†ä½ è¯•å›¾è§£é‡Šçš„å˜å¼‚æ€»é‡ã€‚</li>
</ul></li>
<li><strong>SS_reg (Regression Sum of Squares):</strong> <span
class="math inline">\(\sum(\hat{y}_i - \bar{y})^2\)</span>
<ul>
<li><strong>Concept:</strong> This measures the <strong>explained
variation</strong>. Itâ€™s the amount of variation in <code>y</code> that
is captured by your regression model. It calculates the difference
between your modelâ€™s predictions (<code>Å·</code>) and the simple average
(<code>È³</code>). A large SS_reg means your modelâ€™s predictions are a
big improvement over just guessing the average.
<strong>å®ƒè¡¡é‡</strong>è§£é‡Šå˜å¼‚**ã€‚å®ƒæ˜¯å›å½’æ¨¡å‹æ•æ‰åˆ°çš„ y
çš„å˜å¼‚é‡ã€‚å®ƒè®¡ç®—æ¨¡å‹é¢„æµ‹å€¼ï¼ˆâ€œÅ·â€ï¼‰ä¸ç®€å•å¹³å‡å€¼ï¼ˆâ€œÈ³â€ï¼‰ä¹‹é—´çš„å·®å¼‚ã€‚è¾ƒå¤§çš„
SS_reg æ„å‘³ç€æ¨¡å‹çš„é¢„æµ‹ç»“æœæ¯”ä»…ä»…çŒœæµ‹å¹³å‡å€¼æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ul></li>
<li><strong>SS_error (Error Sum of Squares):</strong> <span
class="math inline">\(\sum(y_i - \hat{y}_i)^2\)</span>
<ul>
<li><strong>Concept:</strong> This measures the <strong>unexplained
variation</strong> (also called the Residual Sum of Squares). Itâ€™s the
amount of variation your model <em>failed</em> to capture. Itâ€™s the sum
of the squared differences between the actual data (<code>y</code>) and
your modelâ€™s predictions (<code>Å·</code>).
<strong>å®ƒè¡¡é‡</strong>æœªè§£é‡Šå˜å¼‚**ï¼ˆä¹Ÿç§°ä¸ºæ®‹å·®å¹³æ–¹å’Œï¼‰ã€‚å®ƒæ˜¯æ¨¡å‹<em>æœªèƒ½</em>æ•æ‰åˆ°çš„å˜å¼‚é‡ã€‚å®ƒæ˜¯å®é™…æ•°æ®
(<code>y</code>) ä¸æ¨¡å‹é¢„æµ‹å€¼ (<code>Å·</code>) ä¹‹é—´å¹³æ–¹å·®ä¹‹å’Œã€‚</li>
</ul></li>
</ul>
<p>The <strong>R-squared</strong> value is a direct consequence of this
decomposition. Itâ€™s the proportion of the total variance that is
explained by the model: <strong>R å¹³æ–¹</strong>
å€¼æ˜¯è¿™ç§åˆ†è§£çš„ç›´æ¥ç»“æœã€‚å®ƒæ˜¯æ¨¡å‹è§£é‡Šçš„æ€»æ–¹å·®çš„æ¯”ä¾‹ï¼š</p>
<p><span class="math display">\[R^2 =
\frac{SS_{reg}}{SS_{total}}\]</span></p>
<h4 id="the-anova-table-and-the-f-test">2. The ANOVA Table and the
F-test</h4>
<p>æ–¹å·®åˆ†æè¡¨å’Œ F æ£€éªŒ The second slide organizes these sums of squares
to perform a formal hypothesis test.
ç¬¬äºŒå¼ å¹»ç¯ç‰‡æ•´ç†äº†è¿™äº›å¹³æ–¹å’Œï¼Œä»¥è¿›è¡Œæ­£å¼çš„å‡è®¾æ£€éªŒã€‚</p>
<ul>
<li><strong>The Question:</strong> â€œIs there <em>any</em> relationship
between my set of predictors and the response variable?â€ or â€œIs my model
better than nothing?â€
â€œæˆ‘çš„é¢„æµ‹å˜é‡é›†å’Œå“åº”å˜é‡ä¹‹é—´æ˜¯å¦å­˜åœ¨<em>ä»»ä½•</em>å…³ç³»ï¼Ÿâ€æˆ–â€œæˆ‘çš„æ¨¡å‹æ¯”æ²¡æœ‰æ¨¡å‹å¥½å—ï¼Ÿâ€</li>
<li><strong>The Hypotheses:</strong>
<ul>
<li><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>):</strong> <span
class="math inline">\(\beta_1 = \beta_2 = \dots = \beta_p = 0\)</span>.
(None of the predictors have a relationship with the response; the model
is useless). <strong>é›¶å‡è®¾ (<span
class="math inline">\(H_0\)</span>)</strong>ï¼š<span
class="math inline">\(\beta_1 = \beta_2 = \dots = \beta_p = 0\)</span>ã€‚
ï¼ˆæ‰€æœ‰é¢„æµ‹å˜é‡éƒ½ä¸å“åº”å˜é‡æ— å…³ï¼›è¯¥æ¨¡å‹æ¯«æ— ç”¨å¤„ï¼‰ã€‚</li>
<li><strong>Alternative Hypothesis (<span
class="math inline">\(H_1\)</span>):</strong> At least one <span
class="math inline">\(\beta_j\)</span> is not zero. (The model has some
predictive value). <strong>å¤‡æ‹©å‡è®¾ (<span
class="math inline">\(H_1\)</span>)ï¼š</strong>è‡³å°‘æœ‰ä¸€ä¸ª <span
class="math inline">\(\beta_j\)</span>
ä¸ä¸ºé›¶ã€‚ï¼ˆè¯¥æ¨¡å‹å…·æœ‰ä¸€å®šçš„é¢„æµ‹å€¼ï¼‰ã€‚</li>
</ul></li>
</ul>
<p>To test this, we canâ€™t just compare the raw SS values, because they
depend on the number of data points and predictors. We need to normalize
them. ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä¸èƒ½ä»…ä»…æ¯”è¾ƒåŸå§‹çš„ SS
å€¼ï¼Œå› ä¸ºå®ƒä»¬å–å†³äºæ•°æ®ç‚¹å’Œé¢„æµ‹å˜é‡çš„æ•°é‡ã€‚æˆ‘ä»¬éœ€è¦å¯¹å®ƒä»¬è¿›è¡Œå½’ä¸€åŒ–ã€‚</p>
<ul>
<li><strong>Mean Squares (MS):</strong> This is the â€œaverageâ€ variation.
We calculate it by dividing the Sum of Squares by its <strong>degrees of
freedom (df)</strong>.
<strong>è¿™æ˜¯â€œå¹³å‡â€å˜å¼‚ã€‚æˆ‘ä»¬é€šè¿‡å°†å¹³æ–¹å’Œé™¤ä»¥å…¶</strong>è‡ªç”±åº¦ (df)**
æ¥è®¡ç®—å®ƒã€‚
<ul>
<li><strong>MS_reg</strong> = <span class="math inline">\(SS_{reg} /
p\)</span>. This is the average explained variation <em>per
predictor</em>. è¿™æ˜¯<em>æ¯ä¸ªé¢„æµ‹å˜é‡</em>çš„å¹³å‡è§£é‡Šå˜å¼‚ã€‚</li>
<li><strong>MS_error</strong> = <span class="math inline">\(SS_{error} /
(n - p - 1)\)</span>. This is the average unexplained variation, which
is our estimate of the error variance, <span
class="math inline">\(s^2\)</span>. è¿™æ˜¯å¹³å‡æœªè§£é‡Šå˜å¼‚ï¼Œå³æˆ‘ä»¬å¯¹è¯¯å·®æ–¹å·®
<span class="math inline">\(s^2\)</span> çš„ä¼°è®¡å€¼ã€‚</li>
</ul></li>
</ul>
<h4 id="the-connection-the-f-statistic-è”ç³»f-ç»Ÿè®¡é‡">3. The Connection:
The F-statistic è”ç³»ï¼šF ç»Ÿè®¡é‡</h4>
<p>The <strong>F-statistic</strong> is the key that connects everything.
Itâ€™s the ratio of the two mean squares: <strong>F
ç»Ÿè®¡é‡</strong>æ˜¯è¿æ¥ä¸€åˆ‡çš„å…³é”®ã€‚å®ƒæ˜¯ä¸¤ä¸ªå‡æ–¹çš„æ¯”å€¼ï¼š <span
class="math display">\[F = \frac{\text{Mean Squared
Regression}}{\text{Mean Squared Error}} =
\frac{MS_{reg}}{MS_{error}}\]</span></p>
<ul>
<li><strong>Intuitive Meaning:</strong> The F-statistic is a ratio of
the <strong>average explained variation</strong> to the <strong>average
unexplained variation</strong>. F
ç»Ÿè®¡é‡æ˜¯<strong>å¹³å‡è§£é‡Šå˜å¼‚</strong>ä¸<strong>å¹³å‡æœªè§£é‡Šå˜å¼‚</strong>çš„æ¯”å€¼ã€‚
<ul>
<li>If your model is useless (<span class="math inline">\(H_0\)</span>
is true), the explained variation should be about the same as the
random, unexplained variation. The F-statistic will be close to 1.
å¦‚æœä½ çš„æ¨¡å‹æ— æ•ˆï¼ˆH_0$
ä¸ºçœŸï¼‰ï¼Œåˆ™è§£é‡Šå˜å¼‚åº”è¯¥ä¸éšæœºçš„æœªè§£é‡Šå˜å¼‚å¤§è‡´ç›¸åŒã€‚F ç»Ÿè®¡é‡æ¥è¿‘ 1ã€‚</li>
<li>If your model is useful (<span class="math inline">\(H_1\)</span> is
true), the explained variation should be significantly larger than the
unexplained variation. The F-statistic will be much greater than 1.
å¦‚æœä½ çš„æ¨¡å‹æœ‰æ•ˆï¼ˆH_1$ ä¸ºçœŸï¼‰ï¼Œåˆ™è§£é‡Šå˜å¼‚åº”è¯¥æ˜¾è‘—å¤§äºæœªè§£é‡Šå˜å¼‚ã€‚ F
ç»Ÿè®¡é‡å°†è¿œå¤§äº 1ã€‚</li>
</ul></li>
</ul>
<p>We compare our calculated F-statistic to an
<strong>F-distribution</strong> to get a <strong>p-value</strong>. A
small p-value (&lt; 0.05) provides strong evidence to reject the null
hypothesis and conclude that your model, as a whole, is statistically
significant. æˆ‘ä»¬å°†è®¡ç®—å‡ºçš„ F ç»Ÿè®¡é‡ä¸<strong>F
åˆ†å¸ƒ</strong>è¿›è¡Œæ¯”è¾ƒï¼Œå¾—å‡º<strong>p å€¼</strong>ã€‚è¾ƒå°çš„ p å€¼ï¼ˆ&lt;
0.05ï¼‰å¯ä»¥æä¾›å¼ºæœ‰åŠ›çš„è¯æ®æ¥æ‹’ç»é›¶å‡è®¾ï¼Œå¹¶å¾—å‡ºæ‚¨çš„æ¨¡å‹æ•´ä½“å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§çš„ç»“è®ºã€‚</p>
<h1 id="construct-different-types-of-intervals-2">12.construct different
types of intervals</h1>
<p><img src="/imgs/5054C3/Gauss_Markov1.png">
<img src="/imgs/5054C3/Gauss_Markov2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>: These slides explain the <strong>Gauss-Markov
theorem</strong>, a cornerstone result in statistics that establishes
why the Least Squares Estimator (LSE) is considered the gold standard
for fitting linear models under a specific set of assumptions.
è¿™äº›å¹»ç¯ç‰‡è§£é‡Šäº†<strong>é«˜æ–¯-é©¬å°”å¯å¤«å®šç†</strong>ï¼Œè¿™æ˜¯ç»Ÿè®¡å­¦ä¸­çš„ä¸€ä¸ªåŸºçŸ³æ€§æˆæœï¼Œå®ƒé˜æ˜äº†ä¸ºä»€ä¹ˆæœ€å°äºŒä¹˜ä¼°è®¡é‡
(LSE) è¢«è®¤ä¸ºæ˜¯åœ¨ç‰¹å®šå‡è®¾æ¡ä»¶ä¸‹æ‹Ÿåˆçº¿æ€§æ¨¡å‹çš„é»„é‡‘æ ‡å‡†ã€‚</li>
</ul>
<h3 id="summary-6">Summary</h3>
<p>The slides argue for the superiority of the Least Squares Estimator
(LSE) by highlighting its key properties: itâ€™s easy to compute,
consistent, and efficient. This culminates in the <strong>Gauss-Markov
Theorem</strong>, which proves that LSE is <strong>BLUE</strong>: the
<strong>B</strong>est <strong>L</strong>inear <strong>U</strong>nbiased
<strong>E</strong>stimator. This means that among all estimators that
are both linear and unbiased, the LSE is the â€œbestâ€ because it has the
smallest possible variance, making it the most precise. The second slide
provides the key steps for the mathematical proof of this important
theorem. è¿™äº›å¹»ç¯ç‰‡é€šè¿‡å¼ºè°ƒæœ€å°äºŒä¹˜ä¼°è®¡é‡ (LSE)
çš„å…³é”®ç‰¹æ€§æ¥è®ºè¯å…¶ä¼˜è¶Šæ€§ï¼šæ˜“äºè®¡ç®—ã€ä¸€è‡´æ€§é«˜ä¸”é«˜æ•ˆã€‚æœ€ç»ˆå¾—å‡ºäº†<strong>é«˜æ–¯-é©¬å°”å¯å¤«å®šç†</strong>ï¼Œè¯¥å®šç†è¯æ˜äº†
LSE
æ˜¯<strong>BLUE</strong>ï¼š<strong>æœ€ä½³</strong>çº¿æ€§<strong>æ— å</strong>ä¼°è®¡é‡ã€‚è¿™æ„å‘³ç€åœ¨æ‰€æœ‰çº¿æ€§ä¸”æ— åçš„ä¼°è®¡é‡ä¸­ï¼ŒLSE
æ˜¯â€œæœ€ä½³â€çš„ï¼Œå› ä¸ºå®ƒå…·æœ‰æœ€å°çš„æ–¹å·®ï¼Œå› æ­¤ç²¾åº¦æœ€é«˜ã€‚ç¬¬äºŒå¼ å¹»ç¯ç‰‡æä¾›äº†è¿™ä¸€é‡è¦å®šç†çš„æ•°å­¦è¯æ˜çš„å…³é”®æ­¥éª¤ã€‚</p>
<h3 id="deeper-dive-into-the-concepts">Deeper Dive into the
Concepts</h3>
<h4 id="properties-of-lse-slide-1-å±€éƒ¨æ­£äº¤ä¼°è®¡-lse-çš„æ€§è´¨">Properties of
LSE (Slide 1) å±€éƒ¨æ­£äº¤ä¼°è®¡ (LSE) çš„æ€§è´¨</h4>
<ul>
<li><strong>Easy Computationæ˜“äºè®¡ç®—:</strong> The LSE has a direct,
closed-form solution called the Normal Equation (<span
class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>). You can
calculate it directly without needing complex iterative algorithms.</li>
<li><strong>Consistencyä¸€è‡´æ€§:</strong> As your sample size gets larger
and larger, the LSE estimate (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) is guaranteed
to get closer and closer to the true population value (<span
class="math inline">\(\boldsymbol{\beta}\)</span>). With enough data, it
will find the truth. éšç€æ ·æœ¬é‡è¶Šæ¥è¶Šå¤§ï¼ŒLSE ä¼°è®¡å€¼ (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>)
å¿…ç„¶ä¼šè¶Šæ¥è¶Šæ¥è¿‘çœŸå®çš„æ€»ä½“å€¼ (<span
class="math inline">\(\boldsymbol{\beta}\)</span>)ã€‚åªè¦æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œå®ƒå°±èƒ½æ‰¾åˆ°çœŸç›¸ã€‚</li>
<li><strong>Efficiencyæ•ˆç‡:</strong> An efficient estimator is the one
with the lowest possible variance. This means its estimates are the most
precise and least spread out.
é«˜æ•ˆçš„ä¼°è®¡å™¨æ˜¯æ–¹å·®å°½å¯èƒ½ä½çš„ä¼°è®¡å™¨ã€‚è¿™æ„å‘³ç€å®ƒçš„ä¼°è®¡å€¼æœ€ç²¾ç¡®ï¼Œä¸”åˆ†å¸ƒæœ€å‡åŒ€ã€‚</li>
<li><strong>BLUE (Best Linear Unbiased
Estimator)BLUEï¼ˆæœ€ä½³çº¿æ€§æ— åä¼°è®¡å™¨ï¼‰:</strong> This acronym elegantly
summarizes the Gauss-Markov theorem.
è¿™ä¸ªç¼©å†™å®Œç¾åœ°æ¦‚æ‹¬äº†é«˜æ–¯-é©¬å°”å¯å¤«å®šç†ã€‚
<ul>
<li><strong>Linear:</strong> The estimator is a linear function of the
response variable <strong>y</strong>. We can write it as <span
class="math inline">\(\hat{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span> for some matrix <strong>A</strong>.
ä¼°è®¡å™¨æ˜¯å“åº”å˜é‡<strong>y</strong>çš„çº¿æ€§å‡½æ•°ã€‚å¯¹äºæŸä¸ªçŸ©é˜µ<strong>A</strong>ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶å†™æˆ
<span class="math inline">\(\hat{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span>ã€‚</li>
<li><strong>Unbiased:</strong> The estimator does not systematically
overestimate or underestimate the true parameter. On average, its
expected value is the true value: <span
class="math inline">\(E[\hat{\boldsymbol{\beta}}] =
\boldsymbol{\beta}\)</span>.
ä¼°è®¡å™¨ä¸ä¼šç³»ç»Ÿæ€§åœ°é«˜ä¼°æˆ–ä½ä¼°çœŸå®å‚æ•°ã€‚å¹³å‡è€Œè¨€ï¼Œå…¶é¢„æœŸå€¼å³ä¸ºçœŸå®å€¼ï¼š<span
class="math inline">\(E[\hat{\boldsymbol{\beta}}] =
\boldsymbol{\beta}\)</span>ã€‚</li>
<li><strong>Best:</strong> It has the <strong>minimum variance</strong>
of all possible linear unbiased estimators. Itâ€™s the most precise and
reliable estimator in its class.
åœ¨æ‰€æœ‰å¯èƒ½çš„çº¿æ€§æ— åä¼°è®¡å™¨ä¸­ï¼Œå®ƒçš„<strong>æ–¹å·®</strong>æœ€å°ã€‚å®ƒæ˜¯åŒç±»ä¸­æœ€ç²¾ç¡®ã€æœ€å¯é çš„ä¼°è®¡å™¨ã€‚</li>
</ul></li>
</ul>
<h4 id="the-gauss-markov-theorem-é«˜æ–¯-é©¬å°”å¯å¤«å®šç†">The Gauss-Markov
Theorem é«˜æ–¯-é©¬å°”å¯å¤«å®šç†</h4>
<p>The theorem provides the theoretical justification for using OLS.
è¯¥å®šç†ä¸ºä½¿ç”¨æœ€å°äºŒä¹˜æ³• (OLS) æä¾›äº†ç†è®ºä¾æ®ã€‚ * <strong>The Core
Idea:</strong> You could invent many different ways to estimate the
coefficients of a linear model. As long as your proposed methods are
both linear and unbiased, the Gauss-Markov theorem guarantees that none
of them will be more precise than the standard least squares method. LSE
gives the â€œsharpestâ€ possible estimates.
ä½ å¯ä»¥å‘æ˜è®¸å¤šä¸åŒçš„æ–¹æ³•æ¥ä¼°è®¡çº¿æ€§æ¨¡å‹çš„ç³»æ•°ã€‚åªè¦ä½ æå‡ºçš„æ–¹æ³•æ˜¯çº¿æ€§çš„ä¸”æ— åçš„ï¼Œé«˜æ–¯-é©¬å°”å¯å¤«å®šç†å°±èƒ½ä¿è¯ï¼Œå®ƒä»¬éƒ½ä¸ä¼šæ¯”æ ‡å‡†æœ€å°äºŒä¹˜æ³•æ›´ç²¾ç¡®ã€‚æœ€å°äºŒä¹˜æ³•
(LSE) ç»™å‡ºäº†â€œæœ€ç²¾ç¡®â€çš„ä¼°è®¡å€¼ã€‚</p>
<ul>
<li><strong>The Logic of the Proof (Slide 2) è¯æ˜é€»è¾‘:</strong> The
proof is a clever comparison of variances. **è¯¥è¯æ˜å·§å¦™åœ°æ¯”è¾ƒäº†æ–¹å·®ã€‚
<ol type="1">
<li>It starts by defining <strong>any</strong> other linear unbiased
estimator as <span class="math inline">\(\tilde{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span>.
é¦–å…ˆï¼Œå°†<strong>ä»»ä½•</strong>å…¶ä»–çº¿æ€§æ— åä¼°è®¡é‡å®šä¹‰ä¸º <span
class="math inline">\(\tilde{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span>ã€‚</li>
<li>It uses the â€œunbiasedâ€ property to force a condition on the matrix
<strong>A</strong>, which ultimately leads to the insight that
<strong>A</strong> can be written in terms of the LSE matrix plus some
other matrix <strong>D</strong>, where <strong>DX = 0</strong>.
å®ƒåˆ©ç”¨â€œæ— åâ€æ€§è´¨å¯¹çŸ©é˜µ<strong>A</strong>å¼ºåˆ¶æ–½åŠ ä¸€ä¸ªæ¡ä»¶ï¼Œæœ€ç»ˆå¾—å‡º<strong>A</strong>å¯ä»¥å†™æˆLSEçŸ©é˜µåŠ ä¸Šå¦ä¸€ä¸ªçŸ©é˜µ<strong>D</strong>ï¼Œå…¶ä¸­<strong>DX
= 0</strong>ã€‚</li>
<li>It then calculates the variance of this other estimator, which turns
out to be: <span class="math display">\[Var(\tilde{\boldsymbol{\beta}})
= Var(\text{LSE}) + \text{a non-negative term involving }
\mathbf{D}\]</span> ç„¶åè®¡ç®—å¦ä¸€ä¸ªä¼°è®¡é‡çš„æ–¹å·®ï¼Œç»“æœä¸ºï¼š <span
class="math display">\[Var(\tilde{\boldsymbol{\beta}}) = Var(\text{LSE})
+ \text{ä¸€ä¸ªåŒ…å« } \mathbf{D} çš„éè´Ÿé¡¹\]</span></li>
<li>Since the variance of any other linear unbiased estimator is the
variance of the LSE <em>plus</em> something non-negative, the variance
of the LSE must be the smallest possible value.
ç”±äºä»»ä½•å…¶ä»–çº¿æ€§æ— åä¼°è®¡é‡çš„æ–¹å·®éƒ½æ˜¯LSEçš„æ–¹å·®<em>åŠ ä¸Š</em>ä¸€ä¸ªéè´Ÿé¡¹ï¼Œå› æ­¤LSEçš„æ–¹å·®å¿…é¡»æ˜¯æœ€å°çš„å¯èƒ½å€¼ã€‚</li>
</ol></li>
</ul>
<h3 id="further-understandings-beyond-the-slides">Further Understandings
Beyond the Slides</h3>
<h4 id="what-are-the-required-assumptionséœ€è¦å“ªäº›å‡è®¾">1. What are the
required assumptions?éœ€è¦å“ªäº›å‡è®¾ï¼Ÿ</h4>
<p>The Gauss-Markov theorem is powerful, but itâ€™s not magic. It only
holds if a set of assumptions about the modelâ€™s errors (<span
class="math inline">\(\epsilon\)</span>) are met:
é«˜æ–¯-é©¬å°”å¯å¤«å®šç†è™½ç„¶å¼ºå¤§ï¼Œä½†å¹¶éé­”æ³•ã€‚å®ƒä»…åœ¨æ»¡è¶³ä»¥ä¸‹å…³äºæ¨¡å‹è¯¯å·® (<span
class="math inline">\(\epsilon\)</span>) çš„å‡è®¾æ—¶æˆç«‹ï¼š * <strong>Zero
Meané›¶å‡å€¼:</strong> The average of the errors is zero (<span
class="math inline">\(E[\epsilon] = 0\)</span>). è¯¯å·®çš„å¹³å‡å€¼ä¸ºé›¶ (<span
class="math inline">\(E[\epsilon] = 0\)</span>)ã€‚ * <strong>Constant
Variance (Homoscedasticity)æ’å®šæ–¹å·®ï¼ˆåŒæ–¹å·®æ€§ï¼‰:</strong> The errors
have the same variance, <span class="math inline">\(\sigma^2\)</span>,
at all levels of the predictors.
<strong>åœ¨é¢„æµ‹å˜é‡çš„å„ä¸ªæ°´å¹³ä¸Šï¼Œè¯¯å·®å…·æœ‰ç›¸åŒçš„æ–¹å·® <span
class="math inline">\(\sigma^2\)</span>ã€‚ * </strong>Uncorrelated
Errorsä¸ç›¸å…³è¯¯å·®:** The error for one observation is not correlated with
the error for another. ä¸€ä¸ªè§‚æµ‹å€¼çš„è¯¯å·®ä¸å¦ä¸€ä¸ªè§‚æµ‹å€¼çš„è¯¯å·®ä¸ç›¸å…³ã€‚ *
<strong>No Perfect Multicollinearityéå®Œå…¨å¤šé‡å…±çº¿æ€§:</strong> The
predictor variables are not perfectly linearly related.
é¢„æµ‹å˜é‡å¹¶éå®Œå…¨çº¿æ€§ç›¸å…³ã€‚</p>
<p><strong>Crucially, the Gauss-Markov theorem does NOT require the
errors to be normally distributed.</strong> The normality assumption is
only needed later for constructing confidence intervals and conducting
t-tests and F-tests.
è‡³å…³é‡è¦çš„æ˜¯ï¼Œé«˜æ–¯-é©¬å°”å¯å¤«å®šç†å¹¶ä¸è¦æ±‚è¯¯å·®æœä»æ­£æ€åˆ†å¸ƒã€‚**æ­£æ€æ€§å‡è®¾ä»…åœ¨æ„å»ºç½®ä¿¡åŒºé—´ä»¥åŠè¿›è¡Œ
t æ£€éªŒå’Œ F æ£€éªŒæ—¶éœ€è¦ã€‚</p>
<h4
id="when-is-lse-not-the-best-the-bias-variance-tradeoff-ä»€ä¹ˆæ—¶å€™-lse-ä¸æ˜¯æœ€ä½³é€‰æ‹©-åå·®-æ–¹å·®æƒè¡¡">2.
When is LSE NOT the Best? (The Bias-Variance Tradeoff) ä»€ä¹ˆæ—¶å€™ LSE
ä¸æ˜¯æœ€ä½³é€‰æ‹©ï¼Ÿ ï¼ˆåå·®-æ–¹å·®æƒè¡¡ï¼‰</h4>
<p>While LSE is the best <em>unbiased</em> estimator, sometimes we can
get better predictive performance by accepting a little bit of bias in
exchange for a large reduction in variance. This is the core idea behind
modern regularization methods: è™½ç„¶ LSE
æ˜¯æœ€å¥½çš„<em>æ— å</em>ä¼°è®¡å™¨ï¼Œä½†æœ‰æ—¶æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¥å—å°‘é‡åå·®æ¥å¤§å¹…é™ä½æ–¹å·®ï¼Œä»è€Œè·å¾—æ›´å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚è¿™æ˜¯ç°ä»£æ­£åˆ™åŒ–æ–¹æ³•èƒŒåçš„æ ¸å¿ƒæ€æƒ³ï¼š
* <strong>Ridge Regression and LASSOå²­å›å½’å’Œ LASSO:</strong> These are
popular techniques that produce <em>biased</em> estimates of the
coefficients. However, by introducing this small amount of bias, they
can often produce models with a lower overall error (Mean Squared Error)
than LSE, especially when predictors are highly correlated.
è¿™äº›æ˜¯äº§ç”Ÿ<em>æœ‰å</em>ç³»æ•°ä¼°è®¡çš„æµè¡ŒæŠ€æœ¯ã€‚ç„¶è€Œï¼Œé€šè¿‡å¼•å…¥å°‘é‡åå·®ï¼Œå®ƒä»¬é€šå¸¸å¯ä»¥ç”Ÿæˆæ¯”
LSE
å…·æœ‰æ›´ä½æ€»ä½“è¯¯å·®ï¼ˆå‡æ–¹è¯¯å·®ï¼‰çš„æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨é¢„æµ‹å˜é‡é«˜åº¦ç›¸å…³çš„æƒ…å†µä¸‹ã€‚
Therefore, while LSE is the theoretical champion in the world of
unbiased estimators, in the world of predictive modeling, methods that
intentionally introduce bias can sometimes be superior. å› æ­¤ï¼Œè™½ç„¶ LSE
æ˜¯æ— åä¼°è®¡é¢†åŸŸçš„ç†è®ºå† å†›ï¼Œä½†åœ¨é¢„æµ‹æ¨¡å‹é¢†åŸŸï¼Œæœ‰æ„å¼•å…¥åå·®çš„æ–¹æ³•æœ‰æ—¶ä¼šæ›´èƒœä¸€ç­¹ã€‚</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="ä¸‹ä¸€é¡µ"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> å¼ºåŠ›é©±åŠ¨
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
