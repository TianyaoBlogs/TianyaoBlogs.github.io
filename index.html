<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianyaoblogs.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="TianyaoBlogs">
<meta property="og:url" content="https://tianyaoblogs.github.io/index.html">
<meta property="og:site_name" content="TianyaoBlogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://tianyaoblogs.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>TianyaoBlogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TianyaoBlogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/16/5054C3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/16/5054C3/" class="post-title-link" itemprop="url">Statistical Machine Learning-L3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-09-16 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-16T21:00:00+08:00">2025-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-18 10:44:28" itemprop="dateModified" datetime="2025-09-18T10:44:28+08:00">2025-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>统计机器学习Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="general-linear-regression-model.">1. General linear regression
model.</h1>

<h2 id="general-linear-regression-model">1.1 general linear regression
model</h2>
<ul>
<li><strong>内容</strong>: <strong>general linear regression
model</strong>.</li>
</ul>
<p>the fundamental equation:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1x_{i1} + \dots +
\beta_px_{ip} + \epsilon_i\]</span></p>
<p>And it correctly identifies the main goal: to <strong>estimate the
parameters</strong> (the coefficients <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span>) from
data so we can make predictions on new data.</p>
<p>核心目标：通过数据<strong>估计参数</strong>（即系数 <span class="math inline">\(\beta_0, \beta_1, \dots,
\beta_p\)</span>），从而对新数据进行预测。</p>
<h2 id="how-we-actually-find-the-best-values-for-the-β-coefficients-parameter-estimation">1.2
How we actually find the best values for the <span class="math inline">\(β\)</span> coefficients (parameter
estimation)?:</h2>
<ul>
<li><strong>内容</strong>: We find the best values for the <span class="math inline">\(\beta\)</span> coefficients by finding the values
that <strong>minimize the overall error</strong> of the model. The most
common and fundamental method for this is called <strong>Ordinary Least
Squares (OLS)</strong>.</li>
</ul>
<hr>
<h3 id="the-main-method-ordinary-least-squares-ols-普通最小二乘法-ols">##
The Main Method: Ordinary Least Squares (OLS) 普通最小二乘法 (OLS)</h3>
<p>The core idea of OLS is to find the line (or hyperplane in multiple
dimensions) that is as close as possible to all the data points
simultaneously. OLS
的核心思想是找到一条尽可能同时接近所有数据点的直线（或多维超平面）。</p>
<h4 id="define-the-error-residuals-误差">1. Define the Error (Residuals)
误差</h4>
<p>First, we need to define what “error” means. For any single data
point, the error is the difference between the actual value (<span class="math inline">\(y_i\)</span>) and the value predicted by our model
(<span class="math inline">\(\hat{y}_i\)</span>). This difference is
called the <strong>residual</strong>.
首先，需要定义“误差”的含义。对于任何单个数据点，误差是实际值 (<span class="math inline">\(y_i\)</span>) 与模型预测值 (<span class="math inline">\(\hat{y}_i\)</span>)
之间的差值。这个差值称为<strong>残差</strong>。</p>
<p><strong>Residual</strong> = Actual Value - Predicted Value
<strong>残差</strong> = 实际值 - 预测值 <span class="math display">\[e_i
= y_i - \hat{y}_i\]</span></p>
<p>You can visualize residuals as the vertical distance from each data
point to the regression line.
可以将残差可视化为每个数据点到回归线的垂直距离。</p>
<h4 id="the-cost-function-sum-of-squared-residuals-成本函数残差平方和">2.
The Cost Function: Sum of Squared Residuals 成本函数：残差平方和</h4>
<p>We want to make all these residuals as small as possible. We can’t
just add them up, because some are positive and some are negative, and
they would cancel each other out.
所有残差尽可能小。不能简单地将它们相加，因为有些是正数，有些是负数，它们会相互抵消。</p>
<p>So, we square each residual (which makes them all positive) and then
sum them up. This gives us the <strong>Sum of Squared Residuals
(SSR)</strong>, which is our “cost function.”
因此，将每个残差求平方（使它们都为正数），然后将它们相加。这就得到了<strong>残差平方和
(SSR)</strong>，也就是“成本函数”。</p>
<p><span class="math display">\[SSR = \sum_{i=1}^{n} e_i^2 =
\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span></p>
<p>The goal of OLS is simple: <strong>find the values of <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span> that
make this SSR value as small as possible.</strong></p>
<h4 id="solving-for-the-coefficients-the-normal-equation-求解系数正态方程">3.
Solving for the Coefficients: The Normal Equation
求解系数：正态方程</h4>
<p>For linear regression, calculus provides a direct, exact solution to
this minimization problem. By taking the derivative of the SSR function
with respect to each <span class="math inline">\(\beta\)</span>
coefficient and setting it to zero, we can solve for the optimal values.
对于线性回归，微积分为这个最小化问题提供了直接、精确的解。通过对 SSR
函数的每个 <span class="math inline">\(\beta\)</span>
系数求导并将其设为零，就可以求解出最优值。</p>
<p>This process results in a formula known as the <strong>Normal
Equation</strong>, which can be expressed cleanly using matrix algebra:
这个过程会得到一个称为<strong>正态方程</strong>的公式，它可以用矩阵代数清晰地表示出来：</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the
vector of our estimated coefficients.估计系数的向量。</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is a matrix where
each row is an observation and each column is a feature (with an added
column of 1s for the intercept <span class="math inline">\(\beta_0\)</span>).其中每一行代表一个观测值，每一列代表一个特征（截距
<span class="math inline">\(\beta_0\)</span> 增加了一列全为 1
的值）。</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of the
actual response values.实际响应值的向量。</li>
</ul>
<p>Statistical software and programming libraries (like Scikit-learn in
Python) use this equation (or more computationally stable versions of
it) to find the best coefficients for you instantly.</p>
<h3 id="an-alternative-method-gradient-descent-梯度下降">## An
Alternative Method: Gradient Descent 梯度下降</h3>
<p>While the Normal Equation gives a direct answer, it can be very slow
if you have a massive number of features (e.g., hundreds of thousands).
An alternative, iterative method used across machine learning is
<strong>Gradient Descent</strong>.</p>
<p><strong>The Intuition:</strong> Imagine the SSR cost function is a
big valley. Your initial (random) <span class="math inline">\(\beta\)</span> coefficients place you somewhere on
the slope of this valley.</p>
<ol type="1">
<li><strong>Check the slope</strong> (the gradient) at your current
position. <strong>检查您当前位置的斜率</strong>（梯度）。</li>
<li><strong>Take a small step</strong> in the steepest <em>downhill</em>
direction. <strong>朝最陡的<em>下坡</em>方向</strong>迈出一小步**。</li>
<li><strong>Repeat.</strong> You keep taking steps downhill until you
reach the bottom of the valley. The bottom of the valley represents the
minimum SSR, and your coordinates at that point are the optimal <span class="math inline">\(\beta\)</span> coefficients.
<strong>重复</strong>。您继续向下走，直到到达山谷底部。谷底代表最小SSR，该点的坐标即为最优<span class="math inline">\(\beta\)</span>系数。</li>
</ol>
<p>The size of each “step” you take is controlled by a parameter called
the <strong>learning rate</strong>. Gradient Descent is the foundational
optimization algorithm for many complex models, including neural
networks.
每次“步进”的大小由一个称为<strong>学习率</strong>的参数控制。梯度下降是许多复杂模型（包括神经网络）的基础优化算法。</p>
<h3 id="summary-ols-vs.-gradient-descent">## Summary: OLS vs. Gradient
Descent</h3>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Ordinary Least Squares (OLS)</th>
<th style="text-align: left;">Gradient Descent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>How it works</strong></td>
<td style="text-align: left;">Direct calculation using the Normal
Equation.</td>
<td style="text-align: left;">Iterative; takes steps to minimize
error.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Pros</strong></td>
<td style="text-align: left;">Provides an exact, optimal solution. No
parameters to tune.</td>
<td style="text-align: left;">More efficient for very large datasets.
Very versatile.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Cons</strong></td>
<td style="text-align: left;">Can be computationally expensive with many
features.</td>
<td style="text-align: left;">Requires choosing a learning rate. May not
find the exact minimum.</td>
</tr>
</tbody>
</table>
<h1 id="simple-linear-regression">2. Simple Linear Regression</h1>

<h2 id="simple-linear-regression-1">2.1 Simple Linear Regression</h2>
<ul>
<li><strong>内容</strong>: <strong>Simple Linear Regression:</strong> a
special case of the general model you showed earlier where you only have
<strong>one</strong> predictor variable (<span class="math inline">\(p=1\)</span>).</li>
</ul>
<h3 id="the-model-and-the-goal-模型和目标">## The Model and the Goal
模型和目标</h3>
<p>Sets up the simplified equation for a line: <span class="math display">\[y_i = \beta_0 + \beta_1x_i + \epsilon_i\]</span>
* <span class="math inline">\(y_i\)</span> is the outcome you want to
predict.要预测的结果。 * <span class="math inline">\(x_i\)</span> is
your single input feature or covariate.单个输入特征或协变量。 * <span class="math inline">\(\beta_1\)</span> is the <strong>slope</strong> of
the line. It tells you how much <span class="math inline">\(y\)</span>
is expected to increase for a one-unit increase in <span class="math inline">\(x\)</span>.表示 <span class="math inline">\(x\)</span> 每增加一个单位，<span class="math inline">\(y\)</span> 预计会增加多少。 * <span class="math inline">\(\beta_0\)</span> is the
<strong>intercept</strong>. It’s the predicted value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> is zero.当 <span class="math inline">\(x\)</span> 为零时 <span class="math inline">\(y\)</span> 的预测值。 * <span class="math inline">\(\epsilon_i\)</span> is the random error
term.是随机误差项。</p>
<p>The goal, stated as “Minimize the sum of squares of err,” is exactly
the <strong>Ordinary Least Squares (OLS)</strong> method we just
discussed. It’s written here as: <span class="math display">\[\min_{a,b}
\sum_{i=1}^{n} (y_i - a - bx_i)^2\]</span> This is just a different way
of writing the same thing, where they use <code>a</code> for the
intercept (<span class="math inline">\(\beta_0\)</span>) and
<code>b</code> for the slope (<span class="math inline">\(\beta_1\)</span>). You’re trying to find the
specific values of the slope and intercept that make the sum of all the
squared errors as small as possible.
目标，即“最小化误差平方和”，正是<strong>普通最小二乘法
(OLS)</strong>。： <span class="math display">\[\min_{a,b}
\sum_{i=1}^{n} (y_i - a - bx_i)^2\]</span> 这是另一种写法，其中用
<code>a</code> 表示截距 (<span class="math inline">\(\beta_0\)</span>)，<code>b</code> 表示斜率 (<span class="math inline">\(\beta_1\)</span>)。尝试找到斜率和截距的具体值，使得所有平方误差之和尽可能小。</p>
<h3 id="the-solution-the-estimator-formulas-解决方案估计公式">## The
Solution: The Estimator Formulas 解决方案：估计公式</h3>
<p>The most important part of this slide is the
<strong>solution</strong>. For the simple case with only one variable,
you don’t need complex matrix algebra (the Normal Equation). Instead,
the minimization problem can be solved with these two straightforward
formulas:
对于只有一个变量的简单情况，不需要复杂的矩阵代数（正态方程）。相反，最小化问题可以用以下两个简单的公式来解决：</p>
<h4 id="the-slope-hatbeta_1">1. The Slope: <span class="math inline">\(\hat{\beta}_1\)</span></h4>
<p><span class="math display">\[\hat{\beta}_1 = \frac{\sum_{i=1}^{n}
(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i -
\bar{x})^2}\]</span> * <strong>Intuition:</strong> This formula might
look complex, but it’s actually very intuitive. * The numerator, <span class="math inline">\(\sum(x_i - \bar{x})(y_i - \bar{y})\)</span>, is
closely related to the <strong>covariance</strong> between X and Y. It
measures whether X and Y tend to move in the same direction (positive
slope) or in opposite directions (negative slope). 与 X 和 Y
之间的<strong>协方差</strong>密切相关。它衡量 X 和 Y
是倾向于朝相同方向（正斜率）还是朝相反方向（负斜率）移动。 * The
denominator, <span class="math inline">\(\sum(x_i - \bar{x})^2\)</span>,
is related to the <strong>variance</strong> of X. It measures how much X
varies on its own. 它衡量 X 自身的变化量。 * <strong>In short, the slope
is a measure of how X and Y vary together, scaled by how much X varies
by itself.</strong> 斜率衡量的是 X 和 Y 共同变化的程度，并以 X
自身的变化量为标度。</p>
<h4 id="the-intercept-hatbeta_0-截距">2. The Intercept: <span class="math inline">\(\hat{\beta}_0\)</span> 截距</h4>
<p><span class="math display">\[\hat{\beta}_0 = \bar{y} -
\hat{\beta}_1\bar{x}\]</span> * <strong>Intuition:</strong> This formula
is even simpler and has a wonderful geometric meaning. It ensures that
the <strong>line of best fit always passes through the “center of mass”
of the data</strong>, which is the point of averages <span class="math inline">\((\bar{x}, \bar{y})\)</span>.
它确保<strong>最佳拟合线始终穿过数据的“质心”</strong>，即平均值 <span class="math inline">\((\bar{x}, \bar{y})\)</span> 的点。计算出最佳斜率
(<span class="math inline">\(\hat{\beta}_1\)</span>)
后，就可以将其代入此公式。然后，可以调整截距 (<span class="math inline">\(\hat{\beta}_0\)</span>)，使直线完美地围绕数据云的中心点旋转。
* Once you’ve calculated the best slope (<span class="math inline">\(\hat{\beta}_1\)</span>), you can plug it into this
formula. You then adjust the intercept (<span class="math inline">\(\hat{\beta}_0\)</span>) so that the line pivots
perfectly around the central point of your data cloud.</p>
<p>In summary, this slide provides the precise, closed-form formulas to
calculate the slope and intercept for the line of best fit in a simple
linear regression model.</p>
<h1 id="statistical-inference">3. Statistical Inference</h1>


<h2 id="statistical-inference-1">3.1 Statistical Inference</h2>
<ul>
<li><strong>内容</strong>: <strong>Statistical Inference:</strong> These
two slides are deeply connected and explain how we go from just
<em>calculating</em> the coefficients to understanding how
<em>accurate</em> and <em>reliable</em> they are.
解释了我们如何从仅仅<em>计算</em>系数到理解它们的<em>准确性</em>和<em>可靠性</em>。</li>
</ul>
<h3 id="the-core-problem-quantifying-uncertainty-量化不确定性">## The
Core Problem: Quantifying Uncertainty 量化不确定性</h3>
<p>The second slide poses the fundamental questions: * “How accurate are
<span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>?”准确性如何？ * “What are
the distributions of <span class="math inline">\(\hat{\beta}_0\)</span>
and <span class="math inline">\(\hat{\beta}_1\)</span>?”分布是什么？</p>
<p>The reason we ask this is that our estimated coefficients (<span class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>) were
calculated from a <strong>specific sample of data</strong>. If we
collected a different random sample from the same population, we would
get slightly different estimates.估计的系数 (<span class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>)
是根据<strong>特定的数据样本</strong>计算出来的。如果我们从同一总体中随机抽取不同的样本，我们得到的估计值会略有不同。</p>
<p>The goal of statistical inference is to use the estimates from our
single sample to make conclusions about the <strong>true, unknown
population parameters</strong> (<span class="math inline">\(\beta_0,
\beta_1\)</span>) and to quantify our uncertainty about
them.统计推断的目标是利用单个样本的估计值得出关于<strong>真实、未知的总体参数</strong>（<span class="math inline">\(\beta_0,
\beta_1\)</span>）的结论，并量化对这些参数的不确定性。</p>
<h3 id="the-key-assumption-that-makes-it-possible-实现这一目标的关键假设">##
The Key Assumption That Makes It Possible 实现这一目标的关键假设</h3>
<p>To figure out the distribution of our estimates, we must make an
assumption about the distribution of the errors. This is the most
important assumption in linear regression for inference:
为了确定估计值的分布，必须对误差的分布做出假设。这是线性回归推断中最重要的假设：
<strong>Assumption:</strong> <span class="math inline">\(\epsilon_i
\stackrel{\text{i.i.d.}}{\sim} N(0, \sigma^2)\)</span></p>
<p>This means we assume the random error terms are: * <strong>Normally
distributed</strong> (<span class="math inline">\(N\)</span>).*
<strong>正态分布</strong>（<span class="math inline">\(N\)</span>）。 *
Have a mean of <strong>zero</strong> (our model is correct on average).*
均值为<strong>零</strong>（模型平均而言是正确的）。 * Have a constant
variance <strong><span class="math inline">\(\sigma^2\)</span></strong>
(homoscedasticity).* 方差为常数<strong><span class="math inline">\(\sigma^2\)</span></strong>（方差齐性）。 * Are
<strong>independent and identically distributed</strong> (i.i.d.),
meaning each error is independent of the others.*
是<strong>独立同分布</strong>（i.i.d.）的，这意味着每个误差都独立于其他误差。</p>
<p><strong>Why is this important?</strong> Because our coefficients
<span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are calculated as weighted
sums of the <span class="math inline">\(y_i\)</span> values, and the
<span class="math inline">\(y_i\)</span> values depend on the errors
<span class="math inline">\(\epsilon_i\)</span>. This assumption about
the errors allows us to prove that our estimated coefficients themselves
are also normally distributed. 系数 <span class="math inline">\(\hat{\beta}_0\)</span> 和 <span class="math inline">\(\hat{\beta}_1\)</span> 是通过 <span class="math inline">\(y_i\)</span> 值的加权和计算的，而 <span class="math inline">\(y_i\)</span> 值取决于误差 <span class="math inline">\(\epsilon_i\)</span>。这个关于误差的假设使能够证明估计的系数本身也服从正态分布。</p>
<h3 id="the-solution-the-theorem-and-the-t-distribution-定理和-t-分布">##
The Solution: The Theorem and the t-distribution 定理和 t 分布</h3>
<p>The first slide provides the central theorem that allows us to
perform inference. It tells us exactly how to standardize our estimated
coefficients so they follow a known distribution.
第一张幻灯片提供了进行推断的核心定理。它准确地告诉我们如何对估计的系数进行标准化，使其服从已知的分布。</p>
<h4 id="the-standard-error-s.e.-标准误差-s.e.">1. The Standard Error
(s.e.) 标准误差 (s.e.)</h4>
<p>First, look at the denominators in the red dotted boxes. These are
the <strong>standard errors</strong> of the coefficients,
<code>s.e.($\hat&#123;\beta&#125;_1$)</code> and
<code>s.e.($\hat&#123;\beta&#125;_0$)</code>.
第一张幻灯片提供了进行推断的核心定理。它准确地告诉我们如何对估计的系数进行标准化，使其服从已知的分布。</p>
<ul>
<li><strong>What it is:</strong> The standard error is the estimated
<strong>standard deviation of the coefficient’s sampling
distribution</strong>. In simpler terms, it’s a measure of the average
amount by which our estimate <span class="math inline">\(\hat{\beta}_1\)</span> would differ from the true
<span class="math inline">\(\beta_1\)</span> if we were to repeat the
experiment many times.
标准误差是系数抽样分布的<strong>标准差</strong>估计值。简单来说，它衡量的是如果我们重复实验多次，我们估计的
<span class="math inline">\(\hat{\beta}_1\)</span> 与真实的 <span class="math inline">\(\beta_1\)</span> 之间的平均差异。</li>
<li><strong>A smaller standard error means a more precise and reliable
estimate.</strong>
<strong>标准误差越小，估计值越精确可靠。</strong></li>
</ul>
<h4 id="the-t-statistic-t-统计量">2. The t-statistic t 统计量</h4>
<p>The theorem shows two fractions that form a
<strong>t-statistic</strong>. The general structure for this is:
该定理展示了两个构成<strong>t 统计量</strong>的分数。其一般结构如下：
<span class="math display">\[t = \frac{\text{ (Sample Estimate - True
Value) }}{\text{ Standard Error of the Estimate }}\]</span></p>
<p>For <span class="math inline">\(\beta_1\)</span>, this is: <span class="math inline">\(\frac{\hat{\beta}_1 -
\beta_1}{\text{s.e.}(\hat{\beta}_1)}\)</span>.</p>
<p>The key insight is that this specific quantity follows a
<strong>Student’s t-distribution</strong> with <strong><span class="math inline">\(n-2\)</span> degrees of freedom</strong>.
关键在于，这个特定量服从<strong>学生 t
分布</strong>，其自由度为<strong><span class="math inline">\(n-2\)</span>。 * </strong>Student’s
t-distribution:** This is a probability distribution that looks very
similar to the normal distribution but has slightly “heavier” tails. We
use it instead of the normal distribution because we had to
<em>estimate</em> the standard deviation of the errors (<code>s</code>
in the formula), which adds extra uncertainty.
这是一种概率分布，与正态分布非常相似，但尾部略重。使用它来代替正态分布，是因为必须<em>估计</em>误差的标准差（公式中的
<code>s</code>），这会增加额外的不确定性。 * <strong>Degrees of Freedom
(n-2):</strong> We start with <code>n</code> data points, but we lose
two degrees of freedom because we used the data to estimate two
parameters: <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. 从 <code>n</code>
个数据点开始，但由于用这些数据估计了两个参数：<span class="math inline">\(\beta_0\)</span> 和 <span class="math inline">\(\beta_1\)</span>，因此损失了两个自由度。 #### 3.
Estimating the Error Variance (<span class="math inline">\(s^2\)</span>)估计误差方差 (<span class="math inline">\(s^2\)</span>) To calculate the standard errors, we
need a value for <code>s</code>, which is our estimate of the true error
standard deviation <span class="math inline">\(\sigma\)</span>. This is
calculated from the <strong>Residual Sum of Squares (RSS)</strong>.
为了计算标准误差，我们需要一个 <code>s</code> 的值，它是对真实误差标准差
<span class="math inline">\(\sigma\)</span>
的估计值。该值由<strong>残差平方和 (RSS)</strong> 计算得出。 *
<strong>RSS:</strong> First, we calculate the RSS = <span class="math inline">\(\sum(y_i - \hat{y}_i)^2\)</span>, which is the sum
of all the squared errors.* <strong>RSS</strong>：首先，计算 RSS = <span class="math inline">\(\sum(y_i -
\hat{y}_i)^2\)</span>，即所有平方误差之和。 * <strong><span class="math inline">\(s^2\)</span>:</strong> Then, we find the estimate
of the error variance: <span class="math inline">\(s^2 = \text{RSS} /
(n-2)\)</span>. We divide by <span class="math inline">\(n-2\)</span> to
get an unbiased estimate. * <strong><span class="math inline">\(s^2\)</span></strong>：然后，计算误差方差的估计值：<span class="math inline">\(s^2 = \text{RSS} / (n-2)\)</span>。我们将其除以
<span class="math inline">\(n-2\)</span> 即可得到无偏估计值。 *
<code>s</code> is simply the square root of <span class="math inline">\(s^2\)</span>. This <code>s</code> is the value
used in the standard error formulas.* <code>s</code> 就是 <span class="math inline">\(s^2\)</span> 的平方根。这个 <code>s</code>
是标准误差公式中使用的值。</p>
<h3 id="what-this-allows-us-to-do-the-practical-use">## What This Allows
Us To Do (The Practical Use)</h3>
<p>Because we know the exact distribution of our t-statistic, we can now
achieve our goal of quantifying uncertainty: 因为知道 t
统计量的精确分布，所以现在可以实现量化不确定性的目标：</p>
<ol type="1">
<li><strong>Hypothesis Testing:</strong> We can test if a predictor is
actually useful. The most common test is for the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>. If we can prove the
observed <span class="math inline">\(\hat{\beta}_1\)</span> is very
unlikely to occur if the true <span class="math inline">\(\beta_1\)</span> were zero, we can conclude there
is a statistically significant relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.
可以检验一个预测变量是否真的有用。最常见的检验是零假设 <span class="math inline">\(H_0: \beta_1 = 0\)</span>。如果能证明，当真实的
<span class="math inline">\(\beta_1\)</span> 为零时，观测到的 <span class="math inline">\(\hat{\beta}_1\)</span>
不太可能发生，那么就可以得出结论，<span class="math inline">\(x\)</span>
和 <span class="math inline">\(y\)</span>
之间存在统计学上的显著关系。</li>
<li><strong>Confidence Intervals:</strong> We can construct a range of
plausible values for the true coefficient. For example, we can calculate
a 95% confidence interval for <span class="math inline">\(\beta_1\)</span>. This gives us a range where we
are 95% confident the true value of <span class="math inline">\(\beta_1\)</span> lies.
可以为真实系数构建一系列合理的值。</li>
</ol>
<h1 id="multiple-linear-regression">4. Multiple Linear Regression</h1>


<h2 id="multiple-linear-regression-1">4.1 Multiple Linear
Regression</h2>
<ul>
<li><strong>内容</strong>: <strong>Multiple Linear
Regression:</strong></li>
</ul>
<p>Here’s a detailed breakdown that connects both slides.</p>
<h3 id="the-model-from-one-to-many-predictors-从单预测变量到多预测变量">##
The Model: From One to Many Predictors 从单预测变量到多预测变量</h3>
<p>The first slide introduces the <strong>Multiple Linear Regression
model</strong>. This is a direct extension of the simple model, but
instead of using just one predictor variable, we use multiple (<span class="math inline">\(p\)</span>) predictors to explain our response
variable.
多元线性回归模型是简单模型的直接扩展，但不是只使用一个预测变量，而是使用多个（<span class="math inline">\(p\)</span>）预测变量来解释响应变量。</p>
<p>The general formula is: <span class="math display">\[y_i = \beta_0 +
\beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip} +
\epsilon_i\]</span></p>
<h4 id="key-change-in-interpretation">Key Change in Interpretation</h4>
<p>This is the most important new concept. In simple regression, <span class="math inline">\(\beta_1\)</span> was just the slope. In multiple
regression, each coefficient has a more nuanced meaning:
在简单回归中，<span class="math inline">\(\beta_1\)</span>
只是斜率。在多元回归中，每个系数都有更微妙的含义：</p>
<p><strong><span class="math inline">\(\beta_j\)</span> is the average
change in <span class="math inline">\(y\)</span> for a one-unit increase
in <span class="math inline">\(x_j\)</span>, while holding all other
predictors constant.</strong></p>
<p>This is incredibly powerful. Using the advertising example from your
slide: * <span class="math inline">\(y_i = \beta_0 +
\beta_1(\text{TV}_i) + \beta_2(\text{Radio}_i) +
\beta_3(\text{Newspaper}_i) + \epsilon_i\)</span> * <span class="math inline">\(\beta_1\)</span> represents the effect of TV
advertising on sales, <strong>after controlling for</strong> the amount
spent on Radio and Newspaper ads. This allows you to isolate the unique
contribution of each advertising
channel.表示在<strong>控制</strong>广播和报纸广告支出后，电视广告对销售额的影响。这可以让您区分每个广告渠道的独特贡献。</p>
<h3 id="the-solution-deriving-the-normal-equation-推导正态方程">## The
Solution: Deriving the Normal Equation 推导正态方程</h3>
<p>The second slide shows the mathematical process for finding the best
coefficients (<span class="math inline">\(\beta_0, \beta_1, \dots,
\beta_p\)</span>) using the <strong>Ordinary Least Squares
(OLS)</strong> method. It’s essentially a condensed derivation of the
<strong>Normal Equation</strong>. 使用<strong>普通最小二乘法
(OLS)</strong> 寻找最佳系数 (<span class="math inline">\(\beta_0,
\beta_1, \dots, \beta_p\)</span>)
的数学过程。它本质上是<strong>正态方程</strong>的简化推导。</p>
<h4 id="the-goal-minimizing-the-sum-of-squares-最小化平方和">1. The
Goal: Minimizing the Sum of Squares 最小化平方和</h4>
<p>Just like before, our goal is to minimize the sum of the squared
errors (or residuals): 目标是最小化平方误差（或残差）之和。</p>
<ul>
<li><strong>Scalar Form:</strong> <span class="math inline">\(\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_{i1} -
\beta_2x_{i2} - \beta_3x_{i3})^2\)</span>
<ul>
<li>This is easy to read but gets very long with more variables.
代码易于阅读，但变量越多，代码越长。</li>
</ul></li>
<li><strong>Vector Form:</strong> <span class="math inline">\(\sum_{i=1}^{n} (y_i - \boldsymbol{\beta}^T
\mathbf{x}_i)^2\)</span>
<ul>
<li>This is a more compact and powerful way to write the same thing
using linear algebra, where <span class="math inline">\(\boldsymbol{\beta}^T \mathbf{x}_i\)</span> is the
dot product that calculates the entire predicted value <span class="math inline">\(\hat{y}_i\)</span>.
这是一种更简洁、更强大的线性代数表示方法，其中 <span class="math inline">\(\boldsymbol{\beta}^T \mathbf{x}_i\)</span>
是计算整个预测值 <span class="math inline">\(\hat{y}_i\)</span>
的点积。</li>
</ul></li>
</ul>
<h4 id="the-method-using-calculus-to-find-the-minimum-使用微积分求最小值">2.
The Method: Using Calculus to Find the Minimum 使用微积分求最小值</h4>
<p>To find the set of <span class="math inline">\(\beta\)</span> values
that results in the lowest possible error, we use calculus.</p>
<ul>
<li><p><strong>The Derivative (Gradient):</strong> Since our error
function depends on multiple <span class="math inline">\(\beta\)</span>
coefficients, we can’t take a simple derivative. Instead, we take the
<strong>gradient</strong>, which is a vector of partial derivatives (one
for each coefficient). This tells us the “slope” of the error function
in every direction. 导数（梯度） 误差函数依赖于多个 <span class="math inline">\(\beta\)</span>
系数，因此我们不能简单地求导数。相反，采用<strong>梯度</strong>，它是一个由偏导数组成的向量（每个系数对应一个偏导数）。这告诉误差函数在各个方向上的“斜率”。</p></li>
<li><p><strong>Setting the Gradient to Zero:</strong> The minimum of a
function occurs where its slope is zero (the very bottom of the error
“valley”). The slide shows the result of taking this gradient and
setting it to
zero.函数的最小值出现在其斜率为零的地方（即误差“谷底”的最低点）。幻灯片展示了取此梯度并将其设为零的结果。</p></li>
</ul>
<p>The equation shown on the slide: <span class="math display">\[2
\sum_{i=1}^{n} (\boldsymbol{\beta}^T \mathbf{x}_i - y_i)\mathbf{x}_i^T =
0\]</span> …is the result of this calculus step. The goal is now to
algebraically rearrange this equation to solve for <span class="math inline">\(\boldsymbol{\beta}\)</span>.
是这一微积分步骤的结果。现在的目标是用代数方法重新排列这个方程，以求解
<span class="math inline">\(\boldsymbol{\beta}\)</span>。</p>
<h4 id="the-result-the-normal-equation-正则方程">3. The Result: The
Normal Equation 正则方程</h4>
<p>After rearranging the equation from the previous step and expressing
the sums in their full matrix form, we arrive at a clean and beautiful
solution. While the slide doesn’t show the final step, the result of
“Setting the gradient zero and solve <span class="math inline">\(\beta\)</span>” is the <strong>Normal
Equation</strong>:
重新排列上一步中的方程，并将和表示为完整的矩阵形式后，得到了一个简洁美观的解。虽然幻灯片没有展示最后一步，“设置梯度零点并求解
<span class="math inline">\(\beta\)</span>”
的结果就是<strong>正态方程</strong>：</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the
vector of our optimal coefficient estimates.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is the “design
matrix” where each row is an observation and each column is a predictor
variable. <span class="math inline">\(\mathbf{X}\)</span>
是“设计矩阵”，其中每一行代表一个观测值，每一列代表一个预测变量。</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of our
response variable. <span class="math inline">\(\mathbf{y}\)</span>
是我们的响应变量的向量。</li>
</ul>
<p>This single equation is the general solution for finding the OLS
coefficients for <strong>any</strong> linear regression model, no matter
how many predictors you have. This is what statistical software
calculates for you under the hood.
无论有多少个预测变量，这个简单的方程都是<strong>任何</strong>线性回归模型中
OLS 系数的通解。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/15/Pandoc_Deployment/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/15/Pandoc_Deployment/" class="post-title-link" itemprop="url">Pandoc Deployment</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-15 10:00:00 / 修改时间：12:10:02" itemprop="dateCreated datePublished" datetime="2025-09-15T10:00:00+08:00">2025-09-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Download <a
target="_blank" rel="noopener" href="https://github.com/jgm/pandoc/releases/tag/3.8">Pandoc</a>!</p>
<p><strong>pandoc-3.8-windows-x86_64.msi</strong></p>
<h2
id="问题主要为了解决默认的next渲染器无法渲染复杂公式的问题">【问题】主要为了解决默认的Next渲染器无法渲染复杂公式的问题</h2>
<h3
id="step1在系统变量中找到path点击编辑">Step1:在系统变量中找到<strong>Path→点击编辑</strong></h3>
<h3
id="step2点击新建输入pandoc.exe的父目录路径c点击确定">Step2:点击新建→输入pandoc.exe的父目录路径（C:）→点击确定</h3>
<h3 id="step3重启终端">Step3:重启终端</h3>
<h3
id="step4安装pandoc-3.8-windows-x86_64.msi">Step4:安装pandoc-3.8-windows-x86_64.msi</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pandoc --version</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm list --depth=0 | Select-String <span class="string">&quot;renderer&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-markdown-it --save</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm list --depth=0 | Select-String <span class="string">&quot;renderer&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://blog.csdn.net/gitblog_00216/article/details/141763934">Ref</a></p>
<h3
id="step5在你所在的博客头加入必要的引入">Step5:在你所在的博客头加入必要的引入</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mathjax: <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3
id="step6在d_ailab_hkust_machine_learning的_config.yaml文件中确保你的pandoc路径能被找到">Step6:在D:_AILab_HKUST_Machine_Learning的_config.yaml文件中确保你的Pandoc路径能被找到</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ pandoc:</span><br><span class="line">$ pandoc_path: <span class="string">&quot;C:/Users/Aprine/AppData/Local/Pandoc/pandoc.exe&quot;</span> <span class="comment"># </span></span><br><span class="line">$ args:</span><br><span class="line">$   - <span class="string">&quot;--mathjax&quot;</span></span><br></pre></td></tr></table></figure>
<h3
id="step7在d_ailab_hkust_machine_learning_config.yaml文件中确保你的math信息配置正确">Step7:在D:_AILab_HKUST_Machine_Learning_config.yaml文件中确保你的math信息配置正确</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ mathjax:</span><br><span class="line">$   <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">$   <span class="comment"># See: https://mhchem.github.io/MathJax-mhchem/</span></span><br><span class="line">$   mhchem: <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3
id="step8修改你的head文件的基础格式">Step8:修改你的head文件的基础格式</h3>
<p>More info: <a
target="_blank" rel="noopener" href="https://blog.csdn.net/ALexander_Monster/article/details/105717091">Ref</a></p>
<h3 id="push-new-blog">Push new blog</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$   hexo g -d</span><br></pre></td></tr></table></figure>
<p>对比损失函数（InfoNCE/NT-Xent Loss）定义为： <span
class="math inline">\(\mathcal{L}_{\text{q}} = -\log \underbrace{\left(
\frac{\exp\left( \mathbf{q} \cdot \mathbf{k}^{+} / \tau
\right)}{\exp\left( \mathbf{q} \cdot \mathbf{k}^{+} / \tau \right) +
\sum\limits_{i=1}^{N} \exp\left( \mathbf{q} \cdot \mathbf{k}_{i}^{-} /
\tau \right)} \right)}_{\text{Softmax 概率}}\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/14/contrastive_learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/14/contrastive_learning/" class="post-title-link" itemprop="url">Contrastive Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-09-14 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-14T21:00:00+08:00">2025-09-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-15 12:13:57" itemprop="dateModified" datetime="2025-09-15T12:13:57+08:00">2025-09-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>用于无监督视觉表示学习的动量对比 <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.05722v3">Momentum Contrast for
Unsupervised Visual Representation Learning</a></p>
<h1 id="momentum-contrast">1. Momentum Contrast</h1>
<h2 id="定义">1.1 定义</h2>
<ul>
<li><strong>内容</strong>: 对比学习（Contrastive Learning）
通过让模型学习区分相似<strong>正样本</strong>与不相似<strong>负样本</strong>的数据点来学习有用的特征。</li>
</ul>
<h2 id="正样本对positive-pairs">1.2 正样本对（Positive Pairs）:</h2>
<ul>
<li><strong>内容</strong>:
通常来自同一数据点的不同数据增强视图（例如，同一张图片的两次随机裁剪、颜色抖动等）。它们应该具有相似的语义信息。</li>
</ul>
<h2 id="负样本negatives">1.3 负样本（Negatives）:</h2>
<ul>
<li><strong>内容</strong>:
来自与<strong>查询样本</strong>不同的其他数据点。它们代表不同的语义内容。</li>
</ul>
<h2 id="目标-模型的目标是学习一个编码器encoder">1.4 目标:
模型的目标是学习一个编码器（Encoder）</h2>
<ul>
<li><strong>内容</strong>:
查询样本与其对应的正样本在特征空间中的距离<strong>很近</strong>（相似度高）。</li>
<li><strong>内容</strong>:
查询样本与大量负样本在特征空间中的距离<strong>很远</strong>（相似度低）。</li>
</ul>
<h1 id="创新">2. 创新</h1>
<h2 id="动态字典dynamic-dictionary">2.1 动态字典（Dynamic
Dictionary）:</h2>
<ul>
<li><strong>内容</strong>: MoCo
维护一个<strong>先进先出FIFO</strong>的队列来存储编码后的特征<strong>Keys</strong>。</li>
<li><strong>内容</strong>:当前批次的数据经过键编码器编码后，其特征被入队添加到字典队列尾部。</li>
<li><strong>内容</strong>:同时，队列中最老的批次特征被出<strong>队dequeue</strong>
移除。</li>
<li><strong>内容</strong>:队列可以将字典大小 <span
class="math inline">\(K\)</span>
设计得远大于单个批次的大小，从而提供海量且一致的负样本来源（一致性由下面的动量更新保证）。队列解耦了字典大小与批次大小的限制。</li>
</ul>
<h2 id="动量更新编码器momentum-update-of-key-encoder">2.2
动量更新编码器（Momentum Update of Key Encoder）:</h2>
<ul>
<li><strong>内容</strong>:
查询编码器使用标准的梯度下降更新（<strong>SGD</strong>）</li>
<li><strong>内容</strong>:
键编码器<strong>不通过反向传播更新</strong>。</li>
<li><strong>内容</strong>: 键编码器的参数 <span
class="math inline">\(θ_k\)</span> 通过动量更新<strong>Momentum
Update</strong>从查询编码器的参数 <span
class="math inline">\(θ_q\)</span> 获得：<span class="math display">\[
\theta_k \gets m \cdot \theta_k + (1 - m) \cdot \theta_q \]</span>其中
<span class="math inline">\(m\)</span> 是一个动量系数（如 <span
class="math inline">\(m\)</span> = <span
class="math inline">\(0.999\)</span>），非常接近<span
class="math inline">\(1\)</span>。 ## 2.3 优势:</li>
<li><strong>内容</strong>:动量更新使得键编码器 <span
class="math inline">\(f_k\)</span> 的参数变化非常缓慢和平滑。</li>
</ul>
<h1 id="对比损失函数infonce-loss">3. 对比损失函数（InfoNCE Loss:</h1>
<h2 id="infonce-loss-noise-contrastive-estimation-loss">3.1 InfoNCE Loss
(Noise-Contrastive Estimation Loss)：</h2>
<p>对比损失函数（InfoNCE/NT-Xent Loss）定义为： <span
class="math inline">\(\mathcal{L}_{\text{q}} = -\log \underbrace{\left(
\frac{\exp\left( \mathbf{q} \cdot \mathbf{k}^{+} / \tau
\right)}{\exp\left( \mathbf{q} \cdot \mathbf{k}^{+} / \tau \right) +
\sum\limits_{i=1}^{N} \exp\left( \mathbf{q} \cdot \mathbf{k}_{i}^{-} /
\tau \right)} \right)}_{\text{Softmax 概率}}\)</span></p>
<h2 id="其中">3.2其中：</h2>
<table>
<colgroup>
<col style="width: 23%" />
<col style="width: 76%" />
</colgroup>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math inline">\(\mathbf{q}\)</span></td>
<td>查询向量（Query Vector）：由查询编码器 <span
class="math inline">\(f_q\)</span> 输出（如 <span
class="math inline">\(2048\)</span> 维）。</td>
</tr>
<tr>
<td><span class="math inline">\(\mathbf{k}^{+}\)</span></td>
<td>正样本键向量（Positive Key）：由键编码器 <span
class="math inline">\(f_k\)</span>
输出（来自同一数据的不同增强视图）。</td>
</tr>
<tr>
<td><span class="math inline">\(\mathbf{k}_{i}^{-}\)</span></td>
<td>负样本键向量（Negative Keys）：来自字典队列的其他数据样本（数量为
<span class="math inline">\(N\)</span>，如 <span
class="math inline">\(65536\)</span>）。</td>
</tr>
<tr>
<td><span class="math inline">\(\tau\)</span></td>
<td>温度参数（Temperature）：控制相似度分布的尖锐程度（典型值 <span
class="math inline">\(0.05 \sim 0.2\)</span>）。</td>
</tr>
<tr>
<td><span class="math inline">\(\cdot\)</span></td>
<td>向量点积（L2归一化后等价于余弦相似度，即 <span
class="math inline">\(\mathbf{q} \cdot \mathbf{k} =
\cos\theta\)</span>）。</td>
</tr>
</tbody>
</table>
<h2 id="数据流">3.2 数据流：</h2>
<ul>
<li><strong>原始输入</strong>: 一张图片 <strong>P.jpg</strong>
(<strong>256x256</strong> 原始尺寸)</li>
<li><strong>预处理Step1</strong>: 随机裁剪出 <strong>224x224</strong>
的区域</li>
<li><strong>预处理Step2</strong>: 随机轻微改变颜色和亮度。</li>
<li><strong>预处理Step3</strong>: 归一化像素值。</li>
<li><strong>结果</strong>: <strong>[3, 224, 224]</strong> (一个
<strong>3x224x224</strong> 的张量)。输入到编码器的形式。</li>
<li><strong>查询编码器 <span class="math inline">\(f_q\)</span>
处理</strong>:</li>
<li><strong>Step1</strong>: 输入：<strong>[3, 224, 224]</strong>
张量。</li>
<li><strong>Step2</strong>: 经过<strong>Model</strong></li>
<li><strong>Step3</strong>: 全局平均池化层 (Global Average Pooling)
将空间维度压缩掉。</li>
<li><strong>Step4</strong>: 一个线性投影层将特征维度映射到
<strong>D</strong>。</li>
<li><strong>结果</strong>:一个 <span class="math inline">\(D\)</span>
维（如 <span class="math inline">\(M\)</span> 维）的归一化向量 <span
class="math inline">\(q\)</span>。这个 <span
class="math inline">\(q\)</span>
代表了经过裁剪、颜色扰动后的猫头像的抽象特征。例如， <strong>[0.12,
-0.05, 0.87, …, 0.03]</strong> (<span
class="math inline">\(M\)</span>个数值)。</li>
<li><strong>键编码器 <span class="math inline">\(f_k\)</span>
处理</strong>:</li>
<li><strong>Step1</strong>: 输入：<strong>[3, 224, 224]</strong>
张量。(对 <strong>P.jpg</strong> 应用另一组随机预处理得到的另一个
<strong>[3, 224, 224]</strong> 张量。)</li>
<li><strong>Step2</strong>: 经过结构相同但参数由动量更新的 <span
class="math inline">\(f_k\)</span>。</li>
<li><strong>Step3</strong>: 一个 <span class="math inline">\(D\)</span>
维（如 <span class="math inline">\(M\)</span> 维）的归一化向量 <span
class="math inline">\(k\)</span>。例如 <strong>[0.15, -0.08, 0.84, …,
0.02]</strong>。这个 <span class="math inline">\(k\)</span>
代表了同一数据但不同视角/颜色下的抽象特征。</li>
<li><strong>动态字典</strong>:</li>
<li><strong>Step1</strong>:包含之前通过 <span
class="math inline">\(f_k\)</span> 计算出的 <span
class="math inline">\(k\)</span> 向量。例如，队列大小是 <span
class="math inline">\(L\)</span>，里面存储了 <span
class="math inline">\(L\)</span> 个不同的 <strong>D=128</strong>
维向量，每个代表处理过的一张数据的特征。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/14/CaltechData/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/14/CaltechData/" class="post-title-link" itemprop="url">Criegee, H10 chain, small radicals, water bond dissociation, and QMSpin energy datasets with MOB features for MOB-ML(KA-GPR)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-09-14 20:30:00" itemprop="dateCreated datePublished" datetime="2025-09-14T20:30:00+08:00">2025-09-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-15 01:42:44" itemprop="dateModified" datetime="2025-09-15T01:42:44+08:00">2025-09-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI4chemistry/" itemprop="url" rel="index"><span itemprop="name">AI4chemistry</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>用于MOB-ML（KA-GPR）的Criegee、H10链、小自由基、水键解离和具有MOB特征的QMSpin能量数据集
<a
target="_blank" rel="noopener" href="https://data.caltech.edu/records/cvjkq-tjt86">CaltechData</a>!</p>
<p>Criegee, H10 chain, small radicals, water bond dissociation, and
QMSpin energy datasets with MOB features for MOB-ML(KA-GPR)</p>
<h2 id="目录">目录</h2>
<ol type="1">
<li><a href="#Criegee数据集%20criegee.zip">Criegee数据集)</a><br />
</li>
<li><a href="#H10链数据集%20h10.zip">H10链数据集</a><br />
</li>
<li><a
href="#小自由基数据集%20small_radicals.zip">小自由基数据集</a><br />
</li>
<li><a
href="#水分子键解离数据集%20h2o_dissociation.zip">水分子键解离数据集</a></li>
<li><a href="#QMSpin数据集%20qmspin.zip">QMSpin数据集</a></li>
</ol>
<h2 id="criegee数据集">1. Criegee数据集</h2>
<h3 id="能量文件-criegee.csv">1.1 能量文件 (criegee.csv)</h3>
<ul>
<li><strong>内容</strong>: 包含 RHF 和 MRCI+Q 能量（cc-pVTZ 基组计算）。
### 1.2 诊断文件 (criegee_diagnostic.csv):</li>
<li><strong>内容</strong>: 包含 CCSD/cc-pVTZ 计算的 T1 和 D1 诊断指标。
### 1.3 结构文件夹 (共 800 个):</li>
<li><strong>内容</strong>: geo.xyz: 分子构型坐标</li>
<li><strong>内容</strong>: features_tz.hdf5: 用于 KA-GPR 的 对角 MOB
特征。</li>
</ul>
<h2 id="待学习rhf-mrciq-cc-ka-gpr-mob">待学习：RHF, MRCI+Q, CC, KA-GPR,
MOB</h2>
<h2 id="h10-链数据集-h10.zip">2. H10 链数据集 (h10.zip)</h2>
<h3 id="能量文件-h10.csv">2.1 能量文件 (h10.csv):</h3>
<ul>
<li><strong>内容</strong>: 包含 RHF 和 MRCI+Q-F12 能量（cc-pVTZ-F12
基组计算）。 ### 2.2 结构文件夹:</li>
<li><strong>内容</strong>: geo.xyz: 分子构型坐标</li>
<li><strong>内容</strong>: features_tz.hdf5: 用于 KA-GPR 的 对角 MOB
特征。</li>
</ul>
<h2 id="待学习rhf-mrciq-cc-ka-gpr-mob-1">待学习：RHF, MRCI+Q, CC,
KA-GPR, MOB</h2>
<h2 id="小自由基数据集-small_radicals.zip">3. 小自由基数据集
(small_radicals.zip)</h2>
<h3 id="包含-9-种自由基每种自由基有">3.1 包含 9
种自由基，每种自由基有：</h3>
<h3 id="能量文件-x.csv">3.1.1 能量文件 (x.csv):</h3>
<ul>
<li><strong>内容</strong>: 包含 ROHF 和 MRCI+Q 能量（cc-pVTZ
基组计算）。 ### 3.1.2 热化结构文件夹 (每种 200 个):</li>
<li><strong>内容</strong>: geo.xyz: 分子构型坐标</li>
<li><strong>内容</strong>: features_alpha.hdf5: α 自旋轨道的 MOB
特征</li>
<li><strong>内容</strong>: features_beta.hdf5: β 自旋轨道的 MOB
特征。</li>
</ul>
<h2
id="待学习rhf-rohf-mrciq-cc-ka-gpr-mob-α-自旋轨道的-mob-特征-β-自旋轨道的-mob-特征">待学习：RHF,
ROHF, MRCI+Q, CC, KA-GPR, MOB, α 自旋轨道的 MOB 特征, β 自旋轨道的 MOB
特征</h2>
<h2 id="水分子键解离数据集-h2o_dissociation.zip">4. 水分子键解离数据集
(h2o_dissociation.zip)</h2>
<h3 id="能量文件-h2o_dissociation.csv">4.1 能量文件
(h2o_dissociation.csv)</h3>
<ul>
<li><strong>内容</strong>: 包含 初始构象 ID、OH
键解离路径的键长比例因子、ROHF 和 MRCI+Q 能量（aug-cc-pVTZ 基组计算）。
### 4.2 初始构象文件夹 (共 50 个): ### 4.2.1 每个构象包含 20
个解离路径结构:</li>
<li><strong>内容</strong>: features_alpha.hdf5: α 自旋轨道的 MOB
特征</li>
<li><strong>内容</strong>: features_beta.hdf5: β 自旋轨道的 MOB
特征。</li>
</ul>
<h2
id="待学习初始构象-idoh-键解离路径的键长比例因子-rohf-mrciq-cc-α-自旋轨道的-mob-特征-β-自旋轨道的-mob-特征">待学习：初始构象
ID、OH 键解离路径的键长比例因子, ROHF, MRCI+Q, CC, α 自旋轨道的 MOB
特征, β 自旋轨道的 MOB 特征</h2>
<h2 id="qmspin-数据集-qmspin.zip">3. QMSpin 数据集 (qmspin.zip)</h2>
<h3 id="能量文件-qmspin.csv">3.1 能量文件 (qmspin.csv)：</h3>
<ul>
<li><strong>内容</strong>: 包含 单重态 (RHF) 和 三重态 (ROHF) 的 MRCI+Q
能量（cc-pVDZ 基组计算），自旋状态标记为 0（单重态）或 2（三重态）。 ###
3.2 结构文件夹:</li>
<li><strong>内容</strong>: geometries_singlet: 单重态优化结构</li>
<li><strong>内容</strong>: geometries_triplet: 三重态优化结构</li>
<li><strong>内容</strong>: 单重态特征文件:features_dz_singlet.hdf5:
单重态能量的 MOB 特征</li>
<li><strong>内容</strong>: 单重态特征文件:features_dz_singlet.hdf5:
单重态能量的 MOB 特征</li>
<li><strong>内容</strong>:
三重态特征文件:features_alpha_dz_triplet.hdf5: α 自旋轨道特征
features_beta_dz_triplet.hdf5: β 自旋轨道特征。</li>
</ul>
<h2
id="待学习rhf-rohf-mrciq-cc-mob-α-自旋轨道的-mob-特征-β-自旋轨道的-mob-特征">待学习：RHF,
ROHF, MRCI+Q, CC, MOB, α 自旋轨道的 MOB 特征, β 自旋轨道的 MOB 特征</h2>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/12/Build_up_my_UST_Blogs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/12/Build_up_my_UST_Blogs/" class="post-title-link" itemprop="url">Manage double Github Accounts and Build up my UST Blogs</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-12 18:30:00 / 修改时间：19:29:12" itemprop="dateCreated datePublished" datetime="2025-09-12T18:30:00+08:00">2025-09-12</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a
href="https://tianyaoblogs.github.io/">TianyaoBlogs</a>! This is my very
first post. It has shown my process of building blogs!</p>
<h2 id="start-to-own-your-blogs">Start to own your blogs</h2>
<h3 id="download-nodejs-npm-and-check-its-version">Download NodeJS &amp;
npm and check its version</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ node -v</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm -v</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://www.nodejs.com.cn/download.html">Download</a> More info:
<a
target="_blank" rel="noopener" href="https://blog.csdn.net/Natsuago/article/details/145567734">Download</a></p>
<h3 id="prepare-your-path">Prepare your path</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ npm config <span class="built_in">set</span> prefix <span class="string">&quot;D:\Program Files (x86)\node_modules\node_global&quot;</span></span><br><span class="line">$ npm config <span class="built_in">set</span> cache <span class="string">&quot;D:\Program Files (x86)\node_modules\node_cache&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://blog.csdn.net/Natsuago/article/details/145567734">Check</a></p>
<h3 id="check-your-environmental-path">Check your environmental
path</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> %NODE_PATH%</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://blog.csdn.net/Natsuago/article/details/145567734">Check</a></p>
<h3
id="generate-ssh-keys-for-the-second-account-for-those-people-who-have-two-or-more-github-account">Generate
SSH keys for the second account (For those people who have two or more
github account)</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t ed25519 -C <span class="string">&quot;your_email@second_account.com&quot;</span></span><br></pre></td></tr></table></figure>
<h3 id="add-the-new-public-key-to-the-second-github-account">Add the new
public key to the second GitHub account</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$   <span class="built_in">cat</span> ~/.ssh/id_ed25519_second.pub</span><br></pre></td></tr></table></figure>
<h3 id="configuring-ssh-multi-account-rules">Configuring SSH
multi-account rules</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$    cnano ~/.ssh/config</span><br><span class="line">$    <span class="comment">#  (peninsula824)</span></span><br><span class="line">$    Host github.com-peninsula</span><br><span class="line">$    HostName github.com</span><br><span class="line">$    User git</span><br><span class="line">$    IdentityFile ~/.ssh/id_rsa  <span class="comment"># peninsula824_key</span></span><br><span class="line">$    IdentitiesOnly <span class="built_in">yes</span></span><br><span class="line"></span><br><span class="line">$    <span class="comment">#  (TianyaoBlogs)</span></span><br><span class="line">$    Host github.com-tianyao</span><br><span class="line">$    HostName github.com</span><br><span class="line">$    User git</span><br><span class="line">$    IdentityFile ~/.ssh/0901102262  <span class="comment"># TianyaoBlogs_key</span></span><br><span class="line">$    IdentitiesOnly <span class="built_in">yes</span></span><br></pre></td></tr></table></figure>
<h3 id="test-the-connection">Test the connection</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$   ssh -T git@github.com-peninsula</span><br><span class="line">$   ssh -T git@github.com-tianyao</span><br></pre></td></tr></table></figure>
<h3 id="update-hexo-deployment-configuration">Update Hexo deployment
configuration</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$   deploy:</span><br><span class="line">$     <span class="built_in">type</span>: git</span><br><span class="line">$    repo: </span><br><span class="line">$       github: git@github.com-tianyao:TianyaoBlogs/TianyaoBlogs.github.io.git</span><br><span class="line">$     branch: main</span><br></pre></td></tr></table></figure>
<h3 id="complete-the-deployment">Complete the deployment</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$   hexo clean</span><br><span class="line">$   hexo generate</span><br><span class="line">$   hexo deploy</span><br></pre></td></tr></table></figure>
<h3 id="push-new-blog">Push new blog</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$   hexo g -d</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
