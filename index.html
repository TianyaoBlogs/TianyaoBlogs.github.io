<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianyaoblogs.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="TianyaoBlogs">
<meta property="og:url" content="https://tianyaoblogs.github.io/index.html">
<meta property="og:site_name" content="TianyaoBlogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://tianyaoblogs.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>TianyaoBlogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TianyaoBlogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/01/5054C4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/01/5054C4/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-10-01 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-01T21:00:00+08:00">2025-10-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-10-18 23:00:24" itemprop="dateModified" datetime="2025-10-18T23:00:24+08:00">2025-10-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-4</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="what-is-classification">1. What is Classification?</h1>
<p>Classification is a type of <strong>supervised machine
learning</strong> where the goal is to predict a
<strong>categorical</strong> or qualitative response. Unlike regression
where you predict a continuous numerical value (like a price or
temperature), classification assigns an input to a specific category or
class.
åˆ†ç±»æ˜¯ä¸€ç§<strong>ç›‘ç£å¼æœºå™¨å­¦ä¹ </strong>ï¼Œå…¶ç›®æ ‡æ˜¯é¢„æµ‹<strong>åˆ†ç±»</strong>æˆ–å®šæ€§å“åº”ã€‚ä¸é¢„æµ‹è¿ç»­æ•°å€¼ï¼ˆä¾‹å¦‚ä»·æ ¼æˆ–æ¸©åº¦ï¼‰çš„å›å½’ä¸åŒï¼Œåˆ†ç±»å°†è¾“å…¥åˆ†é…åˆ°ç‰¹å®šçš„ç±»åˆ«æˆ–ç±»åˆ«ã€‚</p>
<p><strong>Key characteristics:</strong></p>
<ul>
<li><p><strong>Goal:</strong> Predict the class of a subject based on
input features.</p></li>
<li><p><strong>Output (Response):</strong> The output is a category,
such as â€˜Yesâ€™/â€˜Noâ€™, â€˜Spamâ€™/â€˜Not Spamâ€™, or
â€˜Highâ€™/â€˜Mediumâ€™/â€˜Lowâ€™.</p></li>
<li><p><strong>Applications:</strong> Common examples include email spam
detectors, medical diagnosis (e.g., virus carrier vs.Â non-carrier), and
fraud detection.</p>
<ul>
<li><strong>ç›®æ ‡</strong>ï¼šæ ¹æ®è¾“å…¥ç‰¹å¾é¢„æµ‹ä¸»é¢˜çš„ç±»åˆ«ã€‚</li>
<li><strong>è¾“å‡ºï¼ˆå“åº”ï¼‰ï¼š</strong>è¾“å‡ºæ˜¯ä¸€ä¸ªç±»åˆ«ï¼Œä¾‹å¦‚â€œæ˜¯â€/â€œå¦â€ã€â€œåƒåœ¾é‚®ä»¶â€/â€œéåƒåœ¾é‚®ä»¶â€æˆ–â€œé«˜â€/â€œä¸­â€/â€œä½â€ã€‚</li>
<li><strong>åº”ç”¨</strong>ï¼šå¸¸è§ç¤ºä¾‹åŒ…æ‹¬åƒåœ¾é‚®ä»¶æ£€æµ‹å™¨ã€åŒ»å­¦è¯Šæ–­ï¼ˆä¾‹å¦‚ï¼Œç—…æ¯’æºå¸¦è€…ä¸éç—…æ¯’æºå¸¦è€…ï¼‰å’Œæ¬ºè¯ˆæ£€æµ‹ã€‚
The example used in the slides is a credit card <strong>Default
dataset</strong>. The goal is to predict whether a customer will
<strong>default</strong> (â€˜Yesâ€™ or â€˜Noâ€™) on their payments based on
their monthly <strong>income</strong> and account
<strong>balance</strong>.</li>
</ul></li>
</ul>
<p>## Why Not Use Linear Regression?ä¸ºä»€ä¹ˆä¸ä½¿ç”¨çº¿æ€§å›å½’ï¼Ÿ</p>
<p>At first, it might seem possible to use linear regression for
classification. For a binary (two-class) problem like the default
dataset, you could code the outcomes as numbers, for example:</p>
<ul>
<li>Default = â€˜Noâ€™ =&gt; <span class="math inline">\(y = 0\)</span></li>
<li>Default = â€˜Yesâ€™ =&gt; <span class="math inline">\(y =
1\)</span></li>
</ul>
<p>You could then fit a standard linear regression model: <span
class="math inline">\(Y \approx \beta_0 + \beta_1 X\)</span>. In this
context, we would interpret the prediction <span
class="math inline">\(\hat{y}\)</span> as the <em>probability</em> of
default, so weâ€™d be modeling <span class="math inline">\(P(Y=1|X) =
\beta_0 + \beta_1 X\)</span>.</p>
<p>However, this approach has two major problems:
ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æœ‰ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š <strong>1. The Output Is Not a
Probability</strong> A linear model can produce outputs that are less
than 0 or greater than 1. This doesnâ€™t make sense for a probability,
which must always be between 0 and 1.</p>
<p>The image below is the most important one for understanding this
issue. The left plot shows a linear regression line fit to the 0/1
default data. You can see the line goes below 0 and would eventually go
above 1 for higher balances. The right plot shows a logistic regression
curve, which always stays between 0 and 1.</p>
<ul>
<li><strong>Left (Linear Regression):</strong> The straight blue line
predicts probabilities &lt; 0 for low balances.</li>
<li><strong>Right (Logistic Regression):</strong> The S-shaped blue
curve correctly constrains the probability output between 0 and 1.</li>
</ul>
<p><strong>2. It Doesnâ€™t Work for Multi-Class Problems</strong> If you
have more than two categories (e.g., â€˜mildâ€™, â€˜moderateâ€™, â€˜severeâ€™), you
might code them as 0, 1, and 2. A linear regression model would
incorrectly assume that the â€œdistanceâ€ between â€˜mildâ€™ and â€˜moderateâ€™ is
the same as the distance between â€˜moderateâ€™ and â€˜severeâ€™, which is
usually not a valid assumption.</p>
<p><strong>1. è¾“å‡ºä¸æ˜¯æ¦‚ç‡</strong> çº¿æ€§æ¨¡å‹å¯ä»¥äº§ç”Ÿå°äº 0 æˆ–å¤§äº 1
çš„è¾“å‡ºã€‚è¿™å¯¹äºæ¦‚ç‡æ¥è¯´æ¯«æ— æ„ä¹‰ï¼Œå› ä¸ºæ¦‚ç‡å¿…é¡»å§‹ç»ˆä»‹äº 0 å’Œ 1 ä¹‹é—´ã€‚</p>
<p>ä¸‹å›¾æ˜¯ç†è§£è¿™ä¸ªé—®é¢˜æœ€é‡è¦çš„å›¾ã€‚å·¦å›¾æ˜¾ç¤ºäº†ä¸ 0/1
é»˜è®¤æ•°æ®æ‹Ÿåˆçš„çº¿æ€§å›å½’çº¿ã€‚æ‚¨å¯ä»¥çœ‹åˆ°ï¼Œè¯¥çº¿ä½äº
0ï¼Œå¹¶ä¸”æœ€ç»ˆä¼šéšç€ä½™é¢çš„å¢åŠ è€Œé«˜äº
1ã€‚å³å›¾æ˜¾ç¤ºäº†é€»è¾‘å›å½’æ›²çº¿ï¼Œå®ƒå§‹ç»ˆä¿æŒåœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚</p>
<ul>
<li><strong>å·¦å›¾ï¼ˆçº¿æ€§å›å½’ï¼‰ï¼š</strong>è“è‰²ç›´çº¿é¢„æµ‹ä½ä½™é¢çš„æ¦‚ç‡å°äº
0ã€‚</li>
<li><strong>å³å›¾ï¼ˆé€»è¾‘å›å½’ï¼‰ï¼š</strong>S
å½¢è“è‰²æ›²çº¿æ­£ç¡®åœ°å°†æ¦‚ç‡è¾“å‡ºé™åˆ¶åœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚</li>
</ul>
<p><strong>2.å®ƒä¸é€‚ç”¨äºå¤šç±»åˆ«é—®é¢˜</strong>
å¦‚æœæ‚¨æœ‰ä¸¤ä¸ªä»¥ä¸Šçš„ç±»åˆ«ï¼ˆä¾‹å¦‚ï¼Œâ€œè½»åº¦â€ã€â€œä¸­åº¦â€ã€â€œé‡åº¦â€ï¼‰ï¼Œæ‚¨å¯èƒ½ä¼šå°†å®ƒä»¬ç¼–ç ä¸º
0ã€1 å’Œ
2ã€‚çº¿æ€§å›å½’æ¨¡å‹ä¼šé”™è¯¯åœ°å‡è®¾â€œè½»åº¦â€å’Œâ€œä¸­åº¦â€ä¹‹é—´çš„â€œè·ç¦»â€ä¸â€œä¸­åº¦â€å’Œâ€œé‡åº¦â€ä¹‹é—´çš„è·ç¦»ç›¸åŒï¼Œè¿™é€šå¸¸ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å‡è®¾ã€‚</p>
<p>## The Solution: Logistic Regression</p>
<p>Instead of modeling the response <span
class="math inline">\(y\)</span> directly, logistic regression models
the <strong>probability</strong> that <span
class="math inline">\(y\)</span> belongs to a particular class. To solve
the issue of the output not being a probability, it uses the
<strong>logistic function</strong> (also known as the sigmoid
function).</p>
<p>This function takes any real-valued input and squeezes it into an
output between 0 and 1.</p>
<p>The formula for the probability in a logistic regression model is:
<span class="math display">\[P(Y=1|X) = \frac{e^{\beta_0 + \beta_1 X}}{1
+ e^{\beta_0 + \beta_1 X}}\]</span> This S-shaped function, shown in the
right-hand plot above, ensures that the output is always a valid
probability. We can then set a threshold (e.g., 0.5) to make the final
class prediction. If <span class="math inline">\(P(Y=1|X) &gt;
0.5\)</span>, we predict â€˜Yesâ€™; otherwise, we predict â€˜Noâ€™.</p>
<p>## è§£å†³æ–¹æ¡ˆï¼šé€»è¾‘å›å½’</p>
<p>é€»è¾‘å›å½’ä¸æ˜¯ç›´æ¥å¯¹å“åº” <span class="math inline">\(y\)</span>
è¿›è¡Œå»ºæ¨¡ï¼Œè€Œæ˜¯å¯¹ <span class="math inline">\(y\)</span>
å±äºç‰¹å®šç±»åˆ«çš„<strong>æ¦‚ç‡</strong>è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¾“å‡ºä¸æ˜¯æ¦‚ç‡çš„é—®é¢˜ï¼Œå®ƒä½¿ç”¨äº†<strong>é€»è¾‘å‡½æ•°</strong>ï¼ˆä¹Ÿç§°ä¸º
S å‹å‡½æ•°ï¼‰ã€‚</p>
<p>æ­¤å‡½æ•°æ¥å—ä»»ä½•å®å€¼è¾“å…¥ï¼Œå¹¶å°†å…¶å‹ç¼©ä¸ºä»‹äº 0 å’Œ 1 ä¹‹é—´çš„è¾“å‡ºã€‚</p>
<p>é€»è¾‘å›å½’æ¨¡å‹ä¸­çš„æ¦‚ç‡å…¬å¼ä¸ºï¼š <span class="math display">\[P(Y=1|X) =
\frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}\]</span>
å¦‚ä¸Šå›¾å³ä¾§æ‰€ç¤ºï¼Œè¿™ä¸ª S
å½¢å‡½æ•°ç¡®ä¿è¾“å‡ºå§‹ç»ˆæ˜¯æœ‰æ•ˆæ¦‚ç‡ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ªé˜ˆå€¼ï¼ˆä¾‹å¦‚
0.5ï¼‰æ¥è¿›è¡Œæœ€ç»ˆçš„ç±»åˆ«é¢„æµ‹ã€‚å¦‚æœ <span class="math inline">\(P(Y=1|X)
&gt; 0.5\)</span>ï¼Œåˆ™é¢„æµ‹â€œæ˜¯â€ï¼›å¦åˆ™ï¼Œé¢„æµ‹â€œå¦â€ã€‚</p>
<p>## Data Visualization &amp; Code in Python</p>
<p>The slides use R to visualize the data. The boxplots are particularly
important because they show which variable is a better predictor.</p>
<ul>
<li><p><strong>Balance vs.Â Default:</strong> The boxplots for balance
show a clear difference. The median balance for those who default
(â€˜Yesâ€™) is much higher than for those who do not (â€˜Noâ€™). This suggests
<strong>balance is a strong predictor</strong>.</p></li>
<li><p><strong>Income vs.Â Default:</strong> The boxplots for income show
a lot of overlap. The median incomes for both groups are very similar.
This suggests <strong>income is a weak predictor</strong>.</p></li>
<li><p><strong>ä½™é¢
vs.Â è¿çº¦</strong>ï¼šä½™é¢çš„ç®±çº¿å›¾æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„å·®å¼‚ã€‚è¿çº¦è€…ï¼ˆâ€œæ˜¯â€ï¼‰çš„ä½™é¢ä¸­ä½æ•°è¿œé«˜äºæœªè¿çº¦è€…ï¼ˆâ€œå¦â€ï¼‰ã€‚è¿™è¡¨æ˜<strong>ä½™é¢æ˜¯ä¸€ä¸ªå¼ºæœ‰åŠ›çš„é¢„æµ‹æŒ‡æ ‡</strong>ã€‚</p></li>
<li><p><strong>æ”¶å…¥
vs.Â è¿çº¦</strong>ï¼šæ”¶å…¥çš„ç®±çº¿å›¾æ˜¾ç¤ºå‡ºå¾ˆå¤§çš„é‡å ã€‚ä¸¤ç»„çš„æ”¶å…¥ä¸­ä½æ•°éå¸¸ç›¸ä¼¼ã€‚è¿™è¡¨æ˜<strong>æ”¶å…¥æ˜¯ä¸€ä¸ªå¼±çš„é¢„æµ‹æŒ‡æ ‡</strong>ã€‚</p></li>
</ul>
<p>Hereâ€™s how you could perform similar analysis and modeling in Python
using <code>seaborn</code> and <code>scikit-learn</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;default_data.csv&#x27; has columns: &#x27;default&#x27; (Yes/No), &#x27;balance&#x27;, &#x27;income&#x27;</span></span><br><span class="line"><span class="comment"># You would load your data like this:</span></span><br><span class="line"><span class="comment"># df = pd.read_csv(&#x27;default_data.csv&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># For demonstration, let&#x27;s create some sample data</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;balance&#x27;</span>: [<span class="number">1200</span>, <span class="number">2100</span>, <span class="number">800</span>, <span class="number">1800</span>, <span class="number">500</span>, <span class="number">1600</span>, <span class="number">2200</span>, <span class="number">1900</span>],</span><br><span class="line">    <span class="string">&#x27;income&#x27;</span>: [<span class="number">45000</span>, <span class="number">60000</span>, <span class="number">30000</span>, <span class="number">55000</span>, <span class="number">25000</span>, <span class="number">48000</span>, <span class="number">70000</span>, <span class="number">65000</span>],</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>: [<span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. Data Visualization (like the slides) ---</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">14</span>, <span class="number">5</span>))</span><br><span class="line">fig.suptitle(<span class="string">&#x27;Predictor Analysis for Default&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boxplot for Balance</span></span><br><span class="line">sns.boxplot(ax=axes[<span class="number">0</span>], x=<span class="string">&#x27;default&#x27;</span>, y=<span class="string">&#x27;balance&#x27;</span>, data=df)</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&#x27;Balance vs. Default Status&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boxplot for Income</span></span><br><span class="line">sns.boxplot(ax=axes[<span class="number">1</span>], x=<span class="string">&#x27;default&#x27;</span>, y=<span class="string">&#x27;income&#x27;</span>, data=df)</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">&#x27;Income vs. Default Status&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. Logistic Regression Modeling ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert categorical &#x27;default&#x27; column to 0s and 1s</span></span><br><span class="line">df[<span class="string">&#x27;default_encoded&#x27;</span>] = df[<span class="string">&#x27;default&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x == <span class="string">&#x27;Yes&#x27;</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define features (X) and target (y)</span></span><br><span class="line">X = df[[<span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line">y = df[<span class="string">&#x27;default_encoded&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create and train the logistic regression model</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on new data</span></span><br><span class="line"><span class="comment"># For example, a person with a $2000 balance and $50,000 income</span></span><br><span class="line">new_customer = [[<span class="number">2000</span>, <span class="number">50000</span>]]</span><br><span class="line">predicted_prob = model.predict_proba(new_customer)</span><br><span class="line">prediction = model.predict(new_customer)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Customer data: Balance=2000, Income=50000&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Probability of No Default vs. Default: <span class="subst">&#123;predicted_prob&#125;</span>&quot;</span>) <span class="comment"># [[P(No), P(Yes)]]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Final Prediction (0=No, 1=Yes): <span class="subst">&#123;prediction&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="the-mathematical-foundation-of-logistic-regression">2. the
mathematical foundation of logistic regression</h1>
<p>This set of slides explains the mathematical foundation of logistic
regression, how its parameters are estimated using Maximum Likelihood
Estimation (MLE), and how an iterative algorithm called Newton-Raphson
is used to perform this estimation.</p>
<p>é€»è¾‘å›å½’çš„æ•°å­¦åŸºç¡€ã€å¦‚ä½•ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)
ä¼°è®¡å…¶å‚æ•°ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨åä¸º Newton-Raphson çš„è¿­ä»£ç®—æ³•è¿›è¡Œä¼°è®¡ã€‚</p>
<h2
id="the-logistic-regression-model-from-probabilities-to-log-oddsé€»è¾‘å›å½’æ¨¡å‹ä»æ¦‚ç‡åˆ°å¯¹æ•°å‡ ç‡">2.1
The Logistic Regression Model: From Probabilities to
Log-Oddsé€»è¾‘å›å½’æ¨¡å‹ï¼šä»æ¦‚ç‡åˆ°å¯¹æ•°å‡ ç‡</h2>
<p>The core of logistic regression is transforming a linear model into a
valid probability. This is done using the <strong>logistic
function</strong>, also known as the sigmoid function.
é€»è¾‘å›å½’çš„æ ¸å¿ƒæ˜¯å°†çº¿æ€§æ¨¡å‹è½¬æ¢ä¸ºæœ‰æ•ˆçš„æ¦‚ç‡ã€‚è¿™å¯ä»¥é€šè¿‡<strong>é€»è¾‘å‡½æ•°</strong>ï¼ˆä¹Ÿç§°ä¸º
S å‹å‡½æ•°ï¼‰æ¥å®ç°ã€‚ #### <strong>Key Mathematical Formulas</strong></p>
<ol type="1">
<li><p><strong>Probability of Class 1:</strong> The model assumes the
probability of an observation <span
class="math inline">\(\mathbf{x}\)</span> belonging to class 1 is given
by the sigmoid function: <span class="math display">\[
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-\beta^T \mathbf{x})} =
\frac{\exp(\beta^T \mathbf{x})}{1 + \exp(\beta^T \mathbf{x})}
\]</span> This function always outputs a value between 0 and 1, making
it perfect for modeling probabilities.</p></li>
<li><p><strong>Odds:</strong> The odds are the ratio of the probability
of an event happening to the probability of it not happening. <span
class="math display">\[
\text{Odds} = \frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})} = \exp(\beta^T
\mathbf{x})
\]</span></p></li>
<li><p><strong>Log-Odds (Logit):</strong> By taking the natural
logarithm of the odds, we get a linear relationship with the predictors.
This is called the <strong>logit transformation</strong>. <span
class="math display">\[
\text{logit}(P(y=1|\mathbf{x})) =
\log\left(\frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})}\right) = \beta^T
\mathbf{x}
\]</span> This final equation is the heart of the model. It states that
the log-odds of the outcome are a linear function of the predictors.
This provides a great interpretation: a one-unit increase in a predictor
<span class="math inline">\(x_j\)</span> changes the log-odds by <span
class="math inline">\(\beta_j\)</span>.</p></li>
<li><p><strong>ç±»åˆ« 1 çš„æ¦‚ç‡</strong>ï¼šè¯¥æ¨¡å‹å‡è®¾è§‚æµ‹å€¼ <span
class="math inline">\(\mathbf{x}\)</span> å±äºç±»åˆ« 1 çš„æ¦‚ç‡ç”± S
å‹å‡½æ•°ç»™å‡ºï¼š <span class="math display">\[
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-\beta^T \mathbf{x})} =
\frac{\exp(\beta^T \mathbf{x})}{1 + \exp(\beta^T \mathbf{x})}
\]</span> æ­¤å‡½æ•°çš„è¾“å‡ºå€¼å§‹ç»ˆä»‹äº 0 å’Œ 1
ä¹‹é—´ï¼Œéå¸¸é€‚åˆç”¨äºæ¦‚ç‡å»ºæ¨¡ã€‚</p></li>
<li><p><strong>å‡ ç‡</strong>ï¼š**å‡ ç‡æ˜¯äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ä¸ä¸å‘ç”Ÿçš„æ¦‚ç‡ä¹‹æ¯”ã€‚
<span class="math display">\[
\text{Odds} = \frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})} = \exp(\beta^T
\mathbf{x})
\]</span></p></li>
<li><p><strong>å¯¹æ•°æ¦‚ç‡
(Logit)</strong>ï¼šé€šè¿‡å¯¹æ¦‚ç‡å–è‡ªç„¶å¯¹æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ¦‚ç‡ä¸é¢„æµ‹å˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚è¿™è¢«ç§°ä¸º<strong>logit
å˜æ¢</strong>ã€‚ <span class="math display">\[
\text{logit}(P(y=1|\mathbf{x})) =
\log\left(\frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})}\right) = \beta^T
\mathbf{x}
\]</span>
æœ€åä¸€ä¸ªæ–¹ç¨‹æ˜¯æ¨¡å‹çš„æ ¸å¿ƒã€‚å®ƒæŒ‡å‡ºç»“æœçš„å¯¹æ•°æ¦‚ç‡æ˜¯é¢„æµ‹å˜é‡çš„çº¿æ€§å‡½æ•°ã€‚è¿™æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„è§£é‡Šï¼šé¢„æµ‹å˜é‡
<span class="math inline">\(x_j\)</span>
æ¯å¢åŠ ä¸€ä¸ªå•ä½ï¼Œå¯¹æ•°æ¦‚ç‡å°±ä¼šæ”¹å˜ <span
class="math inline">\(\beta_j\)</span>ã€‚</p></li>
</ol>
<h2
id="fitting-the-model-maximum-likelihood-estimation-mle-æ‹Ÿåˆæ¨¡å‹æœ€å¤§ä¼¼ç„¶ä¼°è®¡-mle">2.2
Fitting the Model: Maximum Likelihood Estimation (MLE)
æ‹Ÿåˆæ¨¡å‹ï¼šæœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)</h2>
<p>Unlike linear regression, which uses least squares to find the
best-fit line, logistic regression uses <strong>Maximum Likelihood
Estimation (MLE)</strong>. The goal of MLE is to find the parameter
values (the <span class="math inline">\(\beta\)</span> coefficients)
that maximize the probability of observing the actual data that we have.
ä¸ä½¿ç”¨æœ€å°äºŒä¹˜æ³•å¯»æ‰¾æœ€ä½³æ‹Ÿåˆçº¿çš„çº¿æ€§å›å½’ä¸åŒï¼Œé€»è¾‘å›å½’ä½¿ç”¨<strong>æœ€å¤§ä¼¼ç„¶ä¼°è®¡
(MLE)</strong>ã€‚MLE
çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä½¿è§‚æµ‹åˆ°å®é™…æ•°æ®çš„æ¦‚ç‡æœ€å¤§åŒ–çš„å‚æ•°å€¼ï¼ˆ<span
class="math inline">\(\beta\)</span> ç³»æ•°ï¼‰ã€‚</p>
<ol type="1">
<li><p><strong>Likelihood Function:</strong> This is the joint
probability of observing all the data points in our sample. Assuming
each observation is independent, itâ€™s the product of the individual
probabilities:
1.<strong>ä¼¼ç„¶å‡½æ•°</strong>ï¼šè¿™æ˜¯è§‚æµ‹åˆ°æ ·æœ¬ä¸­æ‰€æœ‰æ•°æ®ç‚¹çš„è”åˆæ¦‚ç‡ã€‚å‡è®¾æ¯ä¸ªè§‚æµ‹å€¼éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œå®ƒæ˜¯å„ä¸ªæ¦‚ç‡çš„ä¹˜ç§¯ï¼š
<span class="math display">\[
L(\beta) = \prod_{i=1}^{n} P(y_i|\mathbf{x}_i)
\]</span> A clever way to write this for a binary (0/1) outcome is:
<span class="math display">\[
L(\beta) = \prod_{i=1}^{n} \frac{\exp(y_i \beta^T \mathbf{x}_i)}{1 +
\exp(\beta^T \mathbf{x}_i)}
\]</span></p></li>
<li><p><strong>Log-Likelihood Function:</strong> Products are difficult
to work with mathematically, so we work with the logarithm of the
likelihood, which turns the product into a sum. Maximizing the
log-likelihood is the same as maximizing the likelihood.</p></li>
<li><p><strong>å¯¹æ•°ä¼¼ç„¶å‡½æ•°</strong>ï¼šä¹˜ç§¯åœ¨æ•°å­¦ä¸Šå¾ˆéš¾å¤„ç†ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ä¼¼ç„¶çš„å¯¹æ•°ï¼Œå°†ä¹˜ç§¯è½¬åŒ–ä¸ºå’Œã€‚æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ä¸æœ€å¤§åŒ–ä¼¼ç„¶ç›¸åŒã€‚
<span class="math display">\[
\ell(\beta) = \log(L(\beta)) = \sum_{i=1}^{n} \left[ y_i \beta^T
\mathbf{x}_i - \log(1 + \exp(\beta^T \mathbf{x}_i)) \right]
\]</span> <strong>Key Takeaway:</strong> The slides correctly state that
there is <strong>no explicit formula</strong> to solve for the <span
class="math inline">\(\hat{\beta}\)</span> that maximizes this function.
We must find it using a numerical optimization algorithm.
æ²¡æœ‰<strong>æ˜ç¡®çš„å…¬å¼</strong>æ¥æ±‚è§£æœ€å¤§åŒ–è¯¥å‡½æ•°çš„<span
class="math inline">\(\hat{\beta}\)</span>ã€‚æˆ‘ä»¬å¿…é¡»ä½¿ç”¨æ•°å€¼ä¼˜åŒ–ç®—æ³•æ¥æ‰¾åˆ°å®ƒã€‚</p></li>
</ol>
<h2 id="the-algorithm-newton-raphson-ç®—æ³•ç‰›é¡¿-æ‹‰å¤«æ£®ç®—æ³•">2.3 The
Algorithm: Newton-Raphson ç®—æ³•ï¼šç‰›é¡¿-æ‹‰å¤«æ£®ç®—æ³•</h2>
<p>The slides introduce the <strong>Newton-Raphson algorithm</strong> as
the method to find the optimal <span
class="math inline">\(\hat{\beta}\)</span>. Itâ€™s an efficient iterative
algorithm for finding the roots of a function (i.e., where <span
class="math inline">\(f(x)=0\)</span>).</p>
<p><strong>How does this apply to logistic regression?</strong> To
maximize the log-likelihood function <span
class="math inline">\(\ell(\beta)\)</span>, we need to find the point
where its derivative (gradient) is equal to zero. So, Newton-Raphson is
used to solve <span class="math inline">\(\frac{d\ell(\beta)}{d\beta} =
0\)</span>.</p>
<p>å®ƒæ˜¯ä¸€ç§é«˜æ•ˆçš„è¿­ä»£ç®—æ³•ï¼Œç”¨äºæ±‚å‡½æ•°çš„æ ¹ï¼ˆå³ï¼Œå½“<span
class="math inline">\(f(x)=0\)</span>æ—¶ï¼‰ã€‚</p>
<p><strong>è¿™å¦‚ä½•åº”ç”¨äºé€»è¾‘å›å½’ï¼Ÿ</strong> ä¸ºäº†æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶å‡½æ•° <span
class="math inline">\(\ell(\beta)\)</span>ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°å…¶å¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰ç­‰äºé›¶çš„ç‚¹ã€‚å› æ­¤ï¼Œç‰›é¡¿-æ‹‰å¤«æ£®æ³•ç”¨äºæ±‚è§£
<span class="math inline">\(\frac{d\ell(\beta)}{d\beta} =
0\)</span>ã€‚</p>
<h4 id="the-general-newton-raphson-method"><strong>The General
Newton-Raphson Method</strong></h4>
<p>The algorithm starts with an initial guess, <span
class="math inline">\(x^{old}\)</span>, and iteratively refines it using
the following update rule, which is based on a Taylor series
approximation: <span class="math display">\[
x^{new} = x^{old} - \frac{f(x^{old})}{f&#39;(x^{old})}
\]</span> where <span class="math inline">\(f&#39;(x)\)</span> is the
derivative of <span class="math inline">\(f(x)\)</span>. You repeat this
step until the value of <span class="math inline">\(x\)</span>
converges.</p>
<p>è¯¥ç®—æ³•ä»åˆå§‹ä¼°è®¡ <span class="math inline">\(x^{old}\)</span>
å¼€å§‹ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹åŸºäºæ³°å‹’çº§æ•°è¿‘ä¼¼çš„æ›´æ–°è§„åˆ™è¿­ä»£åœ°å¯¹å…¶è¿›è¡Œä¼˜åŒ–ï¼š <span
class="math display">\[
x^{new} = x^{old} - \frac{f(x^{old})}{f&#39;(x^{old})}
\]</span> å…¶ä¸­ <span class="math inline">\(f&#39;(x)\)</span> æ˜¯ <span
class="math inline">\(f(x)\)</span> çš„å¯¼æ•°ã€‚é‡å¤æ­¤æ­¥éª¤ï¼Œç›´åˆ° <span
class="math inline">\(x\)</span> çš„å€¼æ”¶æ•›ã€‚</p>
<h4
id="important-image-newton-raphson-example-x3---4-0"><strong>Important
Image: Newton-Raphson Example (<span class="math inline">\(x^3 - 4 =
0\)</span>)</strong></h4>
<p>[Image showing iterations of Newton-Raphson]</p>
<p>This slide is a great illustration of the algorithmâ€™s power. *
<strong>Goal:</strong> Find <span class="math inline">\(x\)</span> such
that <span class="math inline">\(f(x) = x^3 - 4 = 0\)</span>. *
<strong>Function:</strong> <span class="math inline">\(f(x) = x^3 -
4\)</span> * <strong>Derivative:</strong> <span
class="math inline">\(f&#39;(x) = 3x^2\)</span> * <strong>Update
Rule:</strong> <span class="math inline">\(x^{new} = x^{old} -
\frac{(x^{old})^3 - 4}{3(x^{old})^2}\)</span> Starting with a guess of
<span class="math inline">\(x^{old} = 2\)</span>, the algorithm
converges to the true answer (<span class="math inline">\(4^{1/3}
\approx 1.5874\)</span>) in just 4 steps.</p>
<ul>
<li><strong>ç›®æ ‡</strong>ï¼šæ‰¾åˆ° <span
class="math inline">\(x\)</span>ï¼Œä½¿å¾— <span class="math inline">\(f(x)
= x^3 - 4 = 0\)</span>ã€‚</li>
<li><strong>å‡½æ•°</strong>ï¼š<span class="math inline">\(f(x) = x^3 -
4\)</span></li>
<li><strong>å¯¼æ•°</strong>ï¼š<span class="math inline">\(f&#39;(x) =
3x^2\)</span></li>
<li><strong>æ›´æ–°è§„åˆ™</strong>ï¼š<span class="math inline">\(x^{new} =
x^{old} - \frac{(x^{old})^3 - 4}{3(x^{old})^2}\)</span> ä» <span
class="math inline">\(x^{old} = 2\)</span> çš„çŒœæµ‹å¼€å§‹ï¼Œè¯¥ç®—æ³•ä»…ç”¨ 4
æ­¥å°±æ”¶æ•›åˆ°çœŸå®ç­”æ¡ˆ (<span class="math inline">\(4^{1/3} \approx
1.5874\)</span>)ã€‚</li>
</ul>
<h4 id="code-understanding-python"><strong>Code Understanding
(Python)</strong></h4>
<p>The slides show Python code implementing Newton-Raphson. Letâ€™s break
down the key function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the function we want to find the root of</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) - x*x + <span class="number">3</span> * np.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define its derivative</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_prime</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) - <span class="number">2</span>*x + <span class="number">3</span> * np.cos(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Newton-Raphson method</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">newton_raphson</span>(<span class="params">x0, tol=<span class="number">1e-10</span>, max_iter=<span class="number">100</span></span>):</span><br><span class="line">    x = x0 <span class="comment"># Start with the initial guess</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        fx = f(x)      <span class="comment"># Calculate f(x_old)</span></span><br><span class="line">        fpx = f_prime(x) <span class="comment"># Calculate f&#x27;(x_old)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> fpx == <span class="number">0</span>: <span class="comment"># Cannot divide by zero</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Zero derivative. No solution found.&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is the core update rule</span></span><br><span class="line">        x_new = x - fx / fpx</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check if the change is small enough to stop</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(x_new - x) &lt; tol:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Converged to <span class="subst">&#123;x_new&#125;</span> after <span class="subst">&#123;i+<span class="number">1</span>&#125;</span> iterations.&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> x_new</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update x for the next iteration</span></span><br><span class="line">        x = x_new</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Exceeded maximum iterations. No solution found.&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial guess and execution</span></span><br><span class="line">x0 = <span class="number">0.5</span></span><br><span class="line">root = newton_raphson(x0)</span><br></pre></td></tr></table></figure>
<p>The slides show that with a good initial guess
(<code>x0 = 0.5</code>), the algorithm converges quickly. With a bad one
(<code>x0 = 50</code>), it still converges but takes many more steps.
This highlights the importance of the starting point. The slides also
show an implementation of <strong>Gradient Descent</strong>, another
popular optimization algorithm which uses the update rule
<code>x_new = x - learning_rate * gradient</code>.</p>
<h1
id="provide-a-great-case-study-on-logistic-regression-particularly-on-the-important-concept-of-confounding-variables.-heres-a-summary-covering-the-math-code-and-key-insights.">Provide
a great case study on logistic regression, particularly on the important
concept of confounding variables. Hereâ€™s a summary covering the math,
code, and key insights.</h1>
<ol start="3" type="1">
<li><h1 id="core-concept-logistic-regression-æ ¸å¿ƒæ¦‚å¿µé€»è¾‘å›å½’">Core
Concept: Logistic Regression ğŸ“ˆ # æ ¸å¿ƒæ¦‚å¿µï¼šé€»è¾‘å›å½’ ğŸ“ˆ</h1></li>
</ol>
<p>Logistic regression is a statistical method used for <strong>binary
classification</strong>, which means predicting an outcome that can only
be one of two things (e.g., Yes/No, True/False, 1/0).</p>
<p>In this example, the goal is to predict the probability that a
customer will <strong>default</strong> on a loan (Yes or No) based on
factors like their account <code>balance</code>, <code>income</code>,
and whether they are a <code>student</code>.</p>
<p>The core of logistic regression is the <strong>sigmoid (or logistic)
function</strong>, which takes any real-valued number and squishes it to
a value between 0 and 1, representing a probability.</p>
<p><span class="math display">\[
\hat{P}(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_p
X_p)}}
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{P}(Y=1|X)\)</span> is the predicted
probability of the outcome being â€œYesâ€ (e.g., default).</li>
<li><span class="math inline">\(\beta_0\)</span> is the intercept.</li>
<li><span class="math inline">\(\beta_1, ..., \beta_p\)</span> are the
coefficients for each input variable (<span class="math inline">\(X_1,
..., X_p\)</span>). The modelâ€™s job is to find the best values for these
<span class="math inline">\(\beta\)</span> coefficients.</li>
</ul>
<hr />
<p>é€»è¾‘å›å½’æ˜¯ä¸€ç§ç”¨äº<strong>äºŒå…ƒåˆ†ç±»</strong>çš„ç»Ÿè®¡æ–¹æ³•ï¼Œè¿™æ„å‘³ç€é¢„æµ‹ç»“æœåªèƒ½æ˜¯ä¸¤ç§æƒ…å†µä¹‹ä¸€ï¼ˆä¾‹å¦‚ï¼Œæ˜¯/å¦ã€çœŸ/å‡ã€1/0ï¼‰ã€‚</p>
<p>åœ¨æœ¬ä¾‹ä¸­ï¼Œç›®æ ‡æ˜¯æ ¹æ®å®¢æˆ·è´¦æˆ·â€œä½™é¢â€ã€â€œæ”¶å…¥â€ä»¥åŠæ˜¯å¦ä¸ºâ€œå­¦ç”Ÿâ€ç­‰å› ç´ ï¼Œé¢„æµ‹å®¢æˆ·<strong>æ‹–æ¬ </strong>è´·æ¬¾ï¼ˆæ˜¯æˆ–å¦ï¼‰çš„æ¦‚ç‡ã€‚</p>
<p>é€»è¾‘å›å½’çš„æ ¸å¿ƒæ˜¯<strong>Sigmoidï¼ˆæˆ–é€»è¾‘ï¼‰å‡½æ•°</strong>ï¼Œå®ƒå°†ä»»ä½•å®æ•°å‹ç¼©ä¸ºä»‹äº
0 å’Œ 1 ä¹‹é—´çš„å€¼ï¼Œä»¥è¡¨ç¤ºæ¦‚ç‡ã€‚</p>
<p><span class="math display">\[
\hat{P}(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_p
X_p)}}
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{P}(Y=1|X)\)</span>
æ˜¯ç»“æœä¸ºâ€œæ˜¯â€ï¼ˆä¾‹å¦‚ï¼Œé»˜è®¤ï¼‰çš„é¢„æµ‹æ¦‚ç‡ã€‚</li>
<li><span class="math inline">\(\beta_0\)</span> æ˜¯æˆªè·ã€‚</li>
<li><span class="math inline">\(\beta_1, ..., \beta_p\)</span>
æ˜¯æ¯ä¸ªè¾“å…¥å˜é‡ (<span class="math inline">\(X_1, ..., X_p\)</span>)
çš„ç³»æ•°ã€‚æ¨¡å‹çš„ä»»åŠ¡æ˜¯æ‰¾åˆ°è¿™äº› <span class="math inline">\(\beta\)</span>
ç³»æ•°çš„æœ€ä½³å€¼ã€‚</li>
</ul>
<h2 id="how-the-model-learns-mathematical-foundation">3.1 How the Model
â€œLearnsâ€ (Mathematical Foundation)</h2>
<p>The slides show that the modelâ€™s coefficients (<span
class="math inline">\(\beta\)</span>) are found using an algorithm like
<strong>Newton-Raphson</strong>. This is an iterative process to find
the values that <strong>maximize the log-likelihood function</strong>.
Think of this as finding the coefficient values that make the observed
data most
probable.è¿™æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œç”¨äºæŸ¥æ‰¾<strong>æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶å‡½æ•°</strong>çš„å€¼ã€‚å¯ä»¥å°†å…¶è§†ä¸ºæŸ¥æ‰¾ä½¿è§‚æµ‹æ•°æ®æ¦‚ç‡æœ€å¤§çš„ç³»æ•°å€¼ã€‚</p>
<p>The key slide for this is the one titled â€œNewton-Raphson Iterative
Algorithmâ€. It shows the formulas for: * The <strong>Gradient</strong>
(<span class="math inline">\(\nabla\ell\)</span>): The direction of the
steepest ascent of the log-likelihood function. * The
<strong>Hessian</strong> (<span class="math inline">\(H\)</span>): The
curvature of the log-likelihood function.</p>
<ul>
<li><strong>æ¢¯åº¦</strong> (<span
class="math inline">\(\nabla\ell\)</span>)ï¼šå¯¹æ•°ä¼¼ç„¶å‡½æ•°æœ€é™¡ä¸Šå‡çš„æ–¹å‘ã€‚</li>
<li><strong>é»‘æ£®çŸ©é˜µ</strong> (<span
class="math inline">\(H\)</span>)ï¼šå¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„æ›²ç‡ã€‚</li>
</ul>
<p>The updating rule is given by: <span class="math display">\[
\beta^{new} = \beta^{old} - H^{-1}\nabla\ell
\]</span> This formula is used repeatedly until the coefficient values
stop changing significantly, meaning the algorithm has converged to the
best fit. This process is also referred to as <strong>Iteratively
Reweighted Least Squares (IRLS)</strong>.
æ­¤å…¬å¼åå¤ä½¿ç”¨ï¼Œç›´åˆ°ç³»æ•°å€¼ä¸å†å‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œè¿™æ„å‘³ç€ç®—æ³•å·²æ”¶æ•›åˆ°æœ€ä½³æ‹Ÿåˆå€¼ã€‚æ­¤è¿‡ç¨‹ä¹Ÿç§°ä¸º<strong>è¿­ä»£é‡åŠ æƒæœ€å°äºŒä¹˜æ³•
(IRLS)</strong>ã€‚</p>
<hr />
<h2 id="the-puzzle-a-tale-of-two-models">3.2 The Puzzle: A Tale of Two
Models ğŸ•µï¸â€â™‚ï¸</h2>
<p>The most important story in these slides is how the effect of being a
student changes depending on the model. This is a classic example of a
<strong>confounding variable</strong>.</p>
<h4 id="model-1-simple-logistic-regression-default-vs.-student">Model 1:
Simple Logistic Regression (Default vs.Â Student)</h4>
<p>When predicting default using <em>only</em> student status, the model
is: <code>default ~ student</code></p>
<p>From the slides, the coefficients are: * Intercept (<span
class="math inline">\(\beta_0\)</span>): -3.5041 * student[Yes] (<span
class="math inline">\(\beta_1\)</span>): <strong>0.4049</strong>
(positive)</p>
<p>The equation for the log-odds is: <span class="math display">\[
\log\left(\frac{P(\text{default})}{1-P(\text{default})}\right) = -3.5041
+ 0.4049 \times (\text{is\_student})
\]</span></p>
<p><strong>Conclusion:</strong> The positive coefficient (0.4049)
suggests that <strong>students are more likely to default</strong> than
non-students. The slides calculate the probabilities: * <strong>Student
Default Probability:</strong> 4.31% * <strong>Non-Student Default
Probability:</strong> 2.92%</p>
<p>å­¦ç”Ÿèº«ä»½çš„å½±å“å¦‚ä½•æ ¹æ®æ¨¡å‹è€Œå˜åŒ–ã€‚è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„<strong>æ··æ‚å˜é‡</strong>çš„ä¾‹å­ã€‚</p>
<h4 id="æ¨¡å‹-1ç®€å•é€»è¾‘å›å½’è¿çº¦-vs.-å­¦ç”Ÿ">æ¨¡å‹ 1ï¼šç®€å•é€»è¾‘å›å½’ï¼ˆè¿çº¦
vs.Â å­¦ç”Ÿï¼‰</h4>
<p>ä»…ä½¿ç”¨å­¦ç”Ÿèº«ä»½é¢„æµ‹è¿çº¦æ—¶ï¼Œæ¨¡å‹ä¸ºï¼š <code>default ~ student</code></p>
<p>å¹»ç¯ç‰‡ä¸­æ˜¾ç¤ºçš„ç³»æ•°ä¸ºï¼š * æˆªè· (<span
class="math inline">\(\beta_0\)</span>): -3.5041 * å­¦ç”Ÿ[æ˜¯] (<span
class="math inline">\(\beta_1\)</span>):
<strong>0.4049</strong>ï¼ˆæ­£ï¼‰</p>
<p>å¯¹æ•°æ¦‚ç‡å…¬å¼ä¸ºï¼š <span class="math display">\[
\log\left(\frac{P(\text{default})}{1-P(\text{default})}\right) = -3.5041
+ 0.4049 \times (\text{is\_student})
\]</span></p>
<p><strong>ç»“è®º</strong>ï¼šæ­£ç³»æ•° (0.4049)
è¡¨æ˜<strong>å­¦ç”Ÿæ¯”éå­¦ç”Ÿæ›´æœ‰å¯èƒ½è¿çº¦</strong>ã€‚å¹»ç¯ç‰‡è®¡ç®—äº†ä»¥ä¸‹æ¦‚ç‡ï¼š *
<strong>å­¦ç”Ÿè¿çº¦æ¦‚ç‡</strong>ï¼š4.31% *
<strong>éå­¦ç”Ÿè¿çº¦æ¦‚ç‡</strong>ï¼š2.92%</p>
<h2
id="model-2-multiple-logistic-regression-default-vs.-all-variables-æ¨¡å‹-2å¤šå…ƒé€»è¾‘å›å½’è¿çº¦-vs.-æ‰€æœ‰å˜é‡">3.3
Model 2: Multiple Logistic Regression (Default vs.Â All Variables) æ¨¡å‹
2ï¼šå¤šå…ƒé€»è¾‘å›å½’ï¼ˆè¿çº¦ vs.Â æ‰€æœ‰å˜é‡ï¼‰</h2>
<p>When we add <code>balance</code> and <code>income</code> to the
model, it becomes: <code>default ~ student + balance + income</code></p>
<p>From the slides, the new coefficients are: * Intercept (<span
class="math inline">\(\beta_0\)</span>): -10.8690 * balance (<span
class="math inline">\(\beta_1\)</span>): 0.0057 * income (<span
class="math inline">\(\beta_2\)</span>): 0.0030 * student[Yes] (<span
class="math inline">\(\beta_3\)</span>): <strong>-0.6468</strong>
(negative)</p>
<p><strong>The Shocking Twist!</strong> The coefficient for
<code>student[Yes]</code> is now <strong>negative</strong>.</p>
<p><strong>Conclusion:</strong> When we control for balance and income,
<strong>students are actually <em>less</em> likely to default</strong>
than non-students with the same balance and income.</p>
<h4 id="why-the-change-the-confounding-variable-explained">Why the
Change? The Confounding Variable Explained</h4>
<p>The key insight, explained on the slide with multi-colored text
bubbles, is that <strong>students, on average, have higher credit card
balances</strong>.</p>
<ul>
<li>In the simple model, the <code>student</code> variable was
inadvertently capturing the risk associated with having a high
<code>balance</code>. The model mistakenly concluded â€œbeing a student
causes default.â€</li>
<li>In the multiple model, the <code>balance</code> variable properly
accounts for the risk from a high balance. With that effect isolated,
the <code>student</code> variable can show its true, underlying
relationship with default, which is negative.</li>
</ul>
<p>This demonstrates why itâ€™s crucial to consider multiple relevant
variables to avoid drawing incorrect conclusions. <strong>The most
important slides are the ones that present this paradox and its
explanation.</strong></p>
<p><strong>ä»¤äººéœ‡æƒŠçš„è½¬æŠ˜ï¼</strong> <code>student[Yes]</code>
çš„ç³»æ•°ç°åœ¨ä¸º<strong>è´Ÿ</strong>ã€‚</p>
<p><strong>ç»“è®ºï¼š</strong>å½“æˆ‘ä»¬æ§åˆ¶ä½™é¢å’Œæ”¶å…¥æ—¶ï¼Œ<strong>å­¦ç”Ÿå®é™…ä¸Šæ¯”å…·æœ‰ç›¸åŒä½™é¢å’Œæ”¶å…¥çš„éå­¦ç”Ÿæ›´<em>ä½</em>äºè¿çº¦</strong>ã€‚</p>
<h4 id="ä¸ºä»€ä¹ˆä¼šæœ‰å˜åŒ–æ··æ‚å˜é‡è§£é‡Š">ä¸ºä»€ä¹ˆä¼šæœ‰å˜åŒ–ï¼Ÿæ··æ‚å˜é‡è§£é‡Š</h4>
<p>å¹»ç¯ç‰‡ä¸Šç”¨å½©è‰²æ–‡å­—æ°”æ³¡è§£é‡Šäº†å…³é”®çš„è§è§£ï¼Œå³<strong>å­¦ç”Ÿå¹³å‡æ‹¥æœ‰æ›´é«˜çš„ä¿¡ç”¨å¡ä½™é¢</strong>ã€‚</p>
<ul>
<li>åœ¨ç®€å•æ¨¡å‹ä¸­ï¼Œâ€œå­¦ç”Ÿâ€å˜é‡æ— æ„ä¸­æ•æ‰åˆ°äº†é«˜ä½™é¢å¸¦æ¥çš„é£é™©ã€‚è¯¥æ¨¡å‹é”™è¯¯åœ°å¾—å‡ºäº†â€œå­¦ç”Ÿèº«ä»½å¯¼è‡´è¿çº¦â€çš„ç»“è®ºã€‚</li>
<li>åœ¨å¤šå…ƒæ¨¡å‹ä¸­ï¼Œâ€œä½™é¢â€å˜é‡æ­£ç¡®åœ°è§£é‡Šäº†é«˜ä½™é¢å¸¦æ¥çš„é£é™©ã€‚åœ¨åˆ†ç¦»å‡ºè¿™ä¸€å½±å“åï¼Œâ€œå­¦ç”Ÿâ€å˜é‡å¯ä»¥æ˜¾ç¤ºå…¶ä¸è¿çº¦ä¹‹é—´çœŸå®çš„æ½œåœ¨å…³ç³»ï¼Œå³è´Ÿç›¸å…³å…³ç³»ã€‚</li>
</ul>
<p>è¿™è¯´æ˜äº†ä¸ºä»€ä¹ˆè€ƒè™‘å¤šä¸ªç›¸å…³å˜é‡ä»¥é¿å…å¾—å‡ºé”™è¯¯ç»“è®ºè‡³å…³é‡è¦ã€‚</p>
<hr />
<h3 id="code-implementation-r-vs.-python">Code Implementation: R
vs.Â Python</h3>
<p>The slides use Râ€™s <code>glm()</code> (Generalized Linear Model)
function. Hereâ€™s how you would replicate this in Python.</p>
<h4 id="r-code-from-slides">R Code (from slides)</h4>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Simple Model</span></span><br><span class="line">glmod2 <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>default <span class="operator">~</span> student<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> family<span class="operator">=</span>binomial<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>glmod2<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model</span></span><br><span class="line">glmod3 <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>default <span class="operator">~</span> .<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> family<span class="operator">=</span>binomial<span class="punctuation">)</span> <span class="comment"># &#x27;.&#x27; means all other variables</span></span><br><span class="line">summary<span class="punctuation">(</span>glmod3<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<h4 id="python-equivalent">Python Equivalent</h4>
<p>We can use two popular libraries: <code>statsmodels</code> (which
gives R-style summaries) and <code>scikit-learn</code> (the standard for
machine learning).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Default&#x27; is a pandas DataFrame with columns:</span></span><br><span class="line"><span class="comment"># &#x27;default&#x27; (0/1), &#x27;student&#x27; (0/1), &#x27;balance&#x27;, &#x27;income&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Using statsmodels (recommended for interpretation) ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data</span></span><br><span class="line"><span class="comment"># For statsmodels, we need to manually add the intercept</span></span><br><span class="line">X_simple = Default[[<span class="string">&#x27;student&#x27;</span>]]</span><br><span class="line">X_simple = sm.add_constant(X_simple)</span><br><span class="line">y = Default[<span class="string">&#x27;default&#x27;</span>]</span><br><span class="line"></span><br><span class="line">X_multiple = Default[[<span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line">X_multiple = sm.add_constant(X_multiple)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simple Model: default ~ student</span></span><br><span class="line">model_simple = sm.Logit(y, X_simple).fit()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- Simple Model ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_simple.summary())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model: default ~ student + balance + income</span></span><br><span class="line">model_multiple = sm.Logit(y, X_multiple).fit()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Multiple Model ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_multiple.summary())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Using scikit-learn (recommended for prediction tasks) ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data (scikit-learn adds intercept by default)</span></span><br><span class="line">X_simple_sk = Default[[<span class="string">&#x27;student&#x27;</span>]]</span><br><span class="line">y_sk = Default[<span class="string">&#x27;default&#x27;</span>]</span><br><span class="line"></span><br><span class="line">X_multiple_sk = Default[[<span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simple Model</span></span><br><span class="line">clf_simple = LogisticRegression().fit(X_simple_sk, y_sk)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nSimple Model Intercept (scikit-learn): <span class="subst">&#123;clf_simple.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Simple Model Coefficient (scikit-learn): <span class="subst">&#123;clf_simple.coef_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model</span></span><br><span class="line">clf_multiple = LogisticRegression().fit(X_multiple_sk, y_sk)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nMultiple Model Intercept (scikit-learn): <span class="subst">&#123;clf_multiple.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Multiple Model Coefficients (scikit-learn): <span class="subst">&#123;clf_multiple.coef_&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1
id="making-predictions-and-the-decision-boundary-è¿›è¡Œé¢„æµ‹å’Œå†³ç­–è¾¹ç•Œ">4
Making Predictions and the Decision Boundary ğŸ¯è¿›è¡Œé¢„æµ‹å’Œå†³ç­–è¾¹ç•Œ</h1>
<p>Once the model is trained (i.e., we have the coefficients <span
class="math inline">\(\hat{\beta}\)</span>), we can make predictions.
ä¸€æ—¦æ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆå³ï¼Œæˆ‘ä»¬æœ‰äº†ç³»æ•° <span
class="math inline">\(\hat{\beta}\)</span>ï¼‰ï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œé¢„æµ‹äº†ã€‚ ##
Math Behind Predictions</p>
<p>The model outputs the <strong>log-odds</strong>, which can be
converted into a probability. A key concept is the <strong>decision
boundary</strong>, which is the threshold where the model is uncertain
(probability = 50%).
æ¨¡å‹è¾“å‡º<strong>å¯¹æ•°æ¦‚ç‡</strong>ï¼Œå®ƒå¯ä»¥è½¬æ¢ä¸ºæ¦‚ç‡ã€‚ä¸€ä¸ªå…³é”®æ¦‚å¿µæ˜¯<strong>å†³ç­–è¾¹ç•Œ</strong>ï¼Œå®ƒæ˜¯æ¨¡å‹ä¸ç¡®å®šçš„é˜ˆå€¼ï¼ˆæ¦‚ç‡
= 50%ï¼‰ã€‚</p>
<ol type="1">
<li><p><strong>The Estimated Odds</strong>: The core output of the
linear part of the model is the exponential of the linear equation,
which gives the odds of the outcome being â€˜Yesâ€™ (or 1).
<strong>ä¼°è®¡æ¦‚ç‡</strong>ï¼šæ¨¡å‹çº¿æ€§éƒ¨åˆ†çš„æ ¸å¿ƒè¾“å‡ºæ˜¯çº¿æ€§æ–¹ç¨‹çš„æŒ‡æ•°ï¼Œå®ƒç»™å‡ºäº†ç»“æœä¸ºâ€œæ˜¯â€ï¼ˆæˆ–
1ï¼‰çš„æ¦‚ç‡ã€‚</p>
<p><span class="math display">\[
\]</span>$$\frac{\hat{P}(y=1|\mathbf{x}_0)}{\hat{P}(y=0|\mathbf{x}_0)} =
\exp(\hat{\beta}^\top \mathbf{x}_0)</p>
<p><span class="math display">\[
\]</span><span class="math display">\[
\]</span></p></li>
<li><p><strong>The Decision Rule</strong>: We classify a new observation
<span class="math inline">\(\mathbf{x}_0\)</span> by comparing its
predicted odds to a threshold <span
class="math inline">\(\delta\)</span>.
<strong>å†³ç­–è§„åˆ™</strong>ï¼šæˆ‘ä»¬é€šè¿‡æ¯”è¾ƒæ–°è§‚æµ‹å€¼ <span
class="math inline">\(\mathbf{x}_0\)</span> çš„é¢„æµ‹æ¦‚ç‡ä¸é˜ˆå€¼ <span
class="math inline">\(\delta\)</span> æ¥å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚</p>
<ul>
<li>Predict <span class="math inline">\(y=1\)</span> if <span
class="math inline">\(\exp(\hat{\beta}^\top \mathbf{x}_0) &gt;
\delta\)</span></li>
<li>Predict <span class="math inline">\(y=0\)</span> if <span
class="math inline">\(\exp(\hat{\beta}^\top \mathbf{x}_0) &lt;
\delta\)</span> A common default is <span
class="math inline">\(\delta=1\)</span>, which means we predict â€˜Yesâ€™ if
the probability is greater than 0.5.</li>
</ul></li>
<li><p><strong>The Linear Boundary</strong>: The decision boundary
itself is where the odds are exactly equal to the threshold. By taking
the logarithm, we see that this boundary is a <strong>linear
equation</strong>. This is why logistic regression is called a
<strong>linear classifier</strong>.
<strong>çº¿æ€§è¾¹ç•Œ</strong>ï¼šå†³ç­–è¾¹ç•Œæœ¬èº«å°±æ˜¯æ¦‚ç‡æ°å¥½ç­‰äºé˜ˆå€¼çš„åœ°æ–¹ã€‚å–å¯¹æ•°åï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸ªè¾¹ç•Œæ˜¯ä¸€ä¸ª<strong>çº¿æ€§æ–¹ç¨‹</strong>ã€‚è¿™å°±æ˜¯é€»è¾‘å›å½’è¢«ç§°ä¸º<strong>çº¿æ€§åˆ†ç±»å™¨</strong>çš„åŸå› ã€‚
<span class="math display">\[
\]</span>$$\hat{\beta}^\top \mathbf{x} = \log(\delta)</p>
<p><span class="math display">\[
\]</span>$$For <span class="math inline">\(\delta=1\)</span>, the
boundary is simply <span class="math inline">\(\hat{\beta}^\top
\mathbf{x} = 0\)</span>.</p></li>
</ol>
<p>This concept is visualized perfectly in the slide titled â€œLinear
Classifier,â€ which shows a straight line neatly separating two classes
of data points.
é¢˜ä¸ºâ€œçº¿æ€§åˆ†ç±»å™¨â€çš„å¹»ç¯ç‰‡å®Œç¾åœ°å±•ç¤ºäº†è¿™ä¸€æ¦‚å¿µï¼Œå®ƒå±•ç¤ºäº†ä¸€æ¡ç›´çº¿ï¼Œå°†ä¸¤ç±»æ•°æ®ç‚¹å·§å¦™åœ°åˆ†éš”å¼€æ¥ã€‚</p>
<h2 id="visualizing-the-confounding-effect">Visualizing the Confounding
Effect</h2>
<p>The most important image in this set is <strong>Figure 4.3</strong>,
as it visually explains the confounding puzzle from the first set of
slides.</p>
<ul>
<li><strong>Right Panel (Boxplots)</strong>: This shows that
<strong>students (Yes) tend to have higher credit card balances</strong>
than non-students (No). This is the source of the confounding.</li>
<li><strong>Left Panel (Default Rates)</strong>:
<ul>
<li>The <strong>dashed lines</strong> show the <em>overall</em> default
rates. The orange line (students) is higher than the blue line
(non-students). This matches our simple model
(<code>default ~ student</code>).</li>
<li>The <strong>solid S-shaped curves</strong> show the probability of
default as a function of balance. For any <em>given</em> balance, the
blue curve (non-students) is slightly higher than the orange curve
(students). This means that <strong>at the same level of debt, students
are <em>less</em> likely to default</strong>. This matches our multiple
regression model
(<code>default ~ student + balance + income</code>).</li>
</ul></li>
</ul>
<p>This single figure brilliantly illustrates how a variable can appear
to have one effect in isolation but the opposite effect when controlling
for a confounding factor. *
<strong>å³ä¾§é¢æ¿ï¼ˆç®±çº¿å›¾ï¼‰</strong>ï¼šè¿™è¡¨æ˜<strong>å­¦ç”Ÿï¼ˆæ˜¯ï¼‰çš„ä¿¡ç”¨å¡ä½™é¢å¾€å¾€é«˜äºéå­¦ç”Ÿï¼ˆå¦ï¼‰ã€‚è¿™å°±æ˜¯æ··æ‚æ•ˆåº”çš„æ ¹æºã€‚
* </strong>å·¦å›¾ï¼ˆè¿çº¦ç‡ï¼‰<strong>ï¼š *
</strong>è™šçº¿<strong>æ˜¾ç¤º<em>æ€»ä½“</em>è¿çº¦ç‡ã€‚æ©™è‰²çº¿ï¼ˆå­¦ç”Ÿï¼‰é«˜äºè“è‰²çº¿ï¼ˆéå­¦ç”Ÿï¼‰ã€‚è¿™ä¸æˆ‘ä»¬çš„ç®€å•æ¨¡å‹ï¼ˆâ€œè¿çº¦
~ å­¦ç”Ÿâ€ï¼‰ç›¸ç¬¦ã€‚ * </strong>S
å½¢å®çº¿<strong>æ˜¾ç¤ºè¿çº¦æ¦‚ç‡ä¸ä½™é¢çš„å…³ç³»ã€‚å¯¹äºä»»ä½•<em>ç»™å®š</em>çš„ä½™é¢ï¼Œè“è‰²æ›²çº¿ï¼ˆéå­¦ç”Ÿï¼‰ç•¥é«˜äºæ©™è‰²æ›²çº¿ï¼ˆå­¦ç”Ÿï¼‰ã€‚è¿™æ„å‘³ç€</strong>åœ¨ç›¸åŒçš„å€ºåŠ¡æ°´å¹³ä¸‹ï¼Œå­¦ç”Ÿè¿çº¦çš„å¯èƒ½æ€§<em>è¾ƒå°</em>ã€‚è¿™ä¸æˆ‘ä»¬çš„å¤šå…ƒå›å½’æ¨¡å‹ï¼ˆâ€œè¿çº¦
~ å­¦ç”Ÿ + ä½™é¢ + æ”¶å…¥â€ï¼‰ç›¸ç¬¦ã€‚</p>
<p>è¿™å¼ å›¾å·§å¦™åœ°è¯´æ˜äº†ä¸ºä»€ä¹ˆä¸€ä¸ªå˜é‡åœ¨å•ç‹¬ä½¿ç”¨æ—¶ä¼¼ä¹ä¼šäº§ç”Ÿä¸€ç§å½±å“ï¼Œä½†åœ¨æ§åˆ¶æ··æ‚å› ç´ åå´ä¼šäº§ç”Ÿç›¸åçš„å½±å“ã€‚</p>
<h2 id="an-important-edge-case-perfect-separation">An Important Edge
Case: Perfect Separation âš ï¸</h2>
<p>What happens if the data can be perfectly separated by a straight
line? å¦‚æœæ•°æ®å¯ä»¥ç”¨ä¸€æ¡ç›´çº¿å®Œç¾åˆ†ç¦»ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ</p>
<p>One might think this is the ideal scenario, but it causes a problem
for the logistic regression algorithm. The model will try to find
coefficients that make the probabilities for each class as close to 1
and 0 as possible. To do this, the magnitude of the coefficients (<span
class="math inline">\(\hat{\beta}\)</span>) must grow infinitely large.
äººä»¬å¯èƒ½è®¤ä¸ºè¿™æ˜¯ç†æƒ³æƒ…å†µï¼Œä½†å®ƒä¼šç»™é€»è¾‘å›å½’ç®—æ³•å¸¦æ¥é—®é¢˜ã€‚æ¨¡å‹ä¼šå°è¯•æ‰¾åˆ°ä½¿æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡å°½å¯èƒ½æ¥è¿‘
1 å’Œ 0 çš„ç³»æ•°ã€‚ä¸ºæ­¤ï¼Œç³»æ•° (<span
class="math inline">\(\hat{\beta}\)</span>) çš„å¤§å°å¿…é¡»æ— é™å¤§ã€‚</p>
<p>The slide â€œNon-convergence for perfectly separated caseâ€ demonstrates
this:</p>
<ul>
<li><p><strong>The Code</strong>: It generates two distinct,
non-overlapping clusters of data points using Pythonâ€™s
<code>scikit-learn</code>.</p></li>
<li><p><strong>Parameter Estimates Graph</strong>: It shows the
<code>Intercept</code>, <code>Coefficient 1</code>, and
<code>Coefficient 2</code> values increasing or decreasing without limit
as the algorithm runs through more iterations. They never converge to a
stable value.</p></li>
<li><p><strong>Decision Boundary Graph</strong>: The decision boundary
itself might look reasonable, but the underlying coefficients are
unstable.</p></li>
<li><p><strong>ä»£ç </strong>ï¼šå®ƒä½¿ç”¨ Python çš„ <code>scikit-learn</code>
ç”Ÿæˆä¸¤ä¸ªä¸åŒçš„ã€ä¸é‡å çš„æ•°æ®ç‚¹èšç±»ã€‚</p></li>
<li><p><strong>å‚æ•°ä¼°è®¡å›¾</strong>ï¼šå®ƒæ˜¾ç¤ºâ€œæˆªè·â€ã€â€œç³»æ•° 1â€å’Œâ€œç³»æ•°
2â€çš„å€¼éšç€ç®—æ³•è¿­ä»£æ¬¡æ•°çš„å¢åŠ æˆ–å‡å°‘è€Œæ— é™å¢å¤§æˆ–å‡å°ã€‚å®ƒä»¬æ°¸è¿œä¸ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªç¨³å®šçš„å€¼ã€‚</p></li>
<li><p><strong>å†³ç­–è¾¹ç•Œå›¾</strong>ï¼šå†³ç­–è¾¹ç•Œæœ¬èº«å¯èƒ½çœ‹èµ·æ¥åˆç†ï¼Œä½†åº•å±‚ç³»æ•°æ˜¯ä¸ç¨³å®šçš„ã€‚</p></li>
</ul>
<p><strong>Key Takeaway</strong>: If your logistic regression model
fails to converge, the first thing you should check for is perfect
separation in your training data.
<strong>å…³é”®è¦ç‚¹</strong>ï¼šå¦‚æœæ‚¨çš„é€»è¾‘å›å½’æ¨¡å‹æœªèƒ½æ”¶æ•›ï¼Œæ‚¨åº”è¯¥æ£€æŸ¥çš„ç¬¬ä¸€ä»¶äº‹å°±æ˜¯è®­ç»ƒæ•°æ®æ˜¯å¦å®Œç¾åˆ†ç¦»ã€‚</p>
<h2 id="code-understanding">Code Understanding</h2>
<p>The slides provide useful code snippets in both R and Python.</p>
<h2 id="r-code-plotting-predictions">R Code (Plotting Predictions)</h2>
<p>This code generates the plot with the two S-shaped curves (one for
students, one for non-students) showing the probability of default as
balance increases.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Create a data frame for prediction with a range of balances</span></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># One version for students, one for non-students</span></span><br><span class="line">Default.st <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>balance<span class="operator">=</span>seq<span class="punctuation">(</span><span class="number">500</span><span class="punctuation">,</span> <span class="number">2500</span><span class="punctuation">,</span> by<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> student<span class="operator">=</span><span class="string">&quot;Yes&quot;</span><span class="punctuation">)</span></span><br><span class="line">Default.nonst <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>balance<span class="operator">=</span>seq<span class="punctuation">(</span><span class="number">500</span><span class="punctuation">,</span> <span class="number">2500</span><span class="punctuation">,</span> by<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> student<span class="operator">=</span><span class="string">&quot;No&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Use the trained multiple regression model (glmod3) to predict probabilities</span></span><br><span class="line">pred.st <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>glmod3<span class="punctuation">,</span> Default.st<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line">pred.nonst <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>glmod3<span class="punctuation">,</span> Default.nonst<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Plot the results</span></span><br><span class="line">plot<span class="punctuation">(</span>Default.st<span class="operator">$</span>balance<span class="punctuation">,</span> pred.st<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;l&quot;</span><span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;red&quot;</span><span class="punctuation">,</span> ...<span class="punctuation">)</span> <span class="operator">/</span><span class="operator">/</span> Students</span><br><span class="line">lines<span class="punctuation">(</span>Default.nonst<span class="operator">$</span>balance<span class="punctuation">,</span> pred.nonst<span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;blue&quot;</span><span class="punctuation">,</span> ...<span class="punctuation">)</span> <span class="operator">/</span><span class="operator">/</span> Non<span class="operator">-</span>students</span><br></pre></td></tr></table></figure>
<h4 id="python-code-visualizing-the-decision-boundary">Python Code
(Visualizing the Decision Boundary)</h4>
<p>This Python code uses <code>scikit-learn</code> and
<code>matplotlib</code> to create the plot showing the linear decision
boundary.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Generate synthetic data with two classes</span></span><br><span class="line">X, y = make_classification(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Initialize and fit the logistic regression model</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create a mesh grid of points to make predictions over the entire plot area</span></span><br><span class="line">xx, yy = np.meshgrid(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Predict the probability for each point on the grid</span></span><br><span class="line">probs = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Plot the decision boundary where the probability is 0.5</span></span><br><span class="line">plt.contour(xx, yy, probs.reshape(xx.shape), levels=[<span class="number">0.5</span>], ...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. Scatter plot the actual data points</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, ...)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="other-important-remarks">Other Important Remarks</h3>
<p>The â€œRemarksâ€ slide briefly mentions some key extensions:</p>
<ul>
<li><p><strong>Probit Model</strong>: An alternative to logistic
regression that uses the cumulative distribution function (CDF) of the
standard normal distribution instead of the sigmoid function. The
results are often very similar.</p></li>
<li><p><strong>Softmax Regression</strong>: An extension of logistic
regression used for multi-class classification (when there are more than
two possible outcomes).</p></li>
<li><p><strong>Probit
æ¨¡å‹</strong>ï¼šé€»è¾‘å›å½’çš„æ›¿ä»£æ–¹æ³•ï¼Œå®ƒä½¿ç”¨æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
(CDF) ä»£æ›¿ S å‹å‡½æ•°ã€‚ç»“æœé€šå¸¸éå¸¸ç›¸ä¼¼ã€‚</p></li>
<li><p><strong>Softmax
å›å½’</strong>ï¼šé€»è¾‘å›å½’çš„æ‰©å±•ï¼Œç”¨äºå¤šç±»åˆ†ç±»ï¼ˆå½“å­˜åœ¨ä¸¤ä¸ªä»¥ä¸Šå¯èƒ½ç»“æœæ—¶ï¼‰ã€‚</p></li>
</ul>
<h1
id="here-is-a-summary-of-the-slides-on-linear-discriminant-analysis-lda-including-the-key-mathematical-formulas-visual-explanations-and-how-to-implement-it-in-python.">5.
Here is a summary of the slides on Linear Discriminant Analysis (LDA),
including the key mathematical formulas, visual explanations, and how to
implement it in Python.</h1>
<h2
id="the-main-idea-classification-using-probabilities-ä½¿ç”¨æ¦‚ç‡è¿›è¡Œåˆ†ç±»">The
Main Idea: Classification Using Probabilities ä½¿ç”¨æ¦‚ç‡è¿›è¡Œåˆ†ç±»</h2>
<p>Linear Discriminant Analysis (LDA) is a classification method. For a
given input <strong>x</strong>, it calculates the probability that
<strong>x</strong> belongs to each class and then assigns
<strong>x</strong> to the class with the <strong>highest
probability</strong>.</p>
<p>It does this using <strong>Bayesâ€™ Theorem</strong>, which provides a
formula for the posterior probability <span class="math inline">\(P(Y=k
| X=x)\)</span>, or the probability that the class is <span
class="math inline">\(k\)</span> given the input <span
class="math inline">\(x\)</span>. çº¿æ€§åˆ¤åˆ«åˆ†æ (LDA)
æ˜¯ä¸€ç§åˆ†ç±»æ–¹æ³•ã€‚å¯¹äºç»™å®šçš„è¾“å…¥ <strong>x</strong>ï¼Œå®ƒè®¡ç®—
<strong>x</strong> å±äºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œç„¶åå°† <strong>x</strong>
åˆ†é…ç»™<strong>æ¦‚ç‡æœ€é«˜</strong>çš„ç±»åˆ«ã€‚</p>
<p>å®ƒä½¿ç”¨<strong>è´å¶æ–¯å®šç†</strong>æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¯¥å®šç†æä¾›äº†åéªŒæ¦‚ç‡
<span class="math inline">\(P(Y=k | X=x)\)</span> çš„å…¬å¼ï¼Œå³ç»™å®šè¾“å…¥
<span class="math inline">\(x\)</span>ï¼Œè¯¥ç±»åˆ«å±äº <span
class="math inline">\(k\)</span> çš„æ¦‚ç‡ã€‚ <span class="math display">\[
p_k(x) = P(Y=k|X=x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)}
\]</span></p>
<ul>
<li><span class="math inline">\(p_k(x)\)</span> is the <strong>posterior
probability</strong> we want to maximize.</li>
<li><span class="math inline">\(\pi_k = P(Y=k)\)</span> is the
<strong>prior probability</strong> of class <span
class="math inline">\(k\)</span> (how common the class is overall).</li>
<li><span class="math inline">\(f_k(x) = f(x|Y=k)\)</span> is the
<strong>class-conditional probability density function</strong> of
observing input <span class="math inline">\(x\)</span> if it belongs to
class <span class="math inline">\(k\)</span>.</li>
</ul>
<p>To classify a new observation <span class="math inline">\(x\)</span>,
we simply find the class <span class="math inline">\(k\)</span> that
makes <span class="math inline">\(p_k(x)\)</span> the largest.
ä¸ºäº†å¯¹æ–°çš„è§‚å¯Ÿå€¼ <span class="math inline">\(x\)</span>
è¿›è¡Œåˆ†ç±»ï¼Œæˆ‘ä»¬åªéœ€æ‰¾åˆ°ä½¿ <span class="math inline">\(p_k(x)\)</span>
æœ€å¤§çš„ç±»åˆ« <span class="math inline">\(k\)</span> å³å¯ã€‚</p>
<hr />
<h2 id="key-assumptions-of-lda">Key Assumptions of LDA</h2>
<p>LDAâ€™s power comes from a specific, simplifying assumption about the
dataâ€™s distribution. LDA
çš„å¼ºå¤§ä¹‹å¤„åœ¨äºå®ƒå¯¹æ•°æ®åˆ†å¸ƒè¿›è¡Œäº†ç‰¹å®šçš„ç®€åŒ–å‡è®¾ã€‚</p>
<ol type="1">
<li><p><strong>Gaussian Distribution:</strong> LDA assumes that the data
within each class <span class="math inline">\(k\)</span> follows a
p-dimensional multivariate normal (or Gaussian) distribution, denoted as
<span class="math inline">\(X|Y=k \sim \mathcal{N}(\mu_k,
\Sigma)\)</span>.</p></li>
<li><p><strong>Common Covariance:</strong> A crucial assumption is that
all classes share the <strong>same covariance matrix</strong> <span
class="math inline">\(\Sigma\)</span>. This means that while the classes
may have different centers (means, <span
class="math inline">\(\mu_k\)</span>), their shape and orientation
(covariance, <span class="math inline">\(\Sigma\)</span>) are
identical.</p></li>
<li><p><strong>é«˜æ–¯åˆ†å¸ƒ</strong>ï¼šLDA å‡è®¾æ¯ä¸ªç±» <span
class="math inline">\(k\)</span> ä¸­çš„æ•°æ®æœä» p
ç»´å¤šå…ƒæ­£æ€ï¼ˆæˆ–é«˜æ–¯ï¼‰åˆ†å¸ƒï¼Œè¡¨ç¤ºä¸º <span class="math inline">\(X|Y=k \sim
\mathcal{N}(\mu_k, \Sigma)\)</span>ã€‚</p></li>
<li><p><strong>å…±åŒåæ–¹å·®</strong>ï¼šä¸€ä¸ªå…³é”®å‡è®¾æ˜¯æ‰€æœ‰ç±»å…±äº«<strong>ç›¸åŒçš„åæ–¹å·®çŸ©é˜µ</strong>
<span
class="math inline">\(\Sigma\)</span>ã€‚è¿™æ„å‘³ç€è™½ç„¶ç±»å¯èƒ½å…·æœ‰ä¸åŒçš„ä¸­å¿ƒï¼ˆå‡å€¼ï¼Œ<span
class="math inline">\(\mu_k\)</span>ï¼‰ï¼Œä½†å®ƒä»¬çš„å½¢çŠ¶å’Œæ–¹å‘ï¼ˆåæ–¹å·®ï¼Œ<span
class="math inline">\(\Sigma\)</span>ï¼‰æ˜¯ç›¸åŒçš„ã€‚</p></li>
</ol>
<p>The probability density function for a class <span
class="math inline">\(k\)</span> is: <span class="math display">\[
f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp \left( -\frac{1}{2}(x
- \mu_k)^T \Sigma^{-1} (x - \mu_k) \right)
\]</span></p>
<p>The image above (from your slide â€œKnowing normal distributionâ€)
illustrates this. The two â€œbellsâ€ have different centers (different
<span class="math inline">\(\mu_k\)</span>) but similar shapes. The one
on the right is â€œtilted,â€ indicating correlation between variables,
which is captured in the shared covariance matrix <span
class="math inline">\(\Sigma\)</span>.
ä¸Šå›¾ï¼ˆæ‘˜è‡ªå¹»ç¯ç‰‡â€œäº†è§£æ­£æ€åˆ†å¸ƒâ€ï¼‰è¯´æ˜äº†è¿™ä¸€ç‚¹ã€‚ä¸¤ä¸ªâ€œé’Ÿâ€å½¢çš„ä¸­å¿ƒä¸åŒï¼ˆ<span
class="math inline">\(\mu_k\)</span>
ä¸åŒï¼‰ï¼Œä½†å½¢çŠ¶ç›¸ä¼¼ã€‚å³è¾¹çš„é’Ÿå½¢â€œå€¾æ–œâ€ï¼Œè¡¨ç¤ºå˜é‡ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œè¿™ä½“ç°åœ¨å…±äº«åæ–¹å·®çŸ©é˜µ
<span class="math inline">\(\Sigma\)</span> ä¸­ã€‚</p>
<hr />
<h2 id="the-math-behind-lda-the-discriminant-function-åˆ¤åˆ«å‡½æ•°">The Math
Behind LDA: The Discriminant Function åˆ¤åˆ«å‡½æ•°</h2>
<p>Since we only need to find the class <span
class="math inline">\(k\)</span> that maximizes the posterior
probability <span class="math inline">\(p_k(x)\)</span>, we can simplify
the math. The denominator in Bayesâ€™ theorem is the same for all classes,
so we only need to maximize the numerator: <span
class="math inline">\(\pi_k f_k(x)\)</span>.
ç”±äºæˆ‘ä»¬åªéœ€è¦æ‰¾åˆ°ä½¿åéªŒæ¦‚ç‡ <span class="math inline">\(p_k(x)\)</span>
æœ€å¤§åŒ–çš„ç±»åˆ« <span
class="math inline">\(k\)</span>ï¼Œå› æ­¤å¯ä»¥ç®€åŒ–æ•°å­¦è®¡ç®—ã€‚è´å¶æ–¯å®šç†ä¸­çš„åˆ†æ¯å¯¹äºæ‰€æœ‰ç±»åˆ«éƒ½æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦æœ€å¤§åŒ–åˆ†å­ï¼š<span
class="math inline">\(\pi_k f_k(x)\)</span>ã€‚ Taking the logarithm
(which doesnâ€™t change which class is maximal) and removing constant
terms gives us the <strong>linear discriminant function</strong>, <span
class="math inline">\(\delta_k(x)\)</span>:
å–å¯¹æ•°ï¼ˆè¿™ä¸ä¼šæ”¹å˜å“ªä¸ªç±»åˆ«æ˜¯æœ€å¤§å€¼ï¼‰å¹¶ç§»é™¤å¸¸æ•°é¡¹ï¼Œå¾—åˆ°<strong>çº¿æ€§åˆ¤åˆ«å‡½æ•°</strong>ï¼Œ<span
class="math inline">\(\delta_k(x)\)</span>ï¼š</p>
<p><span class="math display">\[
\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1}
\mu_k + \log(\pi_k)
\]</span></p>
<p>This function is <strong>linear</strong> in <span
class="math inline">\(x\)</span>, which is why the method is called
<em>Linear</em> Discriminant Analysis. The decision boundary between any
two classes, say class <span class="math inline">\(k\)</span> and class
<span class="math inline">\(l\)</span>, is the set of points where <span
class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>, which defines
a linear hyperplane. è¯¥å‡½æ•°å…³äº <span class="math inline">\(x\)</span>
æ˜¯<strong>çº¿æ€§</strong>çš„ï¼Œå› æ­¤è¯¥æ–¹æ³•è¢«ç§°ä¸º<em>çº¿æ€§</em>åˆ¤åˆ«åˆ†æã€‚ä»»æ„ä¸¤ä¸ªç±»åˆ«ï¼ˆä¾‹å¦‚ç±»åˆ«
<span class="math inline">\(k\)</span> å’Œç±»åˆ« <span
class="math inline">\(l\)</span>ï¼‰ä¹‹é—´çš„å†³ç­–è¾¹ç•Œæ˜¯æ»¡è¶³ <span
class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>
çš„ç‚¹çš„é›†åˆï¼Œè¿™å®šä¹‰äº†ä¸€ä¸ªçº¿æ€§è¶…å¹³é¢ã€‚</p>
<p>The image above (from your â€œGraph of LDAâ€ slide) is very important. *
<strong>Left:</strong> The ellipses show the true 95% probability
contours for three Gaussian classes. The dashed lines are the ideal
Bayes decision boundaries, which are perfectly linear because the
assumption of common covariance holds. * <strong>Right:</strong> This
shows a sample of data points drawn from those distributions. The solid
lines are the LDA decision boundaries calculated from the sample. They
are a very good estimate of the ideal boundaries. ä¸Šå›¾ï¼ˆæ¥è‡ªæ‚¨çš„â€œLDA
å›¾â€å¹»ç¯ç‰‡ï¼‰éå¸¸é‡è¦ã€‚ *
<strong>å·¦å›¾ï¼š</strong>æ¤­åœ†æ˜¾ç¤ºäº†ä¸‰ä¸ªé«˜æ–¯ç±»åˆ«çš„çœŸå® 95%
æ¦‚ç‡è½®å»“ã€‚è™šçº¿æ˜¯ç†æƒ³çš„è´å¶æ–¯å†³ç­–è¾¹ç•Œï¼Œç”±äºå…±åŒåæ–¹å·®å‡è®¾æˆç«‹ï¼Œå› æ­¤å®ƒä»¬æ˜¯å®Œç¾çš„çº¿æ€§ã€‚
*
<strong>å³å›¾ï¼š</strong>è¿™æ˜¾ç¤ºäº†ä»è¿™äº›åˆ†å¸ƒä¸­æŠ½å–çš„æ•°æ®ç‚¹æ ·æœ¬ã€‚å®çº¿æ˜¯æ ¹æ®æ ·æœ¬è®¡ç®—å‡ºçš„
LDA å†³ç­–è¾¹ç•Œã€‚å®ƒä»¬æ˜¯å¯¹ç†æƒ³è¾¹ç•Œçš„éå¸¸å¥½çš„ä¼°è®¡ã€‚ ***</p>
<h2
id="practical-implementation-estimating-the-parameters-å®é™…åº”ç”¨ä¼°è®¡å‚æ•°">Practical
Implementation: Estimating the Parameters å®é™…åº”ç”¨ï¼šä¼°è®¡å‚æ•°</h2>
<p>In a real-world scenario, we donâ€™t know the true parameters (<span
class="math inline">\(\mu_k\)</span>, <span
class="math inline">\(\Sigma\)</span>, <span
class="math inline">\(\pi_k\)</span>). Instead, we
<strong>estimate</strong> them from our training data (<span
class="math inline">\(n\)</span> total samples, with <span
class="math inline">\(n_k\)</span> samples in class <span
class="math inline">\(k\)</span>).
åœ¨å®é™…åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬ä¸çŸ¥é“çœŸæ­£çš„å‚æ•°ï¼ˆ<span
class="math inline">\(\mu_k\)</span>ã€<span
class="math inline">\(\Sigma\)</span>ã€<span
class="math inline">\(\pi_k\)</span>ï¼‰ã€‚ç›¸åï¼Œæˆ‘ä»¬æ ¹æ®è®­ç»ƒæ•°æ®ï¼ˆ<span
class="math inline">\(n\)</span> ä¸ªæ ·æœ¬ï¼Œ<span
class="math inline">\(n_k\)</span> ä¸ªæ ·æœ¬å±äº <span
class="math inline">\(k\)</span> ç±»ï¼‰æ¥<strong>ä¼°è®¡</strong>å®ƒä»¬ã€‚</p>
<ul>
<li><strong>Prior Probability (<span
class="math inline">\(\hat{\pi}_k\)</span>):</strong> The proportion of
training samples in class <span class="math inline">\(k\)</span>. <span
class="math display">\[\hat{\pi}_k = \frac{n_k}{n}\]</span></li>
<li><strong>Class Mean (<span
class="math inline">\(\hat{\mu}_k\)</span>):</strong> The average of the
training samples in class <span class="math inline">\(k\)</span>. <span
class="math display">\[\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i=k}
x_i\]</span></li>
<li><strong>Common Covariance (<span
class="math inline">\(\hat{\Sigma}\)</span>):</strong> A weighted
average of the sample covariance matrices for each class. This is often
called the â€œpooledâ€ covariance. <span
class="math display">\[\hat{\Sigma} = \frac{1}{n-K} \sum_{k=1}^{K}
\sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T\]</span></li>
<li><strong>å…ˆéªŒæ¦‚ç‡ (<span
class="math inline">\(\hat{\pi}_k\)</span>)ï¼š</strong>è®­ç»ƒæ ·æœ¬åœ¨ <span
class="math inline">\(k\)</span> ç±»ä¸­çš„æ¯”ä¾‹ã€‚ <span
class="math display">\[\hat{\pi}_k = \frac{n_k}{n}\]</span></li>
<li><strong>ç±»åˆ«å‡å€¼ (<span
class="math inline">\(\hat{\mu}_k\)</span>)ï¼š</strong>è®­ç»ƒæ ·æœ¬åœ¨ <span
class="math inline">\(k\)</span> ç±»ä¸­çš„å¹³å‡å€¼ã€‚ <span
class="math display">\[\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i=k}
x_i\]</span></li>
<li><strong>å…¬å…±åæ–¹å·® (<span
class="math inline">\(\hat{\Sigma}\)</span>)ï¼š</strong>æ¯ä¸ªç±»çš„æ ·æœ¬åæ–¹å·®çŸ©é˜µçš„åŠ æƒå¹³å‡å€¼ã€‚è¿™é€šå¸¸è¢«ç§°ä¸ºâ€œåˆå¹¶â€åæ–¹å·®ã€‚
<span class="math display">\[\hat{\Sigma} = \frac{1}{n-K} \sum_{k=1}^{K}
\sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T\]</span></li>
</ul>
<p>We then plug these estimates into the discriminant function to get
<span class="math inline">\(\hat{\delta}_k(x)\)</span> and classify a
new observation <span class="math inline">\(x\)</span> to the class with
the largest score. ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›ä¼°è®¡å€¼ä»£å…¥åˆ¤åˆ«å‡½æ•°ï¼Œå¾—åˆ° <span
class="math inline">\(\hat{\delta}_k(x)\)</span>ï¼Œå¹¶å°†æ–°çš„è§‚æµ‹å€¼ <span
class="math inline">\(x\)</span> å½’ç±»åˆ°å¾—åˆ†æœ€é«˜çš„ç±»åˆ«ã€‚ ***</p>
<h2 id="evaluating-performance">Evaluating Performance</h2>
<p>After training the model, we evaluate its performance using a
<strong>confusion matrix</strong>.
è®­ç»ƒæ¨¡å‹åï¼Œæˆ‘ä»¬ä½¿ç”¨<strong>æ··æ·†çŸ©é˜µ</strong>æ¥è¯„ä¼°å…¶æ€§èƒ½ã€‚</p>
<p>This matrix shows the true classes versus the predicted classes. *
<strong>Diagonal elements</strong> (9644, 81) are correct predictions. *
<strong>Off-diagonal elements</strong> (23, 252) are errors.
è¯¥çŸ©é˜µæ˜¾ç¤ºäº†çœŸå®ç±»åˆ«ä¸é¢„æµ‹ç±»åˆ«çš„å¯¹æ¯”ã€‚ * <strong>å¯¹è§’çº¿å…ƒç´ </strong>
(9644, 81) è¡¨ç¤ºæ­£ç¡®é¢„æµ‹ã€‚ * <strong>éå¯¹è§’çº¿å…ƒç´ </strong> (23, 252)
è¡¨ç¤ºé”™è¯¯é¢„æµ‹ã€‚</p>
<p>From this matrix, we can calculate key metrics: * <strong>Overall
Error Rate:</strong> Total incorrect predictions / Total predictions. *
Example: <span class="math inline">\((252 + 23) / 10000 =
2.75\%\)</span> * <strong>Sensitivity (True Positive Rate):</strong>
Correctly predicted positives / Total actual positives. It answers: â€œOf
all the people who actually defaulted, what fraction did we catch?â€ *
Example: <span class="math inline">\(81 / 333 = 24.3\%\)</span>. The
sensitivity is <span class="math inline">\(1 - 75.7\% = 24.3\%\)</span>.
* <strong>Specificity (True Negative Rate):</strong> Correctly predicted
negatives / Total actual negatives. It answers: â€œOf all the people who
did not default, what fraction did we correctly identify?â€ * Example:
<span class="math inline">\(9644 / 9667 = 99.8\%\)</span>. The
specificity is <span class="math inline">\(1 - 0.24\% =
99.8\%\)</span>.</p>
<p>The example in your slides shows a high error rate for â€œdefaultâ€
people (75.7%) because the classes are <strong>unbalanced</strong>â€”there
are far fewer defaulters. This highlights the importance of looking at
class-specific metrics, not just the overall error rate.</p>
<hr />
<h2 id="python-code-understanding">Python Code Understanding</h2>
<p>In Python, you can easily implement LDA using the
<code>scikit-learn</code> library. The code conceptually mirrors the
steps we discussed.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume you have your data X (features) and y (labels)</span></span><br><span class="line"><span class="comment"># X = features (e.g., balance, income)</span></span><br><span class="line"><span class="comment"># y = labels (e.g., 0 for &#x27;no-default&#x27;, 1 for &#x27;default&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create an instance of the LDA model</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit the model to the training data</span></span><br><span class="line"><span class="comment"># This is where the model calculates the estimates:</span></span><br><span class="line"><span class="comment">#  - Prior probabilities (pi_k)</span></span><br><span class="line"><span class="comment">#  - Class means (mu_k)</span></span><br><span class="line"><span class="comment">#  - Pooled covariance matrix (Sigma)</span></span><br><span class="line">lda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Make predictions on new, unseen data</span></span><br><span class="line">predictions = lda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Evaluate the model&#x27;s performance</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, predictions))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, predictions))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>LinearDiscriminantAnalysis()</code> creates the classifier
object.</li>
<li><code>lda.fit(X_train, y_train)</code> is the core training step
where the model learns the <span
class="math inline">\(\hat{\pi}_k\)</span>, <span
class="math inline">\(\hat{\mu}_k\)</span>, and <span
class="math inline">\(\hat{\Sigma}\)</span> parameters from the
data.</li>
<li><code>lda.predict(X_test)</code> uses the learned discriminant
function <span class="math inline">\(\hat{\delta}_k(x)\)</span> to
classify each sample in the test set.</li>
<li><code>confusion_matrix</code> and <code>classification_report</code>
are tools to evaluate the results, just like in the slides.</li>
</ul>
<h1
id="here-is-a-summary-of-the-provided-slides-on-linear-discriminant-analysis-lda-focusing-on-mathematical-concepts-python-code-interpretation-and-key-visuals.">6.
Here is a summary of the provided slides on Linear Discriminant Analysis
(LDA), focusing on mathematical concepts, Python code interpretation,
and key visuals.</h1>
<h2 id="core-concept-lda-for-classification">Core Concept: LDA for
Classification</h2>
<p>Linear Discriminant Analysis (LDA) is a classification method that
models the probability that an observation belongs to a certain class.
It works by finding a linear combination of features that best separates
two or more classes.</p>
<p>The decision is based on <strong>Bayesâ€™ theorem</strong>. For a given
observation with features <span class="math inline">\(X=x\)</span>, LDA
calculates the <strong>posterior probability</strong>, <span
class="math inline">\(p_k(x) = Pr(Y=k|X=x)\)</span>, for each class
<span class="math inline">\(k\)</span>. This is the probability that the
observation belongs to class <span class="math inline">\(k\)</span>
given its features. çº¿æ€§åˆ¤åˆ«åˆ†æ (LDA)
æ˜¯ä¸€ç§åˆ†ç±»æ–¹æ³•ï¼Œå®ƒå¯¹è§‚æµ‹å€¼å±äºæŸä¸ªç±»åˆ«çš„æ¦‚ç‡è¿›è¡Œå»ºæ¨¡ã€‚å®ƒçš„å·¥ä½œåŸç†æ˜¯æ‰¾åˆ°èƒ½å¤Ÿæœ€å¥½åœ°åŒºåˆ†ä¸¤ä¸ªæˆ–å¤šä¸ªç±»åˆ«çš„ç‰¹å¾çš„çº¿æ€§ç»„åˆã€‚</p>
<p>è¯¥å†³ç­–åŸºäº<strong>è´å¶æ–¯å®šç†</strong>ã€‚å¯¹äºç‰¹å¾ä¸º <span
class="math inline">\(X=x\)</span> çš„ç»™å®šè§‚æµ‹å€¼ï¼ŒLDA ä¼šè®¡ç®—æ¯ä¸ªç±»åˆ«
<span class="math inline">\(k\)</span>
çš„<strong>åéªŒæ¦‚ç‡</strong>ï¼Œ<span class="math inline">\(p_k(x) =
Pr(Y=k|X=x)\)</span>ã€‚è¿™æ˜¯ç»™å®šè§‚æµ‹å€¼çš„ç‰¹å¾åï¼Œè¯¥è§‚æµ‹å€¼å±äºç±»åˆ« <span
class="math inline">\(k\)</span> çš„æ¦‚ç‡ã€‚</p>
<p>By default, the Bayes classifier assigns an observation to the class
with the highest posterior probability. For a binary (two-class) problem
like â€˜Yesâ€™ vs.Â â€˜Noâ€™, this means:
é»˜è®¤æƒ…å†µä¸‹ï¼Œè´å¶æ–¯åˆ†ç±»å™¨å°†è§‚æµ‹å€¼åˆ†é…ç»™åéªŒæ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ã€‚å¯¹äºåƒâ€œæ˜¯â€ä¸â€œå¦â€è¿™æ ·çš„äºŒåˆ†ç±»é—®é¢˜ï¼Œè¿™æ„å‘³ç€ï¼š</p>
<ul>
<li>Assign to â€˜Yesâ€™ if <span class="math inline">\(Pr(Y=\text{Yes}|X=x)
&gt; 0.5\)</span></li>
<li>Assign to â€˜Noâ€™ otherwise</li>
</ul>
<h2 id="modifying-the-decision-threshold">Modifying the Decision
Threshold</h2>
<p>The default 0.5 threshold isnâ€™t always optimal. In many real-world
scenarios, the cost of one type of error is much higher than another.
For example, in credit card default prediction: é»˜è®¤çš„ 0.5
é˜ˆå€¼å¹¶éæ€»æ˜¯æœ€ä¼˜çš„ã€‚åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­ï¼Œä¸€ç§é”™è¯¯çš„ä»£ä»·è¿œé«˜äºå¦ä¸€ç§ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¿¡ç”¨å¡è¿çº¦é¢„æµ‹ä¸­ï¼š</p>
<ul>
<li><strong>False Negative:</strong> Incorrectly classifying a person
who will default as someone who wonâ€™t. (The bank loses money).</li>
<li><strong>False Positive:</strong> Incorrectly classifying a person
who wonâ€™t default as someone who will. (The bank loses a potential
customer).</li>
</ul>
<p>A bank might decide that missing a defaulter is much worse than
denying a good customer. To catch more potential defaulters, they can
<strong>lower the probability threshold</strong>.
é“¶è¡Œå¯èƒ½ä¼šè®¤ä¸ºé”™è¿‡ä¸€ä¸ªè¿çº¦è€…æ¯”æ‹’ç»ä¸€ä¸ªä¼˜è´¨å®¢æˆ·æ›´ç³Ÿç³•ã€‚ä¸ºäº†æ•æ‰æ›´å¤šæ½œåœ¨çš„è¿çº¦è€…ï¼Œä»–ä»¬å¯ä»¥<strong>é™ä½æ¦‚ç‡é˜ˆå€¼</strong>ã€‚</p>
<p>A modified rule could be: <span class="math display">\[
Pr(\text{default}=\text{Yes}|X=x) &gt; 0.2
\]</span> This makes the model more â€œsensitiveâ€ to flagging potential
defaulters, even at the cost of misclassifying more non-defaulters.
é™ä½é˜ˆå€¼<strong>ä¼šæé«˜æ•æ„Ÿåº¦</strong>ï¼Œä½†<strong>ä¼šé™ä½ç‰¹å¼‚æ€§</strong>ã€‚</p>
<p>This decision leads to a <strong>trade-off</strong> between two key
performance metrics: * <strong>Sensitivity (True Positive
Rate):</strong> The ability to correctly identify positive cases. (e.g.,
<code>Correctly identified defaulters / Total actual defaulters</code>).
* <strong>Specificity (True Negative Rate):</strong> The ability to
correctly identify negative cases. (e.g.,
<code>Correctly identified non-defaulters / Total actual non-defaulters</code>).</p>
<p>è¿™ä¸€å†³ç­–ä¼šå¯¼è‡´ä¸¤ä¸ªå…³é”®ç»©æ•ˆæŒ‡æ ‡ä¹‹é—´çš„<strong>æƒè¡¡</strong>ï¼š *
<strong>æ•æ„Ÿåº¦ï¼ˆçœŸé˜³æ€§ç‡ï¼‰ï¼š</strong>æ­£ç¡®è¯†åˆ«é˜³æ€§æ¡ˆä¾‹çš„èƒ½åŠ›ã€‚ï¼ˆä¾‹å¦‚ï¼Œâ€œæ­£ç¡®è¯†åˆ«çš„è¿çº¦è€…/å®é™…è¿çº¦è€…æ€»æ•°â€ï¼‰ã€‚
*
<strong>ç‰¹å¼‚æ€§ï¼ˆçœŸé˜´æ€§ç‡ï¼‰ï¼š</strong>æ­£ç¡®è¯†åˆ«é˜´æ€§æ¡ˆä¾‹çš„èƒ½åŠ›ã€‚ï¼ˆä¾‹å¦‚ï¼Œâ€œæ­£ç¡®è¯†åˆ«çš„éè¿çº¦è€…/å®é™…éè¿çº¦è€…æ€»æ•°â€ï¼‰ã€‚</p>
<p>Lowering the threshold <strong>increases sensitivity</strong> but
<strong>decreases specificity</strong>. ## Python Code Explained</p>
<p>The slides show how to implement and adjust LDA using Pythonâ€™s
<code>scikit-learn</code> library.</p>
<h2 id="basic-lda-implementation">Basic LDA Implementation</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the necessary library</span></span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize and train the LDA model</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line">lda_train = lda.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions using the default 0.5 threshold</span></span><br><span class="line">y_pred = lda.predict(X)</span><br></pre></td></tr></table></figure>
<p>This code trains an LDA model and makes predictions using the
standard 50% probability boundary.</p>
<h2 id="adjusting-the-prediction-threshold">Adjusting the Prediction
Threshold</h2>
<p>To use a custom threshold (e.g., 0.2), you donâ€™t use the
<code>.predict()</code> method. Instead, you get the class probabilities
with <code>.predict_proba()</code> and apply the threshold manually.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Get the probabilities for each class</span></span><br><span class="line"><span class="comment"># lda.predict_proba(X) returns an array like [[P(No), P(Yes)], ...]</span></span><br><span class="line"><span class="comment"># We select the second column [:, 1] for the &#x27;Yes&#x27; class probability</span></span><br><span class="line">lda_probs = lda.predict_proba(X)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Define a custom threshold</span></span><br><span class="line">threshold = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Apply the threshold to get new predictions</span></span><br><span class="line"><span class="comment"># This creates a boolean array (True where prob &gt; 0.2, else False)</span></span><br><span class="line"><span class="comment"># We then convert True/False to &#x27;Yes&#x27;/&#x27;No&#x27; labels</span></span><br><span class="line">lda_pred1 = np.where(lda_probs &gt; threshold, <span class="string">&quot;Yes&quot;</span>, <span class="string">&quot;No&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>This is the core technique for tuning the classifierâ€™s behavior to
meet specific business needs, as demonstrated on slides 55 and 56 for
both LDA and Logistic Regression.</p>
<h2 id="important-images-to-understand">Important Images to
Understand</h2>
<ol type="1">
<li><strong>Confusion Matrix (Slide 49):</strong> This table is crucial.
It breaks down the modelâ€™s predictions into True Positives, True
Negatives, False Positives, and False Negatives. All key metrics like
error rate, sensitivity, and specificity are calculated from this
matrix. <strong>æ··æ·†çŸ©é˜µï¼ˆå¹»ç¯ç‰‡
49ï¼‰ï¼š</strong>è¿™å¼ è¡¨è‡³å…³é‡è¦ã€‚å®ƒå°†æ¨¡å‹çš„é¢„æµ‹åˆ†è§£ä¸ºçœŸé˜³æ€§ã€çœŸé˜´æ€§ã€å‡é˜³æ€§å’Œå‡é˜´æ€§ã€‚æ‰€æœ‰å…³é”®æŒ‡æ ‡ï¼Œä¾‹å¦‚é”™è¯¯ç‡ã€çµæ•åº¦å’Œç‰¹å¼‚æ€§ï¼Œéƒ½åŸºäºæ­¤çŸ©é˜µè®¡ç®—å¾—å‡ºã€‚</li>
<li><strong>LDA Decision Boundaries (Slide 51):</strong> This plot
provides a powerful visual intuition. It shows the data points for two
classes and the decision boundary line. The different parallel lines
show how changing the threshold from 0.5 to 0.1 or 0.9 shifts the
boundary, making the model classify more or fewer points into the
minority class. <strong>LDA å†³ç­–è¾¹ç•Œï¼ˆå¹»ç¯ç‰‡
51ï¼‰ï¼š</strong>è¿™å¼ å›¾æä¾›äº†å¼ºå¤§çš„è§†è§‰ç›´è§‚æ€§ã€‚å®ƒå±•ç¤ºäº†ä¸¤ä¸ªç±»åˆ«çš„æ•°æ®ç‚¹å’Œå†³ç­–è¾¹ç•Œçº¿ã€‚ä¸åŒçš„å¹³è¡Œçº¿æ˜¾ç¤ºäº†å°†é˜ˆå€¼ä»
0.5 æ›´æ”¹ä¸º 0.1 æˆ– 0.9
æ—¶è¾¹ç•Œå¦‚ä½•ç§»åŠ¨ï¼Œä»è€Œä½¿æ¨¡å‹å°†æ›´å¤šæˆ–æ›´å°‘çš„ç‚¹å½’å…¥å°‘æ•°ç±»</li>
<li><strong>Error Rate Tradeoff Curve (Slide 53):</strong> This graph is
the most important for understanding the business implication of
changing the threshold. It clearly shows that as the threshold changes,
the error rate for one class goes down while the error rate for the
other goes up. The overall error is minimized at a certain point, but
that may not be the optimal point from a business perspective.
<strong>é”™è¯¯ç‡æƒè¡¡æ›²çº¿ï¼ˆå¹»ç¯ç‰‡
53ï¼‰ï¼š</strong>è¿™å¼ å›¾å¯¹äºç†è§£æ›´æ”¹é˜ˆå€¼çš„ä¸šåŠ¡å«ä¹‰è‡³å…³é‡è¦ã€‚å®ƒæ¸…æ¥šåœ°è¡¨æ˜ï¼Œéšç€é˜ˆå€¼çš„å˜åŒ–ï¼Œä¸€ä¸ªç±»åˆ«çš„é”™è¯¯ç‡ä¸‹é™ï¼Œè€Œå¦ä¸€ä¸ªç±»åˆ«çš„é”™è¯¯ç‡ä¸Šå‡ã€‚æ€»ä½“è¯¯å·®åœ¨æŸä¸ªç‚¹è¾¾åˆ°æœ€å°ï¼Œä½†ä»ä¸šåŠ¡è§’åº¦æ¥çœ‹ï¼Œè¿™å¯èƒ½å¹¶éæœ€ä½³ç‚¹ã€‚</li>
<li><strong>ROC Curve (Slides 54 &amp; 55):</strong> The Receiver
Operating Characteristic (ROC) curve plots Sensitivity vs.Â (1 -
Specificity) for <em>all possible thresholds</em>. An ideal classifier
has a curve that â€œhugsâ€ the top-left corner, indicating high sensitivity
and high specificity. Itâ€™s a standard way to visualize and compare the
overall performance of different classifiers. <strong>ROC æ›²çº¿ï¼ˆå¹»ç¯ç‰‡
54 å’Œ 55ï¼‰ï¼š</strong> æ¥æ”¶è€…æ“ä½œç‰¹æ€§ (ROC)
æ›²çº¿ç»˜åˆ¶äº†<em>æ‰€æœ‰å¯èƒ½é˜ˆå€¼</em>çš„çµæ•åº¦ä¸ï¼ˆ1 -
ç‰¹å¼‚æ€§ï¼‰çš„å…³ç³»ã€‚ç†æƒ³çš„åˆ†ç±»å™¨æ›²çº¿â€œç´§è´´â€å·¦ä¸Šè§’ï¼Œè¡¨ç¤ºé«˜çµæ•åº¦å’Œé«˜ç‰¹å¼‚æ€§ã€‚è¿™æ˜¯å¯è§†åŒ–å’Œæ¯”è¾ƒä¸åŒåˆ†ç±»å™¨æ•´ä½“æ€§èƒ½çš„æ ‡å‡†æ–¹æ³•ã€‚</li>
</ol>
<h1
id="here-is-a-summary-of-the-provided-slides-on-linear-and-quadratic-discriminant-analysis-including-the-key-formulas-python-code-equivalents-and-explanations-of-the-important-concepts.">7.
Here is a summary of the provided slides on Linear and Quadratic
Discriminant Analysis, including the key formulas, Python code
equivalents, and explanations of the important concepts.</h1>
<h2 id="key-goal-classification"><strong>Key Goal:
Classification</strong></h2>
<p>Both <strong>Linear Discriminant Analysis (LDA)</strong> and
<strong>Quadratic Discriminant Analysis (QDA)</strong> are
classification algorithms. Their main goal is to find a decision
boundary to separate different classes (e.g., â€œdefaultâ€ vs.Â â€œnot
defaultâ€) in the data. <strong>çº¿æ€§åˆ¤åˆ«åˆ†æ (LDA)</strong> å’Œ
<strong>äºŒæ¬¡åˆ¤åˆ«åˆ†æ (QDA)</strong>
éƒ½æ˜¯åˆ†ç±»ç®—æ³•ã€‚å®ƒä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªå†³ç­–è¾¹ç•Œæ¥åŒºåˆ†æ•°æ®ä¸­çš„ä¸åŒç±»åˆ«ï¼ˆä¾‹å¦‚ï¼Œâ€œé»˜è®¤â€ä¸â€œéé»˜è®¤â€ï¼‰ã€‚</p>
<h3 id="linear-discriminant-analysis-lda">## Linear Discriminant
Analysis (LDA)</h3>
<p>LDA creates a <strong>linear</strong> decision boundary between
classes. LDA åœ¨ç±»åˆ«ä¹‹é—´åˆ›å»º<strong>çº¿æ€§</strong>å†³ç­–è¾¹ç•Œã€‚</p>
<h4 id="core-idea-fishers-interpretation"><strong>Core Idea (Fisherâ€™s
Interpretation)</strong></h4>
<p>Imagine you have data points for different classes in a 3D space.
Fisherâ€™s idea is to find the best angle to shine a â€œflashlightâ€ on the
data to project its shadow onto a 2D wall (or a 1D line). The â€œbestâ€
projection is the one where the shadows of the different classes are
<strong>as far apart from each other as possible</strong>, while the
shadows within each class are <strong>as tightly packed as
possible</strong>. æƒ³è±¡ä¸€ä¸‹ï¼Œä½ åœ¨ä¸‰ç»´ç©ºé—´ä¸­æ‹¥æœ‰ä¸åŒç±»åˆ«çš„æ•°æ®ç‚¹ã€‚Fisher
çš„æ€æƒ³æ˜¯æ‰¾åˆ°æœ€ä½³è§’åº¦ï¼Œç”¨â€œæ‰‹ç”µç­’â€ç…§å°„æ•°æ®ï¼Œå°†å…¶é˜´å½±æŠ•å°„åˆ°äºŒç»´å¢™å£ï¼ˆæˆ–ä¸€ç»´çº¿ä¸Šï¼‰ã€‚
â€œæœ€ä½³â€æŠ•å½±æ˜¯ä¸åŒç±»åˆ«çš„é˜´å½±<strong>å½¼æ­¤ä¹‹é—´å°½å¯èƒ½è¿œ</strong>ï¼Œè€Œæ¯ä¸ªç±»åˆ«å†…çš„é˜´å½±<strong>å°½å¯èƒ½ç´§å¯†</strong>çš„æŠ•å½±ã€‚</p>
<ul>
<li><strong>Maximize:</strong> The distance between the means of the
projected classes (Between-Class Variance).
æŠ•å½±ç±»åˆ«å‡å€¼ä¹‹é—´çš„è·ç¦»ï¼ˆç±»é—´æ–¹å·®ï¼‰ã€‚</li>
<li><strong>Minimize:</strong> The spread or variance within each
projected class (Within-Class Variance).
æ¯ä¸ªæŠ•å½±ç±»åˆ«å†…çš„æ‰©æ•£æˆ–æ–¹å·®ï¼ˆç±»å†…æ–¹å·®ï¼‰ã€‚ This is the most important
image for understanding the intuition behind LDA. It shows how
projecting the data onto a specific line (defined by vector
<code>w</code>) can make the two classes clearly separable.
è¿™æ˜¯ç†è§£LDAèƒŒåç›´è§‰çš„æœ€é‡è¦å›¾åƒã€‚å®ƒå±•ç¤ºäº†å¦‚ä½•å°†æ•°æ®æŠ•å½±åˆ°ç‰¹å®šç›´çº¿ï¼ˆç”±å‘é‡â€œwâ€å®šä¹‰ï¼‰ä¸Šï¼Œä»è€Œä½¿ä¸¤ä¸ªç±»åˆ«æ¸…æ™°å¯åˆ†ã€‚</li>
</ul>
<h4 id="key-mathematical-formulas"><strong>Key Mathematical
Formulas</strong></h4>
<p>To achieve this, LDA maximizes a ratio called the <strong>Rayleigh
quotient</strong>. LDAæœ€å¤§åŒ–ä¸€ä¸ªç§°ä¸º<strong>ç‘åˆ©å•†</strong>çš„æ¯”ç‡ã€‚</p>
<ol type="1">
<li><strong>Within-Class Covariance (<span
class="math inline">\(\hat{\Sigma}_W\)</span>)</strong>: Measures the
spread of data <em>inside</em> each class. <strong>ç±»å†…åæ–¹å·® (<span
class="math inline">\(\hat{\Sigma}_W\)</span>)</strong>ï¼šè¡¡é‡æ¯ä¸ªç±»åˆ«<em>å†…éƒ¨</em>æ•°æ®çš„æ‰©æ•£ç¨‹åº¦ã€‚
<span class="math display">\[\hat{\Sigma}_W = \frac{1}{n-K}
\sum_{k=1}^{K} \sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i -
\hat{\mu}_k)^\top\]</span></li>
<li><strong>Between-Class Covariance (<span
class="math inline">\(\hat{\Sigma}_B\)</span>)</strong>: Measures the
spread <em>between</em> the means of different classes.
<strong>ç±»é—´åæ–¹å·® (<span
class="math inline">\(\hat{\Sigma}_B\)</span>)</strong>ï¼šè¡¡é‡ä¸åŒç±»åˆ«å‡å€¼<em>ä¹‹é—´</em>çš„å·®å¼‚ã€‚
<span class="math display">\[\hat{\Sigma}_B = \sum_{k=1}^{K} n_k
(\hat{\mu}_k - \hat{\mu})(\hat{\mu}_k - \hat{\mu})^\top\]</span></li>
<li><strong>Objective Function</strong>: Find the projection vector
<span class="math inline">\(w\)</span> that maximizes the ratio of
between-class variance to within-class variance.
<strong>ç›®æ ‡å‡½æ•°</strong>ï¼šæ‰¾åˆ°æŠ•å½±å‘é‡ <span
class="math inline">\(w\)</span>ï¼Œä½¿ç±»é—´æ–¹å·®ä¸ç±»å†…æ–¹å·®ä¹‹æ¯”æœ€å¤§åŒ–ã€‚ <span
class="math display">\[\max_w \frac{w^\top \hat{\Sigma}_B w}{w^\top
\hat{\Sigma}_W w}\]</span></li>
</ol>
<h4 id="ldas-main-assumption"><strong>LDAâ€™s Main
Assumption</strong></h4>
<p>The key assumption of LDA is that all classes share the <strong>same
covariance matrix (<span
class="math inline">\(\Sigma\)</span>)</strong>. They can have different
means (<span class="math inline">\(\mu_k\)</span>), but their spread and
orientation must be identical. This assumption is what results in a
linear decision boundary. LDA
çš„å…³é”®å‡è®¾æ˜¯æ‰€æœ‰ç±»åˆ«å…±äº«<strong>ç›¸åŒçš„åæ–¹å·®çŸ©é˜µ (<span
class="math inline">\(\Sigma\)</span>)</strong>ã€‚å®ƒä»¬å¯ä»¥å…·æœ‰ä¸åŒçš„å‡å€¼
(<span
class="math inline">\(\mu_k\)</span>)ï¼Œä½†å®ƒä»¬çš„æ•£åº¦å’Œæ–¹å‘å¿…é¡»ç›¸åŒã€‚æ­£æ˜¯è¿™ä¸€å‡è®¾å¯¼è‡´äº†çº¿æ€§å†³ç­–è¾¹ç•Œã€‚</p>
<h3 id="quadratic-discriminant-analysis-qda">## Quadratic Discriminant
Analysis (QDA)</h3>
<p>QDA is a more flexible extension of LDA that creates a
<strong>quadratic</strong> (curved) decision boundary. QDA æ˜¯ LDA
çš„æ›´çµæ´»çš„æ‰©å±•ï¼Œå®ƒåˆ›å»ºäº†<strong>äºŒæ¬¡</strong>ï¼ˆæ›²çº¿ï¼‰å†³ç­–è¾¹ç•Œã€‚ ####
<strong>Core Idea &amp; Key Assumption</strong></p>
<p>QDA starts with the same principles as LDA but drops the key
assumption. QDA assumes that <strong>each class has its own unique
covariance matrix (<span
class="math inline">\(\Sigma_k\)</span>)</strong>. QDA çš„åŸç†ä¸ LDA
ç›¸åŒï¼Œä½†æ”¾å¼ƒäº†å…³é”®å‡è®¾ã€‚QDA å‡è®¾<strong>æ¯ä¸ªç±»åˆ«éƒ½æœ‰è‡ªå·±ç‹¬ç‰¹çš„åæ–¹å·®çŸ©é˜µ
(<span class="math inline">\(\Sigma_k\)</span>)</strong>ã€‚</p>
<p>This means each class can have its own spread, shape, and
orientation. This additional flexibility allows for a more complex,
curved decision boundary.
è¿™æ„å‘³ç€æ¯ä¸ªç±»åˆ«å¯ä»¥æ‹¥æœ‰è‡ªå·±çš„æ•£åº¦ã€å½¢çŠ¶å’Œæ–¹å‘ã€‚è¿™ç§é¢å¤–çš„çµæ´»æ€§ä½¿å¾—å†³ç­–è¾¹ç•Œæ›´åŠ å¤æ‚ã€æ›²çº¿åŒ–ã€‚</p>
<h4 id="key-mathematical-formula"><strong>Key Mathematical
Formula</strong></h4>
<p>The classification is made using a discrimination function, <span
class="math inline">\(\delta_k(x)\)</span>. We assign a data point <span
class="math inline">\(x\)</span> to the class <span
class="math inline">\(k\)</span> for which <span
class="math inline">\(\delta_k(x)\)</span> is largest. The function for
QDA is: <span class="math display">\[\delta_k(x) = -\frac{1}{2}(x -
\mu_k)^\top \Sigma_k^{-1}(x - \mu_k) - \frac{1}{2}\log(|\Sigma_k|) +
\log \pi_k\]</span> The term containing <span
class="math inline">\(x^\top \Sigma_k^{-1} x\)</span> makes this
function a <strong>quadratic</strong> function of <span
class="math inline">\(x\)</span>.</p>
<h3 id="lda-vs.-qda-the-trade-off">## LDA vs.Â QDA: The Trade-Off</h3>
<p>The choice between LDA and QDA is a classic <strong>bias-variance
trade-off</strong>. åœ¨ LDA å’Œ QDA
ä¹‹é—´è¿›è¡Œé€‰æ‹©æ˜¯å…¸å‹çš„<strong>åå·®-æ–¹å·®æƒè¡¡</strong>ã€‚</p>
<ul>
<li><p><strong>Use LDA when:</strong></p>
<ul>
<li>The assumption of a common covariance matrix is reasonable (the
classes have similar shapes).</li>
<li>You have a small amount of training data, as LDA is less prone to
overfitting.</li>
<li>Simplicity is preferred. LDA is less flexible (high bias) but has
lower variance.</li>
<li>å‡è®¾å…±åŒåæ–¹å·®çŸ©é˜µæ˜¯åˆç†çš„ï¼ˆç±»åˆ«å…·æœ‰ç›¸ä¼¼çš„å½¢çŠ¶ï¼‰ã€‚</li>
<li>è®­ç»ƒæ•°æ®é‡è¾ƒå°‘ï¼Œå› ä¸º LDA ä¸æ˜“è¿‡æ‹Ÿåˆã€‚</li>
<li>ç®€æ´æ˜¯é¦–é€‰ã€‚LDA çµæ´»æ€§è¾ƒå·®ï¼ˆåå·®è¾ƒå¤§ï¼‰ï¼Œä½†æ–¹å·®è¾ƒå°ã€‚</li>
</ul></li>
<li><p><strong>Use QDA when:</strong></p>
<ul>
<li>The classes have clearly different shapes and spreads (different
covariance matrices).</li>
<li>You have a large amount of training data to properly estimate the
separate covariance matrices for each class.</li>
<li>QDA is more flexible (low bias) but can have high variance, meaning
it might overfit on smaller datasets.</li>
<li>ç±»åˆ«å…·æœ‰æ˜æ˜¾ä¸åŒçš„å½¢çŠ¶å’Œåˆ†å¸ƒï¼ˆä¸åŒçš„åæ–¹å·®çŸ©é˜µï¼‰ã€‚</li>
<li>æ‹¥æœ‰å¤§é‡è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥æ­£ç¡®ä¼°è®¡æ¯ä¸ªç±»åˆ«çš„ç‹¬ç«‹åæ–¹å·®çŸ©é˜µã€‚</li>
<li>QDA
æ›´çµæ´»ï¼ˆåå·®è¾ƒå°ï¼‰ï¼Œä½†æ–¹å·®è¾ƒå¤§ï¼Œè¿™æ„å‘³ç€å®ƒå¯èƒ½åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆã€‚
<strong>Rule of Thumb:</strong> If the class variances are equal or
close, LDA is better. Otherwise, QDA is better.
<strong>ç»éªŒæ³•åˆ™ï¼š</strong> å¦‚æœç±»åˆ«æ–¹å·®ç›¸ç­‰æˆ–æ¥è¿‘ï¼Œåˆ™ LDA
æ›´ä½³ã€‚å¦åˆ™ï¼ŒQDA æ›´å¥½ã€‚</li>
</ul></li>
</ul>
<h3 id="code-understanding-python-equivalent">## Code Understanding
(Python Equivalent)</h3>
<p>The slides show code in R. Hereâ€™s how you would perform LDA and
evaluate it in Python using the popular <code>scikit-learn</code>
library.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, accuracy_score, roc_curve, auc</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;df&#x27; is your DataFrame with features and a &#x27;target&#x27; column</span></span><br><span class="line"><span class="comment"># X = df.drop(&#x27;target&#x27;, axis=1)</span></span><br><span class="line"><span class="comment"># y = df[&#x27;target&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit an LDA model (equivalent to lda() in R)</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line">lda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Make predictions (equivalent to predict() in R)</span></span><br><span class="line">y_pred_lda = lda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># To fit a QDA model, the process is identical:</span></span><br><span class="line"><span class="comment"># qda = QuadraticDiscriminantAnalysis()</span></span><br><span class="line"><span class="comment"># qda.fit(X_train, y_train)</span></span><br><span class="line"><span class="comment"># y_pred_qda = qda.predict(X_test)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Create a confusion matrix (equivalent to table())</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LDA Confusion Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, y_pred_lda))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Plot the ROC Curve (equivalent to the R code for ROC)</span></span><br><span class="line"><span class="comment"># Get prediction probabilities for the positive class</span></span><br><span class="line">y_pred_proba = lda.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate ROC curve points</span></span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate Area Under the Curve (AUC)</span></span><br><span class="line">roc_auc = auc(fpr, tpr)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(fpr, tpr, color=<span class="string">&#x27;blue&#x27;</span>, lw=<span class="number">2</span>, label=<span class="string">f&#x27;LDA ROC curve (area = <span class="subst">&#123;roc_auc:<span class="number">.2</span>f&#125;</span>)&#x27;</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">&#x27;gray&#x27;</span>, lw=<span class="number">2</span>, linestyle=<span class="string">&#x27;--&#x27;</span>) <span class="comment"># Random guess line</span></span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate (1 - Specificity)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate (Sensitivity)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Receiver Operating Characteristic (ROC) Curve&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="understanding-the-roc-curve"><strong>Understanding the ROC
Curve</strong></h4>
<p>The <strong>ROC Curve</strong> is another important image. It helps
you visualize a classifierâ€™s performance across all possible
classification thresholds. <strong>ROC æ›²çº¿</strong>
æ˜¯å¦ä¸€ä¸ªé‡è¦çš„å›¾åƒã€‚å®ƒå¯ä»¥å¸®åŠ©æ‚¨ç›´è§‚åœ°äº†è§£åˆ†ç±»å™¨åœ¨æ‰€æœ‰å¯èƒ½çš„åˆ†ç±»é˜ˆå€¼ä¸‹çš„æ€§èƒ½ã€‚</p>
<ul>
<li>The <strong>Y-axis</strong> is the <strong>True Positive
Rate</strong> (Sensitivity): â€œOf all the actual positives, how many did
we correctly identify?â€</li>
<li>The <strong>X-axis</strong> is the <strong>False Positive
Rate</strong>: â€œOf all the actual negatives, how many did we incorrectly
label as positive?â€</li>
<li>A perfect classifier would have a curve that goes straight up to the
top-left corner (100% TPR, 0% FPR). The diagonal line represents a
random guess. The <strong>Area Under the Curve (AUC)</strong> summarizes
the modelâ€™s performance; a value closer to 1.0 is better.</li>
<li><strong>Y è½´</strong>
è¡¨ç¤º<strong>çœŸé˜³æ€§ç‡</strong>ï¼ˆæ•æ„Ÿåº¦ï¼‰ï¼šâ€œåœ¨æ‰€æœ‰å®é™…çš„é˜³æ€§æ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬æ­£ç¡®è¯†åˆ«äº†å¤šå°‘ä¸ªï¼Ÿâ€</li>
<li><strong>X è½´</strong>
è¡¨ç¤º<strong>å‡é˜³æ€§ç‡</strong>ï¼šâ€œåœ¨æ‰€æœ‰å®é™…çš„é˜´æ€§æ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬é”™è¯¯åœ°å°†å¤šå°‘ä¸ªæ ‡è®°ä¸ºé˜³æ€§ï¼Ÿâ€</li>
<li>ä¸€ä¸ªå®Œç¾çš„åˆ†ç±»å™¨åº”è¯¥æœ‰ä¸€æ¡ç›´çº¿ä¸Šå‡åˆ°å·¦ä¸Šè§’çš„æ›²çº¿ï¼ˆçœŸé˜³æ€§ç‡
100%ï¼Œå‡é˜³æ€§ç‡ 0%ï¼‰ã€‚å¯¹è§’çº¿è¡¨ç¤ºéšæœºçŒœæµ‹ã€‚<strong>æ›²çº¿ä¸‹é¢ç§¯
(AUC)</strong> æ¦‚æ‹¬äº†æ¨¡å‹çš„æ€§èƒ½ï¼›è¯¥å€¼è¶Šæ¥è¿‘ 1.0 è¶Šå¥½ã€‚</li>
</ul>
<h1
id="here-is-a-summary-of-the-provided-slides-on-quadratic-discriminant-analysis-qda-including-the-key-formulas-code-explanations-with-python-equivalents-and-a-guide-to-the-most-important-images.">8.
Here is a summary of the provided slides on Quadratic Discriminant
Analysis (QDA), including the key formulas, code explanations with
Python equivalents, and a guide to the most important images.</h1>
<h3 id="core-concept-qda-vs.-lda">## Core Concept: QDA vs.Â LDA</h3>
<p>The main difference between <strong>Linear Discriminant Analysis
(LDA)</strong> and <strong>Quadratic Discriminant Analysis
(QDA)</strong> lies in their assumptions about the data.
<strong>çº¿æ€§åˆ¤åˆ«åˆ†æ (LDA)</strong> å’Œ <strong>äºŒæ¬¡åˆ¤åˆ«åˆ†æ
(QDA)</strong> çš„ä¸»è¦åŒºåˆ«åœ¨äºå®ƒä»¬å¯¹æ•°æ®çš„å‡è®¾ã€‚ * <strong>LDA</strong>
assumes that all classes share the <strong>same covariance
matrix</strong> (<span class="math inline">\(\Sigma\)</span>). It models
each class as a normal distribution with a different mean (<span
class="math inline">\(\mu_k\)</span>) but the same shape and
orientation. This results in a <em>linear</em> decision boundary between
classes. å‡è®¾æ‰€æœ‰ç±»åˆ«å…±äº«<strong>ç›¸åŒçš„åæ–¹å·®çŸ©é˜µ</strong> (<span
class="math inline">\(\Sigma\)</span>)ã€‚å®ƒå°†æ¯ä¸ªç±»åˆ«å»ºæ¨¡ä¸ºå‡å€¼ä¸åŒ
(<span class="math inline">\(\mu_k\)</span>)
ä½†å½¢çŠ¶å’Œæ–¹å‘ç›¸åŒçš„æ­£æ€åˆ†å¸ƒã€‚è¿™ä¼šå¯¼è‡´ç±»åˆ«ä¹‹é—´å‡ºç° <em>çº¿æ€§</em>
å†³ç­–è¾¹ç•Œã€‚ * <strong>QDA</strong> is more flexible. It assumes that each
class <span class="math inline">\(k\)</span> has its <strong>own,
separate covariance matrix</strong> (<span
class="math inline">\(\Sigma_k\)</span>). This allows each classâ€™s
distribution to have a unique shape, size, and orientation. This
flexibility results in a <em>quadratic</em> decision boundary (like a
parabola, hyperbola, or ellipse). æ›´çµæ´»ã€‚å®ƒå‡è®¾æ¯ä¸ªç±»åˆ« <span
class="math inline">\(k\)</span> éƒ½æœ‰å…¶<strong>ç‹¬ç«‹çš„åæ–¹å·®çŸ©é˜µ</strong>
(<span
class="math inline">\(\Sigma_k\)</span>)ã€‚è¿™ä½¿å¾—æ¯ä¸ªç±»åˆ«çš„åˆ†å¸ƒå…·æœ‰ç‹¬ç‰¹çš„å½¢çŠ¶ã€å¤§å°å’Œæ–¹å‘ã€‚è¿™ç§çµæ´»æ€§å¯¼è‡´äº†<em>äºŒæ¬¡</em>å†³ç­–è¾¹ç•Œï¼ˆç±»ä¼¼äºæŠ›ç‰©çº¿ã€åŒæ›²çº¿æˆ–æ¤­åœ†ï¼‰ã€‚
<strong>Analogy</strong> ğŸ’¡: Imagine youâ€™re drawing boundaries around
different clusters of stars. LDA gives you only straight lines to
separate the clusters. QDA gives you curved lines (circles, ellipses),
which can create a much better fit if the clusters themselves are
elliptical and point in different directions.
æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£åœ¨å›´ç»•ä¸åŒçš„æ˜Ÿå›¢ç»˜åˆ¶è¾¹ç•Œã€‚LDA åªæä¾›ç›´çº¿æ¥åˆ†éš”æ˜Ÿå›¢ã€‚QDA
æä¾›æ›²çº¿ï¼ˆåœ†å½¢ã€æ¤­åœ†å½¢ï¼‰ï¼Œå¦‚æœæ˜Ÿå›¢æœ¬èº«æ˜¯æ¤­åœ†å½¢ä¸”æŒ‡å‘ä¸åŒçš„æ–¹å‘ï¼Œåˆ™å¯ä»¥äº§ç”Ÿæ›´å¥½çš„æ‹Ÿåˆæ•ˆæœã€‚</p>
<h3 id="the-math-behind-qda">## The Math Behind QDA</h3>
<p>QDA classifies a new observation <span
class="math inline">\(x\)</span> to the class <span
class="math inline">\(k\)</span> that has the highest discriminant
score, <span class="math inline">\(\delta_k(x)\)</span>. The formula for
this score is what makes the boundary quadratic. QDA å°†æ–°çš„è§‚æµ‹å€¼ <span
class="math inline">\(x\)</span> å½’ç±»åˆ°å…·æœ‰æœ€é«˜åˆ¤åˆ«åˆ†æ•° <span
class="math inline">\(\delta_k(x)\)</span> çš„ç±» <span
class="math inline">\(k\)</span>
ä¸­ã€‚è¯¥åˆ†æ•°çš„å…¬å¼ä½¿å¾—è¾¹ç•Œå…·æœ‰äºŒæ¬¡é¡¹ã€‚</p>
<p>The discriminant function for class <span
class="math inline">\(k\)</span> is: <span
class="math display">\[\delta_k(x) = -\frac{1}{2}(x - \mu_k)^T
\Sigma_k^{-1}(x - \mu_k) - \frac{1}{2}\log(|\Sigma_k|) +
\log(\pi_k)\]</span></p>
<p>Letâ€™s break it down:</p>
<ul>
<li><span class="math inline">\((x - \mu_k)^T \Sigma_k^{-1}(x -
\mu_k)\)</span>: This is a quadratic term (since it involves <span
class="math inline">\(x^T \Sigma_k^{-1} x\)</span>). It measures the
squared Mahalanobis distance from <span class="math inline">\(x\)</span>
to the class mean <span class="math inline">\(\mu_k\)</span>, scaled by
that classâ€™s specific covariance <span
class="math inline">\(\Sigma_k\)</span>.</li>
<li><span class="math inline">\(\log(|\Sigma_k|)\)</span>: A term that
penalizes classes with larger variance.</li>
<li><span class="math inline">\(\log(\pi_k)\)</span>: The prior
probability of class <span class="math inline">\(k\)</span>. This is our
initial belief about how likely class <span
class="math inline">\(k\)</span> is, before seeing the data.
<ul>
<li><span class="math inline">\((x - \mu_k)^T \Sigma_k^{-1}(x -
\mu_k)\)</span>ï¼šè¿™æ˜¯ä¸€ä¸ªäºŒæ¬¡é¡¹ï¼ˆå› ä¸ºå®ƒæ¶‰åŠ <span
class="math inline">\(x^T \Sigma_k^{-1} x\)</span>ï¼‰ã€‚å®ƒæµ‹é‡ä» <span
class="math inline">\(x\)</span> åˆ°ç±»å‡å€¼ <span
class="math inline">\(\mu_k\)</span>
çš„å¹³æ–¹é©¬æ°è·ç¦»ï¼Œå¹¶æ ¹æ®è¯¥ç±»çš„ç‰¹å®šåæ–¹å·® <span
class="math inline">\(\Sigma_k\)</span> è¿›è¡Œç¼©æ”¾ã€‚</li>
<li><span
class="math inline">\(\log(|\Sigma_k|)\)</span>ï¼šç”¨äºæƒ©ç½šæ–¹å·®è¾ƒå¤§çš„ç±»çš„é¡¹ã€‚</li>
<li><span class="math inline">\(\log(\pi_k)\)</span>ï¼šç±» <span
class="math inline">\(k\)</span> çš„å…ˆéªŒæ¦‚ç‡ã€‚è¿™æ˜¯æˆ‘ä»¬åœ¨çœ‹åˆ°æ•°æ®ä¹‹å‰å¯¹ç±»
<span class="math inline">\(k\)</span> å¯èƒ½æ€§çš„åˆå§‹ä¿¡å¿µã€‚ Because each
class <span class="math inline">\(k\)</span> has its own <span
class="math inline">\(\Sigma_k\)</span>, the quadratic term doesnâ€™t
cancel out when comparing scores between classes, leading to a quadratic
boundary. ç”±äºæ¯ä¸ªç±» <span class="math inline">\(k\)</span> éƒ½æœ‰å…¶è‡ªå·±çš„
<span
class="math inline">\(\Sigma_k\)</span>ï¼Œå› æ­¤åœ¨æ¯”è¾ƒç±»ä¹‹é—´çš„åˆ†æ•°æ—¶ï¼ŒäºŒæ¬¡é¡¹ä¸ä¼šæŠµæ¶ˆï¼Œä»è€Œå¯¼è‡´äºŒæ¬¡è¾¹ç•Œã€‚
<strong>Key Trade-off</strong>:</li>
</ul></li>
<li>If the class variances (<span
class="math inline">\(\Sigma_k\)</span>) are truly different,
<strong>QDA is better</strong>.</li>
<li>If the class variances are similar, <strong>LDA is often
better</strong> because itâ€™s less flexible and less likely to overfit,
especially with a small number of training samples.</li>
<li>å¦‚æœç±»æ–¹å·® (<span class="math inline">\(\Sigma_k\)</span>)
ç¡®å®ä¸åŒï¼Œ<strong>QDA æ›´å¥½</strong>ã€‚</li>
<li>å¦‚æœç±»æ–¹å·®ç›¸ä¼¼ï¼Œ<strong>LDA
é€šå¸¸æ›´å¥½</strong>ï¼Œå› ä¸ºå®ƒçš„çµæ´»æ€§è¾ƒå·®ï¼Œå¹¶ä¸”ä¸å¤ªå¯èƒ½è¿‡æ‹Ÿåˆï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ ·æœ¬æ•°é‡è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚</li>
</ul>
<h3 id="code-implementation-r-and-python">## Code Implementation: R and
Python</h3>
<p>The slides provide R code for fitting a QDA model and evaluating it.
Below is an explanation of the R code and its equivalent in Python using
the popular <code>scikit-learn</code> library.</p>
<h4 id="r-code-from-the-slides">R Code (from the slides)</h4>
<p>The code uses the <code>MASS</code> library for QDA and the
<code>ROCR</code> library for evaluation.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ######## QDA ##########</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Fit the model on the training data</span></span><br><span class="line"><span class="comment"># This formula `Default~.` means &quot;predict &#x27;Default&#x27; using all other variables&quot;.</span></span><br><span class="line">qda.fit.mod2 <span class="operator">&lt;-</span> qda<span class="punctuation">(</span>Default<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> subset<span class="operator">=</span>train.ids<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Make predictions on the test data</span></span><br><span class="line"><span class="comment"># We are interested in the posterior probabilities for the ROC curve</span></span><br><span class="line">qda.fit.pred3 <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>qda.fit.mod2<span class="punctuation">,</span> Default_test<span class="punctuation">)</span><span class="operator">$</span>posterior<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Evaluate using ROC and AUC</span></span><br><span class="line"><span class="comment"># &#x27;prediction&#x27; and &#x27;performance&#x27; are functions from the ROCR library</span></span><br><span class="line">perf <span class="operator">&lt;-</span> performance<span class="punctuation">(</span>prediction<span class="punctuation">(</span>qda.fit.pred3<span class="punctuation">,</span> Default_test<span class="operator">$</span>Default<span class="punctuation">)</span><span class="punctuation">,</span> <span class="string">&quot;auc&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Get the AUC value</span></span><br><span class="line">auc_value <span class="operator">&lt;-</span> perf<span class="operator">@</span>y.values<span class="punctuation">[[</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">]</span></span><br><span class="line"><span class="comment"># Result from slide: 0.9638683</span></span><br></pre></td></tr></table></figure>
<h4 id="python-equivalent-scikit-learn">Python Equivalent
(<code>scikit-learn</code>)</h4>
<p>Hereâ€™s how you would perform the same steps in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> QuadraticDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score, roc_curve</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Default&#x27; is your DataFrame and &#x27;default&#x27; is the target column</span></span><br><span class="line"><span class="comment"># (preprocessing &#x27;student&#x27; and &#x27;default&#x27; columns to numbers)</span></span><br><span class="line"><span class="comment"># Default[&#x27;default_num&#x27;] = Default[&#x27;default&#x27;].apply(lambda x: 1 if x == &#x27;Yes&#x27; else 0)</span></span><br><span class="line"><span class="comment"># X = Default[[&#x27;balance&#x27;, &#x27;income&#x27;, ...]]</span></span><br><span class="line"><span class="comment"># y = Default[&#x27;default_num&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Initialize and fit the QDA model</span></span><br><span class="line">qda = QuadraticDiscriminantAnalysis()</span><br><span class="line">qda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Predict probabilities on the test set</span></span><br><span class="line"><span class="comment"># We need the probability of the positive class (&#x27;Yes&#x27;) for the AUC calculation</span></span><br><span class="line">y_pred_proba = qda.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Calculate the AUC score</span></span><br><span class="line">auc_score = roc_auc_score(y_test, y_pred_proba)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;AUC Score for QDA: <span class="subst">&#123;auc_score:<span class="number">.7</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also plot the ROC curve</span></span><br><span class="line"><span class="comment"># fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)</span></span><br><span class="line"><span class="comment"># plt.plot(fpr, tpr)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure>
<h3 id="model-evaluation-roc-and-auc">## Model Evaluation: ROC and
AUC</h3>
<p>The slides correctly emphasize using the <strong>ROC curve</strong>
and the <strong>Area Under the Curve (AUC)</strong> to compare model
performance.</p>
<ul>
<li><p><strong>ROC Curve (Receiver Operating Characteristic)</strong>:
This plot shows how well a model can distinguish between two classes. It
plots the <strong>True Positive Rate</strong> (y-axis) against the
<strong>False Positive Rate</strong> (x-axis) at all possible
classification thresholds. A better model has a curve that is closer to
the top-left corner.</p></li>
<li><p><strong>AUC (Area Under the Curve)</strong>: This is a single
number that summarizes the entire ROC curve.</p>
<ul>
<li><strong>AUC = 1</strong>: Perfect classifier.</li>
<li><strong>AUC = 0.5</strong>: A useless classifier (equivalent to
random guessing).</li>
<li><strong>AUC &gt; 0.7</strong>: Generally considered an acceptable
model.</li>
</ul></li>
<li><p><strong>ROC
æ›²çº¿ï¼ˆæ¥æ”¶è€…æ“ä½œç‰¹å¾ï¼‰</strong>ï¼šæ­¤å›¾æ˜¾ç¤ºäº†æ¨¡å‹åŒºåˆ†ä¸¤ä¸ªç±»åˆ«çš„èƒ½åŠ›ã€‚å®ƒç»˜åˆ¶äº†æ‰€æœ‰å¯èƒ½çš„åˆ†ç±»é˜ˆå€¼ä¸‹çš„
<strong>çœŸé˜³æ€§ç‡</strong>ï¼ˆy è½´ï¼‰ä¸ <strong>å‡é˜³æ€§ç‡</strong>ï¼ˆx
è½´ï¼‰çš„å¯¹æ¯”å›¾ã€‚æ›´å¥½çš„æ¨¡å‹çš„æ›²çº¿è¶Šé è¿‘å·¦ä¸Šè§’ï¼Œæ•ˆæœå°±è¶Šå¥½ã€‚</p>
<ul>
<li><p><strong>AUCï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰</strong>ï¼šè¿™æ˜¯ä¸€ä¸ªæ¦‚æ‹¬æ•´ä¸ª ROC
æ›²çº¿çš„æ•°å€¼ã€‚</p></li>
<li><p><strong>AUC = 1</strong>ï¼šå®Œç¾çš„åˆ†ç±»å™¨ã€‚</p></li>
<li><p><strong>AUC =
0.5</strong>ï¼šæ— ç”¨çš„åˆ†ç±»å™¨ï¼ˆç›¸å½“äºéšæœºçŒœæµ‹ï¼‰ã€‚</p></li>
<li><p><strong>AUC &gt;
0.7</strong>ï¼šé€šå¸¸è¢«è®¤ä¸ºæ˜¯å¯æ¥å—çš„æ¨¡å‹ã€‚</p></li>
</ul></li>
</ul>
<p>The slides show that for the <code>Default</code> dataset,
<strong>LDAâ€™s AUC (0.9647) was slightly higher than QDAâ€™s
(0.9639)</strong>. This suggests that the assumption of a common
covariance matrix (LDA) was a slightly better fit for this particular
test set, possibly because QDAâ€™s extra flexibility wasnâ€™t needed and it
may have slightly overfit the training data.
è¿™è¡¨æ˜ï¼Œå¯¹äºè¿™ä¸ªç‰¹å®šçš„æµ‹è¯•é›†ï¼Œå…¬å…±åæ–¹å·®çŸ©é˜µ (LDA)
çš„å‡è®¾æ‹Ÿåˆåº¦ç•¥é«˜ï¼Œå¯èƒ½æ˜¯å› ä¸º QDA
çš„é¢å¤–çµæ´»æ€§å¹¶éå¿…éœ€ï¼Œå¹¶ä¸”å¯èƒ½å¯¹è®­ç»ƒæ•°æ®ç•¥å¾®è¿‡æ‹Ÿåˆã€‚</p>
<h3 id="key-takeaways-and-important-images">## Key Takeaways and
Important Images</h3>
<h3
id="heres-a-ranking-of-the-most-important-visual-aids-in-your-slides">Hereâ€™s
a ranking of the most important visual aids in your slides:</h3>
<ol type="1">
<li><p><strong>Slide 68/69 (Model Assumption &amp; Formula)</strong>:
These are the <strong>most critical</strong> slides. They present the
core theoretical difference between LDA and QDA and provide the
mathematical foundation (the discriminant function formula).
Understanding these is key to understanding QDA.</p></li>
<li><p><strong>Slide 73 (ROC Comparison)</strong>: This is the most
important image for <strong>practical evaluation</strong>. It visually
compares the performance of LDA and QDA side-by-side, making it easy to
see which one performs better on this specific dataset. The concept of
AUC is introduced here as the method for comparison.</p></li>
<li><p><strong>Slide 71 (Decision Boundaries with Different
Thresholds)</strong>: This is an excellent conceptual image. It shows
how the quadratic decision boundary (the curved lines) separates the
data points. It also illustrates how changing the probability threshold
(from 0.1 to 0.5 to 0.9) shifts the boundary, trading off between
precision and recall.</p></li>
</ol>
<p>Of course. Here is a summary of the remaining slides, which compare
QDA to other popular classification models like Logistic Regression and
K-Nearest Neighbors (KNN).</p>
<hr />
<h3 id="visualizing-the-core-trade-off-lda-vs.-qda">Visualizing the Core
Trade-off: LDA vs.Â QDA</h3>
<p>This is the most important concept in these slides. The choice
between LDA and QDA depends entirely on the underlying structure of your
data.</p>
<p>The slide shows two scenarios: 1. <strong>Left Plot (<span
class="math inline">\(\Sigma_1 = \Sigma_2\)</span>):</strong> When the
true covariance matrices of the classes are the same, the optimal
decision boundary (the Bayes classifier) is a straight line. LDA, which
assumes equal covariances, creates a linear boundary that approximates
this optimal boundary very well. QDAâ€™s flexible, curved boundary is
unnecessarily complex and might overfit the training data. <strong>In
this case, LDA is better.</strong> 2. <strong>Right Plot (<span
class="math inline">\(\Sigma_1 \neq \Sigma_2\)</span>):</strong> When
the true covariance matrices are different, the optimal decision
boundary is a curve. QDAâ€™s quadratic model can capture this
non-linearity much better than LDAâ€™s rigid linear model. <strong>In this
case, QDA is better.</strong></p>
<p>This perfectly illustrates the <strong>bias-variance
tradeoff</strong>. LDA has higher bias (itâ€™s less flexible) but lower
variance. QDA has lower bias (itâ€™s more flexible) but higher
variance.</p>
<hr />
<h3 id="comparing-performance-on-the-default-dataset">Comparing
Performance on the â€œDefaultâ€ Dataset</h3>
<p>The slides compare four different models on the same classification
task. Letâ€™s look at their performance using the <strong>Area Under the
Curve (AUC)</strong>, where a higher score is better.</p>
<ul>
<li><strong>LDA AUC:</strong> 0.9647</li>
<li><strong>QDA AUC:</strong> 0.9639</li>
<li><strong>Logistic Regression AUC:</strong> 0.9645</li>
<li><strong>K-Nearest Neighbors (KNN):</strong> The plot shows test
error vs.Â K. The error is lowest around K=4, but itâ€™s not directly
converted to an AUC score in the slides.</li>
</ul>
<p>Interestingly, for this particular dataset, LDA, QDA, and Logistic
Regression perform almost identically. This suggests that the decision
boundary for this problem is likely very close to linear, meaning the
extra flexibility of QDA isnâ€™t providing much benefit.</p>
<hr />
<h3 id="pros-and-cons-which-model-to-choose">Pros and Cons: Which Model
to Choose?</h3>
<p>The final slide asks for a comparison of the models. Hereâ€™s a summary
of their key characteristics:</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Decision Boundary</th>
<th style="text-align: left;">Key Pro</th>
<th style="text-align: left;">Key Con</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Logistic Regression</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">Highly interpretable, no strong
assumptions about data distribution.</td>
<td style="text-align: left;">Inflexible; cannot capture non-linear
relationships.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Linear Discriminant Analysis
(LDA)</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">More stable than Logistic Regression when
classes are well-separated.</td>
<td style="text-align: left;">Assumes data is normally distributed with
equal covariance matrices for all classes.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Quadratic Discriminant Analysis
(QDA)</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Quadratic (Curved)</td>
<td style="text-align: left;">More flexible than LDA; can model
non-linear boundaries.</td>
<td style="text-align: left;">Requires more data to estimate parameters
and is more prone to overfitting. Assumes normality.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>K-Nearest Neighbors
(KNN)</strong></td>
<td style="text-align: left;">Non-Parametric</td>
<td style="text-align: left;">Highly Non-linear</td>
<td style="text-align: left;">Extremely flexible; makes no assumptions
about the dataâ€™s distribution.</td>
<td style="text-align: left;">Can be slow on large datasets and suffers
from the â€œcurse of dimensionality.â€ Less interpretable.</td>
</tr>
</tbody>
</table>
<h4 id="summary-of-the-comparison">Summary of the Comparison:</h4>
<ul>
<li><strong>Linear Models (Logistic Regression &amp; LDA):</strong>
Choose these for simplicity, interpretability, and when you believe the
relationship between predictors and the class is linear. LDA often
outperforms Logistic Regression if its normality assumptions are
met.</li>
<li><strong>Non-Linear Models (QDA &amp; KNN):</strong> Choose these
when the decision boundary is likely more complex. QDA is a good middle
ground, offering more flexibility than LDA without being as completely
data-driven as KNN. KNN is the most flexible but requires careful tuning
of the parameter K to avoid overfitting or underfitting.</li>
</ul>
<h1
id="here-is-a-more-detailed-slide-by-slide-analysis-of-the-presentation.">9.
Here is a more detailed, slide-by-slide analysis of the
presentation.</h1>
<h2 id="four-classification-methods-comparison-by-simulation">4.6 Four
Classification Methods: Comparison by Simulation</h2>
<p>This section (slides 81-87) introduces four classification methods
and systematically compares their performance on six different simulated
datasets. The goal is to see which method works best under different
conditions (e.g., linear vs.Â non-linear boundaries, normal
vs.Â non-normal data).</p>
<p>The four methods being compared are: * <strong>Logistic
Regression:</strong> A linear method that models the log-odds as a
linear function of the predictors. * <strong>Linear Discriminant
Analysis (LDA):</strong> Another linear method. It also assumes a linear
decision boundary but makes stronger assumptions than logistic
regression (e.g., that data within each class is normally distributed
with a common covariance matrix). * <strong>Quadratic Discriminant
Analysis (QDA):</strong> A non-linear method. It assumes the log-odds
are a <em>quadratic</em> function, which creates a more flexible, curved
decision boundary. It assumes data within each class is normally
distributed, but <em>without</em> a common covariance matrix. *
<strong>K-Nearest Neighbors (KNN):</strong> A non-parametric, highly
flexible method. Two versions are tested: * <strong>KNN-1 (<span
class="math inline">\(K=1\)</span>):</strong> A very flexible (high
variance) model. * <strong>KNN-CV:</strong> A tuned model where the best
<span class="math inline">\(K\)</span> is chosen via
cross-validation.</p>
<p>æ¯”è¾ƒçš„å››ç§æ–¹æ³•æ˜¯ï¼š *
<strong>é€»è¾‘å›å½’</strong>ï¼šä¸€ç§å°†å¯¹æ•°æ¦‚ç‡å»ºæ¨¡ä¸ºé¢„æµ‹å˜é‡çº¿æ€§å‡½æ•°çš„çº¿æ€§æ–¹æ³•ã€‚
* <strong>çº¿æ€§åˆ¤åˆ«åˆ†æ
(LDA)</strong>ï¼šå¦ä¸€ç§çº¿æ€§æ–¹æ³•ã€‚å®ƒä¹Ÿå‡è®¾çº¿æ€§å†³ç­–è¾¹ç•Œï¼Œä½†æ¯”é€»è¾‘å›å½’åšå‡ºæ›´å¼ºçš„å‡è®¾ï¼ˆä¾‹å¦‚ï¼Œæ¯ä¸ªç±»ä¸­çš„æ•°æ®å‘ˆæ­£æ€åˆ†å¸ƒï¼Œä¸”å…·æœ‰å…±åŒçš„åæ–¹å·®çŸ©é˜µï¼‰ã€‚
* <strong>äºŒæ¬¡åˆ¤åˆ«åˆ†æ
(QDA)</strong>ï¼šä¸€ç§éçº¿æ€§æ–¹æ³•ã€‚å®ƒå‡è®¾å¯¹æ•°æ¦‚ç‡ä¸º<em>äºŒæ¬¡</em>å‡½æ•°ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªæ›´çµæ´»ã€æ›´å¼¯æ›²çš„å†³ç­–è¾¹ç•Œã€‚å®ƒå‡è®¾æ¯ä¸ªç±»ä¸­çš„æ•°æ®æœä»æ­£æ€åˆ†å¸ƒï¼Œä½†<em>æ²¡æœ‰</em>å…±åŒçš„åæ–¹å·®çŸ©é˜µã€‚
* <strong>Kæœ€è¿‘é‚»
(KNN)</strong>ï¼šä¸€ç§éå‚æ•°åŒ–ã€é«˜åº¦çµæ´»çš„æ–¹æ³•ã€‚æµ‹è¯•äº†ä¸¤ä¸ªç‰ˆæœ¬ï¼š *
<strong>KNN-1 (<span
class="math inline">\(K=1\)</span>)</strong>ï¼šä¸€ä¸ªéå¸¸çµæ´»ï¼ˆé«˜æ–¹å·®ï¼‰çš„æ¨¡å‹ã€‚
*
<strong>KNN-CV</strong>ï¼šä¸€ä¸ªç»è¿‡è°ƒæ•´çš„æ¨¡å‹ï¼Œé€šè¿‡äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³çš„<span
class="math inline">\(K\)</span>ã€‚</p>
<h3 id="analysis-of-simulation-scenarios">Analysis of Simulation
Scenarios</h3>
<p>The performance is measured by the <strong>test error rate</strong>
(lower is better), shown in the boxplots for each scenario.
æ€§èƒ½é€šè¿‡<strong>æµ‹è¯•é”™è¯¯ç‡</strong>ï¼ˆè¶Šä½è¶Šå¥½ï¼‰æ¥è¡¡é‡ï¼Œæ¯ä¸ªåœºæ™¯çš„ç®±çº¿å›¾éƒ½æ˜¾ç¤ºäº†è¯¥é”™è¯¯ç‡ã€‚</p>
<ul>
<li><strong>Scenario 1 (Slide 82):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>linear</strong> decision boundary.
Data is <strong>normally distributed</strong> with <em>uncorrelated</em>
predictors.</li>
<li><strong>Result:</strong> <strong>LDA and Logistic Regression perform
best</strong>. Their test error rates are low and similar. This is
expected, as the setup perfectly matches their core assumption (linear
boundary). QDA is slightly worse because its extra flexibility (being
quadratic) is unnecessary. KNN-1 is the worst, as its high flexibility
leads to high variance (overfitting).</li>
<li><strong>ç»“æœï¼š</strong> <strong>LDA
å’Œé€»è¾‘å›å½’è¡¨ç°æœ€ä½³</strong>ã€‚å®ƒä»¬çš„æµ‹è¯•é”™è¯¯ç‡è¾ƒä½ä¸”ç›¸ä¼¼ã€‚è¿™æ˜¯æ„æ–™ä¹‹ä¸­çš„ï¼Œå› ä¸ºè®¾ç½®å®Œå…¨ç¬¦åˆå®ƒä»¬çš„æ ¸å¿ƒå‡è®¾ï¼ˆçº¿æ€§è¾¹ç•Œï¼‰ã€‚QDA
ç•¥å·®ï¼Œå› ä¸ºå…¶é¢å¤–çš„çµæ´»æ€§ï¼ˆäºŒæ¬¡æ–¹ï¼‰æ˜¯ä¸å¿…è¦çš„ã€‚KNN-1
æœ€å·®ï¼Œå› ä¸ºå…¶é«˜çµæ´»æ€§å¯¼è‡´æ–¹å·®è¾ƒå¤§ï¼ˆè¿‡æ‹Ÿåˆï¼‰ã€‚</li>
</ul></li>
<li><strong>Scenario 2 (Slide 83):</strong>
<ul>
<li><strong>Setup:</strong> Same as Scenario 1 (<strong>linear</strong>
boundary, <strong>normal</strong> data), but now the two predictors have
a <strong>correlation of 0.5</strong>.</li>
<li><strong>Result:</strong> <strong>Almost no change</strong> from
Scenario 1. <strong>LDA and Logistic Regression are still the
best</strong>. This shows that these linear methods are robust to
correlation between predictors.</li>
<li><strong>ç»“æœï¼š</strong>ä¸åœºæ™¯ 1
ç›¸æ¯”<strong>å‡ ä¹æ²¡æœ‰å˜åŒ–</strong>ã€‚<strong>LDA
å’Œé€»è¾‘å›å½’ä»ç„¶æ˜¯æœ€ä½³</strong>ã€‚è¿™è¡¨æ˜è¿™äº›çº¿æ€§æ–¹æ³•å¯¹é¢„æµ‹å› å­ä¹‹é—´çš„ç›¸å…³æ€§å…·æœ‰é²æ£’æ€§ã€‚</li>
</ul></li>
<li><strong>Scenario 3 (Slide 84):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>linear</strong> decision boundary,
but the data is drawn from a <strong>t-distribution</strong> (which is
non-normal and has â€œheavy tails,â€ or more extreme outliers).</li>
<li><strong>Result:</strong> <strong>Logistic Regression is the clear
winner</strong>. LDAâ€™s performance gets worse because its assumption of
<em>normality</em> is violated by the t-distribution. QDAâ€™s performance
deteriorates significantly due to the non-normality. This highlights a
key difference: logistic regression is more robust to violations of the
normality assumption.</li>
<li><strong>ç»“æœï¼š</strong>é€»è¾‘å›å½’æ˜æ˜¾èƒœå‡º**ã€‚LDA çš„æ€§èƒ½ä¼šå˜å·®ï¼Œå› ä¸º t
åˆ†å¸ƒè¿åäº†å…¶æ­£æ€æ€§å‡è®¾ã€‚QDA
çš„æ€§èƒ½ç”±äºéæ­£æ€æ€§è€Œæ˜¾è‘—ä¸‹é™ã€‚è¿™å‡¸æ˜¾äº†ä¸€ä¸ªå…³é”®åŒºåˆ«ï¼šé€»è¾‘å›å½’å¯¹è¿åæ­£æ€æ€§å‡è®¾çš„æƒ…å†µæ›´ç¨³å¥ã€‚</li>
</ul></li>
<li><strong>Scenario 4 (Slide 85):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>quadratic</strong> decision
boundary. Data is <strong>normally distributed</strong> with different
correlations in each class.</li>
<li><strong>Result:</strong> <strong>QDA is the clear winner</strong> by
a large margin. This setup perfectly matches QDAâ€™s assumption (quadratic
boundary from normal data with different covariance structures). All
other methods (LDA, Logistic, KNN) are linear or not flexible enough, so
they perform poorly.</li>
<li><strong>ç»“æœï¼š</strong>QDA æ˜æ˜¾èƒœå‡º**ï¼Œä¸”é¥é¥é¢†å…ˆã€‚æ­¤è®¾ç½®å®Œå…¨ç¬¦åˆ
QDA
çš„å‡è®¾ï¼ˆæ¥è‡ªå…·æœ‰ä¸åŒåæ–¹å·®ç»“æ„çš„æ­£æ€æ•°æ®çš„äºŒæ¬¡è¾¹ç•Œï¼‰ã€‚æ‰€æœ‰å…¶ä»–æ–¹æ³•ï¼ˆLDAã€Logisticã€KNNï¼‰éƒ½æ˜¯çº¿æ€§çš„æˆ–ä¸å¤Ÿçµæ´»ï¼Œå› æ­¤æ€§èƒ½ä¸ä½³ã€‚</li>
</ul></li>
<li><strong>Scenario 5 (Slide 86):</strong>
<ul>
<li><strong>Setup:</strong> Another <strong>quadratic</strong> boundary,
but generated in a different way (using a logistic function of quadratic
terms).</li>
<li><strong>Result:</strong> <strong>QDA performs best again</strong>,
closely followed by the flexible <strong>KNN-CV</strong>. The linear
methods (LDA, Logistic) have poor performance because they cannot
capture the curve.</li>
<li><strong>ç»“æœï¼šQDA
å†æ¬¡è¡¨ç°æœ€ä½³</strong>ï¼Œç´§éšå…¶åçš„æ˜¯çµæ´»çš„<strong>KNN-CV</strong>ã€‚çº¿æ€§æ–¹æ³•ï¼ˆLDAã€Logisticï¼‰æ€§èƒ½è¾ƒå·®ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•æ•æ‰æ›²çº¿ã€‚</li>
</ul></li>
<li><strong>Scenario 6 (Slide 87):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>complex, non-linear</strong>
decision boundary (more complex than a simple quadratic curve).</li>
<li><strong>Result:</strong> The <strong>flexible KNN-CV method is the
winner</strong>. Its non-parametric nature allows it to approximate the
complex shape. QDA is not flexible <em>enough</em> and performs worse.
This slide highlights the bias-variance trade-off: the overly simple
KNN-1 is the worst, but the <em>tuned</em> KNN-CV is the best.</li>
<li><strong>ç»“æœï¼š</strong>çµæ´»çš„ KNN-CV
æ–¹æ³•èƒœå‡º**ã€‚å…¶éå‚æ•°ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿè¿‘ä¼¼å¤æ‚çš„å½¢çŠ¶ã€‚ QDA
ä¸å¤Ÿçµæ´»ï¼Œæ€§èƒ½è¾ƒå·®ã€‚è¿™å¼ å¹»ç¯ç‰‡é‡ç‚¹ä»‹ç»äº†åå·®-æ–¹å·®æƒè¡¡ï¼šè¿‡äºç®€å•çš„ KNN-1
æœ€å·®ï¼Œè€Œ <em>è°ƒæ•´åçš„</em> KNN-CV æœ€å¥½ã€‚</li>
</ul></li>
</ul>
<h2 id="r-example-on-smarket-data">4.7 R Example on Smarket Data</h2>
<p>This section (slides 88-93) applies Logistic Regression and LDA to
the <code>Smarket</code> dataset from the <code>ISLR</code> package to
predict the stock marketâ€™s <code>Direction</code> (Up or Down).
æœ¬èŠ‚ï¼ˆå¹»ç¯ç‰‡ 88-93ï¼‰å°†é€»è¾‘å›å½’å’Œ LDA
åº”ç”¨äºâ€œISLRâ€åŒ…ä¸­çš„â€œSmarketâ€æ•°æ®é›†ï¼Œä»¥é¢„æµ‹è‚¡å¸‚çš„â€œæ–¹å‘â€ï¼ˆä¸Šæ¶¨æˆ–ä¸‹è·Œï¼‰ã€‚
### Data Preparation (Slides 88, 89, 90)</p>
<ol type="1">
<li><strong>Load Data:</strong> The <code>ISLR</code> library is loaded,
and the <code>Smarket</code> dataset is explored. It contains daily
percentage returns (<code>Lag1</code>â€¦<code>Lag5</code> for the previous
5 days, <code>Today</code>), <code>Volume</code>, and the
<code>Year</code>.</li>
<li><strong>Explore Data:</strong> A correlation matrix
(<code>cor(Smarket[,-9])</code>) is computed, and a plot of
<code>Volume</code> over time is generated.</li>
<li><strong>Split Data:</strong> The data is split into a training set
(Years 2001-2004) and a test set (Year 2005).
<ul>
<li><code>train &lt;- (Year&lt;2005)</code></li>
<li><code>Smarket.2005 &lt;- Smarket[!train,]</code></li>
<li><code>Direction.2005 &lt;- Direction[!train]</code></li>
<li>The test set has 252 observations.</li>
</ul></li>
<li><strong>åŠ è½½æ•°æ®</strong>ï¼šåŠ è½½â€œISLRâ€åº“ï¼Œå¹¶æ¢ç´¢â€œSmarketâ€æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¯æ—¥ç™¾åˆ†æ¯”æ”¶ç›Šç‡ï¼ˆå‰
5 å¤©çš„â€œLag1â€â€¦â€œLag5â€ï¼Œâ€œä»Šæ—¥â€ï¼‰ã€â€œæˆäº¤é‡â€å’Œâ€œå¹´ä»½â€ã€‚</li>
<li><strong>æ¢ç´¢æ•°æ®</strong>ï¼šè®¡ç®—ç›¸å…³çŸ©é˜µ
(<code>cor(Smarket[,-9])</code>)ï¼Œå¹¶ç”Ÿæˆâ€œæˆäº¤é‡â€éšæ—¶é—´å˜åŒ–çš„å›¾è¡¨ã€‚</li>
<li><strong>æ‹†åˆ†æ•°æ®</strong>ï¼šå°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆå¹´ä»½
2001-2004ï¼‰å’Œæµ‹è¯•é›†ï¼ˆå¹´ä»½ 2005ï¼‰ã€‚
<ul>
<li><code>train &lt;- (Year&lt;2005)</code></li>
<li><code>Smarket.2005 &lt;- Smarket[!train,]</code></li>
<li><code>Direction.2005 &lt;- Direction[!train]</code></li>
<li>æµ‹è¯•é›†åŒ…å« 252 ä¸ªè§‚æµ‹å€¼ã€‚</li>
</ul></li>
</ol>
<h3 id="model-1-logistic-regression-all-predictors-slide-90">Model 1:
Logistic Regression (All Predictors) (Slide 90)</h3>
<ul>
<li><strong>Model:</strong> A logistic regression model is fit on the
training data using <em>all</em> predictors.
<ul>
<li><code>glm.fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data=Smarket, family=binomial, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> The model is used to predict the
direction for the 2005 test data.
<ul>
<li><code>glm.probs &lt;- predict(glm.fit, Smarket.2005, type="response")</code></li>
<li>A threshold of 0.5 is used to classify: if <span
class="math inline">\(P(\text{Up}) &gt; 0.5\)</span>, predict â€œUpâ€.</li>
</ul></li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.5198 (or <strong>48.0%
accuracy</strong>).</li>
<li><strong>Conclusion:</strong> This is â€œnot good!â€â€”itâ€™s worse than
flipping a coin. This suggests the model is either too complex or the
predictors are not useful.</li>
</ul></li>
</ul>
<h3 id="model-2-logistic-regression-lag1-lag2-slide-91">Model 2:
Logistic Regression (Lag1 &amp; Lag2) (Slide 91)</h3>
<ul>
<li><strong>Model:</strong> Based on the poor results, a simpler model
is tried, using only <code>Lag1</code> and <code>Lag2</code>.
<ul>
<li><code>glm.fit &lt;- glm(Direction ~ Lag1 + Lag2, data=Smarket, family=binomial, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> Predictions are made on the 2005 test
set.</li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.4404 (or <strong>55.95%
accuracy</strong>). This is an improvement.</li>
<li><strong>Confusion Matrix:</strong> | | True Down | True Up | | :â€” |
:â€” | :â€” | | <strong>Pred Down</strong> | 77 | 69 | | <strong>Pred
Up</strong> | 35 | 71 |</li>
<li><strong>ROC and AUC:</strong> The ROC (Receiver Operating
Characteristic) curve is plotted, and the AUC (Area Under the Curve) is
calculated.</li>
<li><strong>AUC Value:</strong> <strong>0.5584</strong>. This is very
close to 0.5 (which represents a random-chance model), indicating that
the model has very weak predictive power, even though its accuracy is
above 50%.</li>
</ul></li>
</ul>
<h3 id="model-3-lda-lag1-lag2-slide-92">Model 3: LDA (Lag1 &amp; Lag2)
(Slide 92)</h3>
<ul>
<li><strong>Model:</strong> LDA is now performed using the same setup:
<code>Lag1</code> and <code>Lag2</code> as predictors, trained on the
2001-2004 data.
<ul>
<li><code>library(MASS)</code></li>
<li><code>lda.fit &lt;- lda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> Predictions are made on the 2005 test
set.
<ul>
<li><code>lda.pred &lt;- predict(lda.fit, Smarket.2005)</code></li>
</ul></li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.4404 (or <strong>55.95%
accuracy</strong>).</li>
<li><strong>Confusion Matrix:</strong> | | True Down | True Up | | :â€” |
:â€” | :â€” | | <strong>Pred Down</strong> | 77 | 69 | | <strong>Pred
Up</strong> | 35 | 71 |</li>
<li><strong>Observation:</strong> The confusion matrix and accuracy are
<em>identical</em> to the logistic regression model.</li>
</ul></li>
</ul>
<h3 id="final-comparison-slide-93">Final Comparison (Slide 93)</h3>
<ul>
<li><strong>ROC and AUC for LDA:</strong> The ROC curve for the LDA
model is plotted.</li>
<li><strong>AUC Value:</strong> <strong>0.5584</strong>.</li>
<li><strong>Main Conclusion:</strong> As highlighted in the green box,
<strong>â€œLDA has identical performance as Logistic regression!â€</strong>
In this specific practical example, using these two predictors, both
linear methods produce the exact same confusion matrix, the same
accuracy (56%), and the same AUC (0.558). This reinforces the
theoretical idea that both are fitting a linear boundary.</li>
</ul>
<h3 id="æœ€ç»ˆæ¯”è¾ƒå¹»ç¯ç‰‡-93">æœ€ç»ˆæ¯”è¾ƒï¼ˆå¹»ç¯ç‰‡ 93ï¼‰</h3>
<ul>
<li><strong>LDA çš„ ROC å’Œ AUCï¼š</strong>ç»˜åˆ¶äº† LDA æ¨¡å‹çš„ ROC
æ›²çº¿ã€‚</li>
<li><strong>AUC å€¼ï¼š</strong>0.5584**ã€‚</li>
<li><strong>ä¸»è¦ç»“è®ºï¼š</strong>å¦‚ç»¿è‰²æ–¹æ¡†æ‰€ç¤ºï¼Œâ€œLDA çš„æ€§èƒ½ä¸ Logistic
å›å½’ç›¸åŒï¼â€**
åœ¨è¿™ä¸ªå…·ä½“çš„å®é™…ç¤ºä¾‹ä¸­ï¼Œä½¿ç”¨è¿™ä¸¤ä¸ªé¢„æµ‹å˜é‡ï¼Œä¸¤ç§çº¿æ€§æ–¹æ³•éƒ½äº§ç”Ÿäº†å®Œå…¨ç›¸åŒçš„æ··æ·†çŸ©é˜µã€ç›¸åŒçš„å‡†ç¡®ç‡ï¼ˆ56%ï¼‰å’Œç›¸åŒçš„
AUCï¼ˆ0.558ï¼‰ã€‚è¿™å¼ºåŒ–äº†ä¸¤è€…å‡æ‹Ÿåˆçº¿æ€§è¾¹ç•Œçš„ç†è®ºè§‚ç‚¹ã€‚</li>
</ul>
<h2 id="r-example-on-smarket-data-continued">4.7 R Example on Smarket
Data (Continued)</h2>
<p>The previous slides showed that Logistic Regression and Linear
Discriminant Analysis (LDA) had <strong>identical performance</strong>
on the Smarket dataset (using <code>Lag1</code> and <code>Lag2</code>),
both achieving 56% test accuracy and an AUC of 0.558. The analysis now
tests a more flexible method, QDA.</p>
<h3 id="model-3-qda-lag1-lag2-slides-94-95">Model 3: QDA (Lag1 &amp;
Lag2) (Slides 94-95)</h3>
<ul>
<li><strong>Model:</strong> A Quadratic Discriminant Analysis (QDA)
model is fit on the same training data (2001-2004) using only the
<code>Lag1</code> and <code>Lag2</code> predictors.
<ul>
<li><code>qda.fit &lt;- qda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> The model is used to predict the market
direction for the 2005 test set.</li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Accuracy:</strong> The model achieves a test accuracy
of <strong>0.5992 (or 60%)</strong>.</li>
<li><strong>AUC:</strong> The Area Under the Curve (AUC) for the QDA
model is <strong>0.562</strong>.</li>
</ul></li>
<li><strong>Conclusion:</strong> As the slide highlights, <strong>â€œQDA
has better test performance than LDA and Logistic
regression!â€</strong></li>
</ul>
<h3 id="smarket-example-summary">Smarket Example Summary</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Model Type</th>
<th style="text-align: left;">Test Accuracy</th>
<th style="text-align: left;">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Logistic Regression</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">~56%</td>
<td style="text-align: left;">0.558</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LDA</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">~56%</td>
<td style="text-align: left;">0.558</td>
</tr>
<tr>
<td style="text-align: left;"><strong>QDA</strong></td>
<td style="text-align: left;">Quadratic</td>
<td style="text-align: left;"><strong>~60%</strong></td>
<td style="text-align: left;"><strong>0.562</strong></td>
</tr>
</tbody>
</table>
<p>This practical example reinforces the lessons from the simulations
(Section 4.6). The two linear methods (LDA, Logistic) had identical
performance. The more flexible, non-linear QDA model performed better,
suggesting that the true decision boundary between â€œUpâ€ and â€œDownâ€
(based on <code>Lag1</code> and <code>Lag2</code>) is not perfectly
linear.</p>
<h2 id="kernel-lda">4.8 Kernel LDA</h2>
<p>This new section introduces an even more advanced non-linear method,
Kernel LDA.</p>
<h3 id="the-problem-linear-inseparability-slide-97">The Problem: Linear
Inseparability (Slide 97)</h3>
<p>The section starts with a clear visual example. A dataset of two
concentric circles (a â€œdonutâ€ shape) is <strong>linearly
inseparable</strong>. It is impossible to draw a single straight line to
separate the inner (purple) class from the outer (yellow) class.</p>
<h3 id="the-solution-the-kernel-trick-slides-97-99">The Solution: The
Kernel Trick (Slides 97, 99)</h3>
<ol type="1">
<li><strong>Nonlinear Transformation:</strong> The data is â€œliftedâ€ into
a higher-dimensional <em>feature space</em> using a <strong>nonlinear
transformation</strong>, <span class="math inline">\(x \mapsto
\phi(x)\)</span>. In the example on the slide, the 2D data is
transformed, and in this new space, the two classes <em>become</em>
<strong>linearly separable</strong>.</li>
<li><strong>The â€œKernel Trickâ€:</strong> The main idea (from slide 99)
is that we donâ€™t need to explicitly compute this complex transformation
<span class="math inline">\(\phi(x)\)</span>. LDA (based on Fisherâ€™s
approach) only requires inner products of the data points. The â€œkernel
trickâ€ allows us to replace the inner product in the high-dimensional
feature space (<span class="math inline">\(x_i^T x_j\)</span>) with a
simple <strong>kernel function</strong>, <span
class="math inline">\(k(x_i, x_j)\)</span>, computed in the original,
low-dimensional space.
<ul>
<li>An example of such a kernel is the <strong>Gaussian (RBF)
kernel</strong>: <span class="math inline">\(k(x_i, x_j) \propto
e^{-\|x_i - x_j\|^2 / \sigma^2}\)</span>.</li>
</ul></li>
</ol>
<h3 id="academic-foundations-slide-98">Academic Foundations (Slide
98)</h3>
<p>This method is based on foundational academic papers that generalized
linear methods using kernels: * <strong>Fisher discriminant analysis
with kernels</strong> (Mika, 1999) * <strong>Generalized Discriminant
Analysis Using a Kernel Approach</strong> (Baudat, 2000) *
<strong>Kernel principal component analysis</strong> (SchÃ¶lkopf,
1997)</p>
<p>In short, Kernel LDA is an extension of LDA that uses the kernel
trick to find a linear boundary in a high-dimensional feature space,
which corresponds to a highly non-linear boundary in the original
space.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/27/QM9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/27/QM9/" class="post-title-link" itemprop="url">QM9 Dataset</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-27 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-27T21:00:00+08:00">2025-09-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-29 03:57:13" itemprop="dateModified" datetime="2025-09-29T03:57:13+08:00">2025-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/dataset/" itemprop="url" rel="index"><span itemprop="name">dataset</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="qm9-æ•°æ®é›†çš„xyzæ ¼å¼è¯¦è§£">1. QM9 æ•°æ®é›†çš„XYZæ ¼å¼è¯¦è§£</h3>
<p>è¿™ä¸ªæ•°æ®é›†ä½¿ç”¨çš„ â€œXYZ-likeâ€
æ ¼å¼æ˜¯ä¸€ç§<strong>æ‰©å±•çš„ã€éæ ‡å‡†çš„XYZæ ¼å¼</strong>ã€‚</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 43%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr>
<th>è¡Œå·</th>
<th>å†…å®¹</th>
<th>è§£é‡Š</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ç¬¬ 1 è¡Œ</strong></td>
<td><code>na</code></td>
<td>ä¸€ä¸ªæ•´æ•°ï¼Œä»£è¡¨åˆ†å­ä¸­çš„åŸå­æ€»æ•°ã€‚</td>
</tr>
<tr>
<td><strong>ç¬¬ 2 è¡Œ</strong></td>
<td><code>Properties 1-17</code></td>
<td>åŒ…å«17ä¸ªç†åŒ–æ€§è´¨çš„æ•°å€¼ï¼Œç”¨åˆ¶è¡¨ç¬¦æˆ–ç©ºæ ¼åˆ†éš”ã€‚</td>
</tr>
<tr>
<td><strong>ç¬¬ 3 åˆ° na+2 è¡Œ</strong></td>
<td><code>Element  x  y  z  charge</code></td>
<td>æ¯è¡Œä»£è¡¨ä¸€ä¸ªåŸå­ã€‚ä¾æ¬¡æ˜¯ï¼šå…ƒç´ ç¬¦å·ã€x/y/zåæ ‡ï¼ˆå•ä½ï¼šåŸƒï¼‰ã€Mullikenéƒ¨åˆ†ç”µè·ï¼ˆå•ä½ï¼šeï¼‰ã€‚</td>
</tr>
<tr>
<td><strong>ç¬¬ na+3 è¡Œ</strong></td>
<td><code>Frequencies</code></td>
<td>åˆ†å­çš„æŒ¯åŠ¨é¢‘ç‡ï¼ˆ3na-5æˆ–3na-6ä¸ªï¼‰ã€‚</td>
</tr>
<tr>
<td><strong>ç¬¬ na+4 è¡Œ</strong></td>
<td><code>SMILES_GDB9   SMILES_relaxed</code></td>
<td>æ¥è‡ªGDB9çš„SMILESå­—ç¬¦ä¸²å’Œå¼›è±«åçš„å‡ ä½•æ„å‹çš„SMILESå­—ç¬¦ä¸²ã€‚</td>
</tr>
<tr>
<td><strong>ç¬¬ na+5 è¡Œ</strong></td>
<td><code>InChI_GDB9    InChI_relaxed</code></td>
<td>å¯¹åº”çš„InChIå­—ç¬¦ä¸²ã€‚</td>
</tr>
</tbody>
</table>
<p><strong>ä¸æ ‡å‡†XYZæ ¼å¼å¯¹æ¯”ï¼š</strong> *
<strong>æ ‡å‡†æ ¼å¼</strong>åªæœ‰ç¬¬1è¡Œï¼ˆåŸå­æ•°ï¼‰ã€ç¬¬2è¡Œï¼ˆæ³¨é‡Šï¼‰å’Œåç»­çš„åŸå­åæ ‡è¡Œï¼ˆä»…å«å…ƒç´ å’Œxyzåæ ‡ï¼‰ã€‚
*
<strong>QM9æ ¼å¼</strong>åœ¨ç¬¬2è¡Œæ’å…¥äº†å¤§é‡å±æ€§æ•°æ®ï¼Œåœ¨åŸå­åæ ‡è¡Œå¢åŠ äº†ç”µè·åˆ—ï¼Œå¹¶åœ¨æ–‡ä»¶æœ«å°¾é™„åŠ äº†é¢‘ç‡ã€SMILESå’ŒInChIä¿¡æ¯ã€‚</p>
<h3 id="readme">2. readme</h3>
<ol type="1">
<li><strong>æ•°æ®é›†æ ¸å¿ƒå†…å®¹</strong>:
<ul>
<li>å®ƒåŒ…å«äº†<strong>133,885ä¸ª</strong>å°å‹æœ‰æœºåˆ†å­ï¼ˆç”±H, C, N, O,
Få…ƒç´ ç»„æˆï¼‰çš„é‡å­åŒ–å­¦è®¡ç®—æ•°æ®ã€‚</li>
<li>æ‰€æœ‰åˆ†å­çš„å‡ ä½•æ„å‹éƒ½ç»è¿‡äº†<strong>DFT/B3LYP/6-31G(2df,p)</strong>æ°´å¹³çš„ä¼˜åŒ–ã€‚</li>
<li><code>dsC7O2H10nsd.xyz.tar.bz2</code>æ˜¯è¯¥æ•°æ®é›†çš„ä¸€ä¸ªå­é›†ï¼Œä¸“é—¨åŒ…å«<strong>6,095ä¸ªCâ‚‡Hâ‚â‚€Oâ‚‚çš„åŒåˆ†å¼‚æ„ä½“</strong>ï¼Œå…¶èƒ½é‡å­¦æ€§è´¨åœ¨æ›´é«˜ç²¾åº¦çš„<strong>G4MP2</strong>ç†è®ºæ°´å¹³ä¸‹è®¡ç®—ã€‚</li>
</ul></li>
<li><strong>æ–‡ä»¶ç»“æ„ä¸æ ¼å¼</strong>:
<ul>
<li>æ˜ç¡®æŒ‡å‡ºæ¯ä¸ªåˆ†å­å­˜å‚¨åœ¨å•ç‹¬çš„<code>.xyz</code>æ–‡ä»¶ä¸­ï¼Œå¹¶è¯¦ç»†æè¿°äº†ä¸Šè¿°çš„<strong>éæ ‡å‡†XYZæ‰©å±•æ ¼å¼</strong>ã€‚</li>
<li>è¯¦ç»†åˆ—å‡ºäº†è®°å½•åœ¨æ–‡ä»¶ç¬¬2è¡Œçš„<strong>17ç§ç†åŒ–æ€§è´¨</strong>ï¼ŒåŒ…æ‹¬è½¬åŠ¨å¸¸æ•°(A,
B,
C)ã€å¶æçŸ©(mu)ã€HOMO/LUMOèƒ½çº§ã€é›¶ç‚¹æŒ¯åŠ¨èƒ½(zpve)ã€å†…èƒ½(U)ã€ç„“(H)å’Œå‰å¸ƒæ–¯è‡ªç”±èƒ½(G)ç­‰ã€‚</li>
</ul></li>
<li><strong>æ•°æ®æ¥æºä¸è®¡ç®—æ–¹æ³•</strong>:
<ul>
<li>æ•°æ®æºäº<strong>GDB-9</strong>åŒ–å­¦æ•°æ®åº“ã€‚</li>
<li>ä¸»è¦ä½¿ç”¨äº†ä¸¤ç§é‡å­åŒ–å­¦ç†è®ºæ°´å¹³ï¼š<strong>B3LYP</strong>ç”¨äºå¤§éƒ¨åˆ†å±æ€§è®¡ç®—ï¼Œ<strong>G4MP2</strong>ç”¨äºCâ‚‡Hâ‚â‚€Oâ‚‚å­é›†çš„èƒ½é‡è®¡ç®—ã€‚</li>
</ul></li>
<li><strong>å¼•ç”¨è¦æ±‚</strong>:
<ul>
<li>æ–‡ä»¶æ˜ç¡®è¦æ±‚ï¼Œå¦‚æœä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œéœ€è¦å¼•ç”¨Raghunathan
Ramakrishnanç­‰äººåœ¨2014å¹´å‘è¡¨äºã€ŠScientific Dataã€‹çš„è®ºæ–‡ã€‚</li>
</ul></li>
<li><strong>å…¶ä»–ä¿¡æ¯</strong>:
<ul>
<li>æä¾›äº†ä¸€äº›é¢å¤–æ–‡ä»¶ï¼ˆå¦‚<code>validation.txt</code>,
<code>uncharacterized.txt</code>ï¼‰çš„è¯´æ˜ã€‚</li>
<li>æåˆ°äº†æ•°æ®é›†ä¸­æœ‰å°‘æ•°å‡ ä¸ªåˆ†å­åœ¨å‡ ä½•ä¼˜åŒ–æ—¶éš¾ä»¥æ”¶æ•›ã€‚</li>
</ul></li>
</ol>
<h3 id="å¯è§†åŒ–">3. å¯è§†åŒ–</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import ase.io</span><br><span class="line">import nglview as nv</span><br><span class="line">import io</span><br><span class="line"></span><br><span class="line">def parse_qm9_xyz(file_path):</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    Parses a QM9 extended XYZ file and returns a standard XYZ string.</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    with open(file_path, <span class="string">&#x27;r&#x27;</span>) as f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First line is the number of atoms</span></span><br><span class="line">    num_atoms = int(lines[0].strip())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The next line is properties (skip it)</span></span><br><span class="line">    <span class="comment"># The next num_atoms lines are the coordinates</span></span><br><span class="line">    coord_lines = lines[2:2+num_atoms]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Rebuild a standard XYZ format string in memory</span></span><br><span class="line">    standard_xyz = f<span class="string">&quot;&#123;num_atoms&#125;\n&quot;</span></span><br><span class="line">    standard_xyz += <span class="string">&quot;Comment line\n&quot;</span> <span class="comment"># Add a standard comment line</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> coord_lines:</span><br><span class="line">        parts = line.split()</span><br><span class="line">        <span class="comment"># Keep only the element and the x, y, z coordinates</span></span><br><span class="line">        standard_xyz += f<span class="string">&quot;&#123;parts[0]&#125; &#123;parts[1]&#125; &#123;parts[2]&#125; &#123;parts[3]&#125;\n&quot;</span></span><br><span class="line">        </span><br><span class="line">    <span class="built_in">return</span> standard_xyz</span><br><span class="line"></span><br><span class="line"><span class="comment"># Path to your data file</span></span><br><span class="line">file_path = <span class="string">&quot;/root/QM9/QM9/Data_for_6095_constitutional_isomers_of_C7H10O2.xyz/dsC7O2H10nsd_0001.xyz&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Parse the special file format into a standard XYZ string</span></span><br><span class="line">standard_xyz_data = parse_qm9_xyz(file_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. ASE reads the standard XYZ data from the string variable</span></span><br><span class="line"><span class="comment">#    We use io.StringIO to make the string behave like a file</span></span><br><span class="line">atoms = ase.io.read(io.StringIO(standard_xyz_data), format=<span class="string">&quot;xyz&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create the nglview visualization widget</span></span><br><span class="line">view = nv.show_ase(atoms)</span><br><span class="line">view.add_ball_and_stick()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the widget in the notebook output</span></span><br><span class="line">view</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol type="1">
<li><strong>å®šä¹‰è§£æå‡½æ•° <code>parse_qm9_xyz</code></strong>:
<ul>
<li><strong>ç›®çš„</strong>:
å°†è¿™ä¸ªå‡½æ•°ä½œä¸ºä¸“é—¨å¤„ç†QM9ç‰¹æ®Šæ ¼å¼çš„å·¥å…·ã€‚ä»£ç ä¸»ä½“æ¸…æ™°ï¼Œæ˜“äºå¤ç”¨ã€‚</li>
<li><strong>è¯»å–æ–‡ä»¶</strong>: <code>with open(...)</code>
å®‰å…¨åœ°æ‰“å¼€æ–‡ä»¶ï¼Œå¹¶ç”¨ <code>f.readlines()</code>
å°†æ–‡ä»¶æ‰€æœ‰è¡Œä¸€æ¬¡æ€§è¯»å…¥ä¸€ä¸ªåˆ—è¡¨ <code>lines</code> ä¸­ã€‚</li>
<li><strong>æå–åŸå­æ•°é‡</strong>:
<code>num_atoms = int(lines[0].strip())</code>
è¯»å–ç¬¬ä¸€è¡Œï¼ˆ<code>lines[0]</code>ï¼‰ï¼Œå»é™¤å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼ï¼ˆ<code>.strip()</code>ï¼‰ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ•´æ•°ã€‚è¿™æ˜¯æ„å»ºæ ‡å‡†XYZæ ¼å¼çš„å¿…è¦ä¿¡æ¯ã€‚</li>
<li><strong>æå–åæ ‡ä¿¡æ¯</strong>:
<code>coord_lines = lines[2:2+num_atoms]</code>
æ ‡ä¿¡æ¯ä»ç¬¬3è¡Œå¼€å§‹ï¼ˆç´¢å¼•ä¸º2ï¼‰ï¼ŒæŒç»­<code>num_atoms</code>è¡Œã€‚é€šè¿‡åˆ—è¡¨åˆ‡ç‰‡ï¼Œç²¾ç¡®åœ°æå–å‡ºæ‰€æœ‰åŒ…å«åŸå­åæ ‡çš„è¡Œï¼Œè·³è¿‡äº†ç¬¬2è¡Œçš„å±æ€§ä¿¡æ¯ã€‚</li>
<li><strong>æ„å»ºæ ‡å‡†XYZæ ¼å¼å­—ç¬¦ä¸²</strong>:
<ul>
<li>åˆ›å»ºä¸€ä¸ªåä¸º <code>standard_xyz</code> çš„æ–°å­—ç¬¦ä¸²ã€‚</li>
<li>é¦–å…ˆï¼Œå°†åŸå­æ•°é‡å’Œæ¢è¡Œç¬¦å†™å…¥ã€‚</li>
<li>ç„¶åï¼Œæ·»åŠ ä¸€è¡Œæ ‡å‡†çš„æ³¨é‡Šï¼ˆâ€œComment
lineâ€ï¼‰ï¼Œè¿™æ˜¯æ ‡å‡†XYZæ ¼å¼æ‰€è¦æ±‚çš„ã€‚</li>
<li>æœ€åï¼Œéå†åˆšåˆšæå–çš„ <code>coord_lines</code> åˆ—è¡¨ã€‚å¯¹äºæ¯ä¸€è¡Œï¼Œä½¿ç”¨
<code>.split()</code>
å°†å…¶æ‹†åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼ˆä¾‹å¦‚ï¼š<code>['C', 'x', 'y', 'z', 'charge']</code>ï¼‰ã€‚åªå–å‰å››éƒ¨åˆ†ï¼ˆå…ƒç´ ç¬¦å·å’Œxyzåæ ‡ï¼‰ï¼Œå¹¶é‡æ–°ç»„åˆæˆæ–°çš„ä¸€è¡Œï¼Œ<strong>ä»è€Œä¸¢å¼ƒäº†æœ«å°¾çš„Mullikenç”µè·æ•°æ®</strong>ã€‚</li>
</ul></li>
<li><strong>è¿”å›ç»“æœ</strong>:
å‡½æ•°è¿”å›ä¸€ä¸ªåŒ…å«äº†æ ‡å‡†XYZæ ¼å¼æ•°æ®çš„ã€å¹²å‡€çš„å­—ç¬¦ä¸²ã€‚</li>
</ul></li>
<li><strong>ä¸»ç¨‹åºæ‰§è¡Œæµç¨‹</strong>:
<ul>
<li><strong>è°ƒç”¨å‡½æ•°</strong>:
<code>standard_xyz_data = parse_qm9_xyz(file_path)</code>
è°ƒç”¨ä¸Šé¢çš„å‡½æ•°ï¼Œå®Œæˆä»æ–‡ä»¶åˆ°æ ‡å‡†æ ¼å¼å­—ç¬¦ä¸²çš„è½¬æ¢ã€‚</li>
<li><strong>åœ¨å†…å­˜ä¸­è¯»å–</strong>:
<code>ase.io.read(io.StringIO(standard_xyz_data), format="xyz")</code>
è¿™ä¸€æ­¥éå¸¸é«˜æ•ˆã€‚<code>io.StringIO</code> å°†æˆ‘ä»¬çš„å­—ç¬¦ä¸²å˜é‡
<code>standard_xyz_data</code>
æ¨¡æ‹Ÿæˆä¸€ä¸ªå†…å­˜ä¸­çš„æ–‡æœ¬æ–‡ä»¶ã€‚è¿™æ ·ï¼Œ<code>ase.io.read</code>
å°±å¯ä»¥ç›´æ¥è¯»å–å®ƒï¼Œè€Œæ— éœ€å…ˆå°†æ¸…æ´—åçš„æ•°æ®å†™å…¥ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶å†è¯»å–ï¼ŒèŠ‚çœäº†ç£ç›˜I/Oæ“ä½œã€‚</li>
<li><strong>å¯è§†åŒ–</strong>: æ¥ä¸‹æ¥çš„ä»£ç  (<code>nv.show_ase</code>ç­‰)
å°±å’Œæœ€åˆçš„è®¾æƒ³ä¸€æ ·äº†ï¼Œå› ä¸ºæ­¤æ—¶ <code>atoms</code>
å¯¹è±¡å·²ç»æ˜¯é€šè¿‡æ ‡å‡†ã€å¹²å‡€çš„æ•°æ®æˆåŠŸåˆ›å»ºçš„äº†ã€‚</li>
</ul></li>
</ol>
<p><img src="/imgs/QM9/C7O2H10/C7O2H10.png" alt="C7O2H10"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/27/fusionnetwork/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/27/fusionnetwork/" class="post-title-link" itemprop="url">FusionProt - è®ºæ–‡é˜…è¯»</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-27 11:00:00" itemprop="dateCreated datePublished" datetime="2025-09-27T11:00:00+08:00">2025-09-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-29 03:56:14" itemprop="dateModified" datetime="2025-09-29T03:56:14+08:00">2025-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Reading/" itemprop="url" rel="index"><span itemprop="name">Paper Reading</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Fusing Sequence and Structural Information for Unified Protein
Representation Learning</p>
<p><a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=imcinaOHod">FusionProt</a></p>
<h2 id="è›‹ç™½è´¨è¡¨ç¤ºå­¦ä¹ ">1 è›‹ç™½è´¨è¡¨ç¤ºå­¦ä¹ ï¼š</h2>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>FusionProt :å¯å­¦ä¹ èåˆ
tokenå’Œè¿­ä»£åŒå‘ä¿¡æ¯äº¤æ¢ï¼Œå®ç°åºåˆ—ä¸ç»“æ„çš„åŠ¨æ€ååŒå­¦ä¹ ï¼Œè€Œéé™æ€æ‹¼æ¥ã€‚</p>
<h2 id="ä¸€ç»´1dæ°¨åŸºé…¸åºåˆ—å’Œä¸‰ç»´3dç©ºé—´ç»“æ„">2.
ä¸€ç»´ï¼ˆ1Dï¼‰æ°¨åŸºé…¸åºåˆ—å’Œä¸‰ç»´ï¼ˆ3Dï¼‰ç©ºé—´ç»“æ„ï¼š</h2>
<ul>
<li><p><strong>å•æ¨¡æ€ä¾èµ–:</strong>
ProteinBERTã€ESM-2ä»…åŸºäºåºåˆ—</p></li>
<li><p><strong>é™æ€èåˆç¼ºé™· :</strong>ESM-GearNetã€SaProt
ç»“åˆåºåˆ—ä¸ç»“æ„ï¼Œä½†é‡‡ç”¨ â€œå•å‘ / ä¸€æ¬¡æ€§èåˆâ€</p></li>
</ul>
<p>å¥½çš„ï¼Œå®Œå…¨æ²¡æœ‰é—®é¢˜ã€‚è¿™æ˜¯å¯¹ <code>FusionNetwork</code>
æ¨¡å‹æ¶æ„ä»£ç çš„ä¸­æ–‡å¤è¿°åˆ†æã€‚</p>
<h2 id="æ¨¡å‹æ€»ä½“">3. æ¨¡å‹æ€»ä½“</h2>
<p><img src="/imgs/fusionProt/FusionProt.png" alt="fusion">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@R.register(<span class="params"><span class="string">&quot;models.FusionNetwork&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FusionNetwork</span>(nn.Module, core.Configurable):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sequence_model, structure_model, fusion=<span class="string">&quot;series&quot;</span>, cross_dim=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(FusionNetwork, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.sequence_model = sequence_model</span><br><span class="line">        <span class="variable language_">self</span>.structure_model = structure_model</span><br><span class="line">        <span class="variable language_">self</span>.output_dim = sequence_model.output_dim + structure_model.output_dim</span><br><span class="line">        <span class="variable language_">self</span>.inject_step = <span class="number">5</span>   <span class="comment"># (sequence_layers / structure_layers) layers</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong><code>class FusionNetwork(...)</code></strong>:
å®šä¹‰äº†æ¨¡å‹ç±»ï¼Œå®ƒç»§æ‰¿è‡ª PyTorch çš„åŸºç¡€æ¨¡å— <code>nn.Module</code>ã€‚</li>
<li><strong><code>__init__(...)</code></strong>:
æ„é€ å‡½æ•°ï¼Œæ¥æ”¶å·²ç»åˆå§‹åŒ–å¥½çš„ <code>sequence_model</code> å’Œ
<code>structure_model</code> ä½œä¸ºè¾“å…¥ã€‚</li>
<li><strong><code>self.output_dim</code></strong>:
å®šä¹‰äº†æ¨¡å‹æœ€ç»ˆè¾“å‡ºç‰¹å¾çš„ç»´åº¦ã€‚å› ä¸ºæœ€åä¼šå°†ä¸¤ä¸ªæ¨¡å‹çš„ç‰¹å¾æ‹¼æ¥èµ·æ¥ï¼Œæ‰€ä»¥æ˜¯ä¸¤è€…è¾“å‡ºç»´åº¦ä¹‹å’Œã€‚</li>
<li><strong><code>self.inject_step = 5</code></strong>:å®šä¹‰äº†ä¿¡æ¯â€œæ³¨å…¥â€æˆ–â€œäº¤æµâ€çš„é¢‘ç‡ã€‚è¿™é‡Œè®¾ç½®ä¸º
5ï¼Œæ„å‘³ç€<strong>æ¯ç»è¿‡åºåˆ—æ¨¡å‹çš„ 5
å±‚ï¼Œå°±ä¼šè¿›è¡Œä¸€æ¬¡ä¿¡æ¯äº¤æ¢</strong>ã€‚</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Structure embeddings layer</span></span><br><span class="line">raw_input_dim = <span class="number">21</span>  <span class="comment"># amino acid tokens</span></span><br><span class="line"><span class="variable language_">self</span>.structure_embed_linear = nn.Linear(raw_input_dim, structure_model.input_dim)</span><br><span class="line"><span class="variable language_">self</span>.embedding_batch_norm = nn.BatchNorm1d(structure_model.input_dim)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>self.structure_embed_linear</code></strong>:
ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç”¨äºå°†åŸå§‹çš„ç»“æ„è¾“å…¥ï¼ˆæ¯”å¦‚ 21
ç§æ°¨åŸºé…¸çš„ç‹¬çƒ­ç¼–ç ï¼‰è½¬æ¢ä¸ºç»“æ„æ¨¡å‹ï¼ˆGNNï¼‰æ‰€æœŸæœ›çš„è¾“å…¥ç»´åº¦ã€‚</li>
<li><strong><code>self.embedding_batch_norm</code></strong>:
æ‰¹å½’ä¸€åŒ–å±‚ï¼Œç”¨äºç¨³å®šç»“æ„åµŒå…¥å±‚çš„è®­ç»ƒè¿‡ç¨‹ã€‚</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normal Initialization of the 3D structure token</span></span><br><span class="line">structure_token = nn.Parameter(torch.Tensor(structure_model.input_dim).unsqueeze(<span class="number">0</span>))</span><br><span class="line">nn.init.normal_(structure_token, mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>)</span><br><span class="line"><span class="variable language_">self</span>.structure_token = nn.Parameter(structure_token.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>self.structure_token</code></strong>: ä¸€ä¸ªå¯å­¦ä¹ çš„å‘é‡
(<code>nn.Parameter</code>)ã€‚è¿™ä¸ªâ€œä»¤ç‰Œâ€ä¸ä»£è¡¨ä»»ä½•çœŸå®çš„åŸå­æˆ–æ°¨åŸºé…¸ï¼Œè€Œæ˜¯ä¸€ä¸ªæŠ½è±¡çš„è½½ä½“ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®ƒå°†<strong>å­¦ä¹ å¦‚ä½•ç¼–ç å’Œè¡¨ç¤ºæ•´ä¸ªè›‹ç™½è´¨çš„å…¨å±€
3D ç»“æ„ä¿¡æ¯</strong>ã€‚å®ƒå°±åƒä¸€ä¸ªä¿¡æ¯ä¿¡ä½¿ã€‚</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Linear Transformation between structure to sequential spaces</span></span><br><span class="line"><span class="variable language_">self</span>.structure_linears = nn.ModuleList([...])</span><br><span class="line"><span class="variable language_">self</span>.seq_linears = nn.ModuleList([...])</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>self.structure_linears</code> /
<code>self.seq_linears</code></strong>:
åºåˆ—æ¨¡å‹å’Œç»“æ„æ¨¡å‹å†…éƒ¨å¤„ç†çš„ç‰¹å¾å‘é‡ç»´åº¦å¯èƒ½ä¸åŒã€‚å½“â€œ3D
ä»¤ç‰Œâ€éœ€è¦åœ¨ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´ä¼ é€’æ—¶ï¼Œè¿™äº›çº¿æ€§å±‚è´Ÿè´£å°†å®ƒçš„è¡¨ç¤ºä»ä¸€ä¸ªæ¨¡å‹çš„ç‰¹å¾ç©ºé—´è½¬æ¢åˆ°å¦ä¸€ä¸ªæ¨¡å‹çš„ç‰¹å¾ç©ºé—´ã€‚</li>
</ul>
<h2 id="å‰å‘">4. å‰å‘</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, graph, <span class="built_in">input</span>, all_loss=<span class="literal">None</span>, metric=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># Build a new protein graph with the 3D token (the lase node)</span></span><br><span class="line">    new_graph = <span class="variable language_">self</span>.build_protein_graph_with_3d_token(graph)</span><br></pre></td></tr></table></figure>
<ul>
<li>é¦–å…ˆè°ƒç”¨è¾…åŠ©å‡½æ•°ï¼Œå°†è¾“å…¥çš„è›‹ç™½è´¨å›¾è°±è¿›è¡Œæ”¹é€ ï¼šä¸ºå›¾è°±å¢åŠ ä¸€ä¸ªä»£è¡¨â€œ3D
ä»¤ç‰Œâ€çš„æ–°èŠ‚ç‚¹ï¼Œå¹¶å°†è¿™ä¸ªæ–°èŠ‚ç‚¹ä¸å›¾ä¸­æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹è¿æ¥èµ·æ¥ã€‚</li>
</ul>
<h5 id="åºåˆ—æ¨¡å‹çš„åˆå§‹åŒ–"><strong>åºåˆ—æ¨¡å‹çš„åˆå§‹åŒ–</strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequence (ESM) model initialization</span></span><br><span class="line">sequence_input = <span class="variable language_">self</span>.sequence_model.mapping[graph.residue_type]</span><br><span class="line">sequence_input[sequence_input == -<span class="number">1</span>] = graph.residue_type[sequence_input == -<span class="number">1</span>]</span><br><span class="line">size = graph.num_residues</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check if sequence size is not bigger than max seq length</span></span><br><span class="line"><span class="keyword">if</span> (size &gt; <span class="variable language_">self</span>.sequence_model.max_input_length).<span class="built_in">any</span>():</span><br><span class="line">    starts = size.cumsum(<span class="number">0</span>) - size</span><br><span class="line">    size = size.clamp(<span class="built_in">max</span>=<span class="variable language_">self</span>.sequence_model.max_input_length)</span><br><span class="line">    ends = starts + size</span><br><span class="line">    mask = functional.multi_slice_mask(starts, ends, graph.num_residues)</span><br><span class="line">    sequence_input = sequence_input[mask]</span><br><span class="line">    graph = graph.subresidue(mask)</span><br><span class="line">size_ext = size</span><br><span class="line"></span><br><span class="line"><span class="comment"># BOS == CLS</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.alphabet.prepend_bos:</span><br><span class="line">    bos = torch.ones(graph.batch_size, dtype=torch.long, device=<span class="variable language_">self</span>.sequence_model.device) * <span class="variable language_">self</span>.sequence_model.alphabet.cls_idx</span><br><span class="line">    sequence_input, size_ext = functional._extend(bos, torch.ones_like(size_ext), sequence_input, size_ext)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.alphabet.append_eos:</span><br><span class="line">    eos = torch.ones(graph.batch_size, dtype=torch.long, device=<span class="variable language_">self</span>.sequence_model.device) * <span class="variable language_">self</span>.sequence_model.alphabet.eos_idx</span><br><span class="line">    sequence_input, size_ext = functional._extend(sequence_input, size_ext, eos, torch.ones_like(size_ext))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Padding</span></span><br><span class="line">tokens = functional.variadic_to_padded(sequence_input, size_ext, value=<span class="variable language_">self</span>.sequence_model.alphabet.padding_idx)[<span class="number">0</span>]</span><br><span class="line">repr_layers = [<span class="variable language_">self</span>.sequence_model.repr_layer]</span><br><span class="line"><span class="keyword">assert</span> tokens.ndim == <span class="number">2</span></span><br><span class="line">padding_mask = tokens.eq(<span class="variable language_">self</span>.sequence_model.model.padding_idx)  <span class="comment"># B, T</span></span><br></pre></td></tr></table></figure>
<ul>
<li>åºåˆ—æ•°æ®è¿›è¡Œ Transformer æ¨¡å‹ï¼ˆå¦‚ ESMï¼‰æ‰€éœ€çš„æ ‡å‡†é¢„å¤„ç†ã€‚</li>
<li>åŒ…æ‹¬æ·»åŠ åºåˆ—å¼€å§‹ï¼ˆBOSï¼‰å’Œç»“æŸï¼ˆEOSï¼‰æ ‡è®°ï¼Œä»¥åŠå°†æ‰€æœ‰åºåˆ—å¡«å……ï¼ˆPaddingï¼‰åˆ°ç›¸åŒé•¿åº¦ï¼Œä»¥ä¾¿è¿›è¡Œæ‰¹å¤„ç†ã€‚</li>
</ul>
<h5 id="æ¨¡å‹åˆå§‹åŒ–ä¸åˆæ¬¡èåˆ"><strong>æ¨¡å‹åˆå§‹åŒ–ä¸åˆæ¬¡èåˆ</strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequence embedding layer</span></span><br><span class="line">x = <span class="variable language_">self</span>.sequence_model.model.embed_scale * <span class="variable language_">self</span>.sequence_model.model.embed_tokens(tokens)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.model.token_dropout:</span><br><span class="line">    x.masked_fill_((tokens == <span class="variable language_">self</span>.sequence_model.model.mask_idx).unsqueeze(-<span class="number">1</span>), <span class="number">0.0</span>)</span><br><span class="line">    <span class="comment"># x: B x T x C</span></span><br><span class="line">    mask_ratio_train = <span class="number">0.15</span> * <span class="number">0.8</span></span><br><span class="line">    src_lengths = (~padding_mask).<span class="built_in">sum</span>(-<span class="number">1</span>)</span><br><span class="line">    mask_ratio_observed = (tokens == <span class="variable language_">self</span>.sequence_model.model.mask_idx).<span class="built_in">sum</span>(-<span class="number">1</span>).to(x.dtype) / src_lengths</span><br><span class="line">    x = x * (<span class="number">1</span> - mask_ratio_train) / (<span class="number">1</span> - mask_ratio_observed)[:, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Structure model initialization</span></span><br><span class="line">structure_hiddens = []</span><br><span class="line">batch_size = graph.batch_size</span><br><span class="line">structure_embedding = <span class="variable language_">self</span>.embedding_batch_norm(<span class="variable language_">self</span>.structure_embed_linear(<span class="built_in">input</span>))</span><br><span class="line">structure_token_batched = <span class="variable language_">self</span>.structure_token.unsqueeze(<span class="number">0</span>).expand(batch_size, -<span class="number">1</span>)</span><br><span class="line">structure_input = torch.cat([structure_embedding.squeeze(<span class="number">1</span>), structure_token_batched], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the 3D token representation</span></span><br><span class="line">structure_token_expanded = <span class="variable language_">self</span>.structure_token.unsqueeze(<span class="number">0</span>).expand(x.size(<span class="number">0</span>), -<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">x = torch.cat((x[:, :-<span class="number">1</span>], structure_token_expanded, x[:, -<span class="number">1</span>:]), dim=<span class="number">1</span>)</span><br><span class="line">padding_mask = torch.cat([padding_mask[:, :-<span class="number">1</span>],</span><br><span class="line">                          torch.zeros(padding_mask.size(<span class="number">0</span>), <span class="number">1</span>).to(padding_mask), padding_mask[:, -<span class="number">1</span>:]], dim=<span class="number">1</span>)</span><br><span class="line">size_ext += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> padding_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    x = x * (<span class="number">1</span> - padding_mask.unsqueeze(-<span class="number">1</span>).type_as(x))</span><br><span class="line"></span><br><span class="line">repr_layers = <span class="built_in">set</span>(repr_layers)</span><br><span class="line">hidden_representations = &#123;&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="number">0</span> <span class="keyword">in</span> repr_layers:</span><br><span class="line">    hidden_representations[<span class="number">0</span>] = x</span><br><span class="line"></span><br><span class="line"><span class="comment"># (B, T, E) =&gt; (T, B, E)</span></span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> padding_mask.<span class="built_in">any</span>():</span><br><span class="line">    padding_mask = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>å°† 3D ä»¤ç‰Œæ’å…¥åºåˆ—ã€‚</strong>
<ol type="1">
<li>ä¸ºåºåˆ—æ•°æ®ç”Ÿæˆåˆå§‹çš„è¯åµŒå…¥è¡¨ç¤º <code>x</code>ã€‚</li>
<li>å°† <code>self.structure_token</code> çš„åˆå§‹çŠ¶æ€æ’å…¥åˆ°åºåˆ—åµŒå…¥
<code>x</code> ä¸­ï¼Œé€šå¸¸æ˜¯æ”¾åœ¨åºåˆ—ç»“æŸæ ‡è®°ï¼ˆEOSï¼‰ä¹‹å‰ã€‚</li>
<li>åºåˆ—æ¨¡å‹çœ‹åˆ°çš„è¾“å…¥åºåˆ—å˜æˆäº†
<code>[BOS, æ®‹åŸº1, æ®‹åŸº2, ..., æ®‹åŸºN, **3Dä»¤ç‰Œ**, EOS]</code>
çš„å½¢å¼ã€‚</li>
</ol></li>
</ul>
<h5 id="èåˆå¾ªç¯"><strong>èåˆå¾ªç¯ </strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> seq_layer_idx, seq_layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.sequence_model.model.layers):</span><br><span class="line">    x, attn = seq_layer(</span><br><span class="line">        x,</span><br><span class="line">        self_attn_padding_mask=padding_mask,</span><br><span class="line">        need_head_weights=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> (seq_layer_idx + <span class="number">1</span>) <span class="keyword">in</span> repr_layers:</span><br><span class="line">        hidden_representations[seq_layer_idx + <span class="number">1</span>] = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>æ¨¡å‹å¼€å§‹é€å±‚éå†åºåˆ—æ¨¡å‹çš„æ‰€æœ‰å±‚ï¼ˆä¾‹å¦‚ Transformer
çš„ç¼–ç å™¨å±‚ï¼‰ã€‚<code>x</code> åœ¨æ¯ä¸€å±‚éƒ½ä¼šè¢«æ›´æ–°ã€‚</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> seq_layer_idx &gt; <span class="number">0</span> <span class="keyword">and</span> seq_layer_idx % <span class="variable language_">self</span>.inject_step == <span class="number">0</span>:</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ä¿¡æ¯æ³¨å…¥ç‚¹</strong>ï¼šæ¯å½“å±‚æ•°çš„ç´¢å¼•èƒ½è¢«
<code>inject_step</code> (å³ 5) æ•´é™¤æ—¶ï¼Œå°±è§¦å‘ä¸€æ¬¡ä¿¡æ¯äº¤æ¢ã€‚</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. ä»åºåˆ—ä¸­æå– 3D ä»¤ç‰Œçš„è¡¨ç¤º</span></span><br><span class="line"><span class="keyword">if</span> structure_layer_index == <span class="number">0</span>:</span><br><span class="line">    structure_input = torch.cat((structure_input[:-<span class="number">1</span> * batch_size],  x[-<span class="number">2</span>, :, :]), dim=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    structure_input = torch.cat((structure_input[:-<span class="number">1</span> * batch_size],</span><br><span class="line">                                 <span class="variable language_">self</span>.seq_linears[structure_layer_index](x[-<span class="number">2</span>, :, :])), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. ç”¨ç»“æ„æ¨¡å‹çš„ä¸€å±‚æ¥å¤„ç†</span></span><br><span class="line">hidden = <span class="variable language_">self</span>.structure_model.layers[structure_layer_index](new_graph, structure_input)</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.structure_model.short_cut <span class="keyword">and</span> hidden.shape == structure_input.shape:</span><br><span class="line">    hidden = hidden + structure_input</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.structure_model.batch_norm:</span><br><span class="line">    hidden = <span class="variable language_">self</span>.structure_model.batch_norms[structure_layer_index](hidden)</span><br><span class="line"></span><br><span class="line">structure_hiddens.append(hidden)</span><br><span class="line">structure_input = hidden</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. å°†æ›´æ–°åçš„ 3D ä»¤ç‰Œè¡¨ç¤ºæ’å›åºåˆ—</span></span><br><span class="line">updated_structure_token = <span class="variable language_">self</span>.structure_linears[...](structure_input[-<span class="number">1</span> * batch_size:])</span><br><span class="line">x = torch.cat((x[:-<span class="number">2</span>, :, :], updated_structure_token.unsqueeze(<span class="number">0</span>), x[-<span class="number">1</span>:, :, :]), dim=<span class="number">0</span>)</span><br><span class="line">structure_layer_index += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>ä¿¡æ¯æµç¨‹</strong>ï¼š
<ol type="1">
<li><strong>ä»åºåˆ—åˆ°ç»“æ„</strong>ï¼šæ¨¡å‹ä»åºåˆ—è¡¨ç¤º <code>x</code>
ä¸­æå–å‡ºâ€œ3D
ä»¤ç‰Œâ€çš„æœ€æ–°å‘é‡ã€‚è¿™ä¸ªå‘é‡æ­¤æ—¶å·²ç»å¸æ”¶äº†å‰é¢å‡ å±‚åºåˆ—æ¨¡å‹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ç„¶åï¼Œé€šè¿‡ï¼ˆ<code>seq_linears</code>ï¼‰å°†å…¶è½¬æ¢åï¼Œæ›´æ–°åˆ°ç»“æ„æ¨¡å‹çš„è¾“å…¥ä¸­ã€‚</li>
<li><strong>ç»“æ„ä¿¡æ¯å¤„ç†</strong>ï¼šè¿è¡Œä¸€å±‚ç»“æ„æ¨¡å‹ï¼ˆGNNï¼‰ã€‚GNN
æ ¹æ®å›¾çš„è¿æ¥å…³ç³»æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹çš„è¡¨ç¤ºï¼Œå½“ç„¶ä¹ŸåŒ…æ‹¬â€œ3D
ä»¤ç‰Œâ€è¿™ä¸ªç‰¹æ®ŠèŠ‚ç‚¹ã€‚</li>
<li><strong>ä»ç»“æ„åˆ°åºåˆ—</strong>ï¼šä» GNN çš„è¾“å‡ºä¸­ï¼Œå†æ¬¡æå–å‡ºâ€œ3D
ä»¤ç‰Œâ€çš„å‘é‡ã€‚è¿™ä¸ªå‘é‡åŒ…å«æ›´æ–°åçš„ç»“æ„ä¿¡æ¯ã€‚å†é€šè¿‡ï¼ˆ<code>structure_linears</code>ï¼‰è½¬æ¢åï¼ŒæŠŠå®ƒ<strong>æ’å›</strong>åˆ°åºåˆ—è¡¨ç¤º
<code>x</code> ä¸­ï¼Œæ›¿æ¢æ‰æ—§çš„ç‰ˆæœ¬ã€‚</li>
</ol></li>
</ul>
<p>è¿™ä¸ªå¾ªç¯ä¸æ–­é‡å¤ã€‚</p>
<h5 id="è¾“å‡º"><strong>è¾“å‡º</strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Structural Output</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.structure_model.concat_hidden:</span><br><span class="line">    structure_node_feature = torch.cat(structure_hiddens, dim=-<span class="number">1</span>)[:-<span class="number">1</span> * batch_size]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    structure_node_feature = structure_hiddens[-<span class="number">1</span>][:-<span class="number">1</span> * batch_size]</span><br><span class="line"></span><br><span class="line">structure_graph_feature = <span class="variable language_">self</span>.structure_model.readout(graph, structure_node_feature)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sequence Output</span></span><br><span class="line">x = <span class="variable language_">self</span>.sequence_model.model.emb_layer_norm_after(x)</span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># (T, B, E) =&gt; (B, T, E)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># last hidden representation should have layer norm applied</span></span><br><span class="line"><span class="keyword">if</span> (seq_layer_idx + <span class="number">1</span>) <span class="keyword">in</span> repr_layers:</span><br><span class="line">    hidden_representations[seq_layer_idx + <span class="number">1</span>] = x</span><br><span class="line">x = <span class="variable language_">self</span>.sequence_model.model.lm_head(x)</span><br><span class="line"></span><br><span class="line">output = &#123;<span class="string">&quot;logits&quot;</span>: x, <span class="string">&quot;representations&quot;</span>: hidden_representations&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sequence (ESM) model outputs</span></span><br><span class="line">residue_feature = output[<span class="string">&quot;representations&quot;</span>][<span class="variable language_">self</span>.sequence_model.repr_layer]</span><br><span class="line">residue_feature = functional.padded_to_variadic(residue_feature, size_ext)</span><br><span class="line">starts = size_ext.cumsum(<span class="number">0</span>) - size_ext</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.alphabet.prepend_bos:</span><br><span class="line">    starts = starts + <span class="number">1</span></span><br><span class="line">ends = starts + size</span><br><span class="line">mask = functional.multi_slice_mask(starts, ends, <span class="built_in">len</span>(residue_feature))</span><br><span class="line">residue_feature = residue_feature[mask]</span><br><span class="line">graph_feature = <span class="variable language_">self</span>.sequence_model.readout(graph, residue_feature)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine both models outputs</span></span><br><span class="line">node_feature = torch.cat(...)</span><br><span class="line">graph_feature = torch.cat(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> &#123;<span class="string">&quot;graph_feature&quot;</span>: graph_feature, <span class="string">&quot;node_feature&quot;</span>: node_feature&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>æå–è¾“å‡º</strong>ï¼šå¾ªç¯ç»“æŸåï¼Œåˆ†åˆ«ä»ä¸¤ä¸ªæ¨¡å‹ä¸­æå–æœ€ç»ˆçš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li><strong>è¯»å‡ºï¼ˆReadoutï¼‰</strong>ï¼šä½¿ç”¨ä¸€ä¸ªâ€œè¯»å‡ºå‡½æ•°â€ï¼ˆå¦‚æ±‚å’Œæˆ–å¹³å‡ï¼‰å°†èŠ‚ç‚¹çº§åˆ«çš„ç‰¹å¾èšåˆæˆä¸€ä¸ªä»£è¡¨æ•´ä¸ªè›‹ç™½è´¨çš„å›¾çº§åˆ«ç‰¹å¾ã€‚</li>
<li><strong>æœ€ç»ˆç»„åˆ</strong>ï¼šå°†æ¥è‡ªåºåˆ—æ¨¡å‹å’Œç»“æ„æ¨¡å‹çš„èŠ‚ç‚¹ç‰¹å¾ï¼ˆ<code>node_feature</code>ï¼‰å’Œå›¾ç‰¹å¾ï¼ˆ<code>graph_feature</code>ï¼‰åˆ†åˆ«æ‹¼æ¥ï¼ˆconcatenateï¼‰èµ·æ¥ã€‚</li>
<li><strong>è¿”å›ç»“æœ</strong>ï¼šè¿”å›ä¸€ä¸ªåŒ…å«ç»„åˆåç‰¹å¾çš„å­—å…¸ï¼Œå¯ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åŠŸèƒ½é¢„æµ‹ã€å±æ€§å›å½’ç­‰ï¼‰ã€‚</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/26/5120C4-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/26/5120C4-2/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>
              

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-26 21:00:00 / ä¿®æ”¹æ—¶é—´ï¼š20:48:45" itemprop="dateCreated datePublished" datetime="2025-09-26T21:00:00+08:00">2025-09-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - è®¡ç®—èƒ½æºææ–™å’Œç”µå­ç»“æ„æ¨¡æ‹Ÿ Lecture-4</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="monte-carlo-mc-method">1 Monte Carlo (MC) Method:</h2>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>This whiteboard provides a concise but detailed overview of two
important and related simulation techniques in computational physics and
chemistry: the Metropolis Monte Carlo (MC) method and Hamiltonian (or
Hybrid) Monte Carlo (HMC). Here is a detailed breakdown of the concepts
presented.</p>
<h3 id="metropolis-monte-carlo-mc-method">1. Metropolis Monte Carlo (MC)
Method</h3>
<p>The heading â€œMetropolis MC methodâ€ introduces a foundational
algorithm in statistical mechanics. Metropolis Monte Carlo is a method
used to generate a sequence of states for a system, allowing for the
calculation of average properties. å·¦ä¸Šè§’çš„è¿™ä¸€éƒ¨åˆ†ä»‹ç»äº†åŸºç¡€çš„
<strong>Metropolis Monte Carlo</strong>
ç®—æ³•ã€‚å®ƒæ˜¯ä¸€ç§ç”ŸæˆçŠ¶æ€åºåˆ—çš„æ–¹æ³•ï¼Œä½¿å¾—å¤„äºä»»ä½•çŠ¶æ€çš„æ¦‚ç‡éƒ½ç¬¦åˆæœŸæœ›çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆåœ¨ç‰©ç†å­¦ä¸­é€šå¸¸æ˜¯ç»å°”å…¹æ›¼åˆ†å¸ƒï¼‰ã€‚</p>
<ul>
<li><strong>Conceptual Diagram:</strong> The small box with numbered
sites (0-5) and an arrow showing a move from state 0 to 2, and then to
3, illustrates a â€œrandom walk.â€ In Metropolis MC, the system transitions
from one state to another by making small, random changes.
å°æ–¹æ¡†ä¸­æ ‡æœ‰ç¼–å·çš„ä½ç‚¹ï¼ˆ0-5ï¼‰ï¼Œç®­å¤´è¡¨ç¤ºä»çŠ¶æ€ 0 åˆ°çŠ¶æ€ 2ï¼Œå†åˆ°çŠ¶æ€ 3
çš„ç§»åŠ¨ï¼Œä»£è¡¨â€œéšæœºæ¸¸èµ°â€ã€‚åœ¨ Metropolis MC
ä¸­ï¼Œç³»ç»Ÿé€šè¿‡è¿›è¡Œå¾®å°çš„éšæœºå˜åŒ–ä»ä¸€ä¸ªçŠ¶æ€è¿‡æ¸¡åˆ°å¦ä¸€ä¸ªçŠ¶æ€ã€‚</li>
<li><strong>Random Number Generation:</strong> The notation
<code>rand t \in (0,1)</code> indicates the use of a random number <span
class="math inline">\(t\)</span> drawn from a uniform distribution
between 0 and 1. This is a core component of the algorithm, used to
decide whether to accept or reject a proposed new state. ç¬¦å·
<code>rand t \in (0,1)</code> è¡¨ç¤ºä½¿ç”¨ä» 0 åˆ° 1
ä¹‹é—´çš„å‡åŒ€åˆ†å¸ƒä¸­æŠ½å–çš„éšæœºæ•° <span
class="math inline">\(t\)</span>ã€‚è¿™æ˜¯ç®—æ³•çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œç”¨äºå†³å®šæ˜¯å¦æ¥å—æˆ–æ‹’ç»æè®®çš„æ–°çŠ¶æ€ã€‚</li>
<li><strong>Detailed Balance Condition:</strong> The equation <span
class="math inline">\(P_o T(o \to n) = P_n T(n \to o)\)</span> is the
principle of detailed balance. It states that in a system at
equilibrium, the probability of being in an old state (<span
class="math inline">\(o\)</span>) and transitioning to a new state
(<span class="math inline">\(n\)</span>) is equal to the probability of
being in the new state and transitioning back to the old one. This
condition is crucial because it ensures that the simulation will
eventually sample states according to their correct thermodynamic
probabilities (the Boltzmann distribution). æ–¹ç¨‹ <span
class="math inline">\(P_o T(o \to n) = P_n T(n \to o)\)</span>
æ˜¯è¯¦ç»†å¹³è¡¡çš„åŸç†ã€‚å®ƒæŒ‡å‡ºï¼Œåœ¨å¹³è¡¡ç³»ç»Ÿä¸­ï¼Œå¤„äºæ—§çŠ¶æ€ (<span
class="math inline">\(o\)</span>) å¹¶è½¬å˜ä¸ºæ–°çŠ¶æ€ (<span
class="math inline">\(n\)</span>)
çš„æ¦‚ç‡ç­‰äºå¤„äºæ–°çŠ¶æ€å¹¶è½¬å˜å›æ—§çŠ¶æ€çš„æ¦‚ç‡ã€‚æ­¤æ¡ä»¶è‡³å…³â€‹â€‹é‡è¦ï¼Œå› ä¸ºå®ƒç¡®ä¿æ¨¡æ‹Ÿæœ€ç»ˆå°†æ ¹æ®æ­£ç¡®çš„çƒ­åŠ›å­¦æ¦‚ç‡ï¼ˆç»å°”å…¹æ›¼åˆ†å¸ƒï¼‰å¯¹çŠ¶æ€è¿›è¡Œé‡‡æ ·ã€‚</li>
<li><strong>Acceptance Rate:</strong> The note <code>\sim 30\%?</code>
likely refers to the target <strong>acceptance rate</strong> for an
efficient Metropolis MC simulation. If new states are accepted too often
or too rarely, the exploration of the systemâ€™s possible configurations
is inefficient. While the famous optimal acceptance rate for certain
high-dimensional problems is around 23.4%, a range of 20-50% is often
considered effective. æ³¨é‡Šâ€œ30%ï¼Ÿâ€æŒ‡çš„æ˜¯é«˜æ•ˆ Metropolis
è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿçš„ç›®æ ‡<strong>æ¥å—ç‡</strong>ã€‚å¦‚æœæ–°çŠ¶æ€æ¥å—è¿‡äºé¢‘ç¹æˆ–è¿‡äºç¨€å°‘ï¼Œç³»ç»Ÿå¯¹å¯èƒ½é…ç½®çš„æ¢ç´¢å°±ä¼šå˜å¾—ä½æ•ˆã€‚è™½ç„¶æŸäº›é«˜ç»´é—®é¢˜çš„æœ€ä½³æ¥å—ç‡çº¦ä¸º
23.4%ï¼Œä½†é€šå¸¸è®¤ä¸º 20-50% çš„èŒƒå›´æ˜¯æœ‰æ•ˆçš„ã€‚</li>
</ul>
<h3 id="hamiltonian-hybrid-monte-carlo-hmc">2. Hamiltonian / Hybrid
Monte Carlo (HMC)</h3>
<p>The second topic, â€œHamiltonian/Hybrid MC (HMC),â€ is a more advanced
Monte Carlo method that uses principles from classical mechanics to
propose new states more intelligently than the simple random-walk
approach of the standard Metropolis method. This often leads to a much
higher acceptance rate and more efficient exploration of the state
space. ç¬¬äºŒä¸ªä¸»é¢˜â€œå“ˆå¯†é¡¿/æ··åˆè’™ç‰¹å¡ç½—
(HMC)â€æ˜¯ä¸€ç§æ›´å…ˆè¿›çš„è’™ç‰¹å¡ç½—æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ç»å…¸åŠ›å­¦åŸç†ï¼Œæ¯”æ ‡å‡† Metropolis
æ–¹æ³•ä¸­ç®€å•çš„éšæœºæ¸¸èµ°æ–¹æ³•æ›´æ™ºèƒ½åœ°æå‡ºæ–°çŠ¶æ€ã€‚è¿™é€šå¸¸ä¼šå¸¦æ¥æ›´é«˜çš„æ¥å—ç‡å’Œæ›´é«˜æ•ˆçš„çŠ¶æ€ç©ºé—´æ¢ç´¢ã€‚</p>
<p>The whiteboard outlines a four-step HMC algorithm:</p>
<p><strong>Step 1: Randomize Velocities</strong> The first step is to
randomize the velocities: <span class="math inline">\(\vec{v}_i \sim
\mathcal{N}(0, k_B T)\)</span>. ç¬¬ä¸€æ­¥æ˜¯éšæœºåŒ–é€Ÿåº¦ï¼š<span
class="math inline">\(\vec{v}_i \sim \mathcal{N}(0, k_B T)\)</span>ã€‚ *
This step introduces momentum into the system. For each particle <span
class="math inline">\(i\)</span>, a velocity vector <span
class="math inline">\(\vec{v}_i\)</span> is randomly drawn from a normal
(Gaussian) distribution with a mean of 0 and a variance related to the
temperature <span class="math inline">\(T\)</span> and the Boltzmann
constant <span class="math inline">\(k_B\)</span>.
æ­¤æ­¥éª¤å°†åŠ¨é‡å¼•å…¥ç³»ç»Ÿã€‚å¯¹äºæ¯ä¸ªç²’å­ <span
class="math inline">\(i\)</span>ï¼Œé€Ÿåº¦çŸ¢é‡ <span
class="math inline">\(\vec{v}_i\)</span>
ä¼šéšæœºåœ°ä»æ­£æ€ï¼ˆé«˜æ–¯ï¼‰åˆ†å¸ƒä¸­æŠ½å–ï¼Œè¯¥åˆ†å¸ƒçš„å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸æ¸©åº¦ <span
class="math inline">\(T\)</span> å’Œç»å°”å…¹æ›¼å¸¸æ•° <span
class="math inline">\(k_B\)</span> ç›¸å…³ã€‚ * The full formula for this
probability distribution, <span
class="math inline">\(f(\vec{v})\)</span>, is the
<strong>Maxwell-Boltzmann distribution</strong>, which is written out
further down the board. è¯¥æ¦‚ç‡åˆ†å¸ƒçš„å®Œæ•´å…¬å¼ <span
class="math inline">\(f(\vec{v})\)</span>
æ˜¯<strong>éº¦å…‹æ–¯éŸ¦-ç»å°”å…¹æ›¼åˆ†å¸ƒ</strong>ã€‚</p>
<p><strong>Step 2: Molecular Dynamics (MD) Integration</strong> The
board notes this as <code>t=0 \to h \text&#123; or &#125; mh</code>
<code>MD</code> and mentions the <code>Verlet</code> algorithm.</p>
<ul>
<li>This is the â€œHamiltonian dynamicsâ€ part of the algorithm. Starting
from the current positions and the newly randomized velocities, the
systemâ€™s trajectory is calculated for a short period of time (<span
class="math inline">\(h\)</span> or <span
class="math inline">\(mh\)</span>) using Molecular Dynamics (MD).
è¿™æ˜¯ç®—æ³•çš„â€œå“ˆå¯†é¡¿åŠ¨åŠ›å­¦â€éƒ¨åˆ†ã€‚ä»å½“å‰ä½ç½®å’Œæ–°éšæœºåŒ–çš„é€Ÿåº¦å¼€å§‹ï¼Œä½¿ç”¨åˆ†å­åŠ¨åŠ›å­¦
(MD) è®¡ç®—ç³»ç»Ÿåœ¨çŸ­æ—¶é—´å†…ï¼ˆ<span class="math inline">\(h\)</span> æˆ– <span
class="math inline">\(mh\)</span>ï¼‰çš„è½¨è¿¹ã€‚</li>
<li>The name <strong>Verlet</strong> refers to the Verlet integration
algorithm, a numerical method used to solve Newtonâ€™s equations of
motion. It is popular in MD simulations because it is time-reversible
and conserves energy well over long simulations. æŒ‡çš„æ˜¯ Verlet
ç§¯åˆ†ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ±‚è§£ç‰›é¡¿è¿åŠ¨æ–¹ç¨‹çš„æ•°å€¼æ–¹æ³•ã€‚å®ƒåœ¨ MD
æ¨¡æ‹Ÿä¸­å¾ˆå—æ¬¢è¿ï¼Œå› ä¸ºå®ƒå…·æœ‰æ—¶é—´å¯é€†æ€§ï¼Œå¹¶ä¸”åœ¨é•¿æ—¶é—´æ¨¡æ‹Ÿä¸­èƒ½é‡å®ˆæ’æ•ˆæœè‰¯å¥½ã€‚</li>
</ul>
<p><strong>Step 3: Calculate Total Energy</strong> The third step is to
<code>calculate total energy</code>: <span class="math inline">\(E_n =
K_n + V_n\)</span>. ç¬¬ä¸‰æ­¥æ˜¯â€œè®¡ç®—æ€»èƒ½é‡â€ï¼š<span
class="math inline">\(E_n = K_n + V_n\)</span>ã€‚ * After the MD
trajectory, the system is in a new state <span
class="math inline">\(n\)</span>. The total energy of this new state,
<span class="math inline">\(E_n\)</span>, is calculated as the sum of
its kinetic energy (<span class="math inline">\(K_n\)</span>, from the
velocities) and its potential energy (<span
class="math inline">\(V_n\)</span>, from the positions). MD
è½¨è¿¹ä¹‹åï¼Œç³»ç»Ÿå¤„äºæ–°çŠ¶æ€ <span
class="math inline">\(n\)</span>ã€‚æ–°çŠ¶æ€çš„æ€»èƒ½é‡ <span
class="math inline">\(E_n\)</span> ç­‰äºå…¶åŠ¨èƒ½ (<span
class="math inline">\(K_n\)</span>ï¼Œç”±é€Ÿåº¦è®¡ç®—å¾—å‡ºï¼‰å’ŒåŠ¿èƒ½ (<span
class="math inline">\(V_n\)</span>ï¼Œç”±ä½ç½®è®¡ç®—å¾—å‡º)ä¹‹å’Œã€‚</p>
<p><strong>Step 4: Acceptance Test</strong> The final step is the
acceptance criterion: <span class="math inline">\(\text{acc}(o \to n) =
\min(1, e^{-\beta(E_n - E_o)})\)</span>. æœ€åä¸€æ­¥æ˜¯éªŒæ”¶æ ‡å‡†ï¼š<span
class="math inline">\(\text{acc}(o \to n) = \min(1, e^{-\beta(E_n -
E_o)})\)</span>ã€‚ * This is the Metropolis acceptance criterion. The
algorithm decides whether to accept the new state <span
class="math inline">\(n\)</span> or reject it and stay in the old state
<span class="math inline">\(o\)</span>. è¿™æ˜¯ Metropolis
éªŒæ”¶æ ‡å‡†ã€‚ç®—æ³•å†³å®šæ˜¯æ¥å—æ–°çŠ¶æ€ <span class="math inline">\(n\)</span>
è¿˜æ˜¯æ‹’ç»å®ƒå¹¶ä¿æŒæ—§çŠ¶æ€ <span class="math inline">\(o\)</span>ã€‚ * The
probability of acceptance depends on the change in total energy (<span
class="math inline">\(E_n - E_o\)</span>). If the new energy is lower,
the move is always accepted. If the new energy is higher, it might still
be accepted with a probability <span class="math inline">\(e^{-\beta(E_n
- E_o)}\)</span>, where <span class="math inline">\(\beta = 1/(k_B
T)\)</span>. This allows the system to escape from local energy minima.
éªŒæ”¶æ¦‚ç‡å–å†³äºæ€»èƒ½é‡çš„å˜åŒ– (<span class="math inline">\(E_n -
E_o\)</span>)ã€‚å¦‚æœæ–°èƒ½é‡è¾ƒä½ï¼Œåˆ™å§‹ç»ˆæ¥å—è¯¥ç§»åŠ¨ã€‚å¦‚æœæ–°çš„èƒ½é‡æ›´é«˜ï¼Œå®ƒä»ç„¶å¯èƒ½ä»¥æ¦‚ç‡
<span class="math inline">\(e^{-\beta(E_n - E_o)}\)</span> è¢«æ¥å—ï¼Œå…¶ä¸­
<span class="math inline">\(\beta = 1/(k_B
T)\)</span>ã€‚è¿™ä½¿å¾—ç³»ç»Ÿèƒ½å¤Ÿæ‘†è„±å±€éƒ¨èƒ½é‡æœ€å°å€¼ã€‚</p>
<h3 id="key-formulas-and-notations">Key Formulas and Notations</h3>
<ul>
<li><p><strong>Maxwell-Boltzmann
Distributionéº¦å…‹æ–¯éŸ¦-ç»å°”å…¹æ›¼åˆ†å¸ƒ:</strong> The formula for the velocity
distribution is given as: <span class="math inline">\(f(\vec{v}) =
\left(\frac{m}{2\pi k_B T}\right)^{3/2} \exp\left(-\frac{m v^2}{2 k_B
T}\right)\)</span> This gives the probability density for a particle of
mass <span class="math inline">\(m\)</span> to have a velocity <span
class="math inline">\(\vec{v}\)</span> at a given temperature <span
class="math inline">\(T\)</span>.è´¨é‡ä¸º <span
class="math inline">\(m\)</span> çš„ç²’å­é€Ÿåº¦ä¸º çš„æ¦‚ç‡å¯†åº¦</p></li>
<li><p><strong>Energy Conservation and Acceptance Rate:</strong> The
notes <span class="math inline">\(E_n \approx E_o\)</span> and <span
class="math inline">\(75\%\)</span> highlight a key advantage of HMC.
Because the Verlet integrator approximately conserves energy, the final
energy <span class="math inline">\(E_n\)</span> after the MD trajectory
is usually very close to the initial energy <span
class="math inline">\(E_o\)</span>. This means the term <span
class="math inline">\((E_n - E_o)\)</span> is small, and the acceptance
probability is high. The <span class="math inline">\(75\%\)</span>
indicates a typical or target acceptance rate for HMC, which is
significantly higher than for standard Metropolis MC. æ³¨é‡Š <span
class="math inline">\(E_n \approx E_o\)</span> å’Œ <span
class="math inline">\(75\%\)</span> å‡¸æ˜¾äº† HMC çš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿ã€‚ç”±äº
Verlet ç§¯åˆ†å™¨è¿‘ä¼¼åœ°å®ˆæ’èƒ½é‡ï¼ŒMD è½¨è¿¹åçš„æœ€ç»ˆèƒ½é‡ <span
class="math inline">\(E_n\)</span> é€šå¸¸éå¸¸æ¥è¿‘åˆå§‹èƒ½é‡ <span
class="math inline">\(E_o\)</span>ã€‚è¿™æ„å‘³ç€ <span
class="math inline">\((E_n - E_o)\)</span> é¡¹å¾ˆå°ï¼Œæ¥å—æ¦‚ç‡å¾ˆé«˜ã€‚<span
class="math inline">\(75\%\)</span> è¡¨ç¤º HMC
çš„å…¸å‹æˆ–ç›®æ ‡æ¥å—ç‡ï¼Œæ˜æ˜¾é«˜äºæ ‡å‡† Metropolis MCã€‚</p></li>
<li><p><strong>Hamiltonian Operator:</strong> The symbol <span
class="math inline">\(\hat{H}\)</span> written on the adjacent board
represents the Hamiltonian operator, which gives the total energy of the
system. The note <code>Î” Adiabatic</code> suggests that the MD evolution
is ideally an adiabatic process (no heat exchange), during which the
total energy (the Hamiltonian) is conserved. ç›¸é‚»æ¿ä¸Šçš„ç¬¦å· <span
class="math inline">\(\hat{H}\)</span>
ä»£è¡¨å“ˆå¯†é¡¿ç®—ç¬¦ï¼Œå®ƒç»™å‡ºäº†ç³»ç»Ÿçš„æ€»èƒ½é‡ã€‚æ³¨é‡Šâ€œÎ” Adiabaticâ€è¡¨æ˜ MD
æ¼”åŒ–åœ¨ç†æƒ³æƒ…å†µä¸‹æ˜¯ä¸€ä¸ªç»çƒ­è¿‡ç¨‹ï¼ˆæ— çƒ­äº¤æ¢ï¼‰ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­æ€»èƒ½é‡ï¼ˆå“ˆå¯†é¡¿é‡ï¼‰å®ˆæ’ã€‚</p></li>
</ul>
<p>This whiteboard displays the fundamental equation of quantum
chemistry: the time-dependent SchrÃ¶dinger equation, along with the
detailed breakdown of the molecular Hamiltonian operator. This equation
is the starting point for almost all <em>ab initio</em>
(first-principles) quantum mechanical calculations of molecular systems.
è¿™å—ç™½æ¿å±•ç¤ºäº†é‡å­åŒ–å­¦çš„åŸºæœ¬æ–¹ç¨‹ï¼šå«æ—¶è–›å®šè°”æ–¹ç¨‹ï¼Œä»¥åŠåˆ†å­å“ˆå¯†é¡¿ç®—ç¬¦çš„è¯¦ç»†åˆ†è§£ã€‚è¯¥æ–¹ç¨‹æ˜¯å‡ ä¹æ‰€æœ‰åˆ†å­ç³»ç»Ÿ<em>ä»å¤´ç®—</em>ï¼ˆç¬¬ä¸€æ€§åŸç†ï¼‰é‡å­åŠ›å­¦è®¡ç®—çš„èµ·ç‚¹ã€‚</p>
<h3 id="the-time-dependent-schrÃ¶dinger-equation">3. The Time-Dependent
SchrÃ¶dinger Equation</h3>
<p>At the top of the board, the fundamental equation governing the
evolution of a quantum mechanical system is presented:
ç™½æ¿é¡¶éƒ¨æ˜¾ç¤ºäº†æ§åˆ¶é‡å­åŠ›å­¦ç³»ç»Ÿæ¼”åŒ–çš„åŸºæœ¬æ–¹ç¨‹ï¼š <span
class="math inline">\(i\hbar \frac{\partial \Psi}{\partial t} =
\hat{\mathcal{H}} \Psi\)</span></p>
<ul>
<li><p><strong><span class="math inline">\(\Psi\)</span> (Psi)</strong>
is the <strong>wave function</strong> of the system. It contains all the
information that can be known about the system (e.g., the positions and
momenta of all particles).
æ˜¯ç³»ç»Ÿçš„<strong>æ³¢å‡½æ•°</strong>ã€‚å®ƒåŒ…å«äº†å…³äºç³»ç»Ÿçš„æ‰€æœ‰å·²çŸ¥ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œæ‰€æœ‰ç²’å­çš„ä½ç½®å’ŒåŠ¨é‡ï¼‰ã€‚</p></li>
<li><p><strong><span
class="math inline">\(\hat{\mathcal{H}}\)</span></strong> is the
<strong>Hamiltonian operator</strong>, which represents the total energy
of the system.
æ˜¯<strong>å“ˆå¯†é¡¿ç®—ç¬¦</strong>ï¼Œè¡¨ç¤ºç³»ç»Ÿçš„æ€»èƒ½é‡ã€‚</p></li>
<li><p><strong><span class="math inline">\(i\)</span></strong>
æ˜¯è™šæ•°å•ä½ã€‚</p></li>
<li><p><strong><span class="math inline">\(i\)</span></strong> is the
imaginary unit.</p></li>
<li><p><strong><span class="math inline">\(\hbar\)</span></strong> is
the <strong>reduced Planck
constant</strong>.æ˜¯<strong>çº¦åŒ–æ™®æœ—å…‹å¸¸æ•°</strong>ã€‚</p></li>
<li><p><strong><span class="math inline">\(\frac{\partial \Psi}{\partial
t}\)</span></strong> represents how the wave function changes over
time.è¡¨ç¤ºæ³¢å‡½æ•°éšæ—¶é—´çš„å˜åŒ–ã€‚</p></li>
</ul>
<p>This equation states that the time evolution of the quantum state is
dictated by the systemâ€™s total energy operator, the Hamiltonian. The
note â€œÎ” Adiabatic processâ€ likely connects to the context of the
Born-Oppenheimer approximation, where the electronic SchrÃ¶dinger
equation is solved for fixed nuclear positions, assuming the electrons
adjust adiabatically (instantaneously) to the motion of the nuclei.
è¯¥æ–¹ç¨‹è¡¨æ˜ï¼Œé‡å­æ€çš„æ—¶é—´æ¼”åŒ–ç”±ç³»ç»Ÿçš„æ€»èƒ½é‡ç®—ç¬¦â€”â€”å“ˆå¯†é¡¿ç®—ç¬¦å†³å®šã€‚æ³¨é‡Šâ€œÎ”ç»çƒ­è¿‡ç¨‹â€ä¸ç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼ç›¸å…³ï¼Œåœ¨è¯¥è¿‘ä¼¼ä¸­ï¼Œç”µå­è–›å®šè°”æ–¹ç¨‹æ˜¯é’ˆå¯¹å›ºå®šåŸå­æ ¸ä½ç½®æ±‚è§£çš„ï¼Œå‡è®¾ç”µå­ä»¥ç»çƒ­æ–¹å¼ï¼ˆç¬æ—¶ï¼‰è°ƒæ•´ä»¥é€‚åº”åŸå­æ ¸çš„è¿åŠ¨ã€‚</p>
<h3 id="the-full-molecular-hamiltonian-hatmathcalh">4. The Full
Molecular Hamiltonian (<span
class="math inline">\(\hat{\mathcal{H}}\)</span>)</h3>
<p>The main part of the whiteboard is the detailed expression for the
non-relativistic, time-independent molecular Hamiltonian. It is the sum
of the kinetic and potential energies of all the nuclei and electrons in
the system. The equation can be broken down into five distinct terms:
ç™½æ¿çš„ä¸»è¦éƒ¨åˆ†æ˜¯éç›¸å¯¹è®ºæ€§ã€æ—¶é—´æ— å…³çš„åˆ†å­å“ˆå¯†é¡¿é‡çš„è¯¦ç»†è¡¨è¾¾å¼ã€‚å®ƒæ˜¯ç³»ç»Ÿä¸­æ‰€æœ‰åŸå­æ ¸å’Œç”µå­çš„åŠ¨èƒ½å’ŒåŠ¿èƒ½ä¹‹å’Œã€‚</p>
<p>è¯¥æ–¹ç¨‹å¯ä»¥åˆ†è§£ä¸ºäº”ä¸ªä¸åŒçš„é¡¹ï¼š</p>
<p><span class="math inline">\(\hat{\mathcal{H}} = -\sum_{I=1}^{P}
\frac{\hbar^2}{2M_I}\nabla_I^2 - \sum_{i=1}^{N}
\frac{\hbar^2}{2m}\nabla_i^2 + \frac{e^2}{2}\sum_{I=1}^{P}\sum_{J \neq
I}^{P} \frac{Z_I Z_J}{|\vec{R}_I - \vec{R}_J|} +
\frac{e^2}{2}\sum_{i=1}^{N}\sum_{j \neq i}^{N} \frac{1}{|\vec{r}_i -
\vec{r}_j|} - e^2\sum_{I=1}^{P}\sum_{i=1}^{N} \frac{Z_I}{|\vec{R}_I -
\vec{r}_i|}\)</span></p>
<p>Letâ€™s analyze each component:</p>
<p><strong>A. Kinetic Energy Terms åŠ¨èƒ½é¡¹</strong></p>
<ol type="1">
<li><strong>Kinetic Energy of the Nuclei åŸå­æ ¸çš„åŠ¨èƒ½:</strong> <span
class="math inline">\(-\sum_{I=1}^{P}
\frac{\hbar^2}{2M_I}\nabla_I^2\)</span> This term is the sum of the
kinetic energy operators for all the nuclei in the
system.æ­¤é¡¹æ˜¯ç³»ç»Ÿä¸­æ‰€æœ‰åŸå­æ ¸çš„åŠ¨èƒ½ç®—ç¬¦ä¹‹å’Œã€‚
<ul>
<li>The sum is over all nuclei, indexed by <span
class="math inline">\(I\)</span> from 1 to <span
class="math inline">\(P\)</span>.è¯¥å’Œæ¶µç›–æ‰€æœ‰åŸå­æ ¸ï¼Œç´¢å¼•ä¸º <span
class="math inline">\(I\)</span>ï¼Œä» 1 åˆ° <span
class="math inline">\(P\)</span>ã€‚</li>
<li><span class="math inline">\(M_I\)</span> is the mass of nucleus
<span class="math inline">\(I\)</span>.æ˜¯åŸå­æ ¸ <span
class="math inline">\(I\)</span> çš„è´¨é‡ã€‚</li>
<li><span class="math inline">\(\nabla_I^2\)</span> is the Laplacian
operator, which involves the second spatial derivatives with respect to
the coordinates of nucleus <span
class="math inline">\(I\)</span>.æ˜¯æ‹‰æ™®æ‹‰æ–¯ç®—ç¬¦ï¼Œå®ƒæ¶‰åŠåŸå­æ ¸ <span
class="math inline">\(I\)</span> åæ ‡çš„äºŒé˜¶ç©ºé—´å¯¼æ•°ã€‚</li>
</ul></li>
<li><strong>Kinetic Energy of the Electrons ç”µå­çš„åŠ¨èƒ½:</strong> <span
class="math inline">\(-\sum_{i=1}^{N}
\frac{\hbar^2}{2m}\nabla_i^2\)</span> This is the corresponding sum of
the kinetic energy operators for all the
electrons.è¿™æ˜¯æ‰€æœ‰ç”µå­çš„åŠ¨èƒ½ç®—ç¬¦çš„å¯¹åº”å’Œã€‚
<ul>
<li>The sum is over all electrons, indexed by <span
class="math inline">\(i\)</span> from 1 to <span
class="math inline">\(N\)</span>.è¯¥å’Œæ˜¯é’ˆå¯¹æ‰€æœ‰ç”µå­çš„ï¼Œç´¢å¼•ä¸º <span
class="math inline">\(i\)</span>ï¼Œä» 1 åˆ° <span
class="math inline">\(N\)</span>ã€‚</li>
<li><span class="math inline">\(m\)</span> is the mass of an
electron.æ˜¯ç”µå­çš„è´¨é‡ã€‚</li>
<li><span class="math inline">\(\nabla_i^2\)</span> is the Laplacian
operator with respect to the coordinates of electron <span
class="math inline">\(i\)</span>.æ˜¯å…³äºç”µå­ <span
class="math inline">\(i\)</span> åæ ‡çš„æ‹‰æ™®æ‹‰æ–¯ç®—ç¬¦ã€‚</li>
</ul></li>
</ol>
<p><strong>B. Potential Energy Terms (Electrostatic Interactions)
åŠ¿èƒ½é¡¹ï¼ˆé™ç”µç›¸äº’ä½œç”¨ï¼‰</strong></p>
<ol start="3" type="1">
<li><strong>Nuclear-Nuclear Repulsion æ ¸é—´æ’æ–¥åŠ›:</strong> <span
class="math inline">\(+\frac{e^2}{2}\sum_{I=1}^{P}\sum_{J \neq I}^{P}
\frac{Z_I Z_J}{|\vec{R}_I - \vec{R}_J|}\)</span> This term represents
the potential energy from the electrostatic (Coulomb) repulsion between
all pairs of positively charged
nuclei.è¯¥é¡¹è¡¨ç¤ºæ‰€æœ‰å¸¦æ­£ç”µåŸå­æ ¸å¯¹ä¹‹é—´é™ç”µï¼ˆåº“ä»‘ï¼‰æ’æ–¥åŠ›äº§ç”Ÿçš„åŠ¿èƒ½ã€‚
<ul>
<li>The double summation runs over all unique pairs of nuclei (<span
class="math inline">\(I, J\)</span>).å¯¹æ‰€æœ‰å”¯ä¸€çš„åŸå­æ ¸å¯¹ (<span
class="math inline">\(I, J\)</span>) è¿›è¡ŒåŒé‡æ±‚å’Œã€‚</li>
<li><span class="math inline">\(Z_I\)</span> is the atomic number (i.e.,
the charge) of nucleus <span class="math inline">\(I\)</span>.æ˜¯åŸå­æ ¸
<span class="math inline">\(I\)</span> çš„åŸå­åºæ•°ï¼ˆå³ç”µè·ï¼‰ã€‚</li>
<li><span class="math inline">\(\vec{R}_I\)</span> is the position
vector of nucleus <span class="math inline">\(I\)</span>.æ˜¯åŸå­æ ¸ <span
class="math inline">\(I\)</span> çš„ä½ç½®çŸ¢é‡ã€‚</li>
<li><span class="math inline">\(e\)</span> is the elementary
charge.æ˜¯åŸºæœ¬ç”µè·ã€‚</li>
</ul></li>
<li><strong>Electron-Electron Repulsion ç”µå­é—´æ’æ–¥åŠ›:</strong> <span
class="math inline">\(+\frac{e^2}{2}\sum_{i=1}^{N}\sum_{j \neq i}^{N}
\frac{1}{|\vec{r}_i - \vec{r}_j|}\)</span> This term represents the
potential energy from the electrostatic repulsion between all pairs of
negatively charged
electrons.è¯¥é¡¹è¡¨ç¤ºæ‰€æœ‰å¸¦è´Ÿç”µçš„ç”µå­å¯¹ä¹‹é—´é™ç”µæ’æ–¥çš„åŠ¿èƒ½ã€‚
<ul>
<li>The double summation runs over all unique pairs of electrons (<span
class="math inline">\(i, j\)</span>).å¯¹æ‰€æœ‰ä¸åŒçš„ç”µå­å¯¹ (<span
class="math inline">\(i, j\)</span>) è¿›è¡ŒåŒé‡æ±‚å’Œã€‚</li>
<li><span class="math inline">\(\vec{r}_i\)</span> is the position
vector of electron <span class="math inline">\(i\)</span>.æ˜¯ç”µå­ <span
class="math inline">\(i\)</span> çš„ä½ç½®çŸ¢é‡ã€‚</li>
</ul></li>
<li><strong>Nuclear-Electron Attraction æ ¸-ç”µå­å¼•åŠ›:</strong> <span
class="math inline">\(-e^2\sum_{I=1}^{P}\sum_{i=1}^{N}
\frac{Z_I}{|\vec{R}_I - \vec{r}_i|}\)</span> This final term represents
the potential energy from the electrostatic attraction between the
nuclei and the electrons.è¿™æœ€åä¸€é¡¹è¡¨ç¤ºåŸå­æ ¸å’Œç”µå­ä¹‹é—´é™ç”µå¼•åŠ›çš„åŠ¿èƒ½ã€‚
<ul>
<li>The summation runs over all nuclei and all
electrons.è¯¥æ±‚å’Œé€‚ç”¨äºæ‰€æœ‰åŸå­æ ¸å’Œæ‰€æœ‰ç”µå­ã€‚</li>
</ul></li>
</ol>
<h3 id="notations-and-conventions">5. Notations and Conventions</h3>
<ul>
<li><strong>Atomic Units:</strong> The note <span
class="math inline">\(\frac{1}{4\pi\epsilon_0} = k = 1\)</span> is a key
indicator of the convention being used. This sets the Coulomb constant
to 1, which is a hallmark of <strong>Hartree atomic units</strong>. In
this system, the elementary charge (<span
class="math inline">\(e\)</span>), electron mass (<span
class="math inline">\(m\)</span>), and reduced Planck constant (<span
class="math inline">\(\hbar\)</span>) are also set to 1. This simplifies
the Hamiltonian significantly, removing the physical constants and
making the equations easier to work with computationally.
æ˜¯æ‰€ç”¨çº¦å®šçš„å…³é”®æŒ‡æ ‡ã€‚è¿™å°†åº“ä»‘å¸¸æ•°è®¾ç½®ä¸º 1ï¼Œè¿™æ˜¯<strong>Hartree
åŸå­å•ä½</strong>çš„æ ‡å¿—ã€‚åœ¨è¿™ä¸ªç³»ç»Ÿä¸­ï¼ŒåŸºæœ¬ç”µè· (<span
class="math inline">\(e\)</span>)ã€ç”µå­è´¨é‡ (<span
class="math inline">\(m\)</span>) å’Œâ€‹â€‹çº¦åŒ–æ™®æœ—å…‹å¸¸æ•° (<span
class="math inline">\(\hbar\)</span>) ä¹Ÿè®¾ä¸º
1ã€‚è¿™æ˜¾è‘—ç®€åŒ–äº†å“ˆå¯†é¡¿é‡ï¼Œæ¶ˆé™¤äº†ç‰©ç†å¸¸æ•°ï¼Œä½¿æ–¹ç¨‹æ›´æ˜“äºè®¡ç®—ã€‚</li>
<li><strong>Interaction Terms:</strong> The notations <span
class="math inline">\(\{i, j\}\)</span>, <span
class="math inline">\(\{i, j, k\}\)</span>, etc., refer to the
â€œmany-bodyâ€ problem. The Hamiltonian contains two-body terms
(interactions between pairs of particles), and solving the SchrÃ¶dinger
equation exactly is extremely difficult because the motion of every
particle is correlated with every other particle. Computational methods
are designed to approximate these interactions. ç¬¦å· <span
class="math inline">\(\{i, j\}\)</span>ã€<span
class="math inline">\(\{i, j, k\}\)</span>
ç­‰æŒ‡çš„æ˜¯â€œå¤šä½“â€é—®é¢˜ã€‚å“ˆå¯†é¡¿é‡åŒ…å«äºŒä½“é¡¹ï¼ˆç²’å­å¯¹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼‰ï¼Œè€Œç²¾ç¡®æ±‚è§£è–›å®šè°”æ–¹ç¨‹æå…¶å›°éš¾ï¼Œå› ä¸ºæ¯ä¸ªç²’å­çš„è¿åŠ¨éƒ½ä¸å…¶ä»–ç²’å­ç›¸å…³ã€‚è®¡ç®—æ–¹æ³•æ—¨åœ¨è¿‘ä¼¼è¿™äº›ç›¸äº’ä½œç”¨ã€‚</li>
</ul>
<p>This whiteboard presents the mathematical foundation for
<strong>non-adiabatic molecular dynamics</strong>, a sophisticated
method in theoretical chemistry and physics used to simulate processes
where the Born-Oppenheimer approximation breaks down. This typically
occurs in photochemistry, electron transfer reactions, and when
molecules interact with intense laser fields.
è¿™å—ç™½æ¿å±•ç¤ºäº†<strong>éç»çƒ­åˆ†å­åŠ¨åŠ›å­¦</strong>çš„æ•°å­¦åŸºç¡€ï¼Œè¿™æ˜¯ç†è®ºåŒ–å­¦å’Œç‰©ç†å­¦ä¸­ä¸€ç§å¤æ‚çš„æ–¹æ³•ï¼Œç”¨äºæ¨¡æ‹Ÿç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼å¤±æ•ˆçš„è¿‡ç¨‹ã€‚è¿™é€šå¸¸å‘ç”Ÿåœ¨å…‰åŒ–å­¦ã€ç”µå­è½¬ç§»ååº”ä»¥åŠåˆ†å­ä¸å¼ºæ¿€å…‰åœºç›¸äº’ä½œç”¨æ—¶ã€‚</p>
<h3
id="topic-non-adiabatic-molecular-dynamics-md-éç»çƒ­åˆ†å­åŠ¨åŠ›å­¦-md">6.
Topic: Non-Adiabatic Molecular Dynamics (MD) éç»çƒ­åˆ†å­åŠ¨åŠ›å­¦ (MD)</h3>
<p>The title â€œÎ” non-adiabatic MDâ€ indicates that the topic moves beyond
the standard Born-Oppenheimer approximation. In this approximation, it
is assumed that the light electrons adjust instantaneously to the motion
of the heavy nuclei, allowing the system to be described by a single
potential energy surface. Non-adiabatic methods, by contrast, account
for the quantum mechanical coupling between multiple electronic
states.</p>
<p>æ ‡é¢˜â€œÎ” éç»çƒ­
MDâ€è¡¨æ˜è¯¥ä¸»é¢˜è¶…è¶Šäº†æ ‡å‡†çš„ç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼ã€‚åœ¨è¯¥è¿‘ä¼¼ä¸­ï¼Œå‡è®¾è½»ç”µå­ä¼šæ ¹æ®é‡åŸå­æ ¸çš„è¿åŠ¨è¿›è¡Œç¬æ—¶è°ƒæ•´ï¼Œä»è€Œä½¿ç³»ç»Ÿå¯ä»¥ç”¨å•ä¸ªåŠ¿èƒ½é¢æ¥æè¿°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œéç»çƒ­æ–¹æ³•åˆ™è€ƒè™‘äº†å¤šä¸ªç”µå­æ€ä¹‹é—´çš„é‡å­åŠ›å­¦è€¦åˆã€‚</p>
<h3 id="the-born-huang-ansatz-ç»æ©-é»„æ‹Ÿè®¾">7. The Born-Huang Ansatz
ç»æ©-é»„æ‹Ÿè®¾</h3>
<p>The starting point for this method is the â€œansatzâ€ (an educated guess
for the form of the solution). This is the Born-Huang expansion for the
total molecular wave function, <span
class="math inline">\(\Psi\)</span>.
è¯¥æ–¹æ³•çš„èµ·ç‚¹æ˜¯â€œæ‹Ÿè®¾â€ï¼ˆå¯¹è§£å½¢å¼çš„åˆç†çŒœæµ‹ï¼‰ã€‚è¿™æ˜¯åˆ†å­æ€»æ³¢å‡½æ•° <span
class="math inline">\(\Psi\)</span> çš„ç»æ©-é»„å±•å¼€å¼ã€‚</p>
<p><span class="math inline">\(\Psi(\vec{R}, \vec{r}, t) = \sum_{n}
\Theta_n(\vec{R}, t) \Phi_n(\vec{R}, \vec{r})\)</span></p>
<ul>
<li><p><strong><span class="math inline">\(\Psi(\vec{R}, \vec{r},
t)\)</span></strong> is the total wave function for the entire molecule.
It depends on the coordinates of all nuclei (<span
class="math inline">\(\vec{R}\)</span>), all electrons (<span
class="math inline">\(\vec{r}\)</span>), and time (<span
class="math inline">\(t\)</span>).
æ˜¯æ•´ä¸ªåˆ†å­çš„æ€»æ³¢å‡½æ•°ã€‚å®ƒå–å†³äºæ‰€æœ‰åŸå­æ ¸ (<span
class="math inline">\(\vec{R}\)</span>)ã€æ‰€æœ‰ç”µå­ (<span
class="math inline">\(\vec{r}\)</span>) å’Œæ—¶é—´ (<span
class="math inline">\(t\)</span>) çš„åæ ‡ã€‚</p></li>
<li><p><strong><span class="math inline">\(\Phi_n(\vec{R},
\vec{r})\)</span></strong> are the <strong>electronic wave
functions</strong>. They are the solutions to the electronic SchrÃ¶dinger
equation for a fixed nuclear geometry <span
class="math inline">\(\vec{R}\)</span> and form a complete basis set.
The index <span class="math inline">\(n\)</span> labels the electronic
state (e.g., ground state, first excited state, etc.).
å®ƒä»¬æ˜¯ç»™å®šåŸå­æ ¸å‡ ä½•æ„å‹ <span class="math inline">\(\vec{R}\)</span>
çš„ç”µå­è–›å®šè°”æ–¹ç¨‹çš„è§£ï¼Œå¹¶æ„æˆä¸€ä¸ªå®Œæ•´çš„åŸºç»„ã€‚ä¸‹æ ‡ <span
class="math inline">\(n\)</span>
æ ‡è®°ç”µå­æ€ï¼ˆä¾‹å¦‚ï¼ŒåŸºæ€ã€ç¬¬ä¸€æ¿€å‘æ€ç­‰ï¼‰ã€‚</p></li>
<li><p><strong><span class="math inline">\(\Theta_n(\vec{R},
t)\)</span></strong> are the <strong>nuclear wave functions</strong>.
Each <span class="math inline">\(\Theta_n\)</span> describes the motion
of the nuclei on the potential energy surface of the corresponding
electronic state, <span class="math inline">\(\Phi_n\)</span>.
Crucially, they depend on time. æ˜¯<strong>æ ¸æ³¢å‡½æ•°</strong>ã€‚æ¯ä¸ª <span
class="math inline">\(\Theta_n\)</span> æè¿°åŸå­æ ¸åœ¨ç›¸åº”ç”µå­æ€ <span
class="math inline">\(\Phi_n\)</span>
åŠ¿èƒ½é¢ä¸Šçš„è¿åŠ¨ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œå®ƒä»¬ä¾èµ–äºæ—¶é—´ã€‚</p></li>
</ul>
<p>This ansatz expresses the total molecular state as a superposition of
electronic states, where the coefficients of the superposition are the
nuclear wave functions.
è¯¥æ‹Ÿè®¾å°†æ€»åˆ†å­æ€è¡¨ç¤ºä¸ºç”µå­æ€çš„å åŠ ï¼Œå…¶ä¸­å åŠ çš„ç³»æ•°æ˜¯æ ¸æ³¢å‡½æ•°ã€‚</p>
<h3 id="the-partitioned-molecular-hamiltonian-åˆ†å‰²åˆ†å­å“ˆå¯†é¡¿é‡">8. The
Partitioned Molecular Hamiltonian åˆ†å‰²åˆ†å­å“ˆå¯†é¡¿é‡</h3>
<p>The total molecular Hamiltonian, <span
class="math inline">\(\hat{\mathcal{H}}\)</span>, is partitioned into
terms that act on the nuclei and electrons separately. æ€»åˆ†å­å“ˆå¯†é¡¿é‡
<span class="math inline">\(\hat{\mathcal{H}}\)</span>
è¢«åˆ†å‰²æˆåˆ†åˆ«ä½œç”¨äºåŸå­æ ¸å’Œç”µå­çš„é¡¹ã€‚</p>
<p><span class="math inline">\(\hat{\mathcal{H}} = -\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2 + \hat{\mathcal{H}}_e +
\hat{V}_{nn}\)</span></p>
<ul>
<li><p><strong><span class="math inline">\(-\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2\)</span></strong>: This is the kinetic
energy operator for the nuclei, often denoted as <span
class="math inline">\(\hat{T}_n\)</span>.è¿™æ˜¯åŸå­æ ¸çš„åŠ¨èƒ½ç®—ç¬¦ï¼Œé€šå¸¸è¡¨ç¤ºä¸º
<span class="math inline">\(\hat{T}_n\)</span>ã€‚</p></li>
<li><p><strong><span
class="math inline">\(\hat{\mathcal{H}}_e\)</span></strong>: This is the
<strong>electronic Hamiltonian</strong>, which includes the kinetic
energy of the electrons and the potential energy of electron-electron
and electron-nuclear interactions.
è¿™æ˜¯<strong>ç”µå­å“ˆå¯†é¡¿é‡</strong>ï¼ŒåŒ…å«ç”µå­çš„åŠ¨èƒ½ä»¥åŠç”µå­-ç”µå­å’Œç”µå­-æ ¸ç›¸äº’ä½œç”¨çš„åŠ¿èƒ½ã€‚</p></li>
<li><p><strong><span
class="math inline">\(\hat{V}_{nn}\)</span></strong>: This is the
potential energy operator for <strong>nuclear-nuclear
repulsion</strong>.è¿™æ˜¯<strong>æ ¸-æ ¸æ’æ–¥</strong>çš„åŠ¿èƒ½ç®—ç¬¦ã€‚</p></li>
</ul>
<h3 id="the-electronic-schrÃ¶dinger-equation-ç”µå­è–›å®šè°”æ–¹ç¨‹">9. The
Electronic SchrÃ¶dinger Equation ç”µå­è–›å®šè°”æ–¹ç¨‹</h3>
<p>The electronic basis functions, <span
class="math inline">\(\Phi_n\)</span>, are defined as the eigenfunctions
of the electronic Hamiltonian (plus the nuclear repulsion term) for a
fixed nuclear configuration <span
class="math inline">\(\vec{R}\)</span>. ç”µå­åŸºå‡½æ•° <span
class="math inline">\(\Phi_n\)</span> å®šä¹‰ä¸ºå¯¹äºå›ºå®šçš„æ ¸æ„å‹ <span
class="math inline">\(\vec{R}\)</span>ï¼Œç”µå­å“ˆå¯†é¡¿é‡ï¼ˆåŠ ä¸Šæ ¸æ’æ–¥é¡¹ï¼‰çš„æœ¬å¾å‡½æ•°ã€‚</p>
<p><span class="math inline">\((\hat{\mathcal{H}}_e + \hat{V}_{nn})
\Phi_n(\vec{R}, \vec{r}) = E_n(\vec{R}) \Phi_n(\vec{R},
\vec{r})\)</span></p>
<ul>
<li><strong><span class="math inline">\(E_n(\vec{R})\)</span></strong>
are the eigenvalues, which are the <strong>potential energy surfaces
(PES)</strong>. Each electronic state <span
class="math inline">\(n\)</span> has its own PES, which dictates the
forces acting on the nuclei when the molecule is in that electronic
state. æ˜¯ç‰¹å¾å€¼ï¼Œå³<strong>åŠ¿èƒ½é¢ (PES)</strong>ã€‚æ¯ä¸ªç”µå­æ€ <span
class="math inline">\(n\)</span>
éƒ½æœ‰å…¶è‡ªèº«çš„åŠ¿èƒ½é¢ï¼Œå®ƒå†³å®šäº†åˆ†å­å¤„äºè¯¥ç”µå­æ€æ—¶ä½œç”¨äºåŸå­æ ¸çš„åŠ›ã€‚</li>
</ul>
<h3
id="deriving-the-equations-of-motion-for-the-nuclei-æ¨å¯¼åŸå­æ ¸è¿åŠ¨æ–¹ç¨‹">10.
Deriving the Equations of Motion for the Nuclei æ¨å¯¼åŸå­æ ¸è¿åŠ¨æ–¹ç¨‹</h3>
<p>The final part of the whiteboard begins the derivation of the
time-dependent SchrÃ¶dinger equation for the nuclear wave functions,
<span class="math inline">\(\Theta_k\)</span>. The process starts with
the full time-dependent SchrÃ¶dinger equation, <span
class="math inline">\(i\hbar \frac{\partial \Psi}{\partial t} =
\hat{\mathcal{H}} \Psi\)</span>. To find the equation for a specific
nuclear wave function <span class="math inline">\(\Theta_k\)</span>,
this main equation is projected onto the corresponding electronic basis
state <span class="math inline">\(\Phi_k\)</span>.
ç™½æ¿çš„æœ€åä¸€éƒ¨åˆ†å¼€å§‹æ¨å¯¼åŸå­æ ¸æ³¢å‡½æ•° <span
class="math inline">\(\Theta_k\)</span>
çš„å«æ—¶è–›å®šè°”æ–¹ç¨‹ã€‚è¯¥è¿‡ç¨‹ä»å®Œæ•´çš„å«æ—¶è–›å®šè°”æ–¹ç¨‹ <span
class="math inline">\(i\hbar \frac{\partial \Psi}{\partial t} =
\hat{\mathcal{H}} \Psi\)</span> å¼€å§‹ã€‚ä¸ºäº†æ‰¾åˆ°ç‰¹å®šåŸå­æ ¸æ³¢å‡½æ•° <span
class="math inline">\(\Theta_k\)</span>
çš„æ–¹ç¨‹ï¼Œéœ€è¦å°†è¿™ä¸ªä¸»æ–¹ç¨‹æŠ•å½±åˆ°ç›¸åº”çš„ç”µå­åŸºæ€ <span
class="math inline">\(\Phi_k\)</span> ä¸Šã€‚</p>
<p>This is done by multiplying from the left by the complex conjugate of
the electronic wave function, <span
class="math inline">\(\Phi_k^*\)</span>, and integrating over all
electronic coordinates, <span class="math inline">\(d\vec{r}\)</span>.
å¯ä»¥é€šè¿‡ä»å·¦è¾¹ä¹˜ä»¥ç”µå­æ³¢å‡½æ•° <span
class="math inline">\(\Phi_k^*\)</span> çš„å¤å…±è½­ï¼Œç„¶ååœ¨æ‰€æœ‰ç”µå­åæ ‡
<span class="math inline">\(d\vec{r}\)</span> ä¸Šç§¯åˆ†æ¥å®ç°ã€‚</p>
<p><span class="math inline">\(\int \Phi_k^* i\hbar
\frac{\partial}{\partial t} \Psi \,d\vec{r} = \int \Phi_k^*
\hat{\mathcal{H}} \Psi \,d\vec{r}\)</span></p>
<p>The board then shows the result of substituting the Born-Huang ansatz
for <span class="math inline">\(\Psi\)</span> and the partitioned
Hamiltonian for <span class="math inline">\(\hat{\mathcal{H}}\)</span>
into this projected equation: ç„¶åï¼Œé»‘æ¿æ˜¾ç¤ºå°† Born-Huang æ‹Ÿè®¾å¼ä»£å…¥
<span
class="math inline">\(\Psi\)</span>ï¼Œå°†åˆ†å—å“ˆå¯†é¡¿é‡ä»£å…¥ä»¥ä¸‹æŠ•å½±æ–¹ç¨‹çš„ç»“æœï¼š</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k(\vec{R}, t) = \int \Phi_k^* \left( -\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2 + \hat{\mathcal{H}}_e + \hat{V}_{nn}
\right) \sum_n \Theta_n \Phi_n \,d\vec{r}\)</span></p>
<ul>
<li><p><strong>Left Hand Side</strong>: The left side of the projection
has been simplified. Because the electronic basis functions <span
class="math inline">\(\Phi_n\)</span> form an orthonormal set (<span
class="math inline">\(\int \Phi_k^* \Phi_n d\vec{r} =
\delta_{kn}\)</span>), the sum collapses to a single term for <span
class="math inline">\(n=k\)</span>. æŠ•å½±å·¦ä¾§å·²ç®€åŒ–ã€‚ç”±äºç”µå­åŸºå‡½æ•° <span
class="math inline">\(\Phi_n\)</span> æ„æˆä¸€ä¸ªæ­£äº¤é›† (<span
class="math inline">\(\int \Phi_k^* \Phi_n d\vec{r} =
\delta_{kn}\)</span>ï¼Œå› æ­¤å½“ <span class="math inline">\(n=k\)</span>
æ—¶ï¼Œå’Œå°†æŠ˜å ä¸ºä¸€ä¸ªé¡¹ã€‚</p></li>
<li><p><strong>Right Hand Side</strong>: This complex integral is the
core of non-adiabatic dynamics. When the nuclear kinetic energy
operator, <span class="math inline">\(\nabla_I^2\)</span>, acts on the
product <span class="math inline">\(\Theta_n \Phi_n\)</span>, it acts on
both functions (via the product rule). The terms that arise from <span
class="math inline">\(\nabla_I\)</span> acting on the electronic wave
functions <span class="math inline">\(\Phi_n\)</span> are known as
<strong>non-adiabatic coupling terms</strong>. These terms are
responsible for enabling transitions between different electronic
potential energy surfaces, which is the essence of non-adiabatic
dynamics. è¿™ä¸ªå¤ç§¯åˆ†æ˜¯éç»çƒ­åŠ¨åŠ›å­¦çš„æ ¸å¿ƒã€‚å½“æ ¸åŠ¨èƒ½ç®—ç¬¦ <span
class="math inline">\(\nabla_I^2\)</span> ä½œç”¨äºä¹˜ç§¯ <span
class="math inline">\(\Theta_n \Phi_n\)</span>
æ—¶ï¼Œå®ƒä¼šä½œç”¨äºè¿™ä¸¤ä¸ªå‡½æ•°ï¼ˆé€šè¿‡ä¹˜ç§¯è§„åˆ™ï¼‰ã€‚ç”± <span
class="math inline">\(\nabla_I\)</span> ä½œç”¨äºç”µå­æ³¢å‡½æ•° <span
class="math inline">\(\Phi_n\)</span>
è€Œäº§ç”Ÿçš„é¡¹ç§°ä¸º<strong>éç»çƒ­è€¦åˆé¡¹</strong>ã€‚è¿™äº›æœ¯è¯­è´Ÿè´£å®ç°ä¸åŒç”µå­åŠ¿èƒ½é¢ä¹‹é—´çš„è½¬å˜ï¼Œè¿™æ˜¯éç»çƒ­åŠ¨åŠ›å­¦çš„æœ¬è´¨ã€‚</p></li>
</ul>
<p>This whiteboard continues the mathematical derivation for
non-adiabatic molecular dynamics started in the previous image. It
focuses on expanding the nuclear kinetic energy term to reveal the
crucial couplings between different electronic
states.è¿™å—ç™½æ¿å»¶ç»­äº†ä¸Šä¸€å¼ å›¾ç‰‡ä¸­éç»çƒ­åˆ†å­åŠ¨åŠ›å­¦çš„æ•°å­¦æ¨å¯¼ã€‚å®ƒç€é‡äºæ‰©å±•æ ¸åŠ¨èƒ½é¡¹ï¼Œä»¥æ­ç¤ºä¸åŒç”µå­æ€ä¹‹é—´çš„å…³é”®è€¦åˆã€‚</p>
<h3
id="starting-point-the-projected-schrÃ¶dinger-equation-èµ·ç‚¹æŠ•å½±è–›å®šè°”æ–¹ç¨‹">11.
Starting Point: The Projected SchrÃ¶dinger Equation
èµ·ç‚¹ï¼šæŠ•å½±è–›å®šè°”æ–¹ç¨‹</h3>
<p>The derivation picks up from the equation for the time evolution of
the nuclear wave function, <span
class="math inline">\(\Theta_k\)</span>. The right-hand side of this
equation is being evaluated. æ¨å¯¼è¿‡ç¨‹å–è‡ªæ ¸æ³¢å‡½æ•° <span
class="math inline">\(\Theta_k\)</span>
çš„æ—¶é—´æ¼”åŒ–æ–¹ç¨‹ã€‚è¯¥æ–¹ç¨‹çš„å³è¾¹æ­£åœ¨æ±‚å€¼ã€‚</p>
<p><span class="math inline">\(= \int \Phi_k^* \left( -\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2 \right) \sum_n \Theta_n \Phi_n \,d\vec{r}
+ E_k \Theta_k\)</span></p>
<p>This equation separates the total energy into two parts
è¯¥æ–¹ç¨‹å°†æ€»èƒ½é‡åˆ†ä¸ºä¸¤éƒ¨åˆ† : * The first term is the contribution from the
<strong>nuclear kinetic energy operator</strong>, <span
class="math inline">\(-\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2\)</span>.
ç¬¬ä¸€é¡¹æ˜¯<strong>æ ¸åŠ¨èƒ½ç®—ç¬¦</strong>çš„è´¡çŒ® * The second term, <span
class="math inline">\(E_k \Theta_k\)</span>, is the contribution from
the <strong>potential energy</strong>. This term arises from the action
of the electronic Hamiltonian part <span
class="math inline">\((\hat{\mathcal{H}}_e + \hat{V}_{nn})\)</span> on
the basis functions. Due to the orthonormality of the electronic
wavefunctions (<span class="math inline">\(\int \Phi_k^* \Phi_n
\,d\vec{r} = \delta_{kn}\)</span>), the sum over <span
class="math inline">\(n\)</span> collapses to a single term for the
potential energy. ç¬¬äºŒé¡¹ï¼Œ<span class="math inline">\(E_k
\Theta_k\)</span>ï¼Œæ˜¯<strong>åŠ¿èƒ½</strong>çš„è´¡çŒ®ã€‚è¿™ä¸€é¡¹æºäºç”µå­å“ˆå¯†é¡¿é‡éƒ¨åˆ†
<span class="math inline">\((\hat{\mathcal{H}}_e +
\hat{V}_{nn})\)</span> å¯¹åŸºå‡½æ•°çš„ä½œç”¨ã€‚ç”±äºç”µå­æ³¢å‡½æ•°ï¼ˆ<span
class="math inline">\(\int \Phi_k^* \Phi_n \,d\vec{r} =
\delta_{kn}\)</span>ï¼‰çš„æ­£äº¤æ€§ï¼Œ<span
class="math inline">\(n\)</span>é¡¹çš„å’Œä¼šåç¼©ä¸ºåŠ¿èƒ½çš„ä¸€é¡¹ã€‚</p>
<p>The challenge, and the core of the physics, lies in evaluating the
first term, as the nuclear derivative <span
class="math inline">\(\nabla_I\)</span> acts on <em>both</em> the
nuclear wave function <span class="math inline">\(\Theta_n\)</span> and
the electronic wave function <span
class="math inline">\(\Phi_n\)</span>.
éš¾ç‚¹åœ¨äºï¼Œä¹Ÿæ˜¯ç‰©ç†çš„æ ¸å¿ƒåœ¨äºå¦‚ä½•è®¡ç®—ç¬¬ä¸€é¡¹ï¼Œå› ä¸ºæ ¸å¯¼æ•° <span
class="math inline">\(\nabla_I\)</span> åŒæ—¶ä½œç”¨äºæ ¸æ³¢å‡½æ•° <span
class="math inline">\(\Theta_n\)</span> å’Œç”µå­æ³¢å‡½æ•° <span
class="math inline">\(\Phi_n\)</span>ã€‚</p>
<h3
id="applying-the-product-rule-for-the-laplacian-åº”ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ä¹˜ç§¯è§„åˆ™">12.
Applying the Product Rule for the Laplacian
åº”ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ä¹˜ç§¯è§„åˆ™</h3>
<p>To expand the kinetic energy term, the product rule for the Laplacian
operator acting on two functions (A and B) is used. The board writes
this rule as: ä¸ºäº†å±•å¼€åŠ¨èƒ½é¡¹ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†æ‹‰æ™®æ‹‰æ–¯ç®—å­ä½œç”¨äºä¸¤ä¸ªå‡½æ•°ï¼ˆA å’Œ
Bï¼‰çš„ä¹˜ç§¯è§„åˆ™ã€‚æ£‹ç›˜ä¸Šå°†è¿™æ¡è§„åˆ™å†™æˆï¼š <span
class="math inline">\(\nabla^2(AB) = (\nabla^2 A)B + 2(\nabla
A)\cdot(\nabla B) + A(\nabla^2 B)\)</span></p>
<p>In our case, <span class="math inline">\(A = \Theta_n(\vec{R},
t)\)</span> and <span class="math inline">\(B = \Phi_n(\vec{R},
\vec{r})\)</span>. The derivative <span
class="math inline">\(\nabla_I\)</span> is with respect to the nuclear
coordinates <span class="math inline">\(\vec{R}_I\)</span>.
åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œ<span class="math inline">\(A = \Theta_n(\vec{R},
t)\)</span>ï¼Œ<span class="math inline">\(B = \Phi_n(\vec{R},
\vec{r})\)</span>ã€‚å¯¼æ•° <span class="math inline">\(\nabla_I\)</span>
æ˜¯å…³äºåŸå­æ ¸åæ ‡ <span class="math inline">\(\vec{R}_I\)</span> çš„ã€‚</p>
<h3 id="expanding-the-kinetic-energy-term-å±•å¼€åŠ¨èƒ½é¡¹">13. Expanding the
Kinetic Energy Term å±•å¼€åŠ¨èƒ½é¡¹</h3>
<p>Applying this rule, the integral containing the kinetic energy
operator is expanded: åº”ç”¨æ­¤è§„åˆ™ï¼Œå±•å¼€åŒ…å«åŠ¨èƒ½ç®—ç¬¦çš„ç§¯åˆ†ï¼š <span
class="math inline">\(= -\sum_I \frac{\hbar^2}{2M_I} \int \Phi_k^*
\sum_n \left( (\nabla_I^2 \Theta_n)\Phi_n + 2(\nabla_I
\Theta_n)\cdot(\nabla_I \Phi_n) + \Theta_n(\nabla_I^2 \Phi_n) \right)
d\vec{r} + E_k \Theta_k\)</span></p>
<p>This step explicitly shows how the nuclear kinetic energy operator
gives rise to three distinct types of
terms.æ­¤æ­¥éª¤æ˜ç¡®å±•ç¤ºäº†æ ¸åŠ¨èƒ½ç®—ç¬¦å¦‚ä½•äº§ç”Ÿä¸‰ç§ä¸åŒç±»å‹çš„é¡¹ã€‚</p>
<h3
id="final-result-and-identification-of-coupling-terms-æœ€ç»ˆç»“æœåŠè€¦åˆé¡¹çš„è¯†åˆ«">14.
Final Result and Identification of Coupling Terms
æœ€ç»ˆç»“æœåŠè€¦åˆé¡¹çš„è¯†åˆ«</h3>
<p>The final step is to take the integral over the electronic
coordinates (<span class="math inline">\(d\vec{r}\)</span>) and
rearrange the terms. The expression is simplified by again using the
orthonormality of the electronic wave functions, <span
class="math inline">\(\int \Phi_k^* \Phi_n \, d\vec{r} =
\delta_{kn}\)</span>. æœ€åä¸€æ­¥æ˜¯å¯¹ç”µå­åæ ‡ (<span
class="math inline">\(d\vec{r}\)</span>)
è¿›è¡Œç§¯åˆ†ï¼Œå¹¶é‡æ–°æ’åˆ—å„é¡¹ã€‚å†æ¬¡åˆ©ç”¨ç”µå­æ³¢å‡½æ•°çš„æ­£äº¤æ€§ç®€åŒ–è¡¨è¾¾å¼ï¼Œ<span
class="math inline">\(\int \Phi_k^* \Phi_n \, d\vec{r} =
\delta_{kn}\)</span>ã€‚</p>
<p><span class="math inline">\(= -\sum_I \frac{\hbar^2}{2M_I} \left(
\nabla_I^2 \Theta_k + \sum_n 2 \left( \int \Phi_k^* \nabla_I \Phi_n \,
d\vec{r} \right) \cdot \nabla_I \Theta_n + \sum_n \left( \int \Phi_k^*
\nabla_I^2 \Phi_n \, d\vec{r} \right) \Theta_n \right) + E_k
\Theta_k\)</span></p>
<p>This final equation is profound. It represents the time-independent
SchrÃ¶dinger equation for the nuclear wave function <span
class="math inline">\(\Theta_k\)</span>, but it is coupled to all other
nuclear wave functions <span class="math inline">\(\Theta_n\)</span>.
Letâ€™s break down the key terms within the parentheses:
æœ€åä¸€ä¸ªæ–¹ç¨‹æ„ä¹‰æ·±è¿œã€‚å®ƒä»£è¡¨äº†æ ¸æ³¢å‡½æ•° <span
class="math inline">\(\Theta_k\)</span>
çš„ä¸æ—¶é—´æ— å…³çš„è–›å®šè°”æ–¹ç¨‹ï¼Œä½†å®ƒä¸æ‰€æœ‰å…¶ä»–æ ¸æ³¢å‡½æ•° <span
class="math inline">\(\Theta_n\)</span>
è€¦åˆã€‚è®©æˆ‘ä»¬åˆ†è§£ä¸€ä¸‹æ‹¬å·å†…çš„å…³é”®é¡¹ï¼š</p>
<ul>
<li><p><strong><span class="math inline">\(\nabla_I^2
\Theta_k\)</span></strong>: This is the standard kinetic energy term for
the nuclei moving on the potential energy surface of state <span
class="math inline">\(k\)</span>. This is the only term that would
remain in the simple Born-Oppenheimer (adiabatic) approximation.
è¿™æ˜¯åŸå­æ ¸åœ¨åŠ¿èƒ½é¢ <span class="math inline">\(k\)</span>
ä¸Šè¿åŠ¨çš„æ ‡å‡†åŠ¨èƒ½é¡¹ã€‚è¿™æ˜¯åœ¨ç®€å•çš„
Born-Oppenheimerï¼ˆç»çƒ­ï¼‰è¿‘ä¼¼ä¸­å”¯ä¸€ä¿ç•™çš„é¡¹ã€‚</p></li>
<li><p><strong><span class="math inline">\(\left( \int \Phi_k^* \nabla_I
\Phi_n \, d\vec{r} \right)\)</span></strong>: This is the
<strong>first-derivative non-adiabatic coupling term (NACT)</strong>,
often called the derivative coupling. This vector quantity determines
the strength of the coupling between electronic states <span
class="math inline">\(k\)</span> and <span
class="math inline">\(n\)</span> due to the velocity of the nuclei. It
is the primary term responsible for enabling transitions between
different potential energy surfaces. è¿™æ˜¯<strong>ä¸€é˜¶å¯¼æ•°éç»çƒ­è€¦åˆé¡¹
(NACT)</strong>ï¼Œé€šå¸¸ç§°ä¸ºå¯¼æ•°è€¦åˆã€‚è¯¥çŸ¢é‡å†³å®šäº†ç”±äºåŸå­æ ¸é€Ÿåº¦è€Œå¯¼è‡´çš„ç”µå­æ€
<span class="math inline">\(k\)</span> å’Œ <span
class="math inline">\(n\)</span>
ä¹‹é—´è€¦åˆçš„å¼ºåº¦ã€‚å®ƒæ˜¯å®ç°ä¸åŒåŠ¿èƒ½é¢ä¹‹é—´è·ƒè¿çš„ä¸»è¦é¡¹ã€‚</p></li>
<li><p><strong><span class="math inline">\(\left( \int \Phi_k^*
\nabla_I^2 \Phi_n \, d\vec{r} \right)\)</span></strong>: This is the
<strong>second-derivative non-adiabatic coupling term</strong>, a scalar
quantity. While often smaller than the first-derivative term, it is also
part of the complete description of non-adiabatic effects.
æ˜¯<strong>äºŒé˜¶å¯¼æ•°éç»çƒ­è€¦åˆé¡¹</strong>ï¼Œä¸€ä¸ªæ ‡é‡ã€‚è™½ç„¶å®ƒé€šå¸¸å°äºä¸€é˜¶å¯¼æ•°é¡¹ï¼Œä½†å®ƒä¹Ÿæ˜¯éç»çƒ­æ•ˆåº”å®Œæ•´æè¿°çš„ä¸€éƒ¨åˆ†ã€‚</p></li>
</ul>
<p>In summary, this derivation shows mathematically how the motion of
the nuclei (via the <span class="math inline">\(\nabla_I\)</span>
operator) can induce quantum mechanical transitions between different
electronic states (<span class="math inline">\(\Phi_k \leftrightarrow
\Phi_n\)</span>). The strength of these transitions is governed by the
non-adiabatic coupling terms, which depend on how the electronic wave
functions change as the nuclear geometry changes.
æ€»ä¹‹ï¼Œè¯¥æ¨å¯¼ä»æ•°å­¦ä¸Šå±•ç¤ºäº†åŸå­æ ¸çš„è¿åŠ¨ï¼ˆé€šè¿‡ <span
class="math inline">\(\nabla_I\)</span>
ç®—ç¬¦ï¼‰å¦‚ä½•è¯±å¯¼ä¸åŒç”µå­æ€ä¹‹é—´çš„é‡å­åŠ›å­¦è·ƒè¿ï¼ˆ<span
class="math inline">\(\Phi_k \leftrightarrow
\Phi_n\)</span>ï¼‰ã€‚è¿™äº›è·ƒè¿çš„å¼ºåº¦ç”±éç»çƒ­è€¦åˆé¡¹æ§åˆ¶ï¼Œè€Œéç»çƒ­è€¦åˆé¡¹åˆå–å†³äºç”µå­æ³¢å‡½æ•°å¦‚ä½•éšåŸå­æ ¸å‡ ä½•ç»“æ„çš„å˜åŒ–è€Œå˜åŒ–ã€‚</p>
<p>This whiteboard concludes the derivation of the equations for
non-adiabatic molecular dynamics by defining the coupling operator and
then showing how different levels of approximationâ€”specifically the
Born-Huang and the more restrictive Born-Oppenheimer
approximationsâ€”arise from neglecting certain coupling terms.
è¿™å—ç™½æ¿é€šè¿‡å®šä¹‰è€¦åˆç®—ç¬¦ï¼Œå¹¶å±•ç¤ºä¸åŒç¨‹åº¦çš„è¿‘ä¼¼â€”â€”ç‰¹åˆ«æ˜¯ Born-Huang
è¿‘ä¼¼å’Œæ›´ä¸¥æ ¼çš„ Born-Oppenheimer
è¿‘ä¼¼â€”â€”æ˜¯å¦‚ä½•é€šè¿‡å¿½ç•¥æŸäº›è€¦åˆé¡¹è€Œäº§ç”Ÿçš„ï¼Œä»è€Œæ¨å¯¼å‡ºéç»çƒ­åˆ†å­åŠ¨åŠ›å­¦æ–¹ç¨‹çš„ã€‚</p>
<h3
id="definition-of-the-non-adiabatic-coupling-operator-éç»çƒ­è€¦åˆç®—ç¬¦çš„å®šä¹‰">15.
Definition of the Non-Adiabatic Coupling Operator
éç»çƒ­è€¦åˆç®—ç¬¦çš„å®šä¹‰</h3>
<p>The whiteboard begins by collecting all the non-adiabatic coupling
terms derived previously into a single operator, <span
class="math inline">\(C_{kn}\)</span>.
ç™½æ¿é¦–å…ˆå°†ä¹‹å‰æ¨å¯¼çš„æ‰€æœ‰éç»çƒ­è€¦åˆé¡¹åˆå¹¶ä¸ºä¸€ä¸ªç®—ç¬¦ <span
class="math inline">\(C_{kn}\)</span>ã€‚</p>
<p>Let <span class="math inline">\(C_{kn} = -\sum_{I}
\frac{\hbar^2}{2M_I} \left( 2 \left( \int \Phi_k^* \nabla_I \Phi_n \,
d\vec{r} \right) \cdot \nabla_I + \left( \int \Phi_k^* \nabla_I^2 \Phi_n
\, d\vec{r} \right) \right)\)</span></p>
<ul>
<li>This operator, <span class="math inline">\(C_{kn}\)</span>,
represents the total effect of the coupling between electronic state
<span class="math inline">\(k\)</span> and electronic state <span
class="math inline">\(n\)</span>, which is induced by the kinetic energy
of the nuclei. æ­¤ç®—ç¬¦ <span class="math inline">\(C_{kn}\)</span>
è¡¨ç¤ºç”±åŸå­æ ¸åŠ¨èƒ½å¼•èµ·çš„ç”µå­æ€ <span class="math inline">\(k\)</span>
å’Œç”µå­æ€ <span class="math inline">\(n\)</span> ä¹‹é—´è€¦åˆçš„æ€»æ•ˆåº”ã€‚</li>
<li>The operator acts on the nuclear wave function that follows it in
the full equation. The <span class="math inline">\(\nabla_I\)</span>
term acts as a derivative on that wave function.
è¯¥ç®—ç¬¦ä½œç”¨äºå®Œæ•´æ–¹ç¨‹ä¸­è·Ÿéšå®ƒçš„æ ¸æ³¢å‡½æ•°ã€‚<span
class="math inline">\(\nabla_I\)</span> é¡¹å……å½“è¯¥æ³¢å‡½æ•°çš„å¯¼æ•°ã€‚</li>
</ul>
<h3 id="the-coupled-equations-of-motion-è€¦åˆè¿åŠ¨æ–¹ç¨‹">16. The Coupled
Equations of Motion è€¦åˆè¿åŠ¨æ–¹ç¨‹</h3>
<p>Using this compact definition, the full set of coupled time-dependent
SchrÃ¶dinger equations for the nuclear wave functions can be written as:
åŸºäºæ­¤ç®€æ´å®šä¹‰ï¼Œæ ¸æ³¢å‡½æ•°çš„å®Œæ•´è€¦åˆå«æ—¶è–›å®šè°”æ–¹ç¨‹ç»„å¯ä»¥å†™æˆï¼š</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k \right)
\Theta_k + \sum_n C_{kn} \Theta_n\)</span></p>
<p>This is the central result. It shows that the time evolution of the
nuclear wave function on a given potential energy surface <span
class="math inline">\(k\)</span> (described by <span
class="math inline">\(\Theta_k\)</span>) depends on two things:
è¿™æ˜¯æ ¸å¿ƒç»“è®ºã€‚å®ƒè¡¨æ˜ï¼Œæ ¸æ³¢å‡½æ•°åœ¨ç»™å®šåŠ¿èƒ½é¢ <span
class="math inline">\(k\)</span>ï¼ˆç”¨ <span
class="math inline">\(\Theta_k\)</span>
æè¿°ï¼‰ä¸Šçš„æ—¶é—´æ¼”åŒ–å–å†³äºä¸¤ä¸ªå› ç´ ï¼š 1. The motion on its own surface,
governed by its kinetic energy and the potential <span
class="math inline">\(E_k\)</span>. å…¶è‡ªèº«è¡¨é¢ä¸Šçš„è¿åŠ¨ï¼Œç”±å…¶åŠ¨èƒ½å’ŒåŠ¿èƒ½
<span class="math inline">\(E_k\)</span> æ§åˆ¶ã€‚ 2. The influence of the
nuclear wave functions on <em>all other</em> electronic surfaces (<span
class="math inline">\(\Theta_n\)</span>), mediated by the coupling
operators <span class="math inline">\(C_{kn}\)</span>.
æ ¸æ³¢å‡½æ•°å¯¹<em>æ‰€æœ‰å…¶ä»–</em>ç”µå­è¡¨é¢ï¼ˆ<span
class="math inline">\(\Theta_n\)</span>ï¼‰çš„å½±å“ï¼Œç”±è€¦åˆç®—ç¬¦ <span
class="math inline">\(C_{kn}\)</span> ä»‹å¯¼ã€‚</p>
<h3 id="the-born-huang-approximation-ç»æ©-é»„è¿‘ä¼¼">17. The Born-Huang
Approximation ç»æ©-é»„è¿‘ä¼¼</h3>
<p>The first and most crucial approximation is introduced to simplify
this complex set of coupled equations.
ä¸ºäº†ç®€åŒ–è¿™ç»„å¤æ‚çš„è€¦åˆæ–¹ç¨‹ï¼Œå¼•å…¥äº†ç¬¬ä¸€ä¸ªä¹Ÿæ˜¯æœ€é‡è¦çš„è¿‘ä¼¼ã€‚</p>
<p><strong>If <span class="math inline">\(C_{kn} = 0\)</span> for <span
class="math inline">\(k \neq n\)</span> (Born-Huang
approximation)</strong></p>
<p>This approximation assumes that the <strong>off-diagonal</strong>
coupling terms, which are responsible for transitions between different
electronic states, are negligible. However, it retains the
<strong>diagonal</strong> coupling term (<span
class="math inline">\(C_{kk}\)</span>). This leads to a simplified,
uncoupled equation:
è¯¥è¿‘ä¼¼å‡è®¾å¯¼è‡´ä¸åŒç”µå­æ€ä¹‹é—´è·ƒè¿çš„<strong>éå¯¹è§’</strong>è€¦åˆé¡¹å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚ç„¶è€Œï¼Œå®ƒä¿ç•™äº†<strong>å¯¹è§’</strong>è€¦åˆé¡¹ï¼ˆ<span
class="math inline">\(C_{kk}\)</span>ï¼‰ã€‚è¿™å¯ä»¥å¾—åˆ°ä¸€ä¸ªç®€åŒ–çš„éè€¦åˆæ–¹ç¨‹ï¼š</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k +
C_{kk} \right) \Theta_k\)</span></p>
<p>Substituting the definition of <span
class="math inline">\(C_{kk}\)</span>: ä»£å…¥ <span
class="math inline">\(C_{kk}\)</span> çš„å®šä¹‰ï¼š</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k -
\sum_I \frac{\hbar^2}{2M_I} \left( 2 \left( \int \Phi_k^* \nabla_I
\Phi_k \, d\vec{r} \right) \cdot \nabla_I + \int \Phi_k^* \nabla_I^2
\Phi_k \, d\vec{r} \right) \right) \Theta_k\)</span></p>
<p>The term <span class="math inline">\(C_{kk}\)</span> is known as the
<strong>diagonal Born-Oppenheimer correction (DBOC)</strong>. It
represents a small correction to the potential energy surface <span
class="math inline">\(E_k\)</span> that arises from the fact that the
electrons do not adjust perfectly and instantaneously to the nuclear
motion, even within the same electronic state. <span
class="math inline">\(C_{kk}\)</span>
é¡¹è¢«ç§°ä¸º<strong>å¯¹è§’ç»æ©-å¥¥æœ¬æµ·é»˜ä¿®æ­£ (DBOC)</strong>ã€‚å®ƒè¡¨ç¤ºå¯¹åŠ¿èƒ½é¢
<span class="math inline">\(E_k\)</span>
çš„å¾®å°ä¿®æ­£ï¼Œå…¶åŸå› æ˜¯å³ä½¿åœ¨ç›¸åŒçš„ç”µå­æ€ä¸‹ï¼Œç”µå­ä¹Ÿæ— æ³•å®Œç¾ä¸”å³æ—¶åœ°é€‚åº”æ ¸è¿åŠ¨ã€‚</p>
<ul>
<li><strong>Note on Real Wavefunctions å…³äºå®æ³¢å‡½æ•°çš„æ³¨é‡Š</strong>: The
board shows that for real wavefunctions, the first-derivative part of
the diagonal correction vanishes: <span class="math inline">\(\int
\Phi_k \nabla_I \Phi_k \, d\vec{r} = 0\)</span>. This is because the
integral is related to the gradient of the normalization condition,
<span class="math inline">\(\nabla_I \int \Phi_k^2 \, d\vec{r} =
\nabla_I(1) = 0\)</span>, which expands to <span
class="math inline">\(2\int \Phi_k \nabla_I \Phi_k \, d\vec{r} =
0\)</span>. é»‘æ¿æ˜¾ç¤ºï¼Œå¯¹äºå®æ³¢å‡½æ•°ï¼Œå¯¹è§’ä¿®æ­£çš„ä¸€é˜¶å¯¼æ•°éƒ¨åˆ†ä¸ºé›¶ï¼š<span
class="math inline">\(\int \Phi_k \nabla_I \Phi_k \, d\vec{r} =
0\)</span>ã€‚è¿™æ˜¯å› ä¸ºç§¯åˆ†ä¸å½’ä¸€åŒ–æ¡ä»¶çš„æ¢¯åº¦æœ‰å…³ï¼Œ<span
class="math inline">\(\nabla_I \int \Phi_k^2 \, d\vec{r} = \nabla_I(1) =
0\)</span>ï¼Œå…¶å±•å¼€ä¸º <span class="math inline">\(2\int \Phi_k \nabla_I
\Phi_k \, d\vec{r} = 0\)</span>ã€‚</li>
</ul>
<h3 id="the-born-oppenheimer-approximation-ç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼">18. The
Born-Oppenheimer Approximation ç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼</h3>
<p>The final and most widely used approximation is the Born-Oppenheimer
approximation. It is more restrictive than the Born-Huang approximation.
æœ€åä¸€ç§ä¹Ÿæ˜¯æœ€å¹¿æ³›ä½¿ç”¨çš„è¿‘ä¼¼æ–¹æ³•æ˜¯ç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼ã€‚å®ƒæ¯”ç»æ©-é»„è¿‘ä¼¼æ›´å…·é™åˆ¶æ€§ã€‚</p>
<p><strong>If <span class="math inline">\(C_{kk} = 0\)</span>
(Born-Oppenheimer approximation) è‹¥<span class="math inline">\(C_{kk} =
0\)</span>ï¼ˆç»æ©-å¥¥æœ¬æµ·é»˜è¿‘ä¼¼ï¼‰</strong></p>
<p>This assumes that the diagonal correction term is also negligible. By
setting all <span class="math inline">\(C_{kn}=0\)</span> (both diagonal
and off-diagonal), the equations become completely decoupled, and the
nuclear motion evolves independently on each potential energy surface.
è¿™å‡è®¾å¯¹è§’ä¿®æ­£é¡¹ä¹Ÿå¯å¿½ç•¥ä¸è®¡ã€‚é€šè¿‡ä»¤æ‰€æœ‰<span
class="math inline">\(C_{kn}=0\)</span>ï¼ˆåŒ…æ‹¬å¯¹è§’å’Œéå¯¹è§’ï¼‰ï¼Œæ–¹ç¨‹ç»„å®Œå…¨è§£è€¦ï¼ŒåŸå­æ ¸è¿åŠ¨åœ¨æ¯ä¸ªåŠ¿èƒ½é¢ä¸Šç‹¬ç«‹æ¼”åŒ–ã€‚</p>
<p>The result is the standard <strong>time-dependent SchrÃ¶dinger
equation for the nuclei</strong>:
ç”±æ­¤å¯å¾—æ ‡å‡†çš„<strong>åŸå­æ ¸çš„å«æ—¶è–›å®šè°”æ–¹ç¨‹</strong>ï¼š</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k \right)
\Theta_k\)</span></p>
<p>This equation is the foundation of most of quantum chemistry. It
states that the nuclei move on a static potential energy surface <span
class="math inline">\(E_k(\vec{R})\)</span> provided by the electrons,
without any possibility of transitioning to other electronic states or
having the surface be corrected by their own motion.</p>
<p>è¯¥æ–¹ç¨‹æ˜¯å¤§å¤šæ•°é‡å­åŒ–å­¦çš„åŸºç¡€ã€‚åŸå­æ ¸åœ¨ç”±ç”µå­æä¾›çš„é™æ€åŠ¿èƒ½é¢ <span
class="math inline">\(E_k(\vec{R})\)</span>
ä¸Šè¿åŠ¨ï¼Œä¸å­˜åœ¨è·ƒè¿åˆ°å…¶ä»–ç”µå­æ€æˆ–å› è‡ªèº«è¿åŠ¨è€Œä¿®æ­£åŠ¿èƒ½é¢çš„å¯èƒ½æ€§ã€‚</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/18/img_assert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/18/img_assert/" class="post-title-link" itemprop="url">BLOGS - IMG Assert</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-18 10:00:00" itemprop="dateCreated datePublished" datetime="2025-09-18T10:00:00+08:00">2025-09-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-19 19:24:51" itemprop="dateModified" datetime="2025-09-19T19:24:51+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">æŠ€æœ¯</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="é—®é¢˜ä¸»è¦ä¸ºäº†å›¾åƒä¸æ˜¾ç¤ºé—®é¢˜">ã€é—®é¢˜ã€‘ä¸»è¦ä¸ºäº†å›¾åƒä¸æ˜¾ç¤ºé—®é¢˜</h2>
<h3 id="step1æ ¹ç›®å½•ä¸­çš„é…ç½®æ–‡ä»¶">Step1:æ ¹ç›®å½•ä¸­çš„é…ç½®æ–‡ä»¶</h3>
<h3 id="step2å°†-markdown-è¡Œæ›¿æ¢ä¸ºhtml-ä»£ç ">Step2:å°† Markdown
è¡Œæ›¿æ¢ä¸ºHTML ä»£ç </h3>
<h3 id="step3è®¾ç½®ä¸‹æ–¹æ·»åŠ root">Step3:è®¾ç½®ä¸‹æ–¹æ·»åŠ ROOT</h3>
<h3
id="step4ä¸éœ€è¦æ­¤æ’ä»¶ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å¸è½½æ’ä»¶">Step4:ä¸éœ€è¦æ­¤æ’ä»¶ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å¸è½½æ’ä»¶ï¼š</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment"># URL</span></span><br><span class="line"><span class="comment">## Set your site url here. For example, if you use GitHub Page, set url as &#x27;https://username.github.io/project&#x27;</span></span><br><span class="line">$ url: https://TianyaoBlogs.github.io/</span><br><span class="line"></span><br><span class="line">$ root: /</span><br><span class="line"></span><br><span class="line">$ permalink: :year/:month/:day/:title/</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ &lt;img src=<span class="string">&quot;/imgs/5054C3/General_linear_regression_model.png&quot;</span> alt=<span class="string">&quot;A diagram of the general linear regression model&quot;</span>&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm uninstall hexo-asset-image</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/17/5120C3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/17/5120C3/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W3-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-17 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-17T21:00:00+08:00">2025-09-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-19 20:28:09" itemprop="dateModified" datetime="2025-09-19T20:28:09+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - è®¡ç®—èƒ½æºææ–™å’Œç”µå­ç»“æ„æ¨¡æ‹Ÿ Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="radial-distribution-function">1 radial distribution
function:</h2>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>This whiteboard explains the process of calculating the
<strong>radial distribution function</strong>, often denoted as <span
class="math inline">\(g(r)\)</span>, to analyze the atomic structure of
a material, which is referred to here as a â€œfilmâ€.
æœ¬ç™½æ¿è§£é‡Šäº†è®¡ç®—<strong>å¾„å‘åˆ†å¸ƒå‡½æ•°</strong>ï¼ˆé€šå¸¸è¡¨ç¤ºä¸º <span
class="math inline">\(g(r)\)</span>ï¼‰çš„è¿‡ç¨‹ï¼Œç”¨äºåˆ†æææ–™ï¼ˆæœ¬æ–‡ä¸­ç§°ä¸ºâ€œè–„è†œâ€ï¼‰çš„åŸå­ç»“æ„ã€‚</p>
<p>In simple terms, the radial distribution function tells you the
probability of finding an atom at a certain distance from another
reference atom. Itâ€™s a powerful way to see the local structure in a
disordered system like a liquid or an amorphous solid.</p>
<p>ç®€å•æ¥è¯´ï¼Œå¾„å‘åˆ†å¸ƒå‡½æ•°è¡¨ç¤ºåœ¨è·ç¦»å¦ä¸€ä¸ªå‚è€ƒåŸå­ä¸€å®šè·ç¦»å¤„æ‰¾åˆ°ä¸€ä¸ªåŸå­çš„æ¦‚ç‡ã€‚å®ƒæ˜¯è§‚å¯Ÿæ— åºç³»ç»Ÿï¼ˆä¾‹å¦‚æ¶²ä½“æˆ–éæ™¶æ€å›ºä½“ï¼‰å±€éƒ¨ç»“æ„çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<h3 id="core-concept-radial-distribution-function-å¾„å‘åˆ†å¸ƒå‡½æ•°">## Core
Concept: Radial Distribution Function å¾„å‘åˆ†å¸ƒå‡½æ•°</h3>
<p>The main goal is to compute the radial distribution function, <span
class="math inline">\(g(r)\)</span>, which is defined as the ratio of
the actual number of atoms found in a thin shell at a distance <span
class="math inline">\(r\)</span> to the number of atoms youâ€™d expect to
find if the material were an ideal gas (completely random).
ä¸»è¦ç›®æ ‡æ˜¯è®¡ç®—å¾„å‘åˆ†å¸ƒå‡½æ•° <span
class="math inline">\(g(r)\)</span>ï¼Œå…¶å®šä¹‰ä¸ºåœ¨è·ç¦» <span
class="math inline">\(r\)</span>
çš„è–„å£³å±‚ä¸­å®é™…å‘ç°çš„åŸå­æ•°ä¸ææ–™ä¸ºç†æƒ³æ°”ä½“ï¼ˆå®Œå…¨éšæœºï¼‰æ—¶é¢„æœŸå‘ç°çš„åŸå­æ•°ä¹‹æ¯”ã€‚</p>
<p>The formula is expressed as: <span class="math display">\[g(r)dr =
\frac{n(r)}{\text{ideal gas}}\]</span></p>
<ul>
<li><strong><span class="math inline">\(n(r)\)</span></strong>:
Represents the average number of atoms found in a thin spherical shell
between a distance <span class="math inline">\(r\)</span> and <span
class="math inline">\(r+dr\)</span> from a central atom.
è¡¨ç¤ºè·ç¦»ä¸­å¿ƒåŸå­ <span class="math inline">\(r\)</span> åˆ° <span
class="math inline">\(r+dr\)</span> ä¹‹é—´çš„è–„çƒå£³ä¸­åŸå­çš„å¹³å‡æ•°é‡ã€‚</li>
<li><strong>ideal gas</strong>: Represents the number of atoms you would
expect in that same shell if the atoms were distributed completely
randomly with the same average density (<span
class="math inline">\(\rho\)</span>). The volume of this shell is
approximately <span class="math inline">\(4\pi r^2
dr\)</span>.è¡¨ç¤ºå¦‚æœåŸå­å®Œå…¨éšæœºåˆ†å¸ƒä¸”å¹³å‡å¯†åº¦ (<span
class="math inline">\(\rho\)</span>)
ç›¸åŒï¼Œåˆ™è¯¥çƒå£³ä¸­åŸå­çš„æ•°é‡ã€‚è¯¥çƒå£³çš„ä½“ç§¯çº¦ä¸º <span
class="math inline">\(4\pi r^2 dr\)</span>ã€‚</li>
</ul>
<p>A peak in the <span class="math inline">\(g(r)\)</span> plot
indicates a high probability of finding neighboring atoms at that
specific distance, revealing the materialâ€™s structural shells (e.g.,
nearest neighbors, second-nearest neighbors, etc.).<span
class="math inline">\(g(r)\)</span>
å›¾ä¸­çš„å³°å€¼è¡¨ç¤ºåœ¨è¯¥ç‰¹å®šè·ç¦»å¤„æ‰¾åˆ°ç›¸é‚»åŸå­çš„æ¦‚ç‡å¾ˆé«˜ï¼Œä»è€Œæ­ç¤ºäº†ææ–™çš„ç»“æ„å£³ï¼ˆä¾‹å¦‚ï¼Œæœ€è¿‘é‚»ã€æ¬¡è¿‘é‚»ç­‰ï¼‰ã€‚</p>
<h3 id="calculation-method">## Calculation Method</h3>
<p>The board outlines a two-step averaging process to get a
statistically meaningful result from simulation data (a â€œfilmâ€ at 20
frames per second).</p>
<ol type="1">
<li><p><strong>Average over atoms:</strong> In a single frame (a
snapshot in time), you pick one atom as the center. Then, you count how
many other atoms (<span class="math inline">\(n(r)\)</span>) are in
concentric spherical shells around it. This process is repeated,
treating each atom in the frame as the center, and the results are
averaged.</p></li>
<li><p><strong>Average over frames:</strong> The entire process
described above is repeated for multiple frames from the simulation or
video. This time-averaging ensures that the final result represents the
typical structure of the material over time, smoothing out random
fluctuations.</p></li>
</ol>
<p>The board notes â€œdx = bin width 0.01Ã…â€, which is a practical detail
for the calculation. To create a histogram, the distance <code>r</code>
is divided into small segments (bins) of 0.01 angstroms.</p>
<h3 id="connection-to-experiments">## Connection to Experiments</h3>
<p>Finally, the whiteboard mentions <strong>â€œframe X-ray
scatteringâ€</strong>. This is a crucial point because it connects this
computational analysis to real-world experiments. Experimental
techniques like X-ray or neutron scattering can be used to measure a
quantity called the structure factor, <span
class="math inline">\(S(q)\)</span>, which is directly related to the
radial distribution function <span class="math inline">\(g(r)\)</span>
through a mathematical operation called a Fourier transform. This allows
scientists to directly compare the structure produced in their
simulations with the structure of a real material measured in a lab.
æœ€åï¼Œç™½æ¿ä¸Šæåˆ°äº†<strong>â€œå¸§ X
å°„çº¿æ•£å°„â€</strong>ã€‚è¿™ä¸€ç‚¹è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå°†è®¡ç®—åˆ†æä¸å®é™…å®éªŒè”ç³»èµ·æ¥ã€‚Xå°„çº¿æˆ–ä¸­å­æ•£å°„ç­‰å®éªŒæŠ€æœ¯å¯ä»¥ç”¨æ¥æµ‹é‡ä¸€ä¸ªç§°ä¸ºç»“æ„å› å­<span
class="math inline">\(S(q)\)</span>çš„é‡ï¼Œè¯¥é‡é€šè¿‡å‚…é‡Œå¶å˜æ¢çš„æ•°å­¦è¿ç®—ä¸å¾„å‘åˆ†å¸ƒå‡½æ•°<span
class="math inline">\(g(r)\)</span>ç›´æ¥ç›¸å…³ã€‚è¿™ä½¿å¾—ç§‘å­¦å®¶èƒ½å¤Ÿç›´æ¥å°†æ¨¡æ‹Ÿä¸­äº§ç”Ÿçš„ç»“æ„ä¸å®éªŒå®¤æµ‹é‡çš„çœŸå®ææ–™ç»“æ„è¿›è¡Œæ¯”è¾ƒã€‚</p>
<p>The board correctly links <span class="math inline">\(g(r)\)</span>
to X-ray scattering experiments. The quantity measured in these
experiments is the <strong>static structure factor</strong>, <span
class="math inline">\(S(q)\)</span>, which describes how the material
scatters radiation. The relationship between the two is a Fourier
transform: è¯¥æ¿æ­£ç¡®åœ°å°†<span
class="math inline">\(g(r)\)</span>ä¸Xå°„çº¿æ•£å°„å®éªŒè”ç³»èµ·æ¥ã€‚è¿™äº›å®éªŒä¸­æµ‹é‡çš„é‡æ˜¯<strong>é™æ€ç»“æ„å› å­</strong><span
class="math inline">\(S(q)\)</span>ï¼Œå®ƒæè¿°äº†ææ–™å¦‚ä½•æ•£å°„è¾å°„ã€‚ä¸¤è€…ä¹‹é—´çš„å…³ç³»æ˜¯å‚…é‡Œå¶å˜æ¢ï¼š
<span class="math display">\[S(q) = 1 + 4 \pi \rho \int_0^\infty [g(r) -
1] r^2 \frac{\sin(qr)}{qr} dr\]</span> This equation is crucial because
it bridges the gap between computer simulations (which calculate <span
class="math inline">\(g(r)\)</span>) and physical experiments (which
measure <span class="math inline">\(S(q)\)</span>).
è¿™ä¸ªæ–¹ç¨‹è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¼¥åˆäº†è®¡ç®—æœºæ¨¡æ‹Ÿï¼ˆè®¡ç®— <span
class="math inline">\(g(r)\)</span>ï¼‰å’Œç‰©ç†å®éªŒï¼ˆæµ‹é‡ <span
class="math inline">\(S(q)\)</span>ï¼‰ä¹‹é—´çš„å·®è·ã€‚</p>
<h3
id="the-gaussian-distribution-probability-of-particle-position-é«˜æ–¯åˆ†å¸ƒç²’å­ä½ç½®çš„æ¦‚ç‡">##
2. The Gaussian Distribution: Probability of Particle Position
é«˜æ–¯åˆ†å¸ƒï¼šç²’å­ä½ç½®çš„æ¦‚ç‡</h3>
<p>The board starts with the formula for a one-dimensional
<strong>Gaussian (or normal) distribution</strong>:
ç™½æ¿é¦–å…ˆå±•ç¤ºçš„æ˜¯ä¸€ç»´<strong>é«˜æ–¯ï¼ˆæˆ–æ­£æ€ï¼‰åˆ†å¸ƒ</strong>çš„å…¬å¼ï¼š</p>
<p><span class="math display">\[f(x | \mu, \sigma^2) =
\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</span></p>
<p>This equation describes the probability of finding a particle at a
specific position <code>x</code> after a certain amount of time has
passed. * <strong><span class="math inline">\(\mu\)</span> (mu)</strong>
is the <strong>mean</strong> or average position. For a simple diffusion
process starting at the origin, the particles spread out symmetrically,
so the average position remains at the origin (<span
class="math inline">\(\mu = 0\)</span>). * <strong><span
class="math inline">\(\sigma^2\)</span> (sigma squared)</strong> is the
<strong>variance</strong>, which measures how spread out the particles
are from the mean position. A larger variance means the particles have,
on average, traveled farther from the starting point.
è¿™ä¸ªæ–¹ç¨‹æè¿°äº†ç»è¿‡ä¸€å®šæ—¶é—´åï¼Œåœ¨ç‰¹å®šä½ç½®â€œxâ€æ‰¾åˆ°ç²’å­çš„æ¦‚ç‡ã€‚ *
<strong><span class="math inline">\(\mu\)</span> (mu)</strong>
æ˜¯<strong>å¹³å‡å€¼</strong>æˆ–å¹³å‡ä½ç½®ã€‚å¯¹äºä»åŸç‚¹å¼€å§‹çš„ç®€å•æ‰©æ•£è¿‡ç¨‹ï¼Œç²’å­å¯¹ç§°æ‰©æ•£ï¼Œå› æ­¤å¹³å‡ä½ç½®ä¿æŒåœ¨åŸç‚¹ï¼ˆ<span
class="math inline">\(\mu = 0\)</span>ï¼‰ã€‚ * <strong><span
class="math inline">\(\sigma^2\)</span>ï¼ˆsigma å¹³æ–¹ï¼‰</strong>
æ˜¯<strong>æ–¹å·®</strong>ï¼Œç”¨â€‹â€‹äºè¡¡é‡ç²’å­ä¸å¹³å‡ä½ç½®çš„æ‰©æ•£ç¨‹åº¦ã€‚æ–¹å·®è¶Šå¤§ï¼Œæ„å‘³ç€ç²’å­å¹³å‡è·ç¦»èµ·ç‚¹è¶Šè¿œã€‚</p>
<p>The note â€œBlack-Scholesâ€ is a side reference. The Black-Scholes
model, famous in financial mathematics for pricing options, uses similar
mathematical principles based on Brownian motion to model the random
fluctuations of stock prices. â€œBlack-Scholesâ€æ³¨é‡Šä»…ä¾›å‚è€ƒã€‚Black-Scholes
æ¨¡å‹åœ¨é‡‘èæ•°å­¦ä¸­ä»¥æœŸæƒå®šä»·è€Œé—»åï¼Œå®ƒä½¿ç”¨åŸºäºå¸ƒæœ—è¿åŠ¨çš„ç±»ä¼¼æ•°å­¦åŸç†æ¥æ¨¡æ‹Ÿè‚¡ç¥¨ä»·æ ¼çš„éšæœºæ³¢åŠ¨ã€‚</p>
<h3
id="mean-squared-displacement-msd-quantifying-the-spread-å‡æ–¹ä½ç§»-msdé‡åŒ–æ‰©æ•£">##
3. Mean Squared Displacement (MSD): Quantifying the Spread å‡æ–¹ä½ç§»
(MSD)ï¼šé‡åŒ–æ‰©æ•£</h3>
<p>The core of the board is dedicated to the <strong>Mean Squared
Displacement (MSD)</strong>. This is the primary tool used to measure
how far, on average, particles have moved over a time interval
<code>t</code>. æœ¬ç‰ˆå—çš„æ ¸å¿ƒå†…å®¹æ˜¯<strong>å‡æ–¹ä½ç§»
(MSD)</strong>ã€‚è¿™æ˜¯ç”¨äºæµ‹é‡ç²’å­åœ¨æ—¶é—´é—´éš”â€œtâ€å†…å¹³å‡ç§»åŠ¨è·ç¦»çš„ä¸»è¦å·¥å…·ã€‚</p>
<p>The variance <span class="math inline">\(\sigma^2\)</span> is
formally defined as the average of the squared deviations from the mean:
<span class="math display">\[\sigma^2 = \langle x^2(t) \rangle - \langle
x(t) \rangle^2\]</span> * <span class="math inline">\(\langle x(t)
\rangle\)</span> is the average displacement. As mentioned, for simple
diffusion, <span class="math inline">\(\langle x(t) \rangle =
0\)</span>. * <span class="math inline">\(\langle x^2(t)
\rangle\)</span> is the average of the <em>square</em> of the
displacement. æ–¹å·®<span
class="math inline">\(\sigma^2\)</span>çš„æ­£å¼å®šä¹‰ä¸ºä¸å¹³å‡å€¼åå·®å¹³æ–¹çš„å¹³å‡å€¼ï¼š
<span class="math display">\[\sigma^2 = \langle x^2(t) \rangle - \langle
x(t) \rangle^2\]</span> * <span class="math inline">\(\langle x(t)
\rangle\)</span>æ˜¯å¹³å‡ä½ç§»ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œå¯¹äºç®€å•æ‰©æ•£ï¼Œ<span
class="math inline">\(\langle x(t) \rangle = 0\)</span>ã€‚ * <span
class="math inline">\(\langle x^2(t)
\rangle\)</span>æ˜¯ä½ç§»<em>å¹³æ–¹</em>çš„å¹³å‡å€¼ã€‚</p>
<p>Since <span class="math inline">\(\langle x(t) \rangle = 0\)</span>,
the variance is simply equal to the MSD: <span
class="math display">\[\sigma^2 = \langle x^2(t) \rangle\]</span> ç”±äº
<span class="math inline">\(\langle x(t) \rangle =
0\)</span>ï¼Œæ–¹å·®ç­‰äºå‡æ–¹å·® (MSD)ï¼š <span class="math display">\[\sigma^2
= \langle x^2(t) \rangle\]</span></p>
<p>The crucial insight for a diffusive process is that the <strong>MSD
grows linearly with time</strong>. The rate of this growth is determined
by the <strong>diffusion coefficient, D</strong>. The board shows this
relationship for different dimensions: æ‰©æ•£è¿‡ç¨‹çš„å…³é”®åœ¨äº<strong>MSD
éšæ—¶é—´çº¿æ€§å¢é•¿</strong>ã€‚å…¶å¢é•¿ç‡ç”±<strong>æ‰©æ•£ç³»æ•°
D</strong>å†³å®šã€‚æ£‹ç›˜æ˜¾ç¤ºäº†ä¸åŒç»´åº¦ä¸‹çš„è¿™ç§å…³ç³»ï¼š</p>
<ul>
<li><strong>1D:</strong> <span class="math inline">\(\langle x^2(t)
\rangle = 2Dt\)</span> (Movement along a line) ï¼ˆæ²¿ç›´çº¿è¿åŠ¨ï¼‰</li>
<li><strong>2D:</strong> The board has a slight typo or ambiguity with
<span class="math inline">\(\langle z^2(t) \rangle = 2Dt\)</span>. For
2D motion in the x-y plane, the total MSD would be <span
class="math inline">\(\langle r^2(t) \rangle = \langle x^2(t) \rangle +
\langle y^2(t) \rangle = 4Dt\)</span>. The note on the board might be
referring to just one component of motion. **æ£‹ç›˜ä¸Šçš„ <span
class="math inline">\(\langle z^2(t) \rangle = 2Dt\)</span>
å­˜åœ¨è½»å¾®æ‹¼å†™é”™è¯¯æˆ–æ­§ä¹‰ã€‚å¯¹äº x-y å¹³é¢ä¸Šçš„äºŒç»´è¿åŠ¨ï¼Œæ€»å¹³å‡æ•£å°„å·® (MSD) ä¸º
<span class="math inline">\(\langle r^2(t) \rangle = \langle x^2(t)
\rangle + \langle y^2(t) \rangle =
4Dt\)</span>ã€‚é»‘æ¿ä¸Šçš„æ³¨é‡Šå¯èƒ½ä»…æŒ‡è¿åŠ¨çš„ä¸€ä¸ªåˆ†é‡ã€‚</li>
<li><strong>3D:</strong> <span class="math inline">\(\langle r^2(t)
\rangle = \langle |\vec{r}(t) - \vec{r}(0)|^2 \rangle = 6Dt\)</span>
(Movement in 3D space, which is the most common case in molecular
simulations) ï¼ˆä¸‰ç»´ç©ºé—´ä¸­çš„è¿åŠ¨ï¼Œè¿™æ˜¯åˆ†å­æ¨¡æ‹Ÿä¸­æœ€å¸¸è§çš„æƒ…å†µï¼‰ Here,
<span class="math inline">\(\vec{r}(t)\)</span> is the position vector
of a particle at time <code>t</code>. The quantity <span
class="math inline">\(\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle\)</span> is the average of the squared distance a particle has
traveled from its initial position <span
class="math inline">\(\vec{r}(0)\)</span>. è¿™é‡Œï¼Œ<span
class="math inline">\(\vec{r}(t)\)</span> æ˜¯ç²’å­åœ¨æ—¶é—´ <code>t</code>
çš„ä½ç½®çŸ¢é‡ã€‚ <span class="math inline">\(\langle |\vec{r}(t) -
\vec{r}(0)|^2 \rangle\)</span> æ˜¯ç²’å­ä»å…¶åˆå§‹ä½ç½® <span
class="math inline">\(\vec{r}(0)\)</span> è¡Œè¿›è·ç¦»çš„å¹³æ–¹å¹³å‡å€¼ã€‚</li>
</ul>
<h3
id="the-einstein-relation-connecting-microscopic-motion-to-a-macroscopic-property-çˆ±å› æ–¯å¦å…³ç³»å°†å¾®è§‚è¿åŠ¨ä¸å®è§‚ç‰¹æ€§è”ç³»èµ·æ¥">##
4. The Einstein Relation: Connecting Microscopic Motion to a Macroscopic
Property çˆ±å› æ–¯å¦å…³ç³»ï¼šå°†å¾®è§‚è¿åŠ¨ä¸å®è§‚ç‰¹æ€§è”ç³»èµ·æ¥</h3>
<p>Finally, the board presents the famous <strong>Einstein
relation</strong>, which rearranges the 3D MSD equation to solve for the
diffusion coefficient <code>D</code>:</p>
<p><span class="math display">\[D = \lim_{t \to \infty} \frac{\langle
|\vec{r}(t) - \vec{r}(0)|^2 \rangle}{6t}\]</span></p>
<p>This is a cornerstone equation in statistical mechanics. It provides
a practical way to calculate a macroscopic propertyâ€”the
<strong>diffusion coefficient <code>D</code></strong>â€”from the
microscopic movements of individual particles observed in a computer
simulation.
è¿™æ˜¯ç»Ÿè®¡åŠ›å­¦ä¸­çš„ä¸€ä¸ªåŸºçŸ³æ–¹ç¨‹ã€‚å®ƒæä¾›äº†ä¸€ç§å®ç”¨çš„æ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—æœºæ¨¡æ‹Ÿä¸­è§‚å¯Ÿåˆ°çš„å•ä¸ªç²’å­çš„å¾®è§‚è¿åŠ¨æ¥è®¡ç®—å®è§‚å±æ€§â€”â€”æ‰©æ•£ç³»æ•°â€œDâ€ã€‚</p>
<p>In practice, one would: 1. Run a simulation of particles.
è¿è¡Œç²’å­æ¨¡æ‹Ÿã€‚ 2. Track the position of each particle over time.
è·Ÿè¸ªæ¯ä¸ªç²’å­éšæ—¶é—´çš„ä½ç½®ã€‚ 3. Calculate the squared displacement <span
class="math inline">\(|\vec{r}(t) - \vec{r}(0)|^2\)</span> for each
particle at various time intervals <code>t</code>.
è®¡ç®—æ¯ä¸ªç²’å­åœ¨ä¸åŒæ—¶é—´é—´éš”â€œtâ€çš„ä½ç§»å¹³æ–¹<span
class="math inline">\(|\vec{r}(t) - \vec{r}(0)|^2\)</span>ã€‚ 4. Average
this value over all particles to get the MSD, <span
class="math inline">\(\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle\)</span>. å¯¹æ‰€æœ‰ç²’å­å–å¹³å‡å€¼ï¼Œå¾—åˆ°å‡æ–¹å·®ï¼ˆMSDï¼‰ï¼Œå³<span
class="math inline">\(\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle\)</span>ã€‚ 5. Plot the MSD as a function of time.
å°†MSDç»˜åˆ¶æˆæ—¶é—´å‡½æ•°ã€‚ 6. The slope of this line, divided by 6, gives the
diffusion coefficient <code>D</code>. The <code>lim tâ†’âˆ</code> indicates
that this linear relationship is most accurate for long time scales,
after initial transient effects have died down.
è¿™æ¡ç›´çº¿çš„æ–œç‡é™¤ä»¥6ï¼Œå³æ‰©æ•£ç³»æ•°â€œDâ€ã€‚â€œlim
tâ†’âˆâ€è¡¨æ˜ï¼Œåœ¨åˆå§‹ç¬æ€æ•ˆåº”æ¶ˆé€€åï¼Œè¿™ç§çº¿æ€§å…³ç³»åœ¨é•¿æ—¶é—´å°ºåº¦ä¸Šæœ€ä¸ºå‡†ç¡®ã€‚</p>
<h3 id="right-board-green-kubo-relations">## 5. Right Board: Green-Kubo
Relations</h3>
<p>This board introduces a more advanced and powerful method to
calculate transport coefficients like the diffusion coefficient, known
as the <strong>Green-Kubo relations</strong>.
æœ¬é¢æ¿ä»‹ç»äº†ä¸€ç§æ›´å…ˆè¿›ã€æ›´å¼ºå¤§çš„æ–¹æ³•æ¥è®¡ç®—æ‰©æ•£ç³»æ•°ç­‰ä¼ è¾“ç³»æ•°ï¼Œå³<strong>Green-Kubo
å…³ç³»</strong>ã€‚</p>
<h4 id="velocity-autocorrelation-function-vacf-é€Ÿåº¦è‡ªç›¸å…³å‡½æ•°-vacf">###
<strong>Velocity Autocorrelation Function (VACF)</strong> é€Ÿåº¦è‡ªç›¸å…³å‡½æ•°
(VACF)</h4>
<p>The key idea is to look at how a particleâ€™s velocity at one point in
time is related to its velocity at a later time. This is measured by the
<strong>Velocity Autocorrelation Function (VACF)</strong>: <span
class="math display">\[C_{vv}(t) = \langle \vec{v}(t&#39;) \cdot
\vec{v}(t&#39; + t) \rangle\]</span> This function tells us how long a
particle â€œremembersâ€ its velocity. For a typical liquid, the velocity is
quickly randomized by collisions, so the VACF decays to zero rapidly.
å…¶æ ¸å¿ƒæ€æƒ³æ˜¯è€ƒå¯Ÿç²’å­åœ¨æŸä¸€æ—¶é—´ç‚¹çš„é€Ÿåº¦ä¸å…¶åœ¨ä¹‹åæ—¶é—´ç‚¹çš„é€Ÿåº¦ä¹‹é—´çš„å…³ç³»ã€‚è¿™å¯ä»¥é€šè¿‡<strong>é€Ÿåº¦è‡ªç›¸å…³å‡½æ•°
(VACF)</strong>æ¥æµ‹é‡ï¼š <span class="math display">\[C_{vv}(t) = \langle
\vec{v}(t&#39;) \cdot \vec{v}(t&#39; + t) \rangle\]</span>
æ­¤å‡½æ•°å‘Šè¯‰æˆ‘ä»¬ç²’å­â€œè®°ä½â€å…¶é€Ÿåº¦çš„æ—¶é—´ã€‚å¯¹äºå…¸å‹çš„æ¶²ä½“ï¼Œé€Ÿåº¦ä¼šå› ç¢°æ’è€Œè¿…é€ŸéšæœºåŒ–ï¼Œå› æ­¤
VACF ä¼šè¿…é€Ÿè¡°å‡ä¸ºé›¶ã€‚</p>
<h4 id="connecting-msd-and-vacf">### <strong>Connecting MSD and
VACF</strong></h4>
<p>The board shows the mathematical link between the MSD and the VACF.
Starting with the definition of position as the integral of velocity,
<span class="math inline">\(\vec{r}(t) = \int_0^t \vec{v}(t&#39;)
dt&#39;\)</span>, one can show that the MSD is a double integral of the
VACF. The board writes this as: <span class="math display">\[\langle
x^2(t) \rangle = \left\langle \left( \int_0^t v(t&#39;) dt&#39; \right)
\left( \int_0^t v(t&#39;&#39;) dt&#39;&#39; \right) \right\rangle =
\int_0^t dt&#39; \int_0^t dt&#39;&#39; \langle v(t&#39;) v(t&#39;&#39;)
\rangle\]</span> This shows that the two pictures of motionâ€”the
particleâ€™s displacement (MSD) and its velocity fluctuations (VACF)â€”are
deeply connected. è¯¥é¢æ¿å±•ç¤ºäº† MSD å’Œ VACF
ä¹‹é—´çš„æ•°å­¦è”ç³»ã€‚ä»ä½ç½®å®šä¹‰ä¸ºé€Ÿåº¦çš„ç§¯åˆ†å¼€å§‹ï¼Œ<span
class="math inline">\(\vec{r}(t) = \int_0^t \vec{v}(t&#39;)
dt&#39;\)</span>ï¼Œå¯ä»¥è¯æ˜ MSD æ˜¯ VACF çš„äºŒé‡ç§¯åˆ†ã€‚é»‘æ¿ä¸Šå†™ç€ï¼š <span
class="math display">\[\langle x^2(t) \rangle = \left\langle \left(
\int_0^t v(t&#39;) dt&#39; \right) \left( \int_0^t v(t&#39;&#39;)
dt&#39;&#39; \right) \right\rangle = \int_0^t dt&#39; \int_0^t
dt&#39;&#39; \langle v(t&#39;) v(t&#39;&#39;) \rangle\]</span>
è¿™è¡¨æ˜ï¼Œç²’å­è¿åŠ¨çš„ä¸¤å¹…å›¾åƒâ€”â€”ç²’å­çš„ä½ç§»ï¼ˆMSDï¼‰å’Œé€Ÿåº¦æ¶¨è½ï¼ˆVACFï¼‰â€”â€”ä¹‹é—´å­˜åœ¨ç€æ·±åˆ»çš„è”ç³»ã€‚</p>
<h4 id="the-green-kubo-formula-for-diffusion-æ‰©æ•£çš„æ ¼æ—-ä¹…ä¿å…¬å¼">###
<strong>The Green-Kubo Formula for Diffusion
æ‰©æ•£çš„æ ¼æ—-ä¹…ä¿å…¬å¼</strong></h4>
<p>By combining the Einstein relation with the integral of the VACF, one
arrives at the Green-Kubo formula for the diffusion coefficient: <span
class="math display">\[D = \frac{1}{3} \int_0^\infty \langle \vec{v}(0)
\cdot \vec{v}(t) \rangle dt\]</span> This incredible result states that
the <strong>macroscopic</strong> property of diffusion (<span
class="math inline">\(D\)</span>) is determined by the integral of the
<strong>microscopic</strong> velocity correlations. Itâ€™s often a more
efficient way to compute <span class="math inline">\(D\)</span> in
simulations than calculating the long-time limit of the MSD.
å°†çˆ±å› æ–¯å¦å…³ç³»ä¸VACFç§¯åˆ†ç›¸ç»“åˆï¼Œå¯ä»¥å¾—åˆ°æ‰©æ•£ç³»æ•°çš„æ ¼æ—-ä¹…ä¿å…¬å¼ï¼š <span
class="math display">\[D = \frac{1}{3} \int_0^\infty \langle \vec{v}(0)
\cdot \vec{v}(t) \rangle dt\]</span>
è¿™ä¸ªä»¤äººéš¾ä»¥ç½®ä¿¡çš„ç»“æœè¡¨æ˜ï¼Œæ‰©æ•£çš„<strong>å®è§‚</strong>ç‰¹æ€§ï¼ˆ<span
class="math inline">\(D\)</span>ï¼‰ç”±<strong>å¾®è§‚</strong>é€Ÿåº¦å…³è”çš„ç§¯åˆ†å†³å®šã€‚åœ¨æ¨¡æ‹Ÿä¸­ï¼Œè¿™é€šå¸¸æ˜¯è®¡ç®—<span
class="math inline">\(D\)</span>æ¯”è®¡ç®—MSDçš„é•¿æœŸæé™æ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚</p>
<h3 id="the-grand-narrative-from-micro-to-macro-å®å¤§å™äº‹ä»å¾®è§‚åˆ°å®è§‚">##
6. The Grand Narrative: From Micro to Macro å®å¤§å™äº‹ï¼šä»å¾®è§‚åˆ°å®è§‚</h3>
<p>The previous whiteboards gave us two ways to calculate the
<strong>diffusion constant, D</strong>, from the microscopic random walk
of individual atoms:
ä¹‹å‰çš„ç™½æ¿æä¾›äº†ä¸¤ç§ä»å•ä¸ªåŸå­çš„å¾®è§‚éšæœºæ¸¸åŠ¨è®¡ç®—<strong>æ‰©æ•£å¸¸æ•°
D</strong>çš„æ–¹æ³•ï¼š 1. <strong>Einstein Relation:</strong> From the
long-term slope of the Mean Squared Displacement (MSD). æ ¹æ®å‡æ–¹ä½ç§»
(MSD) çš„é•¿æœŸæ–œç‡ã€‚ 2. <strong>Green-Kubo Relation:</strong> From the
integral of the Velocity Autocorrelation Function (VACF).
æ ¹æ®é€Ÿåº¦è‡ªç›¸å…³å‡½æ•° (VACF) çš„ç§¯åˆ†ã€‚</p>
<p>This new whiteboard shows how that single microscopic parameter,
<code>D</code>, governs the large-scale, observable process of diffusion
described by <strong>Fickâ€™s Laws</strong> and the <strong>Diffusion
Equation</strong>. è¿™å—æ–°çš„ç™½æ¿å±•ç¤ºäº†å•ä¸ªå¾®è§‚å‚æ•° <code>D</code>
å¦‚ä½•æ§åˆ¶<strong>è²å…‹å®šå¾‹</strong>å’Œ<strong>æ‰©æ•£æ–¹ç¨‹</strong>æ‰€æè¿°çš„å¤§è§„æ¨¡å¯è§‚æµ‹æ‰©æ•£è¿‡ç¨‹ã€‚</p>
<h3 id="the-starting-point-a-liquids-structure-èµ·ç‚¹æ¶²ä½“çš„ç»“æ„">## 1. The
Starting Point: A Liquidâ€™s Structure èµ·ç‚¹ï¼šæ¶²ä½“çš„ç»“æ„</h3>
<p>The plot on the top left is the <strong>Radial Distribution Function,
<span class="math inline">\(g(r)\)</span></strong>, which we discussed
in detail from the first whiteboard. å·¦ä¸Šè§’çš„å›¾æ˜¯<strong>å¾„å‘åˆ†å¸ƒå‡½æ•°
<span
class="math inline">\(g(r)\)</span></strong>ï¼Œæˆ‘ä»¬åœ¨ç¬¬ä¸€ä¸ªç™½æ¿ä¸Šè¯¦ç»†è®¨è®ºè¿‡å®ƒã€‚</p>
<ul>
<li><strong>The Plot:</strong> It shows the characteristic structure of
a liquid. The peaks are labeled â€œ1stâ€, â€œ2ndâ€, and â€œ3rdâ€, corresponding
to the first, second, and third <strong>solvation shells</strong>
(layers of neighboring atoms).
å®ƒæ˜¾ç¤ºäº†æ¶²ä½“çš„ç‰¹å¾ç»“æ„ã€‚å³°åˆ†åˆ«æ ‡è®°ä¸ºâ€œç¬¬ä¸€â€ã€â€œç¬¬äºŒâ€å’Œâ€œç¬¬ä¸‰â€ï¼Œåˆ†åˆ«å¯¹åº”äºç¬¬ä¸€ã€ç¬¬äºŒå’Œç¬¬ä¸‰<strong>æº¶å‰‚åŒ–å£³å±‚</strong>ï¼ˆç›¸é‚»åŸå­å±‚ï¼‰ã€‚</li>
<li><strong>The Limit:</strong> The note <code>lim râ†’âˆ g(r) = 1</code>
confirms that at large distances, the liquid has no long-range order, as
expected.æ³¨é‡Šâ€œlim râ†’âˆ g(r) =
1â€è¯å®äº†åœ¨è¿œè·ç¦»ä¸‹ï¼Œæ¶²ä½“æ²¡æœ‰é•¿ç¨‹æœ‰åºï¼Œè¿™ä¸é¢„æœŸä¸€è‡´ã€‚</li>
<li><strong>System Parameters:</strong> The values <code>T = 0.71</code>
and <code>Ï = 0.844</code> are the temperature and density of the
simulated system (likely in reduced or â€œLennard-Jonesâ€ units) for which
this <span class="math inline">\(g(r)\)</span> was calculated. å€¼â€œT =
0.71â€å’Œâ€œÏ =
0.844â€åˆ†åˆ«æ˜¯æ¨¡æ‹Ÿç³»ç»Ÿçš„æ¸©åº¦å’Œå¯†åº¦ï¼ˆå¯èƒ½é‡‡ç”¨çº¦åŒ–æˆ–â€œLennard-Jonesâ€å•ä½ï¼‰ï¼Œç”¨äºè®¡ç®—æ­¤
<span class="math inline">\(g(r)\)</span>ã€‚</li>
</ul>
<p>This section sets the stage: we are looking at the dynamics within a
system that has this specific liquid-like structure.
æœ¬èŠ‚å¥ å®šäº†åŸºç¡€ï¼šæˆ‘ä»¬å°†ç ”ç©¶å…·æœ‰ç‰¹å®šç±»æ¶²ä½“ç»“æ„çš„ç³»ç»Ÿå†…çš„åŠ¨åŠ›å­¦ã€‚</p>
<h3 id="the-macroscopic-laws-of-diffusion-å®è§‚æ‰©æ•£å®šå¾‹">## 2. The
Macroscopic Laws of Diffusion å®è§‚æ‰©æ•£å®šå¾‹</h3>
<p>The bottom-left and top-right sections introduce the continuum
equations that describe how concentration changes in space and time.
å·¦ä¸‹è§’å’Œå³ä¸Šè§’éƒ¨åˆ†ä»‹ç»äº†æè¿°æµ“åº¦éšç©ºé—´å’Œæ—¶é—´å˜åŒ–çš„è¿ç»­æ–¹ç¨‹ã€‚å·¦ä¸‹è§’å’Œå³ä¸Šè§’éƒ¨åˆ†ä»‹ç»äº†æè¿°æµ“åº¦éšç©ºé—´å’Œæ—¶é—´å˜åŒ–çš„è¿ç»­æ–¹ç¨‹ã€‚</p>
<h4 id="ficks-first-law-è²å…‹ç¬¬ä¸€å®šå¾‹">### <strong>Fickâ€™s First Law
è²å…‹ç¬¬ä¸€å®šå¾‹</strong></h4>
<p><span class="math display">\[\vec{J} = -D \nabla C\]</span> This is
Fickâ€™s first law of diffusion. It states that there is a
<strong>flux</strong> of particles (<span
class="math inline">\(\vec{J}\)</span>), meaning a net flow. This flow
is directed from high concentration to low concentration (hence the
minus sign) and its magnitude is proportional to the
<strong>concentration gradient</strong> (<span
class="math inline">\(\nabla C\)</span>).
è¿™æ˜¯è²å…‹ç¬¬ä¸€æ‰©æ•£å®šå¾‹ã€‚å®ƒæŒ‡å‡ºå­˜åœ¨ç²’å­çš„<strong>é€šé‡</strong> (<span
class="math inline">\(\vec{J}\)</span>)ï¼Œå³å‡€æµé‡ã€‚è¯¥æµé‡ä»é«˜æµ“åº¦æµå‘ä½æµ“åº¦ï¼ˆå› æ­¤å¸¦æœ‰è´Ÿå·ï¼‰ï¼Œå…¶å¤§å°ä¸<strong>æµ“åº¦æ¢¯åº¦</strong>
(<span class="math inline">\(\nabla C\)</span>) æˆæ­£æ¯”ã€‚</p>
<p><strong>The Crucial Link:</strong> The proportionality constant is
<strong>D</strong>, the very same <strong>diffusion constant</strong> we
calculated from the microscopic random walk (MSD/VACF). This is the key
connection: the collective result of countless individual random walks
is a predictable net flow of particles.
æ¯”ä¾‹å¸¸æ•°æ˜¯<strong>D</strong>ï¼Œä¸æˆ‘ä»¬æ ¹æ®å¾®è§‚éšæœºæ¸¸èµ° (MSD/VACF)
è®¡ç®—å‡ºçš„<strong>æ‰©æ•£å¸¸æ•°</strong>å®Œå…¨ç›¸åŒã€‚è¿™æ˜¯å…³é”®çš„è”ç³»ï¼šæ— æ•°ä¸ªä½“éšæœºæ¸¸åŠ¨çš„é›†åˆç»“æœæ˜¯å¯é¢„æµ‹çš„ç²’å­å‡€æµã€‚</p>
<h4
id="the-diffusion-equation-ficks-second-law-æ‰©æ•£æ–¹ç¨‹è²å…‹ç¬¬äºŒå®šå¾‹">###
<strong>The Diffusion Equation (Fickâ€™s Second Law)
æ‰©æ•£æ–¹ç¨‹ï¼ˆè²å…‹ç¬¬äºŒå®šå¾‹ï¼‰</strong></h4>
<p><span class="math display">\[\frac{\partial C(\vec{r},t)}{\partial t}
= D \nabla^2 C(\vec{r},t)\]</span> This is the <strong>diffusion
equation</strong>, one of the most important equations in physics and
chemistry (also called the heat equation, as noted). Itâ€™s derived from
Fickâ€™s first law and the principle of mass conservation (<span
class="math inline">\(\frac{\partial C}{\partial t} + \nabla \cdot
\vec{J} = 0\)</span>). Itâ€™s a differential equation that tells you
exactly how the concentration at any point, <span
class="math inline">\(C(\vec{r},t)\)</span>, will change over time.
è¿™å°±æ˜¯<strong>æ‰©æ•£æ–¹ç¨‹</strong>ï¼Œå®ƒæ˜¯ç‰©ç†å­¦å’ŒåŒ–å­¦ä¸­æœ€é‡è¦çš„æ–¹ç¨‹ä¹‹ä¸€ï¼ˆä¹Ÿç§°ä¸ºçƒ­æ–¹ç¨‹ï¼‰ã€‚å®ƒæºäºè²å…‹ç¬¬ä¸€å®šå¾‹å’Œè´¨é‡å®ˆæ’å®šå¾‹ï¼ˆ<span
class="math inline">\(\frac{\partial C}{\partial t} + \nabla \cdot
\vec{J} = 0\)</span>ï¼‰ã€‚å®ƒæ˜¯ä¸€ä¸ªå¾®åˆ†æ–¹ç¨‹ï¼Œå¯ä»¥ç²¾ç¡®åœ°å‘Šè¯‰ä½ ä»»æ„ä¸€ç‚¹çš„æµ“åº¦
<span class="math inline">\(C(\vec{r},t)\)</span> éšæ—¶é—´çš„å˜åŒ–ã€‚</p>
<h3
id="the-solution-connecting-back-to-the-random-walk-ä¸éšæœºæ¸¸åŠ¨è”ç³»èµ·æ¥">##
3. The Solution: Connecting Back to the Random Walk
ä¸éšæœºæ¸¸åŠ¨è”ç³»èµ·æ¥</h3>
<p>This is the most beautiful part. The board shows the solution to the
diffusion equation for a very specific scenario, linking the macroscopic
equation directly back to the microscopic random walk.
é»‘æ¿ä¸Šå±•ç¤ºäº†ä¸€ä¸ªéå¸¸å…·ä½“åœºæ™¯ä¸‹æ‰©æ•£æ–¹ç¨‹çš„è§£ï¼Œå°†å®è§‚æ–¹ç¨‹ç›´æ¥ä¸å¾®è§‚éšæœºæ¸¸åŠ¨è”ç³»èµ·æ¥ã€‚</p>
<h4 id="the-initial-condition-åˆå§‹æ¡ä»¶">### <strong>The Initial
Condition åˆå§‹æ¡ä»¶</strong></h4>
<p>The problem is set up by assuming all particles start at a single
point at time zero: <span class="math display">\[C(\vec{r}, 0) =
\delta(\vec{r})\]</span> This is a <strong>Dirac delta
function</strong>, representing an infinitely concentrated point source
at the origin. é—®é¢˜å‡è®¾æ‰€æœ‰ç²’å­åœ¨æ—¶é—´é›¶ç‚¹å¤„ä»ä¸€ä¸ªç‚¹å¼€å§‹ï¼š <span
class="math display">\[C(\vec{r}, 0) = \delta(\vec{r})\]</span>
è¿™æ˜¯ä¸€ä¸ª<strong>ç‹„æ‹‰å…‹å‡½æ•°</strong>ï¼Œè¡¨ç¤ºä¸€ä¸ªåœ¨åŸç‚¹å¤„æ— é™é›†ä¸­çš„ç‚¹æºã€‚</p>
<h4 id="the-fundamental-solution-greens-function-åŸºæœ¬è§£æ ¼æ—å‡½æ•°">###
<strong>The Fundamental Solution (Greenâ€™s Function)
åŸºæœ¬è§£ï¼ˆæ ¼æ—å‡½æ•°ï¼‰</strong></h4>
<p>The solution to the diffusion equation with this starting condition
is called the <strong>fundamental solution</strong> or <strong>Greenâ€™s
function</strong>. For one dimension, it is: <span
class="math display">\[C(x,t) = \frac{1}{\sqrt{4\pi Dt}}
\exp\left(-\frac{x^2}{4Dt}\right)\]</span></p>
<p><strong>The â€œAha!â€ Moment:</strong> This is a <strong>Gaussian
distribution</strong>. Letâ€™s compare it to the formula from the second
whiteboard: * The mean is <span class="math inline">\(\mu=0\)</span>.
å‡å€¼ä¸º <span class="math inline">\(\mu=0\)</span>ã€‚ * The variance is
<span class="math inline">\(\sigma^2 = 2Dt\)</span>. æ–¹å·®ä¸º <span
class="math inline">\(\sigma^2 = 2Dt\)</span>ã€‚</p>
<p>This is an incredible result. The macroscopic diffusion equation
predicts that a concentration pulse will spread out over time, and the
shape of the concentration profile will be a Gaussian curve. The width
of this curve, measured by its variance <span
class="math inline">\(\sigma^2\)</span>, is <strong>exactly the Mean
Squared Displacement, <span class="math inline">\(\langle x^2(t)
\rangle\)</span>, of the individual random-walking particles.</strong>
å®è§‚æ‰©æ•£æ–¹ç¨‹é¢„æµ‹æµ“åº¦è„‰å†²ä¼šéšæ—¶é—´æ‰©æ•£ï¼Œæµ“åº¦åˆ†å¸ƒçš„å½¢çŠ¶å°†æ˜¯é«˜æ–¯æ›²çº¿ã€‚è¿™æ¡æ›²çº¿çš„å®½åº¦ï¼Œç”¨å…¶æ–¹å·®
<span class="math inline">\(\sigma^2\)</span>
æ¥è¡¡é‡ï¼Œ<strong>æ°å¥½æ˜¯å•ä¸ªéšæœºæ¸¸åŠ¨ç²’å­çš„å‡æ–¹ä½ç§» <span
class="math inline">\(\langle x^2(t) \rangle\)</span>ã€‚</strong></p>
<p>This perfectly unites the two perspectives: * <strong>Microscopicå¾®è§‚
(Board 2):</strong> Particles undergo a random walk, and their average
squared displacement from the origin grows as <span
class="math inline">\(\langle x^2(t) \rangle = 2Dt\)</span>.
ç²’å­è¿›è¡Œéšæœºæ¸¸åŠ¨ï¼Œå®ƒä»¬ç›¸å¯¹äºåŸç‚¹çš„å¹³å‡å¹³æ–¹ä½ç§»éšç€ <span
class="math inline">\(\langle x^2(t) \rangle = 2Dt\)</span>
çš„å¢é•¿è€Œå¢é•¿ã€‚ * <strong>Macroscopicå®è§‚ (This Board):</strong> A
collection of these particles, described by a continuum concentration
<code>C</code>, spreads out in a Gaussian profile whose variance is
<span class="math inline">\(\sigma^2 = 2Dt\)</span>.
è¿™äº›ç²’å­çš„é›†åˆï¼Œç”¨è¿ç»­æµ“åº¦â€œCâ€æ¥æè¿°ï¼Œå‘ˆæ–¹å·®ä¸º <span
class="math inline">\(\sigma^2 = 2Dt\)</span> çš„é«˜æ–¯åˆ†å¸ƒã€‚</p>
<p>The two pictures are mathematically identical.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/17/5120C3-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/17/5120C3-2/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W3-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-17 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-17T21:00:00+08:00">2025-09-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-21 05:21:00" itemprop="dateModified" datetime="2025-09-21T05:21:00+08:00">2025-09-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - è®¡ç®—èƒ½æºææ–™å’Œç”µå­ç»“æ„æ¨¡æ‹Ÿ Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="radial-distribution-function-rdfé™æ€ç»“æ„">1 radial distribution
function RDFé™æ€ç»“æ„:</h2>
<ul>
<li><strong>å†…å®¹</strong>: This whiteboard serves as an excellent
summary, pulling together all the key concepts weâ€™ve discussed into a
single, cohesive picture. Letâ€™s connect everything on this slide to our
detailed conversation.</li>
</ul>
<h3 id="rdf-the-static-structure-rdfé™æ€ç»“æ„">1. RDF: The Static
Structure RDFé™æ€ç»“æ„</h3>
<p>On the top left, you see <strong>RDF (Radial Distribution
Function)</strong>.</p>
<ul>
<li><strong>The Plots:</strong> The board shows the familiar <span
class="math inline">\(g(r)\)</span> plot with its characteristic peaks
for a liquid. Below it is a plot of the interatomic potential energy,
<span class="math inline">\(V(r)\)</span>. This addition is very
insightful! It shows <em>why</em> the first peak in <span
class="math inline">\(g(r)\)</span> exists: it corresponds to the
minimum energy distance (<span class="math inline">\(\sigma\)</span>)
where particles are most stable and likely to be found.
ç™½æ¿å±•ç¤ºäº†æˆ‘ä»¬ç†Ÿæ‚‰çš„<span
class="math inline">\(g(r)\)</span>å›¾ï¼Œå®ƒå¸¦æœ‰æ¶²ä½“çš„ç‰¹å¾å³°ã€‚ä¸‹æ–¹æ˜¯åŸå­é—´åŠ¿èƒ½<span
class="math inline">\(V(r)\)</span>çš„å›¾ã€‚è¿™ä¸ªè¡¥å……éå¸¸æœ‰è§åœ°ï¼å®ƒè§£é‡Šäº†ä¸ºä»€ä¹ˆ
<span class="math inline">\(g(r)\)</span>
ä¸­çš„ç¬¬ä¸€ä¸ªå³°å€¼å­˜åœ¨ï¼šå®ƒå¯¹åº”äºç²’å­æœ€ç¨³å®šä¸”æœ€æœ‰å¯èƒ½è¢«å‘ç°çš„æœ€å°èƒ½é‡è·ç¦»
(<span class="math inline">\(\sigma\)</span>)ã€‚</li>
<li><strong>Connection:</strong> This section summarizes our first
discussion. Itâ€™s the starting point for our analysisâ€”a static snapshot
of the materialâ€™s average atomic arrangement before we consider how the
atoms move.
æœ¬èŠ‚æ€»ç»“äº†æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªè®¨è®ºã€‚è¿™æ˜¯æˆ‘ä»¬åˆ†æçš„èµ·ç‚¹â€”â€”åœ¨æˆ‘ä»¬è€ƒè™‘åŸå­å¦‚ä½•è¿åŠ¨ä¹‹å‰ï¼Œå®ƒæ˜¯ææ–™å¹³å‡åŸå­æ’åˆ—çš„é™æ€å¿«ç…§ã€‚</li>
</ul>
<h3
id="msd-and-the-einstein-relation-the-displacement-picture-å‡æ–¹ä½ç§»-msd-å’Œçˆ±å› æ–¯å¦å…³ç³»ä½ç§»å›¾åƒ">2.
MSD and The Einstein Relation: The Displacement Picture å‡æ–¹ä½ç§» (MSD)
å’Œçˆ±å› æ–¯å¦å…³ç³»ï¼šä½ç§»å›¾åƒ</h3>
<p>The board then moves to dynamics, presenting two methods to calculate
the <strong>diffusion constant, D</strong>. The first is the
<strong>Einstein relation</strong>. ä¸¤ç§è®¡ç®—<strong>æ‰©æ•£å¸¸æ•°
D</strong>çš„æ–¹æ³•ã€‚ç¬¬ä¸€ç§æ˜¯<strong>çˆ±å› æ–¯å¦å…³ç³»</strong>ã€‚</p>
<ul>
<li><strong>The Formula:</strong> It correctly states that the Mean
Squared Displacement (MSD), <span class="math inline">\(\langle r^2
\rangle\)</span>, is equal to <span class="math inline">\(6Dt\)</span>
in three dimensions. It then rearranges this to solve for <span
class="math inline">\(D\)</span>: å®ƒæ­£ç¡®åœ°æŒ‡å‡ºäº†å‡æ–¹ä½ç§» (MSD)ï¼Œ<span
class="math inline">\(\langle r^2 \rangle\)</span>ï¼Œåœ¨ä¸‰ç»´ç©ºé—´ä¸­ç­‰äº
<span class="math inline">\(6Dt\)</span>ã€‚ç„¶åé‡æ–°æ’åˆ—è¯¥å…¬å¼ä»¥æ±‚è§£ <span
class="math inline">\(D\)</span>ï¼š <span class="math display">\[D =
\lim_{t\to\infty} \frac{\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle}{6t}\]</span></li>
<li><strong>The Diagram:</strong> The central diagram beautifully
illustrates the concept. It shows a particle in a simulation box (with
â€œN=108â€ likely being the number of particles simulated) moving from an
initial position <span class="math inline">\(\vec{r}_i(0)\)</span> to a
final position <span class="math inline">\(\vec{r}_i(t_j)\)</span>. The
MSD is the average of the square of this displacement over all particles
and many time origins. The graph labeled â€œMSDâ€ shows how you would plot
this data and find the slope (â€œfittingâ€) to calculate <span
class="math inline">\(D\)</span>.
ä¸­é—´çš„å›¾è¡¨å®Œç¾åœ°é˜é‡Šäº†è¿™ä¸ªæ¦‚å¿µã€‚å®ƒå±•ç¤ºäº†ä¸€ä¸ªç²’å­åœ¨æ¨¡æ‹Ÿæ¡†ä¸­ï¼ˆâ€œN=108â€
å¯èƒ½æ˜¯æ¨¡æ‹Ÿç²’å­çš„æ•°é‡ï¼‰ä»åˆå§‹ä½ç½® <span
class="math inline">\(\vec{r}_i(0)\)</span> ç§»åŠ¨åˆ°æœ€ç»ˆä½ç½® <span
class="math inline">\(\vec{r}_i(t_j)\)</span>ã€‚MSD
æ˜¯è¯¥ä½ç§»å¹³æ–¹åœ¨æ‰€æœ‰ç²’å­å’Œå¤šä¸ªæ—¶é—´åŸç‚¹ä¸Šçš„å¹³å‡å€¼ã€‚æ ‡æœ‰â€œMSDâ€çš„å›¾è¡¨æ˜¾ç¤ºäº†å¦‚ä½•ç»˜åˆ¶è¿™äº›æ•°æ®å¹¶æ‰¾åˆ°æ–œç‡ï¼ˆâ€œæ‹Ÿåˆâ€ï¼‰æ¥è®¡ç®—
<span class="math inline">\(D\)</span>ã€‚</li>
<li><strong>Connection:</strong> This is a perfect summary of the
â€œDisplacement Pictureâ€ we analyzed on the second whiteboard. Itâ€™s the
most intuitive way to think about diffusion: how far particles spread
out over
time.è¿™å®Œç¾åœ°æ€»ç»“äº†æˆ‘ä»¬åœ¨ç¬¬äºŒä¸ªç™½æ¿ä¸Šåˆ†æçš„â€œä½ç§»å›¾â€ã€‚è¿™æ˜¯æ€è€ƒæ‰©æ•£æœ€ç›´è§‚çš„æ–¹å¼ï¼šç²’å­éšæ—¶é—´æ‰©æ•£çš„è·ç¦»ã€‚</li>
</ul>
<h3
id="the-green-kubo-relation-the-fluctuation-picture-æ ¼æ—-ä¹…ä¿å…³ç³»æ¶¨è½å›¾">3.
The Green-Kubo Relation: The Fluctuation Picture
æ ¼æ—-ä¹…ä¿å…³ç³»ï¼šæ¶¨è½å›¾</h3>
<p>Finally, the board presents the more advanced but often more
practical method: the <strong>Green-Kubo relation</strong>.</p>
<ul>
<li><strong>The Equations:</strong> This section displays the two key
equations from our last discussion:
<ol type="1">
<li>The MSD as the double integral of the Velocity Autocorrelation
Function (VACF). é€Ÿåº¦è‡ªç›¸å…³å‡½æ•° (VACF) çš„äºŒé‡ç§¯åˆ†çš„å‡æ–¹å·® (MSD)ã€‚</li>
<li>The crucial derivative step: <span
class="math inline">\(\frac{d\langle x^2(t)\rangle}{dt} = 2 \int_0^t
dt&#39;&#39; \langle V_x(t) V_x(t&#39;&#39;) \rangle\)</span>.
å…³é”®çš„å¯¼æ•°æ­¥éª¤ï¼š<span class="math inline">\(\frac{d\langle
x^2(t)\rangle}{dt} = 2 \int_0^t dt&#39;&#39; \langle V_x(t)
V_x(t&#39;&#39;) \rangle\)</span>ã€‚</li>
</ol></li>
<li><strong>The Diagram:</strong> The small diagram of a square with
axes <span class="math inline">\(t&#39;\)</span> and <span
class="math inline">\(t&#39;&#39;\)</span> visually represents the
two-dimensional domain of integration for the double integral.
ä¸€ä¸ªå¸¦æœ‰è½´ <span class="math inline">\(t&#39;\)</span> å’Œ <span
class="math inline">\(t&#39;&#39;\)</span>
çš„å°æ­£æ–¹å½¢å›¾ç›´è§‚åœ°è¡¨ç¤ºäº†äºŒé‡ç§¯åˆ†çš„äºŒç»´ç§¯åˆ†åŸŸã€‚</li>
<li><strong>Connection:</strong> This summarizes the â€œFluctuation
Picture.â€ It shows the mathematical heart of the derivation that proves
the Einstein and Green-Kubo methods are equivalent. As we concluded,
this method is often numerically superior because it involves
integrating a rapidly decaying function (the VACF) rather than finding
the slope of a noisy, unbounded function (the MSD).
è¿™æ¦‚æ‹¬äº†â€œæ¶¨è½å›¾â€ã€‚å®ƒå±•ç¤ºäº†è¯æ˜çˆ±å› æ–¯å¦æ–¹æ³•å’Œæ ¼æ—-ä¹…ä¿æ–¹æ³•ç­‰ä»·çš„æ¨å¯¼è¿‡ç¨‹çš„æ•°å­¦æ ¸å¿ƒã€‚æ­£å¦‚æˆ‘ä»¬æ€»ç»“çš„é‚£æ ·ï¼Œè¿™ç§æ–¹æ³•é€šå¸¸åœ¨æ•°å€¼ä¸Šæ›´èƒœä¸€ç­¹ï¼Œå› ä¸ºå®ƒæ¶‰åŠå¯¹å¿«é€Ÿè¡°å‡å‡½æ•°ï¼ˆVACFï¼‰è¿›è¡Œç§¯åˆ†ï¼Œè€Œä¸æ˜¯æ±‚å™ªå£°æ— ç•Œå‡½æ•°ï¼ˆMSDï¼‰çš„æ–œç‡ã€‚</li>
</ul>
<p>In essence, this single whiteboard is a complete roadmap for
analyzing diffusion in a molecular simulation. It shows how to first
characterize the materialâ€™s <strong>structure</strong> (<span
class="math inline">\(g(r)\)</span>) and then how to compute its key
dynamic propertyâ€”the <strong>diffusion constant
<code>D</code></strong>â€”using two powerful, interconnected methods.
æœ¬è´¨ä¸Šï¼Œè¿™å—ç™½æ¿å°±æ˜¯åˆ†å­æ¨¡æ‹Ÿä¸­åˆ†ææ‰©æ•£çš„å®Œæ•´è·¯çº¿å›¾ã€‚å®ƒå±•ç¤ºäº†å¦‚ä½•é¦–å…ˆè¡¨å¾ææ–™çš„<strong>ç»“æ„</strong>ï¼ˆ<span
class="math inline">\(g(r)\)</span>ï¼‰ï¼Œç„¶åå¦‚ä½•ä½¿ç”¨ä¸¤ç§å¼ºå¤§ä¸”ç›¸äº’å…³è”çš„æ–¹æ³•è®¡ç®—å…¶å…³é”®çš„åŠ¨æ€ç‰¹æ€§â€”â€”<strong>æ‰©æ•£å¸¸æ•°
<code>D</code></strong>ã€‚</p>
<p>This whiteboard beautifully concludes the derivation of the
Green-Kubo relation, showing the final formulas and how they are used in
practice. It provides the punchline to the mathematical story weâ€™ve been
following.</p>
<p>Letâ€™s break down the details.</p>
<h3 id="finalizing-the-derivation">4. Finalizing the Derivation</h3>
<p>The top lines of the board show the final step in connecting the Mean
Squared Displacement (MSD) to the Velocity Autocorrelation Function
(VACF).</p>
<p><span class="math display">\[\lim_{t\to\infty} \frac{d\langle x^2
\rangle}{dt} = 2 \int_0^\infty d\tau \langle V_x(0) V_x(\tau)
\rangle\]</span></p>
<ul>
<li><strong>The Left Side:</strong> As we know from the <strong>Einstein
relation</strong>, the long-time limit of the derivative of the 1D MSD,
<span class="math inline">\(\lim_{t\to\infty} \frac{d\langle x^2
\rangle}{dt}\)</span>, is simply equal to <strong><span
class="math inline">\(2D\)</span></strong>.</li>
<li><strong>The Right Side:</strong> This is the result of the
mathematical derivation from the previous slide. It shows that this same
quantity is also equal to twice the total integral of the VACF.</li>
</ul>
<p>By equating these two, we can solve for the diffusion coefficient,
<code>D</code>.</p>
<h3 id="the-velocity-autocorrelation-function-vacf">5. The Velocity
Autocorrelation Function (VACF)</h3>
<p>The board explicitly names the key quantity here:</p>
<p><span class="math display">\[\Phi(\tau) = \langle V_x(0) V_x(\tau)
\rangle\]</span></p>
<p>This is the <strong>â€œVelocity autocorrelation functionâ€</strong>
(abbreviated as VAF on the board), which weâ€™ve denoted as VACF. The
variable has been changed from <code>t</code> to <code>Ï„</code> (tau) to
represent a â€œtime lagâ€ or interval, which is common notation.</p>
<ul>
<li><strong>The Plot:</strong> The graph on the board shows a typical
plot of the VACF, <span class="math inline">\(\Phi(\tau)\)</span>,
versus the time lag <span class="math inline">\(\tau\)</span>.
<ul>
<li>It starts at a maximum positive value at <span
class="math inline">\(\tau=0\)</span> (when the velocity is perfectly
correlated with itself).</li>
<li>It rapidly decays towards zero as the particle undergoes collisions
that randomize its velocity.</li>
</ul></li>
<li><strong>The Integral:</strong> The shaded area under this curve
represents the value of the integral <span
class="math inline">\(\int_0^\infty \Phi(\tau) d\tau\)</span>. The
Green-Kubo formula states that the diffusion coefficient is directly
proportional to this area.</li>
</ul>
<h3 id="the-green-kubo-formulas-for-the-diffusion-coefficient">6. The
Green-Kubo Formulas for the Diffusion Coefficient</h3>
<p>After canceling the factor of 2, the board presents the final,
practical formulas for <code>D</code>.</p>
<ul>
<li><strong>In 1 Dimension:</strong> <span class="math display">\[D =
\int_0^\infty d\tau \langle V_x(0) V_x(\tau) \rangle\]</span></li>
<li><strong>In 3 Dimensions:</strong> This is the more general and
useful formula. <span class="math display">\[D = \frac{1}{3}
\int_0^\infty d\tau \langle \vec{v}(0) \cdot \vec{v}(\tau)
\rangle\]</span> There are two important changes for 3D:
<ol type="1">
<li>We use the full <strong>velocity vectors</strong> and their dot
product, <span class="math inline">\(\vec{v}(0) \cdot
\vec{v}(\tau)\)</span>, to capture motion in all directions.</li>
<li>We divide by <strong>3</strong> to get the average contribution to
diffusion in any one direction (x, y, or z).</li>
</ol></li>
</ul>
<h3 id="practical-calculation-in-a-simulation">7. Practical Calculation
in a Simulation</h3>
<p>The last formula on the board shows how this is implemented in a
computer simulation with a finite number of atoms.</p>
<p><span class="math display">\[D = \frac{1}{3N} \int_0^\infty d\tau
\sum_{i=1}^{N} \langle \vec{v}_i(0) \cdot \vec{v}_i(\tau)
\rangle\]</span></p>
<ul>
<li><strong><span
class="math inline">\(\sum_{i=1}^{N}\)</span></strong>: This
<strong>summation</strong> symbol indicates that you must compute the
VACF for <em>each individual atom</em> (from atom <code>i=1</code> to
atom <code>N</code>).</li>
<li><strong><span class="math inline">\(\frac{1}{N}\)</span></strong>:
You then <strong>average</strong> the results over all <code>N</code>
atoms in your simulation box.</li>
<li><strong><span class="math inline">\(\langle \dots
\rangle\)</span></strong>: The angle brackets here still imply an
additional average over multiple different starting times
(<code>t=0</code>) to get good statistics.</li>
</ul>
<p>This formula is the practical recipe: to get the diffusion
coefficient, you track the velocity of every atom, calculate each oneâ€™s
VACF, average them together, and then integrate the result over
time.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/16/5054C3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/16/5054C3/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-16 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-16T21:00:00+08:00">2025-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-19 19:24:11" itemprop="dateModified" datetime="2025-09-19T19:24:11+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="general-linear-regression-model.">1. General linear regression
model.</h1>
<p><img src="/imgs/5054C3/General_linear_regression_model.png" alt="Diagram of a linear regression model">
## 1.1 general linear regression model - <strong>å†…å®¹</strong>:
<strong>general linear regression model</strong>.</p>
<p>the fundamental equation:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1x_{i1} + \dots +
\beta_px_{ip} + \epsilon_i\]</span></p>
<p>And it correctly identifies the main goal: to <strong>estimate the
parameters</strong> (the coefficients <span
class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span>) from
data so we can make predictions on new data.</p>
<p>æ ¸å¿ƒç›®æ ‡ï¼šé€šè¿‡æ•°æ®<strong>ä¼°è®¡å‚æ•°</strong>ï¼ˆå³ç³»æ•° <span
class="math inline">\(\beta_0, \beta_1, \dots,
\beta_p\)</span>ï¼‰ï¼Œä»è€Œå¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚</p>
<h2
id="how-we-actually-find-the-best-values-for-the-Î²-coefficients-parameter-estimation">1.2
How we actually find the best values for the <span
class="math inline">\(Î²\)</span> coefficients (parameter
estimation)?:</h2>
<ul>
<li><strong>å†…å®¹</strong>: We find the best values for the <span
class="math inline">\(\beta\)</span> coefficients by finding the values
that <strong>minimize the overall error</strong> of the model. The most
common and fundamental method for this is called <strong>Ordinary Least
Squares (OLS)</strong>.</li>
</ul>
<h3
id="the-main-method-ordinary-least-squares-ols-æ™®é€šæœ€å°äºŒä¹˜æ³•-ols">##
The Main Method: Ordinary Least Squares (OLS) æ™®é€šæœ€å°äºŒä¹˜æ³• (OLS)</h3>
<p>The core idea of OLS is to find the line (or hyperplane in multiple
dimensions) that is as close as possible to all the data points
simultaneously. OLS
çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ‰¾åˆ°ä¸€æ¡å°½å¯èƒ½åŒæ—¶æ¥è¿‘æ‰€æœ‰æ•°æ®ç‚¹çš„ç›´çº¿ï¼ˆæˆ–å¤šç»´è¶…å¹³é¢ï¼‰ã€‚</p>
<h4 id="define-the-error-residuals-è¯¯å·®">1. Define the Error (Residuals)
è¯¯å·®</h4>
<p>First, we need to define what â€œerrorâ€ means. For any single data
point, the error is the difference between the actual value (<span
class="math inline">\(y_i\)</span>) and the value predicted by our model
(<span class="math inline">\(\hat{y}_i\)</span>). This difference is
called the <strong>residual</strong>.
é¦–å…ˆï¼Œéœ€è¦å®šä¹‰â€œè¯¯å·®â€çš„å«ä¹‰ã€‚å¯¹äºä»»ä½•å•ä¸ªæ•°æ®ç‚¹ï¼Œè¯¯å·®æ˜¯å®é™…å€¼ (<span
class="math inline">\(y_i\)</span>) ä¸æ¨¡å‹é¢„æµ‹å€¼ (<span
class="math inline">\(\hat{y}_i\)</span>)
ä¹‹é—´çš„å·®å€¼ã€‚è¿™ä¸ªå·®å€¼ç§°ä¸º<strong>æ®‹å·®</strong>ã€‚</p>
<p><strong>Residual</strong> = Actual Value - Predicted Value
<strong>æ®‹å·®</strong> = å®é™…å€¼ - é¢„æµ‹å€¼ <span class="math display">\[e_i
= y_i - \hat{y}_i\]</span></p>
<p>You can visualize residuals as the vertical distance from each data
point to the regression line.
å¯ä»¥å°†æ®‹å·®å¯è§†åŒ–ä¸ºæ¯ä¸ªæ•°æ®ç‚¹åˆ°å›å½’çº¿çš„å‚ç›´è·ç¦»ã€‚</p>
<h4
id="the-cost-function-sum-of-squared-residuals-æˆæœ¬å‡½æ•°æ®‹å·®å¹³æ–¹å’Œ">2.
The Cost Function: Sum of Squared Residuals æˆæœ¬å‡½æ•°ï¼šæ®‹å·®å¹³æ–¹å’Œ</h4>
<p>We want to make all these residuals as small as possible. We canâ€™t
just add them up, because some are positive and some are negative, and
they would cancel each other out.
æ‰€æœ‰æ®‹å·®å°½å¯èƒ½å°ã€‚ä¸èƒ½ç®€å•åœ°å°†å®ƒä»¬ç›¸åŠ ï¼Œå› ä¸ºæœ‰äº›æ˜¯æ­£æ•°ï¼Œæœ‰äº›æ˜¯è´Ÿæ•°ï¼Œå®ƒä»¬ä¼šç›¸äº’æŠµæ¶ˆã€‚</p>
<p>So, we square each residual (which makes them all positive) and then
sum them up. This gives us the <strong>Sum of Squared Residuals
(SSR)</strong>, which is our â€œcost function.â€
å› æ­¤ï¼Œå°†æ¯ä¸ªæ®‹å·®æ±‚å¹³æ–¹ï¼ˆä½¿å®ƒä»¬éƒ½ä¸ºæ­£æ•°ï¼‰ï¼Œç„¶åå°†å®ƒä»¬ç›¸åŠ ã€‚è¿™å°±å¾—åˆ°äº†<strong>æ®‹å·®å¹³æ–¹å’Œ
(SSR)</strong>ï¼Œä¹Ÿå°±æ˜¯â€œæˆæœ¬å‡½æ•°â€ã€‚</p>
<p><span class="math display">\[SSR = \sum_{i=1}^{n} e_i^2 =
\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span></p>
<p>The goal of OLS is simple: <strong>find the values of <span
class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span> that
make this SSR value as small as possible.</strong></p>
<h4
id="solving-for-the-coefficients-the-normal-equation-æ±‚è§£ç³»æ•°æ­£æ€æ–¹ç¨‹">3.
Solving for the Coefficients: The Normal Equation
æ±‚è§£ç³»æ•°ï¼šæ­£æ€æ–¹ç¨‹</h4>
<p>For linear regression, calculus provides a direct, exact solution to
this minimization problem. By taking the derivative of the SSR function
with respect to each <span class="math inline">\(\beta\)</span>
coefficient and setting it to zero, we can solve for the optimal values.
å¯¹äºçº¿æ€§å›å½’ï¼Œå¾®ç§¯åˆ†ä¸ºè¿™ä¸ªæœ€å°åŒ–é—®é¢˜æä¾›äº†ç›´æ¥ã€ç²¾ç¡®çš„è§£ã€‚é€šè¿‡å¯¹ SSR
å‡½æ•°çš„æ¯ä¸ª <span class="math inline">\(\beta\)</span>
ç³»æ•°æ±‚å¯¼å¹¶å°†å…¶è®¾ä¸ºé›¶ï¼Œå°±å¯ä»¥æ±‚è§£å‡ºæœ€ä¼˜å€¼ã€‚</p>
<p>This process results in a formula known as the <strong>Normal
Equation</strong>, which can be expressed cleanly using matrix algebra:
è¿™ä¸ªè¿‡ç¨‹ä¼šå¾—åˆ°ä¸€ä¸ªç§°ä¸º<strong>æ­£æ€æ–¹ç¨‹</strong>çš„å…¬å¼ï¼Œå®ƒå¯ä»¥ç”¨çŸ©é˜µä»£æ•°æ¸…æ™°åœ°è¡¨ç¤ºå‡ºæ¥ï¼š</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the
vector of our estimated coefficients.ä¼°è®¡ç³»æ•°çš„å‘é‡ã€‚</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is a matrix where
each row is an observation and each column is a feature (with an added
column of 1s for the intercept <span
class="math inline">\(\beta_0\)</span>).å…¶ä¸­æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªè§‚æµ‹å€¼ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªç‰¹å¾ï¼ˆæˆªè·
<span class="math inline">\(\beta_0\)</span> å¢åŠ äº†ä¸€åˆ—å…¨ä¸º 1
çš„å€¼ï¼‰ã€‚</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of the
actual response values.å®é™…å“åº”å€¼çš„å‘é‡ã€‚</li>
</ul>
<p>Statistical software and programming libraries (like Scikit-learn in
Python) use this equation (or more computationally stable versions of
it) to find the best coefficients for you instantly.</p>
<h3 id="an-alternative-method-gradient-descent-æ¢¯åº¦ä¸‹é™">## An
Alternative Method: Gradient Descent æ¢¯åº¦ä¸‹é™</h3>
<p>While the Normal Equation gives a direct answer, it can be very slow
if you have a massive number of features (e.g., hundreds of thousands).
An alternative, iterative method used across machine learning is
<strong>Gradient Descent</strong>.</p>
<p><strong>The Intuition:</strong> Imagine the SSR cost function is a
big valley. Your initial (random) <span
class="math inline">\(\beta\)</span> coefficients place you somewhere on
the slope of this valley.</p>
<ol type="1">
<li><strong>Check the slope</strong> (the gradient) at your current
position. <strong>æ£€æŸ¥æ‚¨å½“å‰ä½ç½®çš„æ–œç‡</strong>ï¼ˆæ¢¯åº¦ï¼‰ã€‚</li>
<li><strong>Take a small step</strong> in the steepest <em>downhill</em>
direction. <strong>æœæœ€é™¡çš„<em>ä¸‹å¡</em>æ–¹å‘</strong>è¿ˆå‡ºä¸€å°æ­¥**ã€‚</li>
<li><strong>Repeat.</strong> You keep taking steps downhill until you
reach the bottom of the valley. The bottom of the valley represents the
minimum SSR, and your coordinates at that point are the optimal <span
class="math inline">\(\beta\)</span> coefficients.
<strong>é‡å¤</strong>ã€‚æ‚¨ç»§ç»­å‘ä¸‹èµ°ï¼Œç›´åˆ°åˆ°è¾¾å±±è°·åº•éƒ¨ã€‚è°·åº•ä»£è¡¨æœ€å°SSRï¼Œè¯¥ç‚¹çš„åæ ‡å³ä¸ºæœ€ä¼˜<span
class="math inline">\(\beta\)</span>ç³»æ•°ã€‚</li>
</ol>
<p>The size of each â€œstepâ€ you take is controlled by a parameter called
the <strong>learning rate</strong>. Gradient Descent is the foundational
optimization algorithm for many complex models, including neural
networks.
æ¯æ¬¡â€œæ­¥è¿›â€çš„å¤§å°ç”±ä¸€ä¸ªç§°ä¸º<strong>å­¦ä¹ ç‡</strong>çš„å‚æ•°æ§åˆ¶ã€‚æ¢¯åº¦ä¸‹é™æ˜¯è®¸å¤šå¤æ‚æ¨¡å‹ï¼ˆåŒ…æ‹¬ç¥ç»ç½‘ç»œï¼‰çš„åŸºç¡€ä¼˜åŒ–ç®—æ³•ã€‚</p>
<h3 id="summary-ols-vs.-gradient-descent">## Summary: OLS vs.Â Gradient
Descent</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Ordinary Least Squares (OLS)</th>
<th style="text-align: left;">Gradient Descent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>How it works</strong></td>
<td style="text-align: left;">Direct calculation using the Normal
Equation.</td>
<td style="text-align: left;">Iterative; takes steps to minimize
error.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Pros</strong></td>
<td style="text-align: left;">Provides an exact, optimal solution. No
parameters to tune.</td>
<td style="text-align: left;">More efficient for very large datasets.
Very versatile.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Cons</strong></td>
<td style="text-align: left;">Can be computationally expensive with many
features.</td>
<td style="text-align: left;">Requires choosing a learning rate. May not
find the exact minimum.</td>
</tr>
</tbody>
</table>
<h1 id="simple-linear-regression">2. Simple Linear Regression</h1>
<p><img src="/imgs/5054C3/Simple_Linear_Regression.png" alt="Simple_Linear_Regression"></p>
<h2 id="simple-linear-regression-1">2.1 Simple Linear Regression</h2>
<ul>
<li><strong>å†…å®¹</strong>: <strong>Simple Linear Regression:</strong> a
special case of the general model you showed earlier where you only have
<strong>one</strong> predictor variable (<span
class="math inline">\(p=1\)</span>).</li>
</ul>
<h3 id="the-model-and-the-goal-æ¨¡å‹å’Œç›®æ ‡">## The Model and the Goal
æ¨¡å‹å’Œç›®æ ‡</h3>
<p>Sets up the simplified equation for a line: <span
class="math display">\[y_i = \beta_0 + \beta_1x_i + \epsilon_i\]</span>
* <span class="math inline">\(y_i\)</span> is the outcome you want to
predict.è¦é¢„æµ‹çš„ç»“æœã€‚ * <span class="math inline">\(x_i\)</span> is
your single input feature or covariate.å•ä¸ªè¾“å…¥ç‰¹å¾æˆ–åå˜é‡ã€‚ * <span
class="math inline">\(\beta_1\)</span> is the <strong>slope</strong> of
the line. It tells you how much <span class="math inline">\(y\)</span>
is expected to increase for a one-unit increase in <span
class="math inline">\(x\)</span>.è¡¨ç¤º <span
class="math inline">\(x\)</span> æ¯å¢åŠ ä¸€ä¸ªå•ä½ï¼Œ<span
class="math inline">\(y\)</span> é¢„è®¡ä¼šå¢åŠ å¤šå°‘ã€‚ * <span
class="math inline">\(\beta_0\)</span> is the
<strong>intercept</strong>. Itâ€™s the predicted value of <span
class="math inline">\(y\)</span> when <span
class="math inline">\(x\)</span> is zero.å½“ <span
class="math inline">\(x\)</span> ä¸ºé›¶æ—¶ <span
class="math inline">\(y\)</span> çš„é¢„æµ‹å€¼ã€‚ * <span
class="math inline">\(\epsilon_i\)</span> is the random error
term.æ˜¯éšæœºè¯¯å·®é¡¹ã€‚</p>
<p>The goal, stated as â€œMinimize the sum of squares of err,â€ is exactly
the <strong>Ordinary Least Squares (OLS)</strong> method we just
discussed. Itâ€™s written here as: <span class="math display">\[\min_{a,b}
\sum_{i=1}^{n} (y_i - a - bx_i)^2\]</span> This is just a different way
of writing the same thing, where they use <code>a</code> for the
intercept (<span class="math inline">\(\beta_0\)</span>) and
<code>b</code> for the slope (<span
class="math inline">\(\beta_1\)</span>). Youâ€™re trying to find the
specific values of the slope and intercept that make the sum of all the
squared errors as small as possible.
ç›®æ ‡ï¼Œå³â€œæœ€å°åŒ–è¯¯å·®å¹³æ–¹å’Œâ€ï¼Œæ­£æ˜¯<strong>æ™®é€šæœ€å°äºŒä¹˜æ³•
(OLS)</strong>ã€‚ï¼š <span class="math display">\[\min_{a,b}
\sum_{i=1}^{n} (y_i - a - bx_i)^2\]</span> è¿™æ˜¯å¦ä¸€ç§å†™æ³•ï¼Œå…¶ä¸­ç”¨
<code>a</code> è¡¨ç¤ºæˆªè· (<span
class="math inline">\(\beta_0\)</span>)ï¼Œ<code>b</code> è¡¨ç¤ºæ–œç‡ (<span
class="math inline">\(\beta_1\)</span>)ã€‚å°è¯•æ‰¾åˆ°æ–œç‡å’Œæˆªè·çš„å…·ä½“å€¼ï¼Œä½¿å¾—æ‰€æœ‰å¹³æ–¹è¯¯å·®ä¹‹å’Œå°½å¯èƒ½å°ã€‚</p>
<h3 id="the-solution-the-estimator-formulas-è§£å†³æ–¹æ¡ˆä¼°è®¡å…¬å¼">## The
Solution: The Estimator Formulas è§£å†³æ–¹æ¡ˆï¼šä¼°è®¡å…¬å¼</h3>
<p>The most important part of this slide is the
<strong>solution</strong>. For the simple case with only one variable,
you donâ€™t need complex matrix algebra (the Normal Equation). Instead,
the minimization problem can be solved with these two straightforward
formulas:
å¯¹äºåªæœ‰ä¸€ä¸ªå˜é‡çš„ç®€å•æƒ…å†µï¼Œä¸éœ€è¦å¤æ‚çš„çŸ©é˜µä»£æ•°ï¼ˆæ­£æ€æ–¹ç¨‹ï¼‰ã€‚ç›¸åï¼Œæœ€å°åŒ–é—®é¢˜å¯ä»¥ç”¨ä»¥ä¸‹ä¸¤ä¸ªç®€å•çš„å…¬å¼æ¥è§£å†³ï¼š</p>
<h4 id="the-slope-hatbeta_1">1. The Slope: <span
class="math inline">\(\hat{\beta}_1\)</span></h4>
<p><span class="math display">\[\hat{\beta}_1 = \frac{\sum_{i=1}^{n}
(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i -
\bar{x})^2}\]</span> * <strong>Intuition:</strong> This formula might
look complex, but itâ€™s actually very intuitive. * The numerator, <span
class="math inline">\(\sum(x_i - \bar{x})(y_i - \bar{y})\)</span>, is
closely related to the <strong>covariance</strong> between X and Y. It
measures whether X and Y tend to move in the same direction (positive
slope) or in opposite directions (negative slope). ä¸ X å’Œ Y
ä¹‹é—´çš„<strong>åæ–¹å·®</strong>å¯†åˆ‡ç›¸å…³ã€‚å®ƒè¡¡é‡ X å’Œ Y
æ˜¯å€¾å‘äºæœç›¸åŒæ–¹å‘ï¼ˆæ­£æ–œç‡ï¼‰è¿˜æ˜¯æœç›¸åæ–¹å‘ï¼ˆè´Ÿæ–œç‡ï¼‰ç§»åŠ¨ã€‚ * The
denominator, <span class="math inline">\(\sum(x_i - \bar{x})^2\)</span>,
is related to the <strong>variance</strong> of X. It measures how much X
varies on its own. å®ƒè¡¡é‡ X è‡ªèº«çš„å˜åŒ–é‡ã€‚ * <strong>In short, the slope
is a measure of how X and Y vary together, scaled by how much X varies
by itself.</strong> æ–œç‡è¡¡é‡çš„æ˜¯ X å’Œ Y å…±åŒå˜åŒ–çš„ç¨‹åº¦ï¼Œå¹¶ä»¥ X
è‡ªèº«çš„å˜åŒ–é‡ä¸ºæ ‡åº¦ã€‚</p>
<h4 id="the-intercept-hatbeta_0-æˆªè·">2. The Intercept: <span
class="math inline">\(\hat{\beta}_0\)</span> æˆªè·</h4>
<p><span class="math display">\[\hat{\beta}_0 = \bar{y} -
\hat{\beta}_1\bar{x}\]</span> * <strong>Intuition:</strong> This formula
is even simpler and has a wonderful geometric meaning. It ensures that
the <strong>line of best fit always passes through the â€œcenter of massâ€
of the data</strong>, which is the point of averages <span
class="math inline">\((\bar{x}, \bar{y})\)</span>.
å®ƒç¡®ä¿<strong>æœ€ä½³æ‹Ÿåˆçº¿å§‹ç»ˆç©¿è¿‡æ•°æ®çš„â€œè´¨å¿ƒâ€</strong>ï¼Œå³å¹³å‡å€¼ <span
class="math inline">\((\bar{x}, \bar{y})\)</span> çš„ç‚¹ã€‚è®¡ç®—å‡ºæœ€ä½³æ–œç‡
(<span class="math inline">\(\hat{\beta}_1\)</span>)
åï¼Œå°±å¯ä»¥å°†å…¶ä»£å…¥æ­¤å…¬å¼ã€‚ç„¶åï¼Œå¯ä»¥è°ƒæ•´æˆªè· (<span
class="math inline">\(\hat{\beta}_0\)</span>)ï¼Œä½¿ç›´çº¿å®Œç¾åœ°å›´ç»•æ•°æ®äº‘çš„ä¸­å¿ƒç‚¹æ—‹è½¬ã€‚
* Once youâ€™ve calculated the best slope (<span
class="math inline">\(\hat{\beta}_1\)</span>), you can plug it into this
formula. You then adjust the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) so that the line pivots
perfectly around the central point of your data cloud.</p>
<p>In summary, this slide provides the precise, closed-form formulas to
calculate the slope and intercept for the line of best fit in a simple
linear regression model.</p>
<h1 id="statistical-inference">3. Statistical Inference</h1>
<p><img src="/imgs/5054C3/Statistical_Inference1.png" alt="Statistical_Inference1">
<img src="/imgs/5054C3/Statistical_Inference2.png" alt="Statistical_Inference2">
## 3.1 Statistical Inference - <strong>å†…å®¹</strong>:
<strong>Statistical Inference:</strong> These two slides are deeply
connected and explain how we go from just <em>calculating</em> the
coefficients to understanding how <em>accurate</em> and
<em>reliable</em> they are.
è§£é‡Šäº†æˆ‘ä»¬å¦‚ä½•ä»ä»…ä»…<em>è®¡ç®—</em>ç³»æ•°åˆ°ç†è§£å®ƒä»¬çš„<em>å‡†ç¡®æ€§</em>å’Œ<em>å¯é æ€§</em>ã€‚</p>
<h3 id="the-core-problem-quantifying-uncertainty-é‡åŒ–ä¸ç¡®å®šæ€§">## The
Core Problem: Quantifying Uncertainty é‡åŒ–ä¸ç¡®å®šæ€§</h3>
<p>The second slide poses the fundamental questions: * â€œHow accurate are
<span class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span>?â€å‡†ç¡®æ€§å¦‚ä½•ï¼Ÿ * â€œWhat are
the distributions of <span class="math inline">\(\hat{\beta}_0\)</span>
and <span class="math inline">\(\hat{\beta}_1\)</span>?â€åˆ†å¸ƒæ˜¯ä»€ä¹ˆï¼Ÿ</p>
<p>The reason we ask this is that our estimated coefficients (<span
class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>) were
calculated from a <strong>specific sample of data</strong>. If we
collected a different random sample from the same population, we would
get slightly different estimates.ä¼°è®¡çš„ç³»æ•° (<span
class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>)
æ˜¯æ ¹æ®<strong>ç‰¹å®šçš„æ•°æ®æ ·æœ¬</strong>è®¡ç®—å‡ºæ¥çš„ã€‚å¦‚æœæˆ‘ä»¬ä»åŒä¸€æ€»ä½“ä¸­éšæœºæŠ½å–ä¸åŒçš„æ ·æœ¬ï¼Œæˆ‘ä»¬å¾—åˆ°çš„ä¼°è®¡å€¼ä¼šç•¥æœ‰ä¸åŒã€‚</p>
<p>The goal of statistical inference is to use the estimates from our
single sample to make conclusions about the <strong>true, unknown
population parameters</strong> (<span class="math inline">\(\beta_0,
\beta_1\)</span>) and to quantify our uncertainty about
them.ç»Ÿè®¡æ¨æ–­çš„ç›®æ ‡æ˜¯åˆ©ç”¨å•ä¸ªæ ·æœ¬çš„ä¼°è®¡å€¼å¾—å‡ºå…³äº<strong>çœŸå®ã€æœªçŸ¥çš„æ€»ä½“å‚æ•°</strong>ï¼ˆ<span
class="math inline">\(\beta_0,
\beta_1\)</span>ï¼‰çš„ç»“è®ºï¼Œå¹¶é‡åŒ–å¯¹è¿™äº›å‚æ•°çš„ä¸ç¡®å®šæ€§ã€‚</p>
<h3
id="the-key-assumption-that-makes-it-possible-å®ç°è¿™ä¸€ç›®æ ‡çš„å…³é”®å‡è®¾">##
The Key Assumption That Makes It Possible å®ç°è¿™ä¸€ç›®æ ‡çš„å…³é”®å‡è®¾</h3>
<p>To figure out the distribution of our estimates, we must make an
assumption about the distribution of the errors. This is the most
important assumption in linear regression for inference:
ä¸ºäº†ç¡®å®šä¼°è®¡å€¼çš„åˆ†å¸ƒï¼Œå¿…é¡»å¯¹è¯¯å·®çš„åˆ†å¸ƒåšå‡ºå‡è®¾ã€‚è¿™æ˜¯çº¿æ€§å›å½’æ¨æ–­ä¸­æœ€é‡è¦çš„å‡è®¾ï¼š
<strong>Assumption:</strong> <span class="math inline">\(\epsilon_i
\stackrel{\text{i.i.d.}}{\sim} N(0, \sigma^2)\)</span></p>
<p>This means we assume the random error terms are: * <strong>Normally
distributed</strong> (<span class="math inline">\(N\)</span>).*
<strong>æ­£æ€åˆ†å¸ƒ</strong>ï¼ˆ<span class="math inline">\(N\)</span>ï¼‰ã€‚ *
Have a mean of <strong>zero</strong> (our model is correct on average).*
å‡å€¼ä¸º<strong>é›¶</strong>ï¼ˆæ¨¡å‹å¹³å‡è€Œè¨€æ˜¯æ­£ç¡®çš„ï¼‰ã€‚ * Have a constant
variance <strong><span class="math inline">\(\sigma^2\)</span></strong>
(homoscedasticity).* æ–¹å·®ä¸ºå¸¸æ•°<strong><span
class="math inline">\(\sigma^2\)</span></strong>ï¼ˆæ–¹å·®é½æ€§ï¼‰ã€‚ * Are
<strong>independent and identically distributed</strong> (i.i.d.),
meaning each error is independent of the others.*
æ˜¯<strong>ç‹¬ç«‹åŒåˆ†å¸ƒ</strong>ï¼ˆi.i.d.ï¼‰çš„ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªè¯¯å·®éƒ½ç‹¬ç«‹äºå…¶ä»–è¯¯å·®ã€‚</p>
<p><strong>Why is this important?</strong> Because our coefficients
<span class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span> are calculated as weighted
sums of the <span class="math inline">\(y_i\)</span> values, and the
<span class="math inline">\(y_i\)</span> values depend on the errors
<span class="math inline">\(\epsilon_i\)</span>. This assumption about
the errors allows us to prove that our estimated coefficients themselves
are also normally distributed. ç³»æ•° <span
class="math inline">\(\hat{\beta}_0\)</span> å’Œ <span
class="math inline">\(\hat{\beta}_1\)</span> æ˜¯é€šè¿‡ <span
class="math inline">\(y_i\)</span> å€¼çš„åŠ æƒå’Œè®¡ç®—çš„ï¼Œè€Œ <span
class="math inline">\(y_i\)</span> å€¼å–å†³äºè¯¯å·® <span
class="math inline">\(\epsilon_i\)</span>ã€‚è¿™ä¸ªå…³äºè¯¯å·®çš„å‡è®¾ä½¿èƒ½å¤Ÿè¯æ˜ä¼°è®¡çš„ç³»æ•°æœ¬èº«ä¹Ÿæœä»æ­£æ€åˆ†å¸ƒã€‚</p>
<h3
id="the-solution-the-theorem-and-the-t-distribution-å®šç†å’Œ-t-åˆ†å¸ƒ">##
The Solution: The Theorem and the t-distribution å®šç†å’Œ t åˆ†å¸ƒ</h3>
<p>The first slide provides the central theorem that allows us to
perform inference. It tells us exactly how to standardize our estimated
coefficients so they follow a known distribution.
ç¬¬ä¸€å¼ å¹»ç¯ç‰‡æä¾›äº†è¿›è¡Œæ¨æ–­çš„æ ¸å¿ƒå®šç†ã€‚å®ƒå‡†ç¡®åœ°å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•å¯¹ä¼°è®¡çš„ç³»æ•°è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä½¿å…¶æœä»å·²çŸ¥çš„åˆ†å¸ƒã€‚</p>
<h4 id="the-standard-error-s.e.-æ ‡å‡†è¯¯å·®-s.e.">1. The Standard Error
(s.e.) æ ‡å‡†è¯¯å·® (s.e.)</h4>
<p>First, look at the denominators in the red dotted boxes. These are
the <strong>standard errors</strong> of the coefficients,
<code>s.e.($\hat&#123;\beta&#125;_1$)</code> and
<code>s.e.($\hat&#123;\beta&#125;_0$)</code>.
ç¬¬ä¸€å¼ å¹»ç¯ç‰‡æä¾›äº†è¿›è¡Œæ¨æ–­çš„æ ¸å¿ƒå®šç†ã€‚å®ƒå‡†ç¡®åœ°å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•å¯¹ä¼°è®¡çš„ç³»æ•°è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä½¿å…¶æœä»å·²çŸ¥çš„åˆ†å¸ƒã€‚</p>
<ul>
<li><strong>What it is:</strong> The standard error is the estimated
<strong>standard deviation of the coefficientâ€™s sampling
distribution</strong>. In simpler terms, itâ€™s a measure of the average
amount by which our estimate <span
class="math inline">\(\hat{\beta}_1\)</span> would differ from the true
<span class="math inline">\(\beta_1\)</span> if we were to repeat the
experiment many times.
æ ‡å‡†è¯¯å·®æ˜¯ç³»æ•°æŠ½æ ·åˆ†å¸ƒçš„<strong>æ ‡å‡†å·®</strong>ä¼°è®¡å€¼ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒè¡¡é‡çš„æ˜¯å¦‚æœæˆ‘ä»¬é‡å¤å®éªŒå¤šæ¬¡ï¼Œæˆ‘ä»¬ä¼°è®¡çš„
<span class="math inline">\(\hat{\beta}_1\)</span> ä¸çœŸå®çš„ <span
class="math inline">\(\beta_1\)</span> ä¹‹é—´çš„å¹³å‡å·®å¼‚ã€‚</li>
<li><strong>A smaller standard error means a more precise and reliable
estimate.</strong>
<strong>æ ‡å‡†è¯¯å·®è¶Šå°ï¼Œä¼°è®¡å€¼è¶Šç²¾ç¡®å¯é ã€‚</strong></li>
</ul>
<h4 id="the-t-statistic-t-ç»Ÿè®¡é‡">2. The t-statistic t ç»Ÿè®¡é‡</h4>
<p>The theorem shows two fractions that form a
<strong>t-statistic</strong>. The general structure for this is:
è¯¥å®šç†å±•ç¤ºäº†ä¸¤ä¸ªæ„æˆ<strong>t ç»Ÿè®¡é‡</strong>çš„åˆ†æ•°ã€‚å…¶ä¸€èˆ¬ç»“æ„å¦‚ä¸‹ï¼š
<span class="math display">\[t = \frac{\text{ (Sample Estimate - True
Value) }}{\text{ Standard Error of the Estimate }}\]</span></p>
<p>For <span class="math inline">\(\beta_1\)</span>, this is: <span
class="math inline">\(\frac{\hat{\beta}_1 -
\beta_1}{\text{s.e.}(\hat{\beta}_1)}\)</span>.</p>
<p>The key insight is that this specific quantity follows a
<strong>Studentâ€™s t-distribution</strong> with <strong><span
class="math inline">\(n-2\)</span> degrees of freedom</strong>.
å…³é”®åœ¨äºï¼Œè¿™ä¸ªç‰¹å®šé‡æœä»<strong>å­¦ç”Ÿ t
åˆ†å¸ƒ</strong>ï¼Œå…¶è‡ªç”±åº¦ä¸º<strong><span
class="math inline">\(n-2\)</span>ã€‚ * </strong>Studentâ€™s
t-distribution:** This is a probability distribution that looks very
similar to the normal distribution but has slightly â€œheavierâ€ tails. We
use it instead of the normal distribution because we had to
<em>estimate</em> the standard deviation of the errors (<code>s</code>
in the formula), which adds extra uncertainty.
è¿™æ˜¯ä¸€ç§æ¦‚ç‡åˆ†å¸ƒï¼Œä¸æ­£æ€åˆ†å¸ƒéå¸¸ç›¸ä¼¼ï¼Œä½†å°¾éƒ¨ç•¥é‡ã€‚ä½¿ç”¨å®ƒæ¥ä»£æ›¿æ­£æ€åˆ†å¸ƒï¼Œæ˜¯å› ä¸ºå¿…é¡»<em>ä¼°è®¡</em>è¯¯å·®çš„æ ‡å‡†å·®ï¼ˆå…¬å¼ä¸­çš„
<code>s</code>ï¼‰ï¼Œè¿™ä¼šå¢åŠ é¢å¤–çš„ä¸ç¡®å®šæ€§ã€‚ * <strong>Degrees of Freedom
(n-2):</strong> We start with <code>n</code> data points, but we lose
two degrees of freedom because we used the data to estimate two
parameters: <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>. ä» <code>n</code>
ä¸ªæ•°æ®ç‚¹å¼€å§‹ï¼Œä½†ç”±äºç”¨è¿™äº›æ•°æ®ä¼°è®¡äº†ä¸¤ä¸ªå‚æ•°ï¼š<span
class="math inline">\(\beta_0\)</span> å’Œ <span
class="math inline">\(\beta_1\)</span>ï¼Œå› æ­¤æŸå¤±äº†ä¸¤ä¸ªè‡ªç”±åº¦ã€‚ #### 3.
Estimating the Error Variance (<span
class="math inline">\(s^2\)</span>)ä¼°è®¡è¯¯å·®æ–¹å·® (<span
class="math inline">\(s^2\)</span>) To calculate the standard errors, we
need a value for <code>s</code>, which is our estimate of the true error
standard deviation <span class="math inline">\(\sigma\)</span>. This is
calculated from the <strong>Residual Sum of Squares (RSS)</strong>.
ä¸ºäº†è®¡ç®—æ ‡å‡†è¯¯å·®ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª <code>s</code> çš„å€¼ï¼Œå®ƒæ˜¯å¯¹çœŸå®è¯¯å·®æ ‡å‡†å·®
<span class="math inline">\(\sigma\)</span>
çš„ä¼°è®¡å€¼ã€‚è¯¥å€¼ç”±<strong>æ®‹å·®å¹³æ–¹å’Œ (RSS)</strong> è®¡ç®—å¾—å‡ºã€‚ *
<strong>RSS:</strong> First, we calculate the RSS = <span
class="math inline">\(\sum(y_i - \hat{y}_i)^2\)</span>, which is the sum
of all the squared errors.* <strong>RSS</strong>ï¼šé¦–å…ˆï¼Œè®¡ç®— RSS = <span
class="math inline">\(\sum(y_i -
\hat{y}_i)^2\)</span>ï¼Œå³æ‰€æœ‰å¹³æ–¹è¯¯å·®ä¹‹å’Œã€‚ * <strong><span
class="math inline">\(s^2\)</span>:</strong> Then, we find the estimate
of the error variance: <span class="math inline">\(s^2 = \text{RSS} /
(n-2)\)</span>. We divide by <span class="math inline">\(n-2\)</span> to
get an unbiased estimate. * <strong><span
class="math inline">\(s^2\)</span></strong>ï¼šç„¶åï¼Œè®¡ç®—è¯¯å·®æ–¹å·®çš„ä¼°è®¡å€¼ï¼š<span
class="math inline">\(s^2 = \text{RSS} / (n-2)\)</span>ã€‚æˆ‘ä»¬å°†å…¶é™¤ä»¥
<span class="math inline">\(n-2\)</span> å³å¯å¾—åˆ°æ— åä¼°è®¡å€¼ã€‚ *
<code>s</code> is simply the square root of <span
class="math inline">\(s^2\)</span>. This <code>s</code> is the value
used in the standard error formulas.* <code>s</code> å°±æ˜¯ <span
class="math inline">\(s^2\)</span> çš„å¹³æ–¹æ ¹ã€‚è¿™ä¸ª <code>s</code>
æ˜¯æ ‡å‡†è¯¯å·®å…¬å¼ä¸­ä½¿ç”¨çš„å€¼ã€‚</p>
<h3 id="what-this-allows-us-to-do-the-practical-use">## What This Allows
Us To Do (The Practical Use)</h3>
<p>Because we know the exact distribution of our t-statistic, we can now
achieve our goal of quantifying uncertainty: å› ä¸ºçŸ¥é“ t
ç»Ÿè®¡é‡çš„ç²¾ç¡®åˆ†å¸ƒï¼Œæ‰€ä»¥ç°åœ¨å¯ä»¥å®ç°é‡åŒ–ä¸ç¡®å®šæ€§çš„ç›®æ ‡ï¼š</p>
<ol type="1">
<li><strong>Hypothesis Testing:</strong> We can test if a predictor is
actually useful. The most common test is for the null hypothesis <span
class="math inline">\(H_0: \beta_1 = 0\)</span>. If we can prove the
observed <span class="math inline">\(\hat{\beta}_1\)</span> is very
unlikely to occur if the true <span
class="math inline">\(\beta_1\)</span> were zero, we can conclude there
is a statistically significant relationship between <span
class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span>.
å¯ä»¥æ£€éªŒä¸€ä¸ªé¢„æµ‹å˜é‡æ˜¯å¦çœŸçš„æœ‰ç”¨ã€‚æœ€å¸¸è§çš„æ£€éªŒæ˜¯é›¶å‡è®¾ <span
class="math inline">\(H_0: \beta_1 = 0\)</span>ã€‚å¦‚æœèƒ½è¯æ˜ï¼Œå½“çœŸå®çš„
<span class="math inline">\(\beta_1\)</span> ä¸ºé›¶æ—¶ï¼Œè§‚æµ‹åˆ°çš„ <span
class="math inline">\(\hat{\beta}_1\)</span>
ä¸å¤ªå¯èƒ½å‘ç”Ÿï¼Œé‚£ä¹ˆå°±å¯ä»¥å¾—å‡ºç»“è®ºï¼Œ<span class="math inline">\(x\)</span>
å’Œ <span class="math inline">\(y\)</span>
ä¹‹é—´å­˜åœ¨ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—å…³ç³»ã€‚</li>
<li><strong>Confidence Intervals:</strong> We can construct a range of
plausible values for the true coefficient. For example, we can calculate
a 95% confidence interval for <span
class="math inline">\(\beta_1\)</span>. This gives us a range where we
are 95% confident the true value of <span
class="math inline">\(\beta_1\)</span> lies.
å¯ä»¥ä¸ºçœŸå®ç³»æ•°æ„å»ºä¸€ç³»åˆ—åˆç†çš„å€¼ã€‚</li>
</ol>
<h1 id="multiple-linear-regression">4. Multiple Linear Regression</h1>
<p><img src="/imgs/5054C3/Multiple_Linear Regression1.png" alt="Multiple_Linear Regression1">
<img src="/imgs/5054C3/Multiple_Linear Regression2.png" alt="Multiple_Linear Regression2">
## 4.1 Multiple Linear Regression - <strong>å†…å®¹</strong>:
<strong>Multiple Linear Regression:</strong></p>
<p>Hereâ€™s a detailed breakdown that connects both slides.</p>
<h3
id="the-model-from-one-to-many-predictors-ä»å•é¢„æµ‹å˜é‡åˆ°å¤šé¢„æµ‹å˜é‡">##
The Model: From One to Many Predictors ä»å•é¢„æµ‹å˜é‡åˆ°å¤šé¢„æµ‹å˜é‡</h3>
<p>The first slide introduces the <strong>Multiple Linear Regression
model</strong>. This is a direct extension of the simple model, but
instead of using just one predictor variable, we use multiple (<span
class="math inline">\(p\)</span>) predictors to explain our response
variable.
å¤šå…ƒçº¿æ€§å›å½’æ¨¡å‹æ˜¯ç®€å•æ¨¡å‹çš„ç›´æ¥æ‰©å±•ï¼Œä½†ä¸æ˜¯åªä½¿ç”¨ä¸€ä¸ªé¢„æµ‹å˜é‡ï¼Œè€Œæ˜¯ä½¿ç”¨å¤šä¸ªï¼ˆ<span
class="math inline">\(p\)</span>ï¼‰é¢„æµ‹å˜é‡æ¥è§£é‡Šå“åº”å˜é‡ã€‚</p>
<p>The general formula is: <span class="math display">\[y_i = \beta_0 +
\beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip} +
\epsilon_i\]</span></p>
<h4 id="key-change-in-interpretation">Key Change in Interpretation</h4>
<p>This is the most important new concept. In simple regression, <span
class="math inline">\(\beta_1\)</span> was just the slope. In multiple
regression, each coefficient has a more nuanced meaning:
åœ¨ç®€å•å›å½’ä¸­ï¼Œ<span class="math inline">\(\beta_1\)</span>
åªæ˜¯æ–œç‡ã€‚åœ¨å¤šå…ƒå›å½’ä¸­ï¼Œæ¯ä¸ªç³»æ•°éƒ½æœ‰æ›´å¾®å¦™çš„å«ä¹‰ï¼š</p>
<p><strong><span class="math inline">\(\beta_j\)</span> is the average
change in <span class="math inline">\(y\)</span> for a one-unit increase
in <span class="math inline">\(x_j\)</span>, while holding all other
predictors constant.</strong></p>
<p>This is incredibly powerful. Using the advertising example from your
slide: * <span class="math inline">\(y_i = \beta_0 +
\beta_1(\text{TV}_i) + \beta_2(\text{Radio}_i) +
\beta_3(\text{Newspaper}_i) + \epsilon_i\)</span> * <span
class="math inline">\(\beta_1\)</span> represents the effect of TV
advertising on sales, <strong>after controlling for</strong> the amount
spent on Radio and Newspaper ads. This allows you to isolate the unique
contribution of each advertising
channel.è¡¨ç¤ºåœ¨<strong>æ§åˆ¶</strong>å¹¿æ’­å’ŒæŠ¥çº¸å¹¿å‘Šæ”¯å‡ºåï¼Œç”µè§†å¹¿å‘Šå¯¹é”€å”®é¢çš„å½±å“ã€‚è¿™å¯ä»¥è®©æ‚¨åŒºåˆ†æ¯ä¸ªå¹¿å‘Šæ¸ é“çš„ç‹¬ç‰¹è´¡çŒ®ã€‚</p>
<h3 id="the-solution-deriving-the-normal-equation-æ¨å¯¼æ­£æ€æ–¹ç¨‹">## The
Solution: Deriving the Normal Equation æ¨å¯¼æ­£æ€æ–¹ç¨‹</h3>
<p>The second slide shows the mathematical process for finding the best
coefficients (<span class="math inline">\(\beta_0, \beta_1, \dots,
\beta_p\)</span>) using the <strong>Ordinary Least Squares
(OLS)</strong> method. Itâ€™s essentially a condensed derivation of the
<strong>Normal Equation</strong>. ä½¿ç”¨<strong>æ™®é€šæœ€å°äºŒä¹˜æ³•
(OLS)</strong> å¯»æ‰¾æœ€ä½³ç³»æ•° (<span class="math inline">\(\beta_0,
\beta_1, \dots, \beta_p\)</span>)
çš„æ•°å­¦è¿‡ç¨‹ã€‚å®ƒæœ¬è´¨ä¸Šæ˜¯<strong>æ­£æ€æ–¹ç¨‹</strong>çš„ç®€åŒ–æ¨å¯¼ã€‚</p>
<h4 id="the-goal-minimizing-the-sum-of-squares-æœ€å°åŒ–å¹³æ–¹å’Œ">1. The
Goal: Minimizing the Sum of Squares æœ€å°åŒ–å¹³æ–¹å’Œ</h4>
<p>Just like before, our goal is to minimize the sum of the squared
errors (or residuals): ç›®æ ‡æ˜¯æœ€å°åŒ–å¹³æ–¹è¯¯å·®ï¼ˆæˆ–æ®‹å·®ï¼‰ä¹‹å’Œã€‚</p>
<ul>
<li><strong>Scalar Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_{i1} -
\beta_2x_{i2} - \beta_3x_{i3})^2\)</span>
<ul>
<li>This is easy to read but gets very long with more variables.
ä»£ç æ˜“äºé˜…è¯»ï¼Œä½†å˜é‡è¶Šå¤šï¼Œä»£ç è¶Šé•¿ã€‚</li>
</ul></li>
<li><strong>Vector Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \boldsymbol{\beta}^T
\mathbf{x}_i)^2\)</span>
<ul>
<li>This is a more compact and powerful way to write the same thing
using linear algebra, where <span
class="math inline">\(\boldsymbol{\beta}^T \mathbf{x}_i\)</span> is the
dot product that calculates the entire predicted value <span
class="math inline">\(\hat{y}_i\)</span>.
è¿™æ˜¯ä¸€ç§æ›´ç®€æ´ã€æ›´å¼ºå¤§çš„çº¿æ€§ä»£æ•°è¡¨ç¤ºæ–¹æ³•ï¼Œå…¶ä¸­ <span
class="math inline">\(\boldsymbol{\beta}^T \mathbf{x}_i\)</span>
æ˜¯è®¡ç®—æ•´ä¸ªé¢„æµ‹å€¼ <span class="math inline">\(\hat{y}_i\)</span>
çš„ç‚¹ç§¯ã€‚</li>
</ul></li>
</ul>
<h4
id="the-method-using-calculus-to-find-the-minimum-ä½¿ç”¨å¾®ç§¯åˆ†æ±‚æœ€å°å€¼">2.
The Method: Using Calculus to Find the Minimum ä½¿ç”¨å¾®ç§¯åˆ†æ±‚æœ€å°å€¼</h4>
<p>To find the set of <span class="math inline">\(\beta\)</span> values
that results in the lowest possible error, we use calculus.</p>
<ul>
<li><p><strong>The Derivative (Gradient):</strong> Since our error
function depends on multiple <span class="math inline">\(\beta\)</span>
coefficients, we canâ€™t take a simple derivative. Instead, we take the
<strong>gradient</strong>, which is a vector of partial derivatives (one
for each coefficient). This tells us the â€œslopeâ€ of the error function
in every direction. å¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰ è¯¯å·®å‡½æ•°ä¾èµ–äºå¤šä¸ª <span
class="math inline">\(\beta\)</span>
ç³»æ•°ï¼Œå› æ­¤æˆ‘ä»¬ä¸èƒ½ç®€å•åœ°æ±‚å¯¼æ•°ã€‚ç›¸åï¼Œé‡‡ç”¨<strong>æ¢¯åº¦</strong>ï¼Œå®ƒæ˜¯ä¸€ä¸ªç”±åå¯¼æ•°ç»„æˆçš„å‘é‡ï¼ˆæ¯ä¸ªç³»æ•°å¯¹åº”ä¸€ä¸ªåå¯¼æ•°ï¼‰ã€‚è¿™å‘Šè¯‰è¯¯å·®å‡½æ•°åœ¨å„ä¸ªæ–¹å‘ä¸Šçš„â€œæ–œç‡â€ã€‚</p></li>
<li><p><strong>Setting the Gradient to Zero:</strong> The minimum of a
function occurs where its slope is zero (the very bottom of the error
â€œvalleyâ€). The slide shows the result of taking this gradient and
setting it to
zero.å‡½æ•°çš„æœ€å°å€¼å‡ºç°åœ¨å…¶æ–œç‡ä¸ºé›¶çš„åœ°æ–¹ï¼ˆå³è¯¯å·®â€œè°·åº•â€çš„æœ€ä½ç‚¹ï¼‰ã€‚å¹»ç¯ç‰‡å±•ç¤ºäº†å–æ­¤æ¢¯åº¦å¹¶å°†å…¶è®¾ä¸ºé›¶çš„ç»“æœã€‚</p></li>
</ul>
<p>The equation shown on the slide: <span class="math display">\[2
\sum_{i=1}^{n} (\boldsymbol{\beta}^T \mathbf{x}_i - y_i)\mathbf{x}_i^T =
0\]</span> â€¦is the result of this calculus step. The goal is now to
algebraically rearrange this equation to solve for <span
class="math inline">\(\boldsymbol{\beta}\)</span>.
æ˜¯è¿™ä¸€å¾®ç§¯åˆ†æ­¥éª¤çš„ç»“æœã€‚ç°åœ¨çš„ç›®æ ‡æ˜¯ç”¨ä»£æ•°æ–¹æ³•é‡æ–°æ’åˆ—è¿™ä¸ªæ–¹ç¨‹ï¼Œä»¥æ±‚è§£
<span class="math inline">\(\boldsymbol{\beta}\)</span>ã€‚</p>
<h4 id="the-result-the-normal-equation-æ­£åˆ™æ–¹ç¨‹">3. The Result: The
Normal Equation æ­£åˆ™æ–¹ç¨‹</h4>
<p>After rearranging the equation from the previous step and expressing
the sums in their full matrix form, we arrive at a clean and beautiful
solution. While the slide doesnâ€™t show the final step, the result of
â€œSetting the gradient zero and solve <span
class="math inline">\(\beta\)</span>â€ is the <strong>Normal
Equation</strong>:
é‡æ–°æ’åˆ—ä¸Šä¸€æ­¥ä¸­çš„æ–¹ç¨‹ï¼Œå¹¶å°†å’Œè¡¨ç¤ºä¸ºå®Œæ•´çš„çŸ©é˜µå½¢å¼åï¼Œå¾—åˆ°äº†ä¸€ä¸ªç®€æ´ç¾è§‚çš„è§£ã€‚è™½ç„¶å¹»ç¯ç‰‡æ²¡æœ‰å±•ç¤ºæœ€åä¸€æ­¥ï¼Œâ€œè®¾ç½®æ¢¯åº¦é›¶ç‚¹å¹¶æ±‚è§£
<span class="math inline">\(\beta\)</span>â€
çš„ç»“æœå°±æ˜¯<strong>æ­£æ€æ–¹ç¨‹</strong>ï¼š</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the
vector of our optimal coefficient estimates.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is the â€œdesign
matrixâ€ where each row is an observation and each column is a predictor
variable. <span class="math inline">\(\mathbf{X}\)</span>
æ˜¯â€œè®¾è®¡çŸ©é˜µâ€ï¼Œå…¶ä¸­æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªè§‚æµ‹å€¼ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªé¢„æµ‹å˜é‡ã€‚</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of our
response variable. <span class="math inline">\(\mathbf{y}\)</span>
æ˜¯æˆ‘ä»¬çš„å“åº”å˜é‡çš„å‘é‡ã€‚</li>
</ul>
<p>This single equation is the general solution for finding the OLS
coefficients for <strong>any</strong> linear regression model, no matter
how many predictors you have. This is what statistical software
calculates for you under the hood.
æ— è®ºæœ‰å¤šå°‘ä¸ªé¢„æµ‹å˜é‡ï¼Œè¿™ä¸ªç®€å•çš„æ–¹ç¨‹éƒ½æ˜¯<strong>ä»»ä½•</strong>çº¿æ€§å›å½’æ¨¡å‹ä¸­
OLS ç³»æ•°çš„é€šè§£ã€‚</p>
<h1 id="matrix-notatio">5. matrix notatio</h1>
<p><img src="/imgs/5054C3/matrix_notatio.png"></p>
<ul>
<li><strong>å†…å®¹</strong>: This slide introduces the <strong>matrix
notation</strong> for multiple linear regression, which is a powerful
way to represent the entire system of equations in a compact form. This
notation isnâ€™t just for tidinessâ€”itâ€™s the foundation for how the
solutions are derived and calculated in software.</li>
</ul>
<p>å¤šå…ƒçº¿æ€§å›å½’çš„<strong>çŸ©é˜µç¬¦å·</strong>ï¼Œè¿™æ˜¯ä¸€ç§ä»¥ç´§å‡‘å½¢å¼è¡¨ç¤ºæ•´ä¸ªæ–¹ç¨‹ç»„çš„æœ‰æ•ˆæ–¹æ³•ã€‚è¿™ç§ç¬¦å·ä¸ä»…ä»…æ˜¯ä¸ºäº†ç®€æ´ï¼Œå®ƒè¿˜æ˜¯è½¯ä»¶ä¸­æ¨å¯¼å’Œè®¡ç®—è§£çš„åŸºç¡€ã€‚
Here is a more detailed breakdown.</p>
<h3 id="why-use-matrix-notation">## Why Use Matrix Notation?</h3>
<p>Imagine you have 10,000 observations (<span
class="math inline">\(n=10,000\)</span>) and 5 predictor variables
(<span class="math inline">\(p=5\)</span>). Writing out the model
equation for each observation would be impossible: <span
class="math inline">\(y_1 = \beta_0 + \beta_1x_{11} + \dots +
\beta_5x_{15} + \epsilon_1\)</span> <span class="math inline">\(y_2 =
\beta_0 + \beta_1x_{21} + \dots + \beta_5x_{25} + \epsilon_2\)</span>
â€¦and so on for 10,000 lines.</p>
<p>å‡è®¾ä½ æœ‰ 10,000 ä¸ªè§‚æµ‹å€¼ï¼ˆn=10,000ï¼‰å’Œ 5
ä¸ªé¢„æµ‹å˜é‡ï¼ˆp=5ï¼‰ã€‚ä¸ºæ¯ä¸ªè§‚æµ‹å€¼å†™å‡ºæ¨¡å‹æ–¹ç¨‹æ˜¯ä¸å¯èƒ½çš„ï¼š <span
class="math inline">\(y_1 = \beta_0 + \beta_1x_{11} + \dots +
\beta_5x_{15} + \epsilon_1\)</span> <span class="math inline">\(y_2 =
\beta_0 + \beta_1x_{21} + \dots + \beta_5x_{25} + \epsilon_2\)</span>
â€¦â€¦ä»¥æ­¤ç±»æ¨ï¼Œç›´åˆ° 10,000 è¡Œã€‚ Matrix notation allows us to consolidate
this entire system into a single, elegant
equation:çŸ©é˜µç¬¦å·ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†æ•´ä¸ªç³»ç»Ÿåˆå¹¶æˆä¸€ä¸ªç®€æ´çš„æ–¹ç¨‹ï¼š <span
class="math display">\[\mathbf{y} = \mathbf{X}\boldsymbol{\beta} +
\boldsymbol{\epsilon}\]</span> Letâ€™s break down each component shown on
your slide.</p>
<h3 id="the-components-explained">## The Components Explained</h3>
<h4 id="the-design-matrix-mathbfx-è®¾è®¡çŸ©é˜µ">1. The Design Matrix: <span
class="math inline">\(\mathbf{X}\)</span> è®¾è®¡çŸ©é˜µ</h4>
<p><span class="math display">\[\mathbf{X} = \begin{pmatrix} 1 &amp;
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\ 1 &amp; x_{21} &amp;
x_{22} &amp; \cdots &amp; x_{2p} \\ \vdots &amp; \vdots &amp; \vdots
&amp; \ddots &amp; \vdots \\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots
&amp; x_{np} \end{pmatrix}\]</span> This is the most important matrix.
It contains all of your predictor variable
data.è¿™æ˜¯æœ€é‡è¦çš„çŸ©é˜µã€‚å®ƒåŒ…å«æ‰€æœ‰é¢„æµ‹å˜é‡æ•°æ®ã€‚ * <strong>Rows:</strong>
Each row represents a single observation (e.g., a person, a company, a
day). There are <strong>n</strong>
rows.æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªè§‚å¯Ÿå€¼ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªäººã€ä¸€å®¶å…¬å¸ã€ä¸€å¤©ï¼‰ã€‚å…±æœ‰
<strong>n</strong> è¡Œã€‚ * <strong>Columns:</strong> Each column
represents a predictor variable. There are <strong>p</strong> predictor
columns, plus one special column.æ¯åˆ—ä»£è¡¨ä¸€ä¸ªé¢„æµ‹å˜é‡ã€‚å…±æœ‰
<strong>p</strong> ä¸ªé¢„æµ‹åˆ—ï¼Œå¤–åŠ ä¸€ä¸ªç‰¹æ®Šåˆ—ã€‚ * <strong>The Column of
Ones:</strong> This is a crucial detail. This first column of all ones
is a placeholder for the <strong>intercept term (<span
class="math inline">\(\beta_0\)</span>)</strong>. When you perform
matrix multiplication, this <code>1</code> gets multiplied by <span
class="math inline">\(\beta_0\)</span>, ensuring the intercept is
included in the model for every single observation.
è¿™æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦çš„ç»†èŠ‚ã€‚ç¬¬ä¸€åˆ—ï¼ˆå…¨ 1ï¼‰æ˜¯<strong>æˆªè·é¡¹ (<span
class="math inline">\(\beta_0\)</span>)</strong>
çš„å ä½ç¬¦ã€‚æ‰§è¡ŒçŸ©é˜µä¹˜æ³•æ—¶ï¼Œè¿™ä¸ª <code>1</code> ä¼šä¹˜ä»¥ <span
class="math inline">\(\beta_0\)</span>ï¼Œä»¥ç¡®ä¿æˆªè·åŒ…å«åœ¨æ¨¡å‹ä¸­ï¼Œé€‚ç”¨äºæ¯ä¸ªè§‚æµ‹å€¼ã€‚</p>
<h4 id="the-coefficient-vector-boldsymbolbeta-ç³»æ•°å‘é‡">2. The
Coefficient Vector: <span
class="math inline">\(\boldsymbol{\beta}\)</span> ç³»æ•°å‘é‡</h4>
<p><span class="math display">\[\boldsymbol{\beta} = \begin{pmatrix}
\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix}\]</span> This is a
column vector that contains all the model parametersâ€”the unknown values
we want to estimate. The goal of linear regression is to find the
numerical values for this vector.</p>
<h4 id="the-response-vector-mathbfy-å“åº”å‘é‡">3. The Response Vector:
<span class="math inline">\(\mathbf{y}\)</span> å“åº”å‘é‡</h4>
<p><span class="math display">\[\mathbf{y} = \begin{pmatrix} y_1 \\
\vdots \\ y_n \end{pmatrix}\]</span> This is a column vector containing
all the observed outcomes you are trying to predict (e.g., sales, test
scores, stock prices).</p>
<h4 id="the-error-vector-boldsymbolepsilon-è¯¯å·®å‘é‡">4. The Error
Vector: <span class="math inline">\(\boldsymbol{\epsilon}\)</span>
è¯¯å·®å‘é‡</h4>
<p><span class="math display">\[\boldsymbol{\epsilon} = \begin{pmatrix}
\epsilon_1 \\ \vdots \\ \epsilon_n \end{pmatrix}\]</span> This column
vector bundles together all the individual, unobserved random errors. It
represents the portion of <strong>y</strong> that our model cannot
explain with <strong>X</strong>.</p>
<h3 id="putting-it-all-together">## Putting It All Together</h3>
<p>When you write the equation <span class="math inline">\(\mathbf{y} =
\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span>, you are
actually representing the entire system of individual equations.</p>
<p>Letâ€™s look at the multiplication <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>: <span
class="math display">\[\begin{pmatrix} 1 &amp; x_{11} &amp; \dots &amp;
x_{1p} \\ 1 &amp; x_{21} &amp; \dots &amp; x_{2p} \\ \vdots &amp; \vdots
&amp; \ddots &amp; \vdots \\ 1 &amp; x_{n1} &amp; \dots &amp; x_{np}
\end{pmatrix} \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
\end{pmatrix} = \begin{pmatrix} 1\cdot\beta_0 + x_{11}\cdot\beta_1 +
\dots + x_{1p}\cdot\beta_p \\ 1\cdot\beta_0 + x_{21}\cdot\beta_1 + \dots
+ x_{2p}\cdot\beta_p \\ \vdots \\ 1\cdot\beta_0 + x_{n1}\cdot\beta_1 +
\dots + x_{np}\cdot\beta_p \end{pmatrix}\]</span> As you can see, the
result of this multiplication is a single column vector where each row
is the â€œpredictorâ€ part of the regression equation for that observation.
æ­¤ä¹˜æ³•çš„ç»“æœæ˜¯ä¸€ä¸ªå•åˆ—å‘é‡ï¼Œå…¶ä¸­æ¯ä¸€è¡Œéƒ½æ˜¯è¯¥è§‚æµ‹å€¼çš„å›å½’æ–¹ç¨‹çš„â€œé¢„æµ‹å˜é‡â€éƒ¨åˆ†ã€‚</p>
<p>By setting this equal to <span class="math inline">\(\mathbf{y} -
\boldsymbol{\epsilon}\)</span>, you perfectly recreate the entire set of
<code>n</code> equations in one clean statement. This compact form is
what allows us to easily derive and compute the <strong>Normal
Equation</strong> solution: <span
class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>.è¿™ç§ç´§å‡‘å½¢å¼ä½¿æˆ‘ä»¬èƒ½å¤Ÿè½»æ¾æ¨å¯¼å’Œè®¡ç®—<strong>æ­£æ€æ–¹ç¨‹</strong>çš„è§£</p>
<h1
id="the-core-mathematical-conclusion-of-ordinary-least-squares-ols">6.
the core mathematical conclusion of Ordinary Least Squares (OLS)</h1>
<p><img src="/imgs/5054C3/OLS1.png">
<img src="/imgs/5054C3/OLS2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>: Of course. These slides present the core
mathematical conclusion of Ordinary Least Squares (OLS) and a key
geometric property that explains <em>why</em> this solution works.
å±•ç¤ºäº†æ™®é€šæœ€å°äºŒä¹˜æ³• (OLS)
çš„æ ¸å¿ƒæ•°å­¦ç»“è®ºï¼Œä»¥åŠä¸€ä¸ªå…³é”®çš„å‡ ä½•æ€§è´¨ï¼Œè§£é‡Šäº†è¯¥è§£å†³æ–¹æ¡ˆ<em>ä¸ºä½•</em>æœ‰æ•ˆã€‚
Letâ€™s break down the concepts and the calculation processes in
detail.</li>
</ul>
<hr />
<h3 id="part-1-the-objective-and-the-solution-slide-1-æœ€å°åŒ–å‡ ä½•è·ç¦»">##
Part 1: The Objective and the Solution (Slide 1) æœ€å°åŒ–å‡ ä½•è·ç¦»</h3>
<p>This slide summarizes the entire OLS problem and its solution in the
language of matrix algebra.</p>
<h4 id="the-concept-minimizing-geometric-distance"><strong>The Concept:
Minimizing Geometric Distance</strong></h4>
<p>â€œæœ€å°äºŒä¹˜å‡†åˆ™â€æ˜¯æˆ‘ä»¬æ¨¡å‹çš„ç›®æ ‡ã€‚ The â€œleast squares criterionâ€ is the
objective of our model. The slide shows it in two equivalent forms:</p>
<ol type="1">
<li><strong>Summation Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_{i1} -
\dots - \beta_px_{ip})^2\)</span> This is the sum of the squared
differences between the actual values (<span
class="math inline">\(y_i\)</span>) and the predicted values. è¿™æ˜¯å®é™…å€¼
(<span class="math inline">\(y_i\)</span>) ä¸é¢„æµ‹å€¼ä¹‹å·®çš„å¹³æ–¹å’Œã€‚</li>
<li><strong>Matrix Form:</strong> <span
class="math inline">\(||\mathbf{y} - \mathbf{X}\boldsymbol{\beta}||^2 =
(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} -
\mathbf{X}\boldsymbol{\beta})\)</span> This is the more powerful way to
view the problem. Think of <span
class="math inline">\(\mathbf{y}\)</span> (the vector of all actual
outcomes) and <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> (the vector
of all predicted outcomes) as two points in an n-dimensional space. The
expression <span class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{\beta}||^2\)</span> represents the <strong>squared
Euclidean distance</strong> between these two points. å°† <span
class="math inline">\(\mathbf{y}\)</span>ï¼ˆæ‰€æœ‰å®é™…ç»“æœçš„å‘é‡ï¼‰å’Œ <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>ï¼ˆæ‰€æœ‰é¢„æµ‹ç»“æœçš„å‘é‡ï¼‰è§†ä¸º
n ç»´ç©ºé—´ä¸­çš„ä¸¤ä¸ªç‚¹ã€‚è¡¨è¾¾å¼ <span class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{\beta}||^2\)</span>
è¡¨ç¤ºè¿™ä¸¤ç‚¹ä¹‹é—´çš„<strong>å¹³æ–¹æ¬§æ°è·ç¦»</strong>ã€‚ Therefore, the OLS
problem is a geometric one: <strong>Find the coefficient vector <span
class="math inline">\(\boldsymbol{\beta}\)</span> that makes the
predicted values vector <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> as close as
possible to the actual values vector <span
class="math inline">\(\mathbf{y}\)</span>.</strong> å› æ­¤ï¼ŒOLS
é—®é¢˜æ˜¯ä¸€ä¸ªå‡ ä½•é—®é¢˜ï¼š<strong>æ‰¾åˆ°ä¸€ä¸ªç³»æ•°å‘é‡ <span
class="math inline">\(\boldsymbol{\beta}\)</span>ï¼Œä½¿é¢„æµ‹å€¼å‘é‡ <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>
å°½å¯èƒ½æ¥è¿‘å®é™…å€¼å‘é‡ <span
class="math inline">\(\mathbf{y}\)</span>ã€‚</strong></li>
</ol>
<h4
id="the-solution-the-least-squares-estimator-lseæœ€å°äºŒä¹˜ä¼°è®¡å™¨-lse"><strong>The
Solution: The Least Squares Estimator (LSE)</strong>æœ€å°äºŒä¹˜ä¼°è®¡å™¨
(LSE)</h4>
<p>The slide provides the direct solution to this minimization problem,
which is the <strong>Normal
Equation</strong>:æ­¤æœ€å°åŒ–é—®é¢˜çš„ç›´æ¥è§£ï¼Œå³<strong>æ­£æ€æ–¹ç¨‹</strong>ï¼š</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<p>This formula gives you the exact vector of coefficients <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that minimizes
the squared distance. We get this formula by taking the gradient (the
multidimensional version of a derivative) of the distance function with
respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>,
setting it to zero, and solving, as hinted at in your previous slides.
ç»™å‡ºäº†ä½¿å¹³æ–¹è·ç¦»æœ€å°åŒ–çš„ç²¾ç¡®ç³»æ•°å‘é‡ é€šè¿‡å–è·ç¦»å‡½æ•°å…³äº <span
class="math inline">\(\boldsymbol{\beta}\)</span>
çš„æ¢¯åº¦ï¼ˆå¯¼æ•°çš„å¤šç»´ç‰ˆæœ¬ï¼‰ï¼Œå°†å…¶è®¾ä¸ºé›¶ï¼Œç„¶åæ±‚è§£ï¼Œå³å¯å¾—åˆ°æ­¤å…¬å¼ã€‚
Finally, the slide defines: * <strong>Fitted values:</strong> <span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span> (The vector of predictions
using our optimal coefficients). æ‹Ÿåˆå€¼ * <strong>Residuals:</strong>
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span> (The vector of errors, representing the
difference between actuals and
predictions).è¯¯å·®å‘é‡ï¼Œè¡¨ç¤ºå®é™…å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„å·®å¼‚</p>
<h3
id="part-2-the-geometric-property-and-proofs-slide-2å‡ ä½•æ€§è´¨åŠè¯æ˜">##
Part 2: The Geometric Property and Proofs (Slide 2)å‡ ä½•æ€§è´¨åŠè¯æ˜</h3>
<p>This slide explains a beautiful and fundamental property of the least
squares solution:
<strong>orthogonality</strong>.è§£é‡Šäº†æœ€å°äºŒä¹˜è§£çš„ä¸€ä¸ªç¾å¦™è€ŒåŸºæœ¬çš„æ€§è´¨ï¼š<strong>æ­£äº¤æ€§</strong>ã€‚</p>
<h4 id="the-concept-orthogonality-of-residualsæ®‹å·®çš„æ­£äº¤æ€§"><strong>The
Concept: Orthogonality of Residuals</strong>æ®‹å·®çš„æ­£äº¤æ€§</h4>
<p>The main idea is that the residual vector <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> is
<strong>orthogonal</strong> (perpendicular) to every predictor variable
in your model. ä¸»è¦æ€æƒ³æ˜¯æ®‹å·®å‘é‡ <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>
ä¸æ¨¡å‹ä¸­çš„æ¯ä¸ªé¢„æµ‹å˜é‡<strong>æ­£äº¤</strong>ï¼ˆå‚ç›´ï¼‰ã€‚</p>
<ul>
<li><p><strong>Geometric Intuition:</strong> Think of the columns of
your matrix <span class="math inline">\(\mathbf{X}\)</span> (i.e., your
predictors and the intercept) as defining a flat surface, or a
â€œhyperplane,â€ in a high-dimensional space. Your actual data vector <span
class="math inline">\(\mathbf{y}\)</span> exists somewhere in this
space, likely not on the hyperplane. The OLS process finds the point on
that hyperplane, <span class="math inline">\(\hat{\mathbf{y}}\)</span>,
that is closest to <span class="math inline">\(\mathbf{y}\)</span>. The
shortest line from a point to a plane is always one that is
<strong>perpendicular</strong> to the plane. The residual vector, <span
class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>, <em>is</em> that line. å°†çŸ©é˜µ <span
class="math inline">\(\mathbf{X}\)</span>
çš„åˆ—ï¼ˆå³é¢„æµ‹å˜é‡å’Œæˆªè·ï¼‰æƒ³è±¡æˆåœ¨é«˜ç»´ç©ºé—´ä¸­å®šä¹‰ä¸€ä¸ªå¹³é¢æˆ–â€œè¶…å¹³é¢â€ã€‚å®é™…æ•°æ®å‘é‡
<span class="math inline">\(\mathbf{y}\)</span>
å­˜åœ¨äºè¯¥ç©ºé—´çš„æŸä¸ªä½ç½®ï¼Œå¯èƒ½ä¸åœ¨è¶…å¹³é¢ä¸Šã€‚OLS è¿‡ç¨‹ä¼šåœ¨è¯¥è¶…å¹³é¢ <span
class="math inline">\(\hat{\mathbf{y}}\)</span> ä¸Šæ‰¾åˆ°ä¸ <span
class="math inline">\(\mathbf{y}\)</span>
æœ€æ¥è¿‘çš„ç‚¹ã€‚ä»ä¸€ä¸ªç‚¹åˆ°ä¸€ä¸ªå¹³é¢çš„æœ€çŸ­çº¿å§‹ç»ˆæ˜¯ä¸è¯¥å¹³é¢<strong>å‚ç›´</strong>çš„çº¿ã€‚æ®‹å·®å‘é‡
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span> å°±æ˜¯è¿™æ¡ç›´çº¿ã€‚</p></li>
<li><p><strong>Mathematical Statement:</strong> This geometric property
is stated as <span class="math inline">\(\mathbf{X}^T
\hat{\boldsymbol{\epsilon}} = \mathbf{0}\)</span>. This equation means
that the dot product of the residual vector with every column of <span
class="math inline">\(\mathbf{X}\)</span> is zero, which is the
mathematical definition of orthogonality. è¯¥ç­‰å¼æ„å‘³ç€æ®‹å·®å‘é‡ä¸ <span
class="math inline">\(\mathbf{X}\)</span>
æ¯ä¸€åˆ—çš„ç‚¹ç§¯éƒ½ä¸ºé›¶ï¼Œè¿™æ­£æ˜¯æ­£äº¤æ€§çš„æ•°å­¦å®šä¹‰ã€‚</p></li>
</ul>
<h4 id="the-calculation-process-the-proofs"><strong>The Calculation
Process (The Proofs)</strong></h4>
<p><strong>1. Proof of Orthogonality:</strong> The slide shows a
step-by-step calculation to prove that <span
class="math inline">\(\mathbf{X}^T \hat{\boldsymbol{\epsilon}}\)</span>
is indeed zero. * <strong>Step 1:</strong> Start with the expression to
be proven: <span class="math inline">\(\mathbf{X}^T
\hat{\boldsymbol{\epsilon}}\)</span> ä»å¾…è¯æ˜çš„è¡¨è¾¾å¼å¼€å§‹ï¼š *
<strong>Step 2:</strong> Substitute the definition of the residual,
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\mathbf{X}^T (\mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}})\]</span> ä»£å…¥æ®‹å·®çš„å®šä¹‰ *
<strong>Step 3:</strong> Distribute the <span
class="math inline">\(\mathbf{X}^T\)</span>: <span
class="math display">\[\mathbf{X}^T \mathbf{y} -
\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}}\]</span><em>åˆ†é… </em>
<strong>Step 4:</strong> Substitute the Normal Equation for <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\mathbf{X}^T \mathbf{y} - \mathbf{X}^T\mathbf{X}
[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}]\]</span> *
<strong>Step 5:</strong> The key step is the cancellation. A matrix
<span class="math inline">\((\mathbf{X}^T\mathbf{X})\)</span> multiplied
by its inverse <span
class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span> equals the
identity matrix <span class="math inline">\(\mathbf{I}\)</span>, which
acts like the number 1 in multiplication. <span
class="math display">\[\mathbf{X}^T \mathbf{y} - \mathbf{I}
\mathbf{X}^T\mathbf{y} = \mathbf{X}^T \mathbf{y} -
\mathbf{X}^T\mathbf{y} = \mathbf{0}\]</span> å…³é”®æ­¥éª¤æ˜¯æ¶ˆå»ã€‚ This
completes the proof, showing that the orthogonality property is a direct
consequence of the Normal Equation solution.</p>
<p><strong>2. Proof of LSE:</strong> This is a more abstract proof
showing that our <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> truly gives the
minimum possible error. It uses the orthogonality property and the
Pythagorean theorem for vectors. It essentially shows that for any other
possible coefficient vector <span
class="math inline">\(\boldsymbol{v}\)</span>, the error <span
class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{v}||^2\)</span> will always be greater than or
equal to the error from our LSE, <span
class="math inline">\(||\mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}}||^2\)</span>.</p>
<h1 id="geometric-interpretation">7.geometric interpretation</h1>
<p><img src="/imgs/5054C3/geometric_interpretation1.png">
<img src="/imgs/5054C3/geometric_interpretation2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>These two slides together provide a powerful geometric interpretation
of how Ordinary Least Squares (OLS) works, centered on the concepts of
<strong>orthogonality</strong> and <strong>projection</strong>.
ä»¥<strong>æ­£äº¤æ€§</strong>å’Œ<strong>æŠ•å½±</strong>çš„æ¦‚å¿µä¸ºä¸­å¿ƒï¼Œä»å‡ ä½•è§’åº¦æœ‰åŠ›åœ°è¯ é‡Šäº†æ™®é€šæœ€å°äºŒä¹˜æ³•
(OLS) çš„å·¥ä½œåŸç†ã€‚</p>
<p>Hereâ€™s a detailed summary of the concepts and the processes they
describe.</p>
<h3 id="summary">## Summary</h3>
<p>These slides explain that the process of finding the â€œbest fitâ€ line
in regression is geometrically equivalent to <strong>projecting</strong>
the actual data vector (<span class="math inline">\(\mathbf{y}\)</span>)
onto a hyperplane defined by the predictor variables (<span
class="math inline">\(\mathbf{X}\)</span>). This projection splits the
actual data into two perpendicular components:
è§£é‡Šäº†å›å½’åˆ†æä¸­å¯»æ‰¾â€œæœ€ä½³æ‹Ÿåˆâ€ç›´çº¿çš„è¿‡ç¨‹ï¼Œå…¶å‡ ä½•æ„ä¹‰ç­‰åŒäºå°†å®é™…æ•°æ®å‘é‡
(<span class="math inline">\(\mathbf{y}\)</span>)
<strong>æŠ•å½±</strong>åˆ°ç”±é¢„æµ‹å˜é‡ (<span
class="math inline">\(\mathbf{X}\)</span>)
å®šä¹‰çš„è¶…å¹³é¢ä¸Šã€‚æ­¤æŠ•å½±å°†å®é™…æ•°æ®æ‹†åˆ†ä¸ºä¸¤ä¸ªå‚ç›´åˆ†é‡ï¼š</p>
<ol type="1">
<li><strong>The Fitted Values (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>):</strong> The part of
the data that is perfectly explained by the model (the projection).
æ•°æ®ä¸­èƒ½å¤Ÿè¢«æ¨¡å‹å®Œç¾è§£é‡Šçš„éƒ¨åˆ†ï¼ˆæŠ•å½±ï¼‰ã€‚</li>
<li><strong>The Residuals (<span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>):</strong>
The part of the data that is unexplained (the error), which is
perpendicular to the explained part.
æ•°æ®ä¸­æ— æ³•è§£é‡Šçš„éƒ¨åˆ†ï¼ˆè¯¯å·®ï¼‰ï¼Œå®ƒä¸è¢«è§£é‡Šéƒ¨åˆ†å‚ç›´ã€‚ A special tool called
the <strong>projection matrix (H)</strong>, or â€œhat matrix,â€ is
introduced as the operator that performs this projection.
å¼•å…¥ä¸€ä¸ªç§°ä¸º<strong>æŠ•å½±çŸ©é˜µ
(H)</strong>ï¼ˆæˆ–ç§°â€œå¸½å­çŸ©é˜µâ€ï¼‰çš„ç‰¹æ®Šå·¥å…·ï¼Œä½œä¸ºæ‰§è¡Œæ­¤æŠ•å½±çš„è¿ç®—ç¬¦ã€‚</li>
</ol>
<h3 id="concepts-and-process-explained-in-detail">## Concepts and
Process Explained in Detail</h3>
<h4 id="the-fitted-values-as-a-linear-combination-æ‹Ÿåˆå€¼ä½œä¸ºçº¿æ€§ç»„åˆ">1.
The Fitted Values as a Linear Combination æ‹Ÿåˆå€¼ä½œä¸ºçº¿æ€§ç»„åˆ</h4>
<p>The first slide starts by stating that the fitted value vector <span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span> is a <strong>linear
combination</strong> of the columns of <span
class="math inline">\(\mathbf{X}\)</span> (your predictors).</p>
<ul>
<li><p><strong>Concept:</strong> This means that the vector of fitted
values, <span class="math inline">\(\hat{\mathbf{y}}\)</span>, must lie
within the geometric space (a line, plane, or hyperplane) spanned by
your predictor variables. The model is incapable of producing a
prediction that does not live in this space. è¿™æ„å‘³ç€æ‹Ÿåˆå€¼å‘é‡ <span
class="math inline">\(\hat{\mathbf{y}}\)</span>
å¿…é¡»ä½äºé¢„æµ‹å˜é‡æ‰€æ„æˆçš„å‡ ä½•ç©ºé—´ï¼ˆç›´çº¿ã€å¹³é¢æˆ–è¶…å¹³é¢ï¼‰å†…ã€‚æ¨¡å‹æ— æ³•ç”Ÿæˆä¸å­˜åœ¨äºæ­¤ç©ºé—´çš„é¢„æµ‹ã€‚
#### 2. The Projection Matrix (The â€œHat Matrixâ€) æŠ•å½±çŸ©é˜µï¼ˆâ€œå¸½å­çŸ©é˜µâ€ï¼‰
The second slide introduces the tool that makes this projection happen:
the <strong>projection matrix</strong>, also called the <strong>hat
matrix</strong>, <strong>H</strong>.</p></li>
<li><p><strong>Definition:</strong> <span
class="math inline">\(\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span></p></li>
<li><p><strong>Process:</strong> This matrix has a special job. When you
multiply it by any vector (like our data vector <span
class="math inline">\(\mathbf{y}\)</span>), it projects that vector onto
the space spanned by the columns of <span
class="math inline">\(\mathbf{X}\)</span>. We can see this by starting
with our definition of fitted values and substituting the normal
equation solution for <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\hat{\mathbf{y}} =
\mathbf{X}\hat{\boldsymbol{\beta}} =
\mathbf{X}[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}] =
[\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T]\mathbf{y}\]</span>
This shows that: <span class="math display">\[\hat{\mathbf{y}} =
\mathbf{H}\mathbf{y}\]</span> This is why <strong>H</strong> is
nicknamed the <strong>hat matrix</strong>â€”it â€œputs the hatâ€ on <span
class="math inline">\(\mathbf{y}\)</span>.
è¿™ä¸ªçŸ©é˜µæœ‰å…¶ç‰¹æ®Šçš„ç”¨é€”ã€‚å½“ä½ å°†å®ƒä¹˜ä»¥ä»»ä½•å‘é‡ï¼ˆä¾‹å¦‚æˆ‘ä»¬çš„æ•°æ®å‘é‡ <span
class="math inline">\(\mathbf{y}\)</span>ï¼‰æ—¶ï¼Œå®ƒä¼šå°†è¯¥å‘é‡æŠ•å½±åˆ°ç”±
<span class="math inline">\(\mathbf{X}\)</span> çš„åˆ—æ‰€è·¨è¶Šçš„ç©ºé—´ä¸Šã€‚
#### 3. The Orthogonality of Fitted Values and Residuals
æ‹Ÿåˆå€¼å’Œæ®‹å·®çš„æ­£äº¤æ€§ This is the central concept of the first slide and
a fundamental property of least squares.</p></li>
<li><p><strong>Concept:</strong> The fitted value vector (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>) and the residual vector
(<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>) are <strong>orthogonal</strong>
(perpendicular) to each other.</p></li>
<li><p><strong>Mathematical Statement:</strong> Their dot product is
zero: <span class="math inline">\(\hat{\mathbf{y}}^T(\mathbf{y} -
\hat{\mathbf{y}}) = 0\)</span>.</p></li>
<li><p><strong>Geometric Intuition:</strong> This means the vectors
<span class="math inline">\(\mathbf{y}\)</span>, <span
class="math inline">\(\hat{\mathbf{y}}\)</span>, and <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> form a
<strong>right-angled triangle</strong> in n-dimensional space. The
actual data vector <span class="math inline">\(\mathbf{y}\)</span> is
the hypotenuse, while the modelâ€™s prediction <span
class="math inline">\(\hat{\mathbf{y}}\)</span> and the error <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> are the two
perpendicular legs. è¿™æ„å‘³ç€å‘é‡ <span
class="math inline">\(\mathbf{y}\)</span>ã€<span
class="math inline">\(\hat{\mathbf{y}}\)</span> å’Œ <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> åœ¨ n
ç»´ç©ºé—´ä¸­æ„æˆä¸€ä¸ª<strong>ç›´è§’ä¸‰è§’å½¢</strong>ã€‚å®é™…æ•°æ®å‘é‡ <span
class="math inline">\(\mathbf{y}\)</span> æ˜¯æ–œè¾¹ï¼Œè€Œæ¨¡å‹çš„é¢„æµ‹å€¼ <span
class="math inline">\(\hat{\mathbf{y}}\)</span> å’Œè¯¯å·®å€¼ <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>
æ˜¯ä¸¤æ¡å‚ç›´è¾¹ã€‚</p></li>
</ul>
<h4 id="the-pythagorean-theorem-of-least-squares">4. The Pythagorean
Theorem of Least Squares</h4>
<p>The orthogonality relationship directly implies the Pythagorean
theorem.</p>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(||\mathbf{y}||^2 = ||\hat{\mathbf{y}}||^2 +
||\mathbf{y} - \hat{\mathbf{y}}||^2\)</span></li>
<li><strong>Concept:</strong> This is one of the most important
equations in statistics, as it partitions the total variance in the
data. è¿™æ˜¯ç»Ÿè®¡å­¦ä¸­æœ€é‡è¦çš„æ–¹ç¨‹ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒå¯ä»¥åˆ†å‰²æ•°æ®ä¸­çš„æ€»æ–¹å·®ã€‚
<ul>
<li><span class="math inline">\(||\mathbf{y}||^2\)</span> is
proportional to the <strong>Total Sum of Squares (TSS):</strong> The
total variation of the response variable around its
mean.å“åº”å˜é‡å›´ç»•å…¶å‡å€¼çš„æ€»å˜å¼‚ã€‚</li>
<li><span class="math inline">\(||\hat{\mathbf{y}}||^2\)</span> is
proportional to the <strong>Explained Sum of Squares (ESS):</strong> The
portion of the total variation that is explained by your regression
model.å›å½’æ¨¡å‹å¯ä»¥è§£é‡Šçš„æ€»å˜å¼‚éƒ¨åˆ†ã€‚</li>
<li><span class="math inline">\(||\mathbf{y} -
\hat{\mathbf{y}}||^2\)</span> is the <strong>Residual Sum of Squares
(RSS):</strong> The portion of the total variation that is left
unexplained (the error).æ€»å˜å¼‚ä¸­æœªè§£é‡Šçš„éƒ¨åˆ†ï¼ˆå³è¯¯å·®ï¼‰ã€‚</li>
</ul></li>
</ul>
<p>This relationship, <strong>Total Variation = Explained Variation +
Unexplained Variation</strong>, is the foundation for calculating
metrics like <strong>R-squared (<span
class="math inline">\(R^2\)</span>)</strong>, which measures the
goodness of fit of your model. <strong>æ€»å˜å¼‚ = è§£é‡Šå˜å¼‚ +
æœªè§£é‡Šå˜å¼‚</strong>ï¼Œæ˜¯è®¡ç®—<strong>R å¹³æ–¹ (<span
class="math inline">\(R^2\)</span>)</strong>
ç­‰æŒ‡æ ‡çš„åŸºç¡€ï¼Œè¯¥æŒ‡æ ‡ç”¨äºè¡¡é‡æ¨¡å‹çš„æ‹Ÿåˆä¼˜åº¦ã€‚</p>
<h4 id="residuals-and-the-identity-matrix-æ®‹å·®å’Œå•ä½çŸ©é˜µ">5. Residuals
and the Identity Matrix æ®‹å·®å’Œå•ä½çŸ©é˜µ</h4>
<p>Finally, the second slide shows that just as <strong>H</strong>
projects onto the â€œmodel space,â€ a related matrix projects onto the
â€œerror space.â€ æœ€åï¼Œç¬¬äºŒå¼ å¹»ç¯ç‰‡æ˜¾ç¤ºï¼Œæ­£å¦‚<strong>H</strong>
æŠ•å½±åˆ°â€œæ¨¡å‹ç©ºé—´â€ä¸€æ ·ï¼Œç›¸å…³çŸ©é˜µä¹Ÿä¼šæŠ•å½±åˆ°â€œè¯¯å·®ç©ºé—´â€ã€‚ *
<strong>Process:</strong> We can express the residuals using the hat
matrix: <span class="math display">\[\hat{\boldsymbol{\epsilon}} =
\mathbf{y} - \hat{\mathbf{y}} = \mathbf{y} - \mathbf{H}\mathbf{y} =
(\mathbf{I} - \mathbf{H})\mathbf{y}\]</span> The matrix <span
class="math inline">\((\mathbf{I} - \mathbf{H})\)</span> is also a
projection matrix. It takes the original data vector <span
class="math inline">\(\mathbf{y}\)</span> and projects it onto the space
that is orthogonal to all of your predictors, giving you the residual
vector directly.</p>
<h1
id="visualization-of-ordinary-least-squares-ols-regression">8.visualization
of Ordinary Least Squares (OLS) regression</h1>
<p><img src="/imgs/5054C3/visualization_of_Ordinary_Least_Squares_(OLS)_regression.png"></p>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>This slide provides an excellent geometric visualization of whatâ€™s
happening â€œunder the hoodâ€ in Ordinary Least Squares (OLS) regression.
It translates the algebraic formulas into a more intuitive spatial
concept. è¿™å¼ å¹»ç¯ç‰‡ä»¥å‡ºè‰²çš„å‡ ä½•å¯è§†åŒ–æ–¹å¼å±•ç°äº†æ™®é€šæœ€å°äºŒä¹˜ (OLS)
å›å½’çš„â€œå¹•åâ€æœºåˆ¶ã€‚å®ƒå°†ä»£æ•°å…¬å¼è½¬åŒ–ä¸ºæ›´ç›´è§‚çš„ç©ºé—´æ¦‚å¿µã€‚</p>
<h3 id="summary-1">## Summary</h3>
<p>The image shows that the process of finding the least squares
estimates is geometrically equivalent to taking the <strong>actual
outcome vector</strong> (<span
class="math inline">\(\mathbf{y}\)</span>) and finding its
<strong>orthogonal projection</strong> (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>) onto a
<strong>hyperplane</strong> formed by the predictor variables (<span
class="math inline">\(\mathbf{x}_1\)</span> and <span
class="math inline">\(\mathbf{x}_2\)</span>). The projection <span
class="math inline">\(\hat{\mathbf{y}}\)</span> is the vector of fitted
values, representing the closest possible approximation of the real data
that the model can achieve.</p>
<p>è¯¥å›¾æ˜¾ç¤ºï¼Œå¯»æ‰¾æœ€å°äºŒä¹˜ä¼°è®¡å€¼çš„è¿‡ç¨‹åœ¨å‡ ä½•ä¸Šç­‰åŒäºå°†<strong>å®é™…ç»“æœå‘é‡</strong>
(<span class="math inline">\(\mathbf{y}\)</span>)
æ±‚å‡ºå…¶<strong>æ­£äº¤æŠ•å½±</strong> (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>) åˆ°ç”±é¢„æµ‹å˜é‡ (<span
class="math inline">\(\mathbf{x}_1\)</span> å’Œ <span
class="math inline">\(\mathbf{x}_2\)</span>
æ„æˆçš„<strong>è¶…å¹³é¢</strong>ä¸Šã€‚æŠ•å½± <span
class="math inline">\(\hat{\mathbf{y}}\)</span>
æ˜¯æ‹Ÿåˆå€¼çš„å‘é‡ï¼Œè¡¨ç¤ºæ¨¡å‹èƒ½å¤Ÿè¾¾åˆ°çš„ä¸çœŸå®æ•°æ®æœ€æ¥è¿‘çš„è¿‘ä¼¼å€¼ã€‚</p>
<h3 id="the-concepts-explained-spatiallyç©ºé—´æ¦‚å¿µè§£é‡Š">## The Concepts
Explained Spatiallyç©ºé—´æ¦‚å¿µè§£é‡Š</h3>
<p>Letâ€™s break down each element of the diagram and its meaning:</p>
<h4 id="the-space-itself-ç©ºé—´æœ¬èº«">1. The Space Itself ç©ºé—´æœ¬èº«</h4>
<ul>
<li><strong>Concept:</strong> We are not in a simple 2D or 3D graph
where axes are X and Y. Instead, we are in an <strong>n-dimensional
space</strong>, where <strong>n is the number of observations</strong>
in your dataset. Each axis in this space corresponds to one observation
(e.g., one person, one day).
æˆ‘ä»¬å¹¶éèº«å¤„ä¸€ä¸ªç®€å•çš„äºŒç»´æˆ–ä¸‰ç»´å›¾å½¢ä¸­ï¼Œå…¶ä¸­åæ ‡è½´ä¸º X å’Œ
Yã€‚ç›¸åï¼Œæˆ‘ä»¬èº«å¤„ä¸€ä¸ª <strong>n ç»´ç©ºé—´</strong>ï¼Œå…¶ä¸­ <strong>n
æ˜¯æ•°æ®é›†ä¸­çš„è§‚æµ‹å€¼æ•°é‡</strong>ã€‚æ­¤ç©ºé—´ä¸­çš„æ¯ä¸ªè½´å¯¹åº”ä¸€ä¸ªè§‚æµ‹å€¼ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªäººï¼Œä¸€å¤©ï¼‰ã€‚</li>
<li><strong>Meaning:</strong> A vector like <strong>y</strong> or
<strong>xâ‚</strong> is a single point in this high-dimensional space.
For example, if you have 50 data points, <strong>y</strong> is a vector
pointing to a specific location in a 50-dimensional space. åƒ
<strong>y</strong> æˆ– <strong>xâ‚</strong>
è¿™æ ·çš„å‘é‡æ˜¯è¿™ä¸ªé«˜ç»´ç©ºé—´ä¸­çš„å•ä¸ªç‚¹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ 50
ä¸ªæ•°æ®ç‚¹ï¼Œ<strong>y</strong> å°±æ˜¯æŒ‡å‘ 50 ç»´ç©ºé—´ä¸­ç‰¹å®šä½ç½®çš„å‘é‡ã€‚</li>
</ul>
<h4
id="the-predictor-hyperplane-the-yellow-surfaceé¢„æµ‹å˜é‡è¶…å¹³é¢é»„è‰²è¡¨é¢">2.
The Predictor Hyperplane (The Yellow
Surface)é¢„æµ‹å˜é‡è¶…å¹³é¢ï¼ˆé»„è‰²è¡¨é¢ï¼‰</h4>
<ul>
<li><strong>Concept:</strong> The vectors for your predictor variables,
<strong>xâ‚</strong> and <strong>xâ‚‚</strong>, define a flat surface. If
you had only one predictor, this would be a line. With two, itâ€™s a
plane. With more, itâ€™s a <strong>hyperplane</strong>.é¢„æµ‹å˜é‡çš„å‘é‡
<strong>xâ‚</strong> å’Œ <strong>xâ‚‚</strong>
å®šä¹‰äº†ä¸€ä¸ªå¹³é¢ã€‚å¦‚æœåªæœ‰ä¸€ä¸ªé¢„æµ‹å˜é‡ï¼Œå®ƒå°±æ˜¯ä¸€æ¡çº¿ã€‚å¦‚æœæœ‰ä¸¤ä¸ªï¼Œå®ƒå°±æ˜¯ä¸€ä¸ªå¹³é¢ã€‚å¦‚æœæœ‰æ›´å¤šçš„é¢„æµ‹å˜é‡ï¼Œå®ƒå°±æ˜¯ä¸€ä¸ª<strong>è¶…å¹³é¢</strong>ã€‚</li>
<li><strong>Meaning:</strong> This yellow plane represents the
<strong>â€œworld of possible predictionsâ€</strong> that your model is
allowed to make. Any linear combination of your predictorsâ€”which is what
a linear regression model calculatesâ€”will result in a vector that lies
<em>somewhere</em> on this surface.
è¿™ä¸ªé»„è‰²å¹³é¢ä»£è¡¨ä½ çš„æ¨¡å‹å¯ä»¥åšå‡ºçš„<strong>â€œå¯èƒ½é¢„æµ‹çš„ä¸–ç•Œâ€</strong>ã€‚ä»»ä½•é¢„æµ‹å˜é‡çš„çº¿æ€§ç»„åˆï¼ˆä¹Ÿå°±æ˜¯çº¿æ€§å›å½’æ¨¡å‹è®¡ç®—çš„ç»“æœï¼‰éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªä½äºè¿™ä¸ªå¹³é¢<em>æŸå¤„</em>çš„å‘é‡ã€‚
#### 3. The Actual Outcome Vector (y)å®é™…ç»“æœå‘é‡ (y)</li>
<li><strong>Concept:</strong> The red vector <strong>y</strong>
represents your actual, observed data. Itâ€™s a single point in the
n-dimensional space. çº¢è‰²å‘é‡ <strong>y</strong>
ä»£è¡¨ä½ å®é™…è§‚å¯Ÿåˆ°çš„æ•°æ®ã€‚å®ƒæ˜¯ n ç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ã€‚</li>
<li><strong>Meaning:</strong> Critically, this vector usually does
<strong>not</strong> lie on the predictor hyperplane. If it did, your
model would be a perfect fit with zero error. The fact that itâ€™s â€œoff
the planeâ€ represents the real-world noise and variation that the model
cannot fully capture.
è‡³å…³é‡è¦çš„æ˜¯ï¼Œè¿™ä¸ªå‘é‡é€šå¸¸<strong>ä¸</strong>ä½äºé¢„æµ‹å˜é‡è¶…å¹³é¢ä¸Šã€‚å¦‚æœå®ƒä½äºè¶…å¹³é¢ä¸Šï¼Œä½ çš„æ¨¡å‹å°†å®Œç¾æ‹Ÿåˆï¼Œè¯¯å·®ä¸ºé›¶ã€‚å®ƒâ€œåç¦»å¹³é¢â€ä»£è¡¨äº†æ¨¡å‹æ— æ³•å®Œå…¨æ•æ‰åˆ°çš„çœŸå®ä¸–ç•Œçš„å™ªå£°å’Œå˜åŒ–ã€‚</li>
</ul>
<h4 id="the-fitted-value-vector-Å·æ‹Ÿåˆå€¼å‘é‡-Å·">4. The Fitted Value
Vector (Å·)æ‹Ÿåˆå€¼å‘é‡ (Å·)</h4>
<ul>
<li><strong>Concept:</strong> Since <strong>y</strong> is not on the
plane, we find the point on the plane that is <strong>geometrically
closest</strong> to <strong>y</strong>. This closest point is found by
dropping a perpendicular line from <strong>y</strong> to the plane. The
point where it lands is the <strong>orthogonal projection</strong>,
labeled <strong>Å·</strong> (y-hat). ç”±äº <strong>y</strong>
ä¸åœ¨å¹³é¢ä¸Šï¼Œå› æ­¤æˆ‘ä»¬åœ¨å¹³é¢ä¸Šæ‰¾åˆ°ä¸ <strong>y</strong>
<strong>å‡ ä½•ä¸Šæœ€æ¥è¿‘çš„ç‚¹ã€‚è¿™ä¸ªæœ€æ¥è¿‘ç‚¹æ˜¯é€šè¿‡ä» </strong>y**
åˆ°å¹³é¢åšä¸€æ¡å‚ç›´çº¿æ‰¾åˆ°çš„ã€‚å‚ç›´çº¿æ‰€åœ¨çš„ç‚¹å°±æ˜¯<strong>æ­£äº¤æŠ•å½±</strong>ï¼Œæ ‡è®°ä¸º
<strong>Å·</strong> (y-hat)ã€‚</li>
<li><strong>Meaning:</strong> <strong>Å· is the vector of your modelâ€™s
fitted values.</strong> It is the â€œbestâ€ possible approximation of the
real data that can be created using the given predictors because it
minimizes the distance (and therefore the squared error) between the
actual data (<strong>y</strong>) and the modelâ€™s prediction. <strong>Å·
æ˜¯æ¨¡å‹æ‹Ÿåˆå€¼çš„å‘é‡ã€‚</strong>å®ƒæ˜¯ä½¿ç”¨ç»™å®šé¢„æµ‹å˜é‡å¯ä»¥åˆ›å»ºçš„å¯¹çœŸå®æ•°æ®çš„â€œæœ€ä½³â€è¿‘ä¼¼å€¼ï¼Œå› ä¸ºå®ƒæœ€å°åŒ–äº†å®é™…æ•°æ®
(<strong>y</strong>)
ä¸æ¨¡å‹é¢„æµ‹å€¼ä¹‹é—´çš„è·ç¦»ï¼ˆä»è€Œæœ€å°åŒ–äº†å¹³æ–¹è¯¯å·®ï¼‰ã€‚</li>
</ul>
<h4 id="the-residual-vector-the-dashed-lineæ®‹å·®å‘é‡è™šçº¿">5. The Residual
Vector (The Dashed Line)æ®‹å·®å‘é‡ï¼ˆè™šçº¿ï¼‰</h4>
<ul>
<li><strong>Concept:</strong> The dashed line connecting the tip of
<strong>y</strong> to the tip of <strong>Å·</strong> is the
<strong>residual vector</strong> (<span
class="math inline">\(\boldsymbol{\epsilon} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>). Its length is the shortest possible distance
from <strong>y</strong> to the hyperplane.
è¿æ¥<strong>y</strong>é¡¶ç‚¹å’Œ<strong>Å·</strong>é¡¶ç‚¹çš„è™šçº¿æ˜¯<strong>æ®‹å·®å‘é‡</strong>
(<span class="math inline">\(\boldsymbol{\epsilon} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>)ã€‚å…¶é•¿åº¦æ˜¯ä»<strong>y</strong>åˆ°è¶…å¹³é¢çš„æœ€çŸ­å¯èƒ½è·ç¦»ã€‚</li>
<li><strong>Meaning:</strong> This vector represents the
<strong>error</strong> of the modelâ€”the part of the actual data that is
left over after accounting for the predictors. The right-angle symbol
(â””) is the most important part of the diagram, as it shows this error is
<strong>orthogonal</strong> (perpendicular) to the prediction and to all
the predictors. This visualizes the core property that the modelâ€™s
errors are uncorrelated with the predictors.
è¿æ¥<strong>y</strong>é¡¶ç‚¹å’Œ<strong>Å·</strong>é¡¶ç‚¹çš„è™šçº¿æ˜¯<strong>æ®‹å·®å‘é‡</strong>
(<span class="math inline">\(\boldsymbol{\epsilon} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>)ã€‚å…¶é•¿åº¦æ˜¯ä»<strong>y</strong>åˆ°è¶…å¹³é¢çš„æœ€çŸ­å¯èƒ½è·ç¦»ã€‚</li>
</ul>
<h1 id="singular-value-decomposition-svd-å¥‡å¼‚å€¼åˆ†è§£-svd">9.Singular
Value Decomposition (SVD) å¥‡å¼‚å€¼åˆ†è§£ (SVD)</h1>
<p><img src="/imgs/5054C3/SVD1.png">
<img src="/imgs/5054C3/SVD2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>These slides delve into the more advanced linear algebra behind the
projection matrix (<strong>H</strong>), explaining its fundamental
properties and offering a new way to construct it using <strong>Singular
Value Decomposition (SVD)</strong>. æ¢è®¨äº†æŠ•å½±çŸ©é˜µ (<strong>H</strong>)
èƒŒåæ›´é«˜çº§çš„çº¿æ€§ä»£æ•°ï¼Œè§£é‡Šäº†å®ƒçš„åŸºæœ¬æ€§è´¨ï¼Œå¹¶æä¾›äº†ä¸€ç§ä½¿ç”¨<strong>å¥‡å¼‚å€¼åˆ†è§£
(SVD)</strong> æ„é€ å®ƒçš„æ–°æ–¹æ³•ã€‚</p>
<h3 id="summary-2">## Summary</h3>
<p>These slides show that the <strong>projection matrix (H)</strong>,
which is central to least squares, has two key mathematical properties:
itâ€™s <strong>symmetric</strong> and <strong>idempotent</strong>
(projecting twice is the same as projecting once). These properties
dictate that its eigenvalues must be either 1 or 0. Singular Value
Decomposition (SVD) of the data matrix <strong>X</strong> provides an
elegant and numerically stable way to express <strong>H</strong> as
<strong>UUáµ€</strong>, which makes these fundamental properties easier to
understand and prove. è¿™äº›å¹»ç¯ç‰‡å±•ç¤ºäº†<strong>æŠ•å½±çŸ©é˜µ
(H)</strong>ï¼ˆæœ€å°äºŒä¹˜æ³•çš„æ ¸å¿ƒï¼‰çš„ä¸¤ä¸ªå…³é”®æ•°å­¦æ€§è´¨ï¼š<strong>å¯¹ç§°</strong>å’Œ<strong>å¹‚ç­‰</strong>ï¼ˆæŠ•å½±ä¸¤æ¬¡ç­‰äºæŠ•å½±ä¸€æ¬¡ï¼‰ã€‚è¿™äº›æ€§è´¨å†³å®šäº†å®ƒçš„ç‰¹å¾å€¼å¿…é¡»ä¸º
1 æˆ– 0ã€‚æ•°æ®çŸ©é˜µ <strong>X</strong> çš„å¥‡å¼‚å€¼åˆ†è§£ (SVD)
æä¾›äº†ä¸€ç§ä¼˜é›…ä¸”æ•°å€¼ç¨³å®šçš„æ–¹å¼ï¼Œå°†<strong>H</strong> è¡¨ç¤ºä¸º
<strong>UUáµ€</strong>ï¼Œè¿™ä½¿å¾—è¿™äº›åŸºæœ¬æ€§è´¨æ›´å®¹æ˜“ç†è§£å’Œè¯æ˜ã€‚</p>
<h3 id="concepts-and-process-explained-in-detail-1">## Concepts and
Process Explained in Detail</h3>
<h4 id="singular-value-decomposition-svd">1. Singular Value
Decomposition (SVD)</h4>
<p>The first slide introduces SVD, a powerful method for factoring any
matrix.</p>
<ul>
<li><strong>Concept:</strong> SVD breaks down your data matrix
<strong>X</strong> into three simpler matrices: <strong>X =
UDVáµ€</strong>. Think of this as revealing the fundamental structure of
your data.SVD å°†æ•°æ®çŸ©é˜µ <strong>X</strong>
åˆ†è§£ä¸ºä¸‰ä¸ªæ›´ç®€å•çš„çŸ©é˜µï¼š<strong>X =
UDVáµ€</strong>ã€‚è¿™å¯ä»¥ç†è§£ä¸ºæ­ç¤ºæ•°æ®çš„åŸºæœ¬ç»“æ„ã€‚
<ul>
<li><strong>U:</strong> An <strong>orthogonal matrix</strong> whose
columns form a perfect, orthonormal basis for the space spanned by your
predictors (the column space of <strong>X</strong>). These columns
represent the principal directions of your dataâ€™s
space.ä¸€ä¸ª<strong>æ­£äº¤çŸ©é˜µ</strong>ï¼Œå…¶åˆ—æ„æˆé¢„æµ‹å˜é‡æ‰€å ç©ºé—´ï¼ˆ<strong>X</strong>
çš„åˆ—ç©ºé—´ï¼‰çš„å®Œç¾æ­£äº¤åŸºã€‚è¿™äº›åˆ—ä»£è¡¨æ•°æ®ç©ºé—´çš„ä¸»æ–¹å‘ã€‚</li>
<li><strong>D:</strong> A <strong>diagonal matrix</strong> containing
the â€œsingular values,â€ which measure the importance or magnitude of each
of these principal
directions.ä¸€ä¸ª<strong>å¯¹è§’çŸ©é˜µ</strong>ï¼ŒåŒ…å«â€œå¥‡å¼‚å€¼â€ï¼Œç”¨äºè¡¡é‡æ¯ä¸ªä¸»æ–¹å‘çš„é‡è¦æ€§æˆ–å¤§å°ã€‚</li>
<li><strong>V:</strong> Another <strong>orthogonal
matrix</strong>.å¦ä¸€ä¸ª<strong>æ­£äº¤çŸ©é˜µ</strong>ã€‚</li>
</ul></li>
<li><strong>Process (How SVD simplifies the Projection Matrix) SVD
å¦‚ä½•ç®€åŒ–æŠ•å½±çŸ©é˜µ:</strong> The main takeaway from this slide is the new,
simpler formula for the hat matrix: <span
class="math display">\[\mathbf{H} = \mathbf{UU}^T\]</span> This result
is derived by substituting <strong>X = UDVáµ€</strong> into the original,
more complex formula for <strong>H</strong>: <span
class="math display">\[\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\]</span> When you
perform this substitution and use the fact that for orthogonal matrices
<strong>U</strong> and <strong>V</strong>, we have <strong>Uáµ€U =
I</strong> and <strong>Váµ€V = I</strong>, the <strong>D</strong> and
<strong>V</strong> matrices completely cancel out, leaving the
beautifully simple form <strong>H = UUáµ€</strong>. This tells us that
projection is fundamentally about the basis vectors (<strong>U</strong>)
of the predictor space. æ‰§è¡Œæ­¤ä»£å…¥å¹¶åˆ©ç”¨æ­£äº¤çŸ©é˜µ <strong>U</strong> å’Œ
<strong>V</strong> çš„å…¬å¼ï¼Œå³ <strong>Uáµ€U = I</strong> å’Œ <strong>Váµ€V =
I</strong>ï¼Œ<strong>D</strong> å’Œ <strong>V</strong>
çŸ©é˜µå®Œå…¨æŠµæ¶ˆï¼Œå‰©ä¸‹ç®€æ´çš„å½¢å¼ <strong>H =
UUáµ€</strong>ã€‚è¿™å‘Šè¯‰æˆ‘ä»¬ï¼ŒæŠ•å½±æœ¬è´¨ä¸Šæ˜¯å…³äºé¢„æµ‹ç©ºé—´çš„åŸºå‘é‡ï¼ˆ<strong>U</strong>ï¼‰çš„ã€‚</li>
</ul>
<h4 id="the-properties-of-the-projection-matrix-h-æŠ•å½±çŸ©é˜µ-h-çš„æ€§è´¨">2.
The Properties of the Projection Matrix (H) æŠ•å½±çŸ©é˜µ (H) çš„æ€§è´¨</h4>
<p>The second slide describes the essential nature of any projection
matrix.</p>
<ul>
<li><p><strong>Symmetric (H = Háµ€):</strong> This property ensures that
the projection is orthogonal (i.e., it finds the closest point by moving
perpendicularly).
æ­¤æ€§è´¨ç¡®ä¿æŠ•å½±æ˜¯æ­£äº¤çš„ï¼ˆå³ï¼Œå®ƒé€šè¿‡å‚ç›´ç§»åŠ¨æ‰¾åˆ°æœ€è¿‘çš„ç‚¹ï¼‰ã€‚</p></li>
<li><p><strong>Idempotent (HÂ² = H):</strong> This is the most intuitive
property of a projection. è¿™æ˜¯æŠ•å½±æœ€ç›´è§‚çš„æ€§è´¨ã€‚</p>
<ul>
<li><strong>Concept:</strong> â€œDoing it twice is the same as doing it
once.â€ â€œä¸¤æ¬¡å’Œä¸€æ¬¡ç›¸åŒã€‚â€</li>
<li><strong>Geometric Meaning:</strong> Imagine you project a point onto
a flat tabletop. That projected point is now <em>on the table</em>. If
you try to project it onto the table <em>again</em>, it doesnâ€™t move.
The projection of a projection is just the projection itself.
Mathematically, this is <strong>H(Hv) = Hv</strong>, which simplifies to
<strong>HÂ² = H</strong>.
æƒ³è±¡ä¸€ä¸‹ï¼Œä½ å°†ä¸€ä¸ªç‚¹æŠ•å½±åˆ°å¹³å¦çš„æ¡Œé¢ä¸Šã€‚è¿™ä¸ªæŠ•å½±ç‚¹ç°åœ¨<em>åœ¨æ¡Œå­ä¸Š</em>ã€‚å¦‚æœä½ å°è¯•<em>å†æ¬¡</em>å°†å®ƒæŠ•å½±åˆ°æ¡Œå­ä¸Šï¼Œå®ƒä¸ä¼šç§»åŠ¨ã€‚æŠ•å½±çš„æŠ•å½±å°±æ˜¯æŠ•å½±æœ¬èº«ã€‚ä»æ•°å­¦ä¸Šè®²ï¼Œè¿™æ˜¯<strong>H(Hv)
= Hv</strong>ï¼Œç®€åŒ–ä¸º<strong>HÂ² = H</strong>ã€‚</li>
</ul></li>
</ul>
<h4 id="eigenvalues-and-eigenspaces-ç‰¹å¾å€¼å’Œç‰¹å¾ç©ºé—´">3. Eigenvalues and
Eigenspaces ç‰¹å¾å€¼å’Œç‰¹å¾ç©ºé—´</h4>
<p>The idempotency property has a profound consequence for the matrixâ€™s
eigenvalues.</p>
<ul>
<li><strong>Concept:</strong> The eigenvalues of <strong>H</strong> can
only be <strong>1 or 0</strong>.
<strong>H</strong>çš„ç‰¹å¾å€¼åªèƒ½æ˜¯<strong>1</strong>æˆ–0**ã€‚</li>
<li><strong>Process (The Proof):</strong>
<ol type="1">
<li>Let <strong>v</strong> be an eigenvector of <strong>H</strong> with
eigenvalue <span class="math inline">\(\lambda\)</span>. By definition,
<strong>Hv</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>.
è®¾<strong>v</strong>æ˜¯<strong>H</strong>çš„ç‰¹å¾å‘é‡ï¼Œå…¶ç‰¹å¾å€¼ä¸º<span
class="math inline">\(\lambda\)</span>ã€‚æ ¹æ®å®šä¹‰ï¼Œ<strong>Hv</strong> =
<span class="math inline">\(\lambda\)</span><strong>v</strong>ã€‚</li>
<li>If we apply <strong>H</strong> again, we get <strong>H(Hv)</strong>
= <strong>H</strong>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda\)</span>(<strong>Hv</strong>) = <span
class="math inline">\(\lambda\)</span>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>.
å¦‚æœæˆ‘ä»¬å†æ¬¡åº”ç”¨<strong>H</strong>ï¼Œæˆ‘ä»¬å¾—åˆ°<strong>H(Hv)</strong> =
<strong>H</strong>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda\)</span>(<strong>Hv</strong>) = <span
class="math inline">\(\lambda\)</span>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>ã€‚</li>
<li>So, we have <strong>HÂ²v</strong> = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>.
å› æ­¤ï¼Œæˆ‘ä»¬æœ‰<strong>HÂ²v</strong> = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>ã€‚</li>
<li>But since <strong>H</strong> is idempotent, <strong>HÂ² = H</strong>,
which means <strong>HÂ²v = Hv</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>.
ä½†ç”±äº<strong>H</strong>æ˜¯å¹‚ç­‰çš„ï¼Œ<strong>HÂ² =
H</strong>ï¼Œè¿™æ„å‘³ç€<strong>HÂ²v = Hv</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>ã€‚</li>
<li>Therefore, we must have <span
class="math inline">\(\lambda^2\)</span><strong>v</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>, which means
<span class="math inline">\(\lambda^2 = \lambda\)</span>. The only two
numbers in existence that satisfy this equation are <strong>0 and
1</strong>. å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»æœ‰<span
class="math inline">\(\lambda^2\)</span><strong>v</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>ï¼Œè¿™æ„å‘³ç€<span
class="math inline">\(\lambda^2 =
\lambda\)</span>ã€‚æ»¡è¶³æ­¤ç­‰å¼çš„ä»…æœ‰ä¸¤ä¸ªæ•°å­—æ˜¯<strong>0</strong>å’Œ<strong>1</strong>ã€‚</li>
</ol></li>
<li><strong>Connecting Eigenvalues to the Model
å°†ç‰¹å¾å€¼è¿æ¥åˆ°æ¨¡å‹:</strong>
<ul>
<li><p><strong>Eigenvalue = 1:</strong> The eigenvectors associated with
an eigenvalue of 1 are the vectors that <strong>do not change</strong>
when projected. This is only possible if they were already in the space
being projected onto. Therefore, the space <code>Lâ‚</code> is the
<strong>column space of X</strong>â€”the â€œmodel space.â€ <strong>H</strong>
is the projection onto this space. <strong>ä¸ç‰¹å¾å€¼ä¸º 1
ç›¸å…³è”çš„ç‰¹å¾å‘é‡æ˜¯æŠ•å½±æ—¶</strong>ä¸ä¼šæ”¹å˜<strong>çš„å‘é‡ã€‚åªæœ‰å½“å®ƒä»¬å·²ç»å­˜åœ¨äºæŠ•å½±åˆ°çš„ç©ºé—´ä¸­æ—¶ï¼Œè¿™ç§æƒ…å†µæ‰æœ‰å¯èƒ½å‘ç”Ÿã€‚å› æ­¤ï¼Œç©ºé—´
<code>Lâ‚</code> æ˜¯ X çš„</strong>åˆ—ç©ºé—´<strong>â€”â€”â€œæ¨¡å‹ç©ºé—´â€ã€‚</strong>H**
æ˜¯åˆ°è¯¥ç©ºé—´çš„æŠ•å½±ã€‚</p></li>
<li><p><strong>Eigenvalue = 0:</strong> The eigenvectors associated with
an eigenvalue of 0 are the vectors that get sent to the zero vector when
projected. This happens to vectors that are <strong>orthogonal</strong>
to the projection space. Therefore, the space <code>Lâ‚€</code> is the
<strong>orthogonal â€œerrorâ€ space</strong>. The matrix <strong>I -
H</strong> is the projection onto this space.</p></li>
</ul>
<strong>ä¸ç‰¹å¾å€¼ä¸º 0
ç›¸å…³è”çš„ç‰¹å¾å‘é‡æ˜¯æŠ•å½±æ—¶è¢«å‘é€åˆ°é›¶å‘é‡çš„å‘é‡ã€‚è¿™ç§æƒ…å†µå‘ç”Ÿåœ¨ä¸æŠ•å½±ç©ºé—´</strong>æ­£äº¤<strong>çš„å‘é‡ä¸Šã€‚å› æ­¤ï¼Œç©ºé—´
<code>Lâ‚€</code> æ˜¯</strong>æ­£äº¤â€œè¯¯å·®â€ç©ºé—´<strong>ã€‚çŸ©é˜µ </strong>I - H**
æ˜¯åˆ°è¯¥ç©ºé—´çš„æŠ•å½±ã€‚</li>
</ul>
<h1 id="statistical-inference-1">10.statistical inference</h1>
<p><img src="/imgs/5054C3/statistical_inference_in_linear_regression1.png">
<img src="/imgs/5054C3/statistical_inference_in_linear_regression2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>These slides cover the theoretical backbone of statistical inference
in linear regression. They explain the necessary assumptions and the
resulting probability distributions of our estimates, which is what
allows us to perform hypothesis tests and create confidence
intervals.</p>
<p>è¿™äº›å¹»ç¯ç‰‡æ¶µç›–äº†çº¿æ€§å›å½’ä¸­ç»Ÿè®¡æ¨æ–­çš„ç†è®ºåŸºç¡€ã€‚å®ƒä»¬è§£é‡Šäº†å¿…è¦çš„å‡è®¾ä»¥åŠç”±æ­¤å¾—å‡ºçš„ä¼°è®¡æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¿›è¡Œå‡è®¾æ£€éªŒå¹¶åˆ›å»ºç½®ä¿¡åŒºé—´ã€‚</p>
<h3 id="summary-3">## Summary</h3>
<p>These slides lay out the statistical assumptions required for the
Least Squares Estimator (LSE). The core idea is that if we assume the
errors are independent and normally distributed, we can then prove that:
è¿™äº›å¹»ç¯ç‰‡åˆ—å‡ºäº†æœ€å°äºŒä¹˜ä¼°è®¡é‡ (LSE)
æ‰€éœ€çš„ç»Ÿè®¡å‡è®¾ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å‡è®¾è¯¯å·®æ˜¯ç‹¬ç«‹çš„ä¸”æœä»æ­£æ€åˆ†å¸ƒï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è¯æ˜ï¼š</p>
<ol type="1">
<li><p>Our estimated coefficients (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) also follow a
<strong>Normal distribution</strong> (or a
<strong>t-distribution</strong> when standardized). æˆ‘ä»¬çš„ä¼°è®¡ç³»æ•°
(<span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>)
ä¹Ÿæœä»<strong>æ­£æ€åˆ†å¸ƒ</strong>ï¼ˆæ ‡å‡†åŒ–åæœä»<strong>t
åˆ†å¸ƒ</strong>ï¼‰ã€‚</p></li>
<li><p>Our summed-up squared errors (RSS) follow a <strong>Chi-squared
distribution</strong>. æˆ‘ä»¬çš„å¹³æ–¹è¯¯å·®æ€»å’Œ (RSS)
æœä»<strong>å¡æ–¹åˆ†å¸ƒ</strong>ã€‚</p></li>
<li><p>A specific ratio of the explained variance to the unexplained
variance follows an <strong>F-distribution</strong>, which is used to
test the overall significance of the model.
è§£é‡Šæ–¹å·®ä¸æœªè§£é‡Šæ–¹å·®çš„ç‰¹å®šæ¯”ç‡æœä»<strong>F
åˆ†å¸ƒ</strong>ï¼Œè¯¥åˆ†å¸ƒç”¨äºæ£€éªŒæ¨¡å‹çš„æ•´ä½“æ˜¾è‘—æ€§ã€‚</p></li>
</ol>
<p>These known distributions are the foundation for all statistical
inference in linear
models.è¿™äº›å·²çŸ¥çš„åˆ†å¸ƒæ˜¯çº¿æ€§æ¨¡å‹ä¸­æ‰€æœ‰ç»Ÿè®¡æ¨æ–­çš„åŸºç¡€ã€‚</p>
<h3 id="deeper-dive-into-concepts-and-processes">## Deeper Dive into
Concepts and Processes</h3>
<h4 id="the-model-assumptions-the-foundation-æ¨¡å‹å‡è®¾åŸºç¡€">1. The Model
Assumptions (The Foundation) æ¨¡å‹å‡è®¾ï¼ˆåŸºç¡€ï¼‰</h4>
<p>The first slide states the two assumptions that are critical for
everything that follows. Without them, we canâ€™t make claims about the
statistical properties of our estimates.
ç¬¬ä¸€å¼ å¹»ç¯ç‰‡é˜è¿°äº†å¯¹åç»­æ‰€æœ‰å†…å®¹éƒ½è‡³å…³é‡è¦çš„ä¸¤ä¸ªå‡è®¾ã€‚æ²¡æœ‰å®ƒä»¬ï¼Œæˆ‘ä»¬å°±æ— æ³•æ–­è¨€ä¼°è®¡å€¼çš„ç»Ÿè®¡ç‰¹æ€§ã€‚</p>
<ul>
<li><strong>Assumption 1: <span class="math inline">\(\epsilon_i \sim
N(0, \sigma^2)\)</span></strong>
<ul>
<li><strong>Concept:</strong> This assumes that the error terms (the
part of <code>y</code> that the model canâ€™t explain) are drawn from a
normal (bell-curve) distribution with a mean of zero and a constant
variance <span class="math inline">\(\sigma^2\)</span>.
**å‡è®¾è¯¯å·®é¡¹ï¼ˆæ¨¡å‹æ— æ³•è§£é‡Šçš„ y
å€¼éƒ¨åˆ†ï¼‰æœä»æ­£æ€ï¼ˆé’Ÿå½¢æ›²çº¿ï¼‰åˆ†å¸ƒï¼Œè¯¥åˆ†å¸ƒçš„å‡å€¼ä¸ºé›¶ï¼Œæ–¹å·®ä¸ºå¸¸æ•° <span
class="math inline">\(\sigma^2\)</span>ã€‚</li>
<li><strong>Meaning in Plain English:</strong>
<ul>
<li><strong>Mean of 0:</strong> The model is â€œcorrect on average.â€ The
errors are not systematically positive or negative.
**æ¨¡å‹â€œå¹³å‡æ­£ç¡®â€ã€‚è¯¯å·®å¹¶éç³»ç»Ÿåœ°ä¸ºæ­£æˆ–è´Ÿã€‚</li>
<li><strong>Normal Distribution:</strong> Small errors are more likely
than large errors. This is a common assumption for random noise.
**å°è¯¯å·®æ¯”å¤§è¯¯å·®æ›´æœ‰å¯èƒ½å‡ºç°ã€‚è¿™æ˜¯éšæœºå™ªå£°çš„å¸¸è§å‡è®¾ã€‚</li>
<li><strong>Constant Variance (<span
class="math inline">\(\sigma^2\)</span>):</strong> The amount of random
scatter around the regression line is the same at all levels of the
predictor variables. This is called <strong>homoscedasticity</strong>.
å›å½’çº¿å‘¨å›´çš„éšæœºæ•£åº¦åœ¨é¢„æµ‹å˜é‡çš„å„ä¸ªæ°´å¹³ä¸Šéƒ½æ˜¯ç›¸åŒçš„ã€‚è¿™è¢«ç§°ä¸º<strong>åŒæ–¹å·®æ€§</strong>ã€‚</li>
</ul></li>
</ul></li>
<li><strong>Assumption 2: Observations are independent</strong>
è§‚æµ‹å€¼æ˜¯ç‹¬ç«‹çš„**
<ul>
<li><strong>Concept:</strong> Each data point <span
class="math inline">\((x_i, y_i)\)</span> is an independent piece of
information. The value of the error for one observation gives no
information about the error for another observation. æ¯ä¸ªæ•°æ®ç‚¹ <span
class="math inline">\((x_i, y_i)\)</span>
éƒ½æ˜¯ä¸€æ¡ç‹¬ç«‹çš„ä¿¡æ¯ã€‚ä¸€ä¸ªè§‚æµ‹å€¼çš„è¯¯å·®å€¼å¹¶ä¸èƒ½åæ˜ å¦ä¸€ä¸ªè§‚æµ‹å€¼çš„è¯¯å·®ã€‚</li>
<li><strong>Meaning:</strong> This is often true for cross-sectional
data (e.g., a random sample of people) but can be violated in
time-series data where todayâ€™s error might be correlated with
yesterdayâ€™s.
è¿™é€šå¸¸é€‚ç”¨äºæ¨ªæˆªé¢æ•°æ®ï¼ˆä¾‹å¦‚ï¼ŒéšæœºæŠ½æ ·çš„äººç¾¤ï¼‰ï¼Œä½†åœ¨æ—¶é—´åºåˆ—æ•°æ®ä¸­å¯èƒ½ä¸æˆç«‹ï¼Œå› ä¸ºä»Šå¤©çš„è¯¯å·®å¯èƒ½ä¸æ˜¨å¤©çš„è¯¯å·®ç›¸å…³ã€‚</li>
</ul></li>
</ul>
<h4
id="the-distribution-of-the-coefficients-theorem-of-lse-ç³»æ•°åˆ†å¸ƒæœ€å°äºŒä¹˜æ³•å®šç†">2.
The Distribution of the Coefficients (Theorem of LSE)
ç³»æ•°åˆ†å¸ƒï¼ˆæœ€å°äºŒä¹˜æ³•å®šç†ï¼‰</h4>
<p>This is the most important result for understanding the accuracy of
our individual predictors.</p>
<ul>
<li><strong>Concept 1: The Sampling Distribution of <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></strong>
<ul>
<li><p><strong>Formula:</strong> <span
class="math inline">\(\hat{\boldsymbol{\beta}} \sim
N(\boldsymbol{\beta},
\sigma^2(\mathbf{X}^T\mathbf{X})^{-1})\)</span></p></li>
<li><p><strong>Meaning:</strong> If you were to take many different
random samples from the population and calculate the coefficients <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> for each sample,
the distribution of those coefficients would be a multivariate normal
distribution. **å¦‚æœä»æ€»ä½“ä¸­éšæœºæŠ½å–è®¸å¤šä¸åŒçš„æ ·æœ¬ï¼Œå¹¶è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç³»æ•°
<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>ï¼Œåˆ™è¿™äº›ç³»æ•°çš„åˆ†å¸ƒå°†æœä»å¤šå…ƒæ­£æ€åˆ†å¸ƒã€‚</p>
<ul>
<li>The center of this distribution is the <strong>true population
coefficient vector <span
class="math inline">\(\boldsymbol{\beta}\)</span></strong>. This means
our estimator is <strong>unbiased</strong>â€”on average, it finds the
right answer. è¯¥åˆ†å¸ƒçš„ä¸­å¿ƒæ˜¯<strong>çœŸå®çš„æ€»ä½“ç³»æ•°å‘é‡ <span
class="math inline">\(\boldsymbol{\beta}\)</span></strong>ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬çš„ä¼°è®¡å™¨æ˜¯<strong>æ— åçš„</strong>â€”â€”å¹³å‡è€Œè¨€ï¼Œå®ƒèƒ½å¤Ÿæ‰¾åˆ°æ­£ç¡®çš„ç­”æ¡ˆã€‚</li>
<li>The â€œspreadâ€ of this distribution is its variance-covariance matrix,
<span
class="math inline">\(\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\)</span>.
This tells us the uncertainty in our estimates.
è¯¥åˆ†å¸ƒçš„â€œæ•£åº¦â€æ˜¯å…¶æ–¹å·®-åæ–¹å·®çŸ©é˜µ</li>
</ul></li>
</ul></li>
<li><strong>Concept 2: The t-statistic</strong> t ç»Ÿè®¡é‡
<ul>
<li><strong>Formula:</strong> The standardized coefficient, <span
class="math inline">\(\frac{\hat{\beta}_j -
\beta_j}{\text{s.e.}(\hat{\beta}_j)}\)</span>, follows a
<strong>t-distribution</strong> with <strong><span
class="math inline">\(n-p-1\)</span> degrees of freedom</strong>.</li>
<li><strong>Process &amp; Meaning:</strong> In the real world, we donâ€™t
know the true error variance <span
class="math inline">\(\sigma^2\)</span>. We have to estimate it using
our sample data, which gives us <span
class="math inline">\(s^2\)</span>. Because we are using an
<em>estimate</em> of the variance, we introduce extra uncertainty. The
t-distribution is like a normal distribution but with slightly â€œfatterâ€
tails to account for this additional uncertainty. The degrees of
freedom, <span class="math inline">\(n-p-1\)</span>, reflect the number
of data points (<code>n</code>) minus the number of parameters we had to
estimate (<code>p</code> slopes + 1 intercept). This is the basis for
t-tests and confidence intervals for each coefficient.
åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œæˆ‘ä»¬ä¸çŸ¥é“çœŸå®çš„è¯¯å·®æ–¹å·® <span
class="math inline">\(\sigma^2\)</span>ã€‚æˆ‘ä»¬å¿…é¡»ä½¿ç”¨æ ·æœ¬æ•°æ®æ¥ä¼°è®¡å®ƒï¼Œä»è€Œå¾—åˆ°
<span
class="math inline">\(s^2\)</span>ã€‚ç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯æ–¹å·®çš„<em>ä¼°è®¡å€¼</em>ï¼Œå› æ­¤å¼•å…¥äº†é¢å¤–çš„ä¸ç¡®å®šæ€§ã€‚
t åˆ†å¸ƒç±»ä¼¼äºæ­£æ€åˆ†å¸ƒï¼Œä½†å°¾éƒ¨ç•¥å¾®â€œä¸°æ»¡â€ï¼Œä»¥è§£é‡Šè¿™ç§é¢å¤–çš„ä¸ç¡®å®šæ€§ã€‚è‡ªç”±åº¦
<span class="math inline">\(n-p-1\)</span>
è¡¨ç¤ºæ•°æ®ç‚¹çš„æ•°é‡ï¼ˆ<code>n</code>ï¼‰å‡å»æˆ‘ä»¬éœ€è¦ä¼°è®¡çš„å‚æ•°æ•°é‡ï¼ˆ<code>p</code>
ä¸ªæ–œç‡ + 1 ä¸ªæˆªè·ï¼‰ã€‚è¿™æ˜¯ t æ£€éªŒå’Œæ¯ä¸ªç³»æ•°ç½®ä¿¡åŒºé—´çš„åŸºç¡€ã€‚</li>
</ul></li>
</ul>
<h4
id="the-distribution-of-the-error-theorem-of-residual-è¯¯å·®åˆ†å¸ƒæ®‹å·®å®šç†">3.
The Distribution of the Error (Theorem of Residual)
è¯¯å·®åˆ†å¸ƒï¼ˆæ®‹å·®å®šç†ï¼‰</h4>
<p>This theorem helps us understand the properties of our modelâ€™s
overall error.</p>
<ul>
<li><p><strong>Concept:</strong> The <strong>Residual Sum of Squares
(RSS)</strong>, when scaled by the true variance, follows a
<strong>Chi-squared (<span class="math inline">\(\chi^2\)</span>)
distribution</strong> with <span class="math inline">\(n-p-1\)</span>
degrees of freedom. <strong>æ®‹å·®å¹³æ–¹å’Œ (RSS)</strong>
ç»çœŸå®æ–¹å·®ç¼©æ”¾åï¼Œæœä»è‡ªç”±åº¦ä¸º <span
class="math inline">\(n-p-1\)</span> çš„<strong>å¡æ–¹ (<span
class="math inline">\(\chi^2\)</span>) åˆ†å¸ƒ</strong>ã€‚</p></li>
<li><p><strong>Process &amp; Meaning:</strong> The Chi-squared
distribution often arises when dealing with sums of squared normal
variables. This theorem provides a formal probability distribution for
our total model error. Its most important consequence is that it allows
us to prove that:
**å¡æ–¹åˆ†å¸ƒé€šå¸¸ç”¨äºå¤„ç†æ­£æ€å˜é‡çš„å¹³æ–¹å’Œã€‚è¯¥å®šç†ä¸ºæˆ‘ä»¬æ¨¡å‹çš„æ€»ä½“è¯¯å·®æä¾›äº†ä¸€ä¸ªæ­£å¼çš„æ¦‚ç‡åˆ†å¸ƒã€‚å®ƒæœ€é‡è¦çš„æ¨è®ºæ˜¯ï¼Œå®ƒä½¿æˆ‘ä»¬èƒ½å¤Ÿè¯æ˜ï¼š</p>
<p><span class="math display">\[s^2 = \text{RSS}/(n - p - 1)\]</span> is
an <strong>unbiased estimate</strong> of the true error variance <span
class="math inline">\(\sigma^2\)</span>. This <span
class="math inline">\(s^2\)</span> is a critical ingredient for
calculating the standard errors of our coefficients. <span
class="math display">\[s^2 = \text{RSS}/(n - p - 1)\]</span>
æ˜¯çœŸå®è¯¯å·®æ–¹å·® <span class="math inline">\(\sigma^2\)</span>
çš„<strong>æ— åä¼°è®¡</strong>ã€‚è¿™ä¸ª <span
class="math inline">\(s^2\)</span>
æ˜¯è®¡ç®—ç³»æ•°æ ‡å‡†è¯¯å·®çš„å…³é”®å› ç´ ã€‚</p></li>
</ul>
<h4 id="the-f-distribution-and-the-overall-model-test">4. The
F-Distribution and the Overall Model Test</h4>
<p>This final theorem combines our findings about the coefficients and
the residuals.</p>
<ul>
<li><p><strong>Concept:</strong> The F-statistic, which is essentially a
ratio of the variance explained by the model to the variance left
unexplained, follows an <strong>F-distribution</strong>. F
ç»Ÿè®¡é‡æœ¬è´¨ä¸Šæ˜¯æ¨¡å‹è§£é‡Šçš„æ–¹å·®ä¸æœªè§£é‡Šæ–¹å·®çš„æ¯”ç‡ï¼Œæœä» F åˆ†å¸ƒã€‚</p></li>
<li><p><strong>Process &amp; Meaning:</strong> This result relies on the
fact that our coefficient estimates (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) are independent
of our total error (RSS). The F-distribution is used for the
<strong>F-test of overall significance</strong>. This test checks the
null hypothesis that <em>all</em> of your slope coefficients are
simultaneously zero (<span class="math inline">\(\beta_1 = \beta_2 =
\dots = \beta_p = 0\)</span>). If the F-test gives a small p-value, you
can conclude that your model, as a whole, is statistically significant
and provides a better fit than a model with no predictors. å¦‚æœ F
æ£€éªŒå¾—å‡ºçš„ p
å€¼è¾ƒå°ï¼Œåˆ™å¯ä»¥å¾—å‡ºç»“è®ºï¼Œæ‚¨çš„æ¨¡å‹æ•´ä½“ä¸Šå…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ï¼Œå¹¶ä¸”æ¯”æ²¡æœ‰é¢„æµ‹å› å­çš„æ¨¡å‹æ‹Ÿåˆæ•ˆæœæ›´å¥½ã€‚</p></li>
</ul>
<h1 id="construct-different-types-of-intervals">11.construct different
types of intervals</h1>
<p><img src="/imgs/5054C3/construct_different_types_of_intervals1.png">
<img src="/imgs/5054C3/construct_different_types_of_intervals2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>These slides explain how to use the statistical properties of the
least squares estimates to construct different types of intervals, which
are essential for quantifying the uncertainty in your modelâ€™s
predictions and parameters.
è¿™äº›å¹»ç¯ç‰‡è§£é‡Šäº†å¦‚ä½•åˆ©ç”¨æœ€å°äºŒä¹˜ä¼°è®¡çš„ç»Ÿè®¡ç‰¹æ€§æ¥æ„å»ºä¸åŒç±»å‹çš„åŒºé—´ï¼Œè¿™å¯¹äºé‡åŒ–æ¨¡å‹é¢„æµ‹å’Œå‚æ•°ä¸­çš„ä¸ç¡®å®šæ€§è‡³å…³é‡è¦ã€‚</p>
<h3 id="summary-4">Summary</h3>
<p>These slides show how to calculate three distinct types of intervals
in linear regression, each answering a different question about
uncertainty:
å±•ç¤ºäº†å¦‚ä½•è®¡ç®—çº¿æ€§å›å½’ä¸­ä¸‰ç§ä¸åŒç±»å‹çš„åŒºé—´ï¼Œæ¯ç§åŒºé—´åˆ†åˆ«å›ç­”äº†å…³äºä¸ç¡®å®šæ€§çš„ä¸åŒé—®é¢˜ï¼š</p>
<ol type="1">
<li><strong>Confidence Interval for a Parameter (<span
class="math inline">\(\beta_j\)</span>):</strong> Provides a plausible
range for a single, true unknown coefficient in the model.
ä¸ºæ¨¡å‹ä¸­å•ä¸ªçœŸå®æœªçŸ¥ç³»æ•°æä¾›ä¸€ä¸ªåˆç†çš„èŒƒå›´ã€‚</li>
<li><strong>Confidence Interval for the Mean Response:</strong> Provides
a plausible range for the <em>average</em> outcome for a given set of
predictor values.
ä¸ºç»™å®šä¸€ç»„é¢„æµ‹å˜é‡å€¼çš„<em>å¹³å‡</em>ç»“æœæä¾›ä¸€ä¸ªåˆç†çš„èŒƒå›´ã€‚</li>
<li><strong>Prediction Interval:</strong> Provides a plausible range for
a <em>single future</em> outcome for a given set of predictor values.
This interval is always wider than the confidence interval for the mean
response because it must also account for individual random error.
ä¸ºç»™å®šä¸€ç»„é¢„æµ‹å˜é‡å€¼çš„<em>å•ä¸ªæœªæ¥</em>ç»“æœæä¾›ä¸€ä¸ªåˆç†çš„èŒƒå›´ã€‚è¯¥åŒºé—´å§‹ç»ˆæ¯”å¹³å‡å“åº”çš„ç½®ä¿¡åŒºé—´æ›´å®½ï¼Œå› ä¸ºå®ƒè¿˜å¿…é¡»è€ƒè™‘å•ä¸ªéšæœºè¯¯å·®ã€‚</li>
</ol>
<h3 id="deeper-dive-into-concepts-and-processes-1">Deeper Dive into
Concepts and Processes</h3>
<h4
id="confidence-interval-for-a-single-parameter-å•ä¸ªå‚æ•°çš„ç½®ä¿¡åŒºé—´">1.
Confidence Interval for a Single Parameter å•ä¸ªå‚æ•°çš„ç½®ä¿¡åŒºé—´</h4>
<p>This interval addresses the uncertainty around one specific
coefficient, like the slope for your most important predictor.
æ­¤åŒºé—´ç”¨äºè§£å†³å›´ç»•æŸä¸ªç‰¹å®šç³»æ•°çš„ä¸ç¡®å®šæ€§ï¼Œä¾‹å¦‚æœ€é‡è¦çš„é¢„æµ‹å› å­çš„æ–œç‡ã€‚</p>
<ul>
<li><strong>The Question It Answers:</strong> â€œIâ€™ve calculated a slope
of <span class="math inline">\(\hat{\beta}_1 = 10.5\)</span>. How sure
am I about this number? What is a plausible range for the <em>true</em>
population slope?â€ æˆ‘è®¡ç®—å‡ºäº†æ–œç‡ä¸º <span
class="math inline">\(\hat{\beta}_1 =
10.5\)</span>ã€‚æˆ‘å¯¹è¿™ä¸ªæ•°å­—æœ‰å¤šç¡®å®šï¼Ÿ<em>çœŸå®</em>æ€»ä½“æ–œç‡çš„åˆç†èŒƒå›´æ˜¯å¤šå°‘ï¼Ÿâ€</li>
<li><strong>The Formula:</strong> <span
class="math inline">\(\hat{\beta}_j \pm t_{n-p-1}(\alpha/2) s
\sqrt{c_{jj}}\)</span>
<ul>
<li><strong><span class="math inline">\(\hat{\beta}_j\)</span></strong>:
This is your best point estimate for the coefficient, taken directly
from the model output. è¿™æ˜¯è¯¥ç³»æ•°çš„æœ€ä½³ç‚¹ä¼°è®¡å€¼ï¼Œç›´æ¥å–è‡ªæ¨¡å‹è¾“å‡ºã€‚</li>
<li><strong><span
class="math inline">\(t_{n-p-1}(\alpha/2)\)</span></strong>: This is the
<strong>critical value</strong> from a t-distribution. Itâ€™s a multiplier
that sets the width of the interval based on your desired confidence
level (e.g., for 95% confidence, <span
class="math inline">\(\alpha=0.05\)</span>). è¿™æ˜¯ t
åˆ†å¸ƒçš„<strong>ä¸´ç•Œå€¼</strong>ã€‚å®ƒæ˜¯ä¸€ä¸ªä¹˜æ•°ï¼Œæ ¹æ®æ‚¨æ‰€éœ€çš„ç½®ä¿¡æ°´å¹³è®¾ç½®åŒºé—´å®½åº¦ï¼ˆä¾‹å¦‚ï¼Œå¯¹äº
95% çš„ç½®ä¿¡åº¦ï¼Œ<span class="math inline">\(\alpha=0.05\)</span>ï¼‰ã€‚</li>
<li><strong><span class="math inline">\(s
\sqrt{c_{jj}}\)</span></strong>: This whole term is the <strong>standard
error</strong> of the coefficient <span
class="math inline">\(\hat{\beta}_j\)</span>. It measures the precision
of your estimate. A smaller standard error means a narrower, more
precise interval. è¿™æ•´ä¸ªé¡¹æ˜¯ç³»æ•° <span
class="math inline">\(\hat{\beta}_j\)</span>
çš„<strong>æ ‡å‡†è¯¯å·®</strong>ã€‚å®ƒè¡¡é‡æ‚¨ä¼°è®¡çš„ç²¾åº¦ã€‚æ ‡å‡†è¯¯å·®è¶Šå°ï¼ŒåŒºé—´è¶Šçª„ï¼Œç²¾åº¦è¶Šé«˜ã€‚</li>
</ul></li>
</ul>
<h4 id="confidence-interval-for-the-mean-response-å¹³å‡å“åº”çš„ç½®ä¿¡åŒºé—´">2.
Confidence Interval for the Mean Response å¹³å‡å“åº”çš„ç½®ä¿¡åŒºé—´</h4>
<p>This interval addresses the uncertainty about the location of the
regression line itself. è¿™ä¸ªåŒºé—´è§£å†³äº†å›å½’çº¿æœ¬èº«ä½ç½®çš„ä¸ç¡®å®šæ€§ã€‚</p>
<ul>
<li><strong>The Question It Answers:</strong> â€œFor a house with 3
bedrooms and 2 bathrooms, what is the plausible range for the
<em>average</em> selling price of <em>all such houses</em>?â€
<strong>å®ƒå›ç­”çš„é—®é¢˜</strong>ï¼šâ€œå¯¹äºä¸€æ ‹æœ‰ 3 é—´å§å®¤å’Œ 2
é—´æµ´å®¤çš„æˆ¿å­ï¼Œ<em>æ‰€æœ‰æ­¤ç±»æˆ¿å±‹</em>çš„<em>å¹³å‡</em>å”®ä»·çš„åˆç†èŒƒå›´æ˜¯å¤šå°‘ï¼Ÿâ€</li>
<li><strong>The Formula:</strong> <span
class="math inline">\(\hat{\boldsymbol{\beta}}^T \mathbf{x} \pm
t_{n-p-1}(\alpha/2)s\sqrt{\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}}\)</span>
<ul>
<li><strong><span class="math inline">\(\hat{\boldsymbol{\beta}}^T
\mathbf{x}\)</span></strong>: This is your point prediction, <span
class="math inline">\(\hat{y}\)</span>, for the given input vector
<strong>x</strong>. è¿™æ˜¯ç»™å®šè¾“å…¥å‘é‡ <strong>x</strong> çš„ç‚¹é¢„æµ‹ <span
class="math inline">\(\hat{y}\)</span>ã€‚</li>
<li><strong><span
class="math inline">\(s\sqrt{\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}}\)</span></strong>:
This is the standard error of the mean response. Its value depends on
how far the input vector <strong>x</strong> is from the center of the
data. This means the confidence interval is narrowest near the average
of your data and gets wider as you move toward the extremes.
è¿™æ˜¯å¹³å‡å“åº”çš„æ ‡å‡†è¯¯å·®ã€‚å…¶å€¼å–å†³äºè¾“å…¥å‘é‡ <strong>x</strong>
è·ç¦»æ•°æ®ä¸­å¿ƒçš„è·ç¦»ã€‚è¿™æ„å‘³ç€ç½®ä¿¡åŒºé—´åœ¨æ•°æ®å¹³å‡å€¼é™„è¿‘æœ€çª„ï¼Œå¹¶ä¸”éšç€æ¥è¿‘æå€¼è€Œå˜å®½ã€‚</li>
</ul></li>
</ul>
<h4
id="prediction-interval-for-an-individual-response-å•ä¸ªå“åº”çš„é¢„æµ‹åŒºé—´">3.
Prediction Interval for an Individual Response å•ä¸ªå“åº”çš„é¢„æµ‹åŒºé—´</h4>
<p>This is the most comprehensive interval and is often the most useful
for making real-world predictions.
è¿™æ˜¯æœ€å…¨é¢çš„åŒºé—´ï¼Œé€šå¸¸å¯¹äºè¿›è¡Œå®é™…é¢„æµ‹æœ€æœ‰ç”¨ã€‚</p>
<ul>
<li><strong>The Question It Answers:</strong> â€œI want to predict the
selling price for <em>one specific house</em> that has 3 bedrooms and 2
bathrooms. What is a plausible price range for this <em>single
house</em>?â€</li>
<li><strong>The Formula:</strong> <span
class="math inline">\(\hat{\boldsymbol{\beta}}^T \mathbf{x} \pm
t_{n-p-1}(\alpha/2)s\sqrt{1 +
\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}}\)</span></li>
<li><strong>The Key Difference:</strong> Notice the formula is identical
to the one above, except for the <strong><code>1 + ...</code></strong>
inside the square root. This â€œ1â€ is critically important. It accounts
for the second source of uncertainty.
<strong>è¯·æ³¨æ„ï¼Œè¯¥å…¬å¼ä¸ä¸Šé¢çš„å…¬å¼å®Œå…¨ç›¸åŒï¼Œåªæ˜¯å¹³æ–¹æ ¹ä¸­çš„</strong><code>1 + ...</code>**ä¸åŒã€‚è¿™ä¸ªâ€œ1â€è‡³å…³é‡è¦ã€‚å®ƒè§£é‡Šäº†ç¬¬äºŒä¸ªä¸ç¡®å®šæ€§æ¥æºã€‚
<ol type="1">
<li><strong>Uncertainty in the model:</strong> We are not perfectly
certain about the true location of the regression line. This is captured
by the <span
class="math inline">\(\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}\)</span>
term. **æˆ‘ä»¬æ— æ³•å®Œå…¨ç¡®å®šå›å½’çº¿çš„çœŸå®ä½ç½®ã€‚è¿™å¯ä»¥é€šè¿‡ <span
class="math inline">\(\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}\)</span>
é¡¹æ¥æ•æ‰ã€‚</li>
<li><strong>Uncertainty in the individual data point:</strong> Even if
we knew the true regression line perfectly, individual data points would
still be scattered around it due to random error (<span
class="math inline">\(\epsilon\)</span>). The â€œ1â€ in the formula
accounts for this irreducible, random error of a single observation.
å³ä½¿æˆ‘ä»¬å®Œå…¨äº†è§£çœŸå®çš„å›å½’çº¿ï¼Œç”±äºéšæœºè¯¯å·® (<span
class="math inline">\(\epsilon\)</span>)ï¼Œå•ä¸ªæ•°æ®ç‚¹ä»ç„¶ä¼šæ•£å¸ƒåœ¨å…¶å‘¨å›´ã€‚å…¬å¼ä¸­çš„â€œ1â€è§£é‡Šäº†å•ä¸ªè§‚æµ‹å€¼ä¸­è¿™ç§ä¸å¯çº¦çš„éšæœºè¯¯å·®ã€‚</li>
</ol></li>
</ul>
<p>Because it accounts for both types of uncertainty, the
<strong>prediction interval is always wider than the confidence interval
for the mean</strong>.
ç”±äºå®ƒåŒæ—¶è€ƒè™‘äº†ä¸¤ç§ä¸ç¡®å®šæ€§ï¼Œå› æ­¤<strong>é¢„æµ‹åŒºé—´</strong>æ€»æ˜¯æ¯”å‡å€¼çš„ç½®ä¿¡åŒºé—´æ›´å®½ã€‚</p>
<h4 id="the-core-difference-an-analogy-ä¸€ä¸ªç±»æ¯”">The Core Difference: An
Analogy ä¸€ä¸ªç±»æ¯”</h4>
<ul>
<li><p><strong>Confidence Interval (Mean) å‡å€¼ç½®ä¿¡åŒºé—´:</strong> Like
predicting the <strong>average</strong> arrival time for a specific
flight that runs every day. After observing it for a year, you can
predict the average very accurately (e.g., 10:05 AM Â± 2 minutes).
å°±åƒé¢„æµ‹æ¯å¤©ç‰¹å®šèˆªç­çš„<strong>å¹³å‡</strong>åˆ°è¾¾æ—¶é—´ã€‚ç»è¿‡ä¸€å¹´çš„è§‚å¯Ÿï¼Œæ‚¨å¯ä»¥éå¸¸å‡†ç¡®åœ°é¢„æµ‹å¹³å‡å€¼ï¼ˆä¾‹å¦‚ï¼Œä¸Šåˆ
10:05 Â± 2 åˆ†é’Ÿï¼‰ã€‚</p></li>
<li><p><strong>Prediction Interval (Individual) ä¸ªä½“é¢„æµ‹åŒºé—´:</strong>
Like predicting the arrival time for that same flight on <strong>one
specific day</strong> next week. You have to account for the uncertainty
in the average <em>plus</em> the potential for random, one-time events
like weather or air traffic delays. Your prediction must be wider to be
safe (e.g., 10:05 AM Â± 15 minutes).
å°±åƒé¢„æµ‹åŒä¸€èˆªç­ä¸‹å‘¨<strong>æŸä¸€å¤©</strong>çš„åˆ°è¾¾æ—¶é—´ã€‚æ‚¨å¿…é¡»è€ƒè™‘å¹³å‡å€¼çš„ä¸ç¡®å®šæ€§ï¼Œä»¥åŠ*å¯èƒ½å‡ºç°çš„éšæœºã€ä¸€æ¬¡æ€§äº‹ä»¶ï¼Œä¾‹å¦‚å¤©æ°”æˆ–ç©ºä¸­äº¤é€šå»¶è¯¯ã€‚æ‚¨çš„é¢„æµ‹èŒƒå›´å¿…é¡»æ›´å¹¿æ‰èƒ½å®‰å…¨ï¼ˆä¾‹å¦‚ï¼Œä¸Šåˆ
10:05 Â± 15 åˆ†é’Ÿï¼‰ã€‚</p></li>
</ul>
<h1 id="construct-different-types-of-intervals-1">12.construct different
types of intervals</h1>
<p><img src="/imgs/5054C3/ANOVA1.png">
<img src="/imgs/5054C3/ANOVA2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<p>These slides explain <strong>Analysis of Variance (ANOVA)</strong>, a
method used in regression to break down the total variability in your
data to test if your model is statistically significant as a
whole.è¿™äº›å¹»ç¯ç‰‡è®²è§£äº†<strong>æ–¹å·®åˆ†æ
(ANOVA)</strong>ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå›å½’åˆ†æçš„æ–¹æ³•ï¼Œç”¨äºåˆ†è§£æ•°æ®ä¸­çš„æ€»å˜å¼‚æ€§ï¼Œä»¥æ£€éªŒæ¨¡å‹æ•´ä½“æ˜¯å¦å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ã€‚</p>
<h3 id="summary-5">Summary</h3>
<p>The core idea is to decompose the total variation in the response
variable (<strong>Total Sum of Squares, SS_total</strong>) into two
parts: the variation that is explained by your regression model
(<strong>Regression Sum of Squares, SS_reg</strong>) and the variation
that is left unexplained (<strong>Error Sum of Squares,
SS_error</strong>).
å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†å“åº”å˜é‡çš„æ€»å˜å¼‚ï¼ˆ<strong>æ€»å¹³æ–¹å’Œï¼ŒSS_total</strong>ï¼‰åˆ†è§£ä¸ºä¸¤éƒ¨åˆ†ï¼šå›å½’æ¨¡å‹å¯ä»¥è§£é‡Šçš„å˜å¼‚ï¼ˆ<strong>å›å½’å¹³æ–¹å’Œï¼ŒSS_reg</strong>ï¼‰å’Œæœªè§£é‡Šçš„å˜å¼‚ï¼ˆ<strong>è¯¯å·®å¹³æ–¹å’Œï¼ŒSS_error</strong>ï¼‰ã€‚</p>
<p>By comparing the size of the explained variation to the unexplained
variation using an <strong>F-statistic</strong>, we can formally test
the hypothesis that our model has predictive power. This entire process
is neatly organized in an <strong>ANOVA table</strong>.
é€šè¿‡ä½¿ç”¨<strong>F
ç»Ÿè®¡é‡</strong>æ¯”è¾ƒå·²è§£é‡Šå˜å¼‚ä¸æœªè§£é‡Šå˜å¼‚çš„å¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥æ­£å¼æ£€éªŒæ¨¡å‹å…·æœ‰é¢„æµ‹èƒ½åŠ›çš„å‡è®¾ã€‚æ•´ä¸ªè¿‡ç¨‹éƒ½æ•´é½åœ°ç»„ç»‡åœ¨<strong>æ–¹å·®åˆ†æè¡¨</strong>ä¸­ã€‚</p>
<h3 id="deeper-dive-into-concepts-and-connections">Deeper Dive into
Concepts and Connections</h3>
<h4
id="the-decomposition-of-variances-the-core-equation-æ–¹å·®åˆ†è§£æ ¸å¿ƒæ–¹ç¨‹">1.
The Decomposition of Variances (The Core Equation)
æ–¹å·®åˆ†è§£ï¼ˆæ ¸å¿ƒæ–¹ç¨‹ï¼‰</h4>
<p>The first slide starts with the fundamental equation of ANOVA, which
stems directly from the geometric properties of least squares:
ç¬¬ä¸€å¼ å¹»ç¯ç‰‡ä»¥æ–¹å·®åˆ†æçš„åŸºæœ¬æ–¹ç¨‹å¼€å¤´ï¼Œè¯¥æ–¹ç¨‹ç›´æ¥æºäºæœ€å°äºŒä¹˜çš„å‡ ä½•æ€§è´¨ï¼š</p>
<p><span class="math display">\[SS_{total} = SS_{reg} +
SS_{error}\]</span></p>
<ul>
<li><strong>SS_total (Total Sum of Squares):</strong> <span
class="math inline">\(\sum(y_i - \bar{y})^2\)</span>
<ul>
<li><strong>Concept:</strong> This measures the <strong>total
variation</strong> in your response variable, <code>y</code>. Imagine
you didnâ€™t have a model and your only prediction for any <code>y</code>
was its overall average, <code>È³</code>. SS_total is the total squared
error of this simple â€œmean-onlyâ€ model. It represents the total amount
of variation you are trying to explain.
è¿™æµ‹é‡çš„æ˜¯å“åº”å˜é‡â€œyâ€çš„<strong>æ€»å˜å¼‚</strong>ã€‚å‡è®¾ä½ æ²¡æœ‰æ¨¡å‹ï¼Œä½ å¯¹ä»»ä½•â€œyâ€çš„å”¯ä¸€é¢„æµ‹æ˜¯å®ƒçš„æ•´ä½“å¹³å‡å€¼â€œÈ³â€ã€‚SS_total
æ˜¯è¿™ä¸ªç®€å•çš„â€œä»…å‡å€¼â€æ¨¡å‹çš„æ€»å¹³æ–¹è¯¯å·®ã€‚å®ƒä»£è¡¨äº†ä½ è¯•å›¾è§£é‡Šçš„å˜å¼‚æ€»é‡ã€‚</li>
</ul></li>
<li><strong>SS_reg (Regression Sum of Squares):</strong> <span
class="math inline">\(\sum(\hat{y}_i - \bar{y})^2\)</span>
<ul>
<li><strong>Concept:</strong> This measures the <strong>explained
variation</strong>. Itâ€™s the amount of variation in <code>y</code> that
is captured by your regression model. It calculates the difference
between your modelâ€™s predictions (<code>Å·</code>) and the simple average
(<code>È³</code>). A large SS_reg means your modelâ€™s predictions are a
big improvement over just guessing the average.
<strong>å®ƒè¡¡é‡</strong>è§£é‡Šå˜å¼‚**ã€‚å®ƒæ˜¯å›å½’æ¨¡å‹æ•æ‰åˆ°çš„ y
çš„å˜å¼‚é‡ã€‚å®ƒè®¡ç®—æ¨¡å‹é¢„æµ‹å€¼ï¼ˆâ€œÅ·â€ï¼‰ä¸ç®€å•å¹³å‡å€¼ï¼ˆâ€œÈ³â€ï¼‰ä¹‹é—´çš„å·®å¼‚ã€‚è¾ƒå¤§çš„
SS_reg æ„å‘³ç€æ¨¡å‹çš„é¢„æµ‹ç»“æœæ¯”ä»…ä»…çŒœæµ‹å¹³å‡å€¼æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ul></li>
<li><strong>SS_error (Error Sum of Squares):</strong> <span
class="math inline">\(\sum(y_i - \hat{y}_i)^2\)</span>
<ul>
<li><strong>Concept:</strong> This measures the <strong>unexplained
variation</strong> (also called the Residual Sum of Squares). Itâ€™s the
amount of variation your model <em>failed</em> to capture. Itâ€™s the sum
of the squared differences between the actual data (<code>y</code>) and
your modelâ€™s predictions (<code>Å·</code>).
<strong>å®ƒè¡¡é‡</strong>æœªè§£é‡Šå˜å¼‚**ï¼ˆä¹Ÿç§°ä¸ºæ®‹å·®å¹³æ–¹å’Œï¼‰ã€‚å®ƒæ˜¯æ¨¡å‹<em>æœªèƒ½</em>æ•æ‰åˆ°çš„å˜å¼‚é‡ã€‚å®ƒæ˜¯å®é™…æ•°æ®
(<code>y</code>) ä¸æ¨¡å‹é¢„æµ‹å€¼ (<code>Å·</code>) ä¹‹é—´å¹³æ–¹å·®ä¹‹å’Œã€‚</li>
</ul></li>
</ul>
<p>The <strong>R-squared</strong> value is a direct consequence of this
decomposition. Itâ€™s the proportion of the total variance that is
explained by the model: <strong>R å¹³æ–¹</strong>
å€¼æ˜¯è¿™ç§åˆ†è§£çš„ç›´æ¥ç»“æœã€‚å®ƒæ˜¯æ¨¡å‹è§£é‡Šçš„æ€»æ–¹å·®çš„æ¯”ä¾‹ï¼š</p>
<p><span class="math display">\[R^2 =
\frac{SS_{reg}}{SS_{total}}\]</span></p>
<h4 id="the-anova-table-and-the-f-test">2. The ANOVA Table and the
F-test</h4>
<p>æ–¹å·®åˆ†æè¡¨å’Œ F æ£€éªŒ The second slide organizes these sums of squares
to perform a formal hypothesis test.
ç¬¬äºŒå¼ å¹»ç¯ç‰‡æ•´ç†äº†è¿™äº›å¹³æ–¹å’Œï¼Œä»¥è¿›è¡Œæ­£å¼çš„å‡è®¾æ£€éªŒã€‚</p>
<ul>
<li><strong>The Question:</strong> â€œIs there <em>any</em> relationship
between my set of predictors and the response variable?â€ or â€œIs my model
better than nothing?â€
â€œæˆ‘çš„é¢„æµ‹å˜é‡é›†å’Œå“åº”å˜é‡ä¹‹é—´æ˜¯å¦å­˜åœ¨<em>ä»»ä½•</em>å…³ç³»ï¼Ÿâ€æˆ–â€œæˆ‘çš„æ¨¡å‹æ¯”æ²¡æœ‰æ¨¡å‹å¥½å—ï¼Ÿâ€</li>
<li><strong>The Hypotheses:</strong>
<ul>
<li><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>):</strong> <span
class="math inline">\(\beta_1 = \beta_2 = \dots = \beta_p = 0\)</span>.
(None of the predictors have a relationship with the response; the model
is useless). <strong>é›¶å‡è®¾ (<span
class="math inline">\(H_0\)</span>)</strong>ï¼š<span
class="math inline">\(\beta_1 = \beta_2 = \dots = \beta_p = 0\)</span>ã€‚
ï¼ˆæ‰€æœ‰é¢„æµ‹å˜é‡éƒ½ä¸å“åº”å˜é‡æ— å…³ï¼›è¯¥æ¨¡å‹æ¯«æ— ç”¨å¤„ï¼‰ã€‚</li>
<li><strong>Alternative Hypothesis (<span
class="math inline">\(H_1\)</span>):</strong> At least one <span
class="math inline">\(\beta_j\)</span> is not zero. (The model has some
predictive value). <strong>å¤‡æ‹©å‡è®¾ (<span
class="math inline">\(H_1\)</span>)ï¼š</strong>è‡³å°‘æœ‰ä¸€ä¸ª <span
class="math inline">\(\beta_j\)</span>
ä¸ä¸ºé›¶ã€‚ï¼ˆè¯¥æ¨¡å‹å…·æœ‰ä¸€å®šçš„é¢„æµ‹å€¼ï¼‰ã€‚</li>
</ul></li>
</ul>
<p>To test this, we canâ€™t just compare the raw SS values, because they
depend on the number of data points and predictors. We need to normalize
them. ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä¸èƒ½ä»…ä»…æ¯”è¾ƒåŸå§‹çš„ SS
å€¼ï¼Œå› ä¸ºå®ƒä»¬å–å†³äºæ•°æ®ç‚¹å’Œé¢„æµ‹å˜é‡çš„æ•°é‡ã€‚æˆ‘ä»¬éœ€è¦å¯¹å®ƒä»¬è¿›è¡Œå½’ä¸€åŒ–ã€‚</p>
<ul>
<li><strong>Mean Squares (MS):</strong> This is the â€œaverageâ€ variation.
We calculate it by dividing the Sum of Squares by its <strong>degrees of
freedom (df)</strong>.
<strong>è¿™æ˜¯â€œå¹³å‡â€å˜å¼‚ã€‚æˆ‘ä»¬é€šè¿‡å°†å¹³æ–¹å’Œé™¤ä»¥å…¶</strong>è‡ªç”±åº¦ (df)**
æ¥è®¡ç®—å®ƒã€‚
<ul>
<li><strong>MS_reg</strong> = <span class="math inline">\(SS_{reg} /
p\)</span>. This is the average explained variation <em>per
predictor</em>. è¿™æ˜¯<em>æ¯ä¸ªé¢„æµ‹å˜é‡</em>çš„å¹³å‡è§£é‡Šå˜å¼‚ã€‚</li>
<li><strong>MS_error</strong> = <span class="math inline">\(SS_{error} /
(n - p - 1)\)</span>. This is the average unexplained variation, which
is our estimate of the error variance, <span
class="math inline">\(s^2\)</span>. è¿™æ˜¯å¹³å‡æœªè§£é‡Šå˜å¼‚ï¼Œå³æˆ‘ä»¬å¯¹è¯¯å·®æ–¹å·®
<span class="math inline">\(s^2\)</span> çš„ä¼°è®¡å€¼ã€‚</li>
</ul></li>
</ul>
<h4 id="the-connection-the-f-statistic-è”ç³»f-ç»Ÿè®¡é‡">3. The Connection:
The F-statistic è”ç³»ï¼šF ç»Ÿè®¡é‡</h4>
<p>The <strong>F-statistic</strong> is the key that connects everything.
Itâ€™s the ratio of the two mean squares: <strong>F
ç»Ÿè®¡é‡</strong>æ˜¯è¿æ¥ä¸€åˆ‡çš„å…³é”®ã€‚å®ƒæ˜¯ä¸¤ä¸ªå‡æ–¹çš„æ¯”å€¼ï¼š <span
class="math display">\[F = \frac{\text{Mean Squared
Regression}}{\text{Mean Squared Error}} =
\frac{MS_{reg}}{MS_{error}}\]</span></p>
<ul>
<li><strong>Intuitive Meaning:</strong> The F-statistic is a ratio of
the <strong>average explained variation</strong> to the <strong>average
unexplained variation</strong>. F
ç»Ÿè®¡é‡æ˜¯<strong>å¹³å‡è§£é‡Šå˜å¼‚</strong>ä¸<strong>å¹³å‡æœªè§£é‡Šå˜å¼‚</strong>çš„æ¯”å€¼ã€‚
<ul>
<li>If your model is useless (<span class="math inline">\(H_0\)</span>
is true), the explained variation should be about the same as the
random, unexplained variation. The F-statistic will be close to 1.
å¦‚æœä½ çš„æ¨¡å‹æ— æ•ˆï¼ˆH_0$
ä¸ºçœŸï¼‰ï¼Œåˆ™è§£é‡Šå˜å¼‚åº”è¯¥ä¸éšæœºçš„æœªè§£é‡Šå˜å¼‚å¤§è‡´ç›¸åŒã€‚F ç»Ÿè®¡é‡æ¥è¿‘ 1ã€‚</li>
<li>If your model is useful (<span class="math inline">\(H_1\)</span> is
true), the explained variation should be significantly larger than the
unexplained variation. The F-statistic will be much greater than 1.
å¦‚æœä½ çš„æ¨¡å‹æœ‰æ•ˆï¼ˆH_1$ ä¸ºçœŸï¼‰ï¼Œåˆ™è§£é‡Šå˜å¼‚åº”è¯¥æ˜¾è‘—å¤§äºæœªè§£é‡Šå˜å¼‚ã€‚ F
ç»Ÿè®¡é‡å°†è¿œå¤§äº 1ã€‚</li>
</ul></li>
</ul>
<p>We compare our calculated F-statistic to an
<strong>F-distribution</strong> to get a <strong>p-value</strong>. A
small p-value (&lt; 0.05) provides strong evidence to reject the null
hypothesis and conclude that your model, as a whole, is statistically
significant. æˆ‘ä»¬å°†è®¡ç®—å‡ºçš„ F ç»Ÿè®¡é‡ä¸<strong>F
åˆ†å¸ƒ</strong>è¿›è¡Œæ¯”è¾ƒï¼Œå¾—å‡º<strong>p å€¼</strong>ã€‚è¾ƒå°çš„ p å€¼ï¼ˆ&lt;
0.05ï¼‰å¯ä»¥æä¾›å¼ºæœ‰åŠ›çš„è¯æ®æ¥æ‹’ç»é›¶å‡è®¾ï¼Œå¹¶å¾—å‡ºæ‚¨çš„æ¨¡å‹æ•´ä½“å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§çš„ç»“è®ºã€‚</p>
<h1 id="construct-different-types-of-intervals-2">12.construct different
types of intervals</h1>
<p><img src="/imgs/5054C3/Gauss_Markov1.png">
<img src="/imgs/5054C3/Gauss_Markov2.png"></p>
<ul>
<li><strong>å†…å®¹</strong>: These slides explain the <strong>Gauss-Markov
theorem</strong>, a cornerstone result in statistics that establishes
why the Least Squares Estimator (LSE) is considered the gold standard
for fitting linear models under a specific set of assumptions.
è¿™äº›å¹»ç¯ç‰‡è§£é‡Šäº†<strong>é«˜æ–¯-é©¬å°”å¯å¤«å®šç†</strong>ï¼Œè¿™æ˜¯ç»Ÿè®¡å­¦ä¸­çš„ä¸€ä¸ªåŸºçŸ³æ€§æˆæœï¼Œå®ƒé˜æ˜äº†ä¸ºä»€ä¹ˆæœ€å°äºŒä¹˜ä¼°è®¡é‡
(LSE) è¢«è®¤ä¸ºæ˜¯åœ¨ç‰¹å®šå‡è®¾æ¡ä»¶ä¸‹æ‹Ÿåˆçº¿æ€§æ¨¡å‹çš„é»„é‡‘æ ‡å‡†ã€‚</li>
</ul>
<h3 id="summary-6">Summary</h3>
<p>The slides argue for the superiority of the Least Squares Estimator
(LSE) by highlighting its key properties: itâ€™s easy to compute,
consistent, and efficient. This culminates in the <strong>Gauss-Markov
Theorem</strong>, which proves that LSE is <strong>BLUE</strong>: the
<strong>B</strong>est <strong>L</strong>inear <strong>U</strong>nbiased
<strong>E</strong>stimator. This means that among all estimators that
are both linear and unbiased, the LSE is the â€œbestâ€ because it has the
smallest possible variance, making it the most precise. The second slide
provides the key steps for the mathematical proof of this important
theorem. è¿™äº›å¹»ç¯ç‰‡é€šè¿‡å¼ºè°ƒæœ€å°äºŒä¹˜ä¼°è®¡é‡ (LSE)
çš„å…³é”®ç‰¹æ€§æ¥è®ºè¯å…¶ä¼˜è¶Šæ€§ï¼šæ˜“äºè®¡ç®—ã€ä¸€è‡´æ€§é«˜ä¸”é«˜æ•ˆã€‚æœ€ç»ˆå¾—å‡ºäº†<strong>é«˜æ–¯-é©¬å°”å¯å¤«å®šç†</strong>ï¼Œè¯¥å®šç†è¯æ˜äº†
LSE
æ˜¯<strong>BLUE</strong>ï¼š<strong>æœ€ä½³</strong>çº¿æ€§<strong>æ— å</strong>ä¼°è®¡é‡ã€‚è¿™æ„å‘³ç€åœ¨æ‰€æœ‰çº¿æ€§ä¸”æ— åçš„ä¼°è®¡é‡ä¸­ï¼ŒLSE
æ˜¯â€œæœ€ä½³â€çš„ï¼Œå› ä¸ºå®ƒå…·æœ‰æœ€å°çš„æ–¹å·®ï¼Œå› æ­¤ç²¾åº¦æœ€é«˜ã€‚ç¬¬äºŒå¼ å¹»ç¯ç‰‡æä¾›äº†è¿™ä¸€é‡è¦å®šç†çš„æ•°å­¦è¯æ˜çš„å…³é”®æ­¥éª¤ã€‚</p>
<h3 id="deeper-dive-into-the-concepts">Deeper Dive into the
Concepts</h3>
<h4 id="properties-of-lse-slide-1-å±€éƒ¨æ­£äº¤ä¼°è®¡-lse-çš„æ€§è´¨">Properties of
LSE (Slide 1) å±€éƒ¨æ­£äº¤ä¼°è®¡ (LSE) çš„æ€§è´¨</h4>
<ul>
<li><strong>Easy Computationæ˜“äºè®¡ç®—:</strong> The LSE has a direct,
closed-form solution called the Normal Equation (<span
class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>). You can
calculate it directly without needing complex iterative algorithms.</li>
<li><strong>Consistencyä¸€è‡´æ€§:</strong> As your sample size gets larger
and larger, the LSE estimate (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) is guaranteed
to get closer and closer to the true population value (<span
class="math inline">\(\boldsymbol{\beta}\)</span>). With enough data, it
will find the truth. éšç€æ ·æœ¬é‡è¶Šæ¥è¶Šå¤§ï¼ŒLSE ä¼°è®¡å€¼ (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>)
å¿…ç„¶ä¼šè¶Šæ¥è¶Šæ¥è¿‘çœŸå®çš„æ€»ä½“å€¼ (<span
class="math inline">\(\boldsymbol{\beta}\)</span>)ã€‚åªè¦æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œå®ƒå°±èƒ½æ‰¾åˆ°çœŸç›¸ã€‚</li>
<li><strong>Efficiencyæ•ˆç‡:</strong> An efficient estimator is the one
with the lowest possible variance. This means its estimates are the most
precise and least spread out.
é«˜æ•ˆçš„ä¼°è®¡å™¨æ˜¯æ–¹å·®å°½å¯èƒ½ä½çš„ä¼°è®¡å™¨ã€‚è¿™æ„å‘³ç€å®ƒçš„ä¼°è®¡å€¼æœ€ç²¾ç¡®ï¼Œä¸”åˆ†å¸ƒæœ€å‡åŒ€ã€‚</li>
<li><strong>BLUE (Best Linear Unbiased
Estimator)BLUEï¼ˆæœ€ä½³çº¿æ€§æ— åä¼°è®¡å™¨ï¼‰:</strong> This acronym elegantly
summarizes the Gauss-Markov theorem.
è¿™ä¸ªç¼©å†™å®Œç¾åœ°æ¦‚æ‹¬äº†é«˜æ–¯-é©¬å°”å¯å¤«å®šç†ã€‚
<ul>
<li><strong>Linear:</strong> The estimator is a linear function of the
response variable <strong>y</strong>. We can write it as <span
class="math inline">\(\hat{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span> for some matrix <strong>A</strong>.
ä¼°è®¡å™¨æ˜¯å“åº”å˜é‡<strong>y</strong>çš„çº¿æ€§å‡½æ•°ã€‚å¯¹äºæŸä¸ªçŸ©é˜µ<strong>A</strong>ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶å†™æˆ
<span class="math inline">\(\hat{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span>ã€‚</li>
<li><strong>Unbiased:</strong> The estimator does not systematically
overestimate or underestimate the true parameter. On average, its
expected value is the true value: <span
class="math inline">\(E[\hat{\boldsymbol{\beta}}] =
\boldsymbol{\beta}\)</span>.
ä¼°è®¡å™¨ä¸ä¼šç³»ç»Ÿæ€§åœ°é«˜ä¼°æˆ–ä½ä¼°çœŸå®å‚æ•°ã€‚å¹³å‡è€Œè¨€ï¼Œå…¶é¢„æœŸå€¼å³ä¸ºçœŸå®å€¼ï¼š<span
class="math inline">\(E[\hat{\boldsymbol{\beta}}] =
\boldsymbol{\beta}\)</span>ã€‚</li>
<li><strong>Best:</strong> It has the <strong>minimum variance</strong>
of all possible linear unbiased estimators. Itâ€™s the most precise and
reliable estimator in its class.
åœ¨æ‰€æœ‰å¯èƒ½çš„çº¿æ€§æ— åä¼°è®¡å™¨ä¸­ï¼Œå®ƒçš„<strong>æ–¹å·®</strong>æœ€å°ã€‚å®ƒæ˜¯åŒç±»ä¸­æœ€ç²¾ç¡®ã€æœ€å¯é çš„ä¼°è®¡å™¨ã€‚</li>
</ul></li>
</ul>
<h4 id="the-gauss-markov-theorem-é«˜æ–¯-é©¬å°”å¯å¤«å®šç†">The Gauss-Markov
Theorem é«˜æ–¯-é©¬å°”å¯å¤«å®šç†</h4>
<p>The theorem provides the theoretical justification for using OLS.
è¯¥å®šç†ä¸ºä½¿ç”¨æœ€å°äºŒä¹˜æ³• (OLS) æä¾›äº†ç†è®ºä¾æ®ã€‚ * <strong>The Core
Idea:</strong> You could invent many different ways to estimate the
coefficients of a linear model. As long as your proposed methods are
both linear and unbiased, the Gauss-Markov theorem guarantees that none
of them will be more precise than the standard least squares method. LSE
gives the â€œsharpestâ€ possible estimates.
ä½ å¯ä»¥å‘æ˜è®¸å¤šä¸åŒçš„æ–¹æ³•æ¥ä¼°è®¡çº¿æ€§æ¨¡å‹çš„ç³»æ•°ã€‚åªè¦ä½ æå‡ºçš„æ–¹æ³•æ˜¯çº¿æ€§çš„ä¸”æ— åçš„ï¼Œé«˜æ–¯-é©¬å°”å¯å¤«å®šç†å°±èƒ½ä¿è¯ï¼Œå®ƒä»¬éƒ½ä¸ä¼šæ¯”æ ‡å‡†æœ€å°äºŒä¹˜æ³•æ›´ç²¾ç¡®ã€‚æœ€å°äºŒä¹˜æ³•
(LSE) ç»™å‡ºäº†â€œæœ€ç²¾ç¡®â€çš„ä¼°è®¡å€¼ã€‚</p>
<ul>
<li><strong>The Logic of the Proof (Slide 2) è¯æ˜é€»è¾‘:</strong> The
proof is a clever comparison of variances. **è¯¥è¯æ˜å·§å¦™åœ°æ¯”è¾ƒäº†æ–¹å·®ã€‚
<ol type="1">
<li>It starts by defining <strong>any</strong> other linear unbiased
estimator as <span class="math inline">\(\tilde{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span>.
é¦–å…ˆï¼Œå°†<strong>ä»»ä½•</strong>å…¶ä»–çº¿æ€§æ— åä¼°è®¡é‡å®šä¹‰ä¸º <span
class="math inline">\(\tilde{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span>ã€‚</li>
<li>It uses the â€œunbiasedâ€ property to force a condition on the matrix
<strong>A</strong>, which ultimately leads to the insight that
<strong>A</strong> can be written in terms of the LSE matrix plus some
other matrix <strong>D</strong>, where <strong>DX = 0</strong>.
å®ƒåˆ©ç”¨â€œæ— åâ€æ€§è´¨å¯¹çŸ©é˜µ<strong>A</strong>å¼ºåˆ¶æ–½åŠ ä¸€ä¸ªæ¡ä»¶ï¼Œæœ€ç»ˆå¾—å‡º<strong>A</strong>å¯ä»¥å†™æˆLSEçŸ©é˜µåŠ ä¸Šå¦ä¸€ä¸ªçŸ©é˜µ<strong>D</strong>ï¼Œå…¶ä¸­<strong>DX
= 0</strong>ã€‚</li>
<li>It then calculates the variance of this other estimator, which turns
out to be: <span class="math display">\[Var(\tilde{\boldsymbol{\beta}})
= Var(\text{LSE}) + \text{a non-negative term involving }
\mathbf{D}\]</span> ç„¶åè®¡ç®—å¦ä¸€ä¸ªä¼°è®¡é‡çš„æ–¹å·®ï¼Œç»“æœä¸ºï¼š <span
class="math display">\[Var(\tilde{\boldsymbol{\beta}}) = Var(\text{LSE})
+ \text{ä¸€ä¸ªåŒ…å« } \mathbf{D} çš„éè´Ÿé¡¹\]</span></li>
<li>Since the variance of any other linear unbiased estimator is the
variance of the LSE <em>plus</em> something non-negative, the variance
of the LSE must be the smallest possible value.
ç”±äºä»»ä½•å…¶ä»–çº¿æ€§æ— åä¼°è®¡é‡çš„æ–¹å·®éƒ½æ˜¯LSEçš„æ–¹å·®<em>åŠ ä¸Š</em>ä¸€ä¸ªéè´Ÿé¡¹ï¼Œå› æ­¤LSEçš„æ–¹å·®å¿…é¡»æ˜¯æœ€å°çš„å¯èƒ½å€¼ã€‚</li>
</ol></li>
</ul>
<h3 id="further-understandings-beyond-the-slides">Further Understandings
Beyond the Slides</h3>
<h4 id="what-are-the-required-assumptionséœ€è¦å“ªäº›å‡è®¾">1. What are the
required assumptions?éœ€è¦å“ªäº›å‡è®¾ï¼Ÿ</h4>
<p>The Gauss-Markov theorem is powerful, but itâ€™s not magic. It only
holds if a set of assumptions about the modelâ€™s errors (<span
class="math inline">\(\epsilon\)</span>) are met:
é«˜æ–¯-é©¬å°”å¯å¤«å®šç†è™½ç„¶å¼ºå¤§ï¼Œä½†å¹¶éé­”æ³•ã€‚å®ƒä»…åœ¨æ»¡è¶³ä»¥ä¸‹å…³äºæ¨¡å‹è¯¯å·® (<span
class="math inline">\(\epsilon\)</span>) çš„å‡è®¾æ—¶æˆç«‹ï¼š * <strong>Zero
Meané›¶å‡å€¼:</strong> The average of the errors is zero (<span
class="math inline">\(E[\epsilon] = 0\)</span>). è¯¯å·®çš„å¹³å‡å€¼ä¸ºé›¶ (<span
class="math inline">\(E[\epsilon] = 0\)</span>)ã€‚ * <strong>Constant
Variance (Homoscedasticity)æ’å®šæ–¹å·®ï¼ˆåŒæ–¹å·®æ€§ï¼‰:</strong> The errors
have the same variance, <span class="math inline">\(\sigma^2\)</span>,
at all levels of the predictors.
<strong>åœ¨é¢„æµ‹å˜é‡çš„å„ä¸ªæ°´å¹³ä¸Šï¼Œè¯¯å·®å…·æœ‰ç›¸åŒçš„æ–¹å·® <span
class="math inline">\(\sigma^2\)</span>ã€‚ * </strong>Uncorrelated
Errorsä¸ç›¸å…³è¯¯å·®:** The error for one observation is not correlated with
the error for another. ä¸€ä¸ªè§‚æµ‹å€¼çš„è¯¯å·®ä¸å¦ä¸€ä¸ªè§‚æµ‹å€¼çš„è¯¯å·®ä¸ç›¸å…³ã€‚ *
<strong>No Perfect Multicollinearityéå®Œå…¨å¤šé‡å…±çº¿æ€§:</strong> The
predictor variables are not perfectly linearly related.
é¢„æµ‹å˜é‡å¹¶éå®Œå…¨çº¿æ€§ç›¸å…³ã€‚</p>
<p><strong>Crucially, the Gauss-Markov theorem does NOT require the
errors to be normally distributed.</strong> The normality assumption is
only needed later for constructing confidence intervals and conducting
t-tests and F-tests.
è‡³å…³é‡è¦çš„æ˜¯ï¼Œé«˜æ–¯-é©¬å°”å¯å¤«å®šç†å¹¶ä¸è¦æ±‚è¯¯å·®æœä»æ­£æ€åˆ†å¸ƒã€‚**æ­£æ€æ€§å‡è®¾ä»…åœ¨æ„å»ºç½®ä¿¡åŒºé—´ä»¥åŠè¿›è¡Œ
t æ£€éªŒå’Œ F æ£€éªŒæ—¶éœ€è¦ã€‚</p>
<h4
id="when-is-lse-not-the-best-the-bias-variance-tradeoff-ä»€ä¹ˆæ—¶å€™-lse-ä¸æ˜¯æœ€ä½³é€‰æ‹©-åå·®-æ–¹å·®æƒè¡¡">2.
When is LSE NOT the Best? (The Bias-Variance Tradeoff) ä»€ä¹ˆæ—¶å€™ LSE
ä¸æ˜¯æœ€ä½³é€‰æ‹©ï¼Ÿ ï¼ˆåå·®-æ–¹å·®æƒè¡¡ï¼‰</h4>
<p>While LSE is the best <em>unbiased</em> estimator, sometimes we can
get better predictive performance by accepting a little bit of bias in
exchange for a large reduction in variance. This is the core idea behind
modern regularization methods: è™½ç„¶ LSE
æ˜¯æœ€å¥½çš„<em>æ— å</em>ä¼°è®¡å™¨ï¼Œä½†æœ‰æ—¶æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¥å—å°‘é‡åå·®æ¥å¤§å¹…é™ä½æ–¹å·®ï¼Œä»è€Œè·å¾—æ›´å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚è¿™æ˜¯ç°ä»£æ­£åˆ™åŒ–æ–¹æ³•èƒŒåçš„æ ¸å¿ƒæ€æƒ³ï¼š
* <strong>Ridge Regression and LASSOå²­å›å½’å’Œ LASSO:</strong> These are
popular techniques that produce <em>biased</em> estimates of the
coefficients. However, by introducing this small amount of bias, they
can often produce models with a lower overall error (Mean Squared Error)
than LSE, especially when predictors are highly correlated.
è¿™äº›æ˜¯äº§ç”Ÿ<em>æœ‰å</em>ç³»æ•°ä¼°è®¡çš„æµè¡ŒæŠ€æœ¯ã€‚ç„¶è€Œï¼Œé€šè¿‡å¼•å…¥å°‘é‡åå·®ï¼Œå®ƒä»¬é€šå¸¸å¯ä»¥ç”Ÿæˆæ¯”
LSE
å…·æœ‰æ›´ä½æ€»ä½“è¯¯å·®ï¼ˆå‡æ–¹è¯¯å·®ï¼‰çš„æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨é¢„æµ‹å˜é‡é«˜åº¦ç›¸å…³çš„æƒ…å†µä¸‹ã€‚
Therefore, while LSE is the theoretical champion in the world of
unbiased estimators, in the world of predictive modeling, methods that
intentionally introduce bias can sometimes be superior. å› æ­¤ï¼Œè™½ç„¶ LSE
æ˜¯æ— åä¼°è®¡é¢†åŸŸçš„ç†è®ºå† å†›ï¼Œä½†åœ¨é¢„æµ‹æ¨¡å‹é¢†åŸŸï¼Œæœ‰æ„å¼•å…¥åå·®çš„æ–¹æ³•æœ‰æ—¶ä¼šæ›´èƒœä¸€ç­¹ã€‚</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/16/2025_9_15%E4%BC%9A%E8%AE%AE%E7%BA%AA%E8%A6%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/16/2025_9_15%E4%BC%9A%E8%AE%AE%E7%BA%AA%E8%A6%81/" class="post-title-link" itemprop="url">MEETING - AI4Chemistry conference notes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-16 01:00:00" itemprop="dateCreated datePublished" datetime="2025-09-16T01:00:00+08:00">2025-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-21 06:46:20" itemprop="dateModified" datetime="2025-09-21T06:46:20+08:00">2025-09-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/meeting-notes/" itemprop="url" rel="index"><span itemprop="name">meeting notes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Meeting Notes</p>
<h1 id="ä¹æœˆä»½è§„åˆ’">1. ä¹æœˆä»½è§„åˆ’ï¼š</h1>
<ul>
<li><strong>å†…å®¹</strong>:</li>
</ul>
<h2 id="ç»å…¸è®ºæ–‡æ¨¡å‹å¤ç°">1.1 ç»å…¸è®ºæ–‡æ¨¡å‹å¤ç°ï¼š</h2>
<ul>
<li><strong>1.1.1</strong> <a
target="_blank" rel="noopener" href="https://www.biorxiv.org/content/10.1101/2025.08.06.668973v2">MetaAI-3Dè›‹ç™½è´¨ç»“æ„å¯¹æ¯”å­¦ä¹ </a>
<a target="_blank" rel="noopener" href="https://github.com/kalifadan/FusionProt">github</a></li>
<li><strong>1.1.2</strong> <a
target="_blank" rel="noopener" href="https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-024-02030-9">DrugDAGT</a>
<a target="_blank" rel="noopener" href="https://github.com/codejiajia/DrugDAGT">github</a> <a
target="_blank" rel="noopener" href="https://zenodo.org/records/6568518#.YrtTKHbMK38">!!! orbnet
qm9çš„graph basedçš„feature</a> <a
target="_blank" rel="noopener" href="https://zenodo.org/records/6568518#.YrtTKHbMK38">github</a></li>
<li><strong>1.1.3 GCL &amp; GCFORMER</strong> <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.13902">ç»å…¸å¯¹æ¯”å­¦ä¹ è®ºæ–‡</a> <a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=u6FuiKzT1K">!! Graph contrastive
learning former - NIPS2024</a> <a
target="_blank" rel="noopener" href="https://github.com/JHL-HUST/GCFormer">github</a></li>
<li><strong>GCL</strong> <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.13902">Graph Contrastive Learning with
Augmentations - NIPS</a> <a
target="_blank" rel="noopener" href="https://github.com/Shen-Lab/GraphCL">github</a></li>
</ul>
<h2 id="æ•°æ®é›†">1.2 æ•°æ®é›†ï¼š</h2>
<p><a
target="_blank" rel="noopener" href="https://quantum-machine.org/datasets/">quantum-machine-QM9</a> <a
target="_blank" rel="noopener" href="https://github.com/bigdata-ustc/QM9nano4USTC">ä¸­ç§‘å¤§å®éªŒè¯¾åšäº†ä¸ªQM9æ•°æ®é›†çš„demo</a></p>
<ul>
<li><p><strong>é—­å£³å±‚åˆ†å­æ•°æ®é›†</strong></p></li>
<li><p><strong>åˆ†å­è§„æ¨¡</strong>ï¼šå« 13.4
ä¸‡ä¸ªç¨³å®šçš„å°åˆ†å­æœ‰æœºç‰©</p></li>
<li><p><strong>å…ƒç´ ç»„æˆ</strong>ï¼šä»…åŒ…å«
Hï¼ˆæ°¢ï¼‰ã€Cï¼ˆç¢³ï¼‰ã€Nï¼ˆæ°®ï¼‰ã€Oï¼ˆæ°§ï¼‰ã€Fï¼ˆæ°Ÿï¼‰5 ç§å…ƒç´ </p></li>
<li><p><strong>è®¡ç®—ç†è®ºæ°´å¹³</strong>ï¼šæ‰€æœ‰åˆ†å­å±æ€§åŸºäºDFTï¼ˆå¯†åº¦æ³›å‡½ç†è®ºï¼‰/B3LYP
æ³›å‡½ / 6-31G (2df,p) åŸºç»„è®¡ç®—</p></li>
<li><p><strong>åŒ…å«å±æ€§</strong>ï¼šå¶æçŸ©ã€HOMOï¼ˆæœ€é«˜å æ®åˆ†å­è½¨é“ï¼‰èƒ½é‡ã€LUMOï¼ˆæœ€ä½æœªå æ®åˆ†å­è½¨é“ï¼‰èƒ½é‡ã€0K
å†…èƒ½ã€298.15K å†…èƒ½ç­‰ï¼Œæ˜¯é—­å£³å±‚åˆ†å­æ€§è´¨é¢„æµ‹çš„åŸºå‡†æ•°æ®é›†</p></li>
<li><p><strong>è®­ç»ƒè¾“å…¥</strong>ï¼šé—­å£³å±‚ä¸‹çš„é‡å­åŒ–å­¦çŸ©é˜µï¼Œå³ Fock
çŸ©é˜µï¼ˆFï¼‰ã€å¯†åº¦çŸ©é˜µï¼ˆPï¼‰ã€å“ˆå¯†é¡¿çŸ©é˜µï¼ˆHï¼‰ã€é‡å çŸ©é˜µï¼ˆSï¼‰ï¼Œæ„æˆå‘é‡ T
çš„é—­å£³å±‚å½¢å¼ï¼ˆå› é—­å£³å±‚è‡ªæ—‹å¯¹ç§°ï¼Œæ— éœ€åŒºåˆ† Î±ã€Î² è‡ªæ—‹ï¼Œæ•…
T=[F,P,H,S]ï¼‰</p></li>
<li><p><strong>ç‰¹å¾æœ¬è´¨</strong>ï¼šè¿™äº›çŸ©é˜µç¼–ç äº†åˆ†å­çš„ç”µå­ç»“æ„ä¿¡æ¯ï¼ˆå¦‚è½¨é“é—´ç›¸äº’ä½œç”¨ã€ç”µå­å¯†åº¦åˆ†å¸ƒï¼‰ï¼Œæ˜¯
OrbNet-Equi å­¦ä¹  â€œåˆ†å­ç»“æ„ - èƒ½é‡â€ æ˜ å°„çš„æ ¸å¿ƒä¾æ®</p></li>
<li><p><strong>è®­ç»ƒè¾“å‡º</strong>ï¼šQM9 ä¸­çš„0K
å†…èƒ½ä½œä¸ºæ ¸å¿ƒè®­ç»ƒç›®æ ‡è¾“å‡ºï¼Œ0K
å†…èƒ½æ˜¯åˆ†å­åŠ¿èƒ½é¢ï¼ˆPESï¼‰è®¡ç®—çš„æ ¸å¿ƒå±æ€§ï¼Œç›´æ¥å…³è”åˆ†å­ç¨³å®šæ€§ä¸ååº”èƒ½å’é¢„æµ‹ï¼Œç›¸æ¯”å…¶ä»–å±æ€§ï¼ˆå¦‚å¶æçŸ©ï¼‰ï¼Œèƒ½é‡æ˜¯é—­å£³å±‚ä¸å¼€å£³å±‚ç³»ç»Ÿå…±æœ‰çš„å…³é”®æŒ‡æ ‡ï¼Œä¾¿äºåç»­æ‰©å±•åˆ°å¼€å£³å±‚èƒ½é‡é¢„æµ‹ã€‚</p></li>
</ul>
<h2 id="å¤ç°æ–¹å¼">1.3 å¤ç°æ–¹å¼ï¼š</h2>
<p>æ•°æ®å°å‹åŒ–å¤ç°ï¼š setp1: 5k bonds and edges</p>
<p>setp2: 10k bonds and edges</p>
<p>setp3: 20k bonds and edges</p>
<h2 id="ä¸¤ç§æ•°æ®ç©ºé—´ä¸€ä¸ªå¾…ç†è§£çš„æ¦‚å¿µopen-shell">1.4
ä¸¤ç§æ•°æ®ç©ºé—´ä¸€ä¸ªå¾…ç†è§£çš„æ¦‚å¿µOpen-shellï¼š</h2>
<ul>
<li><strong>AO (Atomic
Orbital)</strong>ï¼š<strong>åŸå­è½¨é“</strong>ã€‚</li>
<li><strong>MO (Molecular
Orbital)</strong>ï¼š<strong>åˆ†å­è½¨é“</strong>ã€‚</li>
<li><strong>Open-shell</strong>ï¼š<strong>å¼€å£³å±‚ç»„æ€</strong>ã€‚</li>
</ul>
<h4 id="ao-atomic-orbital---åŸå­è½¨é“å±‚é¢-ä»¥åŸå­ä¸ºä¸­å¿ƒçš„æ¨¡å‹">1. AO
(Atomic Orbital) - åŸå­è½¨é“å±‚é¢ / ä»¥åŸå­ä¸ºä¸­å¿ƒçš„æ¨¡å‹</h4>
<p>å°†åˆ†å­çœ‹ä½œæ˜¯åŸå­ï¼ˆèŠ‚ç‚¹ï¼‰å’ŒåŒ–å­¦é”®ï¼ˆè¾¹ï¼‰æ„æˆçš„å›¾ã€‚æ¨¡å‹å­¦ä¹ æ¯ä¸ªåŸå­ä»¥åŠå…¶å‘¨å›´å±€éƒ¨ç¯å¢ƒçš„representationï¼Œé¢„æµ‹æ•´ä¸ªåˆ†å­çš„æ€§è´¨ã€‚</p>
<ul>
<li><p><strong>key</strong>:
åˆ†å­çš„æ€§è´¨æ˜¯ç”±å…¶ç»„æˆåŸå­ä»¥åŠåŸå­é—´çš„ç›¸äº’ä½œç”¨å†³å®šçš„ã€‚</p></li>
<li><p><strong>è¾“å…¥</strong>:
åŸå­çš„åæ ‡ã€åŸå­ç±»å‹ã€ä»¥åŠåŸå­é—´çš„è·ç¦»æˆ–é”®åˆå…³ç³»ã€‚</p></li>
<li><p><strong>æ–¹å¼</strong>: <strong>æ¶ˆæ¯ä¼ é€’å›¾ç¥ç»ç½‘ç»œ (Message
Passing Neural Network, MPNN)</strong>
ã€‚æ¯ä¸ªåŸå­ï¼ˆèŠ‚ç‚¹ï¼‰ä»å…¶é‚»å±…åŸå­é‚£é‡Œæ¥æ”¶â€œæ¶ˆæ¯â€ï¼ˆä¿¡æ¯ï¼‰ï¼Œæ›´æ–°è‡ªå·±çš„çŠ¶æ€ï¼ˆç‰¹å¾å‘é‡ï¼‰ã€‚è¿‡ç¨‹ä¼šé‡å¤å¤šæ¬¡ï¼ˆå¯¹åº”å›¾ç¥ç»ç½‘ç»œçš„å¤šä¸ªGCLå±‚ï¼‰ï¼Œä¿¡æ¯å¯ä»¥åœ¨æ•´ä¸ªåˆ†å­ä¸­ä¼ æ’­ã€‚</p>
<ul>
<li><strong>EGNN (E(n) Equivariant Graph Neural Network)</strong>:
å…¸å‹çš„ä»¥åŸå­ä¸ºä¸­å¿ƒçš„æ¨¡å‹ã€‚<strong>ç­‰å˜æ€§
(Equivariance)</strong>ï¼Œæ—‹è½¬æˆ–ç§»åŠ¨æ•´ä¸ªåˆ†å­æ—¶ï¼Œæ¨¡å‹å†…éƒ¨å­¦ä¹ åˆ°çš„åŸå­è¡¨ç¤ºä¹Ÿä¼šç›¸åº”åœ°æ—‹è½¬æˆ–ç§»åŠ¨ï¼Œæœ€ç»ˆé¢„æµ‹çš„èƒ½é‡ç­‰æ ‡é‡å±æ€§ä¿æŒä¸å˜ã€‚ç¬¦åˆç‰©ç†è§„å¾‹ï¼Œæ€§èƒ½å‡ºè‰²ã€‚å®ƒç›´æ¥åœ¨åŸå­çš„3Dåæ ‡ä¸Šè¿›è¡Œæ“ä½œã€‚</li>
<li><strong>OrbNet</strong>:
<strong>æ•°æ®æ¥æº</strong>é‡‡ç”¨åŠç»éªŒæ–¹æ³•ï¼ˆGFN1-xTBï¼‰ç”Ÿæˆé‡å­åŒ–å­¦çŸ©é˜µï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿ç•™äº†å…³é”®ç‰©ç†ä¿¡æ¯,æ”¯æŒæ•°åƒåŸå­è§„æ¨¡çš„åˆ†å­æ¨¡æ‹Ÿã€‚
é—­å£³å±‚ï¼ˆClosed-shellï¼‰ä¸å¼€å£³å±‚ï¼ˆOpen-shellï¼‰ç³»ç»Ÿçš„åŒºåˆ«ï¼šé—­å£³å±‚ç”µå­è‡ªæ—‹å…¨é…å¯¹ï¼ˆä»…éœ€è€ƒè™‘ç©ºé—´è‡ªç”±åº¦ï¼‰ï¼Œå¼€å£³å±‚å«æœªé…å¯¹ç”µå­ï¼ˆéœ€åŒæ—¶è€ƒè™‘ç©ºé—´å’Œè‡ªæ—‹è‡ªç”±åº¦ï¼‰ï¼Œå¼€å£³å±‚åœ¨è‡ªç”±åŸºã€ååº”ä¸­é—´ä½“ç­‰åœºæ™¯çš„å…³é”®æ„ä¹‰ã€‚
<ul>
<li><strong>key</strong>:
åŸºäºåŸå­è½¨é“ï¼ˆAOï¼‰ç‰¹å¾ï¼ˆè‡ªæ´½åœºï¼ˆSCFï¼‰æ”¶æ•›è¿‡ç¨‹ä¸­çš„é‡å­åŒ–å­¦çŸ©é˜µï¼‰é¢„æµ‹åˆ†å­èƒ½é‡ã€‚</li>
<li><strong>ç‰¹å¾è¡¨ç¤º</strong>: é‡‡ç”¨å¯¹ç§°é€‚é…åŸå­è½¨é“ï¼ˆSAAOï¼‰åŸºç»„ï¼Œå°† AO
ç‰¹å¾ç¼–ç ä¸ºå›¾ç»“æ„æ•°æ®ã€‚</li>
<li><strong>æ¨¡å‹æ¶æ„</strong>:
åŸºäºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ï¼Œè§£ç è¾“å‡ºå¼ é‡å¹¶æ±‚å’Œå¾—åˆ°åˆ†å­èƒ½é‡ã€‚</li>
</ul></li>
</ul></li>
</ul>
<h4 id="mo-molecular-orbital---åˆ†å­è½¨é“å±‚é¢-ä»¥åˆ†å­ä¸ºæ•´ä½“çš„æ¨¡å‹">2. MO
(Molecular Orbital) - åˆ†å­è½¨é“å±‚é¢ / ä»¥åˆ†å­ä¸ºæ•´ä½“çš„æ¨¡å‹</h4>
<p>ç›´æ¥å­¦ä¹ æˆ–é¢„æµ‹æ•´ä¸ªåˆ†å­çš„å…¨å±€å±æ€§ï¼Œåˆ†å­æ•´ä½“ç”µå­ç»“æ„ç›¸å…³çš„å±æ€§ã€‚åˆ†å­è½¨é“æœ¬èº«å°±æ˜¯ç”±æ‰€æœ‰åŸå­è½¨é“çº¿æ€§ç»„åˆè€Œæˆçš„ï¼Œæè¿°äº†ç”µå­åœ¨æ•´ä¸ªåˆ†å­ä¸­çš„è¿åŠ¨çŠ¶æ€ã€‚</p>
<ul>
<li><strong>key</strong>:
ç›´æ¥å¯¹åˆ†å­çš„å…¨å±€ç‰¹å¾æˆ–å…¶ç”µå­ç»“æ„çš„å®è§‚è¡¨ç°ï¼ˆå¦‚è½¨é“èƒ½çº§ï¼‰è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li><strong>å…¸å‹è¾“å…¥</strong>: æ•´ä¸ªåˆ†å­çš„æè¿°ç¬¦ï¼ˆä¾‹å¦‚åˆ†å­æŒ‡çº¹
fingerprintï¼‰ï¼Œæˆ–è€…ç›´æ¥å°†åˆ†å­ç»“æ„ä½œä¸ºè¾“å…¥æ¥é¢„æµ‹åˆ†å­è½¨é“çš„æ€§è´¨ã€‚</li>
<li><strong>å·¥ä½œæ–¹å¼</strong>:
è¿™ç±»æ¨¡å‹å¯èƒ½ä¸å®Œå…¨ä¾èµ–äºåŸå­é—´çš„æ¶ˆæ¯ä¼ é€’ï¼Œè€Œæ˜¯æ—¨åœ¨ç›´æ¥æ„å»ºä¸€ä¸ªä»åˆ†å­åˆ°å…¶å…¨å±€å±æ€§çš„æ˜ å°„ã€‚ä¾‹å¦‚ï¼Œé¢„æµ‹åˆ†å­çš„æœ€é«˜å æ®åˆ†å­è½¨é“
(HOMO) å’Œæœ€ä½æœªå æ®åˆ†å­è½¨é“ (LUMO) çš„èƒ½é‡ã€‚</li>
</ul>
<h4 id="open-shell---å¼€å£³å±‚ç»„æ€">3. Open-shell - å¼€å£³å±‚ç»„æ€</h4>
<p><a
target="_blank" rel="noopener" href="https://www.zhihu.com/question/620638044?write">Open-shell</a></p>
<p>éšä¾¿çœ‹çœ‹çš„ä¸€ç¯‡ICML-2024 <a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=XC9IoAsyEN">ICML-WORKSHOP-2024</a></p>
<p><a
target="_blank" rel="noopener" href="https://www.nature.com/articles/s41524-022-00863-y">NPJ-2022</a></p>
<p><a
target="_blank" rel="noopener" href="https://www.pnas.org/doi/abs/10.1073/pnas.2205221119">PNAS-2022
OrbNet-Equi</a> <a
target="_blank" rel="noopener" href="https://zenodo.org/records/6568518#.YrtTKHbMK38">!!! orbnet
qm9çš„graph basedçš„feature</a></p>
<h1 id="åæœˆä»½è§„åˆ’">2. åæœˆä»½è§„åˆ’ï¼š</h1>
<h2 id="what-we-need-to-do">2.1 What we need to do?</h2>
<h2 id="æˆ‘ä»¬éœ€è¦åˆ†æao-å’Œ-mo-çš„è¡¨ç°">æˆ‘ä»¬éœ€è¦åˆ†æAO å’Œ MO çš„è¡¨ç°ã€‚</h2>
<p>æˆ‘ä»¬ä¸ç¡®å®šMOå’ŒAOçš„variabilityçš„å·®å¼‚ï¼Œæ˜¯ç”±EGNNè¿˜æ˜¯GPRå¸¦æ¥çš„
ä»–ä»¬çš„informationä¸ä¸€æ ·ã€‚</p>
<p>Learning curve - learnabilityå›¾</p>
<p>æˆ‘ä»¬éœ€è¦é€šè¿‡å¯¹æ¯”å­¦ä¹ æ‰¾åˆ° AO å’Œ MO çš„ similarityã€‚</p>
<p>ç†æƒ³åŒ–ç»“æœï¼š
æˆ‘ä»¬å¸Œæœ›AOé€šè¿‡å¯¹æ¯”å­¦ä¹ è¾¾åˆ°MOçš„ç¨‹åº¦ï¼Œæˆ‘ä»¬å¸Œæœ›å¯¹æ¯”å­¦ä¹ å¯¹AOæ›´æœ‰ç”¨ã€‚</p>
<p>æˆ‘ä»¬å¸Œæœ›è¾¾åˆ°GCL + AO</p>
<p>AOä»ç‰©ç†æ„ä¹‰ä¸Šæ›´æœ¬è´¨ï¼ŒMOçš„æ€§è´¨æ›´å¥½ã€‚</p>
<p>Final goal Inverse design éœ€è¦ç”Ÿæˆ AO</p>
<h2 id="linear-combination-of-atomic-orbitals">2.2 (Linear Combination
of Atomic Orbitals)</h2>
<p><strong>LCAO</strong> <strong>åŸå­è½¨é“çº¿æ€§ç»„åˆ (Linear Combination of
Atomic Orbitals)</strong>ã€‚</p>
<ul>
<li><p><strong>key</strong>:
åˆ†å­çš„å¤æ‚è¡Œä¸ºï¼ˆç”±åˆ†å­è½¨é“MOæè¿°ï¼‰å¯ä»¥è¿‘ä¼¼åœ°é€šè¿‡å…¶ç»„æˆåŸå­çš„æ›´ç®€å•çš„è¡Œä¸ºï¼ˆç”±åŸå­è½¨é“AOæè¿°ï¼‰æ¥æ„å»ºã€‚ä¸€ä¸ª<strong>åˆ†å­è½¨é“
(MO)</strong> å¯ä»¥è¡¨ç¤ºä¸ºå¤šä¸ª<strong>åŸå­è½¨é“ (AO)</strong>
çš„åŠ æƒå’Œã€‚</p></li>
<li><p><strong>æ•°å­¦å½¢å¼</strong>: ä¸€ä¸ªåˆ†å­è½¨é“ <span
class="math inline">\(\Psi_{MO}\)</span>ï¼Œå®ƒå¯ä»¥è¡¨ç¤ºä¸ºï¼š <span
class="math display">\[\Psi_{MO} = c_1\phi_1 + c_2\phi_2 + \dots +
c_n\phi_n = \sum_{i=1}^{n} c_i\phi_i\]</span> å…¶ä¸­ï¼š</p>
<ul>
<li><span class="math inline">\(\Psi_{MO}\)</span>
æ˜¯ä¸€ä¸ªåˆ†å­è½¨é“æ³¢å‡½æ•°ã€‚</li>
<li><span class="math inline">\(\phi_i\)</span> æ˜¯ç¬¬ <span
class="math inline">\(i\)</span> ä¸ªåŸå­çš„åŸå­è½¨é“æ³¢å‡½æ•°ã€‚</li>
<li><span class="math inline">\(c_i\)</span>
æ˜¯æ¯ä¸ªåŸå­è½¨é“çš„<strong>ç»„åˆç³»æ•°
(coefficient)</strong>ï¼Œå®ƒæ˜¯ä¸€ä¸ªæƒé‡å€¼ï¼Œè¡¨ç¤ºè¯¥åŸå­è½¨é“å¯¹è¿™ä¸ªåˆ†å­è½¨é“çš„è´¡çŒ®å¤§å°ã€‚ç³»æ•°é€šè¿‡æ±‚è§£è–›å®šè°”æ–¹ç¨‹ï¼ˆé€šå¸¸ä½¿ç”¨Hartree-Fockç­‰è¿‘ä¼¼æ–¹æ³•ï¼‰å¾—åˆ°çš„ã€‚</li>
</ul></li>
<li><p><strong>ex</strong>:
æ°¢åˆ†å­ï¼ˆHâ‚‚ï¼‰ã€‚æœ‰ä¸¤ä¸ªæ°¢åŸå­ï¼Œæ¯ä¸ªæ°¢åŸå­æœ‰ä¸€ä¸ª1såŸå­è½¨é“ï¼ˆ<span
class="math inline">\(\phi_A\)</span> å’Œ <span
class="math inline">\(\phi_B\)</span>ï¼‰ã€‚è¿™ä¸¤ä¸ªåŸå­è½¨é“å¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼çº¿æ€§ç»„åˆï¼Œå½¢æˆä¸¤ä¸ªåˆ†å­è½¨é“ï¼š</p>
<ol type="1">
<li><strong>æˆé”®è½¨é“ (Bonding MO)</strong>: <span
class="math inline">\(\Psi_{\sigma} = c_A\phi_A +
c_B\phi_B\)</span>ã€‚ç”µå­å¤„äºè¿™ä¸ªè½¨é“æ—¶ï¼Œä¼šä¸»è¦åˆ†å¸ƒåœ¨ä¸¤ä¸ªåŸå­æ ¸ä¹‹é—´ï¼Œå½¢æˆç¨³å®šçš„åŒ–å­¦é”®ã€‚èƒ½é‡æ¯”åŸæ¥çš„AOæ›´ä½ã€‚</li>
<li><strong>åé”®è½¨é“ (Antibonding MO)</strong>: <span
class="math inline">\(\Psi_{\sigma^*} = c&#39;_A\phi_A -
c&#39;_B\phi_B\)</span>ã€‚ç”µå­å¤„äºè¿™ä¸ªè½¨é“æ—¶ï¼Œä¼šä¸»è¦åˆ†å¸ƒåœ¨åŸå­æ ¸çš„å¤–ä¾§ï¼Œæ’æ–¥ä¸¤ä¸ªåŸå­æ ¸ï¼Œä¸åˆ©äºæˆé”®ã€‚èƒ½é‡æ¯”åŸæ¥çš„AOæ›´é«˜ã€‚</li>
</ol></li>
</ul>
<h3 id="localization-åˆ†å­è½¨é“å±€åŸŸåŒ–">2.3 Localization
(åˆ†å­è½¨é“å±€åŸŸåŒ–)</h3>
<p>åˆ†å­è½¨é“ï¼ˆMOsï¼‰ï¼Œå°¤å…¶æ˜¯é€šè¿‡æ ‡å‡†è®¡ç®—æ–¹æ³•ï¼ˆå¦‚Hartree-Fockï¼‰ç›´æ¥æ±‚è§£å‡ºæ¥çš„ï¼Œé€šå¸¸æ˜¯<strong>ç¦»åŸŸçš„
(delocalized)</strong>ã€‚è¿™æ„å‘³ç€æ¯ä¸ªMOéƒ½å¯èƒ½æ‰©å±•åˆ°æ•´ä¸ªåˆ†å­ï¼Œç”±åˆ†å­ä¸­å‡ ä¹æ‰€æœ‰åŸå­çš„AOsè´¡çŒ®æ„æˆã€‚ä¾‹å¦‚ï¼Œåœ¨è‹¯ç¯ä¸­ï¼Œè®¡ç®—å‡ºçš„Ï€ç”µå­MOä¼šå‡åŒ€åœ°åˆ†å¸ƒåœ¨å…­ä¸ªç¢³åŸå­ä¸Šã€‚</p>
<p><strong>åˆ†å­è½¨é“å±€åŸŸåŒ– (Localization of Molecular Orbitals)</strong>
å°±æ˜¯ä¸€ä¸ªæ•°å­¦å˜æ¢è¿‡ç¨‹ï¼Œå®ƒå°†è¿™äº›ç¦»åŸŸçš„MOsè½¬åŒ–ä¸ºä¸€ç»„æ–°çš„<strong>å±€åŸŸåŒ–åˆ†å­è½¨é“
(Localized Molecular Orbitals, LMOs)</strong>ã€‚</p>
<ul>
<li><strong>æ ¸å¿ƒç›®æ ‡</strong>:
åœ¨ä¸æ”¹å˜åˆ†å­æ•´ä½“æ³¢å‡½æ•°å’Œæ€»èƒ½é‡çš„å‰æä¸‹ï¼Œå°†åˆ†å­è½¨é“å°½å¯èƒ½åœ°é™åˆ¶åœ¨ç©ºé—´ä¸­çš„ä¸€å°å—åŒºåŸŸå†…ã€‚</li>
<li><strong>å˜æ¢ç»“æœ</strong>:
<ul>
<li>ç¦»åŸŸçš„æˆé”®è½¨é“ <span class="math inline">\(\rightarrow\)</span>
å¯¹åº”äºç‰¹å®š <strong>åŒ–å­¦é”®</strong>
çš„å±€åŸŸè½¨é“ï¼ˆä¾‹å¦‚C-Hé”®ï¼ŒC=CåŒé”®ï¼‰ã€‚</li>
<li>ç¦»åŸŸçš„éé”®è½¨é“ <span class="math inline">\(\rightarrow\)</span>
å¯¹åº”äºç‰¹å®šåŸå­ä¸Šçš„ <strong>å­¤å¯¹ç”µå­ (lone pair)</strong> æˆ–
<strong>å†…å±‚ç”µå­</strong>ã€‚</li>
</ul></li>
<li><strong>å±€åŸŸåŒ–</strong>:
<ol type="1">
<li><strong>åŒ–å­¦ç›´è§‚æ€§</strong>:
LMOsæä¾›äº†æ¸…æ™°çš„åŒ–å­¦å›¾åƒï¼Œä¾¿äºç†è§£å’Œåˆ†æåŒ–å­¦æˆé”®æƒ…å†µã€‚</li>
</ol></li>
</ul>
<h3 id="lcao-å’Œ-mo-çš„å…³ç³»">3. LCAO å’Œ MO çš„å…³ç³»</h3>
<ol type="1">
<li><strong>LCAOæ˜¯æ„å»ºMOçš„æ–¹æ³•</strong>:
LCAOæ˜¯ç”¨äºè¿‘ä¼¼è®¡ç®—å’Œè¡¨ç¤ºåˆ†å­è½¨é“ï¼ˆMOï¼‰çš„æ•°å­¦æ¡†æ¶ã€‚æˆ‘ä»¬å‡è®¾MOå¯ä»¥ç”±ä¸€ç»„å·²çŸ¥çš„åŸºå‡½æ•°ï¼ˆå³åŸå­è½¨é“AOï¼‰çº¿æ€§ç»„åˆè€Œæˆã€‚</li>
<li><strong>MOæ˜¯LCAOæ–¹æ³•çš„ç»“æœ</strong>:
é€šè¿‡LCAOæ–¹æ³•ï¼Œç»“åˆé‡å­åŠ›å­¦å˜åˆ†åŸç†æ±‚è§£è–›å®šè°”æ–¹ç¨‹ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°äº†ä¸€ç³»åˆ—åˆ†å­è½¨é“ï¼ˆMOsï¼‰çš„å…·ä½“å½¢å¼ï¼ˆå³æ¯ä¸ªAOçš„è´¡çŒ®ç³»æ•°<span
class="math inline">\(c_i\)</span>ï¼‰ä»¥åŠå®ƒä»¬çš„èƒ½é‡ã€‚</li>
</ol>
<p><strong>åŸå­è½¨é“ (AO) [è¾“å…¥]</strong> <span
class="math inline">\(\xrightarrow{\text{LCAOæ–¹æ³• [è¿‡ç¨‹/æ¡†æ¶]}}\)</span>
<strong>åˆ†å­è½¨é“ (MO) [è¾“å‡º/ç»“æœ]</strong></p>
<h3 id="é«˜æ–¯è¿‡ç¨‹å›å½’-gaussian-process-regression-gpr">4. é«˜æ–¯è¿‡ç¨‹å›å½’
(Gaussian Process Regression, GPR)</h3>
<p><strong>é«˜æ–¯è¿‡ç¨‹å›å½’ (GPR)</strong>
æ˜¯ä¸€ç§åŸºäºè´å¶æ–¯æ€æƒ³çš„éå‚æ•°å›å½’æ–¹æ³•ã€‚å®ƒåœ¨å¤„ç†å°æ ·æœ¬ã€é«˜ç»´åº¦ã€éœ€è¦ä¸ç¡®å®šæ€§ä¼°è®¡çš„å¤æ‚å›å½’é—®é¢˜æ—¶ç‰¹åˆ«æœ‰æ•ˆã€‚</p>
<h4 id="key">key</h4>
<p><strong>GPRçš„æ ¸å¿ƒæ˜¯ç›´æ¥å¯¹å‡½æ•°æœ¬èº«è¿›è¡Œå»ºæ¨¡</strong>ã€‚å®ƒå‡è®¾æˆ‘ä»¬æƒ³è¦å»ºæ¨¡çš„ç›®æ ‡å‡½æ•°
<span class="math inline">\(f(x)\)</span> æ˜¯ä¸€ä¸ªæœä»<strong>é«˜æ–¯è¿‡ç¨‹
(Gaussian Process, GP)</strong> çš„éšæœºå‡½æ•°ã€‚</p>
<ul>
<li><strong>é«˜æ–¯è¿‡ç¨‹ (GP)</strong>
ä¸€ä¸ªé«˜æ–¯è¿‡ç¨‹æ˜¯æ— ç©·å¤šä¸ªéšæœºå˜é‡çš„é›†åˆï¼Œå…¶ä¸­ä»»æ„æœ‰é™ä¸ªéšæœºå˜é‡çš„ç»„åˆéƒ½æœä»ä¸€ä¸ªè”åˆé«˜æ–¯åˆ†å¸ƒã€‚
ä¸€ä¸ªGPå®šä¹‰äº†ä¸€ä¸ªå…³äº<strong>å‡½æ•°çš„åˆ†å¸ƒ (a distribution over
functions)</strong>ã€‚å½“æˆ‘ä»¬ä»è¿™ä¸ªGPä¸­â€œé‡‡æ ·â€æ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°çš„ä¸æ˜¯ä¸€ä¸ªæ•°å€¼ï¼Œè€Œæ˜¯ä¸€æ•´ä¸ªå‡½æ•°ã€‚</li>
</ul>
<p>ä¸€ä¸ªé«˜æ–¯è¿‡ç¨‹å®Œå…¨ç”±ä¸¤éƒ¨åˆ†å®šä¹‰ï¼š 1. <strong>å‡å€¼å‡½æ•° (Mean Function)
<span class="math inline">\(m(x)\)</span></strong>:
å®šä¹‰äº†å‡½æ•°åˆ†å¸ƒçš„â€œæœŸæœ›â€æˆ–â€œä¸­å¿ƒè¶‹åŠ¿â€ã€‚é€šå¸¸ä¸ºäº†ç®€åŒ–ï¼Œä¼šå‡è®¾å‡å€¼ä¸ºé›¶ã€‚ 2.
<strong>åæ–¹å·®å‡½æ•° (Covariance Function) æˆ– æ ¸å‡½æ•° (Kernel) <span
class="math inline">\(k(x, x&#39;)\)</span></strong>:
å®šä¹‰äº†å‡½æ•°åœ¨ä¸åŒè¾“å…¥ç‚¹ <span class="math inline">\(x\)</span> å’Œ <span
class="math inline">\(x&#39;\)</span>
å¤„çš„å€¼ä¹‹é—´çš„â€œç›¸å…³æ€§â€æˆ–â€œç›¸ä¼¼æ€§â€ã€‚å¦‚æœ <span
class="math inline">\(x\)</span> å’Œ <span
class="math inline">\(x&#39;\)</span> å¾ˆæ¥è¿‘ï¼Œæ ¸å‡½æ•°çš„å€¼å°±å¾ˆå¤§ï¼Œæ„å‘³ç€
<span class="math inline">\(f(x)\)</span> å’Œ <span
class="math inline">\(f(x&#39;)\)</span>
çš„å€¼ä¼šå¾ˆç›¸ä¼¼ã€‚è¿™ç¼–ç äº†æˆ‘ä»¬å¯¹å‡½æ•°å¹³æ»‘æ€§çš„å…ˆéªŒä¿¡å¿µã€‚</p>
<h4 id="gpr-å·¥ä½œ">GPR å·¥ä½œ</h4>
<p>GPRçš„å·¥ä½œæµç¨‹ï¼š</p>
<p><strong>ç¬¬ä¸€æ­¥ï¼šå®šä¹‰å…ˆéªŒåˆ†å¸ƒ (Prior Distribution)</strong>
åœ¨çœ‹åˆ°ä»»ä½•è®­ç»ƒæ•°æ®ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆæ ¹æ®å…ˆéªŒçŸ¥è¯†é€‰æ‹©ä¸€ä¸ªå‡å€¼å‡½æ•°ï¼ˆé€šå¸¸ä¸º0ï¼‰å’Œä¸€ä¸ªæ ¸å‡½æ•°ï¼ˆä¾‹å¦‚å¸¸ç”¨çš„<strong>å¾„å‘åŸºå‡½æ•°æ ¸/RBFæ ¸</strong>ï¼‰ã€‚è¿™ä¸ªGPå®šä¹‰äº†ä¸€ä¸ªå‡½æ•°çš„å…ˆéªŒåˆ†å¸ƒï¼ŒåŒ…å«äº†æˆ‘ä»¬èƒ½æƒ³åˆ°çš„æ‰€æœ‰â€œå¯èƒ½â€çš„å‡½æ•°ã€‚</p>
<p><strong>ç¬¬äºŒæ­¥ï¼šè®¡ç®—åéªŒåˆ†å¸ƒ (Posterior Distribution)</strong>
å½“æˆ‘ä»¬å¾—åˆ°ä¸€ç»„è®­ç»ƒæ•°æ® <span class="math inline">\((X_{train},
Y_{train})\)</span>
åï¼Œæˆ‘ä»¬åˆ©ç”¨è´å¶æ–¯å®šç†æ¥æ›´æ–°æˆ‘ä»¬çš„å‡½æ•°åˆ†å¸ƒã€‚æˆ‘ä»¬ä»å…ˆéªŒåˆ†å¸ƒä¸­â€œç­›é€‰â€æ‰é‚£äº›ä¸è®­ç»ƒæ•°æ®ä¸ç¬¦çš„å‡½æ•°ï¼Œå¾—åˆ°ä¸€ä¸ª<strong>åéªŒåˆ†å¸ƒ
(Posterior Distribution)</strong>ã€‚</p>
<p>è¿™ä¸ªåéªŒåˆ†å¸ƒä»ç„¶æ˜¯ä¸€ä¸ªé«˜æ–¯è¿‡ç¨‹ï¼Œå…¶å‡å€¼å’Œåæ–¹å·®æœ‰è§£æè§£ï¼ˆå¯ä»¥ç›´æ¥è®¡ç®—å‡ºæ¥ï¼‰ï¼Œä¸éœ€è¦å¤æ‚çš„è¿­ä»£ä¼˜åŒ–ã€‚</p>
<h4 id="è¿›è¡Œé¢„æµ‹">è¿›è¡Œé¢„æµ‹</h4>
<p>å¯¹äºä¸€ä¸ªæ–°çš„æµ‹è¯•ç‚¹ <span
class="math inline">\(x_{test}\)</span>ï¼Œæˆ‘ä»¬æƒ³é¢„æµ‹å¯¹åº”çš„ <span
class="math inline">\(y_{test}\)</span>ã€‚åœ¨åéªŒåˆ†å¸ƒä¸‹ï¼Œ<span
class="math inline">\(y_{test}\)</span>
çš„é¢„æµ‹å€¼æœä»ä¸€ä¸ªä¸€ç»´é«˜æ–¯åˆ†å¸ƒï¼Œè¿™ä¸ªåˆ†å¸ƒæœ‰ï¼š 1. <strong>é¢„æµ‹å‡å€¼
(Predicted Mean)</strong>: è¿™å°±æ˜¯æˆ‘ä»¬å¯¹ <span
class="math inline">\(y_{test}\)</span>
çš„æœ€ä½³ç‚¹ä¼°è®¡ã€‚å®ƒæ˜¯ç”±è®­ç»ƒæ•°æ®ç‚¹çš„åŠ æƒå¹³å‡è®¡ç®—å¾—å‡ºçš„ï¼Œæƒé‡ç”±æ ¸å‡½æ•°å†³å®šã€‚
2. <strong>é¢„æµ‹æ–¹å·® (Predicted Variance)</strong>:
è¿™è¡¡é‡äº†æˆ‘ä»¬å¯¹é¢„æµ‹ç»“æœçš„<strong>ä¸ç¡®å®šæ€§</strong>ã€‚åœ¨é è¿‘è®­ç»ƒæ•°æ®ç‚¹çš„åœ°æ–¹ï¼Œæ–¹å·®ä¼šå¾ˆå°ï¼ˆé¢„æµ‹å¾ˆè‡ªä¿¡ï¼‰ï¼›åœ¨è¿œç¦»è®­ç»ƒæ•°æ®ç‚¹çš„æœªçŸ¥åŒºåŸŸï¼Œæ–¹å·®ä¼šå¾ˆå¤§ï¼ˆé¢„æµ‹å¾ˆä¸ç¡®å®šï¼‰ã€‚</p>
<h1 id="åç»­è§„åˆ’">3. åç»­è§„åˆ’ï¼š</h1>
<p>IF AO çš„å­¦ä¹ è¡¨ç°æ¯” MO è¦å¥½ æˆ‘ä»¬å°†ä¼šèšç„¦äº AO ï¼ˆAtomic representation
vs atomic orbitalï¼‰</p>
<ol type="1">
<li><strong>AO one body decomposition</strong></li>
<li><strong>MO two body decomposition</strong></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/15/Pandoc_Deployment/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/15/Pandoc_Deployment/" class="post-title-link" itemprop="url">BLOGS - Pandoc Deployment</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-09-15 10:00:00" itemprop="dateCreated datePublished" datetime="2025-09-15T10:00:00+08:00">2025-09-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-09-19 19:24:55" itemprop="dateModified" datetime="2025-09-19T19:24:55+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">æŠ€æœ¯</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Download <a
target="_blank" rel="noopener" href="https://github.com/jgm/pandoc/releases/tag/3.8">Pandoc</a>!</p>
<p><strong>pandoc-3.8-windows-x86_64.msi</strong></p>
<h2
id="é—®é¢˜ä¸»è¦ä¸ºäº†è§£å†³é»˜è®¤çš„nextæ¸²æŸ“å™¨æ— æ³•æ¸²æŸ“å¤æ‚å…¬å¼çš„é—®é¢˜">ã€é—®é¢˜ã€‘ä¸»è¦ä¸ºäº†è§£å†³é»˜è®¤çš„Nextæ¸²æŸ“å™¨æ— æ³•æ¸²æŸ“å¤æ‚å…¬å¼çš„é—®é¢˜</h2>
<h3
id="step1åœ¨ç³»ç»Ÿå˜é‡ä¸­æ‰¾åˆ°pathç‚¹å‡»ç¼–è¾‘">Step1:åœ¨ç³»ç»Ÿå˜é‡ä¸­æ‰¾åˆ°<strong>Pathâ†’ç‚¹å‡»ç¼–è¾‘</strong></h3>
<h3
id="step2ç‚¹å‡»æ–°å»ºè¾“å…¥pandoc.exeçš„çˆ¶ç›®å½•è·¯å¾„cç‚¹å‡»ç¡®å®š">Step2:ç‚¹å‡»æ–°å»ºâ†’è¾“å…¥pandoc.exeçš„çˆ¶ç›®å½•è·¯å¾„ï¼ˆC:ï¼‰â†’ç‚¹å‡»ç¡®å®š</h3>
<h3 id="step3é‡å¯ç»ˆç«¯">Step3:é‡å¯ç»ˆç«¯</h3>
<h3
id="step4å®‰è£…pandoc-3.8-windows-x86_64.msi">Step4:å®‰è£…pandoc-3.8-windows-x86_64.msi</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pandoc --version</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm list --depth=0 | Select-String <span class="string">&quot;renderer&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-markdown-it --save</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm list --depth=0 | Select-String <span class="string">&quot;renderer&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://blog.csdn.net/gitblog_00216/article/details/141763934">Ref</a></p>
<h3
id="step5åœ¨ä½ æ‰€åœ¨çš„åšå®¢å¤´åŠ å…¥å¿…è¦çš„å¼•å…¥">Step5:åœ¨ä½ æ‰€åœ¨çš„åšå®¢å¤´åŠ å…¥å¿…è¦çš„å¼•å…¥</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mathjax: <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3
id="step6åœ¨d_ailab_hkust_machine_learningçš„_config.yamlæ–‡ä»¶ä¸­ç¡®ä¿ä½ çš„pandocè·¯å¾„èƒ½è¢«æ‰¾åˆ°">Step6:åœ¨D:_AILab_HKUST_Machine_Learningçš„_config.yamlæ–‡ä»¶ä¸­ç¡®ä¿ä½ çš„Pandocè·¯å¾„èƒ½è¢«æ‰¾åˆ°</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ pandoc:</span><br><span class="line">$ pandoc_path: <span class="string">&quot;C:/Users/Aprine/AppData/Local/Pandoc/pandoc.exe&quot;</span> <span class="comment"># </span></span><br><span class="line">$ args:</span><br><span class="line">$   - <span class="string">&quot;--mathjax&quot;</span></span><br></pre></td></tr></table></figure>
<h3
id="step7åœ¨d_ailab_hkust_machine_learning_config.yamlæ–‡ä»¶ä¸­ç¡®ä¿ä½ çš„mathä¿¡æ¯é…ç½®æ­£ç¡®">Step7:åœ¨D:_AILab_HKUST_Machine_Learning_config.yamlæ–‡ä»¶ä¸­ç¡®ä¿ä½ çš„mathä¿¡æ¯é…ç½®æ­£ç¡®</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ mathjax:</span><br><span class="line">$   <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">$   <span class="comment"># See: https://mhchem.github.io/MathJax-mhchem/</span></span><br><span class="line">$   mhchem: <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3
id="step8ä¿®æ”¹ä½ çš„headæ–‡ä»¶çš„åŸºç¡€æ ¼å¼">Step8:ä¿®æ”¹ä½ çš„headæ–‡ä»¶çš„åŸºç¡€æ ¼å¼</h3>
<p>More info: <a
target="_blank" rel="noopener" href="https://blog.csdn.net/ALexander_Monster/article/details/105717091">Ref</a></p>
<h3 id="push-new-blog">Push new blog</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$   hexo g -d</span><br></pre></td></tr></table></figure>
<p>å¯¹æ¯”æŸå¤±å‡½æ•°ï¼ˆInfoNCE/NT-Xent Lossï¼‰å®šä¹‰ä¸ºï¼š <span
class="math inline">\(\mathcal{L}_{\text{q}} = -\log \underbrace{\left(
\frac{\exp\left( \mathbf{q} \cdot \mathbf{k}^{+} / \tau
\right)}{\exp\left( \mathbf{q} \cdot \mathbf{k}^{+} / \tau \right) +
\sum\limits_{i=1}^{N} \exp\left( \mathbf{q} \cdot \mathbf{k}_{i}^{-} /
\tau \right)} \right)}_{\text{Softmax æ¦‚ç‡}}\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="ä¸‹ä¸€é¡µ"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> å¼ºåŠ›é©±åŠ¨
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
