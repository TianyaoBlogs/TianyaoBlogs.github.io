<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianyaoblogs.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="TianyaoBlogs">
<meta property="og:url" content="https://tianyaoblogs.github.io/index.html">
<meta property="og:site_name" content="TianyaoBlogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://tianyaoblogs.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>TianyaoBlogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TianyaoBlogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/01/5054C4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/10/01/5054C4/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-10-01 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-01T21:00:00+08:00">2025-10-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-10-18 23:00:24" itemprop="dateModified" datetime="2025-10-18T23:00:24+08:00">2025-10-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>统计机器学习Lecture-4</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="what-is-classification">1. What is Classification?</h1>
<p>Classification is a type of <strong>supervised machine
learning</strong> where the goal is to predict a
<strong>categorical</strong> or qualitative response. Unlike regression
where you predict a continuous numerical value (like a price or
temperature), classification assigns an input to a specific category or
class.
分类是一种<strong>监督式机器学习</strong>，其目标是预测<strong>分类</strong>或定性响应。与预测连续数值（例如价格或温度）的回归不同，分类将输入分配到特定的类别或类别。</p>
<p><strong>Key characteristics:</strong></p>
<ul>
<li><p><strong>Goal:</strong> Predict the class of a subject based on
input features.</p></li>
<li><p><strong>Output (Response):</strong> The output is a category,
such as ‘Yes’/‘No’, ‘Spam’/‘Not Spam’, or
‘High’/‘Medium’/‘Low’.</p></li>
<li><p><strong>Applications:</strong> Common examples include email spam
detectors, medical diagnosis (e.g., virus carrier vs. non-carrier), and
fraud detection.</p>
<ul>
<li><strong>目标</strong>：根据输入特征预测主题的类别。</li>
<li><strong>输出（响应）：</strong>输出是一个类别，例如“是”/“否”、“垃圾邮件”/“非垃圾邮件”或“高”/“中”/“低”。</li>
<li><strong>应用</strong>：常见示例包括垃圾邮件检测器、医学诊断（例如，病毒携带者与非病毒携带者）和欺诈检测。
The example used in the slides is a credit card <strong>Default
dataset</strong>. The goal is to predict whether a customer will
<strong>default</strong> (‘Yes’ or ‘No’) on their payments based on
their monthly <strong>income</strong> and account
<strong>balance</strong>.</li>
</ul></li>
</ul>
<p>## Why Not Use Linear Regression?为什么不使用线性回归？</p>
<p>At first, it might seem possible to use linear regression for
classification. For a binary (two-class) problem like the default
dataset, you could code the outcomes as numbers, for example:</p>
<ul>
<li>Default = ‘No’ =&gt; <span class="math inline">\(y = 0\)</span></li>
<li>Default = ‘Yes’ =&gt; <span class="math inline">\(y =
1\)</span></li>
</ul>
<p>You could then fit a standard linear regression model: <span
class="math inline">\(Y \approx \beta_0 + \beta_1 X\)</span>. In this
context, we would interpret the prediction <span
class="math inline">\(\hat{y}\)</span> as the <em>probability</em> of
default, so we’d be modeling <span class="math inline">\(P(Y=1|X) =
\beta_0 + \beta_1 X\)</span>.</p>
<p>However, this approach has two major problems:
然而，这种方法有两个主要问题： <strong>1. The Output Is Not a
Probability</strong> A linear model can produce outputs that are less
than 0 or greater than 1. This doesn’t make sense for a probability,
which must always be between 0 and 1.</p>
<p>The image below is the most important one for understanding this
issue. The left plot shows a linear regression line fit to the 0/1
default data. You can see the line goes below 0 and would eventually go
above 1 for higher balances. The right plot shows a logistic regression
curve, which always stays between 0 and 1.</p>
<ul>
<li><strong>Left (Linear Regression):</strong> The straight blue line
predicts probabilities &lt; 0 for low balances.</li>
<li><strong>Right (Logistic Regression):</strong> The S-shaped blue
curve correctly constrains the probability output between 0 and 1.</li>
</ul>
<p><strong>2. It Doesn’t Work for Multi-Class Problems</strong> If you
have more than two categories (e.g., ‘mild’, ‘moderate’, ‘severe’), you
might code them as 0, 1, and 2. A linear regression model would
incorrectly assume that the “distance” between ‘mild’ and ‘moderate’ is
the same as the distance between ‘moderate’ and ‘severe’, which is
usually not a valid assumption.</p>
<p><strong>1. 输出不是概率</strong> 线性模型可以产生小于 0 或大于 1
的输出。这对于概率来说毫无意义，因为概率必须始终介于 0 和 1 之间。</p>
<p>下图是理解这个问题最重要的图。左图显示了与 0/1
默认数据拟合的线性回归线。您可以看到，该线低于
0，并且最终会随着余额的增加而高于
1。右图显示了逻辑回归曲线，它始终保持在 0 和 1 之间。</p>
<ul>
<li><strong>左图（线性回归）：</strong>蓝色直线预测低余额的概率小于
0。</li>
<li><strong>右图（逻辑回归）：</strong>S
形蓝色曲线正确地将概率输出限制在 0 和 1 之间。</li>
</ul>
<p><strong>2.它不适用于多类别问题</strong>
如果您有两个以上的类别（例如，“轻度”、“中度”、“重度”），您可能会将它们编码为
0、1 和
2。线性回归模型会错误地假设“轻度”和“中度”之间的“距离”与“中度”和“重度”之间的距离相同，这通常不是一个有效的假设。</p>
<p>## The Solution: Logistic Regression</p>
<p>Instead of modeling the response <span
class="math inline">\(y\)</span> directly, logistic regression models
the <strong>probability</strong> that <span
class="math inline">\(y\)</span> belongs to a particular class. To solve
the issue of the output not being a probability, it uses the
<strong>logistic function</strong> (also known as the sigmoid
function).</p>
<p>This function takes any real-valued input and squeezes it into an
output between 0 and 1.</p>
<p>The formula for the probability in a logistic regression model is:
<span class="math display">\[P(Y=1|X) = \frac{e^{\beta_0 + \beta_1 X}}{1
+ e^{\beta_0 + \beta_1 X}}\]</span> This S-shaped function, shown in the
right-hand plot above, ensures that the output is always a valid
probability. We can then set a threshold (e.g., 0.5) to make the final
class prediction. If <span class="math inline">\(P(Y=1|X) &gt;
0.5\)</span>, we predict ‘Yes’; otherwise, we predict ‘No’.</p>
<p>## 解决方案：逻辑回归</p>
<p>逻辑回归不是直接对响应 <span class="math inline">\(y\)</span>
进行建模，而是对 <span class="math inline">\(y\)</span>
属于特定类别的<strong>概率</strong>进行建模。为了解决输出不是概率的问题，它使用了<strong>逻辑函数</strong>（也称为
S 型函数）。</p>
<p>此函数接受任何实值输入，并将其压缩为介于 0 和 1 之间的输出。</p>
<p>逻辑回归模型中的概率公式为： <span class="math display">\[P(Y=1|X) =
\frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}\]</span>
如上图右侧所示，这个 S
形函数确保输出始终是有效概率。然后，我们可以设置一个阈值（例如
0.5）来进行最终的类别预测。如果 <span class="math inline">\(P(Y=1|X)
&gt; 0.5\)</span>，则预测“是”；否则，预测“否”。</p>
<p>## Data Visualization &amp; Code in Python</p>
<p>The slides use R to visualize the data. The boxplots are particularly
important because they show which variable is a better predictor.</p>
<ul>
<li><p><strong>Balance vs. Default:</strong> The boxplots for balance
show a clear difference. The median balance for those who default
(‘Yes’) is much higher than for those who do not (‘No’). This suggests
<strong>balance is a strong predictor</strong>.</p></li>
<li><p><strong>Income vs. Default:</strong> The boxplots for income show
a lot of overlap. The median incomes for both groups are very similar.
This suggests <strong>income is a weak predictor</strong>.</p></li>
<li><p><strong>余额
vs. 违约</strong>：余额的箱线图显示出明显的差异。违约者（“是”）的余额中位数远高于未违约者（“否”）。这表明<strong>余额是一个强有力的预测指标</strong>。</p></li>
<li><p><strong>收入
vs. 违约</strong>：收入的箱线图显示出很大的重叠。两组的收入中位数非常相似。这表明<strong>收入是一个弱的预测指标</strong>。</p></li>
</ul>
<p>Here’s how you could perform similar analysis and modeling in Python
using <code>seaborn</code> and <code>scikit-learn</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;default_data.csv&#x27; has columns: &#x27;default&#x27; (Yes/No), &#x27;balance&#x27;, &#x27;income&#x27;</span></span><br><span class="line"><span class="comment"># You would load your data like this:</span></span><br><span class="line"><span class="comment"># df = pd.read_csv(&#x27;default_data.csv&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># For demonstration, let&#x27;s create some sample data</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;balance&#x27;</span>: [<span class="number">1200</span>, <span class="number">2100</span>, <span class="number">800</span>, <span class="number">1800</span>, <span class="number">500</span>, <span class="number">1600</span>, <span class="number">2200</span>, <span class="number">1900</span>],</span><br><span class="line">    <span class="string">&#x27;income&#x27;</span>: [<span class="number">45000</span>, <span class="number">60000</span>, <span class="number">30000</span>, <span class="number">55000</span>, <span class="number">25000</span>, <span class="number">48000</span>, <span class="number">70000</span>, <span class="number">65000</span>],</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>: [<span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. Data Visualization (like the slides) ---</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">14</span>, <span class="number">5</span>))</span><br><span class="line">fig.suptitle(<span class="string">&#x27;Predictor Analysis for Default&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boxplot for Balance</span></span><br><span class="line">sns.boxplot(ax=axes[<span class="number">0</span>], x=<span class="string">&#x27;default&#x27;</span>, y=<span class="string">&#x27;balance&#x27;</span>, data=df)</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&#x27;Balance vs. Default Status&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boxplot for Income</span></span><br><span class="line">sns.boxplot(ax=axes[<span class="number">1</span>], x=<span class="string">&#x27;default&#x27;</span>, y=<span class="string">&#x27;income&#x27;</span>, data=df)</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">&#x27;Income vs. Default Status&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. Logistic Regression Modeling ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert categorical &#x27;default&#x27; column to 0s and 1s</span></span><br><span class="line">df[<span class="string">&#x27;default_encoded&#x27;</span>] = df[<span class="string">&#x27;default&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x == <span class="string">&#x27;Yes&#x27;</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define features (X) and target (y)</span></span><br><span class="line">X = df[[<span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line">y = df[<span class="string">&#x27;default_encoded&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create and train the logistic regression model</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on new data</span></span><br><span class="line"><span class="comment"># For example, a person with a $2000 balance and $50,000 income</span></span><br><span class="line">new_customer = [[<span class="number">2000</span>, <span class="number">50000</span>]]</span><br><span class="line">predicted_prob = model.predict_proba(new_customer)</span><br><span class="line">prediction = model.predict(new_customer)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Customer data: Balance=2000, Income=50000&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Probability of No Default vs. Default: <span class="subst">&#123;predicted_prob&#125;</span>&quot;</span>) <span class="comment"># [[P(No), P(Yes)]]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Final Prediction (0=No, 1=Yes): <span class="subst">&#123;prediction&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="the-mathematical-foundation-of-logistic-regression">2. the
mathematical foundation of logistic regression</h1>
<p>This set of slides explains the mathematical foundation of logistic
regression, how its parameters are estimated using Maximum Likelihood
Estimation (MLE), and how an iterative algorithm called Newton-Raphson
is used to perform this estimation.</p>
<p>逻辑回归的数学基础、如何使用最大似然估计 (MLE)
估计其参数，以及如何使用名为 Newton-Raphson 的迭代算法进行估计。</p>
<h2
id="the-logistic-regression-model-from-probabilities-to-log-odds逻辑回归模型从概率到对数几率">2.1
The Logistic Regression Model: From Probabilities to
Log-Odds逻辑回归模型：从概率到对数几率</h2>
<p>The core of logistic regression is transforming a linear model into a
valid probability. This is done using the <strong>logistic
function</strong>, also known as the sigmoid function.
逻辑回归的核心是将线性模型转换为有效的概率。这可以通过<strong>逻辑函数</strong>（也称为
S 型函数）来实现。 #### <strong>Key Mathematical Formulas</strong></p>
<ol type="1">
<li><p><strong>Probability of Class 1:</strong> The model assumes the
probability of an observation <span
class="math inline">\(\mathbf{x}\)</span> belonging to class 1 is given
by the sigmoid function: <span class="math display">\[
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-\beta^T \mathbf{x})} =
\frac{\exp(\beta^T \mathbf{x})}{1 + \exp(\beta^T \mathbf{x})}
\]</span> This function always outputs a value between 0 and 1, making
it perfect for modeling probabilities.</p></li>
<li><p><strong>Odds:</strong> The odds are the ratio of the probability
of an event happening to the probability of it not happening. <span
class="math display">\[
\text{Odds} = \frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})} = \exp(\beta^T
\mathbf{x})
\]</span></p></li>
<li><p><strong>Log-Odds (Logit):</strong> By taking the natural
logarithm of the odds, we get a linear relationship with the predictors.
This is called the <strong>logit transformation</strong>. <span
class="math display">\[
\text{logit}(P(y=1|\mathbf{x})) =
\log\left(\frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})}\right) = \beta^T
\mathbf{x}
\]</span> This final equation is the heart of the model. It states that
the log-odds of the outcome are a linear function of the predictors.
This provides a great interpretation: a one-unit increase in a predictor
<span class="math inline">\(x_j\)</span> changes the log-odds by <span
class="math inline">\(\beta_j\)</span>.</p></li>
<li><p><strong>类别 1 的概率</strong>：该模型假设观测值 <span
class="math inline">\(\mathbf{x}\)</span> 属于类别 1 的概率由 S
型函数给出： <span class="math display">\[
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-\beta^T \mathbf{x})} =
\frac{\exp(\beta^T \mathbf{x})}{1 + \exp(\beta^T \mathbf{x})}
\]</span> 此函数的输出值始终介于 0 和 1
之间，非常适合用于概率建模。</p></li>
<li><p><strong>几率</strong>：**几率是事件发生的概率与不发生的概率之比。
<span class="math display">\[
\text{Odds} = \frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})} = \exp(\beta^T
\mathbf{x})
\]</span></p></li>
<li><p><strong>对数概率
(Logit)</strong>：通过对概率取自然对数，我们可以得到概率与预测变量之间的线性关系。这被称为<strong>logit
变换</strong>。 <span class="math display">\[
\text{logit}(P(y=1|\mathbf{x})) =
\log\left(\frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})}\right) = \beta^T
\mathbf{x}
\]</span>
最后一个方程是模型的核心。它指出结果的对数概率是预测变量的线性函数。这提供了一个很好的解释：预测变量
<span class="math inline">\(x_j\)</span>
每增加一个单位，对数概率就会改变 <span
class="math inline">\(\beta_j\)</span>。</p></li>
</ol>
<h2
id="fitting-the-model-maximum-likelihood-estimation-mle-拟合模型最大似然估计-mle">2.2
Fitting the Model: Maximum Likelihood Estimation (MLE)
拟合模型：最大似然估计 (MLE)</h2>
<p>Unlike linear regression, which uses least squares to find the
best-fit line, logistic regression uses <strong>Maximum Likelihood
Estimation (MLE)</strong>. The goal of MLE is to find the parameter
values (the <span class="math inline">\(\beta\)</span> coefficients)
that maximize the probability of observing the actual data that we have.
与使用最小二乘法寻找最佳拟合线的线性回归不同，逻辑回归使用<strong>最大似然估计
(MLE)</strong>。MLE
的目标是找到使观测到实际数据的概率最大化的参数值（<span
class="math inline">\(\beta\)</span> 系数）。</p>
<ol type="1">
<li><p><strong>Likelihood Function:</strong> This is the joint
probability of observing all the data points in our sample. Assuming
each observation is independent, it’s the product of the individual
probabilities:
1.<strong>似然函数</strong>：这是观测到样本中所有数据点的联合概率。假设每个观测值都是独立的，它是各个概率的乘积：
<span class="math display">\[
L(\beta) = \prod_{i=1}^{n} P(y_i|\mathbf{x}_i)
\]</span> A clever way to write this for a binary (0/1) outcome is:
<span class="math display">\[
L(\beta) = \prod_{i=1}^{n} \frac{\exp(y_i \beta^T \mathbf{x}_i)}{1 +
\exp(\beta^T \mathbf{x}_i)}
\]</span></p></li>
<li><p><strong>Log-Likelihood Function:</strong> Products are difficult
to work with mathematically, so we work with the logarithm of the
likelihood, which turns the product into a sum. Maximizing the
log-likelihood is the same as maximizing the likelihood.</p></li>
<li><p><strong>对数似然函数</strong>：乘积在数学上很难处理，所以我们使用似然的对数，将乘积转化为和。最大化对数似然与最大化似然相同。
<span class="math display">\[
\ell(\beta) = \log(L(\beta)) = \sum_{i=1}^{n} \left[ y_i \beta^T
\mathbf{x}_i - \log(1 + \exp(\beta^T \mathbf{x}_i)) \right]
\]</span> <strong>Key Takeaway:</strong> The slides correctly state that
there is <strong>no explicit formula</strong> to solve for the <span
class="math inline">\(\hat{\beta}\)</span> that maximizes this function.
We must find it using a numerical optimization algorithm.
没有<strong>明确的公式</strong>来求解最大化该函数的<span
class="math inline">\(\hat{\beta}\)</span>。我们必须使用数值优化算法来找到它。</p></li>
</ol>
<h2 id="the-algorithm-newton-raphson-算法牛顿-拉夫森算法">2.3 The
Algorithm: Newton-Raphson 算法：牛顿-拉夫森算法</h2>
<p>The slides introduce the <strong>Newton-Raphson algorithm</strong> as
the method to find the optimal <span
class="math inline">\(\hat{\beta}\)</span>. It’s an efficient iterative
algorithm for finding the roots of a function (i.e., where <span
class="math inline">\(f(x)=0\)</span>).</p>
<p><strong>How does this apply to logistic regression?</strong> To
maximize the log-likelihood function <span
class="math inline">\(\ell(\beta)\)</span>, we need to find the point
where its derivative (gradient) is equal to zero. So, Newton-Raphson is
used to solve <span class="math inline">\(\frac{d\ell(\beta)}{d\beta} =
0\)</span>.</p>
<p>它是一种高效的迭代算法，用于求函数的根（即，当<span
class="math inline">\(f(x)=0\)</span>时）。</p>
<p><strong>这如何应用于逻辑回归？</strong> 为了最大化对数似然函数 <span
class="math inline">\(\ell(\beta)\)</span>，我们需要找到其导数（梯度）等于零的点。因此，牛顿-拉夫森法用于求解
<span class="math inline">\(\frac{d\ell(\beta)}{d\beta} =
0\)</span>。</p>
<h4 id="the-general-newton-raphson-method"><strong>The General
Newton-Raphson Method</strong></h4>
<p>The algorithm starts with an initial guess, <span
class="math inline">\(x^{old}\)</span>, and iteratively refines it using
the following update rule, which is based on a Taylor series
approximation: <span class="math display">\[
x^{new} = x^{old} - \frac{f(x^{old})}{f&#39;(x^{old})}
\]</span> where <span class="math inline">\(f&#39;(x)\)</span> is the
derivative of <span class="math inline">\(f(x)\)</span>. You repeat this
step until the value of <span class="math inline">\(x\)</span>
converges.</p>
<p>该算法从初始估计 <span class="math inline">\(x^{old}\)</span>
开始，并使用以下基于泰勒级数近似的更新规则迭代地对其进行优化： <span
class="math display">\[
x^{new} = x^{old} - \frac{f(x^{old})}{f&#39;(x^{old})}
\]</span> 其中 <span class="math inline">\(f&#39;(x)\)</span> 是 <span
class="math inline">\(f(x)\)</span> 的导数。重复此步骤，直到 <span
class="math inline">\(x\)</span> 的值收敛。</p>
<h4
id="important-image-newton-raphson-example-x3---4-0"><strong>Important
Image: Newton-Raphson Example (<span class="math inline">\(x^3 - 4 =
0\)</span>)</strong></h4>
<p>[Image showing iterations of Newton-Raphson]</p>
<p>This slide is a great illustration of the algorithm’s power. *
<strong>Goal:</strong> Find <span class="math inline">\(x\)</span> such
that <span class="math inline">\(f(x) = x^3 - 4 = 0\)</span>. *
<strong>Function:</strong> <span class="math inline">\(f(x) = x^3 -
4\)</span> * <strong>Derivative:</strong> <span
class="math inline">\(f&#39;(x) = 3x^2\)</span> * <strong>Update
Rule:</strong> <span class="math inline">\(x^{new} = x^{old} -
\frac{(x^{old})^3 - 4}{3(x^{old})^2}\)</span> Starting with a guess of
<span class="math inline">\(x^{old} = 2\)</span>, the algorithm
converges to the true answer (<span class="math inline">\(4^{1/3}
\approx 1.5874\)</span>) in just 4 steps.</p>
<ul>
<li><strong>目标</strong>：找到 <span
class="math inline">\(x\)</span>，使得 <span class="math inline">\(f(x)
= x^3 - 4 = 0\)</span>。</li>
<li><strong>函数</strong>：<span class="math inline">\(f(x) = x^3 -
4\)</span></li>
<li><strong>导数</strong>：<span class="math inline">\(f&#39;(x) =
3x^2\)</span></li>
<li><strong>更新规则</strong>：<span class="math inline">\(x^{new} =
x^{old} - \frac{(x^{old})^3 - 4}{3(x^{old})^2}\)</span> 从 <span
class="math inline">\(x^{old} = 2\)</span> 的猜测开始，该算法仅用 4
步就收敛到真实答案 (<span class="math inline">\(4^{1/3} \approx
1.5874\)</span>)。</li>
</ul>
<h4 id="code-understanding-python"><strong>Code Understanding
(Python)</strong></h4>
<p>The slides show Python code implementing Newton-Raphson. Let’s break
down the key function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the function we want to find the root of</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) - x*x + <span class="number">3</span> * np.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define its derivative</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_prime</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) - <span class="number">2</span>*x + <span class="number">3</span> * np.cos(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Newton-Raphson method</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">newton_raphson</span>(<span class="params">x0, tol=<span class="number">1e-10</span>, max_iter=<span class="number">100</span></span>):</span><br><span class="line">    x = x0 <span class="comment"># Start with the initial guess</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        fx = f(x)      <span class="comment"># Calculate f(x_old)</span></span><br><span class="line">        fpx = f_prime(x) <span class="comment"># Calculate f&#x27;(x_old)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> fpx == <span class="number">0</span>: <span class="comment"># Cannot divide by zero</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Zero derivative. No solution found.&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is the core update rule</span></span><br><span class="line">        x_new = x - fx / fpx</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check if the change is small enough to stop</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(x_new - x) &lt; tol:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Converged to <span class="subst">&#123;x_new&#125;</span> after <span class="subst">&#123;i+<span class="number">1</span>&#125;</span> iterations.&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> x_new</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update x for the next iteration</span></span><br><span class="line">        x = x_new</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Exceeded maximum iterations. No solution found.&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial guess and execution</span></span><br><span class="line">x0 = <span class="number">0.5</span></span><br><span class="line">root = newton_raphson(x0)</span><br></pre></td></tr></table></figure>
<p>The slides show that with a good initial guess
(<code>x0 = 0.5</code>), the algorithm converges quickly. With a bad one
(<code>x0 = 50</code>), it still converges but takes many more steps.
This highlights the importance of the starting point. The slides also
show an implementation of <strong>Gradient Descent</strong>, another
popular optimization algorithm which uses the update rule
<code>x_new = x - learning_rate * gradient</code>.</p>
<h1
id="provide-a-great-case-study-on-logistic-regression-particularly-on-the-important-concept-of-confounding-variables.-heres-a-summary-covering-the-math-code-and-key-insights.">Provide
a great case study on logistic regression, particularly on the important
concept of confounding variables. Here’s a summary covering the math,
code, and key insights.</h1>
<ol start="3" type="1">
<li><h1 id="core-concept-logistic-regression-核心概念逻辑回归">Core
Concept: Logistic Regression 📈 # 核心概念：逻辑回归 📈</h1></li>
</ol>
<p>Logistic regression is a statistical method used for <strong>binary
classification</strong>, which means predicting an outcome that can only
be one of two things (e.g., Yes/No, True/False, 1/0).</p>
<p>In this example, the goal is to predict the probability that a
customer will <strong>default</strong> on a loan (Yes or No) based on
factors like their account <code>balance</code>, <code>income</code>,
and whether they are a <code>student</code>.</p>
<p>The core of logistic regression is the <strong>sigmoid (or logistic)
function</strong>, which takes any real-valued number and squishes it to
a value between 0 and 1, representing a probability.</p>
<p><span class="math display">\[
\hat{P}(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_p
X_p)}}
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{P}(Y=1|X)\)</span> is the predicted
probability of the outcome being “Yes” (e.g., default).</li>
<li><span class="math inline">\(\beta_0\)</span> is the intercept.</li>
<li><span class="math inline">\(\beta_1, ..., \beta_p\)</span> are the
coefficients for each input variable (<span class="math inline">\(X_1,
..., X_p\)</span>). The model’s job is to find the best values for these
<span class="math inline">\(\beta\)</span> coefficients.</li>
</ul>
<hr />
<p>逻辑回归是一种用于<strong>二元分类</strong>的统计方法，这意味着预测结果只能是两种情况之一（例如，是/否、真/假、1/0）。</p>
<p>在本例中，目标是根据客户账户“余额”、“收入”以及是否为“学生”等因素，预测客户<strong>拖欠</strong>贷款（是或否）的概率。</p>
<p>逻辑回归的核心是<strong>Sigmoid（或逻辑）函数</strong>，它将任何实数压缩为介于
0 和 1 之间的值，以表示概率。</p>
<p><span class="math display">\[
\hat{P}(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_p
X_p)}}
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{P}(Y=1|X)\)</span>
是结果为“是”（例如，默认）的预测概率。</li>
<li><span class="math inline">\(\beta_0\)</span> 是截距。</li>
<li><span class="math inline">\(\beta_1, ..., \beta_p\)</span>
是每个输入变量 (<span class="math inline">\(X_1, ..., X_p\)</span>)
的系数。模型的任务是找到这些 <span class="math inline">\(\beta\)</span>
系数的最佳值。</li>
</ul>
<h2 id="how-the-model-learns-mathematical-foundation">3.1 How the Model
“Learns” (Mathematical Foundation)</h2>
<p>The slides show that the model’s coefficients (<span
class="math inline">\(\beta\)</span>) are found using an algorithm like
<strong>Newton-Raphson</strong>. This is an iterative process to find
the values that <strong>maximize the log-likelihood function</strong>.
Think of this as finding the coefficient values that make the observed
data most
probable.这是一个迭代过程，用于查找<strong>最大化对数似然函数</strong>的值。可以将其视为查找使观测数据概率最大的系数值。</p>
<p>The key slide for this is the one titled “Newton-Raphson Iterative
Algorithm”. It shows the formulas for: * The <strong>Gradient</strong>
(<span class="math inline">\(\nabla\ell\)</span>): The direction of the
steepest ascent of the log-likelihood function. * The
<strong>Hessian</strong> (<span class="math inline">\(H\)</span>): The
curvature of the log-likelihood function.</p>
<ul>
<li><strong>梯度</strong> (<span
class="math inline">\(\nabla\ell\)</span>)：对数似然函数最陡上升的方向。</li>
<li><strong>黑森矩阵</strong> (<span
class="math inline">\(H\)</span>)：对数似然函数的曲率。</li>
</ul>
<p>The updating rule is given by: <span class="math display">\[
\beta^{new} = \beta^{old} - H^{-1}\nabla\ell
\]</span> This formula is used repeatedly until the coefficient values
stop changing significantly, meaning the algorithm has converged to the
best fit. This process is also referred to as <strong>Iteratively
Reweighted Least Squares (IRLS)</strong>.
此公式反复使用，直到系数值不再发生显著变化，这意味着算法已收敛到最佳拟合值。此过程也称为<strong>迭代重加权最小二乘法
(IRLS)</strong>。</p>
<hr />
<h2 id="the-puzzle-a-tale-of-two-models">3.2 The Puzzle: A Tale of Two
Models 🕵️‍♂️</h2>
<p>The most important story in these slides is how the effect of being a
student changes depending on the model. This is a classic example of a
<strong>confounding variable</strong>.</p>
<h4 id="model-1-simple-logistic-regression-default-vs.-student">Model 1:
Simple Logistic Regression (Default vs. Student)</h4>
<p>When predicting default using <em>only</em> student status, the model
is: <code>default ~ student</code></p>
<p>From the slides, the coefficients are: * Intercept (<span
class="math inline">\(\beta_0\)</span>): -3.5041 * student[Yes] (<span
class="math inline">\(\beta_1\)</span>): <strong>0.4049</strong>
(positive)</p>
<p>The equation for the log-odds is: <span class="math display">\[
\log\left(\frac{P(\text{default})}{1-P(\text{default})}\right) = -3.5041
+ 0.4049 \times (\text{is\_student})
\]</span></p>
<p><strong>Conclusion:</strong> The positive coefficient (0.4049)
suggests that <strong>students are more likely to default</strong> than
non-students. The slides calculate the probabilities: * <strong>Student
Default Probability:</strong> 4.31% * <strong>Non-Student Default
Probability:</strong> 2.92%</p>
<p>学生身份的影响如何根据模型而变化。这是一个典型的<strong>混杂变量</strong>的例子。</p>
<h4 id="模型-1简单逻辑回归违约-vs.-学生">模型 1：简单逻辑回归（违约
vs. 学生）</h4>
<p>仅使用学生身份预测违约时，模型为： <code>default ~ student</code></p>
<p>幻灯片中显示的系数为： * 截距 (<span
class="math inline">\(\beta_0\)</span>): -3.5041 * 学生[是] (<span
class="math inline">\(\beta_1\)</span>):
<strong>0.4049</strong>（正）</p>
<p>对数概率公式为： <span class="math display">\[
\log\left(\frac{P(\text{default})}{1-P(\text{default})}\right) = -3.5041
+ 0.4049 \times (\text{is\_student})
\]</span></p>
<p><strong>结论</strong>：正系数 (0.4049)
表明<strong>学生比非学生更有可能违约</strong>。幻灯片计算了以下概率： *
<strong>学生违约概率</strong>：4.31% *
<strong>非学生违约概率</strong>：2.92%</p>
<h2
id="model-2-multiple-logistic-regression-default-vs.-all-variables-模型-2多元逻辑回归违约-vs.-所有变量">3.3
Model 2: Multiple Logistic Regression (Default vs. All Variables) 模型
2：多元逻辑回归（违约 vs. 所有变量）</h2>
<p>When we add <code>balance</code> and <code>income</code> to the
model, it becomes: <code>default ~ student + balance + income</code></p>
<p>From the slides, the new coefficients are: * Intercept (<span
class="math inline">\(\beta_0\)</span>): -10.8690 * balance (<span
class="math inline">\(\beta_1\)</span>): 0.0057 * income (<span
class="math inline">\(\beta_2\)</span>): 0.0030 * student[Yes] (<span
class="math inline">\(\beta_3\)</span>): <strong>-0.6468</strong>
(negative)</p>
<p><strong>The Shocking Twist!</strong> The coefficient for
<code>student[Yes]</code> is now <strong>negative</strong>.</p>
<p><strong>Conclusion:</strong> When we control for balance and income,
<strong>students are actually <em>less</em> likely to default</strong>
than non-students with the same balance and income.</p>
<h4 id="why-the-change-the-confounding-variable-explained">Why the
Change? The Confounding Variable Explained</h4>
<p>The key insight, explained on the slide with multi-colored text
bubbles, is that <strong>students, on average, have higher credit card
balances</strong>.</p>
<ul>
<li>In the simple model, the <code>student</code> variable was
inadvertently capturing the risk associated with having a high
<code>balance</code>. The model mistakenly concluded “being a student
causes default.”</li>
<li>In the multiple model, the <code>balance</code> variable properly
accounts for the risk from a high balance. With that effect isolated,
the <code>student</code> variable can show its true, underlying
relationship with default, which is negative.</li>
</ul>
<p>This demonstrates why it’s crucial to consider multiple relevant
variables to avoid drawing incorrect conclusions. <strong>The most
important slides are the ones that present this paradox and its
explanation.</strong></p>
<p><strong>令人震惊的转折！</strong> <code>student[Yes]</code>
的系数现在为<strong>负</strong>。</p>
<p><strong>结论：</strong>当我们控制余额和收入时，<strong>学生实际上比具有相同余额和收入的非学生更<em>低</em>于违约</strong>。</p>
<h4 id="为什么会有变化混杂变量解释">为什么会有变化？混杂变量解释</h4>
<p>幻灯片上用彩色文字气泡解释了关键的见解，即<strong>学生平均拥有更高的信用卡余额</strong>。</p>
<ul>
<li>在简单模型中，“学生”变量无意中捕捉到了高余额带来的风险。该模型错误地得出了“学生身份导致违约”的结论。</li>
<li>在多元模型中，“余额”变量正确地解释了高余额带来的风险。在分离出这一影响后，“学生”变量可以显示其与违约之间真实的潜在关系，即负相关关系。</li>
</ul>
<p>这说明了为什么考虑多个相关变量以避免得出错误结论至关重要。</p>
<hr />
<h3 id="code-implementation-r-vs.-python">Code Implementation: R
vs. Python</h3>
<p>The slides use R’s <code>glm()</code> (Generalized Linear Model)
function. Here’s how you would replicate this in Python.</p>
<h4 id="r-code-from-slides">R Code (from slides)</h4>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Simple Model</span></span><br><span class="line">glmod2 <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>default <span class="operator">~</span> student<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> family<span class="operator">=</span>binomial<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>glmod2<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model</span></span><br><span class="line">glmod3 <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>default <span class="operator">~</span> .<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> family<span class="operator">=</span>binomial<span class="punctuation">)</span> <span class="comment"># &#x27;.&#x27; means all other variables</span></span><br><span class="line">summary<span class="punctuation">(</span>glmod3<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<h4 id="python-equivalent">Python Equivalent</h4>
<p>We can use two popular libraries: <code>statsmodels</code> (which
gives R-style summaries) and <code>scikit-learn</code> (the standard for
machine learning).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Default&#x27; is a pandas DataFrame with columns:</span></span><br><span class="line"><span class="comment"># &#x27;default&#x27; (0/1), &#x27;student&#x27; (0/1), &#x27;balance&#x27;, &#x27;income&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Using statsmodels (recommended for interpretation) ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data</span></span><br><span class="line"><span class="comment"># For statsmodels, we need to manually add the intercept</span></span><br><span class="line">X_simple = Default[[<span class="string">&#x27;student&#x27;</span>]]</span><br><span class="line">X_simple = sm.add_constant(X_simple)</span><br><span class="line">y = Default[<span class="string">&#x27;default&#x27;</span>]</span><br><span class="line"></span><br><span class="line">X_multiple = Default[[<span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line">X_multiple = sm.add_constant(X_multiple)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simple Model: default ~ student</span></span><br><span class="line">model_simple = sm.Logit(y, X_simple).fit()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- Simple Model ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_simple.summary())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model: default ~ student + balance + income</span></span><br><span class="line">model_multiple = sm.Logit(y, X_multiple).fit()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Multiple Model ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_multiple.summary())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Using scikit-learn (recommended for prediction tasks) ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data (scikit-learn adds intercept by default)</span></span><br><span class="line">X_simple_sk = Default[[<span class="string">&#x27;student&#x27;</span>]]</span><br><span class="line">y_sk = Default[<span class="string">&#x27;default&#x27;</span>]</span><br><span class="line"></span><br><span class="line">X_multiple_sk = Default[[<span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simple Model</span></span><br><span class="line">clf_simple = LogisticRegression().fit(X_simple_sk, y_sk)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nSimple Model Intercept (scikit-learn): <span class="subst">&#123;clf_simple.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Simple Model Coefficient (scikit-learn): <span class="subst">&#123;clf_simple.coef_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model</span></span><br><span class="line">clf_multiple = LogisticRegression().fit(X_multiple_sk, y_sk)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nMultiple Model Intercept (scikit-learn): <span class="subst">&#123;clf_multiple.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Multiple Model Coefficients (scikit-learn): <span class="subst">&#123;clf_multiple.coef_&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1
id="making-predictions-and-the-decision-boundary-进行预测和决策边界">4
Making Predictions and the Decision Boundary 🎯进行预测和决策边界</h1>
<p>Once the model is trained (i.e., we have the coefficients <span
class="math inline">\(\hat{\beta}\)</span>), we can make predictions.
一旦模型训练完成（即，我们有了系数 <span
class="math inline">\(\hat{\beta}\)</span>），我们就可以进行预测了。 ##
Math Behind Predictions</p>
<p>The model outputs the <strong>log-odds</strong>, which can be
converted into a probability. A key concept is the <strong>decision
boundary</strong>, which is the threshold where the model is uncertain
(probability = 50%).
模型输出<strong>对数概率</strong>，它可以转换为概率。一个关键概念是<strong>决策边界</strong>，它是模型不确定的阈值（概率
= 50%）。</p>
<ol type="1">
<li><p><strong>The Estimated Odds</strong>: The core output of the
linear part of the model is the exponential of the linear equation,
which gives the odds of the outcome being ‘Yes’ (or 1).
<strong>估计概率</strong>：模型线性部分的核心输出是线性方程的指数，它给出了结果为“是”（或
1）的概率。</p>
<p><span class="math display">\[
\]</span>$$\frac{\hat{P}(y=1|\mathbf{x}_0)}{\hat{P}(y=0|\mathbf{x}_0)} =
\exp(\hat{\beta}^\top \mathbf{x}_0)</p>
<p><span class="math display">\[
\]</span><span class="math display">\[
\]</span></p></li>
<li><p><strong>The Decision Rule</strong>: We classify a new observation
<span class="math inline">\(\mathbf{x}_0\)</span> by comparing its
predicted odds to a threshold <span
class="math inline">\(\delta\)</span>.
<strong>决策规则</strong>：我们通过比较新观测值 <span
class="math inline">\(\mathbf{x}_0\)</span> 的预测概率与阈值 <span
class="math inline">\(\delta\)</span> 来对其进行分类。</p>
<ul>
<li>Predict <span class="math inline">\(y=1\)</span> if <span
class="math inline">\(\exp(\hat{\beta}^\top \mathbf{x}_0) &gt;
\delta\)</span></li>
<li>Predict <span class="math inline">\(y=0\)</span> if <span
class="math inline">\(\exp(\hat{\beta}^\top \mathbf{x}_0) &lt;
\delta\)</span> A common default is <span
class="math inline">\(\delta=1\)</span>, which means we predict ‘Yes’ if
the probability is greater than 0.5.</li>
</ul></li>
<li><p><strong>The Linear Boundary</strong>: The decision boundary
itself is where the odds are exactly equal to the threshold. By taking
the logarithm, we see that this boundary is a <strong>linear
equation</strong>. This is why logistic regression is called a
<strong>linear classifier</strong>.
<strong>线性边界</strong>：决策边界本身就是概率恰好等于阈值的地方。取对数后，我们发现这个边界是一个<strong>线性方程</strong>。这就是逻辑回归被称为<strong>线性分类器</strong>的原因。
<span class="math display">\[
\]</span>$$\hat{\beta}^\top \mathbf{x} = \log(\delta)</p>
<p><span class="math display">\[
\]</span>$$For <span class="math inline">\(\delta=1\)</span>, the
boundary is simply <span class="math inline">\(\hat{\beta}^\top
\mathbf{x} = 0\)</span>.</p></li>
</ol>
<p>This concept is visualized perfectly in the slide titled “Linear
Classifier,” which shows a straight line neatly separating two classes
of data points.
题为“线性分类器”的幻灯片完美地展示了这一概念，它展示了一条直线，将两类数据点巧妙地分隔开来。</p>
<h2 id="visualizing-the-confounding-effect">Visualizing the Confounding
Effect</h2>
<p>The most important image in this set is <strong>Figure 4.3</strong>,
as it visually explains the confounding puzzle from the first set of
slides.</p>
<ul>
<li><strong>Right Panel (Boxplots)</strong>: This shows that
<strong>students (Yes) tend to have higher credit card balances</strong>
than non-students (No). This is the source of the confounding.</li>
<li><strong>Left Panel (Default Rates)</strong>:
<ul>
<li>The <strong>dashed lines</strong> show the <em>overall</em> default
rates. The orange line (students) is higher than the blue line
(non-students). This matches our simple model
(<code>default ~ student</code>).</li>
<li>The <strong>solid S-shaped curves</strong> show the probability of
default as a function of balance. For any <em>given</em> balance, the
blue curve (non-students) is slightly higher than the orange curve
(students). This means that <strong>at the same level of debt, students
are <em>less</em> likely to default</strong>. This matches our multiple
regression model
(<code>default ~ student + balance + income</code>).</li>
</ul></li>
</ul>
<p>This single figure brilliantly illustrates how a variable can appear
to have one effect in isolation but the opposite effect when controlling
for a confounding factor. *
<strong>右侧面板（箱线图）</strong>：这表明<strong>学生（是）的信用卡余额往往高于非学生（否）。这就是混杂效应的根源。
* </strong>左图（违约率）<strong>： *
</strong>虚线<strong>显示<em>总体</em>违约率。橙色线（学生）高于蓝色线（非学生）。这与我们的简单模型（“违约
~ 学生”）相符。 * </strong>S
形实线<strong>显示违约概率与余额的关系。对于任何<em>给定</em>的余额，蓝色曲线（非学生）略高于橙色曲线（学生）。这意味着</strong>在相同的债务水平下，学生违约的可能性<em>较小</em>。这与我们的多元回归模型（“违约
~ 学生 + 余额 + 收入”）相符。</p>
<p>这张图巧妙地说明了为什么一个变量在单独使用时似乎会产生一种影响，但在控制混杂因素后却会产生相反的影响。</p>
<h2 id="an-important-edge-case-perfect-separation">An Important Edge
Case: Perfect Separation ⚠️</h2>
<p>What happens if the data can be perfectly separated by a straight
line? 如果数据可以用一条直线完美分离，会发生什么？</p>
<p>One might think this is the ideal scenario, but it causes a problem
for the logistic regression algorithm. The model will try to find
coefficients that make the probabilities for each class as close to 1
and 0 as possible. To do this, the magnitude of the coefficients (<span
class="math inline">\(\hat{\beta}\)</span>) must grow infinitely large.
人们可能认为这是理想情况，但它会给逻辑回归算法带来问题。模型会尝试找到使每个类别的概率尽可能接近
1 和 0 的系数。为此，系数 (<span
class="math inline">\(\hat{\beta}\)</span>) 的大小必须无限大。</p>
<p>The slide “Non-convergence for perfectly separated case” demonstrates
this:</p>
<ul>
<li><p><strong>The Code</strong>: It generates two distinct,
non-overlapping clusters of data points using Python’s
<code>scikit-learn</code>.</p></li>
<li><p><strong>Parameter Estimates Graph</strong>: It shows the
<code>Intercept</code>, <code>Coefficient 1</code>, and
<code>Coefficient 2</code> values increasing or decreasing without limit
as the algorithm runs through more iterations. They never converge to a
stable value.</p></li>
<li><p><strong>Decision Boundary Graph</strong>: The decision boundary
itself might look reasonable, but the underlying coefficients are
unstable.</p></li>
<li><p><strong>代码</strong>：它使用 Python 的 <code>scikit-learn</code>
生成两个不同的、不重叠的数据点聚类。</p></li>
<li><p><strong>参数估计图</strong>：它显示“截距”、“系数 1”和“系数
2”的值随着算法迭代次数的增加或减少而无限增大或减小。它们永远不会收敛到一个稳定的值。</p></li>
<li><p><strong>决策边界图</strong>：决策边界本身可能看起来合理，但底层系数是不稳定的。</p></li>
</ul>
<p><strong>Key Takeaway</strong>: If your logistic regression model
fails to converge, the first thing you should check for is perfect
separation in your training data.
<strong>关键要点</strong>：如果您的逻辑回归模型未能收敛，您应该检查的第一件事就是训练数据是否完美分离。</p>
<h2 id="code-understanding">Code Understanding</h2>
<p>The slides provide useful code snippets in both R and Python.</p>
<h2 id="r-code-plotting-predictions">R Code (Plotting Predictions)</h2>
<p>This code generates the plot with the two S-shaped curves (one for
students, one for non-students) showing the probability of default as
balance increases.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Create a data frame for prediction with a range of balances</span></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># One version for students, one for non-students</span></span><br><span class="line">Default.st <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>balance<span class="operator">=</span>seq<span class="punctuation">(</span><span class="number">500</span><span class="punctuation">,</span> <span class="number">2500</span><span class="punctuation">,</span> by<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> student<span class="operator">=</span><span class="string">&quot;Yes&quot;</span><span class="punctuation">)</span></span><br><span class="line">Default.nonst <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>balance<span class="operator">=</span>seq<span class="punctuation">(</span><span class="number">500</span><span class="punctuation">,</span> <span class="number">2500</span><span class="punctuation">,</span> by<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> student<span class="operator">=</span><span class="string">&quot;No&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Use the trained multiple regression model (glmod3) to predict probabilities</span></span><br><span class="line">pred.st <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>glmod3<span class="punctuation">,</span> Default.st<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line">pred.nonst <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>glmod3<span class="punctuation">,</span> Default.nonst<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Plot the results</span></span><br><span class="line">plot<span class="punctuation">(</span>Default.st<span class="operator">$</span>balance<span class="punctuation">,</span> pred.st<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;l&quot;</span><span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;red&quot;</span><span class="punctuation">,</span> ...<span class="punctuation">)</span> <span class="operator">/</span><span class="operator">/</span> Students</span><br><span class="line">lines<span class="punctuation">(</span>Default.nonst<span class="operator">$</span>balance<span class="punctuation">,</span> pred.nonst<span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;blue&quot;</span><span class="punctuation">,</span> ...<span class="punctuation">)</span> <span class="operator">/</span><span class="operator">/</span> Non<span class="operator">-</span>students</span><br></pre></td></tr></table></figure>
<h4 id="python-code-visualizing-the-decision-boundary">Python Code
(Visualizing the Decision Boundary)</h4>
<p>This Python code uses <code>scikit-learn</code> and
<code>matplotlib</code> to create the plot showing the linear decision
boundary.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Generate synthetic data with two classes</span></span><br><span class="line">X, y = make_classification(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Initialize and fit the logistic regression model</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create a mesh grid of points to make predictions over the entire plot area</span></span><br><span class="line">xx, yy = np.meshgrid(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Predict the probability for each point on the grid</span></span><br><span class="line">probs = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Plot the decision boundary where the probability is 0.5</span></span><br><span class="line">plt.contour(xx, yy, probs.reshape(xx.shape), levels=[<span class="number">0.5</span>], ...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. Scatter plot the actual data points</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, ...)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="other-important-remarks">Other Important Remarks</h3>
<p>The “Remarks” slide briefly mentions some key extensions:</p>
<ul>
<li><p><strong>Probit Model</strong>: An alternative to logistic
regression that uses the cumulative distribution function (CDF) of the
standard normal distribution instead of the sigmoid function. The
results are often very similar.</p></li>
<li><p><strong>Softmax Regression</strong>: An extension of logistic
regression used for multi-class classification (when there are more than
two possible outcomes).</p></li>
<li><p><strong>Probit
模型</strong>：逻辑回归的替代方法，它使用标准正态分布的累积分布函数
(CDF) 代替 S 型函数。结果通常非常相似。</p></li>
<li><p><strong>Softmax
回归</strong>：逻辑回归的扩展，用于多类分类（当存在两个以上可能结果时）。</p></li>
</ul>
<h1
id="here-is-a-summary-of-the-slides-on-linear-discriminant-analysis-lda-including-the-key-mathematical-formulas-visual-explanations-and-how-to-implement-it-in-python.">5.
Here is a summary of the slides on Linear Discriminant Analysis (LDA),
including the key mathematical formulas, visual explanations, and how to
implement it in Python.</h1>
<h2
id="the-main-idea-classification-using-probabilities-使用概率进行分类">The
Main Idea: Classification Using Probabilities 使用概率进行分类</h2>
<p>Linear Discriminant Analysis (LDA) is a classification method. For a
given input <strong>x</strong>, it calculates the probability that
<strong>x</strong> belongs to each class and then assigns
<strong>x</strong> to the class with the <strong>highest
probability</strong>.</p>
<p>It does this using <strong>Bayes’ Theorem</strong>, which provides a
formula for the posterior probability <span class="math inline">\(P(Y=k
| X=x)\)</span>, or the probability that the class is <span
class="math inline">\(k\)</span> given the input <span
class="math inline">\(x\)</span>. 线性判别分析 (LDA)
是一种分类方法。对于给定的输入 <strong>x</strong>，它计算
<strong>x</strong> 属于每个类别的概率，然后将 <strong>x</strong>
分配给<strong>概率最高</strong>的类别。</p>
<p>它使用<strong>贝叶斯定理</strong>来实现这一点，该定理提供了后验概率
<span class="math inline">\(P(Y=k | X=x)\)</span> 的公式，即给定输入
<span class="math inline">\(x\)</span>，该类别属于 <span
class="math inline">\(k\)</span> 的概率。 <span class="math display">\[
p_k(x) = P(Y=k|X=x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)}
\]</span></p>
<ul>
<li><span class="math inline">\(p_k(x)\)</span> is the <strong>posterior
probability</strong> we want to maximize.</li>
<li><span class="math inline">\(\pi_k = P(Y=k)\)</span> is the
<strong>prior probability</strong> of class <span
class="math inline">\(k\)</span> (how common the class is overall).</li>
<li><span class="math inline">\(f_k(x) = f(x|Y=k)\)</span> is the
<strong>class-conditional probability density function</strong> of
observing input <span class="math inline">\(x\)</span> if it belongs to
class <span class="math inline">\(k\)</span>.</li>
</ul>
<p>To classify a new observation <span class="math inline">\(x\)</span>,
we simply find the class <span class="math inline">\(k\)</span> that
makes <span class="math inline">\(p_k(x)\)</span> the largest.
为了对新的观察值 <span class="math inline">\(x\)</span>
进行分类，我们只需找到使 <span class="math inline">\(p_k(x)\)</span>
最大的类别 <span class="math inline">\(k\)</span> 即可。</p>
<hr />
<h2 id="key-assumptions-of-lda">Key Assumptions of LDA</h2>
<p>LDA’s power comes from a specific, simplifying assumption about the
data’s distribution. LDA
的强大之处在于它对数据分布进行了特定的简化假设。</p>
<ol type="1">
<li><p><strong>Gaussian Distribution:</strong> LDA assumes that the data
within each class <span class="math inline">\(k\)</span> follows a
p-dimensional multivariate normal (or Gaussian) distribution, denoted as
<span class="math inline">\(X|Y=k \sim \mathcal{N}(\mu_k,
\Sigma)\)</span>.</p></li>
<li><p><strong>Common Covariance:</strong> A crucial assumption is that
all classes share the <strong>same covariance matrix</strong> <span
class="math inline">\(\Sigma\)</span>. This means that while the classes
may have different centers (means, <span
class="math inline">\(\mu_k\)</span>), their shape and orientation
(covariance, <span class="math inline">\(\Sigma\)</span>) are
identical.</p></li>
<li><p><strong>高斯分布</strong>：LDA 假设每个类 <span
class="math inline">\(k\)</span> 中的数据服从 p
维多元正态（或高斯）分布，表示为 <span class="math inline">\(X|Y=k \sim
\mathcal{N}(\mu_k, \Sigma)\)</span>。</p></li>
<li><p><strong>共同协方差</strong>：一个关键假设是所有类共享<strong>相同的协方差矩阵</strong>
<span
class="math inline">\(\Sigma\)</span>。这意味着虽然类可能具有不同的中心（均值，<span
class="math inline">\(\mu_k\)</span>），但它们的形状和方向（协方差，<span
class="math inline">\(\Sigma\)</span>）是相同的。</p></li>
</ol>
<p>The probability density function for a class <span
class="math inline">\(k\)</span> is: <span class="math display">\[
f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp \left( -\frac{1}{2}(x
- \mu_k)^T \Sigma^{-1} (x - \mu_k) \right)
\]</span></p>
<p>The image above (from your slide “Knowing normal distribution”)
illustrates this. The two “bells” have different centers (different
<span class="math inline">\(\mu_k\)</span>) but similar shapes. The one
on the right is “tilted,” indicating correlation between variables,
which is captured in the shared covariance matrix <span
class="math inline">\(\Sigma\)</span>.
上图（摘自幻灯片“了解正态分布”）说明了这一点。两个“钟”形的中心不同（<span
class="math inline">\(\mu_k\)</span>
不同），但形状相似。右边的钟形“倾斜”，表示变量之间存在相关性，这体现在共享协方差矩阵
<span class="math inline">\(\Sigma\)</span> 中。</p>
<hr />
<h2 id="the-math-behind-lda-the-discriminant-function-判别函数">The Math
Behind LDA: The Discriminant Function 判别函数</h2>
<p>Since we only need to find the class <span
class="math inline">\(k\)</span> that maximizes the posterior
probability <span class="math inline">\(p_k(x)\)</span>, we can simplify
the math. The denominator in Bayes’ theorem is the same for all classes,
so we only need to maximize the numerator: <span
class="math inline">\(\pi_k f_k(x)\)</span>.
由于我们只需要找到使后验概率 <span class="math inline">\(p_k(x)\)</span>
最大化的类别 <span
class="math inline">\(k\)</span>，因此可以简化数学计算。贝叶斯定理中的分母对于所有类别都是相同的，因此我们只需要最大化分子：<span
class="math inline">\(\pi_k f_k(x)\)</span>。 Taking the logarithm
(which doesn’t change which class is maximal) and removing constant
terms gives us the <strong>linear discriminant function</strong>, <span
class="math inline">\(\delta_k(x)\)</span>:
取对数（这不会改变哪个类别是最大值）并移除常数项，得到<strong>线性判别函数</strong>，<span
class="math inline">\(\delta_k(x)\)</span>：</p>
<p><span class="math display">\[
\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1}
\mu_k + \log(\pi_k)
\]</span></p>
<p>This function is <strong>linear</strong> in <span
class="math inline">\(x\)</span>, which is why the method is called
<em>Linear</em> Discriminant Analysis. The decision boundary between any
two classes, say class <span class="math inline">\(k\)</span> and class
<span class="math inline">\(l\)</span>, is the set of points where <span
class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>, which defines
a linear hyperplane. 该函数关于 <span class="math inline">\(x\)</span>
是<strong>线性</strong>的，因此该方法被称为<em>线性</em>判别分析。任意两个类别（例如类别
<span class="math inline">\(k\)</span> 和类别 <span
class="math inline">\(l\)</span>）之间的决策边界是满足 <span
class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>
的点的集合，这定义了一个线性超平面。</p>
<p>The image above (from your “Graph of LDA” slide) is very important. *
<strong>Left:</strong> The ellipses show the true 95% probability
contours for three Gaussian classes. The dashed lines are the ideal
Bayes decision boundaries, which are perfectly linear because the
assumption of common covariance holds. * <strong>Right:</strong> This
shows a sample of data points drawn from those distributions. The solid
lines are the LDA decision boundaries calculated from the sample. They
are a very good estimate of the ideal boundaries. 上图（来自您的“LDA
图”幻灯片）非常重要。 *
<strong>左图：</strong>椭圆显示了三个高斯类别的真实 95%
概率轮廓。虚线是理想的贝叶斯决策边界，由于共同协方差假设成立，因此它们是完美的线性。
*
<strong>右图：</strong>这显示了从这些分布中抽取的数据点样本。实线是根据样本计算出的
LDA 决策边界。它们是对理想边界的非常好的估计。 ***</p>
<h2
id="practical-implementation-estimating-the-parameters-实际应用估计参数">Practical
Implementation: Estimating the Parameters 实际应用：估计参数</h2>
<p>In a real-world scenario, we don’t know the true parameters (<span
class="math inline">\(\mu_k\)</span>, <span
class="math inline">\(\Sigma\)</span>, <span
class="math inline">\(\pi_k\)</span>). Instead, we
<strong>estimate</strong> them from our training data (<span
class="math inline">\(n\)</span> total samples, with <span
class="math inline">\(n_k\)</span> samples in class <span
class="math inline">\(k\)</span>).
在实际场景中，我们不知道真正的参数（<span
class="math inline">\(\mu_k\)</span>、<span
class="math inline">\(\Sigma\)</span>、<span
class="math inline">\(\pi_k\)</span>）。相反，我们根据训练数据（<span
class="math inline">\(n\)</span> 个样本，<span
class="math inline">\(n_k\)</span> 个样本属于 <span
class="math inline">\(k\)</span> 类）来<strong>估计</strong>它们。</p>
<ul>
<li><strong>Prior Probability (<span
class="math inline">\(\hat{\pi}_k\)</span>):</strong> The proportion of
training samples in class <span class="math inline">\(k\)</span>. <span
class="math display">\[\hat{\pi}_k = \frac{n_k}{n}\]</span></li>
<li><strong>Class Mean (<span
class="math inline">\(\hat{\mu}_k\)</span>):</strong> The average of the
training samples in class <span class="math inline">\(k\)</span>. <span
class="math display">\[\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i=k}
x_i\]</span></li>
<li><strong>Common Covariance (<span
class="math inline">\(\hat{\Sigma}\)</span>):</strong> A weighted
average of the sample covariance matrices for each class. This is often
called the “pooled” covariance. <span
class="math display">\[\hat{\Sigma} = \frac{1}{n-K} \sum_{k=1}^{K}
\sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T\]</span></li>
<li><strong>先验概率 (<span
class="math inline">\(\hat{\pi}_k\)</span>)：</strong>训练样本在 <span
class="math inline">\(k\)</span> 类中的比例。 <span
class="math display">\[\hat{\pi}_k = \frac{n_k}{n}\]</span></li>
<li><strong>类别均值 (<span
class="math inline">\(\hat{\mu}_k\)</span>)：</strong>训练样本在 <span
class="math inline">\(k\)</span> 类中的平均值。 <span
class="math display">\[\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i=k}
x_i\]</span></li>
<li><strong>公共协方差 (<span
class="math inline">\(\hat{\Sigma}\)</span>)：</strong>每个类的样本协方差矩阵的加权平均值。这通常被称为“合并”协方差。
<span class="math display">\[\hat{\Sigma} = \frac{1}{n-K} \sum_{k=1}^{K}
\sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T\]</span></li>
</ul>
<p>We then plug these estimates into the discriminant function to get
<span class="math inline">\(\hat{\delta}_k(x)\)</span> and classify a
new observation <span class="math inline">\(x\)</span> to the class with
the largest score. 然后，我们将这些估计值代入判别函数，得到 <span
class="math inline">\(\hat{\delta}_k(x)\)</span>，并将新的观测值 <span
class="math inline">\(x\)</span> 归类到得分最高的类别。 ***</p>
<h2 id="evaluating-performance">Evaluating Performance</h2>
<p>After training the model, we evaluate its performance using a
<strong>confusion matrix</strong>.
训练模型后，我们使用<strong>混淆矩阵</strong>来评估其性能。</p>
<p>This matrix shows the true classes versus the predicted classes. *
<strong>Diagonal elements</strong> (9644, 81) are correct predictions. *
<strong>Off-diagonal elements</strong> (23, 252) are errors.
该矩阵显示了真实类别与预测类别的对比。 * <strong>对角线元素</strong>
(9644, 81) 表示正确预测。 * <strong>非对角线元素</strong> (23, 252)
表示错误预测。</p>
<p>From this matrix, we can calculate key metrics: * <strong>Overall
Error Rate:</strong> Total incorrect predictions / Total predictions. *
Example: <span class="math inline">\((252 + 23) / 10000 =
2.75\%\)</span> * <strong>Sensitivity (True Positive Rate):</strong>
Correctly predicted positives / Total actual positives. It answers: “Of
all the people who actually defaulted, what fraction did we catch?” *
Example: <span class="math inline">\(81 / 333 = 24.3\%\)</span>. The
sensitivity is <span class="math inline">\(1 - 75.7\% = 24.3\%\)</span>.
* <strong>Specificity (True Negative Rate):</strong> Correctly predicted
negatives / Total actual negatives. It answers: “Of all the people who
did not default, what fraction did we correctly identify?” * Example:
<span class="math inline">\(9644 / 9667 = 99.8\%\)</span>. The
specificity is <span class="math inline">\(1 - 0.24\% =
99.8\%\)</span>.</p>
<p>The example in your slides shows a high error rate for “default”
people (75.7%) because the classes are <strong>unbalanced</strong>—there
are far fewer defaulters. This highlights the importance of looking at
class-specific metrics, not just the overall error rate.</p>
<hr />
<h2 id="python-code-understanding">Python Code Understanding</h2>
<p>In Python, you can easily implement LDA using the
<code>scikit-learn</code> library. The code conceptually mirrors the
steps we discussed.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume you have your data X (features) and y (labels)</span></span><br><span class="line"><span class="comment"># X = features (e.g., balance, income)</span></span><br><span class="line"><span class="comment"># y = labels (e.g., 0 for &#x27;no-default&#x27;, 1 for &#x27;default&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create an instance of the LDA model</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit the model to the training data</span></span><br><span class="line"><span class="comment"># This is where the model calculates the estimates:</span></span><br><span class="line"><span class="comment">#  - Prior probabilities (pi_k)</span></span><br><span class="line"><span class="comment">#  - Class means (mu_k)</span></span><br><span class="line"><span class="comment">#  - Pooled covariance matrix (Sigma)</span></span><br><span class="line">lda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Make predictions on new, unseen data</span></span><br><span class="line">predictions = lda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Evaluate the model&#x27;s performance</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, predictions))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, predictions))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>LinearDiscriminantAnalysis()</code> creates the classifier
object.</li>
<li><code>lda.fit(X_train, y_train)</code> is the core training step
where the model learns the <span
class="math inline">\(\hat{\pi}_k\)</span>, <span
class="math inline">\(\hat{\mu}_k\)</span>, and <span
class="math inline">\(\hat{\Sigma}\)</span> parameters from the
data.</li>
<li><code>lda.predict(X_test)</code> uses the learned discriminant
function <span class="math inline">\(\hat{\delta}_k(x)\)</span> to
classify each sample in the test set.</li>
<li><code>confusion_matrix</code> and <code>classification_report</code>
are tools to evaluate the results, just like in the slides.</li>
</ul>
<h1
id="here-is-a-summary-of-the-provided-slides-on-linear-discriminant-analysis-lda-focusing-on-mathematical-concepts-python-code-interpretation-and-key-visuals.">6.
Here is a summary of the provided slides on Linear Discriminant Analysis
(LDA), focusing on mathematical concepts, Python code interpretation,
and key visuals.</h1>
<h2 id="core-concept-lda-for-classification">Core Concept: LDA for
Classification</h2>
<p>Linear Discriminant Analysis (LDA) is a classification method that
models the probability that an observation belongs to a certain class.
It works by finding a linear combination of features that best separates
two or more classes.</p>
<p>The decision is based on <strong>Bayes’ theorem</strong>. For a given
observation with features <span class="math inline">\(X=x\)</span>, LDA
calculates the <strong>posterior probability</strong>, <span
class="math inline">\(p_k(x) = Pr(Y=k|X=x)\)</span>, for each class
<span class="math inline">\(k\)</span>. This is the probability that the
observation belongs to class <span class="math inline">\(k\)</span>
given its features. 线性判别分析 (LDA)
是一种分类方法，它对观测值属于某个类别的概率进行建模。它的工作原理是找到能够最好地区分两个或多个类别的特征的线性组合。</p>
<p>该决策基于<strong>贝叶斯定理</strong>。对于特征为 <span
class="math inline">\(X=x\)</span> 的给定观测值，LDA 会计算每个类别
<span class="math inline">\(k\)</span>
的<strong>后验概率</strong>，<span class="math inline">\(p_k(x) =
Pr(Y=k|X=x)\)</span>。这是给定观测值的特征后，该观测值属于类别 <span
class="math inline">\(k\)</span> 的概率。</p>
<p>By default, the Bayes classifier assigns an observation to the class
with the highest posterior probability. For a binary (two-class) problem
like ‘Yes’ vs. ‘No’, this means:
默认情况下，贝叶斯分类器将观测值分配给后验概率最高的类别。对于像“是”与“否”这样的二分类问题，这意味着：</p>
<ul>
<li>Assign to ‘Yes’ if <span class="math inline">\(Pr(Y=\text{Yes}|X=x)
&gt; 0.5\)</span></li>
<li>Assign to ‘No’ otherwise</li>
</ul>
<h2 id="modifying-the-decision-threshold">Modifying the Decision
Threshold</h2>
<p>The default 0.5 threshold isn’t always optimal. In many real-world
scenarios, the cost of one type of error is much higher than another.
For example, in credit card default prediction: 默认的 0.5
阈值并非总是最优的。在许多实际场景中，一种错误的代价远高于另一种。例如，在信用卡违约预测中：</p>
<ul>
<li><strong>False Negative:</strong> Incorrectly classifying a person
who will default as someone who won’t. (The bank loses money).</li>
<li><strong>False Positive:</strong> Incorrectly classifying a person
who won’t default as someone who will. (The bank loses a potential
customer).</li>
</ul>
<p>A bank might decide that missing a defaulter is much worse than
denying a good customer. To catch more potential defaulters, they can
<strong>lower the probability threshold</strong>.
银行可能会认为错过一个违约者比拒绝一个优质客户更糟糕。为了捕捉更多潜在的违约者，他们可以<strong>降低概率阈值</strong>。</p>
<p>A modified rule could be: <span class="math display">\[
Pr(\text{default}=\text{Yes}|X=x) &gt; 0.2
\]</span> This makes the model more “sensitive” to flagging potential
defaulters, even at the cost of misclassifying more non-defaulters.
降低阈值<strong>会提高敏感度</strong>，但<strong>会降低特异性</strong>。</p>
<p>This decision leads to a <strong>trade-off</strong> between two key
performance metrics: * <strong>Sensitivity (True Positive
Rate):</strong> The ability to correctly identify positive cases. (e.g.,
<code>Correctly identified defaulters / Total actual defaulters</code>).
* <strong>Specificity (True Negative Rate):</strong> The ability to
correctly identify negative cases. (e.g.,
<code>Correctly identified non-defaulters / Total actual non-defaulters</code>).</p>
<p>这一决策会导致两个关键绩效指标之间的<strong>权衡</strong>： *
<strong>敏感度（真阳性率）：</strong>正确识别阳性案例的能力。（例如，“正确识别的违约者/实际违约者总数”）。
*
<strong>特异性（真阴性率）：</strong>正确识别阴性案例的能力。（例如，“正确识别的非违约者/实际非违约者总数”）。</p>
<p>Lowering the threshold <strong>increases sensitivity</strong> but
<strong>decreases specificity</strong>. ## Python Code Explained</p>
<p>The slides show how to implement and adjust LDA using Python’s
<code>scikit-learn</code> library.</p>
<h2 id="basic-lda-implementation">Basic LDA Implementation</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the necessary library</span></span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize and train the LDA model</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line">lda_train = lda.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions using the default 0.5 threshold</span></span><br><span class="line">y_pred = lda.predict(X)</span><br></pre></td></tr></table></figure>
<p>This code trains an LDA model and makes predictions using the
standard 50% probability boundary.</p>
<h2 id="adjusting-the-prediction-threshold">Adjusting the Prediction
Threshold</h2>
<p>To use a custom threshold (e.g., 0.2), you don’t use the
<code>.predict()</code> method. Instead, you get the class probabilities
with <code>.predict_proba()</code> and apply the threshold manually.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Get the probabilities for each class</span></span><br><span class="line"><span class="comment"># lda.predict_proba(X) returns an array like [[P(No), P(Yes)], ...]</span></span><br><span class="line"><span class="comment"># We select the second column [:, 1] for the &#x27;Yes&#x27; class probability</span></span><br><span class="line">lda_probs = lda.predict_proba(X)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Define a custom threshold</span></span><br><span class="line">threshold = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Apply the threshold to get new predictions</span></span><br><span class="line"><span class="comment"># This creates a boolean array (True where prob &gt; 0.2, else False)</span></span><br><span class="line"><span class="comment"># We then convert True/False to &#x27;Yes&#x27;/&#x27;No&#x27; labels</span></span><br><span class="line">lda_pred1 = np.where(lda_probs &gt; threshold, <span class="string">&quot;Yes&quot;</span>, <span class="string">&quot;No&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>This is the core technique for tuning the classifier’s behavior to
meet specific business needs, as demonstrated on slides 55 and 56 for
both LDA and Logistic Regression.</p>
<h2 id="important-images-to-understand">Important Images to
Understand</h2>
<ol type="1">
<li><strong>Confusion Matrix (Slide 49):</strong> This table is crucial.
It breaks down the model’s predictions into True Positives, True
Negatives, False Positives, and False Negatives. All key metrics like
error rate, sensitivity, and specificity are calculated from this
matrix. <strong>混淆矩阵（幻灯片
49）：</strong>这张表至关重要。它将模型的预测分解为真阳性、真阴性、假阳性和假阴性。所有关键指标，例如错误率、灵敏度和特异性，都基于此矩阵计算得出。</li>
<li><strong>LDA Decision Boundaries (Slide 51):</strong> This plot
provides a powerful visual intuition. It shows the data points for two
classes and the decision boundary line. The different parallel lines
show how changing the threshold from 0.5 to 0.1 or 0.9 shifts the
boundary, making the model classify more or fewer points into the
minority class. <strong>LDA 决策边界（幻灯片
51）：</strong>这张图提供了强大的视觉直观性。它展示了两个类别的数据点和决策边界线。不同的平行线显示了将阈值从
0.5 更改为 0.1 或 0.9
时边界如何移动，从而使模型将更多或更少的点归入少数类</li>
<li><strong>Error Rate Tradeoff Curve (Slide 53):</strong> This graph is
the most important for understanding the business implication of
changing the threshold. It clearly shows that as the threshold changes,
the error rate for one class goes down while the error rate for the
other goes up. The overall error is minimized at a certain point, but
that may not be the optimal point from a business perspective.
<strong>错误率权衡曲线（幻灯片
53）：</strong>这张图对于理解更改阈值的业务含义至关重要。它清楚地表明，随着阈值的变化，一个类别的错误率下降，而另一个类别的错误率上升。总体误差在某个点达到最小，但从业务角度来看，这可能并非最佳点。</li>
<li><strong>ROC Curve (Slides 54 &amp; 55):</strong> The Receiver
Operating Characteristic (ROC) curve plots Sensitivity vs. (1 -
Specificity) for <em>all possible thresholds</em>. An ideal classifier
has a curve that “hugs” the top-left corner, indicating high sensitivity
and high specificity. It’s a standard way to visualize and compare the
overall performance of different classifiers. <strong>ROC 曲线（幻灯片
54 和 55）：</strong> 接收者操作特性 (ROC)
曲线绘制了<em>所有可能阈值</em>的灵敏度与（1 -
特异性）的关系。理想的分类器曲线“紧贴”左上角，表示高灵敏度和高特异性。这是可视化和比较不同分类器整体性能的标准方法。</li>
</ol>
<h1
id="here-is-a-summary-of-the-provided-slides-on-linear-and-quadratic-discriminant-analysis-including-the-key-formulas-python-code-equivalents-and-explanations-of-the-important-concepts.">7.
Here is a summary of the provided slides on Linear and Quadratic
Discriminant Analysis, including the key formulas, Python code
equivalents, and explanations of the important concepts.</h1>
<h2 id="key-goal-classification"><strong>Key Goal:
Classification</strong></h2>
<p>Both <strong>Linear Discriminant Analysis (LDA)</strong> and
<strong>Quadratic Discriminant Analysis (QDA)</strong> are
classification algorithms. Their main goal is to find a decision
boundary to separate different classes (e.g., “default” vs. “not
default”) in the data. <strong>线性判别分析 (LDA)</strong> 和
<strong>二次判别分析 (QDA)</strong>
都是分类算法。它们的主要目标是找到一个决策边界来区分数据中的不同类别（例如，“默认”与“非默认”）。</p>
<h3 id="linear-discriminant-analysis-lda">## Linear Discriminant
Analysis (LDA)</h3>
<p>LDA creates a <strong>linear</strong> decision boundary between
classes. LDA 在类别之间创建<strong>线性</strong>决策边界。</p>
<h4 id="core-idea-fishers-interpretation"><strong>Core Idea (Fisher’s
Interpretation)</strong></h4>
<p>Imagine you have data points for different classes in a 3D space.
Fisher’s idea is to find the best angle to shine a “flashlight” on the
data to project its shadow onto a 2D wall (or a 1D line). The “best”
projection is the one where the shadows of the different classes are
<strong>as far apart from each other as possible</strong>, while the
shadows within each class are <strong>as tightly packed as
possible</strong>. 想象一下，你在三维空间中拥有不同类别的数据点。Fisher
的思想是找到最佳角度，用“手电筒”照射数据，将其阴影投射到二维墙壁（或一维线上）。
“最佳”投影是不同类别的阴影<strong>彼此之间尽可能远</strong>，而每个类别内的阴影<strong>尽可能紧密</strong>的投影。</p>
<ul>
<li><strong>Maximize:</strong> The distance between the means of the
projected classes (Between-Class Variance).
投影类别均值之间的距离（类间方差）。</li>
<li><strong>Minimize:</strong> The spread or variance within each
projected class (Within-Class Variance).
每个投影类别内的扩散或方差（类内方差）。 This is the most important
image for understanding the intuition behind LDA. It shows how
projecting the data onto a specific line (defined by vector
<code>w</code>) can make the two classes clearly separable.
这是理解LDA背后直觉的最重要图像。它展示了如何将数据投影到特定直线（由向量“w”定义）上，从而使两个类别清晰可分。</li>
</ul>
<h4 id="key-mathematical-formulas"><strong>Key Mathematical
Formulas</strong></h4>
<p>To achieve this, LDA maximizes a ratio called the <strong>Rayleigh
quotient</strong>. LDA最大化一个称为<strong>瑞利商</strong>的比率。</p>
<ol type="1">
<li><strong>Within-Class Covariance (<span
class="math inline">\(\hat{\Sigma}_W\)</span>)</strong>: Measures the
spread of data <em>inside</em> each class. <strong>类内协方差 (<span
class="math inline">\(\hat{\Sigma}_W\)</span>)</strong>：衡量每个类别<em>内部</em>数据的扩散程度。
<span class="math display">\[\hat{\Sigma}_W = \frac{1}{n-K}
\sum_{k=1}^{K} \sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i -
\hat{\mu}_k)^\top\]</span></li>
<li><strong>Between-Class Covariance (<span
class="math inline">\(\hat{\Sigma}_B\)</span>)</strong>: Measures the
spread <em>between</em> the means of different classes.
<strong>类间协方差 (<span
class="math inline">\(\hat{\Sigma}_B\)</span>)</strong>：衡量不同类别均值<em>之间</em>的差异。
<span class="math display">\[\hat{\Sigma}_B = \sum_{k=1}^{K} n_k
(\hat{\mu}_k - \hat{\mu})(\hat{\mu}_k - \hat{\mu})^\top\]</span></li>
<li><strong>Objective Function</strong>: Find the projection vector
<span class="math inline">\(w\)</span> that maximizes the ratio of
between-class variance to within-class variance.
<strong>目标函数</strong>：找到投影向量 <span
class="math inline">\(w\)</span>，使类间方差与类内方差之比最大化。 <span
class="math display">\[\max_w \frac{w^\top \hat{\Sigma}_B w}{w^\top
\hat{\Sigma}_W w}\]</span></li>
</ol>
<h4 id="ldas-main-assumption"><strong>LDA’s Main
Assumption</strong></h4>
<p>The key assumption of LDA is that all classes share the <strong>same
covariance matrix (<span
class="math inline">\(\Sigma\)</span>)</strong>. They can have different
means (<span class="math inline">\(\mu_k\)</span>), but their spread and
orientation must be identical. This assumption is what results in a
linear decision boundary. LDA
的关键假设是所有类别共享<strong>相同的协方差矩阵 (<span
class="math inline">\(\Sigma\)</span>)</strong>。它们可以具有不同的均值
(<span
class="math inline">\(\mu_k\)</span>)，但它们的散度和方向必须相同。正是这一假设导致了线性决策边界。</p>
<h3 id="quadratic-discriminant-analysis-qda">## Quadratic Discriminant
Analysis (QDA)</h3>
<p>QDA is a more flexible extension of LDA that creates a
<strong>quadratic</strong> (curved) decision boundary. QDA 是 LDA
的更灵活的扩展，它创建了<strong>二次</strong>（曲线）决策边界。 ####
<strong>Core Idea &amp; Key Assumption</strong></p>
<p>QDA starts with the same principles as LDA but drops the key
assumption. QDA assumes that <strong>each class has its own unique
covariance matrix (<span
class="math inline">\(\Sigma_k\)</span>)</strong>. QDA 的原理与 LDA
相同，但放弃了关键假设。QDA 假设<strong>每个类别都有自己独特的协方差矩阵
(<span class="math inline">\(\Sigma_k\)</span>)</strong>。</p>
<p>This means each class can have its own spread, shape, and
orientation. This additional flexibility allows for a more complex,
curved decision boundary.
这意味着每个类别可以拥有自己的散度、形状和方向。这种额外的灵活性使得决策边界更加复杂、曲线化。</p>
<h4 id="key-mathematical-formula"><strong>Key Mathematical
Formula</strong></h4>
<p>The classification is made using a discrimination function, <span
class="math inline">\(\delta_k(x)\)</span>. We assign a data point <span
class="math inline">\(x\)</span> to the class <span
class="math inline">\(k\)</span> for which <span
class="math inline">\(\delta_k(x)\)</span> is largest. The function for
QDA is: <span class="math display">\[\delta_k(x) = -\frac{1}{2}(x -
\mu_k)^\top \Sigma_k^{-1}(x - \mu_k) - \frac{1}{2}\log(|\Sigma_k|) +
\log \pi_k\]</span> The term containing <span
class="math inline">\(x^\top \Sigma_k^{-1} x\)</span> makes this
function a <strong>quadratic</strong> function of <span
class="math inline">\(x\)</span>.</p>
<h3 id="lda-vs.-qda-the-trade-off">## LDA vs. QDA: The Trade-Off</h3>
<p>The choice between LDA and QDA is a classic <strong>bias-variance
trade-off</strong>. 在 LDA 和 QDA
之间进行选择是典型的<strong>偏差-方差权衡</strong>。</p>
<ul>
<li><p><strong>Use LDA when:</strong></p>
<ul>
<li>The assumption of a common covariance matrix is reasonable (the
classes have similar shapes).</li>
<li>You have a small amount of training data, as LDA is less prone to
overfitting.</li>
<li>Simplicity is preferred. LDA is less flexible (high bias) but has
lower variance.</li>
<li>假设共同协方差矩阵是合理的（类别具有相似的形状）。</li>
<li>训练数据量较少，因为 LDA 不易过拟合。</li>
<li>简洁是首选。LDA 灵活性较差（偏差较大），但方差较小。</li>
</ul></li>
<li><p><strong>Use QDA when:</strong></p>
<ul>
<li>The classes have clearly different shapes and spreads (different
covariance matrices).</li>
<li>You have a large amount of training data to properly estimate the
separate covariance matrices for each class.</li>
<li>QDA is more flexible (low bias) but can have high variance, meaning
it might overfit on smaller datasets.</li>
<li>类别具有明显不同的形状和分布（不同的协方差矩阵）。</li>
<li>拥有大量训练数据，可以正确估计每个类别的独立协方差矩阵。</li>
<li>QDA
更灵活（偏差较小），但方差较大，这意味着它可能在较小的数据集上过拟合。
<strong>Rule of Thumb:</strong> If the class variances are equal or
close, LDA is better. Otherwise, QDA is better.
<strong>经验法则：</strong> 如果类别方差相等或接近，则 LDA
更佳。否则，QDA 更好。</li>
</ul></li>
</ul>
<h3 id="code-understanding-python-equivalent">## Code Understanding
(Python Equivalent)</h3>
<p>The slides show code in R. Here’s how you would perform LDA and
evaluate it in Python using the popular <code>scikit-learn</code>
library.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, accuracy_score, roc_curve, auc</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;df&#x27; is your DataFrame with features and a &#x27;target&#x27; column</span></span><br><span class="line"><span class="comment"># X = df.drop(&#x27;target&#x27;, axis=1)</span></span><br><span class="line"><span class="comment"># y = df[&#x27;target&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit an LDA model (equivalent to lda() in R)</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line">lda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Make predictions (equivalent to predict() in R)</span></span><br><span class="line">y_pred_lda = lda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># To fit a QDA model, the process is identical:</span></span><br><span class="line"><span class="comment"># qda = QuadraticDiscriminantAnalysis()</span></span><br><span class="line"><span class="comment"># qda.fit(X_train, y_train)</span></span><br><span class="line"><span class="comment"># y_pred_qda = qda.predict(X_test)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Create a confusion matrix (equivalent to table())</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LDA Confusion Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, y_pred_lda))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Plot the ROC Curve (equivalent to the R code for ROC)</span></span><br><span class="line"><span class="comment"># Get prediction probabilities for the positive class</span></span><br><span class="line">y_pred_proba = lda.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate ROC curve points</span></span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate Area Under the Curve (AUC)</span></span><br><span class="line">roc_auc = auc(fpr, tpr)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(fpr, tpr, color=<span class="string">&#x27;blue&#x27;</span>, lw=<span class="number">2</span>, label=<span class="string">f&#x27;LDA ROC curve (area = <span class="subst">&#123;roc_auc:<span class="number">.2</span>f&#125;</span>)&#x27;</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">&#x27;gray&#x27;</span>, lw=<span class="number">2</span>, linestyle=<span class="string">&#x27;--&#x27;</span>) <span class="comment"># Random guess line</span></span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate (1 - Specificity)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate (Sensitivity)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Receiver Operating Characteristic (ROC) Curve&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="understanding-the-roc-curve"><strong>Understanding the ROC
Curve</strong></h4>
<p>The <strong>ROC Curve</strong> is another important image. It helps
you visualize a classifier’s performance across all possible
classification thresholds. <strong>ROC 曲线</strong>
是另一个重要的图像。它可以帮助您直观地了解分类器在所有可能的分类阈值下的性能。</p>
<ul>
<li>The <strong>Y-axis</strong> is the <strong>True Positive
Rate</strong> (Sensitivity): “Of all the actual positives, how many did
we correctly identify?”</li>
<li>The <strong>X-axis</strong> is the <strong>False Positive
Rate</strong>: “Of all the actual negatives, how many did we incorrectly
label as positive?”</li>
<li>A perfect classifier would have a curve that goes straight up to the
top-left corner (100% TPR, 0% FPR). The diagonal line represents a
random guess. The <strong>Area Under the Curve (AUC)</strong> summarizes
the model’s performance; a value closer to 1.0 is better.</li>
<li><strong>Y 轴</strong>
表示<strong>真阳性率</strong>（敏感度）：“在所有实际的阳性样本中，我们正确识别了多少个？”</li>
<li><strong>X 轴</strong>
表示<strong>假阳性率</strong>：“在所有实际的阴性样本中，我们错误地将多少个标记为阳性？”</li>
<li>一个完美的分类器应该有一条直线上升到左上角的曲线（真阳性率
100%，假阳性率 0%）。对角线表示随机猜测。<strong>曲线下面积
(AUC)</strong> 概括了模型的性能；该值越接近 1.0 越好。</li>
</ul>
<h1
id="here-is-a-summary-of-the-provided-slides-on-quadratic-discriminant-analysis-qda-including-the-key-formulas-code-explanations-with-python-equivalents-and-a-guide-to-the-most-important-images.">8.
Here is a summary of the provided slides on Quadratic Discriminant
Analysis (QDA), including the key formulas, code explanations with
Python equivalents, and a guide to the most important images.</h1>
<h3 id="core-concept-qda-vs.-lda">## Core Concept: QDA vs. LDA</h3>
<p>The main difference between <strong>Linear Discriminant Analysis
(LDA)</strong> and <strong>Quadratic Discriminant Analysis
(QDA)</strong> lies in their assumptions about the data.
<strong>线性判别分析 (LDA)</strong> 和 <strong>二次判别分析
(QDA)</strong> 的主要区别在于它们对数据的假设。 * <strong>LDA</strong>
assumes that all classes share the <strong>same covariance
matrix</strong> (<span class="math inline">\(\Sigma\)</span>). It models
each class as a normal distribution with a different mean (<span
class="math inline">\(\mu_k\)</span>) but the same shape and
orientation. This results in a <em>linear</em> decision boundary between
classes. 假设所有类别共享<strong>相同的协方差矩阵</strong> (<span
class="math inline">\(\Sigma\)</span>)。它将每个类别建模为均值不同
(<span class="math inline">\(\mu_k\)</span>)
但形状和方向相同的正态分布。这会导致类别之间出现 <em>线性</em>
决策边界。 * <strong>QDA</strong> is more flexible. It assumes that each
class <span class="math inline">\(k\)</span> has its <strong>own,
separate covariance matrix</strong> (<span
class="math inline">\(\Sigma_k\)</span>). This allows each class’s
distribution to have a unique shape, size, and orientation. This
flexibility results in a <em>quadratic</em> decision boundary (like a
parabola, hyperbola, or ellipse). 更灵活。它假设每个类别 <span
class="math inline">\(k\)</span> 都有其<strong>独立的协方差矩阵</strong>
(<span
class="math inline">\(\Sigma_k\)</span>)。这使得每个类别的分布具有独特的形状、大小和方向。这种灵活性导致了<em>二次</em>决策边界（类似于抛物线、双曲线或椭圆）。
<strong>Analogy</strong> 💡: Imagine you’re drawing boundaries around
different clusters of stars. LDA gives you only straight lines to
separate the clusters. QDA gives you curved lines (circles, ellipses),
which can create a much better fit if the clusters themselves are
elliptical and point in different directions.
想象一下，你正在围绕不同的星团绘制边界。LDA 只提供直线来分隔星团。QDA
提供曲线（圆形、椭圆形），如果星团本身是椭圆形且指向不同的方向，则可以产生更好的拟合效果。</p>
<h3 id="the-math-behind-qda">## The Math Behind QDA</h3>
<p>QDA classifies a new observation <span
class="math inline">\(x\)</span> to the class <span
class="math inline">\(k\)</span> that has the highest discriminant
score, <span class="math inline">\(\delta_k(x)\)</span>. The formula for
this score is what makes the boundary quadratic. QDA 将新的观测值 <span
class="math inline">\(x\)</span> 归类到具有最高判别分数 <span
class="math inline">\(\delta_k(x)\)</span> 的类 <span
class="math inline">\(k\)</span>
中。该分数的公式使得边界具有二次项。</p>
<p>The discriminant function for class <span
class="math inline">\(k\)</span> is: <span
class="math display">\[\delta_k(x) = -\frac{1}{2}(x - \mu_k)^T
\Sigma_k^{-1}(x - \mu_k) - \frac{1}{2}\log(|\Sigma_k|) +
\log(\pi_k)\]</span></p>
<p>Let’s break it down:</p>
<ul>
<li><span class="math inline">\((x - \mu_k)^T \Sigma_k^{-1}(x -
\mu_k)\)</span>: This is a quadratic term (since it involves <span
class="math inline">\(x^T \Sigma_k^{-1} x\)</span>). It measures the
squared Mahalanobis distance from <span class="math inline">\(x\)</span>
to the class mean <span class="math inline">\(\mu_k\)</span>, scaled by
that class’s specific covariance <span
class="math inline">\(\Sigma_k\)</span>.</li>
<li><span class="math inline">\(\log(|\Sigma_k|)\)</span>: A term that
penalizes classes with larger variance.</li>
<li><span class="math inline">\(\log(\pi_k)\)</span>: The prior
probability of class <span class="math inline">\(k\)</span>. This is our
initial belief about how likely class <span
class="math inline">\(k\)</span> is, before seeing the data.
<ul>
<li><span class="math inline">\((x - \mu_k)^T \Sigma_k^{-1}(x -
\mu_k)\)</span>：这是一个二次项（因为它涉及 <span
class="math inline">\(x^T \Sigma_k^{-1} x\)</span>）。它测量从 <span
class="math inline">\(x\)</span> 到类均值 <span
class="math inline">\(\mu_k\)</span>
的平方马氏距离，并根据该类的特定协方差 <span
class="math inline">\(\Sigma_k\)</span> 进行缩放。</li>
<li><span
class="math inline">\(\log(|\Sigma_k|)\)</span>：用于惩罚方差较大的类的项。</li>
<li><span class="math inline">\(\log(\pi_k)\)</span>：类 <span
class="math inline">\(k\)</span> 的先验概率。这是我们在看到数据之前对类
<span class="math inline">\(k\)</span> 可能性的初始信念。 Because each
class <span class="math inline">\(k\)</span> has its own <span
class="math inline">\(\Sigma_k\)</span>, the quadratic term doesn’t
cancel out when comparing scores between classes, leading to a quadratic
boundary. 由于每个类 <span class="math inline">\(k\)</span> 都有其自己的
<span
class="math inline">\(\Sigma_k\)</span>，因此在比较类之间的分数时，二次项不会抵消，从而导致二次边界。
<strong>Key Trade-off</strong>:</li>
</ul></li>
<li>If the class variances (<span
class="math inline">\(\Sigma_k\)</span>) are truly different,
<strong>QDA is better</strong>.</li>
<li>If the class variances are similar, <strong>LDA is often
better</strong> because it’s less flexible and less likely to overfit,
especially with a small number of training samples.</li>
<li>如果类方差 (<span class="math inline">\(\Sigma_k\)</span>)
确实不同，<strong>QDA 更好</strong>。</li>
<li>如果类方差相似，<strong>LDA
通常更好</strong>，因为它的灵活性较差，并且不太可能过拟合，尤其是在训练样本数量较少的情况下。</li>
</ul>
<h3 id="code-implementation-r-and-python">## Code Implementation: R and
Python</h3>
<p>The slides provide R code for fitting a QDA model and evaluating it.
Below is an explanation of the R code and its equivalent in Python using
the popular <code>scikit-learn</code> library.</p>
<h4 id="r-code-from-the-slides">R Code (from the slides)</h4>
<p>The code uses the <code>MASS</code> library for QDA and the
<code>ROCR</code> library for evaluation.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ######## QDA ##########</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Fit the model on the training data</span></span><br><span class="line"><span class="comment"># This formula `Default~.` means &quot;predict &#x27;Default&#x27; using all other variables&quot;.</span></span><br><span class="line">qda.fit.mod2 <span class="operator">&lt;-</span> qda<span class="punctuation">(</span>Default<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> subset<span class="operator">=</span>train.ids<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Make predictions on the test data</span></span><br><span class="line"><span class="comment"># We are interested in the posterior probabilities for the ROC curve</span></span><br><span class="line">qda.fit.pred3 <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>qda.fit.mod2<span class="punctuation">,</span> Default_test<span class="punctuation">)</span><span class="operator">$</span>posterior<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Evaluate using ROC and AUC</span></span><br><span class="line"><span class="comment"># &#x27;prediction&#x27; and &#x27;performance&#x27; are functions from the ROCR library</span></span><br><span class="line">perf <span class="operator">&lt;-</span> performance<span class="punctuation">(</span>prediction<span class="punctuation">(</span>qda.fit.pred3<span class="punctuation">,</span> Default_test<span class="operator">$</span>Default<span class="punctuation">)</span><span class="punctuation">,</span> <span class="string">&quot;auc&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Get the AUC value</span></span><br><span class="line">auc_value <span class="operator">&lt;-</span> perf<span class="operator">@</span>y.values<span class="punctuation">[[</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">]</span></span><br><span class="line"><span class="comment"># Result from slide: 0.9638683</span></span><br></pre></td></tr></table></figure>
<h4 id="python-equivalent-scikit-learn">Python Equivalent
(<code>scikit-learn</code>)</h4>
<p>Here’s how you would perform the same steps in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> QuadraticDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score, roc_curve</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Default&#x27; is your DataFrame and &#x27;default&#x27; is the target column</span></span><br><span class="line"><span class="comment"># (preprocessing &#x27;student&#x27; and &#x27;default&#x27; columns to numbers)</span></span><br><span class="line"><span class="comment"># Default[&#x27;default_num&#x27;] = Default[&#x27;default&#x27;].apply(lambda x: 1 if x == &#x27;Yes&#x27; else 0)</span></span><br><span class="line"><span class="comment"># X = Default[[&#x27;balance&#x27;, &#x27;income&#x27;, ...]]</span></span><br><span class="line"><span class="comment"># y = Default[&#x27;default_num&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Initialize and fit the QDA model</span></span><br><span class="line">qda = QuadraticDiscriminantAnalysis()</span><br><span class="line">qda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Predict probabilities on the test set</span></span><br><span class="line"><span class="comment"># We need the probability of the positive class (&#x27;Yes&#x27;) for the AUC calculation</span></span><br><span class="line">y_pred_proba = qda.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Calculate the AUC score</span></span><br><span class="line">auc_score = roc_auc_score(y_test, y_pred_proba)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;AUC Score for QDA: <span class="subst">&#123;auc_score:<span class="number">.7</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also plot the ROC curve</span></span><br><span class="line"><span class="comment"># fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)</span></span><br><span class="line"><span class="comment"># plt.plot(fpr, tpr)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure>
<h3 id="model-evaluation-roc-and-auc">## Model Evaluation: ROC and
AUC</h3>
<p>The slides correctly emphasize using the <strong>ROC curve</strong>
and the <strong>Area Under the Curve (AUC)</strong> to compare model
performance.</p>
<ul>
<li><p><strong>ROC Curve (Receiver Operating Characteristic)</strong>:
This plot shows how well a model can distinguish between two classes. It
plots the <strong>True Positive Rate</strong> (y-axis) against the
<strong>False Positive Rate</strong> (x-axis) at all possible
classification thresholds. A better model has a curve that is closer to
the top-left corner.</p></li>
<li><p><strong>AUC (Area Under the Curve)</strong>: This is a single
number that summarizes the entire ROC curve.</p>
<ul>
<li><strong>AUC = 1</strong>: Perfect classifier.</li>
<li><strong>AUC = 0.5</strong>: A useless classifier (equivalent to
random guessing).</li>
<li><strong>AUC &gt; 0.7</strong>: Generally considered an acceptable
model.</li>
</ul></li>
<li><p><strong>ROC
曲线（接收者操作特征）</strong>：此图显示了模型区分两个类别的能力。它绘制了所有可能的分类阈值下的
<strong>真阳性率</strong>（y 轴）与 <strong>假阳性率</strong>（x
轴）的对比图。更好的模型的曲线越靠近左上角，效果就越好。</p>
<ul>
<li><p><strong>AUC（曲线下面积）</strong>：这是一个概括整个 ROC
曲线的数值。</p></li>
<li><p><strong>AUC = 1</strong>：完美的分类器。</p></li>
<li><p><strong>AUC =
0.5</strong>：无用的分类器（相当于随机猜测）。</p></li>
<li><p><strong>AUC &gt;
0.7</strong>：通常被认为是可接受的模型。</p></li>
</ul></li>
</ul>
<p>The slides show that for the <code>Default</code> dataset,
<strong>LDA’s AUC (0.9647) was slightly higher than QDA’s
(0.9639)</strong>. This suggests that the assumption of a common
covariance matrix (LDA) was a slightly better fit for this particular
test set, possibly because QDA’s extra flexibility wasn’t needed and it
may have slightly overfit the training data.
这表明，对于这个特定的测试集，公共协方差矩阵 (LDA)
的假设拟合度略高，可能是因为 QDA
的额外灵活性并非必需，并且可能对训练数据略微过拟合。</p>
<h3 id="key-takeaways-and-important-images">## Key Takeaways and
Important Images</h3>
<h3
id="heres-a-ranking-of-the-most-important-visual-aids-in-your-slides">Here’s
a ranking of the most important visual aids in your slides:</h3>
<ol type="1">
<li><p><strong>Slide 68/69 (Model Assumption &amp; Formula)</strong>:
These are the <strong>most critical</strong> slides. They present the
core theoretical difference between LDA and QDA and provide the
mathematical foundation (the discriminant function formula).
Understanding these is key to understanding QDA.</p></li>
<li><p><strong>Slide 73 (ROC Comparison)</strong>: This is the most
important image for <strong>practical evaluation</strong>. It visually
compares the performance of LDA and QDA side-by-side, making it easy to
see which one performs better on this specific dataset. The concept of
AUC is introduced here as the method for comparison.</p></li>
<li><p><strong>Slide 71 (Decision Boundaries with Different
Thresholds)</strong>: This is an excellent conceptual image. It shows
how the quadratic decision boundary (the curved lines) separates the
data points. It also illustrates how changing the probability threshold
(from 0.1 to 0.5 to 0.9) shifts the boundary, trading off between
precision and recall.</p></li>
</ol>
<p>Of course. Here is a summary of the remaining slides, which compare
QDA to other popular classification models like Logistic Regression and
K-Nearest Neighbors (KNN).</p>
<hr />
<h3 id="visualizing-the-core-trade-off-lda-vs.-qda">Visualizing the Core
Trade-off: LDA vs. QDA</h3>
<p>This is the most important concept in these slides. The choice
between LDA and QDA depends entirely on the underlying structure of your
data.</p>
<p>The slide shows two scenarios: 1. <strong>Left Plot (<span
class="math inline">\(\Sigma_1 = \Sigma_2\)</span>):</strong> When the
true covariance matrices of the classes are the same, the optimal
decision boundary (the Bayes classifier) is a straight line. LDA, which
assumes equal covariances, creates a linear boundary that approximates
this optimal boundary very well. QDA’s flexible, curved boundary is
unnecessarily complex and might overfit the training data. <strong>In
this case, LDA is better.</strong> 2. <strong>Right Plot (<span
class="math inline">\(\Sigma_1 \neq \Sigma_2\)</span>):</strong> When
the true covariance matrices are different, the optimal decision
boundary is a curve. QDA’s quadratic model can capture this
non-linearity much better than LDA’s rigid linear model. <strong>In this
case, QDA is better.</strong></p>
<p>This perfectly illustrates the <strong>bias-variance
tradeoff</strong>. LDA has higher bias (it’s less flexible) but lower
variance. QDA has lower bias (it’s more flexible) but higher
variance.</p>
<hr />
<h3 id="comparing-performance-on-the-default-dataset">Comparing
Performance on the “Default” Dataset</h3>
<p>The slides compare four different models on the same classification
task. Let’s look at their performance using the <strong>Area Under the
Curve (AUC)</strong>, where a higher score is better.</p>
<ul>
<li><strong>LDA AUC:</strong> 0.9647</li>
<li><strong>QDA AUC:</strong> 0.9639</li>
<li><strong>Logistic Regression AUC:</strong> 0.9645</li>
<li><strong>K-Nearest Neighbors (KNN):</strong> The plot shows test
error vs. K. The error is lowest around K=4, but it’s not directly
converted to an AUC score in the slides.</li>
</ul>
<p>Interestingly, for this particular dataset, LDA, QDA, and Logistic
Regression perform almost identically. This suggests that the decision
boundary for this problem is likely very close to linear, meaning the
extra flexibility of QDA isn’t providing much benefit.</p>
<hr />
<h3 id="pros-and-cons-which-model-to-choose">Pros and Cons: Which Model
to Choose?</h3>
<p>The final slide asks for a comparison of the models. Here’s a summary
of their key characteristics:</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Decision Boundary</th>
<th style="text-align: left;">Key Pro</th>
<th style="text-align: left;">Key Con</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Logistic Regression</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">Highly interpretable, no strong
assumptions about data distribution.</td>
<td style="text-align: left;">Inflexible; cannot capture non-linear
relationships.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Linear Discriminant Analysis
(LDA)</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">More stable than Logistic Regression when
classes are well-separated.</td>
<td style="text-align: left;">Assumes data is normally distributed with
equal covariance matrices for all classes.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Quadratic Discriminant Analysis
(QDA)</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Quadratic (Curved)</td>
<td style="text-align: left;">More flexible than LDA; can model
non-linear boundaries.</td>
<td style="text-align: left;">Requires more data to estimate parameters
and is more prone to overfitting. Assumes normality.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>K-Nearest Neighbors
(KNN)</strong></td>
<td style="text-align: left;">Non-Parametric</td>
<td style="text-align: left;">Highly Non-linear</td>
<td style="text-align: left;">Extremely flexible; makes no assumptions
about the data’s distribution.</td>
<td style="text-align: left;">Can be slow on large datasets and suffers
from the “curse of dimensionality.” Less interpretable.</td>
</tr>
</tbody>
</table>
<h4 id="summary-of-the-comparison">Summary of the Comparison:</h4>
<ul>
<li><strong>Linear Models (Logistic Regression &amp; LDA):</strong>
Choose these for simplicity, interpretability, and when you believe the
relationship between predictors and the class is linear. LDA often
outperforms Logistic Regression if its normality assumptions are
met.</li>
<li><strong>Non-Linear Models (QDA &amp; KNN):</strong> Choose these
when the decision boundary is likely more complex. QDA is a good middle
ground, offering more flexibility than LDA without being as completely
data-driven as KNN. KNN is the most flexible but requires careful tuning
of the parameter K to avoid overfitting or underfitting.</li>
</ul>
<h1
id="here-is-a-more-detailed-slide-by-slide-analysis-of-the-presentation.">9.
Here is a more detailed, slide-by-slide analysis of the
presentation.</h1>
<h2 id="four-classification-methods-comparison-by-simulation">4.6 Four
Classification Methods: Comparison by Simulation</h2>
<p>This section (slides 81-87) introduces four classification methods
and systematically compares their performance on six different simulated
datasets. The goal is to see which method works best under different
conditions (e.g., linear vs. non-linear boundaries, normal
vs. non-normal data).</p>
<p>The four methods being compared are: * <strong>Logistic
Regression:</strong> A linear method that models the log-odds as a
linear function of the predictors. * <strong>Linear Discriminant
Analysis (LDA):</strong> Another linear method. It also assumes a linear
decision boundary but makes stronger assumptions than logistic
regression (e.g., that data within each class is normally distributed
with a common covariance matrix). * <strong>Quadratic Discriminant
Analysis (QDA):</strong> A non-linear method. It assumes the log-odds
are a <em>quadratic</em> function, which creates a more flexible, curved
decision boundary. It assumes data within each class is normally
distributed, but <em>without</em> a common covariance matrix. *
<strong>K-Nearest Neighbors (KNN):</strong> A non-parametric, highly
flexible method. Two versions are tested: * <strong>KNN-1 (<span
class="math inline">\(K=1\)</span>):</strong> A very flexible (high
variance) model. * <strong>KNN-CV:</strong> A tuned model where the best
<span class="math inline">\(K\)</span> is chosen via
cross-validation.</p>
<p>比较的四种方法是： *
<strong>逻辑回归</strong>：一种将对数概率建模为预测变量线性函数的线性方法。
* <strong>线性判别分析
(LDA)</strong>：另一种线性方法。它也假设线性决策边界，但比逻辑回归做出更强的假设（例如，每个类中的数据呈正态分布，且具有共同的协方差矩阵）。
* <strong>二次判别分析
(QDA)</strong>：一种非线性方法。它假设对数概率为<em>二次</em>函数，从而创建一个更灵活、更弯曲的决策边界。它假设每个类中的数据服从正态分布，但<em>没有</em>共同的协方差矩阵。
* <strong>K最近邻
(KNN)</strong>：一种非参数化、高度灵活的方法。测试了两个版本： *
<strong>KNN-1 (<span
class="math inline">\(K=1\)</span>)</strong>：一个非常灵活（高方差）的模型。
*
<strong>KNN-CV</strong>：一个经过调整的模型，通过交叉验证选择最佳的<span
class="math inline">\(K\)</span>。</p>
<h3 id="analysis-of-simulation-scenarios">Analysis of Simulation
Scenarios</h3>
<p>The performance is measured by the <strong>test error rate</strong>
(lower is better), shown in the boxplots for each scenario.
性能通过<strong>测试错误率</strong>（越低越好）来衡量，每个场景的箱线图都显示了该错误率。</p>
<ul>
<li><strong>Scenario 1 (Slide 82):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>linear</strong> decision boundary.
Data is <strong>normally distributed</strong> with <em>uncorrelated</em>
predictors.</li>
<li><strong>Result:</strong> <strong>LDA and Logistic Regression perform
best</strong>. Their test error rates are low and similar. This is
expected, as the setup perfectly matches their core assumption (linear
boundary). QDA is slightly worse because its extra flexibility (being
quadratic) is unnecessary. KNN-1 is the worst, as its high flexibility
leads to high variance (overfitting).</li>
<li><strong>结果：</strong> <strong>LDA
和逻辑回归表现最佳</strong>。它们的测试错误率较低且相似。这是意料之中的，因为设置完全符合它们的核心假设（线性边界）。QDA
略差，因为其额外的灵活性（二次方）是不必要的。KNN-1
最差，因为其高灵活性导致方差较大（过拟合）。</li>
</ul></li>
<li><strong>Scenario 2 (Slide 83):</strong>
<ul>
<li><strong>Setup:</strong> Same as Scenario 1 (<strong>linear</strong>
boundary, <strong>normal</strong> data), but now the two predictors have
a <strong>correlation of 0.5</strong>.</li>
<li><strong>Result:</strong> <strong>Almost no change</strong> from
Scenario 1. <strong>LDA and Logistic Regression are still the
best</strong>. This shows that these linear methods are robust to
correlation between predictors.</li>
<li><strong>结果：</strong>与场景 1
相比<strong>几乎没有变化</strong>。<strong>LDA
和逻辑回归仍然是最佳</strong>。这表明这些线性方法对预测因子之间的相关性具有鲁棒性。</li>
</ul></li>
<li><strong>Scenario 3 (Slide 84):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>linear</strong> decision boundary,
but the data is drawn from a <strong>t-distribution</strong> (which is
non-normal and has “heavy tails,” or more extreme outliers).</li>
<li><strong>Result:</strong> <strong>Logistic Regression is the clear
winner</strong>. LDA’s performance gets worse because its assumption of
<em>normality</em> is violated by the t-distribution. QDA’s performance
deteriorates significantly due to the non-normality. This highlights a
key difference: logistic regression is more robust to violations of the
normality assumption.</li>
<li><strong>结果：</strong>逻辑回归明显胜出**。LDA 的性能会变差，因为 t
分布违反了其正态性假设。QDA
的性能由于非正态性而显著下降。这凸显了一个关键区别：逻辑回归对违反正态性假设的情况更稳健。</li>
</ul></li>
<li><strong>Scenario 4 (Slide 85):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>quadratic</strong> decision
boundary. Data is <strong>normally distributed</strong> with different
correlations in each class.</li>
<li><strong>Result:</strong> <strong>QDA is the clear winner</strong> by
a large margin. This setup perfectly matches QDA’s assumption (quadratic
boundary from normal data with different covariance structures). All
other methods (LDA, Logistic, KNN) are linear or not flexible enough, so
they perform poorly.</li>
<li><strong>结果：</strong>QDA 明显胜出**，且遥遥领先。此设置完全符合
QDA
的假设（来自具有不同协方差结构的正态数据的二次边界）。所有其他方法（LDA、Logistic、KNN）都是线性的或不够灵活，因此性能不佳。</li>
</ul></li>
<li><strong>Scenario 5 (Slide 86):</strong>
<ul>
<li><strong>Setup:</strong> Another <strong>quadratic</strong> boundary,
but generated in a different way (using a logistic function of quadratic
terms).</li>
<li><strong>Result:</strong> <strong>QDA performs best again</strong>,
closely followed by the flexible <strong>KNN-CV</strong>. The linear
methods (LDA, Logistic) have poor performance because they cannot
capture the curve.</li>
<li><strong>结果：QDA
再次表现最佳</strong>，紧随其后的是灵活的<strong>KNN-CV</strong>。线性方法（LDA、Logistic）性能较差，因为它们无法捕捉曲线。</li>
</ul></li>
<li><strong>Scenario 6 (Slide 87):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>complex, non-linear</strong>
decision boundary (more complex than a simple quadratic curve).</li>
<li><strong>Result:</strong> The <strong>flexible KNN-CV method is the
winner</strong>. Its non-parametric nature allows it to approximate the
complex shape. QDA is not flexible <em>enough</em> and performs worse.
This slide highlights the bias-variance trade-off: the overly simple
KNN-1 is the worst, but the <em>tuned</em> KNN-CV is the best.</li>
<li><strong>结果：</strong>灵活的 KNN-CV
方法胜出**。其非参数特性使其能够近似复杂的形状。 QDA
不够灵活，性能较差。这张幻灯片重点介绍了偏差-方差权衡：过于简单的 KNN-1
最差，而 <em>调整后的</em> KNN-CV 最好。</li>
</ul></li>
</ul>
<h2 id="r-example-on-smarket-data">4.7 R Example on Smarket Data</h2>
<p>This section (slides 88-93) applies Logistic Regression and LDA to
the <code>Smarket</code> dataset from the <code>ISLR</code> package to
predict the stock market’s <code>Direction</code> (Up or Down).
本节（幻灯片 88-93）将逻辑回归和 LDA
应用于“ISLR”包中的“Smarket”数据集，以预测股市的“方向”（上涨或下跌）。
### Data Preparation (Slides 88, 89, 90)</p>
<ol type="1">
<li><strong>Load Data:</strong> The <code>ISLR</code> library is loaded,
and the <code>Smarket</code> dataset is explored. It contains daily
percentage returns (<code>Lag1</code>…<code>Lag5</code> for the previous
5 days, <code>Today</code>), <code>Volume</code>, and the
<code>Year</code>.</li>
<li><strong>Explore Data:</strong> A correlation matrix
(<code>cor(Smarket[,-9])</code>) is computed, and a plot of
<code>Volume</code> over time is generated.</li>
<li><strong>Split Data:</strong> The data is split into a training set
(Years 2001-2004) and a test set (Year 2005).
<ul>
<li><code>train &lt;- (Year&lt;2005)</code></li>
<li><code>Smarket.2005 &lt;- Smarket[!train,]</code></li>
<li><code>Direction.2005 &lt;- Direction[!train]</code></li>
<li>The test set has 252 observations.</li>
</ul></li>
<li><strong>加载数据</strong>：加载“ISLR”库，并探索“Smarket”数据集。该数据集包含每日百分比收益率（前
5 天的“Lag1”…“Lag5”，“今日”）、“成交量”和“年份”。</li>
<li><strong>探索数据</strong>：计算相关矩阵
(<code>cor(Smarket[,-9])</code>)，并生成“成交量”随时间变化的图表。</li>
<li><strong>拆分数据</strong>：将数据拆分为训练集（年份
2001-2004）和测试集（年份 2005）。
<ul>
<li><code>train &lt;- (Year&lt;2005)</code></li>
<li><code>Smarket.2005 &lt;- Smarket[!train,]</code></li>
<li><code>Direction.2005 &lt;- Direction[!train]</code></li>
<li>测试集包含 252 个观测值。</li>
</ul></li>
</ol>
<h3 id="model-1-logistic-regression-all-predictors-slide-90">Model 1:
Logistic Regression (All Predictors) (Slide 90)</h3>
<ul>
<li><strong>Model:</strong> A logistic regression model is fit on the
training data using <em>all</em> predictors.
<ul>
<li><code>glm.fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data=Smarket, family=binomial, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> The model is used to predict the
direction for the 2005 test data.
<ul>
<li><code>glm.probs &lt;- predict(glm.fit, Smarket.2005, type="response")</code></li>
<li>A threshold of 0.5 is used to classify: if <span
class="math inline">\(P(\text{Up}) &gt; 0.5\)</span>, predict “Up”.</li>
</ul></li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.5198 (or <strong>48.0%
accuracy</strong>).</li>
<li><strong>Conclusion:</strong> This is “not good!”—it’s worse than
flipping a coin. This suggests the model is either too complex or the
predictors are not useful.</li>
</ul></li>
</ul>
<h3 id="model-2-logistic-regression-lag1-lag2-slide-91">Model 2:
Logistic Regression (Lag1 &amp; Lag2) (Slide 91)</h3>
<ul>
<li><strong>Model:</strong> Based on the poor results, a simpler model
is tried, using only <code>Lag1</code> and <code>Lag2</code>.
<ul>
<li><code>glm.fit &lt;- glm(Direction ~ Lag1 + Lag2, data=Smarket, family=binomial, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> Predictions are made on the 2005 test
set.</li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.4404 (or <strong>55.95%
accuracy</strong>). This is an improvement.</li>
<li><strong>Confusion Matrix:</strong> | | True Down | True Up | | :— |
:— | :— | | <strong>Pred Down</strong> | 77 | 69 | | <strong>Pred
Up</strong> | 35 | 71 |</li>
<li><strong>ROC and AUC:</strong> The ROC (Receiver Operating
Characteristic) curve is plotted, and the AUC (Area Under the Curve) is
calculated.</li>
<li><strong>AUC Value:</strong> <strong>0.5584</strong>. This is very
close to 0.5 (which represents a random-chance model), indicating that
the model has very weak predictive power, even though its accuracy is
above 50%.</li>
</ul></li>
</ul>
<h3 id="model-3-lda-lag1-lag2-slide-92">Model 3: LDA (Lag1 &amp; Lag2)
(Slide 92)</h3>
<ul>
<li><strong>Model:</strong> LDA is now performed using the same setup:
<code>Lag1</code> and <code>Lag2</code> as predictors, trained on the
2001-2004 data.
<ul>
<li><code>library(MASS)</code></li>
<li><code>lda.fit &lt;- lda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> Predictions are made on the 2005 test
set.
<ul>
<li><code>lda.pred &lt;- predict(lda.fit, Smarket.2005)</code></li>
</ul></li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.4404 (or <strong>55.95%
accuracy</strong>).</li>
<li><strong>Confusion Matrix:</strong> | | True Down | True Up | | :— |
:— | :— | | <strong>Pred Down</strong> | 77 | 69 | | <strong>Pred
Up</strong> | 35 | 71 |</li>
<li><strong>Observation:</strong> The confusion matrix and accuracy are
<em>identical</em> to the logistic regression model.</li>
</ul></li>
</ul>
<h3 id="final-comparison-slide-93">Final Comparison (Slide 93)</h3>
<ul>
<li><strong>ROC and AUC for LDA:</strong> The ROC curve for the LDA
model is plotted.</li>
<li><strong>AUC Value:</strong> <strong>0.5584</strong>.</li>
<li><strong>Main Conclusion:</strong> As highlighted in the green box,
<strong>“LDA has identical performance as Logistic regression!”</strong>
In this specific practical example, using these two predictors, both
linear methods produce the exact same confusion matrix, the same
accuracy (56%), and the same AUC (0.558). This reinforces the
theoretical idea that both are fitting a linear boundary.</li>
</ul>
<h3 id="最终比较幻灯片-93">最终比较（幻灯片 93）</h3>
<ul>
<li><strong>LDA 的 ROC 和 AUC：</strong>绘制了 LDA 模型的 ROC
曲线。</li>
<li><strong>AUC 值：</strong>0.5584**。</li>
<li><strong>主要结论：</strong>如绿色方框所示，“LDA 的性能与 Logistic
回归相同！”**
在这个具体的实际示例中，使用这两个预测变量，两种线性方法都产生了完全相同的混淆矩阵、相同的准确率（56%）和相同的
AUC（0.558）。这强化了两者均拟合线性边界的理论观点。</li>
</ul>
<h2 id="r-example-on-smarket-data-continued">4.7 R Example on Smarket
Data (Continued)</h2>
<p>The previous slides showed that Logistic Regression and Linear
Discriminant Analysis (LDA) had <strong>identical performance</strong>
on the Smarket dataset (using <code>Lag1</code> and <code>Lag2</code>),
both achieving 56% test accuracy and an AUC of 0.558. The analysis now
tests a more flexible method, QDA.</p>
<h3 id="model-3-qda-lag1-lag2-slides-94-95">Model 3: QDA (Lag1 &amp;
Lag2) (Slides 94-95)</h3>
<ul>
<li><strong>Model:</strong> A Quadratic Discriminant Analysis (QDA)
model is fit on the same training data (2001-2004) using only the
<code>Lag1</code> and <code>Lag2</code> predictors.
<ul>
<li><code>qda.fit &lt;- qda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> The model is used to predict the market
direction for the 2005 test set.</li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Accuracy:</strong> The model achieves a test accuracy
of <strong>0.5992 (or 60%)</strong>.</li>
<li><strong>AUC:</strong> The Area Under the Curve (AUC) for the QDA
model is <strong>0.562</strong>.</li>
</ul></li>
<li><strong>Conclusion:</strong> As the slide highlights, <strong>“QDA
has better test performance than LDA and Logistic
regression!”</strong></li>
</ul>
<h3 id="smarket-example-summary">Smarket Example Summary</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Model Type</th>
<th style="text-align: left;">Test Accuracy</th>
<th style="text-align: left;">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Logistic Regression</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">~56%</td>
<td style="text-align: left;">0.558</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LDA</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">~56%</td>
<td style="text-align: left;">0.558</td>
</tr>
<tr>
<td style="text-align: left;"><strong>QDA</strong></td>
<td style="text-align: left;">Quadratic</td>
<td style="text-align: left;"><strong>~60%</strong></td>
<td style="text-align: left;"><strong>0.562</strong></td>
</tr>
</tbody>
</table>
<p>This practical example reinforces the lessons from the simulations
(Section 4.6). The two linear methods (LDA, Logistic) had identical
performance. The more flexible, non-linear QDA model performed better,
suggesting that the true decision boundary between “Up” and “Down”
(based on <code>Lag1</code> and <code>Lag2</code>) is not perfectly
linear.</p>
<h2 id="kernel-lda">4.8 Kernel LDA</h2>
<p>This new section introduces an even more advanced non-linear method,
Kernel LDA.</p>
<h3 id="the-problem-linear-inseparability-slide-97">The Problem: Linear
Inseparability (Slide 97)</h3>
<p>The section starts with a clear visual example. A dataset of two
concentric circles (a “donut” shape) is <strong>linearly
inseparable</strong>. It is impossible to draw a single straight line to
separate the inner (purple) class from the outer (yellow) class.</p>
<h3 id="the-solution-the-kernel-trick-slides-97-99">The Solution: The
Kernel Trick (Slides 97, 99)</h3>
<ol type="1">
<li><strong>Nonlinear Transformation:</strong> The data is “lifted” into
a higher-dimensional <em>feature space</em> using a <strong>nonlinear
transformation</strong>, <span class="math inline">\(x \mapsto
\phi(x)\)</span>. In the example on the slide, the 2D data is
transformed, and in this new space, the two classes <em>become</em>
<strong>linearly separable</strong>.</li>
<li><strong>The “Kernel Trick”:</strong> The main idea (from slide 99)
is that we don’t need to explicitly compute this complex transformation
<span class="math inline">\(\phi(x)\)</span>. LDA (based on Fisher’s
approach) only requires inner products of the data points. The “kernel
trick” allows us to replace the inner product in the high-dimensional
feature space (<span class="math inline">\(x_i^T x_j\)</span>) with a
simple <strong>kernel function</strong>, <span
class="math inline">\(k(x_i, x_j)\)</span>, computed in the original,
low-dimensional space.
<ul>
<li>An example of such a kernel is the <strong>Gaussian (RBF)
kernel</strong>: <span class="math inline">\(k(x_i, x_j) \propto
e^{-\|x_i - x_j\|^2 / \sigma^2}\)</span>.</li>
</ul></li>
</ol>
<h3 id="academic-foundations-slide-98">Academic Foundations (Slide
98)</h3>
<p>This method is based on foundational academic papers that generalized
linear methods using kernels: * <strong>Fisher discriminant analysis
with kernels</strong> (Mika, 1999) * <strong>Generalized Discriminant
Analysis Using a Kernel Approach</strong> (Baudat, 2000) *
<strong>Kernel principal component analysis</strong> (Schölkopf,
1997)</p>
<p>In short, Kernel LDA is an extension of LDA that uses the kernel
trick to find a linear boundary in a high-dimensional feature space,
which corresponds to a highly non-linear boundary in the original
space.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/27/QM9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/27/QM9/" class="post-title-link" itemprop="url">QM9 Dataset</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-09-27 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-27T21:00:00+08:00">2025-09-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-29 03:57:13" itemprop="dateModified" datetime="2025-09-29T03:57:13+08:00">2025-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/dataset/" itemprop="url" rel="index"><span itemprop="name">dataset</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="qm9-数据集的xyz格式详解">1. QM9 数据集的XYZ格式详解</h3>
<p>这个数据集使用的 “XYZ-like”
格式是一种<strong>扩展的、非标准的XYZ格式</strong>。</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 43%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr>
<th>行号</th>
<th>内容</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>第 1 行</strong></td>
<td><code>na</code></td>
<td>一个整数，代表分子中的原子总数。</td>
</tr>
<tr>
<td><strong>第 2 行</strong></td>
<td><code>Properties 1-17</code></td>
<td>包含17个理化性质的数值，用制表符或空格分隔。</td>
</tr>
<tr>
<td><strong>第 3 到 na+2 行</strong></td>
<td><code>Element  x  y  z  charge</code></td>
<td>每行代表一个原子。依次是：元素符号、x/y/z坐标（单位：埃）、Mulliken部分电荷（单位：e）。</td>
</tr>
<tr>
<td><strong>第 na+3 行</strong></td>
<td><code>Frequencies</code></td>
<td>分子的振动频率（3na-5或3na-6个）。</td>
</tr>
<tr>
<td><strong>第 na+4 行</strong></td>
<td><code>SMILES_GDB9   SMILES_relaxed</code></td>
<td>来自GDB9的SMILES字符串和弛豫后的几何构型的SMILES字符串。</td>
</tr>
<tr>
<td><strong>第 na+5 行</strong></td>
<td><code>InChI_GDB9    InChI_relaxed</code></td>
<td>对应的InChI字符串。</td>
</tr>
</tbody>
</table>
<p><strong>与标准XYZ格式对比：</strong> *
<strong>标准格式</strong>只有第1行（原子数）、第2行（注释）和后续的原子坐标行（仅含元素和xyz坐标）。
*
<strong>QM9格式</strong>在第2行插入了大量属性数据，在原子坐标行增加了电荷列，并在文件末尾附加了频率、SMILES和InChI信息。</p>
<h3 id="readme">2. readme</h3>
<ol type="1">
<li><strong>数据集核心内容</strong>:
<ul>
<li>它包含了<strong>133,885个</strong>小型有机分子（由H, C, N, O,
F元素组成）的量子化学计算数据。</li>
<li>所有分子的几何构型都经过了<strong>DFT/B3LYP/6-31G(2df,p)</strong>水平的优化。</li>
<li><code>dsC7O2H10nsd.xyz.tar.bz2</code>是该数据集的一个子集，专门包含<strong>6,095个C₇H₁₀O₂的同分异构体</strong>，其能量学性质在更高精度的<strong>G4MP2</strong>理论水平下计算。</li>
</ul></li>
<li><strong>文件结构与格式</strong>:
<ul>
<li>明确指出每个分子存储在单独的<code>.xyz</code>文件中，并详细描述了上述的<strong>非标准XYZ扩展格式</strong>。</li>
<li>详细列出了记录在文件第2行的<strong>17种理化性质</strong>，包括转动常数(A,
B,
C)、偶极矩(mu)、HOMO/LUMO能级、零点振动能(zpve)、内能(U)、焓(H)和吉布斯自由能(G)等。</li>
</ul></li>
<li><strong>数据来源与计算方法</strong>:
<ul>
<li>数据源于<strong>GDB-9</strong>化学数据库。</li>
<li>主要使用了两种量子化学理论水平：<strong>B3LYP</strong>用于大部分属性计算，<strong>G4MP2</strong>用于C₇H₁₀O₂子集的能量计算。</li>
</ul></li>
<li><strong>引用要求</strong>:
<ul>
<li>文件明确要求，如果使用该数据集，需要引用Raghunathan
Ramakrishnan等人在2014年发表于《Scientific Data》的论文。</li>
</ul></li>
<li><strong>其他信息</strong>:
<ul>
<li>提供了一些额外文件（如<code>validation.txt</code>,
<code>uncharacterized.txt</code>）的说明。</li>
<li>提到了数据集中有少数几个分子在几何优化时难以收敛。</li>
</ul></li>
</ol>
<h3 id="可视化">3. 可视化</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import ase.io</span><br><span class="line">import nglview as nv</span><br><span class="line">import io</span><br><span class="line"></span><br><span class="line">def parse_qm9_xyz(file_path):</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    Parses a QM9 extended XYZ file and returns a standard XYZ string.</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    with open(file_path, <span class="string">&#x27;r&#x27;</span>) as f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First line is the number of atoms</span></span><br><span class="line">    num_atoms = int(lines[0].strip())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The next line is properties (skip it)</span></span><br><span class="line">    <span class="comment"># The next num_atoms lines are the coordinates</span></span><br><span class="line">    coord_lines = lines[2:2+num_atoms]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Rebuild a standard XYZ format string in memory</span></span><br><span class="line">    standard_xyz = f<span class="string">&quot;&#123;num_atoms&#125;\n&quot;</span></span><br><span class="line">    standard_xyz += <span class="string">&quot;Comment line\n&quot;</span> <span class="comment"># Add a standard comment line</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> coord_lines:</span><br><span class="line">        parts = line.split()</span><br><span class="line">        <span class="comment"># Keep only the element and the x, y, z coordinates</span></span><br><span class="line">        standard_xyz += f<span class="string">&quot;&#123;parts[0]&#125; &#123;parts[1]&#125; &#123;parts[2]&#125; &#123;parts[3]&#125;\n&quot;</span></span><br><span class="line">        </span><br><span class="line">    <span class="built_in">return</span> standard_xyz</span><br><span class="line"></span><br><span class="line"><span class="comment"># Path to your data file</span></span><br><span class="line">file_path = <span class="string">&quot;/root/QM9/QM9/Data_for_6095_constitutional_isomers_of_C7H10O2.xyz/dsC7O2H10nsd_0001.xyz&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Parse the special file format into a standard XYZ string</span></span><br><span class="line">standard_xyz_data = parse_qm9_xyz(file_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. ASE reads the standard XYZ data from the string variable</span></span><br><span class="line"><span class="comment">#    We use io.StringIO to make the string behave like a file</span></span><br><span class="line">atoms = ase.io.read(io.StringIO(standard_xyz_data), format=<span class="string">&quot;xyz&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create the nglview visualization widget</span></span><br><span class="line">view = nv.show_ase(atoms)</span><br><span class="line">view.add_ball_and_stick()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display the widget in the notebook output</span></span><br><span class="line">view</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol type="1">
<li><strong>定义解析函数 <code>parse_qm9_xyz</code></strong>:
<ul>
<li><strong>目的</strong>:
将这个函数作为专门处理QM9特殊格式的工具。代码主体清晰，易于复用。</li>
<li><strong>读取文件</strong>: <code>with open(...)</code>
安全地打开文件，并用 <code>f.readlines()</code>
将文件所有行一次性读入一个列表 <code>lines</code> 中。</li>
<li><strong>提取原子数量</strong>:
<code>num_atoms = int(lines[0].strip())</code>
读取第一行（<code>lines[0]</code>），去除可能存在的空格（<code>.strip()</code>），并将其转换为整数。这是构建标准XYZ格式的必要信息。</li>
<li><strong>提取坐标信息</strong>:
<code>coord_lines = lines[2:2+num_atoms]</code>
标信息从第3行开始（索引为2），持续<code>num_atoms</code>行。通过列表切片，精确地提取出所有包含原子坐标的行，跳过了第2行的属性信息。</li>
<li><strong>构建标准XYZ格式字符串</strong>:
<ul>
<li>创建一个名为 <code>standard_xyz</code> 的新字符串。</li>
<li>首先，将原子数量和换行符写入。</li>
<li>然后，添加一行标准的注释（“Comment
line”），这是标准XYZ格式所要求的。</li>
<li>最后，遍历刚刚提取的 <code>coord_lines</code> 列表。对于每一行，使用
<code>.split()</code>
将其拆分成多个部分（例如：<code>['C', 'x', 'y', 'z', 'charge']</code>）。只取前四部分（元素符号和xyz坐标），并重新组合成新的一行，<strong>从而丢弃了末尾的Mulliken电荷数据</strong>。</li>
</ul></li>
<li><strong>返回结果</strong>:
函数返回一个包含了标准XYZ格式数据的、干净的字符串。</li>
</ul></li>
<li><strong>主程序执行流程</strong>:
<ul>
<li><strong>调用函数</strong>:
<code>standard_xyz_data = parse_qm9_xyz(file_path)</code>
调用上面的函数，完成从文件到标准格式字符串的转换。</li>
<li><strong>在内存中读取</strong>:
<code>ase.io.read(io.StringIO(standard_xyz_data), format="xyz")</code>
这一步非常高效。<code>io.StringIO</code> 将我们的字符串变量
<code>standard_xyz_data</code>
模拟成一个内存中的文本文件。这样，<code>ase.io.read</code>
就可以直接读取它，而无需先将清洗后的数据写入一个临时文件再读取，节省了磁盘I/O操作。</li>
<li><strong>可视化</strong>: 接下来的代码 (<code>nv.show_ase</code>等)
就和最初的设想一样了，因为此时 <code>atoms</code>
对象已经是通过标准、干净的数据成功创建的了。</li>
</ul></li>
</ol>
<p><img src="/imgs/QM9/C7O2H10/C7O2H10.png" alt="C7O2H10"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/27/fusionnetwork/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/27/fusionnetwork/" class="post-title-link" itemprop="url">FusionProt - 论文阅读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-09-27 11:00:00" itemprop="dateCreated datePublished" datetime="2025-09-27T11:00:00+08:00">2025-09-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-29 03:56:14" itemprop="dateModified" datetime="2025-09-29T03:56:14+08:00">2025-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-Reading/" itemprop="url" rel="index"><span itemprop="name">Paper Reading</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Fusing Sequence and Structural Information for Unified Protein
Representation Learning</p>
<p><a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=imcinaOHod">FusionProt</a></p>
<h2 id="蛋白质表示学习">1 蛋白质表示学习：</h2>
<ul>
<li><strong>内容</strong>:</li>
</ul>
<p>FusionProt :可学习融合
token和迭代双向信息交换，实现序列与结构的动态协同学习，而非静态拼接。</p>
<h2 id="一维1d氨基酸序列和三维3d空间结构">2.
一维（1D）氨基酸序列和三维（3D）空间结构：</h2>
<ul>
<li><p><strong>单模态依赖:</strong>
ProteinBERT、ESM-2仅基于序列</p></li>
<li><p><strong>静态融合缺陷 :</strong>ESM-GearNet、SaProt
结合序列与结构，但采用 “单向 / 一次性融合”</p></li>
</ul>
<p>好的，完全没有问题。这是对 <code>FusionNetwork</code>
模型架构代码的中文复述分析。</p>
<h2 id="模型总体">3. 模型总体</h2>
<p><img src="/imgs/fusionProt/FusionProt.png" alt="fusion">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@R.register(<span class="params"><span class="string">&quot;models.FusionNetwork&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FusionNetwork</span>(nn.Module, core.Configurable):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sequence_model, structure_model, fusion=<span class="string">&quot;series&quot;</span>, cross_dim=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(FusionNetwork, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.sequence_model = sequence_model</span><br><span class="line">        <span class="variable language_">self</span>.structure_model = structure_model</span><br><span class="line">        <span class="variable language_">self</span>.output_dim = sequence_model.output_dim + structure_model.output_dim</span><br><span class="line">        <span class="variable language_">self</span>.inject_step = <span class="number">5</span>   <span class="comment"># (sequence_layers / structure_layers) layers</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li><strong><code>class FusionNetwork(...)</code></strong>:
定义了模型类，它继承自 PyTorch 的基础模块 <code>nn.Module</code>。</li>
<li><strong><code>__init__(...)</code></strong>:
构造函数，接收已经初始化好的 <code>sequence_model</code> 和
<code>structure_model</code> 作为输入。</li>
<li><strong><code>self.output_dim</code></strong>:
定义了模型最终输出特征的维度。因为最后会将两个模型的特征拼接起来，所以是两者输出维度之和。</li>
<li><strong><code>self.inject_step = 5</code></strong>:定义了信息“注入”或“交流”的频率。这里设置为
5，意味着<strong>每经过序列模型的 5
层，就会进行一次信息交换</strong>。</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Structure embeddings layer</span></span><br><span class="line">raw_input_dim = <span class="number">21</span>  <span class="comment"># amino acid tokens</span></span><br><span class="line"><span class="variable language_">self</span>.structure_embed_linear = nn.Linear(raw_input_dim, structure_model.input_dim)</span><br><span class="line"><span class="variable language_">self</span>.embedding_batch_norm = nn.BatchNorm1d(structure_model.input_dim)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>self.structure_embed_linear</code></strong>:
一个线性层，用于将原始的结构输入（比如 21
种氨基酸的独热编码）转换为结构模型（GNN）所期望的输入维度。</li>
<li><strong><code>self.embedding_batch_norm</code></strong>:
批归一化层，用于稳定结构嵌入层的训练过程。</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normal Initialization of the 3D structure token</span></span><br><span class="line">structure_token = nn.Parameter(torch.Tensor(structure_model.input_dim).unsqueeze(<span class="number">0</span>))</span><br><span class="line">nn.init.normal_(structure_token, mean=<span class="number">0.0</span>, std=<span class="number">0.01</span>)</span><br><span class="line"><span class="variable language_">self</span>.structure_token = nn.Parameter(structure_token.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>self.structure_token</code></strong>: 一个可学习的向量
(<code>nn.Parameter</code>)。这个“令牌”不代表任何真实的原子或氨基酸，而是一个抽象的载体。在训练过程中，它将<strong>学习如何编码和表示整个蛋白质的全局
3D 结构信息</strong>。它就像一个信息信使。</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Linear Transformation between structure to sequential spaces</span></span><br><span class="line"><span class="variable language_">self</span>.structure_linears = nn.ModuleList([...])</span><br><span class="line"><span class="variable language_">self</span>.seq_linears = nn.ModuleList([...])</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>self.structure_linears</code> /
<code>self.seq_linears</code></strong>:
序列模型和结构模型内部处理的特征向量维度可能不同。当“3D
令牌”需要在两个模型之间传递时，这些线性层负责将它的表示从一个模型的特征空间转换到另一个模型的特征空间。</li>
</ul>
<h2 id="前向">4. 前向</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, graph, <span class="built_in">input</span>, all_loss=<span class="literal">None</span>, metric=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># Build a new protein graph with the 3D token (the lase node)</span></span><br><span class="line">    new_graph = <span class="variable language_">self</span>.build_protein_graph_with_3d_token(graph)</span><br></pre></td></tr></table></figure>
<ul>
<li>首先调用辅助函数，将输入的蛋白质图谱进行改造：为图谱增加一个代表“3D
令牌”的新节点，并将这个新节点与图中所有其他节点连接起来。</li>
</ul>
<h5 id="序列模型的初始化"><strong>序列模型的初始化</strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequence (ESM) model initialization</span></span><br><span class="line">sequence_input = <span class="variable language_">self</span>.sequence_model.mapping[graph.residue_type]</span><br><span class="line">sequence_input[sequence_input == -<span class="number">1</span>] = graph.residue_type[sequence_input == -<span class="number">1</span>]</span><br><span class="line">size = graph.num_residues</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check if sequence size is not bigger than max seq length</span></span><br><span class="line"><span class="keyword">if</span> (size &gt; <span class="variable language_">self</span>.sequence_model.max_input_length).<span class="built_in">any</span>():</span><br><span class="line">    starts = size.cumsum(<span class="number">0</span>) - size</span><br><span class="line">    size = size.clamp(<span class="built_in">max</span>=<span class="variable language_">self</span>.sequence_model.max_input_length)</span><br><span class="line">    ends = starts + size</span><br><span class="line">    mask = functional.multi_slice_mask(starts, ends, graph.num_residues)</span><br><span class="line">    sequence_input = sequence_input[mask]</span><br><span class="line">    graph = graph.subresidue(mask)</span><br><span class="line">size_ext = size</span><br><span class="line"></span><br><span class="line"><span class="comment"># BOS == CLS</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.alphabet.prepend_bos:</span><br><span class="line">    bos = torch.ones(graph.batch_size, dtype=torch.long, device=<span class="variable language_">self</span>.sequence_model.device) * <span class="variable language_">self</span>.sequence_model.alphabet.cls_idx</span><br><span class="line">    sequence_input, size_ext = functional._extend(bos, torch.ones_like(size_ext), sequence_input, size_ext)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.alphabet.append_eos:</span><br><span class="line">    eos = torch.ones(graph.batch_size, dtype=torch.long, device=<span class="variable language_">self</span>.sequence_model.device) * <span class="variable language_">self</span>.sequence_model.alphabet.eos_idx</span><br><span class="line">    sequence_input, size_ext = functional._extend(sequence_input, size_ext, eos, torch.ones_like(size_ext))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Padding</span></span><br><span class="line">tokens = functional.variadic_to_padded(sequence_input, size_ext, value=<span class="variable language_">self</span>.sequence_model.alphabet.padding_idx)[<span class="number">0</span>]</span><br><span class="line">repr_layers = [<span class="variable language_">self</span>.sequence_model.repr_layer]</span><br><span class="line"><span class="keyword">assert</span> tokens.ndim == <span class="number">2</span></span><br><span class="line">padding_mask = tokens.eq(<span class="variable language_">self</span>.sequence_model.model.padding_idx)  <span class="comment"># B, T</span></span><br></pre></td></tr></table></figure>
<ul>
<li>序列数据进行 Transformer 模型（如 ESM）所需的标准预处理。</li>
<li>包括添加序列开始（BOS）和结束（EOS）标记，以及将所有序列填充（Padding）到相同长度，以便进行批处理。</li>
</ul>
<h5 id="模型初始化与初次融合"><strong>模型初始化与初次融合</strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sequence embedding layer</span></span><br><span class="line">x = <span class="variable language_">self</span>.sequence_model.model.embed_scale * <span class="variable language_">self</span>.sequence_model.model.embed_tokens(tokens)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.model.token_dropout:</span><br><span class="line">    x.masked_fill_((tokens == <span class="variable language_">self</span>.sequence_model.model.mask_idx).unsqueeze(-<span class="number">1</span>), <span class="number">0.0</span>)</span><br><span class="line">    <span class="comment"># x: B x T x C</span></span><br><span class="line">    mask_ratio_train = <span class="number">0.15</span> * <span class="number">0.8</span></span><br><span class="line">    src_lengths = (~padding_mask).<span class="built_in">sum</span>(-<span class="number">1</span>)</span><br><span class="line">    mask_ratio_observed = (tokens == <span class="variable language_">self</span>.sequence_model.model.mask_idx).<span class="built_in">sum</span>(-<span class="number">1</span>).to(x.dtype) / src_lengths</span><br><span class="line">    x = x * (<span class="number">1</span> - mask_ratio_train) / (<span class="number">1</span> - mask_ratio_observed)[:, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Structure model initialization</span></span><br><span class="line">structure_hiddens = []</span><br><span class="line">batch_size = graph.batch_size</span><br><span class="line">structure_embedding = <span class="variable language_">self</span>.embedding_batch_norm(<span class="variable language_">self</span>.structure_embed_linear(<span class="built_in">input</span>))</span><br><span class="line">structure_token_batched = <span class="variable language_">self</span>.structure_token.unsqueeze(<span class="number">0</span>).expand(batch_size, -<span class="number">1</span>)</span><br><span class="line">structure_input = torch.cat([structure_embedding.squeeze(<span class="number">1</span>), structure_token_batched], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the 3D token representation</span></span><br><span class="line">structure_token_expanded = <span class="variable language_">self</span>.structure_token.unsqueeze(<span class="number">0</span>).expand(x.size(<span class="number">0</span>), -<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">x = torch.cat((x[:, :-<span class="number">1</span>], structure_token_expanded, x[:, -<span class="number">1</span>:]), dim=<span class="number">1</span>)</span><br><span class="line">padding_mask = torch.cat([padding_mask[:, :-<span class="number">1</span>],</span><br><span class="line">                          torch.zeros(padding_mask.size(<span class="number">0</span>), <span class="number">1</span>).to(padding_mask), padding_mask[:, -<span class="number">1</span>:]], dim=<span class="number">1</span>)</span><br><span class="line">size_ext += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> padding_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    x = x * (<span class="number">1</span> - padding_mask.unsqueeze(-<span class="number">1</span>).type_as(x))</span><br><span class="line"></span><br><span class="line">repr_layers = <span class="built_in">set</span>(repr_layers)</span><br><span class="line">hidden_representations = &#123;&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="number">0</span> <span class="keyword">in</span> repr_layers:</span><br><span class="line">    hidden_representations[<span class="number">0</span>] = x</span><br><span class="line"></span><br><span class="line"><span class="comment"># (B, T, E) =&gt; (T, B, E)</span></span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> padding_mask.<span class="built_in">any</span>():</span><br><span class="line">    padding_mask = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>将 3D 令牌插入序列。</strong>
<ol type="1">
<li>为序列数据生成初始的词嵌入表示 <code>x</code>。</li>
<li>将 <code>self.structure_token</code> 的初始状态插入到序列嵌入
<code>x</code> 中，通常是放在序列结束标记（EOS）之前。</li>
<li>序列模型看到的输入序列变成了
<code>[BOS, 残基1, 残基2, ..., 残基N, **3D令牌**, EOS]</code>
的形式。</li>
</ol></li>
</ul>
<h5 id="融合循环"><strong>融合循环 </strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> seq_layer_idx, seq_layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.sequence_model.model.layers):</span><br><span class="line">    x, attn = seq_layer(</span><br><span class="line">        x,</span><br><span class="line">        self_attn_padding_mask=padding_mask,</span><br><span class="line">        need_head_weights=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> (seq_layer_idx + <span class="number">1</span>) <span class="keyword">in</span> repr_layers:</span><br><span class="line">        hidden_representations[seq_layer_idx + <span class="number">1</span>] = x.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>模型开始逐层遍历序列模型的所有层（例如 Transformer
的编码器层）。<code>x</code> 在每一层都会被更新。</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> seq_layer_idx &gt; <span class="number">0</span> <span class="keyword">and</span> seq_layer_idx % <span class="variable language_">self</span>.inject_step == <span class="number">0</span>:</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>信息注入点</strong>：每当层数的索引能被
<code>inject_step</code> (即 5) 整除时，就触发一次信息交换。</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 从序列中提取 3D 令牌的表示</span></span><br><span class="line"><span class="keyword">if</span> structure_layer_index == <span class="number">0</span>:</span><br><span class="line">    structure_input = torch.cat((structure_input[:-<span class="number">1</span> * batch_size],  x[-<span class="number">2</span>, :, :]), dim=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    structure_input = torch.cat((structure_input[:-<span class="number">1</span> * batch_size],</span><br><span class="line">                                 <span class="variable language_">self</span>.seq_linears[structure_layer_index](x[-<span class="number">2</span>, :, :])), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 用结构模型的一层来处理</span></span><br><span class="line">hidden = <span class="variable language_">self</span>.structure_model.layers[structure_layer_index](new_graph, structure_input)</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.structure_model.short_cut <span class="keyword">and</span> hidden.shape == structure_input.shape:</span><br><span class="line">    hidden = hidden + structure_input</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.structure_model.batch_norm:</span><br><span class="line">    hidden = <span class="variable language_">self</span>.structure_model.batch_norms[structure_layer_index](hidden)</span><br><span class="line"></span><br><span class="line">structure_hiddens.append(hidden)</span><br><span class="line">structure_input = hidden</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 将更新后的 3D 令牌表示插回序列</span></span><br><span class="line">updated_structure_token = <span class="variable language_">self</span>.structure_linears[...](structure_input[-<span class="number">1</span> * batch_size:])</span><br><span class="line">x = torch.cat((x[:-<span class="number">2</span>, :, :], updated_structure_token.unsqueeze(<span class="number">0</span>), x[-<span class="number">1</span>:, :, :]), dim=<span class="number">0</span>)</span><br><span class="line">structure_layer_index += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>信息流程</strong>：
<ol type="1">
<li><strong>从序列到结构</strong>：模型从序列表示 <code>x</code>
中提取出“3D
令牌”的最新向量。这个向量此时已经吸收了前面几层序列模型的上下文信息。然后，通过（<code>seq_linears</code>）将其转换后，更新到结构模型的输入中。</li>
<li><strong>结构信息处理</strong>：运行一层结构模型（GNN）。GNN
根据图的连接关系更新所有节点的表示，当然也包括“3D
令牌”这个特殊节点。</li>
<li><strong>从结构到序列</strong>：从 GNN 的输出中，再次提取出“3D
令牌”的向量。这个向量包含更新后的结构信息。再通过（<code>structure_linears</code>）转换后，把它<strong>插回</strong>到序列表示
<code>x</code> 中，替换掉旧的版本。</li>
</ol></li>
</ul>
<p>这个循环不断重复。</p>
<h5 id="输出"><strong>输出</strong></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Structural Output</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.structure_model.concat_hidden:</span><br><span class="line">    structure_node_feature = torch.cat(structure_hiddens, dim=-<span class="number">1</span>)[:-<span class="number">1</span> * batch_size]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    structure_node_feature = structure_hiddens[-<span class="number">1</span>][:-<span class="number">1</span> * batch_size]</span><br><span class="line"></span><br><span class="line">structure_graph_feature = <span class="variable language_">self</span>.structure_model.readout(graph, structure_node_feature)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sequence Output</span></span><br><span class="line">x = <span class="variable language_">self</span>.sequence_model.model.emb_layer_norm_after(x)</span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># (T, B, E) =&gt; (B, T, E)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># last hidden representation should have layer norm applied</span></span><br><span class="line"><span class="keyword">if</span> (seq_layer_idx + <span class="number">1</span>) <span class="keyword">in</span> repr_layers:</span><br><span class="line">    hidden_representations[seq_layer_idx + <span class="number">1</span>] = x</span><br><span class="line">x = <span class="variable language_">self</span>.sequence_model.model.lm_head(x)</span><br><span class="line"></span><br><span class="line">output = &#123;<span class="string">&quot;logits&quot;</span>: x, <span class="string">&quot;representations&quot;</span>: hidden_representations&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sequence (ESM) model outputs</span></span><br><span class="line">residue_feature = output[<span class="string">&quot;representations&quot;</span>][<span class="variable language_">self</span>.sequence_model.repr_layer]</span><br><span class="line">residue_feature = functional.padded_to_variadic(residue_feature, size_ext)</span><br><span class="line">starts = size_ext.cumsum(<span class="number">0</span>) - size_ext</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.sequence_model.alphabet.prepend_bos:</span><br><span class="line">    starts = starts + <span class="number">1</span></span><br><span class="line">ends = starts + size</span><br><span class="line">mask = functional.multi_slice_mask(starts, ends, <span class="built_in">len</span>(residue_feature))</span><br><span class="line">residue_feature = residue_feature[mask]</span><br><span class="line">graph_feature = <span class="variable language_">self</span>.sequence_model.readout(graph, residue_feature)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine both models outputs</span></span><br><span class="line">node_feature = torch.cat(...)</span><br><span class="line">graph_feature = torch.cat(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> &#123;<span class="string">&quot;graph_feature&quot;</span>: graph_feature, <span class="string">&quot;node_feature&quot;</span>: node_feature&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>提取输出</strong>：循环结束后，分别从两个模型中提取最终的特征表示。</li>
<li><strong>读出（Readout）</strong>：使用一个“读出函数”（如求和或平均）将节点级别的特征聚合成一个代表整个蛋白质的图级别特征。</li>
<li><strong>最终组合</strong>：将来自序列模型和结构模型的节点特征（<code>node_feature</code>）和图特征（<code>graph_feature</code>）分别拼接（concatenate）起来。</li>
<li><strong>返回结果</strong>：返回一个包含组合后特征的字典，可用于下游任务（如功能预测、属性回归等）。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/26/5120C4-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/26/5120C4-2/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-26 21:00:00 / 修改时间：20:48:45" itemprop="dateCreated datePublished" datetime="2025-09-26T21:00:00+08:00">2025-09-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - 计算能源材料和电子结构模拟 Lecture-4</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="monte-carlo-mc-method">1 Monte Carlo (MC) Method:</h2>
<ul>
<li><strong>内容</strong>:</li>
</ul>
<p>This whiteboard provides a concise but detailed overview of two
important and related simulation techniques in computational physics and
chemistry: the Metropolis Monte Carlo (MC) method and Hamiltonian (or
Hybrid) Monte Carlo (HMC). Here is a detailed breakdown of the concepts
presented.</p>
<h3 id="metropolis-monte-carlo-mc-method">1. Metropolis Monte Carlo (MC)
Method</h3>
<p>The heading “Metropolis MC method” introduces a foundational
algorithm in statistical mechanics. Metropolis Monte Carlo is a method
used to generate a sequence of states for a system, allowing for the
calculation of average properties. 左上角的这一部分介绍了基础的
<strong>Metropolis Monte Carlo</strong>
算法。它是一种生成状态序列的方法，使得处于任何状态的概率都符合期望的概率分布（在物理学中通常是玻尔兹曼分布）。</p>
<ul>
<li><strong>Conceptual Diagram:</strong> The small box with numbered
sites (0-5) and an arrow showing a move from state 0 to 2, and then to
3, illustrates a “random walk.” In Metropolis MC, the system transitions
from one state to another by making small, random changes.
小方框中标有编号的位点（0-5），箭头表示从状态 0 到状态 2，再到状态 3
的移动，代表“随机游走”。在 Metropolis MC
中，系统通过进行微小的随机变化从一个状态过渡到另一个状态。</li>
<li><strong>Random Number Generation:</strong> The notation
<code>rand t \in (0,1)</code> indicates the use of a random number <span
class="math inline">\(t\)</span> drawn from a uniform distribution
between 0 and 1. This is a core component of the algorithm, used to
decide whether to accept or reject a proposed new state. 符号
<code>rand t \in (0,1)</code> 表示使用从 0 到 1
之间的均匀分布中抽取的随机数 <span
class="math inline">\(t\)</span>。这是算法的核心部分，用于决定是否接受或拒绝提议的新状态。</li>
<li><strong>Detailed Balance Condition:</strong> The equation <span
class="math inline">\(P_o T(o \to n) = P_n T(n \to o)\)</span> is the
principle of detailed balance. It states that in a system at
equilibrium, the probability of being in an old state (<span
class="math inline">\(o\)</span>) and transitioning to a new state
(<span class="math inline">\(n\)</span>) is equal to the probability of
being in the new state and transitioning back to the old one. This
condition is crucial because it ensures that the simulation will
eventually sample states according to their correct thermodynamic
probabilities (the Boltzmann distribution). 方程 <span
class="math inline">\(P_o T(o \to n) = P_n T(n \to o)\)</span>
是详细平衡的原理。它指出，在平衡系统中，处于旧状态 (<span
class="math inline">\(o\)</span>) 并转变为新状态 (<span
class="math inline">\(n\)</span>)
的概率等于处于新状态并转变回旧状态的概率。此条件至关​​重要，因为它确保模拟最终将根据正确的热力学概率（玻尔兹曼分布）对状态进行采样。</li>
<li><strong>Acceptance Rate:</strong> The note <code>\sim 30\%?</code>
likely refers to the target <strong>acceptance rate</strong> for an
efficient Metropolis MC simulation. If new states are accepted too often
or too rarely, the exploration of the system’s possible configurations
is inefficient. While the famous optimal acceptance rate for certain
high-dimensional problems is around 23.4%, a range of 20-50% is often
considered effective. 注释“30%？”指的是高效 Metropolis
蒙特卡罗模拟的目标<strong>接受率</strong>。如果新状态接受过于频繁或过于稀少，系统对可能配置的探索就会变得低效。虽然某些高维问题的最佳接受率约为
23.4%，但通常认为 20-50% 的范围是有效的。</li>
</ul>
<h3 id="hamiltonian-hybrid-monte-carlo-hmc">2. Hamiltonian / Hybrid
Monte Carlo (HMC)</h3>
<p>The second topic, “Hamiltonian/Hybrid MC (HMC),” is a more advanced
Monte Carlo method that uses principles from classical mechanics to
propose new states more intelligently than the simple random-walk
approach of the standard Metropolis method. This often leads to a much
higher acceptance rate and more efficient exploration of the state
space. 第二个主题“哈密顿/混合蒙特卡罗
(HMC)”是一种更先进的蒙特卡罗方法，它利用经典力学原理，比标准 Metropolis
方法中简单的随机游走方法更智能地提出新状态。这通常会带来更高的接受率和更高效的状态空间探索。</p>
<p>The whiteboard outlines a four-step HMC algorithm:</p>
<p><strong>Step 1: Randomize Velocities</strong> The first step is to
randomize the velocities: <span class="math inline">\(\vec{v}_i \sim
\mathcal{N}(0, k_B T)\)</span>. 第一步是随机化速度：<span
class="math inline">\(\vec{v}_i \sim \mathcal{N}(0, k_B T)\)</span>。 *
This step introduces momentum into the system. For each particle <span
class="math inline">\(i\)</span>, a velocity vector <span
class="math inline">\(\vec{v}_i\)</span> is randomly drawn from a normal
(Gaussian) distribution with a mean of 0 and a variance related to the
temperature <span class="math inline">\(T\)</span> and the Boltzmann
constant <span class="math inline">\(k_B\)</span>.
此步骤将动量引入系统。对于每个粒子 <span
class="math inline">\(i\)</span>，速度矢量 <span
class="math inline">\(\vec{v}_i\)</span>
会随机地从正态（高斯）分布中抽取，该分布的均值为 0，方差与温度 <span
class="math inline">\(T\)</span> 和玻尔兹曼常数 <span
class="math inline">\(k_B\)</span> 相关。 * The full formula for this
probability distribution, <span
class="math inline">\(f(\vec{v})\)</span>, is the
<strong>Maxwell-Boltzmann distribution</strong>, which is written out
further down the board. 该概率分布的完整公式 <span
class="math inline">\(f(\vec{v})\)</span>
是<strong>麦克斯韦-玻尔兹曼分布</strong>。</p>
<p><strong>Step 2: Molecular Dynamics (MD) Integration</strong> The
board notes this as <code>t=0 \to h \text&#123; or &#125; mh</code>
<code>MD</code> and mentions the <code>Verlet</code> algorithm.</p>
<ul>
<li>This is the “Hamiltonian dynamics” part of the algorithm. Starting
from the current positions and the newly randomized velocities, the
system’s trajectory is calculated for a short period of time (<span
class="math inline">\(h\)</span> or <span
class="math inline">\(mh\)</span>) using Molecular Dynamics (MD).
这是算法的“哈密顿动力学”部分。从当前位置和新随机化的速度开始，使用分子动力学
(MD) 计算系统在短时间内（<span class="math inline">\(h\)</span> 或 <span
class="math inline">\(mh\)</span>）的轨迹。</li>
<li>The name <strong>Verlet</strong> refers to the Verlet integration
algorithm, a numerical method used to solve Newton’s equations of
motion. It is popular in MD simulations because it is time-reversible
and conserves energy well over long simulations. 指的是 Verlet
积分算法，这是一种用于求解牛顿运动方程的数值方法。它在 MD
模拟中很受欢迎，因为它具有时间可逆性，并且在长时间模拟中能量守恒效果良好。</li>
</ul>
<p><strong>Step 3: Calculate Total Energy</strong> The third step is to
<code>calculate total energy</code>: <span class="math inline">\(E_n =
K_n + V_n\)</span>. 第三步是“计算总能量”：<span
class="math inline">\(E_n = K_n + V_n\)</span>。 * After the MD
trajectory, the system is in a new state <span
class="math inline">\(n\)</span>. The total energy of this new state,
<span class="math inline">\(E_n\)</span>, is calculated as the sum of
its kinetic energy (<span class="math inline">\(K_n\)</span>, from the
velocities) and its potential energy (<span
class="math inline">\(V_n\)</span>, from the positions). MD
轨迹之后，系统处于新状态 <span
class="math inline">\(n\)</span>。新状态的总能量 <span
class="math inline">\(E_n\)</span> 等于其动能 (<span
class="math inline">\(K_n\)</span>，由速度计算得出）和势能 (<span
class="math inline">\(V_n\)</span>，由位置计算得出)之和。</p>
<p><strong>Step 4: Acceptance Test</strong> The final step is the
acceptance criterion: <span class="math inline">\(\text{acc}(o \to n) =
\min(1, e^{-\beta(E_n - E_o)})\)</span>. 最后一步是验收标准：<span
class="math inline">\(\text{acc}(o \to n) = \min(1, e^{-\beta(E_n -
E_o)})\)</span>。 * This is the Metropolis acceptance criterion. The
algorithm decides whether to accept the new state <span
class="math inline">\(n\)</span> or reject it and stay in the old state
<span class="math inline">\(o\)</span>. 这是 Metropolis
验收标准。算法决定是接受新状态 <span class="math inline">\(n\)</span>
还是拒绝它并保持旧状态 <span class="math inline">\(o\)</span>。 * The
probability of acceptance depends on the change in total energy (<span
class="math inline">\(E_n - E_o\)</span>). If the new energy is lower,
the move is always accepted. If the new energy is higher, it might still
be accepted with a probability <span class="math inline">\(e^{-\beta(E_n
- E_o)}\)</span>, where <span class="math inline">\(\beta = 1/(k_B
T)\)</span>. This allows the system to escape from local energy minima.
验收概率取决于总能量的变化 (<span class="math inline">\(E_n -
E_o\)</span>)。如果新能量较低，则始终接受该移动。如果新的能量更高，它仍然可能以概率
<span class="math inline">\(e^{-\beta(E_n - E_o)}\)</span> 被接受，其中
<span class="math inline">\(\beta = 1/(k_B
T)\)</span>。这使得系统能够摆脱局部能量最小值。</p>
<h3 id="key-formulas-and-notations">Key Formulas and Notations</h3>
<ul>
<li><p><strong>Maxwell-Boltzmann
Distribution麦克斯韦-玻尔兹曼分布:</strong> The formula for the velocity
distribution is given as: <span class="math inline">\(f(\vec{v}) =
\left(\frac{m}{2\pi k_B T}\right)^{3/2} \exp\left(-\frac{m v^2}{2 k_B
T}\right)\)</span> This gives the probability density for a particle of
mass <span class="math inline">\(m\)</span> to have a velocity <span
class="math inline">\(\vec{v}\)</span> at a given temperature <span
class="math inline">\(T\)</span>.质量为 <span
class="math inline">\(m\)</span> 的粒子速度为 的概率密度</p></li>
<li><p><strong>Energy Conservation and Acceptance Rate:</strong> The
notes <span class="math inline">\(E_n \approx E_o\)</span> and <span
class="math inline">\(75\%\)</span> highlight a key advantage of HMC.
Because the Verlet integrator approximately conserves energy, the final
energy <span class="math inline">\(E_n\)</span> after the MD trajectory
is usually very close to the initial energy <span
class="math inline">\(E_o\)</span>. This means the term <span
class="math inline">\((E_n - E_o)\)</span> is small, and the acceptance
probability is high. The <span class="math inline">\(75\%\)</span>
indicates a typical or target acceptance rate for HMC, which is
significantly higher than for standard Metropolis MC. 注释 <span
class="math inline">\(E_n \approx E_o\)</span> 和 <span
class="math inline">\(75\%\)</span> 凸显了 HMC 的一个关键优势。由于
Verlet 积分器近似地守恒能量，MD 轨迹后的最终能量 <span
class="math inline">\(E_n\)</span> 通常非常接近初始能量 <span
class="math inline">\(E_o\)</span>。这意味着 <span
class="math inline">\((E_n - E_o)\)</span> 项很小，接受概率很高。<span
class="math inline">\(75\%\)</span> 表示 HMC
的典型或目标接受率，明显高于标准 Metropolis MC。</p></li>
<li><p><strong>Hamiltonian Operator:</strong> The symbol <span
class="math inline">\(\hat{H}\)</span> written on the adjacent board
represents the Hamiltonian operator, which gives the total energy of the
system. The note <code>Δ Adiabatic</code> suggests that the MD evolution
is ideally an adiabatic process (no heat exchange), during which the
total energy (the Hamiltonian) is conserved. 相邻板上的符号 <span
class="math inline">\(\hat{H}\)</span>
代表哈密顿算符，它给出了系统的总能量。注释“Δ Adiabatic”表明 MD
演化在理想情况下是一个绝热过程（无热交换），在此过程中总能量（哈密顿量）守恒。</p></li>
</ul>
<p>This whiteboard displays the fundamental equation of quantum
chemistry: the time-dependent Schrödinger equation, along with the
detailed breakdown of the molecular Hamiltonian operator. This equation
is the starting point for almost all <em>ab initio</em>
(first-principles) quantum mechanical calculations of molecular systems.
这块白板展示了量子化学的基本方程：含时薛定谔方程，以及分子哈密顿算符的详细分解。该方程是几乎所有分子系统<em>从头算</em>（第一性原理）量子力学计算的起点。</p>
<h3 id="the-time-dependent-schrödinger-equation">3. The Time-Dependent
Schrödinger Equation</h3>
<p>At the top of the board, the fundamental equation governing the
evolution of a quantum mechanical system is presented:
白板顶部显示了控制量子力学系统演化的基本方程： <span
class="math inline">\(i\hbar \frac{\partial \Psi}{\partial t} =
\hat{\mathcal{H}} \Psi\)</span></p>
<ul>
<li><p><strong><span class="math inline">\(\Psi\)</span> (Psi)</strong>
is the <strong>wave function</strong> of the system. It contains all the
information that can be known about the system (e.g., the positions and
momenta of all particles).
是系统的<strong>波函数</strong>。它包含了关于系统的所有已知信息（例如，所有粒子的位置和动量）。</p></li>
<li><p><strong><span
class="math inline">\(\hat{\mathcal{H}}\)</span></strong> is the
<strong>Hamiltonian operator</strong>, which represents the total energy
of the system.
是<strong>哈密顿算符</strong>，表示系统的总能量。</p></li>
<li><p><strong><span class="math inline">\(i\)</span></strong>
是虚数单位。</p></li>
<li><p><strong><span class="math inline">\(i\)</span></strong> is the
imaginary unit.</p></li>
<li><p><strong><span class="math inline">\(\hbar\)</span></strong> is
the <strong>reduced Planck
constant</strong>.是<strong>约化普朗克常数</strong>。</p></li>
<li><p><strong><span class="math inline">\(\frac{\partial \Psi}{\partial
t}\)</span></strong> represents how the wave function changes over
time.表示波函数随时间的变化。</p></li>
</ul>
<p>This equation states that the time evolution of the quantum state is
dictated by the system’s total energy operator, the Hamiltonian. The
note “Δ Adiabatic process” likely connects to the context of the
Born-Oppenheimer approximation, where the electronic Schrödinger
equation is solved for fixed nuclear positions, assuming the electrons
adjust adiabatically (instantaneously) to the motion of the nuclei.
该方程表明，量子态的时间演化由系统的总能量算符——哈密顿算符决定。注释“Δ绝热过程”与玻恩-奥本海默近似相关，在该近似中，电子薛定谔方程是针对固定原子核位置求解的，假设电子以绝热方式（瞬时）调整以适应原子核的运动。</p>
<h3 id="the-full-molecular-hamiltonian-hatmathcalh">4. The Full
Molecular Hamiltonian (<span
class="math inline">\(\hat{\mathcal{H}}\)</span>)</h3>
<p>The main part of the whiteboard is the detailed expression for the
non-relativistic, time-independent molecular Hamiltonian. It is the sum
of the kinetic and potential energies of all the nuclei and electrons in
the system. The equation can be broken down into five distinct terms:
白板的主要部分是非相对论性、时间无关的分子哈密顿量的详细表达式。它是系统中所有原子核和电子的动能和势能之和。</p>
<p>该方程可以分解为五个不同的项：</p>
<p><span class="math inline">\(\hat{\mathcal{H}} = -\sum_{I=1}^{P}
\frac{\hbar^2}{2M_I}\nabla_I^2 - \sum_{i=1}^{N}
\frac{\hbar^2}{2m}\nabla_i^2 + \frac{e^2}{2}\sum_{I=1}^{P}\sum_{J \neq
I}^{P} \frac{Z_I Z_J}{|\vec{R}_I - \vec{R}_J|} +
\frac{e^2}{2}\sum_{i=1}^{N}\sum_{j \neq i}^{N} \frac{1}{|\vec{r}_i -
\vec{r}_j|} - e^2\sum_{I=1}^{P}\sum_{i=1}^{N} \frac{Z_I}{|\vec{R}_I -
\vec{r}_i|}\)</span></p>
<p>Let’s analyze each component:</p>
<p><strong>A. Kinetic Energy Terms 动能项</strong></p>
<ol type="1">
<li><strong>Kinetic Energy of the Nuclei 原子核的动能:</strong> <span
class="math inline">\(-\sum_{I=1}^{P}
\frac{\hbar^2}{2M_I}\nabla_I^2\)</span> This term is the sum of the
kinetic energy operators for all the nuclei in the
system.此项是系统中所有原子核的动能算符之和。
<ul>
<li>The sum is over all nuclei, indexed by <span
class="math inline">\(I\)</span> from 1 to <span
class="math inline">\(P\)</span>.该和涵盖所有原子核，索引为 <span
class="math inline">\(I\)</span>，从 1 到 <span
class="math inline">\(P\)</span>。</li>
<li><span class="math inline">\(M_I\)</span> is the mass of nucleus
<span class="math inline">\(I\)</span>.是原子核 <span
class="math inline">\(I\)</span> 的质量。</li>
<li><span class="math inline">\(\nabla_I^2\)</span> is the Laplacian
operator, which involves the second spatial derivatives with respect to
the coordinates of nucleus <span
class="math inline">\(I\)</span>.是拉普拉斯算符，它涉及原子核 <span
class="math inline">\(I\)</span> 坐标的二阶空间导数。</li>
</ul></li>
<li><strong>Kinetic Energy of the Electrons 电子的动能:</strong> <span
class="math inline">\(-\sum_{i=1}^{N}
\frac{\hbar^2}{2m}\nabla_i^2\)</span> This is the corresponding sum of
the kinetic energy operators for all the
electrons.这是所有电子的动能算符的对应和。
<ul>
<li>The sum is over all electrons, indexed by <span
class="math inline">\(i\)</span> from 1 to <span
class="math inline">\(N\)</span>.该和是针对所有电子的，索引为 <span
class="math inline">\(i\)</span>，从 1 到 <span
class="math inline">\(N\)</span>。</li>
<li><span class="math inline">\(m\)</span> is the mass of an
electron.是电子的质量。</li>
<li><span class="math inline">\(\nabla_i^2\)</span> is the Laplacian
operator with respect to the coordinates of electron <span
class="math inline">\(i\)</span>.是关于电子 <span
class="math inline">\(i\)</span> 坐标的拉普拉斯算符。</li>
</ul></li>
</ol>
<p><strong>B. Potential Energy Terms (Electrostatic Interactions)
势能项（静电相互作用）</strong></p>
<ol start="3" type="1">
<li><strong>Nuclear-Nuclear Repulsion 核间排斥力:</strong> <span
class="math inline">\(+\frac{e^2}{2}\sum_{I=1}^{P}\sum_{J \neq I}^{P}
\frac{Z_I Z_J}{|\vec{R}_I - \vec{R}_J|}\)</span> This term represents
the potential energy from the electrostatic (Coulomb) repulsion between
all pairs of positively charged
nuclei.该项表示所有带正电原子核对之间静电（库仑）排斥力产生的势能。
<ul>
<li>The double summation runs over all unique pairs of nuclei (<span
class="math inline">\(I, J\)</span>).对所有唯一的原子核对 (<span
class="math inline">\(I, J\)</span>) 进行双重求和。</li>
<li><span class="math inline">\(Z_I\)</span> is the atomic number (i.e.,
the charge) of nucleus <span class="math inline">\(I\)</span>.是原子核
<span class="math inline">\(I\)</span> 的原子序数（即电荷）。</li>
<li><span class="math inline">\(\vec{R}_I\)</span> is the position
vector of nucleus <span class="math inline">\(I\)</span>.是原子核 <span
class="math inline">\(I\)</span> 的位置矢量。</li>
<li><span class="math inline">\(e\)</span> is the elementary
charge.是基本电荷。</li>
</ul></li>
<li><strong>Electron-Electron Repulsion 电子间排斥力:</strong> <span
class="math inline">\(+\frac{e^2}{2}\sum_{i=1}^{N}\sum_{j \neq i}^{N}
\frac{1}{|\vec{r}_i - \vec{r}_j|}\)</span> This term represents the
potential energy from the electrostatic repulsion between all pairs of
negatively charged
electrons.该项表示所有带负电的电子对之间静电排斥的势能。
<ul>
<li>The double summation runs over all unique pairs of electrons (<span
class="math inline">\(i, j\)</span>).对所有不同的电子对 (<span
class="math inline">\(i, j\)</span>) 进行双重求和。</li>
<li><span class="math inline">\(\vec{r}_i\)</span> is the position
vector of electron <span class="math inline">\(i\)</span>.是电子 <span
class="math inline">\(i\)</span> 的位置矢量。</li>
</ul></li>
<li><strong>Nuclear-Electron Attraction 核-电子引力:</strong> <span
class="math inline">\(-e^2\sum_{I=1}^{P}\sum_{i=1}^{N}
\frac{Z_I}{|\vec{R}_I - \vec{r}_i|}\)</span> This final term represents
the potential energy from the electrostatic attraction between the
nuclei and the electrons.这最后一项表示原子核和电子之间静电引力的势能。
<ul>
<li>The summation runs over all nuclei and all
electrons.该求和适用于所有原子核和所有电子。</li>
</ul></li>
</ol>
<h3 id="notations-and-conventions">5. Notations and Conventions</h3>
<ul>
<li><strong>Atomic Units:</strong> The note <span
class="math inline">\(\frac{1}{4\pi\epsilon_0} = k = 1\)</span> is a key
indicator of the convention being used. This sets the Coulomb constant
to 1, which is a hallmark of <strong>Hartree atomic units</strong>. In
this system, the elementary charge (<span
class="math inline">\(e\)</span>), electron mass (<span
class="math inline">\(m\)</span>), and reduced Planck constant (<span
class="math inline">\(\hbar\)</span>) are also set to 1. This simplifies
the Hamiltonian significantly, removing the physical constants and
making the equations easier to work with computationally.
是所用约定的关键指标。这将库仑常数设置为 1，这是<strong>Hartree
原子单位</strong>的标志。在这个系统中，基本电荷 (<span
class="math inline">\(e\)</span>)、电子质量 (<span
class="math inline">\(m\)</span>) 和​​约化普朗克常数 (<span
class="math inline">\(\hbar\)</span>) 也设为
1。这显著简化了哈密顿量，消除了物理常数，使方程更易于计算。</li>
<li><strong>Interaction Terms:</strong> The notations <span
class="math inline">\(\{i, j\}\)</span>, <span
class="math inline">\(\{i, j, k\}\)</span>, etc., refer to the
“many-body” problem. The Hamiltonian contains two-body terms
(interactions between pairs of particles), and solving the Schrödinger
equation exactly is extremely difficult because the motion of every
particle is correlated with every other particle. Computational methods
are designed to approximate these interactions. 符号 <span
class="math inline">\(\{i, j\}\)</span>、<span
class="math inline">\(\{i, j, k\}\)</span>
等指的是“多体”问题。哈密顿量包含二体项（粒子对之间的相互作用），而精确求解薛定谔方程极其困难，因为每个粒子的运动都与其他粒子相关。计算方法旨在近似这些相互作用。</li>
</ul>
<p>This whiteboard presents the mathematical foundation for
<strong>non-adiabatic molecular dynamics</strong>, a sophisticated
method in theoretical chemistry and physics used to simulate processes
where the Born-Oppenheimer approximation breaks down. This typically
occurs in photochemistry, electron transfer reactions, and when
molecules interact with intense laser fields.
这块白板展示了<strong>非绝热分子动力学</strong>的数学基础，这是理论化学和物理学中一种复杂的方法，用于模拟玻恩-奥本海默近似失效的过程。这通常发生在光化学、电子转移反应以及分子与强激光场相互作用时。</p>
<h3
id="topic-non-adiabatic-molecular-dynamics-md-非绝热分子动力学-md">6.
Topic: Non-Adiabatic Molecular Dynamics (MD) 非绝热分子动力学 (MD)</h3>
<p>The title “Δ non-adiabatic MD” indicates that the topic moves beyond
the standard Born-Oppenheimer approximation. In this approximation, it
is assumed that the light electrons adjust instantaneously to the motion
of the heavy nuclei, allowing the system to be described by a single
potential energy surface. Non-adiabatic methods, by contrast, account
for the quantum mechanical coupling between multiple electronic
states.</p>
<p>标题“Δ 非绝热
MD”表明该主题超越了标准的玻恩-奥本海默近似。在该近似中，假设轻电子会根据重原子核的运动进行瞬时调整，从而使系统可以用单个势能面来描述。相比之下，非绝热方法则考虑了多个电子态之间的量子力学耦合。</p>
<h3 id="the-born-huang-ansatz-玻恩-黄拟设">7. The Born-Huang Ansatz
玻恩-黄拟设</h3>
<p>The starting point for this method is the “ansatz” (an educated guess
for the form of the solution). This is the Born-Huang expansion for the
total molecular wave function, <span
class="math inline">\(\Psi\)</span>.
该方法的起点是“拟设”（对解形式的合理猜测）。这是分子总波函数 <span
class="math inline">\(\Psi\)</span> 的玻恩-黄展开式。</p>
<p><span class="math inline">\(\Psi(\vec{R}, \vec{r}, t) = \sum_{n}
\Theta_n(\vec{R}, t) \Phi_n(\vec{R}, \vec{r})\)</span></p>
<ul>
<li><p><strong><span class="math inline">\(\Psi(\vec{R}, \vec{r},
t)\)</span></strong> is the total wave function for the entire molecule.
It depends on the coordinates of all nuclei (<span
class="math inline">\(\vec{R}\)</span>), all electrons (<span
class="math inline">\(\vec{r}\)</span>), and time (<span
class="math inline">\(t\)</span>).
是整个分子的总波函数。它取决于所有原子核 (<span
class="math inline">\(\vec{R}\)</span>)、所有电子 (<span
class="math inline">\(\vec{r}\)</span>) 和时间 (<span
class="math inline">\(t\)</span>) 的坐标。</p></li>
<li><p><strong><span class="math inline">\(\Phi_n(\vec{R},
\vec{r})\)</span></strong> are the <strong>electronic wave
functions</strong>. They are the solutions to the electronic Schrödinger
equation for a fixed nuclear geometry <span
class="math inline">\(\vec{R}\)</span> and form a complete basis set.
The index <span class="math inline">\(n\)</span> labels the electronic
state (e.g., ground state, first excited state, etc.).
它们是给定原子核几何构型 <span class="math inline">\(\vec{R}\)</span>
的电子薛定谔方程的解，并构成一个完整的基组。下标 <span
class="math inline">\(n\)</span>
标记电子态（例如，基态、第一激发态等）。</p></li>
<li><p><strong><span class="math inline">\(\Theta_n(\vec{R},
t)\)</span></strong> are the <strong>nuclear wave functions</strong>.
Each <span class="math inline">\(\Theta_n\)</span> describes the motion
of the nuclei on the potential energy surface of the corresponding
electronic state, <span class="math inline">\(\Phi_n\)</span>.
Crucially, they depend on time. 是<strong>核波函数</strong>。每个 <span
class="math inline">\(\Theta_n\)</span> 描述原子核在相应电子态 <span
class="math inline">\(\Phi_n\)</span>
势能面上的运动。至关重要的是，它们依赖于时间。</p></li>
</ul>
<p>This ansatz expresses the total molecular state as a superposition of
electronic states, where the coefficients of the superposition are the
nuclear wave functions.
该拟设将总分子态表示为电子态的叠加，其中叠加的系数是核波函数。</p>
<h3 id="the-partitioned-molecular-hamiltonian-分割分子哈密顿量">8. The
Partitioned Molecular Hamiltonian 分割分子哈密顿量</h3>
<p>The total molecular Hamiltonian, <span
class="math inline">\(\hat{\mathcal{H}}\)</span>, is partitioned into
terms that act on the nuclei and electrons separately. 总分子哈密顿量
<span class="math inline">\(\hat{\mathcal{H}}\)</span>
被分割成分别作用于原子核和电子的项。</p>
<p><span class="math inline">\(\hat{\mathcal{H}} = -\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2 + \hat{\mathcal{H}}_e +
\hat{V}_{nn}\)</span></p>
<ul>
<li><p><strong><span class="math inline">\(-\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2\)</span></strong>: This is the kinetic
energy operator for the nuclei, often denoted as <span
class="math inline">\(\hat{T}_n\)</span>.这是原子核的动能算符，通常表示为
<span class="math inline">\(\hat{T}_n\)</span>。</p></li>
<li><p><strong><span
class="math inline">\(\hat{\mathcal{H}}_e\)</span></strong>: This is the
<strong>electronic Hamiltonian</strong>, which includes the kinetic
energy of the electrons and the potential energy of electron-electron
and electron-nuclear interactions.
这是<strong>电子哈密顿量</strong>，包含电子的动能以及电子-电子和电子-核相互作用的势能。</p></li>
<li><p><strong><span
class="math inline">\(\hat{V}_{nn}\)</span></strong>: This is the
potential energy operator for <strong>nuclear-nuclear
repulsion</strong>.这是<strong>核-核排斥</strong>的势能算符。</p></li>
</ul>
<h3 id="the-electronic-schrödinger-equation-电子薛定谔方程">9. The
Electronic Schrödinger Equation 电子薛定谔方程</h3>
<p>The electronic basis functions, <span
class="math inline">\(\Phi_n\)</span>, are defined as the eigenfunctions
of the electronic Hamiltonian (plus the nuclear repulsion term) for a
fixed nuclear configuration <span
class="math inline">\(\vec{R}\)</span>. 电子基函数 <span
class="math inline">\(\Phi_n\)</span> 定义为对于固定的核构型 <span
class="math inline">\(\vec{R}\)</span>，电子哈密顿量（加上核排斥项）的本征函数。</p>
<p><span class="math inline">\((\hat{\mathcal{H}}_e + \hat{V}_{nn})
\Phi_n(\vec{R}, \vec{r}) = E_n(\vec{R}) \Phi_n(\vec{R},
\vec{r})\)</span></p>
<ul>
<li><strong><span class="math inline">\(E_n(\vec{R})\)</span></strong>
are the eigenvalues, which are the <strong>potential energy surfaces
(PES)</strong>. Each electronic state <span
class="math inline">\(n\)</span> has its own PES, which dictates the
forces acting on the nuclei when the molecule is in that electronic
state. 是特征值，即<strong>势能面 (PES)</strong>。每个电子态 <span
class="math inline">\(n\)</span>
都有其自身的势能面，它决定了分子处于该电子态时作用于原子核的力。</li>
</ul>
<h3
id="deriving-the-equations-of-motion-for-the-nuclei-推导原子核运动方程">10.
Deriving the Equations of Motion for the Nuclei 推导原子核运动方程</h3>
<p>The final part of the whiteboard begins the derivation of the
time-dependent Schrödinger equation for the nuclear wave functions,
<span class="math inline">\(\Theta_k\)</span>. The process starts with
the full time-dependent Schrödinger equation, <span
class="math inline">\(i\hbar \frac{\partial \Psi}{\partial t} =
\hat{\mathcal{H}} \Psi\)</span>. To find the equation for a specific
nuclear wave function <span class="math inline">\(\Theta_k\)</span>,
this main equation is projected onto the corresponding electronic basis
state <span class="math inline">\(\Phi_k\)</span>.
白板的最后一部分开始推导原子核波函数 <span
class="math inline">\(\Theta_k\)</span>
的含时薛定谔方程。该过程从完整的含时薛定谔方程 <span
class="math inline">\(i\hbar \frac{\partial \Psi}{\partial t} =
\hat{\mathcal{H}} \Psi\)</span> 开始。为了找到特定原子核波函数 <span
class="math inline">\(\Theta_k\)</span>
的方程，需要将这个主方程投影到相应的电子基态 <span
class="math inline">\(\Phi_k\)</span> 上。</p>
<p>This is done by multiplying from the left by the complex conjugate of
the electronic wave function, <span
class="math inline">\(\Phi_k^*\)</span>, and integrating over all
electronic coordinates, <span class="math inline">\(d\vec{r}\)</span>.
可以通过从左边乘以电子波函数 <span
class="math inline">\(\Phi_k^*\)</span> 的复共轭，然后在所有电子坐标
<span class="math inline">\(d\vec{r}\)</span> 上积分来实现。</p>
<p><span class="math inline">\(\int \Phi_k^* i\hbar
\frac{\partial}{\partial t} \Psi \,d\vec{r} = \int \Phi_k^*
\hat{\mathcal{H}} \Psi \,d\vec{r}\)</span></p>
<p>The board then shows the result of substituting the Born-Huang ansatz
for <span class="math inline">\(\Psi\)</span> and the partitioned
Hamiltonian for <span class="math inline">\(\hat{\mathcal{H}}\)</span>
into this projected equation: 然后，黑板显示将 Born-Huang 拟设式代入
<span
class="math inline">\(\Psi\)</span>，将分块哈密顿量代入以下投影方程的结果：</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k(\vec{R}, t) = \int \Phi_k^* \left( -\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2 + \hat{\mathcal{H}}_e + \hat{V}_{nn}
\right) \sum_n \Theta_n \Phi_n \,d\vec{r}\)</span></p>
<ul>
<li><p><strong>Left Hand Side</strong>: The left side of the projection
has been simplified. Because the electronic basis functions <span
class="math inline">\(\Phi_n\)</span> form an orthonormal set (<span
class="math inline">\(\int \Phi_k^* \Phi_n d\vec{r} =
\delta_{kn}\)</span>), the sum collapses to a single term for <span
class="math inline">\(n=k\)</span>. 投影左侧已简化。由于电子基函数 <span
class="math inline">\(\Phi_n\)</span> 构成一个正交集 (<span
class="math inline">\(\int \Phi_k^* \Phi_n d\vec{r} =
\delta_{kn}\)</span>，因此当 <span class="math inline">\(n=k\)</span>
时，和将折叠为一个项。</p></li>
<li><p><strong>Right Hand Side</strong>: This complex integral is the
core of non-adiabatic dynamics. When the nuclear kinetic energy
operator, <span class="math inline">\(\nabla_I^2\)</span>, acts on the
product <span class="math inline">\(\Theta_n \Phi_n\)</span>, it acts on
both functions (via the product rule). The terms that arise from <span
class="math inline">\(\nabla_I\)</span> acting on the electronic wave
functions <span class="math inline">\(\Phi_n\)</span> are known as
<strong>non-adiabatic coupling terms</strong>. These terms are
responsible for enabling transitions between different electronic
potential energy surfaces, which is the essence of non-adiabatic
dynamics. 这个复积分是非绝热动力学的核心。当核动能算符 <span
class="math inline">\(\nabla_I^2\)</span> 作用于乘积 <span
class="math inline">\(\Theta_n \Phi_n\)</span>
时，它会作用于这两个函数（通过乘积规则）。由 <span
class="math inline">\(\nabla_I\)</span> 作用于电子波函数 <span
class="math inline">\(\Phi_n\)</span>
而产生的项称为<strong>非绝热耦合项</strong>。这些术语负责实现不同电子势能面之间的转变，这是非绝热动力学的本质。</p></li>
</ul>
<p>This whiteboard continues the mathematical derivation for
non-adiabatic molecular dynamics started in the previous image. It
focuses on expanding the nuclear kinetic energy term to reveal the
crucial couplings between different electronic
states.这块白板延续了上一张图片中非绝热分子动力学的数学推导。它着重于扩展核动能项，以揭示不同电子态之间的关键耦合。</p>
<h3
id="starting-point-the-projected-schrödinger-equation-起点投影薛定谔方程">11.
Starting Point: The Projected Schrödinger Equation
起点：投影薛定谔方程</h3>
<p>The derivation picks up from the equation for the time evolution of
the nuclear wave function, <span
class="math inline">\(\Theta_k\)</span>. The right-hand side of this
equation is being evaluated. 推导过程取自核波函数 <span
class="math inline">\(\Theta_k\)</span>
的时间演化方程。该方程的右边正在求值。</p>
<p><span class="math inline">\(= \int \Phi_k^* \left( -\sum_{I}
\frac{\hbar^2}{2M_I}\nabla_I^2 \right) \sum_n \Theta_n \Phi_n \,d\vec{r}
+ E_k \Theta_k\)</span></p>
<p>This equation separates the total energy into two parts
该方程将总能量分为两部分 : * The first term is the contribution from the
<strong>nuclear kinetic energy operator</strong>, <span
class="math inline">\(-\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2\)</span>.
第一项是<strong>核动能算符</strong>的贡献 * The second term, <span
class="math inline">\(E_k \Theta_k\)</span>, is the contribution from
the <strong>potential energy</strong>. This term arises from the action
of the electronic Hamiltonian part <span
class="math inline">\((\hat{\mathcal{H}}_e + \hat{V}_{nn})\)</span> on
the basis functions. Due to the orthonormality of the electronic
wavefunctions (<span class="math inline">\(\int \Phi_k^* \Phi_n
\,d\vec{r} = \delta_{kn}\)</span>), the sum over <span
class="math inline">\(n\)</span> collapses to a single term for the
potential energy. 第二项，<span class="math inline">\(E_k
\Theta_k\)</span>，是<strong>势能</strong>的贡献。这一项源于电子哈密顿量部分
<span class="math inline">\((\hat{\mathcal{H}}_e +
\hat{V}_{nn})\)</span> 对基函数的作用。由于电子波函数（<span
class="math inline">\(\int \Phi_k^* \Phi_n \,d\vec{r} =
\delta_{kn}\)</span>）的正交性，<span
class="math inline">\(n\)</span>项的和会坍缩为势能的一项。</p>
<p>The challenge, and the core of the physics, lies in evaluating the
first term, as the nuclear derivative <span
class="math inline">\(\nabla_I\)</span> acts on <em>both</em> the
nuclear wave function <span class="math inline">\(\Theta_n\)</span> and
the electronic wave function <span
class="math inline">\(\Phi_n\)</span>.
难点在于，也是物理的核心在于如何计算第一项，因为核导数 <span
class="math inline">\(\nabla_I\)</span> 同时作用于核波函数 <span
class="math inline">\(\Theta_n\)</span> 和电子波函数 <span
class="math inline">\(\Phi_n\)</span>。</p>
<h3
id="applying-the-product-rule-for-the-laplacian-应用拉普拉斯算子的乘积规则">12.
Applying the Product Rule for the Laplacian
应用拉普拉斯算子的乘积规则</h3>
<p>To expand the kinetic energy term, the product rule for the Laplacian
operator acting on two functions (A and B) is used. The board writes
this rule as: 为了展开动能项，我们利用了拉普拉斯算子作用于两个函数（A 和
B）的乘积规则。棋盘上将这条规则写成： <span
class="math inline">\(\nabla^2(AB) = (\nabla^2 A)B + 2(\nabla
A)\cdot(\nabla B) + A(\nabla^2 B)\)</span></p>
<p>In our case, <span class="math inline">\(A = \Theta_n(\vec{R},
t)\)</span> and <span class="math inline">\(B = \Phi_n(\vec{R},
\vec{r})\)</span>. The derivative <span
class="math inline">\(\nabla_I\)</span> is with respect to the nuclear
coordinates <span class="math inline">\(\vec{R}_I\)</span>.
在我们的例子中，<span class="math inline">\(A = \Theta_n(\vec{R},
t)\)</span>，<span class="math inline">\(B = \Phi_n(\vec{R},
\vec{r})\)</span>。导数 <span class="math inline">\(\nabla_I\)</span>
是关于原子核坐标 <span class="math inline">\(\vec{R}_I\)</span> 的。</p>
<h3 id="expanding-the-kinetic-energy-term-展开动能项">13. Expanding the
Kinetic Energy Term 展开动能项</h3>
<p>Applying this rule, the integral containing the kinetic energy
operator is expanded: 应用此规则，展开包含动能算符的积分： <span
class="math inline">\(= -\sum_I \frac{\hbar^2}{2M_I} \int \Phi_k^*
\sum_n \left( (\nabla_I^2 \Theta_n)\Phi_n + 2(\nabla_I
\Theta_n)\cdot(\nabla_I \Phi_n) + \Theta_n(\nabla_I^2 \Phi_n) \right)
d\vec{r} + E_k \Theta_k\)</span></p>
<p>This step explicitly shows how the nuclear kinetic energy operator
gives rise to three distinct types of
terms.此步骤明确展示了核动能算符如何产生三种不同类型的项。</p>
<h3
id="final-result-and-identification-of-coupling-terms-最终结果及耦合项的识别">14.
Final Result and Identification of Coupling Terms
最终结果及耦合项的识别</h3>
<p>The final step is to take the integral over the electronic
coordinates (<span class="math inline">\(d\vec{r}\)</span>) and
rearrange the terms. The expression is simplified by again using the
orthonormality of the electronic wave functions, <span
class="math inline">\(\int \Phi_k^* \Phi_n \, d\vec{r} =
\delta_{kn}\)</span>. 最后一步是对电子坐标 (<span
class="math inline">\(d\vec{r}\)</span>)
进行积分，并重新排列各项。再次利用电子波函数的正交性简化表达式，<span
class="math inline">\(\int \Phi_k^* \Phi_n \, d\vec{r} =
\delta_{kn}\)</span>。</p>
<p><span class="math inline">\(= -\sum_I \frac{\hbar^2}{2M_I} \left(
\nabla_I^2 \Theta_k + \sum_n 2 \left( \int \Phi_k^* \nabla_I \Phi_n \,
d\vec{r} \right) \cdot \nabla_I \Theta_n + \sum_n \left( \int \Phi_k^*
\nabla_I^2 \Phi_n \, d\vec{r} \right) \Theta_n \right) + E_k
\Theta_k\)</span></p>
<p>This final equation is profound. It represents the time-independent
Schrödinger equation for the nuclear wave function <span
class="math inline">\(\Theta_k\)</span>, but it is coupled to all other
nuclear wave functions <span class="math inline">\(\Theta_n\)</span>.
Let’s break down the key terms within the parentheses:
最后一个方程意义深远。它代表了核波函数 <span
class="math inline">\(\Theta_k\)</span>
的与时间无关的薛定谔方程，但它与所有其他核波函数 <span
class="math inline">\(\Theta_n\)</span>
耦合。让我们分解一下括号内的关键项：</p>
<ul>
<li><p><strong><span class="math inline">\(\nabla_I^2
\Theta_k\)</span></strong>: This is the standard kinetic energy term for
the nuclei moving on the potential energy surface of state <span
class="math inline">\(k\)</span>. This is the only term that would
remain in the simple Born-Oppenheimer (adiabatic) approximation.
这是原子核在势能面 <span class="math inline">\(k\)</span>
上运动的标准动能项。这是在简单的
Born-Oppenheimer（绝热）近似中唯一保留的项。</p></li>
<li><p><strong><span class="math inline">\(\left( \int \Phi_k^* \nabla_I
\Phi_n \, d\vec{r} \right)\)</span></strong>: This is the
<strong>first-derivative non-adiabatic coupling term (NACT)</strong>,
often called the derivative coupling. This vector quantity determines
the strength of the coupling between electronic states <span
class="math inline">\(k\)</span> and <span
class="math inline">\(n\)</span> due to the velocity of the nuclei. It
is the primary term responsible for enabling transitions between
different potential energy surfaces. 这是<strong>一阶导数非绝热耦合项
(NACT)</strong>，通常称为导数耦合。该矢量决定了由于原子核速度而导致的电子态
<span class="math inline">\(k\)</span> 和 <span
class="math inline">\(n\)</span>
之间耦合的强度。它是实现不同势能面之间跃迁的主要项。</p></li>
<li><p><strong><span class="math inline">\(\left( \int \Phi_k^*
\nabla_I^2 \Phi_n \, d\vec{r} \right)\)</span></strong>: This is the
<strong>second-derivative non-adiabatic coupling term</strong>, a scalar
quantity. While often smaller than the first-derivative term, it is also
part of the complete description of non-adiabatic effects.
是<strong>二阶导数非绝热耦合项</strong>，一个标量。虽然它通常小于一阶导数项，但它也是非绝热效应完整描述的一部分。</p></li>
</ul>
<p>In summary, this derivation shows mathematically how the motion of
the nuclei (via the <span class="math inline">\(\nabla_I\)</span>
operator) can induce quantum mechanical transitions between different
electronic states (<span class="math inline">\(\Phi_k \leftrightarrow
\Phi_n\)</span>). The strength of these transitions is governed by the
non-adiabatic coupling terms, which depend on how the electronic wave
functions change as the nuclear geometry changes.
总之，该推导从数学上展示了原子核的运动（通过 <span
class="math inline">\(\nabla_I\)</span>
算符）如何诱导不同电子态之间的量子力学跃迁（<span
class="math inline">\(\Phi_k \leftrightarrow
\Phi_n\)</span>）。这些跃迁的强度由非绝热耦合项控制，而非绝热耦合项又取决于电子波函数如何随原子核几何结构的变化而变化。</p>
<p>This whiteboard concludes the derivation of the equations for
non-adiabatic molecular dynamics by defining the coupling operator and
then showing how different levels of approximation—specifically the
Born-Huang and the more restrictive Born-Oppenheimer
approximations—arise from neglecting certain coupling terms.
这块白板通过定义耦合算符，并展示不同程度的近似——特别是 Born-Huang
近似和更严格的 Born-Oppenheimer
近似——是如何通过忽略某些耦合项而产生的，从而推导出非绝热分子动力学方程的。</p>
<h3
id="definition-of-the-non-adiabatic-coupling-operator-非绝热耦合算符的定义">15.
Definition of the Non-Adiabatic Coupling Operator
非绝热耦合算符的定义</h3>
<p>The whiteboard begins by collecting all the non-adiabatic coupling
terms derived previously into a single operator, <span
class="math inline">\(C_{kn}\)</span>.
白板首先将之前推导的所有非绝热耦合项合并为一个算符 <span
class="math inline">\(C_{kn}\)</span>。</p>
<p>Let <span class="math inline">\(C_{kn} = -\sum_{I}
\frac{\hbar^2}{2M_I} \left( 2 \left( \int \Phi_k^* \nabla_I \Phi_n \,
d\vec{r} \right) \cdot \nabla_I + \left( \int \Phi_k^* \nabla_I^2 \Phi_n
\, d\vec{r} \right) \right)\)</span></p>
<ul>
<li>This operator, <span class="math inline">\(C_{kn}\)</span>,
represents the total effect of the coupling between electronic state
<span class="math inline">\(k\)</span> and electronic state <span
class="math inline">\(n\)</span>, which is induced by the kinetic energy
of the nuclei. 此算符 <span class="math inline">\(C_{kn}\)</span>
表示由原子核动能引起的电子态 <span class="math inline">\(k\)</span>
和电子态 <span class="math inline">\(n\)</span> 之间耦合的总效应。</li>
<li>The operator acts on the nuclear wave function that follows it in
the full equation. The <span class="math inline">\(\nabla_I\)</span>
term acts as a derivative on that wave function.
该算符作用于完整方程中跟随它的核波函数。<span
class="math inline">\(\nabla_I\)</span> 项充当该波函数的导数。</li>
</ul>
<h3 id="the-coupled-equations-of-motion-耦合运动方程">16. The Coupled
Equations of Motion 耦合运动方程</h3>
<p>Using this compact definition, the full set of coupled time-dependent
Schrödinger equations for the nuclear wave functions can be written as:
基于此简洁定义，核波函数的完整耦合含时薛定谔方程组可以写成：</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k \right)
\Theta_k + \sum_n C_{kn} \Theta_n\)</span></p>
<p>This is the central result. It shows that the time evolution of the
nuclear wave function on a given potential energy surface <span
class="math inline">\(k\)</span> (described by <span
class="math inline">\(\Theta_k\)</span>) depends on two things:
这是核心结论。它表明，核波函数在给定势能面 <span
class="math inline">\(k\)</span>（用 <span
class="math inline">\(\Theta_k\)</span>
描述）上的时间演化取决于两个因素： 1. The motion on its own surface,
governed by its kinetic energy and the potential <span
class="math inline">\(E_k\)</span>. 其自身表面上的运动，由其动能和势能
<span class="math inline">\(E_k\)</span> 控制。 2. The influence of the
nuclear wave functions on <em>all other</em> electronic surfaces (<span
class="math inline">\(\Theta_n\)</span>), mediated by the coupling
operators <span class="math inline">\(C_{kn}\)</span>.
核波函数对<em>所有其他</em>电子表面（<span
class="math inline">\(\Theta_n\)</span>）的影响，由耦合算符 <span
class="math inline">\(C_{kn}\)</span> 介导。</p>
<h3 id="the-born-huang-approximation-玻恩-黄近似">17. The Born-Huang
Approximation 玻恩-黄近似</h3>
<p>The first and most crucial approximation is introduced to simplify
this complex set of coupled equations.
为了简化这组复杂的耦合方程，引入了第一个也是最重要的近似。</p>
<p><strong>If <span class="math inline">\(C_{kn} = 0\)</span> for <span
class="math inline">\(k \neq n\)</span> (Born-Huang
approximation)</strong></p>
<p>This approximation assumes that the <strong>off-diagonal</strong>
coupling terms, which are responsible for transitions between different
electronic states, are negligible. However, it retains the
<strong>diagonal</strong> coupling term (<span
class="math inline">\(C_{kk}\)</span>). This leads to a simplified,
uncoupled equation:
该近似假设导致不同电子态之间跃迁的<strong>非对角</strong>耦合项可以忽略不计。然而，它保留了<strong>对角</strong>耦合项（<span
class="math inline">\(C_{kk}\)</span>）。这可以得到一个简化的非耦合方程：</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k +
C_{kk} \right) \Theta_k\)</span></p>
<p>Substituting the definition of <span
class="math inline">\(C_{kk}\)</span>: 代入 <span
class="math inline">\(C_{kk}\)</span> 的定义：</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k -
\sum_I \frac{\hbar^2}{2M_I} \left( 2 \left( \int \Phi_k^* \nabla_I
\Phi_k \, d\vec{r} \right) \cdot \nabla_I + \int \Phi_k^* \nabla_I^2
\Phi_k \, d\vec{r} \right) \right) \Theta_k\)</span></p>
<p>The term <span class="math inline">\(C_{kk}\)</span> is known as the
<strong>diagonal Born-Oppenheimer correction (DBOC)</strong>. It
represents a small correction to the potential energy surface <span
class="math inline">\(E_k\)</span> that arises from the fact that the
electrons do not adjust perfectly and instantaneously to the nuclear
motion, even within the same electronic state. <span
class="math inline">\(C_{kk}\)</span>
项被称为<strong>对角玻恩-奥本海默修正 (DBOC)</strong>。它表示对势能面
<span class="math inline">\(E_k\)</span>
的微小修正，其原因是即使在相同的电子态下，电子也无法完美且即时地适应核运动。</p>
<ul>
<li><strong>Note on Real Wavefunctions 关于实波函数的注释</strong>: The
board shows that for real wavefunctions, the first-derivative part of
the diagonal correction vanishes: <span class="math inline">\(\int
\Phi_k \nabla_I \Phi_k \, d\vec{r} = 0\)</span>. This is because the
integral is related to the gradient of the normalization condition,
<span class="math inline">\(\nabla_I \int \Phi_k^2 \, d\vec{r} =
\nabla_I(1) = 0\)</span>, which expands to <span
class="math inline">\(2\int \Phi_k \nabla_I \Phi_k \, d\vec{r} =
0\)</span>. 黑板显示，对于实波函数，对角修正的一阶导数部分为零：<span
class="math inline">\(\int \Phi_k \nabla_I \Phi_k \, d\vec{r} =
0\)</span>。这是因为积分与归一化条件的梯度有关，<span
class="math inline">\(\nabla_I \int \Phi_k^2 \, d\vec{r} = \nabla_I(1) =
0\)</span>，其展开为 <span class="math inline">\(2\int \Phi_k \nabla_I
\Phi_k \, d\vec{r} = 0\)</span>。</li>
</ul>
<h3 id="the-born-oppenheimer-approximation-玻恩-奥本海默近似">18. The
Born-Oppenheimer Approximation 玻恩-奥本海默近似</h3>
<p>The final and most widely used approximation is the Born-Oppenheimer
approximation. It is more restrictive than the Born-Huang approximation.
最后一种也是最广泛使用的近似方法是玻恩-奥本海默近似。它比玻恩-黄近似更具限制性。</p>
<p><strong>If <span class="math inline">\(C_{kk} = 0\)</span>
(Born-Oppenheimer approximation) 若<span class="math inline">\(C_{kk} =
0\)</span>（玻恩-奥本海默近似）</strong></p>
<p>This assumes that the diagonal correction term is also negligible. By
setting all <span class="math inline">\(C_{kn}=0\)</span> (both diagonal
and off-diagonal), the equations become completely decoupled, and the
nuclear motion evolves independently on each potential energy surface.
这假设对角修正项也可忽略不计。通过令所有<span
class="math inline">\(C_{kn}=0\)</span>（包括对角和非对角），方程组完全解耦，原子核运动在每个势能面上独立演化。</p>
<p>The result is the standard <strong>time-dependent Schrödinger
equation for the nuclei</strong>:
由此可得标准的<strong>原子核的含时薛定谔方程</strong>：</p>
<p><span class="math inline">\(i\hbar \frac{\partial}{\partial t}
\Theta_k = \left( -\sum_{I} \frac{\hbar^2}{2M_I}\nabla_I^2 + E_k \right)
\Theta_k\)</span></p>
<p>This equation is the foundation of most of quantum chemistry. It
states that the nuclei move on a static potential energy surface <span
class="math inline">\(E_k(\vec{R})\)</span> provided by the electrons,
without any possibility of transitioning to other electronic states or
having the surface be corrected by their own motion.</p>
<p>该方程是大多数量子化学的基础。原子核在由电子提供的静态势能面 <span
class="math inline">\(E_k(\vec{R})\)</span>
上运动，不存在跃迁到其他电子态或因自身运动而修正势能面的可能性。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/18/img_assert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/18/img_assert/" class="post-title-link" itemprop="url">BLOGS - IMG Assert</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-09-18 10:00:00" itemprop="dateCreated datePublished" datetime="2025-09-18T10:00:00+08:00">2025-09-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-19 19:24:51" itemprop="dateModified" datetime="2025-09-19T19:24:51+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="问题主要为了图像不显示问题">【问题】主要为了图像不显示问题</h2>
<h3 id="step1根目录中的配置文件">Step1:根目录中的配置文件</h3>
<h3 id="step2将-markdown-行替换为html-代码">Step2:将 Markdown
行替换为HTML 代码</h3>
<h3 id="step3设置下方添加root">Step3:设置下方添加ROOT</h3>
<h3
id="step4不需要此插件终端中运行以下命令来卸载插件">Step4:不需要此插件终端中运行以下命令来卸载插件：</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="comment"># URL</span></span><br><span class="line"><span class="comment">## Set your site url here. For example, if you use GitHub Page, set url as &#x27;https://username.github.io/project&#x27;</span></span><br><span class="line">$ url: https://TianyaoBlogs.github.io/</span><br><span class="line"></span><br><span class="line">$ root: /</span><br><span class="line"></span><br><span class="line">$ permalink: :year/:month/:day/:title/</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ &lt;img src=<span class="string">&quot;/imgs/5054C3/General_linear_regression_model.png&quot;</span> alt=<span class="string">&quot;A diagram of the general linear regression model&quot;</span>&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm uninstall hexo-asset-image</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/17/5120C3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/17/5120C3/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W3-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-09-17 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-17T21:00:00+08:00">2025-09-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-19 20:28:09" itemprop="dateModified" datetime="2025-09-19T20:28:09+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - 计算能源材料和电子结构模拟 Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="radial-distribution-function">1 radial distribution
function:</h2>
<ul>
<li><strong>内容</strong>:</li>
</ul>
<p>This whiteboard explains the process of calculating the
<strong>radial distribution function</strong>, often denoted as <span
class="math inline">\(g(r)\)</span>, to analyze the atomic structure of
a material, which is referred to here as a “film”.
本白板解释了计算<strong>径向分布函数</strong>（通常表示为 <span
class="math inline">\(g(r)\)</span>）的过程，用于分析材料（本文中称为“薄膜”）的原子结构。</p>
<p>In simple terms, the radial distribution function tells you the
probability of finding an atom at a certain distance from another
reference atom. It’s a powerful way to see the local structure in a
disordered system like a liquid or an amorphous solid.</p>
<p>简单来说，径向分布函数表示在距离另一个参考原子一定距离处找到一个原子的概率。它是观察无序系统（例如液体或非晶态固体）局部结构的有效方法。</p>
<h3 id="core-concept-radial-distribution-function-径向分布函数">## Core
Concept: Radial Distribution Function 径向分布函数</h3>
<p>The main goal is to compute the radial distribution function, <span
class="math inline">\(g(r)\)</span>, which is defined as the ratio of
the actual number of atoms found in a thin shell at a distance <span
class="math inline">\(r\)</span> to the number of atoms you’d expect to
find if the material were an ideal gas (completely random).
主要目标是计算径向分布函数 <span
class="math inline">\(g(r)\)</span>，其定义为在距离 <span
class="math inline">\(r\)</span>
的薄壳层中实际发现的原子数与材料为理想气体（完全随机）时预期发现的原子数之比。</p>
<p>The formula is expressed as: <span class="math display">\[g(r)dr =
\frac{n(r)}{\text{ideal gas}}\]</span></p>
<ul>
<li><strong><span class="math inline">\(n(r)\)</span></strong>:
Represents the average number of atoms found in a thin spherical shell
between a distance <span class="math inline">\(r\)</span> and <span
class="math inline">\(r+dr\)</span> from a central atom.
表示距离中心原子 <span class="math inline">\(r\)</span> 到 <span
class="math inline">\(r+dr\)</span> 之间的薄球壳中原子的平均数量。</li>
<li><strong>ideal gas</strong>: Represents the number of atoms you would
expect in that same shell if the atoms were distributed completely
randomly with the same average density (<span
class="math inline">\(\rho\)</span>). The volume of this shell is
approximately <span class="math inline">\(4\pi r^2
dr\)</span>.表示如果原子完全随机分布且平均密度 (<span
class="math inline">\(\rho\)</span>)
相同，则该球壳中原子的数量。该球壳的体积约为 <span
class="math inline">\(4\pi r^2 dr\)</span>。</li>
</ul>
<p>A peak in the <span class="math inline">\(g(r)\)</span> plot
indicates a high probability of finding neighboring atoms at that
specific distance, revealing the material’s structural shells (e.g.,
nearest neighbors, second-nearest neighbors, etc.).<span
class="math inline">\(g(r)\)</span>
图中的峰值表示在该特定距离处找到相邻原子的概率很高，从而揭示了材料的结构壳（例如，最近邻、次近邻等）。</p>
<h3 id="calculation-method">## Calculation Method</h3>
<p>The board outlines a two-step averaging process to get a
statistically meaningful result from simulation data (a “film” at 20
frames per second).</p>
<ol type="1">
<li><p><strong>Average over atoms:</strong> In a single frame (a
snapshot in time), you pick one atom as the center. Then, you count how
many other atoms (<span class="math inline">\(n(r)\)</span>) are in
concentric spherical shells around it. This process is repeated,
treating each atom in the frame as the center, and the results are
averaged.</p></li>
<li><p><strong>Average over frames:</strong> The entire process
described above is repeated for multiple frames from the simulation or
video. This time-averaging ensures that the final result represents the
typical structure of the material over time, smoothing out random
fluctuations.</p></li>
</ol>
<p>The board notes “dx = bin width 0.01Å”, which is a practical detail
for the calculation. To create a histogram, the distance <code>r</code>
is divided into small segments (bins) of 0.01 angstroms.</p>
<h3 id="connection-to-experiments">## Connection to Experiments</h3>
<p>Finally, the whiteboard mentions <strong>“frame X-ray
scattering”</strong>. This is a crucial point because it connects this
computational analysis to real-world experiments. Experimental
techniques like X-ray or neutron scattering can be used to measure a
quantity called the structure factor, <span
class="math inline">\(S(q)\)</span>, which is directly related to the
radial distribution function <span class="math inline">\(g(r)\)</span>
through a mathematical operation called a Fourier transform. This allows
scientists to directly compare the structure produced in their
simulations with the structure of a real material measured in a lab.
最后，白板上提到了<strong>“帧 X
射线散射”</strong>。这一点至关重要，因为它将计算分析与实际实验联系起来。X射线或中子散射等实验技术可以用来测量一个称为结构因子<span
class="math inline">\(S(q)\)</span>的量，该量通过傅里叶变换的数学运算与径向分布函数<span
class="math inline">\(g(r)\)</span>直接相关。这使得科学家能够直接将模拟中产生的结构与实验室测量的真实材料结构进行比较。</p>
<p>The board correctly links <span class="math inline">\(g(r)\)</span>
to X-ray scattering experiments. The quantity measured in these
experiments is the <strong>static structure factor</strong>, <span
class="math inline">\(S(q)\)</span>, which describes how the material
scatters radiation. The relationship between the two is a Fourier
transform: 该板正确地将<span
class="math inline">\(g(r)\)</span>与X射线散射实验联系起来。这些实验中测量的量是<strong>静态结构因子</strong><span
class="math inline">\(S(q)\)</span>，它描述了材料如何散射辐射。两者之间的关系是傅里叶变换：
<span class="math display">\[S(q) = 1 + 4 \pi \rho \int_0^\infty [g(r) -
1] r^2 \frac{\sin(qr)}{qr} dr\]</span> This equation is crucial because
it bridges the gap between computer simulations (which calculate <span
class="math inline">\(g(r)\)</span>) and physical experiments (which
measure <span class="math inline">\(S(q)\)</span>).
这个方程至关重要，因为它弥合了计算机模拟（计算 <span
class="math inline">\(g(r)\)</span>）和物理实验（测量 <span
class="math inline">\(S(q)\)</span>）之间的差距。</p>
<h3
id="the-gaussian-distribution-probability-of-particle-position-高斯分布粒子位置的概率">##
2. The Gaussian Distribution: Probability of Particle Position
高斯分布：粒子位置的概率</h3>
<p>The board starts with the formula for a one-dimensional
<strong>Gaussian (or normal) distribution</strong>:
白板首先展示的是一维<strong>高斯（或正态）分布</strong>的公式：</p>
<p><span class="math display">\[f(x | \mu, \sigma^2) =
\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</span></p>
<p>This equation describes the probability of finding a particle at a
specific position <code>x</code> after a certain amount of time has
passed. * <strong><span class="math inline">\(\mu\)</span> (mu)</strong>
is the <strong>mean</strong> or average position. For a simple diffusion
process starting at the origin, the particles spread out symmetrically,
so the average position remains at the origin (<span
class="math inline">\(\mu = 0\)</span>). * <strong><span
class="math inline">\(\sigma^2\)</span> (sigma squared)</strong> is the
<strong>variance</strong>, which measures how spread out the particles
are from the mean position. A larger variance means the particles have,
on average, traveled farther from the starting point.
这个方程描述了经过一定时间后，在特定位置“x”找到粒子的概率。 *
<strong><span class="math inline">\(\mu\)</span> (mu)</strong>
是<strong>平均值</strong>或平均位置。对于从原点开始的简单扩散过程，粒子对称扩散，因此平均位置保持在原点（<span
class="math inline">\(\mu = 0\)</span>）。 * <strong><span
class="math inline">\(\sigma^2\)</span>（sigma 平方）</strong>
是<strong>方差</strong>，用​​于衡量粒子与平均位置的扩散程度。方差越大，意味着粒子平均距离起点越远。</p>
<p>The note “Black-Scholes” is a side reference. The Black-Scholes
model, famous in financial mathematics for pricing options, uses similar
mathematical principles based on Brownian motion to model the random
fluctuations of stock prices. “Black-Scholes”注释仅供参考。Black-Scholes
模型在金融数学中以期权定价而闻名，它使用基于布朗运动的类似数学原理来模拟股票价格的随机波动。</p>
<h3
id="mean-squared-displacement-msd-quantifying-the-spread-均方位移-msd量化扩散">##
3. Mean Squared Displacement (MSD): Quantifying the Spread 均方位移
(MSD)：量化扩散</h3>
<p>The core of the board is dedicated to the <strong>Mean Squared
Displacement (MSD)</strong>. This is the primary tool used to measure
how far, on average, particles have moved over a time interval
<code>t</code>. 本版块的核心内容是<strong>均方位移
(MSD)</strong>。这是用于测量粒子在时间间隔“t”内平均移动距离的主要工具。</p>
<p>The variance <span class="math inline">\(\sigma^2\)</span> is
formally defined as the average of the squared deviations from the mean:
<span class="math display">\[\sigma^2 = \langle x^2(t) \rangle - \langle
x(t) \rangle^2\]</span> * <span class="math inline">\(\langle x(t)
\rangle\)</span> is the average displacement. As mentioned, for simple
diffusion, <span class="math inline">\(\langle x(t) \rangle =
0\)</span>. * <span class="math inline">\(\langle x^2(t)
\rangle\)</span> is the average of the <em>square</em> of the
displacement. 方差<span
class="math inline">\(\sigma^2\)</span>的正式定义为与平均值偏差平方的平均值：
<span class="math display">\[\sigma^2 = \langle x^2(t) \rangle - \langle
x(t) \rangle^2\]</span> * <span class="math inline">\(\langle x(t)
\rangle\)</span>是平均位移。如上所述，对于简单扩散，<span
class="math inline">\(\langle x(t) \rangle = 0\)</span>。 * <span
class="math inline">\(\langle x^2(t)
\rangle\)</span>是位移<em>平方</em>的平均值。</p>
<p>Since <span class="math inline">\(\langle x(t) \rangle = 0\)</span>,
the variance is simply equal to the MSD: <span
class="math display">\[\sigma^2 = \langle x^2(t) \rangle\]</span> 由于
<span class="math inline">\(\langle x(t) \rangle =
0\)</span>，方差等于均方差 (MSD)： <span class="math display">\[\sigma^2
= \langle x^2(t) \rangle\]</span></p>
<p>The crucial insight for a diffusive process is that the <strong>MSD
grows linearly with time</strong>. The rate of this growth is determined
by the <strong>diffusion coefficient, D</strong>. The board shows this
relationship for different dimensions: 扩散过程的关键在于<strong>MSD
随时间线性增长</strong>。其增长率由<strong>扩散系数
D</strong>决定。棋盘显示了不同维度下的这种关系：</p>
<ul>
<li><strong>1D:</strong> <span class="math inline">\(\langle x^2(t)
\rangle = 2Dt\)</span> (Movement along a line) （沿直线运动）</li>
<li><strong>2D:</strong> The board has a slight typo or ambiguity with
<span class="math inline">\(\langle z^2(t) \rangle = 2Dt\)</span>. For
2D motion in the x-y plane, the total MSD would be <span
class="math inline">\(\langle r^2(t) \rangle = \langle x^2(t) \rangle +
\langle y^2(t) \rangle = 4Dt\)</span>. The note on the board might be
referring to just one component of motion. **棋盘上的 <span
class="math inline">\(\langle z^2(t) \rangle = 2Dt\)</span>
存在轻微拼写错误或歧义。对于 x-y 平面上的二维运动，总平均散射差 (MSD) 为
<span class="math inline">\(\langle r^2(t) \rangle = \langle x^2(t)
\rangle + \langle y^2(t) \rangle =
4Dt\)</span>。黑板上的注释可能仅指运动的一个分量。</li>
<li><strong>3D:</strong> <span class="math inline">\(\langle r^2(t)
\rangle = \langle |\vec{r}(t) - \vec{r}(0)|^2 \rangle = 6Dt\)</span>
(Movement in 3D space, which is the most common case in molecular
simulations) （三维空间中的运动，这是分子模拟中最常见的情况） Here,
<span class="math inline">\(\vec{r}(t)\)</span> is the position vector
of a particle at time <code>t</code>. The quantity <span
class="math inline">\(\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle\)</span> is the average of the squared distance a particle has
traveled from its initial position <span
class="math inline">\(\vec{r}(0)\)</span>. 这里，<span
class="math inline">\(\vec{r}(t)\)</span> 是粒子在时间 <code>t</code>
的位置矢量。 <span class="math inline">\(\langle |\vec{r}(t) -
\vec{r}(0)|^2 \rangle\)</span> 是粒子从其初始位置 <span
class="math inline">\(\vec{r}(0)\)</span> 行进距离的平方平均值。</li>
</ul>
<h3
id="the-einstein-relation-connecting-microscopic-motion-to-a-macroscopic-property-爱因斯坦关系将微观运动与宏观特性联系起来">##
4. The Einstein Relation: Connecting Microscopic Motion to a Macroscopic
Property 爱因斯坦关系：将微观运动与宏观特性联系起来</h3>
<p>Finally, the board presents the famous <strong>Einstein
relation</strong>, which rearranges the 3D MSD equation to solve for the
diffusion coefficient <code>D</code>:</p>
<p><span class="math display">\[D = \lim_{t \to \infty} \frac{\langle
|\vec{r}(t) - \vec{r}(0)|^2 \rangle}{6t}\]</span></p>
<p>This is a cornerstone equation in statistical mechanics. It provides
a practical way to calculate a macroscopic property—the
<strong>diffusion coefficient <code>D</code></strong>—from the
microscopic movements of individual particles observed in a computer
simulation.
这是统计力学中的一个基石方程。它提供了一种实用的方法，可以通过计算机模拟中观察到的单个粒子的微观运动来计算宏观属性——扩散系数“D”。</p>
<p>In practice, one would: 1. Run a simulation of particles.
运行粒子模拟。 2. Track the position of each particle over time.
跟踪每个粒子随时间的位置。 3. Calculate the squared displacement <span
class="math inline">\(|\vec{r}(t) - \vec{r}(0)|^2\)</span> for each
particle at various time intervals <code>t</code>.
计算每个粒子在不同时间间隔“t”的位移平方<span
class="math inline">\(|\vec{r}(t) - \vec{r}(0)|^2\)</span>。 4. Average
this value over all particles to get the MSD, <span
class="math inline">\(\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle\)</span>. 对所有粒子取平均值，得到均方差（MSD），即<span
class="math inline">\(\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle\)</span>。 5. Plot the MSD as a function of time.
将MSD绘制成时间函数。 6. The slope of this line, divided by 6, gives the
diffusion coefficient <code>D</code>. The <code>lim t→∞</code> indicates
that this linear relationship is most accurate for long time scales,
after initial transient effects have died down.
这条直线的斜率除以6，即扩散系数“D”。“lim
t→∞”表明，在初始瞬态效应消退后，这种线性关系在长时间尺度上最为准确。</p>
<h3 id="right-board-green-kubo-relations">## 5. Right Board: Green-Kubo
Relations</h3>
<p>This board introduces a more advanced and powerful method to
calculate transport coefficients like the diffusion coefficient, known
as the <strong>Green-Kubo relations</strong>.
本面板介绍了一种更先进、更强大的方法来计算扩散系数等传输系数，即<strong>Green-Kubo
关系</strong>。</p>
<h4 id="velocity-autocorrelation-function-vacf-速度自相关函数-vacf">###
<strong>Velocity Autocorrelation Function (VACF)</strong> 速度自相关函数
(VACF)</h4>
<p>The key idea is to look at how a particle’s velocity at one point in
time is related to its velocity at a later time. This is measured by the
<strong>Velocity Autocorrelation Function (VACF)</strong>: <span
class="math display">\[C_{vv}(t) = \langle \vec{v}(t&#39;) \cdot
\vec{v}(t&#39; + t) \rangle\]</span> This function tells us how long a
particle “remembers” its velocity. For a typical liquid, the velocity is
quickly randomized by collisions, so the VACF decays to zero rapidly.
其核心思想是考察粒子在某一时间点的速度与其在之后时间点的速度之间的关系。这可以通过<strong>速度自相关函数
(VACF)</strong>来测量： <span class="math display">\[C_{vv}(t) = \langle
\vec{v}(t&#39;) \cdot \vec{v}(t&#39; + t) \rangle\]</span>
此函数告诉我们粒子“记住”其速度的时间。对于典型的液体，速度会因碰撞而迅速随机化，因此
VACF 会迅速衰减为零。</p>
<h4 id="connecting-msd-and-vacf">### <strong>Connecting MSD and
VACF</strong></h4>
<p>The board shows the mathematical link between the MSD and the VACF.
Starting with the definition of position as the integral of velocity,
<span class="math inline">\(\vec{r}(t) = \int_0^t \vec{v}(t&#39;)
dt&#39;\)</span>, one can show that the MSD is a double integral of the
VACF. The board writes this as: <span class="math display">\[\langle
x^2(t) \rangle = \left\langle \left( \int_0^t v(t&#39;) dt&#39; \right)
\left( \int_0^t v(t&#39;&#39;) dt&#39;&#39; \right) \right\rangle =
\int_0^t dt&#39; \int_0^t dt&#39;&#39; \langle v(t&#39;) v(t&#39;&#39;)
\rangle\]</span> This shows that the two pictures of motion—the
particle’s displacement (MSD) and its velocity fluctuations (VACF)—are
deeply connected. 该面板展示了 MSD 和 VACF
之间的数学联系。从位置定义为速度的积分开始，<span
class="math inline">\(\vec{r}(t) = \int_0^t \vec{v}(t&#39;)
dt&#39;\)</span>，可以证明 MSD 是 VACF 的二重积分。黑板上写着： <span
class="math display">\[\langle x^2(t) \rangle = \left\langle \left(
\int_0^t v(t&#39;) dt&#39; \right) \left( \int_0^t v(t&#39;&#39;)
dt&#39;&#39; \right) \right\rangle = \int_0^t dt&#39; \int_0^t
dt&#39;&#39; \langle v(t&#39;) v(t&#39;&#39;) \rangle\]</span>
这表明，粒子运动的两幅图像——粒子的位移（MSD）和速度涨落（VACF）——之间存在着深刻的联系。</p>
<h4 id="the-green-kubo-formula-for-diffusion-扩散的格林-久保公式">###
<strong>The Green-Kubo Formula for Diffusion
扩散的格林-久保公式</strong></h4>
<p>By combining the Einstein relation with the integral of the VACF, one
arrives at the Green-Kubo formula for the diffusion coefficient: <span
class="math display">\[D = \frac{1}{3} \int_0^\infty \langle \vec{v}(0)
\cdot \vec{v}(t) \rangle dt\]</span> This incredible result states that
the <strong>macroscopic</strong> property of diffusion (<span
class="math inline">\(D\)</span>) is determined by the integral of the
<strong>microscopic</strong> velocity correlations. It’s often a more
efficient way to compute <span class="math inline">\(D\)</span> in
simulations than calculating the long-time limit of the MSD.
将爱因斯坦关系与VACF积分相结合，可以得到扩散系数的格林-久保公式： <span
class="math display">\[D = \frac{1}{3} \int_0^\infty \langle \vec{v}(0)
\cdot \vec{v}(t) \rangle dt\]</span>
这个令人难以置信的结果表明，扩散的<strong>宏观</strong>特性（<span
class="math inline">\(D\)</span>）由<strong>微观</strong>速度关联的积分决定。在模拟中，这通常是计算<span
class="math inline">\(D\)</span>比计算MSD的长期极限更有效的方法。</p>
<h3 id="the-grand-narrative-from-micro-to-macro-宏大叙事从微观到宏观">##
6. The Grand Narrative: From Micro to Macro 宏大叙事：从微观到宏观</h3>
<p>The previous whiteboards gave us two ways to calculate the
<strong>diffusion constant, D</strong>, from the microscopic random walk
of individual atoms:
之前的白板提供了两种从单个原子的微观随机游动计算<strong>扩散常数
D</strong>的方法： 1. <strong>Einstein Relation:</strong> From the
long-term slope of the Mean Squared Displacement (MSD). 根据均方位移
(MSD) 的长期斜率。 2. <strong>Green-Kubo Relation:</strong> From the
integral of the Velocity Autocorrelation Function (VACF).
根据速度自相关函数 (VACF) 的积分。</p>
<p>This new whiteboard shows how that single microscopic parameter,
<code>D</code>, governs the large-scale, observable process of diffusion
described by <strong>Fick’s Laws</strong> and the <strong>Diffusion
Equation</strong>. 这块新的白板展示了单个微观参数 <code>D</code>
如何控制<strong>菲克定律</strong>和<strong>扩散方程</strong>所描述的大规模可观测扩散过程。</p>
<h3 id="the-starting-point-a-liquids-structure-起点液体的结构">## 1. The
Starting Point: A Liquid’s Structure 起点：液体的结构</h3>
<p>The plot on the top left is the <strong>Radial Distribution Function,
<span class="math inline">\(g(r)\)</span></strong>, which we discussed
in detail from the first whiteboard. 左上角的图是<strong>径向分布函数
<span
class="math inline">\(g(r)\)</span></strong>，我们在第一个白板上详细讨论过它。</p>
<ul>
<li><strong>The Plot:</strong> It shows the characteristic structure of
a liquid. The peaks are labeled “1st”, “2nd”, and “3rd”, corresponding
to the first, second, and third <strong>solvation shells</strong>
(layers of neighboring atoms).
它显示了液体的特征结构。峰分别标记为“第一”、“第二”和“第三”，分别对应于第一、第二和第三<strong>溶剂化壳层</strong>（相邻原子层）。</li>
<li><strong>The Limit:</strong> The note <code>lim r→∞ g(r) = 1</code>
confirms that at large distances, the liquid has no long-range order, as
expected.注释“lim r→∞ g(r) =
1”证实了在远距离下，液体没有长程有序，这与预期一致。</li>
<li><strong>System Parameters:</strong> The values <code>T = 0.71</code>
and <code>ρ = 0.844</code> are the temperature and density of the
simulated system (likely in reduced or “Lennard-Jones” units) for which
this <span class="math inline">\(g(r)\)</span> was calculated. 值“T =
0.71”和“ρ =
0.844”分别是模拟系统的温度和密度（可能采用约化或“Lennard-Jones”单位），用于计算此
<span class="math inline">\(g(r)\)</span>。</li>
</ul>
<p>This section sets the stage: we are looking at the dynamics within a
system that has this specific liquid-like structure.
本节奠定了基础：我们将研究具有特定类液体结构的系统内的动力学。</p>
<h3 id="the-macroscopic-laws-of-diffusion-宏观扩散定律">## 2. The
Macroscopic Laws of Diffusion 宏观扩散定律</h3>
<p>The bottom-left and top-right sections introduce the continuum
equations that describe how concentration changes in space and time.
左下角和右上角部分介绍了描述浓度随空间和时间变化的连续方程。左下角和右上角部分介绍了描述浓度随空间和时间变化的连续方程。</p>
<h4 id="ficks-first-law-菲克第一定律">### <strong>Fick’s First Law
菲克第一定律</strong></h4>
<p><span class="math display">\[\vec{J} = -D \nabla C\]</span> This is
Fick’s first law of diffusion. It states that there is a
<strong>flux</strong> of particles (<span
class="math inline">\(\vec{J}\)</span>), meaning a net flow. This flow
is directed from high concentration to low concentration (hence the
minus sign) and its magnitude is proportional to the
<strong>concentration gradient</strong> (<span
class="math inline">\(\nabla C\)</span>).
这是菲克第一扩散定律。它指出存在粒子的<strong>通量</strong> (<span
class="math inline">\(\vec{J}\)</span>)，即净流量。该流量从高浓度流向低浓度（因此带有负号），其大小与<strong>浓度梯度</strong>
(<span class="math inline">\(\nabla C\)</span>) 成正比。</p>
<p><strong>The Crucial Link:</strong> The proportionality constant is
<strong>D</strong>, the very same <strong>diffusion constant</strong> we
calculated from the microscopic random walk (MSD/VACF). This is the key
connection: the collective result of countless individual random walks
is a predictable net flow of particles.
比例常数是<strong>D</strong>，与我们根据微观随机游走 (MSD/VACF)
计算出的<strong>扩散常数</strong>完全相同。这是关键的联系：无数个体随机游动的集合结果是可预测的粒子净流。</p>
<h4
id="the-diffusion-equation-ficks-second-law-扩散方程菲克第二定律">###
<strong>The Diffusion Equation (Fick’s Second Law)
扩散方程（菲克第二定律）</strong></h4>
<p><span class="math display">\[\frac{\partial C(\vec{r},t)}{\partial t}
= D \nabla^2 C(\vec{r},t)\]</span> This is the <strong>diffusion
equation</strong>, one of the most important equations in physics and
chemistry (also called the heat equation, as noted). It’s derived from
Fick’s first law and the principle of mass conservation (<span
class="math inline">\(\frac{\partial C}{\partial t} + \nabla \cdot
\vec{J} = 0\)</span>). It’s a differential equation that tells you
exactly how the concentration at any point, <span
class="math inline">\(C(\vec{r},t)\)</span>, will change over time.
这就是<strong>扩散方程</strong>，它是物理学和化学中最重要的方程之一（也称为热方程）。它源于菲克第一定律和质量守恒定律（<span
class="math inline">\(\frac{\partial C}{\partial t} + \nabla \cdot
\vec{J} = 0\)</span>）。它是一个微分方程，可以精确地告诉你任意一点的浓度
<span class="math inline">\(C(\vec{r},t)\)</span> 随时间的变化。</p>
<h3
id="the-solution-connecting-back-to-the-random-walk-与随机游动联系起来">##
3. The Solution: Connecting Back to the Random Walk
与随机游动联系起来</h3>
<p>This is the most beautiful part. The board shows the solution to the
diffusion equation for a very specific scenario, linking the macroscopic
equation directly back to the microscopic random walk.
黑板上展示了一个非常具体场景下扩散方程的解，将宏观方程直接与微观随机游动联系起来。</p>
<h4 id="the-initial-condition-初始条件">### <strong>The Initial
Condition 初始条件</strong></h4>
<p>The problem is set up by assuming all particles start at a single
point at time zero: <span class="math display">\[C(\vec{r}, 0) =
\delta(\vec{r})\]</span> This is a <strong>Dirac delta
function</strong>, representing an infinitely concentrated point source
at the origin. 问题假设所有粒子在时间零点处从一个点开始： <span
class="math display">\[C(\vec{r}, 0) = \delta(\vec{r})\]</span>
这是一个<strong>狄拉克函数</strong>，表示一个在原点处无限集中的点源。</p>
<h4 id="the-fundamental-solution-greens-function-基本解格林函数">###
<strong>The Fundamental Solution (Green’s Function)
基本解（格林函数）</strong></h4>
<p>The solution to the diffusion equation with this starting condition
is called the <strong>fundamental solution</strong> or <strong>Green’s
function</strong>. For one dimension, it is: <span
class="math display">\[C(x,t) = \frac{1}{\sqrt{4\pi Dt}}
\exp\left(-\frac{x^2}{4Dt}\right)\]</span></p>
<p><strong>The “Aha!” Moment:</strong> This is a <strong>Gaussian
distribution</strong>. Let’s compare it to the formula from the second
whiteboard: * The mean is <span class="math inline">\(\mu=0\)</span>.
均值为 <span class="math inline">\(\mu=0\)</span>。 * The variance is
<span class="math inline">\(\sigma^2 = 2Dt\)</span>. 方差为 <span
class="math inline">\(\sigma^2 = 2Dt\)</span>。</p>
<p>This is an incredible result. The macroscopic diffusion equation
predicts that a concentration pulse will spread out over time, and the
shape of the concentration profile will be a Gaussian curve. The width
of this curve, measured by its variance <span
class="math inline">\(\sigma^2\)</span>, is <strong>exactly the Mean
Squared Displacement, <span class="math inline">\(\langle x^2(t)
\rangle\)</span>, of the individual random-walking particles.</strong>
宏观扩散方程预测浓度脉冲会随时间扩散，浓度分布的形状将是高斯曲线。这条曲线的宽度，用其方差
<span class="math inline">\(\sigma^2\)</span>
来衡量，<strong>恰好是单个随机游动粒子的均方位移 <span
class="math inline">\(\langle x^2(t) \rangle\)</span>。</strong></p>
<p>This perfectly unites the two perspectives: * <strong>Microscopic微观
(Board 2):</strong> Particles undergo a random walk, and their average
squared displacement from the origin grows as <span
class="math inline">\(\langle x^2(t) \rangle = 2Dt\)</span>.
粒子进行随机游动，它们相对于原点的平均平方位移随着 <span
class="math inline">\(\langle x^2(t) \rangle = 2Dt\)</span>
的增长而增长。 * <strong>Macroscopic宏观 (This Board):</strong> A
collection of these particles, described by a continuum concentration
<code>C</code>, spreads out in a Gaussian profile whose variance is
<span class="math inline">\(\sigma^2 = 2Dt\)</span>.
这些粒子的集合，用连续浓度“C”来描述，呈方差为 <span
class="math inline">\(\sigma^2 = 2Dt\)</span> 的高斯分布。</p>
<p>The two pictures are mathematically identical.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/17/5120C3-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/17/5120C3-2/" class="post-title-link" itemprop="url">PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W3-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-09-17 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-17T21:00:00+08:00">2025-09-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-21 05:21:00" itemprop="dateModified" datetime="2025-09-21T05:21:00+08:00">2025-09-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PHYS-5120/" itemprop="url" rel="index"><span itemprop="name">PHYS-5120</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PHYS 5120 - 计算能源材料和电子结构模拟 Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://chem.hkust.edu.hk/people/ding-pan-panding">Lecturer:
Prof.PAN DING</a></p>
<h2 id="radial-distribution-function-rdf静态结构">1 radial distribution
function RDF静态结构:</h2>
<ul>
<li><strong>内容</strong>: This whiteboard serves as an excellent
summary, pulling together all the key concepts we’ve discussed into a
single, cohesive picture. Let’s connect everything on this slide to our
detailed conversation.</li>
</ul>
<h3 id="rdf-the-static-structure-rdf静态结构">1. RDF: The Static
Structure RDF静态结构</h3>
<p>On the top left, you see <strong>RDF (Radial Distribution
Function)</strong>.</p>
<ul>
<li><strong>The Plots:</strong> The board shows the familiar <span
class="math inline">\(g(r)\)</span> plot with its characteristic peaks
for a liquid. Below it is a plot of the interatomic potential energy,
<span class="math inline">\(V(r)\)</span>. This addition is very
insightful! It shows <em>why</em> the first peak in <span
class="math inline">\(g(r)\)</span> exists: it corresponds to the
minimum energy distance (<span class="math inline">\(\sigma\)</span>)
where particles are most stable and likely to be found.
白板展示了我们熟悉的<span
class="math inline">\(g(r)\)</span>图，它带有液体的特征峰。下方是原子间势能<span
class="math inline">\(V(r)\)</span>的图。这个补充非常有见地！它解释了为什么
<span class="math inline">\(g(r)\)</span>
中的第一个峰值存在：它对应于粒子最稳定且最有可能被发现的最小能量距离
(<span class="math inline">\(\sigma\)</span>)。</li>
<li><strong>Connection:</strong> This section summarizes our first
discussion. It’s the starting point for our analysis—a static snapshot
of the material’s average atomic arrangement before we consider how the
atoms move.
本节总结了我们的第一个讨论。这是我们分析的起点——在我们考虑原子如何运动之前，它是材料平均原子排列的静态快照。</li>
</ul>
<h3
id="msd-and-the-einstein-relation-the-displacement-picture-均方位移-msd-和爱因斯坦关系位移图像">2.
MSD and The Einstein Relation: The Displacement Picture 均方位移 (MSD)
和爱因斯坦关系：位移图像</h3>
<p>The board then moves to dynamics, presenting two methods to calculate
the <strong>diffusion constant, D</strong>. The first is the
<strong>Einstein relation</strong>. 两种计算<strong>扩散常数
D</strong>的方法。第一种是<strong>爱因斯坦关系</strong>。</p>
<ul>
<li><strong>The Formula:</strong> It correctly states that the Mean
Squared Displacement (MSD), <span class="math inline">\(\langle r^2
\rangle\)</span>, is equal to <span class="math inline">\(6Dt\)</span>
in three dimensions. It then rearranges this to solve for <span
class="math inline">\(D\)</span>: 它正确地指出了均方位移 (MSD)，<span
class="math inline">\(\langle r^2 \rangle\)</span>，在三维空间中等于
<span class="math inline">\(6Dt\)</span>。然后重新排列该公式以求解 <span
class="math inline">\(D\)</span>： <span class="math display">\[D =
\lim_{t\to\infty} \frac{\langle |\vec{r}(t) - \vec{r}(0)|^2
\rangle}{6t}\]</span></li>
<li><strong>The Diagram:</strong> The central diagram beautifully
illustrates the concept. It shows a particle in a simulation box (with
“N=108” likely being the number of particles simulated) moving from an
initial position <span class="math inline">\(\vec{r}_i(0)\)</span> to a
final position <span class="math inline">\(\vec{r}_i(t_j)\)</span>. The
MSD is the average of the square of this displacement over all particles
and many time origins. The graph labeled “MSD” shows how you would plot
this data and find the slope (“fitting”) to calculate <span
class="math inline">\(D\)</span>.
中间的图表完美地阐释了这个概念。它展示了一个粒子在模拟框中（“N=108”
可能是模拟粒子的数量）从初始位置 <span
class="math inline">\(\vec{r}_i(0)\)</span> 移动到最终位置 <span
class="math inline">\(\vec{r}_i(t_j)\)</span>。MSD
是该位移平方在所有粒子和多个时间原点上的平均值。标有“MSD”的图表显示了如何绘制这些数据并找到斜率（“拟合”）来计算
<span class="math inline">\(D\)</span>。</li>
<li><strong>Connection:</strong> This is a perfect summary of the
“Displacement Picture” we analyzed on the second whiteboard. It’s the
most intuitive way to think about diffusion: how far particles spread
out over
time.这完美地总结了我们在第二个白板上分析的“位移图”。这是思考扩散最直观的方式：粒子随时间扩散的距离。</li>
</ul>
<h3
id="the-green-kubo-relation-the-fluctuation-picture-格林-久保关系涨落图">3.
The Green-Kubo Relation: The Fluctuation Picture
格林-久保关系：涨落图</h3>
<p>Finally, the board presents the more advanced but often more
practical method: the <strong>Green-Kubo relation</strong>.</p>
<ul>
<li><strong>The Equations:</strong> This section displays the two key
equations from our last discussion:
<ol type="1">
<li>The MSD as the double integral of the Velocity Autocorrelation
Function (VACF). 速度自相关函数 (VACF) 的二重积分的均方差 (MSD)。</li>
<li>The crucial derivative step: <span
class="math inline">\(\frac{d\langle x^2(t)\rangle}{dt} = 2 \int_0^t
dt&#39;&#39; \langle V_x(t) V_x(t&#39;&#39;) \rangle\)</span>.
关键的导数步骤：<span class="math inline">\(\frac{d\langle
x^2(t)\rangle}{dt} = 2 \int_0^t dt&#39;&#39; \langle V_x(t)
V_x(t&#39;&#39;) \rangle\)</span>。</li>
</ol></li>
<li><strong>The Diagram:</strong> The small diagram of a square with
axes <span class="math inline">\(t&#39;\)</span> and <span
class="math inline">\(t&#39;&#39;\)</span> visually represents the
two-dimensional domain of integration for the double integral.
一个带有轴 <span class="math inline">\(t&#39;\)</span> 和 <span
class="math inline">\(t&#39;&#39;\)</span>
的小正方形图直观地表示了二重积分的二维积分域。</li>
<li><strong>Connection:</strong> This summarizes the “Fluctuation
Picture.” It shows the mathematical heart of the derivation that proves
the Einstein and Green-Kubo methods are equivalent. As we concluded,
this method is often numerically superior because it involves
integrating a rapidly decaying function (the VACF) rather than finding
the slope of a noisy, unbounded function (the MSD).
这概括了“涨落图”。它展示了证明爱因斯坦方法和格林-久保方法等价的推导过程的数学核心。正如我们总结的那样，这种方法通常在数值上更胜一筹，因为它涉及对快速衰减函数（VACF）进行积分，而不是求噪声无界函数（MSD）的斜率。</li>
</ul>
<p>In essence, this single whiteboard is a complete roadmap for
analyzing diffusion in a molecular simulation. It shows how to first
characterize the material’s <strong>structure</strong> (<span
class="math inline">\(g(r)\)</span>) and then how to compute its key
dynamic property—the <strong>diffusion constant
<code>D</code></strong>—using two powerful, interconnected methods.
本质上，这块白板就是分子模拟中分析扩散的完整路线图。它展示了如何首先表征材料的<strong>结构</strong>（<span
class="math inline">\(g(r)\)</span>），然后如何使用两种强大且相互关联的方法计算其关键的动态特性——<strong>扩散常数
<code>D</code></strong>。</p>
<p>This whiteboard beautifully concludes the derivation of the
Green-Kubo relation, showing the final formulas and how they are used in
practice. It provides the punchline to the mathematical story we’ve been
following.</p>
<p>Let’s break down the details.</p>
<h3 id="finalizing-the-derivation">4. Finalizing the Derivation</h3>
<p>The top lines of the board show the final step in connecting the Mean
Squared Displacement (MSD) to the Velocity Autocorrelation Function
(VACF).</p>
<p><span class="math display">\[\lim_{t\to\infty} \frac{d\langle x^2
\rangle}{dt} = 2 \int_0^\infty d\tau \langle V_x(0) V_x(\tau)
\rangle\]</span></p>
<ul>
<li><strong>The Left Side:</strong> As we know from the <strong>Einstein
relation</strong>, the long-time limit of the derivative of the 1D MSD,
<span class="math inline">\(\lim_{t\to\infty} \frac{d\langle x^2
\rangle}{dt}\)</span>, is simply equal to <strong><span
class="math inline">\(2D\)</span></strong>.</li>
<li><strong>The Right Side:</strong> This is the result of the
mathematical derivation from the previous slide. It shows that this same
quantity is also equal to twice the total integral of the VACF.</li>
</ul>
<p>By equating these two, we can solve for the diffusion coefficient,
<code>D</code>.</p>
<h3 id="the-velocity-autocorrelation-function-vacf">5. The Velocity
Autocorrelation Function (VACF)</h3>
<p>The board explicitly names the key quantity here:</p>
<p><span class="math display">\[\Phi(\tau) = \langle V_x(0) V_x(\tau)
\rangle\]</span></p>
<p>This is the <strong>“Velocity autocorrelation function”</strong>
(abbreviated as VAF on the board), which we’ve denoted as VACF. The
variable has been changed from <code>t</code> to <code>τ</code> (tau) to
represent a “time lag” or interval, which is common notation.</p>
<ul>
<li><strong>The Plot:</strong> The graph on the board shows a typical
plot of the VACF, <span class="math inline">\(\Phi(\tau)\)</span>,
versus the time lag <span class="math inline">\(\tau\)</span>.
<ul>
<li>It starts at a maximum positive value at <span
class="math inline">\(\tau=0\)</span> (when the velocity is perfectly
correlated with itself).</li>
<li>It rapidly decays towards zero as the particle undergoes collisions
that randomize its velocity.</li>
</ul></li>
<li><strong>The Integral:</strong> The shaded area under this curve
represents the value of the integral <span
class="math inline">\(\int_0^\infty \Phi(\tau) d\tau\)</span>. The
Green-Kubo formula states that the diffusion coefficient is directly
proportional to this area.</li>
</ul>
<h3 id="the-green-kubo-formulas-for-the-diffusion-coefficient">6. The
Green-Kubo Formulas for the Diffusion Coefficient</h3>
<p>After canceling the factor of 2, the board presents the final,
practical formulas for <code>D</code>.</p>
<ul>
<li><strong>In 1 Dimension:</strong> <span class="math display">\[D =
\int_0^\infty d\tau \langle V_x(0) V_x(\tau) \rangle\]</span></li>
<li><strong>In 3 Dimensions:</strong> This is the more general and
useful formula. <span class="math display">\[D = \frac{1}{3}
\int_0^\infty d\tau \langle \vec{v}(0) \cdot \vec{v}(\tau)
\rangle\]</span> There are two important changes for 3D:
<ol type="1">
<li>We use the full <strong>velocity vectors</strong> and their dot
product, <span class="math inline">\(\vec{v}(0) \cdot
\vec{v}(\tau)\)</span>, to capture motion in all directions.</li>
<li>We divide by <strong>3</strong> to get the average contribution to
diffusion in any one direction (x, y, or z).</li>
</ol></li>
</ul>
<h3 id="practical-calculation-in-a-simulation">7. Practical Calculation
in a Simulation</h3>
<p>The last formula on the board shows how this is implemented in a
computer simulation with a finite number of atoms.</p>
<p><span class="math display">\[D = \frac{1}{3N} \int_0^\infty d\tau
\sum_{i=1}^{N} \langle \vec{v}_i(0) \cdot \vec{v}_i(\tau)
\rangle\]</span></p>
<ul>
<li><strong><span
class="math inline">\(\sum_{i=1}^{N}\)</span></strong>: This
<strong>summation</strong> symbol indicates that you must compute the
VACF for <em>each individual atom</em> (from atom <code>i=1</code> to
atom <code>N</code>).</li>
<li><strong><span class="math inline">\(\frac{1}{N}\)</span></strong>:
You then <strong>average</strong> the results over all <code>N</code>
atoms in your simulation box.</li>
<li><strong><span class="math inline">\(\langle \dots
\rangle\)</span></strong>: The angle brackets here still imply an
additional average over multiple different starting times
(<code>t=0</code>) to get good statistics.</li>
</ul>
<p>This formula is the practical recipe: to get the diffusion
coefficient, you track the velocity of every atom, calculate each one’s
VACF, average them together, and then integrate the result over
time.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/16/5054C3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/16/5054C3/" class="post-title-link" itemprop="url">MSDM 5054 - Statistical Machine Learning-L3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-09-16 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-16T21:00:00+08:00">2025-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-19 19:24:11" itemprop="dateModified" datetime="2025-09-19T19:24:11+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>统计机器学习Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="general-linear-regression-model.">1. General linear regression
model.</h1>
<p><img src="/imgs/5054C3/General_linear_regression_model.png" alt="Diagram of a linear regression model">
## 1.1 general linear regression model - <strong>内容</strong>:
<strong>general linear regression model</strong>.</p>
<p>the fundamental equation:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1x_{i1} + \dots +
\beta_px_{ip} + \epsilon_i\]</span></p>
<p>And it correctly identifies the main goal: to <strong>estimate the
parameters</strong> (the coefficients <span
class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span>) from
data so we can make predictions on new data.</p>
<p>核心目标：通过数据<strong>估计参数</strong>（即系数 <span
class="math inline">\(\beta_0, \beta_1, \dots,
\beta_p\)</span>），从而对新数据进行预测。</p>
<h2
id="how-we-actually-find-the-best-values-for-the-β-coefficients-parameter-estimation">1.2
How we actually find the best values for the <span
class="math inline">\(β\)</span> coefficients (parameter
estimation)?:</h2>
<ul>
<li><strong>内容</strong>: We find the best values for the <span
class="math inline">\(\beta\)</span> coefficients by finding the values
that <strong>minimize the overall error</strong> of the model. The most
common and fundamental method for this is called <strong>Ordinary Least
Squares (OLS)</strong>.</li>
</ul>
<h3
id="the-main-method-ordinary-least-squares-ols-普通最小二乘法-ols">##
The Main Method: Ordinary Least Squares (OLS) 普通最小二乘法 (OLS)</h3>
<p>The core idea of OLS is to find the line (or hyperplane in multiple
dimensions) that is as close as possible to all the data points
simultaneously. OLS
的核心思想是找到一条尽可能同时接近所有数据点的直线（或多维超平面）。</p>
<h4 id="define-the-error-residuals-误差">1. Define the Error (Residuals)
误差</h4>
<p>First, we need to define what “error” means. For any single data
point, the error is the difference between the actual value (<span
class="math inline">\(y_i\)</span>) and the value predicted by our model
(<span class="math inline">\(\hat{y}_i\)</span>). This difference is
called the <strong>residual</strong>.
首先，需要定义“误差”的含义。对于任何单个数据点，误差是实际值 (<span
class="math inline">\(y_i\)</span>) 与模型预测值 (<span
class="math inline">\(\hat{y}_i\)</span>)
之间的差值。这个差值称为<strong>残差</strong>。</p>
<p><strong>Residual</strong> = Actual Value - Predicted Value
<strong>残差</strong> = 实际值 - 预测值 <span class="math display">\[e_i
= y_i - \hat{y}_i\]</span></p>
<p>You can visualize residuals as the vertical distance from each data
point to the regression line.
可以将残差可视化为每个数据点到回归线的垂直距离。</p>
<h4
id="the-cost-function-sum-of-squared-residuals-成本函数残差平方和">2.
The Cost Function: Sum of Squared Residuals 成本函数：残差平方和</h4>
<p>We want to make all these residuals as small as possible. We can’t
just add them up, because some are positive and some are negative, and
they would cancel each other out.
所有残差尽可能小。不能简单地将它们相加，因为有些是正数，有些是负数，它们会相互抵消。</p>
<p>So, we square each residual (which makes them all positive) and then
sum them up. This gives us the <strong>Sum of Squared Residuals
(SSR)</strong>, which is our “cost function.”
因此，将每个残差求平方（使它们都为正数），然后将它们相加。这就得到了<strong>残差平方和
(SSR)</strong>，也就是“成本函数”。</p>
<p><span class="math display">\[SSR = \sum_{i=1}^{n} e_i^2 =
\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span></p>
<p>The goal of OLS is simple: <strong>find the values of <span
class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span> that
make this SSR value as small as possible.</strong></p>
<h4
id="solving-for-the-coefficients-the-normal-equation-求解系数正态方程">3.
Solving for the Coefficients: The Normal Equation
求解系数：正态方程</h4>
<p>For linear regression, calculus provides a direct, exact solution to
this minimization problem. By taking the derivative of the SSR function
with respect to each <span class="math inline">\(\beta\)</span>
coefficient and setting it to zero, we can solve for the optimal values.
对于线性回归，微积分为这个最小化问题提供了直接、精确的解。通过对 SSR
函数的每个 <span class="math inline">\(\beta\)</span>
系数求导并将其设为零，就可以求解出最优值。</p>
<p>This process results in a formula known as the <strong>Normal
Equation</strong>, which can be expressed cleanly using matrix algebra:
这个过程会得到一个称为<strong>正态方程</strong>的公式，它可以用矩阵代数清晰地表示出来：</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the
vector of our estimated coefficients.估计系数的向量。</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is a matrix where
each row is an observation and each column is a feature (with an added
column of 1s for the intercept <span
class="math inline">\(\beta_0\)</span>).其中每一行代表一个观测值，每一列代表一个特征（截距
<span class="math inline">\(\beta_0\)</span> 增加了一列全为 1
的值）。</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of the
actual response values.实际响应值的向量。</li>
</ul>
<p>Statistical software and programming libraries (like Scikit-learn in
Python) use this equation (or more computationally stable versions of
it) to find the best coefficients for you instantly.</p>
<h3 id="an-alternative-method-gradient-descent-梯度下降">## An
Alternative Method: Gradient Descent 梯度下降</h3>
<p>While the Normal Equation gives a direct answer, it can be very slow
if you have a massive number of features (e.g., hundreds of thousands).
An alternative, iterative method used across machine learning is
<strong>Gradient Descent</strong>.</p>
<p><strong>The Intuition:</strong> Imagine the SSR cost function is a
big valley. Your initial (random) <span
class="math inline">\(\beta\)</span> coefficients place you somewhere on
the slope of this valley.</p>
<ol type="1">
<li><strong>Check the slope</strong> (the gradient) at your current
position. <strong>检查您当前位置的斜率</strong>（梯度）。</li>
<li><strong>Take a small step</strong> in the steepest <em>downhill</em>
direction. <strong>朝最陡的<em>下坡</em>方向</strong>迈出一小步**。</li>
<li><strong>Repeat.</strong> You keep taking steps downhill until you
reach the bottom of the valley. The bottom of the valley represents the
minimum SSR, and your coordinates at that point are the optimal <span
class="math inline">\(\beta\)</span> coefficients.
<strong>重复</strong>。您继续向下走，直到到达山谷底部。谷底代表最小SSR，该点的坐标即为最优<span
class="math inline">\(\beta\)</span>系数。</li>
</ol>
<p>The size of each “step” you take is controlled by a parameter called
the <strong>learning rate</strong>. Gradient Descent is the foundational
optimization algorithm for many complex models, including neural
networks.
每次“步进”的大小由一个称为<strong>学习率</strong>的参数控制。梯度下降是许多复杂模型（包括神经网络）的基础优化算法。</p>
<h3 id="summary-ols-vs.-gradient-descent">## Summary: OLS vs. Gradient
Descent</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Ordinary Least Squares (OLS)</th>
<th style="text-align: left;">Gradient Descent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>How it works</strong></td>
<td style="text-align: left;">Direct calculation using the Normal
Equation.</td>
<td style="text-align: left;">Iterative; takes steps to minimize
error.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Pros</strong></td>
<td style="text-align: left;">Provides an exact, optimal solution. No
parameters to tune.</td>
<td style="text-align: left;">More efficient for very large datasets.
Very versatile.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Cons</strong></td>
<td style="text-align: left;">Can be computationally expensive with many
features.</td>
<td style="text-align: left;">Requires choosing a learning rate. May not
find the exact minimum.</td>
</tr>
</tbody>
</table>
<h1 id="simple-linear-regression">2. Simple Linear Regression</h1>
<p><img src="/imgs/5054C3/Simple_Linear_Regression.png" alt="Simple_Linear_Regression"></p>
<h2 id="simple-linear-regression-1">2.1 Simple Linear Regression</h2>
<ul>
<li><strong>内容</strong>: <strong>Simple Linear Regression:</strong> a
special case of the general model you showed earlier where you only have
<strong>one</strong> predictor variable (<span
class="math inline">\(p=1\)</span>).</li>
</ul>
<h3 id="the-model-and-the-goal-模型和目标">## The Model and the Goal
模型和目标</h3>
<p>Sets up the simplified equation for a line: <span
class="math display">\[y_i = \beta_0 + \beta_1x_i + \epsilon_i\]</span>
* <span class="math inline">\(y_i\)</span> is the outcome you want to
predict.要预测的结果。 * <span class="math inline">\(x_i\)</span> is
your single input feature or covariate.单个输入特征或协变量。 * <span
class="math inline">\(\beta_1\)</span> is the <strong>slope</strong> of
the line. It tells you how much <span class="math inline">\(y\)</span>
is expected to increase for a one-unit increase in <span
class="math inline">\(x\)</span>.表示 <span
class="math inline">\(x\)</span> 每增加一个单位，<span
class="math inline">\(y\)</span> 预计会增加多少。 * <span
class="math inline">\(\beta_0\)</span> is the
<strong>intercept</strong>. It’s the predicted value of <span
class="math inline">\(y\)</span> when <span
class="math inline">\(x\)</span> is zero.当 <span
class="math inline">\(x\)</span> 为零时 <span
class="math inline">\(y\)</span> 的预测值。 * <span
class="math inline">\(\epsilon_i\)</span> is the random error
term.是随机误差项。</p>
<p>The goal, stated as “Minimize the sum of squares of err,” is exactly
the <strong>Ordinary Least Squares (OLS)</strong> method we just
discussed. It’s written here as: <span class="math display">\[\min_{a,b}
\sum_{i=1}^{n} (y_i - a - bx_i)^2\]</span> This is just a different way
of writing the same thing, where they use <code>a</code> for the
intercept (<span class="math inline">\(\beta_0\)</span>) and
<code>b</code> for the slope (<span
class="math inline">\(\beta_1\)</span>). You’re trying to find the
specific values of the slope and intercept that make the sum of all the
squared errors as small as possible.
目标，即“最小化误差平方和”，正是<strong>普通最小二乘法
(OLS)</strong>。： <span class="math display">\[\min_{a,b}
\sum_{i=1}^{n} (y_i - a - bx_i)^2\]</span> 这是另一种写法，其中用
<code>a</code> 表示截距 (<span
class="math inline">\(\beta_0\)</span>)，<code>b</code> 表示斜率 (<span
class="math inline">\(\beta_1\)</span>)。尝试找到斜率和截距的具体值，使得所有平方误差之和尽可能小。</p>
<h3 id="the-solution-the-estimator-formulas-解决方案估计公式">## The
Solution: The Estimator Formulas 解决方案：估计公式</h3>
<p>The most important part of this slide is the
<strong>solution</strong>. For the simple case with only one variable,
you don’t need complex matrix algebra (the Normal Equation). Instead,
the minimization problem can be solved with these two straightforward
formulas:
对于只有一个变量的简单情况，不需要复杂的矩阵代数（正态方程）。相反，最小化问题可以用以下两个简单的公式来解决：</p>
<h4 id="the-slope-hatbeta_1">1. The Slope: <span
class="math inline">\(\hat{\beta}_1\)</span></h4>
<p><span class="math display">\[\hat{\beta}_1 = \frac{\sum_{i=1}^{n}
(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i -
\bar{x})^2}\]</span> * <strong>Intuition:</strong> This formula might
look complex, but it’s actually very intuitive. * The numerator, <span
class="math inline">\(\sum(x_i - \bar{x})(y_i - \bar{y})\)</span>, is
closely related to the <strong>covariance</strong> between X and Y. It
measures whether X and Y tend to move in the same direction (positive
slope) or in opposite directions (negative slope). 与 X 和 Y
之间的<strong>协方差</strong>密切相关。它衡量 X 和 Y
是倾向于朝相同方向（正斜率）还是朝相反方向（负斜率）移动。 * The
denominator, <span class="math inline">\(\sum(x_i - \bar{x})^2\)</span>,
is related to the <strong>variance</strong> of X. It measures how much X
varies on its own. 它衡量 X 自身的变化量。 * <strong>In short, the slope
is a measure of how X and Y vary together, scaled by how much X varies
by itself.</strong> 斜率衡量的是 X 和 Y 共同变化的程度，并以 X
自身的变化量为标度。</p>
<h4 id="the-intercept-hatbeta_0-截距">2. The Intercept: <span
class="math inline">\(\hat{\beta}_0\)</span> 截距</h4>
<p><span class="math display">\[\hat{\beta}_0 = \bar{y} -
\hat{\beta}_1\bar{x}\]</span> * <strong>Intuition:</strong> This formula
is even simpler and has a wonderful geometric meaning. It ensures that
the <strong>line of best fit always passes through the “center of mass”
of the data</strong>, which is the point of averages <span
class="math inline">\((\bar{x}, \bar{y})\)</span>.
它确保<strong>最佳拟合线始终穿过数据的“质心”</strong>，即平均值 <span
class="math inline">\((\bar{x}, \bar{y})\)</span> 的点。计算出最佳斜率
(<span class="math inline">\(\hat{\beta}_1\)</span>)
后，就可以将其代入此公式。然后，可以调整截距 (<span
class="math inline">\(\hat{\beta}_0\)</span>)，使直线完美地围绕数据云的中心点旋转。
* Once you’ve calculated the best slope (<span
class="math inline">\(\hat{\beta}_1\)</span>), you can plug it into this
formula. You then adjust the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) so that the line pivots
perfectly around the central point of your data cloud.</p>
<p>In summary, this slide provides the precise, closed-form formulas to
calculate the slope and intercept for the line of best fit in a simple
linear regression model.</p>
<h1 id="statistical-inference">3. Statistical Inference</h1>
<p><img src="/imgs/5054C3/Statistical_Inference1.png" alt="Statistical_Inference1">
<img src="/imgs/5054C3/Statistical_Inference2.png" alt="Statistical_Inference2">
## 3.1 Statistical Inference - <strong>内容</strong>:
<strong>Statistical Inference:</strong> These two slides are deeply
connected and explain how we go from just <em>calculating</em> the
coefficients to understanding how <em>accurate</em> and
<em>reliable</em> they are.
解释了我们如何从仅仅<em>计算</em>系数到理解它们的<em>准确性</em>和<em>可靠性</em>。</p>
<h3 id="the-core-problem-quantifying-uncertainty-量化不确定性">## The
Core Problem: Quantifying Uncertainty 量化不确定性</h3>
<p>The second slide poses the fundamental questions: * “How accurate are
<span class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span>?”准确性如何？ * “What are
the distributions of <span class="math inline">\(\hat{\beta}_0\)</span>
and <span class="math inline">\(\hat{\beta}_1\)</span>?”分布是什么？</p>
<p>The reason we ask this is that our estimated coefficients (<span
class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>) were
calculated from a <strong>specific sample of data</strong>. If we
collected a different random sample from the same population, we would
get slightly different estimates.估计的系数 (<span
class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>)
是根据<strong>特定的数据样本</strong>计算出来的。如果我们从同一总体中随机抽取不同的样本，我们得到的估计值会略有不同。</p>
<p>The goal of statistical inference is to use the estimates from our
single sample to make conclusions about the <strong>true, unknown
population parameters</strong> (<span class="math inline">\(\beta_0,
\beta_1\)</span>) and to quantify our uncertainty about
them.统计推断的目标是利用单个样本的估计值得出关于<strong>真实、未知的总体参数</strong>（<span
class="math inline">\(\beta_0,
\beta_1\)</span>）的结论，并量化对这些参数的不确定性。</p>
<h3
id="the-key-assumption-that-makes-it-possible-实现这一目标的关键假设">##
The Key Assumption That Makes It Possible 实现这一目标的关键假设</h3>
<p>To figure out the distribution of our estimates, we must make an
assumption about the distribution of the errors. This is the most
important assumption in linear regression for inference:
为了确定估计值的分布，必须对误差的分布做出假设。这是线性回归推断中最重要的假设：
<strong>Assumption:</strong> <span class="math inline">\(\epsilon_i
\stackrel{\text{i.i.d.}}{\sim} N(0, \sigma^2)\)</span></p>
<p>This means we assume the random error terms are: * <strong>Normally
distributed</strong> (<span class="math inline">\(N\)</span>).*
<strong>正态分布</strong>（<span class="math inline">\(N\)</span>）。 *
Have a mean of <strong>zero</strong> (our model is correct on average).*
均值为<strong>零</strong>（模型平均而言是正确的）。 * Have a constant
variance <strong><span class="math inline">\(\sigma^2\)</span></strong>
(homoscedasticity).* 方差为常数<strong><span
class="math inline">\(\sigma^2\)</span></strong>（方差齐性）。 * Are
<strong>independent and identically distributed</strong> (i.i.d.),
meaning each error is independent of the others.*
是<strong>独立同分布</strong>（i.i.d.）的，这意味着每个误差都独立于其他误差。</p>
<p><strong>Why is this important?</strong> Because our coefficients
<span class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span> are calculated as weighted
sums of the <span class="math inline">\(y_i\)</span> values, and the
<span class="math inline">\(y_i\)</span> values depend on the errors
<span class="math inline">\(\epsilon_i\)</span>. This assumption about
the errors allows us to prove that our estimated coefficients themselves
are also normally distributed. 系数 <span
class="math inline">\(\hat{\beta}_0\)</span> 和 <span
class="math inline">\(\hat{\beta}_1\)</span> 是通过 <span
class="math inline">\(y_i\)</span> 值的加权和计算的，而 <span
class="math inline">\(y_i\)</span> 值取决于误差 <span
class="math inline">\(\epsilon_i\)</span>。这个关于误差的假设使能够证明估计的系数本身也服从正态分布。</p>
<h3
id="the-solution-the-theorem-and-the-t-distribution-定理和-t-分布">##
The Solution: The Theorem and the t-distribution 定理和 t 分布</h3>
<p>The first slide provides the central theorem that allows us to
perform inference. It tells us exactly how to standardize our estimated
coefficients so they follow a known distribution.
第一张幻灯片提供了进行推断的核心定理。它准确地告诉我们如何对估计的系数进行标准化，使其服从已知的分布。</p>
<h4 id="the-standard-error-s.e.-标准误差-s.e.">1. The Standard Error
(s.e.) 标准误差 (s.e.)</h4>
<p>First, look at the denominators in the red dotted boxes. These are
the <strong>standard errors</strong> of the coefficients,
<code>s.e.($\hat&#123;\beta&#125;_1$)</code> and
<code>s.e.($\hat&#123;\beta&#125;_0$)</code>.
第一张幻灯片提供了进行推断的核心定理。它准确地告诉我们如何对估计的系数进行标准化，使其服从已知的分布。</p>
<ul>
<li><strong>What it is:</strong> The standard error is the estimated
<strong>standard deviation of the coefficient’s sampling
distribution</strong>. In simpler terms, it’s a measure of the average
amount by which our estimate <span
class="math inline">\(\hat{\beta}_1\)</span> would differ from the true
<span class="math inline">\(\beta_1\)</span> if we were to repeat the
experiment many times.
标准误差是系数抽样分布的<strong>标准差</strong>估计值。简单来说，它衡量的是如果我们重复实验多次，我们估计的
<span class="math inline">\(\hat{\beta}_1\)</span> 与真实的 <span
class="math inline">\(\beta_1\)</span> 之间的平均差异。</li>
<li><strong>A smaller standard error means a more precise and reliable
estimate.</strong>
<strong>标准误差越小，估计值越精确可靠。</strong></li>
</ul>
<h4 id="the-t-statistic-t-统计量">2. The t-statistic t 统计量</h4>
<p>The theorem shows two fractions that form a
<strong>t-statistic</strong>. The general structure for this is:
该定理展示了两个构成<strong>t 统计量</strong>的分数。其一般结构如下：
<span class="math display">\[t = \frac{\text{ (Sample Estimate - True
Value) }}{\text{ Standard Error of the Estimate }}\]</span></p>
<p>For <span class="math inline">\(\beta_1\)</span>, this is: <span
class="math inline">\(\frac{\hat{\beta}_1 -
\beta_1}{\text{s.e.}(\hat{\beta}_1)}\)</span>.</p>
<p>The key insight is that this specific quantity follows a
<strong>Student’s t-distribution</strong> with <strong><span
class="math inline">\(n-2\)</span> degrees of freedom</strong>.
关键在于，这个特定量服从<strong>学生 t
分布</strong>，其自由度为<strong><span
class="math inline">\(n-2\)</span>。 * </strong>Student’s
t-distribution:** This is a probability distribution that looks very
similar to the normal distribution but has slightly “heavier” tails. We
use it instead of the normal distribution because we had to
<em>estimate</em> the standard deviation of the errors (<code>s</code>
in the formula), which adds extra uncertainty.
这是一种概率分布，与正态分布非常相似，但尾部略重。使用它来代替正态分布，是因为必须<em>估计</em>误差的标准差（公式中的
<code>s</code>），这会增加额外的不确定性。 * <strong>Degrees of Freedom
(n-2):</strong> We start with <code>n</code> data points, but we lose
two degrees of freedom because we used the data to estimate two
parameters: <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>. 从 <code>n</code>
个数据点开始，但由于用这些数据估计了两个参数：<span
class="math inline">\(\beta_0\)</span> 和 <span
class="math inline">\(\beta_1\)</span>，因此损失了两个自由度。 #### 3.
Estimating the Error Variance (<span
class="math inline">\(s^2\)</span>)估计误差方差 (<span
class="math inline">\(s^2\)</span>) To calculate the standard errors, we
need a value for <code>s</code>, which is our estimate of the true error
standard deviation <span class="math inline">\(\sigma\)</span>. This is
calculated from the <strong>Residual Sum of Squares (RSS)</strong>.
为了计算标准误差，我们需要一个 <code>s</code> 的值，它是对真实误差标准差
<span class="math inline">\(\sigma\)</span>
的估计值。该值由<strong>残差平方和 (RSS)</strong> 计算得出。 *
<strong>RSS:</strong> First, we calculate the RSS = <span
class="math inline">\(\sum(y_i - \hat{y}_i)^2\)</span>, which is the sum
of all the squared errors.* <strong>RSS</strong>：首先，计算 RSS = <span
class="math inline">\(\sum(y_i -
\hat{y}_i)^2\)</span>，即所有平方误差之和。 * <strong><span
class="math inline">\(s^2\)</span>:</strong> Then, we find the estimate
of the error variance: <span class="math inline">\(s^2 = \text{RSS} /
(n-2)\)</span>. We divide by <span class="math inline">\(n-2\)</span> to
get an unbiased estimate. * <strong><span
class="math inline">\(s^2\)</span></strong>：然后，计算误差方差的估计值：<span
class="math inline">\(s^2 = \text{RSS} / (n-2)\)</span>。我们将其除以
<span class="math inline">\(n-2\)</span> 即可得到无偏估计值。 *
<code>s</code> is simply the square root of <span
class="math inline">\(s^2\)</span>. This <code>s</code> is the value
used in the standard error formulas.* <code>s</code> 就是 <span
class="math inline">\(s^2\)</span> 的平方根。这个 <code>s</code>
是标准误差公式中使用的值。</p>
<h3 id="what-this-allows-us-to-do-the-practical-use">## What This Allows
Us To Do (The Practical Use)</h3>
<p>Because we know the exact distribution of our t-statistic, we can now
achieve our goal of quantifying uncertainty: 因为知道 t
统计量的精确分布，所以现在可以实现量化不确定性的目标：</p>
<ol type="1">
<li><strong>Hypothesis Testing:</strong> We can test if a predictor is
actually useful. The most common test is for the null hypothesis <span
class="math inline">\(H_0: \beta_1 = 0\)</span>. If we can prove the
observed <span class="math inline">\(\hat{\beta}_1\)</span> is very
unlikely to occur if the true <span
class="math inline">\(\beta_1\)</span> were zero, we can conclude there
is a statistically significant relationship between <span
class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span>.
可以检验一个预测变量是否真的有用。最常见的检验是零假设 <span
class="math inline">\(H_0: \beta_1 = 0\)</span>。如果能证明，当真实的
<span class="math inline">\(\beta_1\)</span> 为零时，观测到的 <span
class="math inline">\(\hat{\beta}_1\)</span>
不太可能发生，那么就可以得出结论，<span class="math inline">\(x\)</span>
和 <span class="math inline">\(y\)</span>
之间存在统计学上的显著关系。</li>
<li><strong>Confidence Intervals:</strong> We can construct a range of
plausible values for the true coefficient. For example, we can calculate
a 95% confidence interval for <span
class="math inline">\(\beta_1\)</span>. This gives us a range where we
are 95% confident the true value of <span
class="math inline">\(\beta_1\)</span> lies.
可以为真实系数构建一系列合理的值。</li>
</ol>
<h1 id="multiple-linear-regression">4. Multiple Linear Regression</h1>
<p><img src="/imgs/5054C3/Multiple_Linear Regression1.png" alt="Multiple_Linear Regression1">
<img src="/imgs/5054C3/Multiple_Linear Regression2.png" alt="Multiple_Linear Regression2">
## 4.1 Multiple Linear Regression - <strong>内容</strong>:
<strong>Multiple Linear Regression:</strong></p>
<p>Here’s a detailed breakdown that connects both slides.</p>
<h3
id="the-model-from-one-to-many-predictors-从单预测变量到多预测变量">##
The Model: From One to Many Predictors 从单预测变量到多预测变量</h3>
<p>The first slide introduces the <strong>Multiple Linear Regression
model</strong>. This is a direct extension of the simple model, but
instead of using just one predictor variable, we use multiple (<span
class="math inline">\(p\)</span>) predictors to explain our response
variable.
多元线性回归模型是简单模型的直接扩展，但不是只使用一个预测变量，而是使用多个（<span
class="math inline">\(p\)</span>）预测变量来解释响应变量。</p>
<p>The general formula is: <span class="math display">\[y_i = \beta_0 +
\beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip} +
\epsilon_i\]</span></p>
<h4 id="key-change-in-interpretation">Key Change in Interpretation</h4>
<p>This is the most important new concept. In simple regression, <span
class="math inline">\(\beta_1\)</span> was just the slope. In multiple
regression, each coefficient has a more nuanced meaning:
在简单回归中，<span class="math inline">\(\beta_1\)</span>
只是斜率。在多元回归中，每个系数都有更微妙的含义：</p>
<p><strong><span class="math inline">\(\beta_j\)</span> is the average
change in <span class="math inline">\(y\)</span> for a one-unit increase
in <span class="math inline">\(x_j\)</span>, while holding all other
predictors constant.</strong></p>
<p>This is incredibly powerful. Using the advertising example from your
slide: * <span class="math inline">\(y_i = \beta_0 +
\beta_1(\text{TV}_i) + \beta_2(\text{Radio}_i) +
\beta_3(\text{Newspaper}_i) + \epsilon_i\)</span> * <span
class="math inline">\(\beta_1\)</span> represents the effect of TV
advertising on sales, <strong>after controlling for</strong> the amount
spent on Radio and Newspaper ads. This allows you to isolate the unique
contribution of each advertising
channel.表示在<strong>控制</strong>广播和报纸广告支出后，电视广告对销售额的影响。这可以让您区分每个广告渠道的独特贡献。</p>
<h3 id="the-solution-deriving-the-normal-equation-推导正态方程">## The
Solution: Deriving the Normal Equation 推导正态方程</h3>
<p>The second slide shows the mathematical process for finding the best
coefficients (<span class="math inline">\(\beta_0, \beta_1, \dots,
\beta_p\)</span>) using the <strong>Ordinary Least Squares
(OLS)</strong> method. It’s essentially a condensed derivation of the
<strong>Normal Equation</strong>. 使用<strong>普通最小二乘法
(OLS)</strong> 寻找最佳系数 (<span class="math inline">\(\beta_0,
\beta_1, \dots, \beta_p\)</span>)
的数学过程。它本质上是<strong>正态方程</strong>的简化推导。</p>
<h4 id="the-goal-minimizing-the-sum-of-squares-最小化平方和">1. The
Goal: Minimizing the Sum of Squares 最小化平方和</h4>
<p>Just like before, our goal is to minimize the sum of the squared
errors (or residuals): 目标是最小化平方误差（或残差）之和。</p>
<ul>
<li><strong>Scalar Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_{i1} -
\beta_2x_{i2} - \beta_3x_{i3})^2\)</span>
<ul>
<li>This is easy to read but gets very long with more variables.
代码易于阅读，但变量越多，代码越长。</li>
</ul></li>
<li><strong>Vector Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \boldsymbol{\beta}^T
\mathbf{x}_i)^2\)</span>
<ul>
<li>This is a more compact and powerful way to write the same thing
using linear algebra, where <span
class="math inline">\(\boldsymbol{\beta}^T \mathbf{x}_i\)</span> is the
dot product that calculates the entire predicted value <span
class="math inline">\(\hat{y}_i\)</span>.
这是一种更简洁、更强大的线性代数表示方法，其中 <span
class="math inline">\(\boldsymbol{\beta}^T \mathbf{x}_i\)</span>
是计算整个预测值 <span class="math inline">\(\hat{y}_i\)</span>
的点积。</li>
</ul></li>
</ul>
<h4
id="the-method-using-calculus-to-find-the-minimum-使用微积分求最小值">2.
The Method: Using Calculus to Find the Minimum 使用微积分求最小值</h4>
<p>To find the set of <span class="math inline">\(\beta\)</span> values
that results in the lowest possible error, we use calculus.</p>
<ul>
<li><p><strong>The Derivative (Gradient):</strong> Since our error
function depends on multiple <span class="math inline">\(\beta\)</span>
coefficients, we can’t take a simple derivative. Instead, we take the
<strong>gradient</strong>, which is a vector of partial derivatives (one
for each coefficient). This tells us the “slope” of the error function
in every direction. 导数（梯度） 误差函数依赖于多个 <span
class="math inline">\(\beta\)</span>
系数，因此我们不能简单地求导数。相反，采用<strong>梯度</strong>，它是一个由偏导数组成的向量（每个系数对应一个偏导数）。这告诉误差函数在各个方向上的“斜率”。</p></li>
<li><p><strong>Setting the Gradient to Zero:</strong> The minimum of a
function occurs where its slope is zero (the very bottom of the error
“valley”). The slide shows the result of taking this gradient and
setting it to
zero.函数的最小值出现在其斜率为零的地方（即误差“谷底”的最低点）。幻灯片展示了取此梯度并将其设为零的结果。</p></li>
</ul>
<p>The equation shown on the slide: <span class="math display">\[2
\sum_{i=1}^{n} (\boldsymbol{\beta}^T \mathbf{x}_i - y_i)\mathbf{x}_i^T =
0\]</span> …is the result of this calculus step. The goal is now to
algebraically rearrange this equation to solve for <span
class="math inline">\(\boldsymbol{\beta}\)</span>.
是这一微积分步骤的结果。现在的目标是用代数方法重新排列这个方程，以求解
<span class="math inline">\(\boldsymbol{\beta}\)</span>。</p>
<h4 id="the-result-the-normal-equation-正则方程">3. The Result: The
Normal Equation 正则方程</h4>
<p>After rearranging the equation from the previous step and expressing
the sums in their full matrix form, we arrive at a clean and beautiful
solution. While the slide doesn’t show the final step, the result of
“Setting the gradient zero and solve <span
class="math inline">\(\beta\)</span>” is the <strong>Normal
Equation</strong>:
重新排列上一步中的方程，并将和表示为完整的矩阵形式后，得到了一个简洁美观的解。虽然幻灯片没有展示最后一步，“设置梯度零点并求解
<span class="math inline">\(\beta\)</span>”
的结果就是<strong>正态方程</strong>：</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the
vector of our optimal coefficient estimates.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is the “design
matrix” where each row is an observation and each column is a predictor
variable. <span class="math inline">\(\mathbf{X}\)</span>
是“设计矩阵”，其中每一行代表一个观测值，每一列代表一个预测变量。</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of our
response variable. <span class="math inline">\(\mathbf{y}\)</span>
是我们的响应变量的向量。</li>
</ul>
<p>This single equation is the general solution for finding the OLS
coefficients for <strong>any</strong> linear regression model, no matter
how many predictors you have. This is what statistical software
calculates for you under the hood.
无论有多少个预测变量，这个简单的方程都是<strong>任何</strong>线性回归模型中
OLS 系数的通解。</p>
<h1 id="matrix-notatio">5. matrix notatio</h1>
<p><img src="/imgs/5054C3/matrix_notatio.png"></p>
<ul>
<li><strong>内容</strong>: This slide introduces the <strong>matrix
notation</strong> for multiple linear regression, which is a powerful
way to represent the entire system of equations in a compact form. This
notation isn’t just for tidiness—it’s the foundation for how the
solutions are derived and calculated in software.</li>
</ul>
<p>多元线性回归的<strong>矩阵符号</strong>，这是一种以紧凑形式表示整个方程组的有效方法。这种符号不仅仅是为了简洁，它还是软件中推导和计算解的基础。
Here is a more detailed breakdown.</p>
<h3 id="why-use-matrix-notation">## Why Use Matrix Notation?</h3>
<p>Imagine you have 10,000 observations (<span
class="math inline">\(n=10,000\)</span>) and 5 predictor variables
(<span class="math inline">\(p=5\)</span>). Writing out the model
equation for each observation would be impossible: <span
class="math inline">\(y_1 = \beta_0 + \beta_1x_{11} + \dots +
\beta_5x_{15} + \epsilon_1\)</span> <span class="math inline">\(y_2 =
\beta_0 + \beta_1x_{21} + \dots + \beta_5x_{25} + \epsilon_2\)</span>
…and so on for 10,000 lines.</p>
<p>假设你有 10,000 个观测值（n=10,000）和 5
个预测变量（p=5）。为每个观测值写出模型方程是不可能的： <span
class="math inline">\(y_1 = \beta_0 + \beta_1x_{11} + \dots +
\beta_5x_{15} + \epsilon_1\)</span> <span class="math inline">\(y_2 =
\beta_0 + \beta_1x_{21} + \dots + \beta_5x_{25} + \epsilon_2\)</span>
……以此类推，直到 10,000 行。 Matrix notation allows us to consolidate
this entire system into a single, elegant
equation:矩阵符号使我们能够将整个系统合并成一个简洁的方程： <span
class="math display">\[\mathbf{y} = \mathbf{X}\boldsymbol{\beta} +
\boldsymbol{\epsilon}\]</span> Let’s break down each component shown on
your slide.</p>
<h3 id="the-components-explained">## The Components Explained</h3>
<h4 id="the-design-matrix-mathbfx-设计矩阵">1. The Design Matrix: <span
class="math inline">\(\mathbf{X}\)</span> 设计矩阵</h4>
<p><span class="math display">\[\mathbf{X} = \begin{pmatrix} 1 &amp;
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\ 1 &amp; x_{21} &amp;
x_{22} &amp; \cdots &amp; x_{2p} \\ \vdots &amp; \vdots &amp; \vdots
&amp; \ddots &amp; \vdots \\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots
&amp; x_{np} \end{pmatrix}\]</span> This is the most important matrix.
It contains all of your predictor variable
data.这是最重要的矩阵。它包含所有预测变量数据。 * <strong>Rows:</strong>
Each row represents a single observation (e.g., a person, a company, a
day). There are <strong>n</strong>
rows.每一行代表一个观察值（例如，一个人、一家公司、一天）。共有
<strong>n</strong> 行。 * <strong>Columns:</strong> Each column
represents a predictor variable. There are <strong>p</strong> predictor
columns, plus one special column.每列代表一个预测变量。共有
<strong>p</strong> 个预测列，外加一个特殊列。 * <strong>The Column of
Ones:</strong> This is a crucial detail. This first column of all ones
is a placeholder for the <strong>intercept term (<span
class="math inline">\(\beta_0\)</span>)</strong>. When you perform
matrix multiplication, this <code>1</code> gets multiplied by <span
class="math inline">\(\beta_0\)</span>, ensuring the intercept is
included in the model for every single observation.
这是一个至关重要的细节。第一列（全 1）是<strong>截距项 (<span
class="math inline">\(\beta_0\)</span>)</strong>
的占位符。执行矩阵乘法时，这个 <code>1</code> 会乘以 <span
class="math inline">\(\beta_0\)</span>，以确保截距包含在模型中，适用于每个观测值。</p>
<h4 id="the-coefficient-vector-boldsymbolbeta-系数向量">2. The
Coefficient Vector: <span
class="math inline">\(\boldsymbol{\beta}\)</span> 系数向量</h4>
<p><span class="math display">\[\boldsymbol{\beta} = \begin{pmatrix}
\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix}\]</span> This is a
column vector that contains all the model parameters—the unknown values
we want to estimate. The goal of linear regression is to find the
numerical values for this vector.</p>
<h4 id="the-response-vector-mathbfy-响应向量">3. The Response Vector:
<span class="math inline">\(\mathbf{y}\)</span> 响应向量</h4>
<p><span class="math display">\[\mathbf{y} = \begin{pmatrix} y_1 \\
\vdots \\ y_n \end{pmatrix}\]</span> This is a column vector containing
all the observed outcomes you are trying to predict (e.g., sales, test
scores, stock prices).</p>
<h4 id="the-error-vector-boldsymbolepsilon-误差向量">4. The Error
Vector: <span class="math inline">\(\boldsymbol{\epsilon}\)</span>
误差向量</h4>
<p><span class="math display">\[\boldsymbol{\epsilon} = \begin{pmatrix}
\epsilon_1 \\ \vdots \\ \epsilon_n \end{pmatrix}\]</span> This column
vector bundles together all the individual, unobserved random errors. It
represents the portion of <strong>y</strong> that our model cannot
explain with <strong>X</strong>.</p>
<h3 id="putting-it-all-together">## Putting It All Together</h3>
<p>When you write the equation <span class="math inline">\(\mathbf{y} =
\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span>, you are
actually representing the entire system of individual equations.</p>
<p>Let’s look at the multiplication <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>: <span
class="math display">\[\begin{pmatrix} 1 &amp; x_{11} &amp; \dots &amp;
x_{1p} \\ 1 &amp; x_{21} &amp; \dots &amp; x_{2p} \\ \vdots &amp; \vdots
&amp; \ddots &amp; \vdots \\ 1 &amp; x_{n1} &amp; \dots &amp; x_{np}
\end{pmatrix} \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
\end{pmatrix} = \begin{pmatrix} 1\cdot\beta_0 + x_{11}\cdot\beta_1 +
\dots + x_{1p}\cdot\beta_p \\ 1\cdot\beta_0 + x_{21}\cdot\beta_1 + \dots
+ x_{2p}\cdot\beta_p \\ \vdots \\ 1\cdot\beta_0 + x_{n1}\cdot\beta_1 +
\dots + x_{np}\cdot\beta_p \end{pmatrix}\]</span> As you can see, the
result of this multiplication is a single column vector where each row
is the “predictor” part of the regression equation for that observation.
此乘法的结果是一个单列向量，其中每一行都是该观测值的回归方程的“预测变量”部分。</p>
<p>By setting this equal to <span class="math inline">\(\mathbf{y} -
\boldsymbol{\epsilon}\)</span>, you perfectly recreate the entire set of
<code>n</code> equations in one clean statement. This compact form is
what allows us to easily derive and compute the <strong>Normal
Equation</strong> solution: <span
class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>.这种紧凑形式使我们能够轻松推导和计算<strong>正态方程</strong>的解</p>
<h1
id="the-core-mathematical-conclusion-of-ordinary-least-squares-ols">6.
the core mathematical conclusion of Ordinary Least Squares (OLS)</h1>
<p><img src="/imgs/5054C3/OLS1.png">
<img src="/imgs/5054C3/OLS2.png"></p>
<ul>
<li><strong>内容</strong>: Of course. These slides present the core
mathematical conclusion of Ordinary Least Squares (OLS) and a key
geometric property that explains <em>why</em> this solution works.
展示了普通最小二乘法 (OLS)
的核心数学结论，以及一个关键的几何性质，解释了该解决方案<em>为何</em>有效。
Let’s break down the concepts and the calculation processes in
detail.</li>
</ul>
<hr />
<h3 id="part-1-the-objective-and-the-solution-slide-1-最小化几何距离">##
Part 1: The Objective and the Solution (Slide 1) 最小化几何距离</h3>
<p>This slide summarizes the entire OLS problem and its solution in the
language of matrix algebra.</p>
<h4 id="the-concept-minimizing-geometric-distance"><strong>The Concept:
Minimizing Geometric Distance</strong></h4>
<p>“最小二乘准则”是我们模型的目标。 The “least squares criterion” is the
objective of our model. The slide shows it in two equivalent forms:</p>
<ol type="1">
<li><strong>Summation Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_{i1} -
\dots - \beta_px_{ip})^2\)</span> This is the sum of the squared
differences between the actual values (<span
class="math inline">\(y_i\)</span>) and the predicted values. 这是实际值
(<span class="math inline">\(y_i\)</span>) 与预测值之差的平方和。</li>
<li><strong>Matrix Form:</strong> <span
class="math inline">\(||\mathbf{y} - \mathbf{X}\boldsymbol{\beta}||^2 =
(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} -
\mathbf{X}\boldsymbol{\beta})\)</span> This is the more powerful way to
view the problem. Think of <span
class="math inline">\(\mathbf{y}\)</span> (the vector of all actual
outcomes) and <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> (the vector
of all predicted outcomes) as two points in an n-dimensional space. The
expression <span class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{\beta}||^2\)</span> represents the <strong>squared
Euclidean distance</strong> between these two points. 将 <span
class="math inline">\(\mathbf{y}\)</span>（所有实际结果的向量）和 <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>（所有预测结果的向量）视为
n 维空间中的两个点。表达式 <span class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{\beta}||^2\)</span>
表示这两点之间的<strong>平方欧氏距离</strong>。 Therefore, the OLS
problem is a geometric one: <strong>Find the coefficient vector <span
class="math inline">\(\boldsymbol{\beta}\)</span> that makes the
predicted values vector <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> as close as
possible to the actual values vector <span
class="math inline">\(\mathbf{y}\)</span>.</strong> 因此，OLS
问题是一个几何问题：<strong>找到一个系数向量 <span
class="math inline">\(\boldsymbol{\beta}\)</span>，使预测值向量 <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>
尽可能接近实际值向量 <span
class="math inline">\(\mathbf{y}\)</span>。</strong></li>
</ol>
<h4
id="the-solution-the-least-squares-estimator-lse最小二乘估计器-lse"><strong>The
Solution: The Least Squares Estimator (LSE)</strong>最小二乘估计器
(LSE)</h4>
<p>The slide provides the direct solution to this minimization problem,
which is the <strong>Normal
Equation</strong>:此最小化问题的直接解，即<strong>正态方程</strong>：</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<p>This formula gives you the exact vector of coefficients <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that minimizes
the squared distance. We get this formula by taking the gradient (the
multidimensional version of a derivative) of the distance function with
respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>,
setting it to zero, and solving, as hinted at in your previous slides.
给出了使平方距离最小化的精确系数向量 通过取距离函数关于 <span
class="math inline">\(\boldsymbol{\beta}\)</span>
的梯度（导数的多维版本），将其设为零，然后求解，即可得到此公式。
Finally, the slide defines: * <strong>Fitted values:</strong> <span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span> (The vector of predictions
using our optimal coefficients). 拟合值 * <strong>Residuals:</strong>
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span> (The vector of errors, representing the
difference between actuals and
predictions).误差向量，表示实际值与预测值之间的差异</p>
<h3
id="part-2-the-geometric-property-and-proofs-slide-2几何性质及证明">##
Part 2: The Geometric Property and Proofs (Slide 2)几何性质及证明</h3>
<p>This slide explains a beautiful and fundamental property of the least
squares solution:
<strong>orthogonality</strong>.解释了最小二乘解的一个美妙而基本的性质：<strong>正交性</strong>。</p>
<h4 id="the-concept-orthogonality-of-residuals残差的正交性"><strong>The
Concept: Orthogonality of Residuals</strong>残差的正交性</h4>
<p>The main idea is that the residual vector <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> is
<strong>orthogonal</strong> (perpendicular) to every predictor variable
in your model. 主要思想是残差向量 <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>
与模型中的每个预测变量<strong>正交</strong>（垂直）。</p>
<ul>
<li><p><strong>Geometric Intuition:</strong> Think of the columns of
your matrix <span class="math inline">\(\mathbf{X}\)</span> (i.e., your
predictors and the intercept) as defining a flat surface, or a
“hyperplane,” in a high-dimensional space. Your actual data vector <span
class="math inline">\(\mathbf{y}\)</span> exists somewhere in this
space, likely not on the hyperplane. The OLS process finds the point on
that hyperplane, <span class="math inline">\(\hat{\mathbf{y}}\)</span>,
that is closest to <span class="math inline">\(\mathbf{y}\)</span>. The
shortest line from a point to a plane is always one that is
<strong>perpendicular</strong> to the plane. The residual vector, <span
class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>, <em>is</em> that line. 将矩阵 <span
class="math inline">\(\mathbf{X}\)</span>
的列（即预测变量和截距）想象成在高维空间中定义一个平面或“超平面”。实际数据向量
<span class="math inline">\(\mathbf{y}\)</span>
存在于该空间的某个位置，可能不在超平面上。OLS 过程会在该超平面 <span
class="math inline">\(\hat{\mathbf{y}}\)</span> 上找到与 <span
class="math inline">\(\mathbf{y}\)</span>
最接近的点。从一个点到一个平面的最短线始终是与该平面<strong>垂直</strong>的线。残差向量
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span> 就是这条直线。</p></li>
<li><p><strong>Mathematical Statement:</strong> This geometric property
is stated as <span class="math inline">\(\mathbf{X}^T
\hat{\boldsymbol{\epsilon}} = \mathbf{0}\)</span>. This equation means
that the dot product of the residual vector with every column of <span
class="math inline">\(\mathbf{X}\)</span> is zero, which is the
mathematical definition of orthogonality. 该等式意味着残差向量与 <span
class="math inline">\(\mathbf{X}\)</span>
每一列的点积都为零，这正是正交性的数学定义。</p></li>
</ul>
<h4 id="the-calculation-process-the-proofs"><strong>The Calculation
Process (The Proofs)</strong></h4>
<p><strong>1. Proof of Orthogonality:</strong> The slide shows a
step-by-step calculation to prove that <span
class="math inline">\(\mathbf{X}^T \hat{\boldsymbol{\epsilon}}\)</span>
is indeed zero. * <strong>Step 1:</strong> Start with the expression to
be proven: <span class="math inline">\(\mathbf{X}^T
\hat{\boldsymbol{\epsilon}}\)</span> 从待证明的表达式开始： *
<strong>Step 2:</strong> Substitute the definition of the residual,
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\mathbf{X}^T (\mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}})\]</span> 代入残差的定义 *
<strong>Step 3:</strong> Distribute the <span
class="math inline">\(\mathbf{X}^T\)</span>: <span
class="math display">\[\mathbf{X}^T \mathbf{y} -
\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}}\]</span><em>分配 </em>
<strong>Step 4:</strong> Substitute the Normal Equation for <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\mathbf{X}^T \mathbf{y} - \mathbf{X}^T\mathbf{X}
[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}]\]</span> *
<strong>Step 5:</strong> The key step is the cancellation. A matrix
<span class="math inline">\((\mathbf{X}^T\mathbf{X})\)</span> multiplied
by its inverse <span
class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span> equals the
identity matrix <span class="math inline">\(\mathbf{I}\)</span>, which
acts like the number 1 in multiplication. <span
class="math display">\[\mathbf{X}^T \mathbf{y} - \mathbf{I}
\mathbf{X}^T\mathbf{y} = \mathbf{X}^T \mathbf{y} -
\mathbf{X}^T\mathbf{y} = \mathbf{0}\]</span> 关键步骤是消去。 This
completes the proof, showing that the orthogonality property is a direct
consequence of the Normal Equation solution.</p>
<p><strong>2. Proof of LSE:</strong> This is a more abstract proof
showing that our <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> truly gives the
minimum possible error. It uses the orthogonality property and the
Pythagorean theorem for vectors. It essentially shows that for any other
possible coefficient vector <span
class="math inline">\(\boldsymbol{v}\)</span>, the error <span
class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{v}||^2\)</span> will always be greater than or
equal to the error from our LSE, <span
class="math inline">\(||\mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}}||^2\)</span>.</p>
<h1 id="geometric-interpretation">7.geometric interpretation</h1>
<p><img src="/imgs/5054C3/geometric_interpretation1.png">
<img src="/imgs/5054C3/geometric_interpretation2.png"></p>
<ul>
<li><strong>内容</strong>:</li>
</ul>
<p>These two slides together provide a powerful geometric interpretation
of how Ordinary Least Squares (OLS) works, centered on the concepts of
<strong>orthogonality</strong> and <strong>projection</strong>.
以<strong>正交性</strong>和<strong>投影</strong>的概念为中心，从几何角度有力地诠释了普通最小二乘法
(OLS) 的工作原理。</p>
<p>Here’s a detailed summary of the concepts and the processes they
describe.</p>
<h3 id="summary">## Summary</h3>
<p>These slides explain that the process of finding the “best fit” line
in regression is geometrically equivalent to <strong>projecting</strong>
the actual data vector (<span class="math inline">\(\mathbf{y}\)</span>)
onto a hyperplane defined by the predictor variables (<span
class="math inline">\(\mathbf{X}\)</span>). This projection splits the
actual data into two perpendicular components:
解释了回归分析中寻找“最佳拟合”直线的过程，其几何意义等同于将实际数据向量
(<span class="math inline">\(\mathbf{y}\)</span>)
<strong>投影</strong>到由预测变量 (<span
class="math inline">\(\mathbf{X}\)</span>)
定义的超平面上。此投影将实际数据拆分为两个垂直分量：</p>
<ol type="1">
<li><strong>The Fitted Values (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>):</strong> The part of
the data that is perfectly explained by the model (the projection).
数据中能够被模型完美解释的部分（投影）。</li>
<li><strong>The Residuals (<span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>):</strong>
The part of the data that is unexplained (the error), which is
perpendicular to the explained part.
数据中无法解释的部分（误差），它与被解释部分垂直。 A special tool called
the <strong>projection matrix (H)</strong>, or “hat matrix,” is
introduced as the operator that performs this projection.
引入一个称为<strong>投影矩阵
(H)</strong>（或称“帽子矩阵”）的特殊工具，作为执行此投影的运算符。</li>
</ol>
<h3 id="concepts-and-process-explained-in-detail">## Concepts and
Process Explained in Detail</h3>
<h4 id="the-fitted-values-as-a-linear-combination-拟合值作为线性组合">1.
The Fitted Values as a Linear Combination 拟合值作为线性组合</h4>
<p>The first slide starts by stating that the fitted value vector <span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span> is a <strong>linear
combination</strong> of the columns of <span
class="math inline">\(\mathbf{X}\)</span> (your predictors).</p>
<ul>
<li><p><strong>Concept:</strong> This means that the vector of fitted
values, <span class="math inline">\(\hat{\mathbf{y}}\)</span>, must lie
within the geometric space (a line, plane, or hyperplane) spanned by
your predictor variables. The model is incapable of producing a
prediction that does not live in this space. 这意味着拟合值向量 <span
class="math inline">\(\hat{\mathbf{y}}\)</span>
必须位于预测变量所构成的几何空间（直线、平面或超平面）内。模型无法生成不存在于此空间的预测。
#### 2. The Projection Matrix (The “Hat Matrix”) 投影矩阵（“帽子矩阵”）
The second slide introduces the tool that makes this projection happen:
the <strong>projection matrix</strong>, also called the <strong>hat
matrix</strong>, <strong>H</strong>.</p></li>
<li><p><strong>Definition:</strong> <span
class="math inline">\(\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span></p></li>
<li><p><strong>Process:</strong> This matrix has a special job. When you
multiply it by any vector (like our data vector <span
class="math inline">\(\mathbf{y}\)</span>), it projects that vector onto
the space spanned by the columns of <span
class="math inline">\(\mathbf{X}\)</span>. We can see this by starting
with our definition of fitted values and substituting the normal
equation solution for <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\hat{\mathbf{y}} =
\mathbf{X}\hat{\boldsymbol{\beta}} =
\mathbf{X}[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}] =
[\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T]\mathbf{y}\]</span>
This shows that: <span class="math display">\[\hat{\mathbf{y}} =
\mathbf{H}\mathbf{y}\]</span> This is why <strong>H</strong> is
nicknamed the <strong>hat matrix</strong>—it “puts the hat” on <span
class="math inline">\(\mathbf{y}\)</span>.
这个矩阵有其特殊的用途。当你将它乘以任何向量（例如我们的数据向量 <span
class="math inline">\(\mathbf{y}\)</span>）时，它会将该向量投影到由
<span class="math inline">\(\mathbf{X}\)</span> 的列所跨越的空间上。
#### 3. The Orthogonality of Fitted Values and Residuals
拟合值和残差的正交性 This is the central concept of the first slide and
a fundamental property of least squares.</p></li>
<li><p><strong>Concept:</strong> The fitted value vector (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>) and the residual vector
(<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>) are <strong>orthogonal</strong>
(perpendicular) to each other.</p></li>
<li><p><strong>Mathematical Statement:</strong> Their dot product is
zero: <span class="math inline">\(\hat{\mathbf{y}}^T(\mathbf{y} -
\hat{\mathbf{y}}) = 0\)</span>.</p></li>
<li><p><strong>Geometric Intuition:</strong> This means the vectors
<span class="math inline">\(\mathbf{y}\)</span>, <span
class="math inline">\(\hat{\mathbf{y}}\)</span>, and <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> form a
<strong>right-angled triangle</strong> in n-dimensional space. The
actual data vector <span class="math inline">\(\mathbf{y}\)</span> is
the hypotenuse, while the model’s prediction <span
class="math inline">\(\hat{\mathbf{y}}\)</span> and the error <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> are the two
perpendicular legs. 这意味着向量 <span
class="math inline">\(\mathbf{y}\)</span>、<span
class="math inline">\(\hat{\mathbf{y}}\)</span> 和 <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> 在 n
维空间中构成一个<strong>直角三角形</strong>。实际数据向量 <span
class="math inline">\(\mathbf{y}\)</span> 是斜边，而模型的预测值 <span
class="math inline">\(\hat{\mathbf{y}}\)</span> 和误差值 <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>
是两条垂直边。</p></li>
</ul>
<h4 id="the-pythagorean-theorem-of-least-squares">4. The Pythagorean
Theorem of Least Squares</h4>
<p>The orthogonality relationship directly implies the Pythagorean
theorem.</p>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(||\mathbf{y}||^2 = ||\hat{\mathbf{y}}||^2 +
||\mathbf{y} - \hat{\mathbf{y}}||^2\)</span></li>
<li><strong>Concept:</strong> This is one of the most important
equations in statistics, as it partitions the total variance in the
data. 这是统计学中最重要的方程之一，因为它可以分割数据中的总方差。
<ul>
<li><span class="math inline">\(||\mathbf{y}||^2\)</span> is
proportional to the <strong>Total Sum of Squares (TSS):</strong> The
total variation of the response variable around its
mean.响应变量围绕其均值的总变异。</li>
<li><span class="math inline">\(||\hat{\mathbf{y}}||^2\)</span> is
proportional to the <strong>Explained Sum of Squares (ESS):</strong> The
portion of the total variation that is explained by your regression
model.回归模型可以解释的总变异部分。</li>
<li><span class="math inline">\(||\mathbf{y} -
\hat{\mathbf{y}}||^2\)</span> is the <strong>Residual Sum of Squares
(RSS):</strong> The portion of the total variation that is left
unexplained (the error).总变异中未解释的部分（即误差）。</li>
</ul></li>
</ul>
<p>This relationship, <strong>Total Variation = Explained Variation +
Unexplained Variation</strong>, is the foundation for calculating
metrics like <strong>R-squared (<span
class="math inline">\(R^2\)</span>)</strong>, which measures the
goodness of fit of your model. <strong>总变异 = 解释变异 +
未解释变异</strong>，是计算<strong>R 平方 (<span
class="math inline">\(R^2\)</span>)</strong>
等指标的基础，该指标用于衡量模型的拟合优度。</p>
<h4 id="residuals-and-the-identity-matrix-残差和单位矩阵">5. Residuals
and the Identity Matrix 残差和单位矩阵</h4>
<p>Finally, the second slide shows that just as <strong>H</strong>
projects onto the “model space,” a related matrix projects onto the
“error space.” 最后，第二张幻灯片显示，正如<strong>H</strong>
投影到“模型空间”一样，相关矩阵也会投影到“误差空间”。 *
<strong>Process:</strong> We can express the residuals using the hat
matrix: <span class="math display">\[\hat{\boldsymbol{\epsilon}} =
\mathbf{y} - \hat{\mathbf{y}} = \mathbf{y} - \mathbf{H}\mathbf{y} =
(\mathbf{I} - \mathbf{H})\mathbf{y}\]</span> The matrix <span
class="math inline">\((\mathbf{I} - \mathbf{H})\)</span> is also a
projection matrix. It takes the original data vector <span
class="math inline">\(\mathbf{y}\)</span> and projects it onto the space
that is orthogonal to all of your predictors, giving you the residual
vector directly.</p>
<h1
id="visualization-of-ordinary-least-squares-ols-regression">8.visualization
of Ordinary Least Squares (OLS) regression</h1>
<p><img src="/imgs/5054C3/visualization_of_Ordinary_Least_Squares_(OLS)_regression.png"></p>
<ul>
<li><strong>内容</strong>:</li>
</ul>
<p>This slide provides an excellent geometric visualization of what’s
happening “under the hood” in Ordinary Least Squares (OLS) regression.
It translates the algebraic formulas into a more intuitive spatial
concept. 这张幻灯片以出色的几何可视化方式展现了普通最小二乘 (OLS)
回归的“幕后”机制。它将代数公式转化为更直观的空间概念。</p>
<h3 id="summary-1">## Summary</h3>
<p>The image shows that the process of finding the least squares
estimates is geometrically equivalent to taking the <strong>actual
outcome vector</strong> (<span
class="math inline">\(\mathbf{y}\)</span>) and finding its
<strong>orthogonal projection</strong> (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>) onto a
<strong>hyperplane</strong> formed by the predictor variables (<span
class="math inline">\(\mathbf{x}_1\)</span> and <span
class="math inline">\(\mathbf{x}_2\)</span>). The projection <span
class="math inline">\(\hat{\mathbf{y}}\)</span> is the vector of fitted
values, representing the closest possible approximation of the real data
that the model can achieve.</p>
<p>该图显示，寻找最小二乘估计值的过程在几何上等同于将<strong>实际结果向量</strong>
(<span class="math inline">\(\mathbf{y}\)</span>)
求出其<strong>正交投影</strong> (<span
class="math inline">\(\hat{\mathbf{y}}\)</span>) 到由预测变量 (<span
class="math inline">\(\mathbf{x}_1\)</span> 和 <span
class="math inline">\(\mathbf{x}_2\)</span>
构成的<strong>超平面</strong>上。投影 <span
class="math inline">\(\hat{\mathbf{y}}\)</span>
是拟合值的向量，表示模型能够达到的与真实数据最接近的近似值。</p>
<h3 id="the-concepts-explained-spatially空间概念解释">## The Concepts
Explained Spatially空间概念解释</h3>
<p>Let’s break down each element of the diagram and its meaning:</p>
<h4 id="the-space-itself-空间本身">1. The Space Itself 空间本身</h4>
<ul>
<li><strong>Concept:</strong> We are not in a simple 2D or 3D graph
where axes are X and Y. Instead, we are in an <strong>n-dimensional
space</strong>, where <strong>n is the number of observations</strong>
in your dataset. Each axis in this space corresponds to one observation
(e.g., one person, one day).
我们并非身处一个简单的二维或三维图形中，其中坐标轴为 X 和
Y。相反，我们身处一个 <strong>n 维空间</strong>，其中 <strong>n
是数据集中的观测值数量</strong>。此空间中的每个轴对应一个观测值（例如，一个人，一天）。</li>
<li><strong>Meaning:</strong> A vector like <strong>y</strong> or
<strong>x₁</strong> is a single point in this high-dimensional space.
For example, if you have 50 data points, <strong>y</strong> is a vector
pointing to a specific location in a 50-dimensional space. 像
<strong>y</strong> 或 <strong>x₁</strong>
这样的向量是这个高维空间中的单个点。例如，如果您有 50
个数据点，<strong>y</strong> 就是指向 50 维空间中特定位置的向量。</li>
</ul>
<h4
id="the-predictor-hyperplane-the-yellow-surface预测变量超平面黄色表面">2.
The Predictor Hyperplane (The Yellow
Surface)预测变量超平面（黄色表面）</h4>
<ul>
<li><strong>Concept:</strong> The vectors for your predictor variables,
<strong>x₁</strong> and <strong>x₂</strong>, define a flat surface. If
you had only one predictor, this would be a line. With two, it’s a
plane. With more, it’s a <strong>hyperplane</strong>.预测变量的向量
<strong>x₁</strong> 和 <strong>x₂</strong>
定义了一个平面。如果只有一个预测变量，它就是一条线。如果有两个，它就是一个平面。如果有更多的预测变量，它就是一个<strong>超平面</strong>。</li>
<li><strong>Meaning:</strong> This yellow plane represents the
<strong>“world of possible predictions”</strong> that your model is
allowed to make. Any linear combination of your predictors—which is what
a linear regression model calculates—will result in a vector that lies
<em>somewhere</em> on this surface.
这个黄色平面代表你的模型可以做出的<strong>“可能预测的世界”</strong>。任何预测变量的线性组合（也就是线性回归模型计算的结果）都会产生一个位于这个平面<em>某处</em>的向量。
#### 3. The Actual Outcome Vector (y)实际结果向量 (y)</li>
<li><strong>Concept:</strong> The red vector <strong>y</strong>
represents your actual, observed data. It’s a single point in the
n-dimensional space. 红色向量 <strong>y</strong>
代表你实际观察到的数据。它是 n 维空间中的一个点。</li>
<li><strong>Meaning:</strong> Critically, this vector usually does
<strong>not</strong> lie on the predictor hyperplane. If it did, your
model would be a perfect fit with zero error. The fact that it’s “off
the plane” represents the real-world noise and variation that the model
cannot fully capture.
至关重要的是，这个向量通常<strong>不</strong>位于预测变量超平面上。如果它位于超平面上，你的模型将完美拟合，误差为零。它“偏离平面”代表了模型无法完全捕捉到的真实世界的噪声和变化。</li>
</ul>
<h4 id="the-fitted-value-vector-ŷ拟合值向量-ŷ">4. The Fitted Value
Vector (ŷ)拟合值向量 (ŷ)</h4>
<ul>
<li><strong>Concept:</strong> Since <strong>y</strong> is not on the
plane, we find the point on the plane that is <strong>geometrically
closest</strong> to <strong>y</strong>. This closest point is found by
dropping a perpendicular line from <strong>y</strong> to the plane. The
point where it lands is the <strong>orthogonal projection</strong>,
labeled <strong>ŷ</strong> (y-hat). 由于 <strong>y</strong>
不在平面上，因此我们在平面上找到与 <strong>y</strong>
<strong>几何上最接近的点。这个最接近点是通过从 </strong>y**
到平面做一条垂直线找到的。垂直线所在的点就是<strong>正交投影</strong>，标记为
<strong>ŷ</strong> (y-hat)。</li>
<li><strong>Meaning:</strong> <strong>ŷ is the vector of your model’s
fitted values.</strong> It is the “best” possible approximation of the
real data that can be created using the given predictors because it
minimizes the distance (and therefore the squared error) between the
actual data (<strong>y</strong>) and the model’s prediction. <strong>ŷ
是模型拟合值的向量。</strong>它是使用给定预测变量可以创建的对真实数据的“最佳”近似值，因为它最小化了实际数据
(<strong>y</strong>)
与模型预测值之间的距离（从而最小化了平方误差）。</li>
</ul>
<h4 id="the-residual-vector-the-dashed-line残差向量虚线">5. The Residual
Vector (The Dashed Line)残差向量（虚线）</h4>
<ul>
<li><strong>Concept:</strong> The dashed line connecting the tip of
<strong>y</strong> to the tip of <strong>ŷ</strong> is the
<strong>residual vector</strong> (<span
class="math inline">\(\boldsymbol{\epsilon} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>). Its length is the shortest possible distance
from <strong>y</strong> to the hyperplane.
连接<strong>y</strong>顶点和<strong>ŷ</strong>顶点的虚线是<strong>残差向量</strong>
(<span class="math inline">\(\boldsymbol{\epsilon} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>)。其长度是从<strong>y</strong>到超平面的最短可能距离。</li>
<li><strong>Meaning:</strong> This vector represents the
<strong>error</strong> of the model—the part of the actual data that is
left over after accounting for the predictors. The right-angle symbol
(└) is the most important part of the diagram, as it shows this error is
<strong>orthogonal</strong> (perpendicular) to the prediction and to all
the predictors. This visualizes the core property that the model’s
errors are uncorrelated with the predictors.
连接<strong>y</strong>顶点和<strong>ŷ</strong>顶点的虚线是<strong>残差向量</strong>
(<span class="math inline">\(\boldsymbol{\epsilon} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>)。其长度是从<strong>y</strong>到超平面的最短可能距离。</li>
</ul>
<h1 id="singular-value-decomposition-svd-奇异值分解-svd">9.Singular
Value Decomposition (SVD) 奇异值分解 (SVD)</h1>
<p><img src="/imgs/5054C3/SVD1.png">
<img src="/imgs/5054C3/SVD2.png"></p>
<ul>
<li><strong>内容</strong>:</li>
</ul>
<p>These slides delve into the more advanced linear algebra behind the
projection matrix (<strong>H</strong>), explaining its fundamental
properties and offering a new way to construct it using <strong>Singular
Value Decomposition (SVD)</strong>. 探讨了投影矩阵 (<strong>H</strong>)
背后更高级的线性代数，解释了它的基本性质，并提供了一种使用<strong>奇异值分解
(SVD)</strong> 构造它的新方法。</p>
<h3 id="summary-2">## Summary</h3>
<p>These slides show that the <strong>projection matrix (H)</strong>,
which is central to least squares, has two key mathematical properties:
it’s <strong>symmetric</strong> and <strong>idempotent</strong>
(projecting twice is the same as projecting once). These properties
dictate that its eigenvalues must be either 1 or 0. Singular Value
Decomposition (SVD) of the data matrix <strong>X</strong> provides an
elegant and numerically stable way to express <strong>H</strong> as
<strong>UUᵀ</strong>, which makes these fundamental properties easier to
understand and prove. 这些幻灯片展示了<strong>投影矩阵
(H)</strong>（最小二乘法的核心）的两个关键数学性质：<strong>对称</strong>和<strong>幂等</strong>（投影两次等于投影一次）。这些性质决定了它的特征值必须为
1 或 0。数据矩阵 <strong>X</strong> 的奇异值分解 (SVD)
提供了一种优雅且数值稳定的方式，将<strong>H</strong> 表示为
<strong>UUᵀ</strong>，这使得这些基本性质更容易理解和证明。</p>
<h3 id="concepts-and-process-explained-in-detail-1">## Concepts and
Process Explained in Detail</h3>
<h4 id="singular-value-decomposition-svd">1. Singular Value
Decomposition (SVD)</h4>
<p>The first slide introduces SVD, a powerful method for factoring any
matrix.</p>
<ul>
<li><strong>Concept:</strong> SVD breaks down your data matrix
<strong>X</strong> into three simpler matrices: <strong>X =
UDVᵀ</strong>. Think of this as revealing the fundamental structure of
your data.SVD 将数据矩阵 <strong>X</strong>
分解为三个更简单的矩阵：<strong>X =
UDVᵀ</strong>。这可以理解为揭示数据的基本结构。
<ul>
<li><strong>U:</strong> An <strong>orthogonal matrix</strong> whose
columns form a perfect, orthonormal basis for the space spanned by your
predictors (the column space of <strong>X</strong>). These columns
represent the principal directions of your data’s
space.一个<strong>正交矩阵</strong>，其列构成预测变量所占空间（<strong>X</strong>
的列空间）的完美正交基。这些列代表数据空间的主方向。</li>
<li><strong>D:</strong> A <strong>diagonal matrix</strong> containing
the “singular values,” which measure the importance or magnitude of each
of these principal
directions.一个<strong>对角矩阵</strong>，包含“奇异值”，用于衡量每个主方向的重要性或大小。</li>
<li><strong>V:</strong> Another <strong>orthogonal
matrix</strong>.另一个<strong>正交矩阵</strong>。</li>
</ul></li>
<li><strong>Process (How SVD simplifies the Projection Matrix) SVD
如何简化投影矩阵:</strong> The main takeaway from this slide is the new,
simpler formula for the hat matrix: <span
class="math display">\[\mathbf{H} = \mathbf{UU}^T\]</span> This result
is derived by substituting <strong>X = UDVᵀ</strong> into the original,
more complex formula for <strong>H</strong>: <span
class="math display">\[\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\]</span> When you
perform this substitution and use the fact that for orthogonal matrices
<strong>U</strong> and <strong>V</strong>, we have <strong>UᵀU =
I</strong> and <strong>VᵀV = I</strong>, the <strong>D</strong> and
<strong>V</strong> matrices completely cancel out, leaving the
beautifully simple form <strong>H = UUᵀ</strong>. This tells us that
projection is fundamentally about the basis vectors (<strong>U</strong>)
of the predictor space. 执行此代入并利用正交矩阵 <strong>U</strong> 和
<strong>V</strong> 的公式，即 <strong>UᵀU = I</strong> 和 <strong>VᵀV =
I</strong>，<strong>D</strong> 和 <strong>V</strong>
矩阵完全抵消，剩下简洁的形式 <strong>H =
UUᵀ</strong>。这告诉我们，投影本质上是关于预测空间的基向量（<strong>U</strong>）的。</li>
</ul>
<h4 id="the-properties-of-the-projection-matrix-h-投影矩阵-h-的性质">2.
The Properties of the Projection Matrix (H) 投影矩阵 (H) 的性质</h4>
<p>The second slide describes the essential nature of any projection
matrix.</p>
<ul>
<li><p><strong>Symmetric (H = Hᵀ):</strong> This property ensures that
the projection is orthogonal (i.e., it finds the closest point by moving
perpendicularly).
此性质确保投影是正交的（即，它通过垂直移动找到最近的点）。</p></li>
<li><p><strong>Idempotent (H² = H):</strong> This is the most intuitive
property of a projection. 这是投影最直观的性质。</p>
<ul>
<li><strong>Concept:</strong> “Doing it twice is the same as doing it
once.” “两次和一次相同。”</li>
<li><strong>Geometric Meaning:</strong> Imagine you project a point onto
a flat tabletop. That projected point is now <em>on the table</em>. If
you try to project it onto the table <em>again</em>, it doesn’t move.
The projection of a projection is just the projection itself.
Mathematically, this is <strong>H(Hv) = Hv</strong>, which simplifies to
<strong>H² = H</strong>.
想象一下，你将一个点投影到平坦的桌面上。这个投影点现在<em>在桌子上</em>。如果你尝试<em>再次</em>将它投影到桌子上，它不会移动。投影的投影就是投影本身。从数学上讲，这是<strong>H(Hv)
= Hv</strong>，简化为<strong>H² = H</strong>。</li>
</ul></li>
</ul>
<h4 id="eigenvalues-and-eigenspaces-特征值和特征空间">3. Eigenvalues and
Eigenspaces 特征值和特征空间</h4>
<p>The idempotency property has a profound consequence for the matrix’s
eigenvalues.</p>
<ul>
<li><strong>Concept:</strong> The eigenvalues of <strong>H</strong> can
only be <strong>1 or 0</strong>.
<strong>H</strong>的特征值只能是<strong>1</strong>或0**。</li>
<li><strong>Process (The Proof):</strong>
<ol type="1">
<li>Let <strong>v</strong> be an eigenvector of <strong>H</strong> with
eigenvalue <span class="math inline">\(\lambda\)</span>. By definition,
<strong>Hv</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>.
设<strong>v</strong>是<strong>H</strong>的特征向量，其特征值为<span
class="math inline">\(\lambda\)</span>。根据定义，<strong>Hv</strong> =
<span class="math inline">\(\lambda\)</span><strong>v</strong>。</li>
<li>If we apply <strong>H</strong> again, we get <strong>H(Hv)</strong>
= <strong>H</strong>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda\)</span>(<strong>Hv</strong>) = <span
class="math inline">\(\lambda\)</span>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>.
如果我们再次应用<strong>H</strong>，我们得到<strong>H(Hv)</strong> =
<strong>H</strong>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda\)</span>(<strong>Hv</strong>) = <span
class="math inline">\(\lambda\)</span>(<span
class="math inline">\(\lambda\)</span><strong>v</strong>) = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>。</li>
<li>So, we have <strong>H²v</strong> = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>.
因此，我们有<strong>H²v</strong> = <span
class="math inline">\(\lambda^2\)</span><strong>v</strong>。</li>
<li>But since <strong>H</strong> is idempotent, <strong>H² = H</strong>,
which means <strong>H²v = Hv</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>.
但由于<strong>H</strong>是幂等的，<strong>H² =
H</strong>，这意味着<strong>H²v = Hv</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>。</li>
<li>Therefore, we must have <span
class="math inline">\(\lambda^2\)</span><strong>v</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>, which means
<span class="math inline">\(\lambda^2 = \lambda\)</span>. The only two
numbers in existence that satisfy this equation are <strong>0 and
1</strong>. 因此，我们必须有<span
class="math inline">\(\lambda^2\)</span><strong>v</strong> = <span
class="math inline">\(\lambda\)</span><strong>v</strong>，这意味着<span
class="math inline">\(\lambda^2 =
\lambda\)</span>。满足此等式的仅有两个数字是<strong>0</strong>和<strong>1</strong>。</li>
</ol></li>
<li><strong>Connecting Eigenvalues to the Model
将特征值连接到模型:</strong>
<ul>
<li><p><strong>Eigenvalue = 1:</strong> The eigenvectors associated with
an eigenvalue of 1 are the vectors that <strong>do not change</strong>
when projected. This is only possible if they were already in the space
being projected onto. Therefore, the space <code>L₁</code> is the
<strong>column space of X</strong>—the “model space.” <strong>H</strong>
is the projection onto this space. <strong>与特征值为 1
相关联的特征向量是投影时</strong>不会改变<strong>的向量。只有当它们已经存在于投影到的空间中时，这种情况才有可能发生。因此，空间
<code>L₁</code> 是 X 的</strong>列空间<strong>——“模型空间”。</strong>H**
是到该空间的投影。</p></li>
<li><p><strong>Eigenvalue = 0:</strong> The eigenvectors associated with
an eigenvalue of 0 are the vectors that get sent to the zero vector when
projected. This happens to vectors that are <strong>orthogonal</strong>
to the projection space. Therefore, the space <code>L₀</code> is the
<strong>orthogonal “error” space</strong>. The matrix <strong>I -
H</strong> is the projection onto this space.</p></li>
</ul>
<strong>与特征值为 0
相关联的特征向量是投影时被发送到零向量的向量。这种情况发生在与投影空间</strong>正交<strong>的向量上。因此，空间
<code>L₀</code> 是</strong>正交“误差”空间<strong>。矩阵 </strong>I - H**
是到该空间的投影。</li>
</ul>
<h1 id="statistical-inference-1">10.statistical inference</h1>
<p><img src="/imgs/5054C3/statistical_inference_in_linear_regression1.png">
<img src="/imgs/5054C3/statistical_inference_in_linear_regression2.png"></p>
<ul>
<li><strong>内容</strong>:</li>
</ul>
<p>These slides cover the theoretical backbone of statistical inference
in linear regression. They explain the necessary assumptions and the
resulting probability distributions of our estimates, which is what
allows us to perform hypothesis tests and create confidence
intervals.</p>
<p>这些幻灯片涵盖了线性回归中统计推断的理论基础。它们解释了必要的假设以及由此得出的估计概率分布，这使我们能够进行假设检验并创建置信区间。</p>
<h3 id="summary-3">## Summary</h3>
<p>These slides lay out the statistical assumptions required for the
Least Squares Estimator (LSE). The core idea is that if we assume the
errors are independent and normally distributed, we can then prove that:
这些幻灯片列出了最小二乘估计量 (LSE)
所需的统计假设。其核心思想是，如果我们假设误差是独立的且服从正态分布，那么我们可以证明：</p>
<ol type="1">
<li><p>Our estimated coefficients (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) also follow a
<strong>Normal distribution</strong> (or a
<strong>t-distribution</strong> when standardized). 我们的估计系数
(<span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>)
也服从<strong>正态分布</strong>（标准化后服从<strong>t
分布</strong>）。</p></li>
<li><p>Our summed-up squared errors (RSS) follow a <strong>Chi-squared
distribution</strong>. 我们的平方误差总和 (RSS)
服从<strong>卡方分布</strong>。</p></li>
<li><p>A specific ratio of the explained variance to the unexplained
variance follows an <strong>F-distribution</strong>, which is used to
test the overall significance of the model.
解释方差与未解释方差的特定比率服从<strong>F
分布</strong>，该分布用于检验模型的整体显著性。</p></li>
</ol>
<p>These known distributions are the foundation for all statistical
inference in linear
models.这些已知的分布是线性模型中所有统计推断的基础。</p>
<h3 id="deeper-dive-into-concepts-and-processes">## Deeper Dive into
Concepts and Processes</h3>
<h4 id="the-model-assumptions-the-foundation-模型假设基础">1. The Model
Assumptions (The Foundation) 模型假设（基础）</h4>
<p>The first slide states the two assumptions that are critical for
everything that follows. Without them, we can’t make claims about the
statistical properties of our estimates.
第一张幻灯片阐述了对后续所有内容都至关重要的两个假设。没有它们，我们就无法断言估计值的统计特性。</p>
<ul>
<li><strong>Assumption 1: <span class="math inline">\(\epsilon_i \sim
N(0, \sigma^2)\)</span></strong>
<ul>
<li><strong>Concept:</strong> This assumes that the error terms (the
part of <code>y</code> that the model can’t explain) are drawn from a
normal (bell-curve) distribution with a mean of zero and a constant
variance <span class="math inline">\(\sigma^2\)</span>.
**假设误差项（模型无法解释的 y
值部分）服从正态（钟形曲线）分布，该分布的均值为零，方差为常数 <span
class="math inline">\(\sigma^2\)</span>。</li>
<li><strong>Meaning in Plain English:</strong>
<ul>
<li><strong>Mean of 0:</strong> The model is “correct on average.” The
errors are not systematically positive or negative.
**模型“平均正确”。误差并非系统地为正或负。</li>
<li><strong>Normal Distribution:</strong> Small errors are more likely
than large errors. This is a common assumption for random noise.
**小误差比大误差更有可能出现。这是随机噪声的常见假设。</li>
<li><strong>Constant Variance (<span
class="math inline">\(\sigma^2\)</span>):</strong> The amount of random
scatter around the regression line is the same at all levels of the
predictor variables. This is called <strong>homoscedasticity</strong>.
回归线周围的随机散度在预测变量的各个水平上都是相同的。这被称为<strong>同方差性</strong>。</li>
</ul></li>
</ul></li>
<li><strong>Assumption 2: Observations are independent</strong>
观测值是独立的**
<ul>
<li><strong>Concept:</strong> Each data point <span
class="math inline">\((x_i, y_i)\)</span> is an independent piece of
information. The value of the error for one observation gives no
information about the error for another observation. 每个数据点 <span
class="math inline">\((x_i, y_i)\)</span>
都是一条独立的信息。一个观测值的误差值并不能反映另一个观测值的误差。</li>
<li><strong>Meaning:</strong> This is often true for cross-sectional
data (e.g., a random sample of people) but can be violated in
time-series data where today’s error might be correlated with
yesterday’s.
这通常适用于横截面数据（例如，随机抽样的人群），但在时间序列数据中可能不成立，因为今天的误差可能与昨天的误差相关。</li>
</ul></li>
</ul>
<h4
id="the-distribution-of-the-coefficients-theorem-of-lse-系数分布最小二乘法定理">2.
The Distribution of the Coefficients (Theorem of LSE)
系数分布（最小二乘法定理）</h4>
<p>This is the most important result for understanding the accuracy of
our individual predictors.</p>
<ul>
<li><strong>Concept 1: The Sampling Distribution of <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></strong>
<ul>
<li><p><strong>Formula:</strong> <span
class="math inline">\(\hat{\boldsymbol{\beta}} \sim
N(\boldsymbol{\beta},
\sigma^2(\mathbf{X}^T\mathbf{X})^{-1})\)</span></p></li>
<li><p><strong>Meaning:</strong> If you were to take many different
random samples from the population and calculate the coefficients <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> for each sample,
the distribution of those coefficients would be a multivariate normal
distribution. **如果从总体中随机抽取许多不同的样本，并计算每个样本的系数
<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>，则这些系数的分布将服从多元正态分布。</p>
<ul>
<li>The center of this distribution is the <strong>true population
coefficient vector <span
class="math inline">\(\boldsymbol{\beta}\)</span></strong>. This means
our estimator is <strong>unbiased</strong>—on average, it finds the
right answer. 该分布的中心是<strong>真实的总体系数向量 <span
class="math inline">\(\boldsymbol{\beta}\)</span></strong>。这意味着我们的估计器是<strong>无偏的</strong>——平均而言，它能够找到正确的答案。</li>
<li>The “spread” of this distribution is its variance-covariance matrix,
<span
class="math inline">\(\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\)</span>.
This tells us the uncertainty in our estimates.
该分布的“散度”是其方差-协方差矩阵</li>
</ul></li>
</ul></li>
<li><strong>Concept 2: The t-statistic</strong> t 统计量
<ul>
<li><strong>Formula:</strong> The standardized coefficient, <span
class="math inline">\(\frac{\hat{\beta}_j -
\beta_j}{\text{s.e.}(\hat{\beta}_j)}\)</span>, follows a
<strong>t-distribution</strong> with <strong><span
class="math inline">\(n-p-1\)</span> degrees of freedom</strong>.</li>
<li><strong>Process &amp; Meaning:</strong> In the real world, we don’t
know the true error variance <span
class="math inline">\(\sigma^2\)</span>. We have to estimate it using
our sample data, which gives us <span
class="math inline">\(s^2\)</span>. Because we are using an
<em>estimate</em> of the variance, we introduce extra uncertainty. The
t-distribution is like a normal distribution but with slightly “fatter”
tails to account for this additional uncertainty. The degrees of
freedom, <span class="math inline">\(n-p-1\)</span>, reflect the number
of data points (<code>n</code>) minus the number of parameters we had to
estimate (<code>p</code> slopes + 1 intercept). This is the basis for
t-tests and confidence intervals for each coefficient.
在现实世界中，我们不知道真实的误差方差 <span
class="math inline">\(\sigma^2\)</span>。我们必须使用样本数据来估计它，从而得到
<span
class="math inline">\(s^2\)</span>。由于我们使用的是方差的<em>估计值</em>，因此引入了额外的不确定性。
t 分布类似于正态分布，但尾部略微“丰满”，以解释这种额外的不确定性。自由度
<span class="math inline">\(n-p-1\)</span>
表示数据点的数量（<code>n</code>）减去我们需要估计的参数数量（<code>p</code>
个斜率 + 1 个截距）。这是 t 检验和每个系数置信区间的基础。</li>
</ul></li>
</ul>
<h4
id="the-distribution-of-the-error-theorem-of-residual-误差分布残差定理">3.
The Distribution of the Error (Theorem of Residual)
误差分布（残差定理）</h4>
<p>This theorem helps us understand the properties of our model’s
overall error.</p>
<ul>
<li><p><strong>Concept:</strong> The <strong>Residual Sum of Squares
(RSS)</strong>, when scaled by the true variance, follows a
<strong>Chi-squared (<span class="math inline">\(\chi^2\)</span>)
distribution</strong> with <span class="math inline">\(n-p-1\)</span>
degrees of freedom. <strong>残差平方和 (RSS)</strong>
经真实方差缩放后，服从自由度为 <span
class="math inline">\(n-p-1\)</span> 的<strong>卡方 (<span
class="math inline">\(\chi^2\)</span>) 分布</strong>。</p></li>
<li><p><strong>Process &amp; Meaning:</strong> The Chi-squared
distribution often arises when dealing with sums of squared normal
variables. This theorem provides a formal probability distribution for
our total model error. Its most important consequence is that it allows
us to prove that:
**卡方分布通常用于处理正态变量的平方和。该定理为我们模型的总体误差提供了一个正式的概率分布。它最重要的推论是，它使我们能够证明：</p>
<p><span class="math display">\[s^2 = \text{RSS}/(n - p - 1)\]</span> is
an <strong>unbiased estimate</strong> of the true error variance <span
class="math inline">\(\sigma^2\)</span>. This <span
class="math inline">\(s^2\)</span> is a critical ingredient for
calculating the standard errors of our coefficients. <span
class="math display">\[s^2 = \text{RSS}/(n - p - 1)\]</span>
是真实误差方差 <span class="math inline">\(\sigma^2\)</span>
的<strong>无偏估计</strong>。这个 <span
class="math inline">\(s^2\)</span>
是计算系数标准误差的关键因素。</p></li>
</ul>
<h4 id="the-f-distribution-and-the-overall-model-test">4. The
F-Distribution and the Overall Model Test</h4>
<p>This final theorem combines our findings about the coefficients and
the residuals.</p>
<ul>
<li><p><strong>Concept:</strong> The F-statistic, which is essentially a
ratio of the variance explained by the model to the variance left
unexplained, follows an <strong>F-distribution</strong>. F
统计量本质上是模型解释的方差与未解释方差的比率，服从 F 分布。</p></li>
<li><p><strong>Process &amp; Meaning:</strong> This result relies on the
fact that our coefficient estimates (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) are independent
of our total error (RSS). The F-distribution is used for the
<strong>F-test of overall significance</strong>. This test checks the
null hypothesis that <em>all</em> of your slope coefficients are
simultaneously zero (<span class="math inline">\(\beta_1 = \beta_2 =
\dots = \beta_p = 0\)</span>). If the F-test gives a small p-value, you
can conclude that your model, as a whole, is statistically significant
and provides a better fit than a model with no predictors. 如果 F
检验得出的 p
值较小，则可以得出结论，您的模型整体上具有统计显著性，并且比没有预测因子的模型拟合效果更好。</p></li>
</ul>
<h1 id="construct-different-types-of-intervals">11.construct different
types of intervals</h1>
<p><img src="/imgs/5054C3/construct_different_types_of_intervals1.png">
<img src="/imgs/5054C3/construct_different_types_of_intervals2.png"></p>
<ul>
<li><strong>内容</strong>:</li>
</ul>
<p>These slides explain how to use the statistical properties of the
least squares estimates to construct different types of intervals, which
are essential for quantifying the uncertainty in your model’s
predictions and parameters.
这些幻灯片解释了如何利用最小二乘估计的统计特性来构建不同类型的区间，这对于量化模型预测和参数中的不确定性至关重要。</p>
<h3 id="summary-4">Summary</h3>
<p>These slides show how to calculate three distinct types of intervals
in linear regression, each answering a different question about
uncertainty:
展示了如何计算线性回归中三种不同类型的区间，每种区间分别回答了关于不确定性的不同问题：</p>
<ol type="1">
<li><strong>Confidence Interval for a Parameter (<span
class="math inline">\(\beta_j\)</span>):</strong> Provides a plausible
range for a single, true unknown coefficient in the model.
为模型中单个真实未知系数提供一个合理的范围。</li>
<li><strong>Confidence Interval for the Mean Response:</strong> Provides
a plausible range for the <em>average</em> outcome for a given set of
predictor values.
为给定一组预测变量值的<em>平均</em>结果提供一个合理的范围。</li>
<li><strong>Prediction Interval:</strong> Provides a plausible range for
a <em>single future</em> outcome for a given set of predictor values.
This interval is always wider than the confidence interval for the mean
response because it must also account for individual random error.
为给定一组预测变量值的<em>单个未来</em>结果提供一个合理的范围。该区间始终比平均响应的置信区间更宽，因为它还必须考虑单个随机误差。</li>
</ol>
<h3 id="deeper-dive-into-concepts-and-processes-1">Deeper Dive into
Concepts and Processes</h3>
<h4
id="confidence-interval-for-a-single-parameter-单个参数的置信区间">1.
Confidence Interval for a Single Parameter 单个参数的置信区间</h4>
<p>This interval addresses the uncertainty around one specific
coefficient, like the slope for your most important predictor.
此区间用于解决围绕某个特定系数的不确定性，例如最重要的预测因子的斜率。</p>
<ul>
<li><strong>The Question It Answers:</strong> “I’ve calculated a slope
of <span class="math inline">\(\hat{\beta}_1 = 10.5\)</span>. How sure
am I about this number? What is a plausible range for the <em>true</em>
population slope?” 我计算出了斜率为 <span
class="math inline">\(\hat{\beta}_1 =
10.5\)</span>。我对这个数字有多确定？<em>真实</em>总体斜率的合理范围是多少？”</li>
<li><strong>The Formula:</strong> <span
class="math inline">\(\hat{\beta}_j \pm t_{n-p-1}(\alpha/2) s
\sqrt{c_{jj}}\)</span>
<ul>
<li><strong><span class="math inline">\(\hat{\beta}_j\)</span></strong>:
This is your best point estimate for the coefficient, taken directly
from the model output. 这是该系数的最佳点估计值，直接取自模型输出。</li>
<li><strong><span
class="math inline">\(t_{n-p-1}(\alpha/2)\)</span></strong>: This is the
<strong>critical value</strong> from a t-distribution. It’s a multiplier
that sets the width of the interval based on your desired confidence
level (e.g., for 95% confidence, <span
class="math inline">\(\alpha=0.05\)</span>). 这是 t
分布的<strong>临界值</strong>。它是一个乘数，根据您所需的置信水平设置区间宽度（例如，对于
95% 的置信度，<span class="math inline">\(\alpha=0.05\)</span>）。</li>
<li><strong><span class="math inline">\(s
\sqrt{c_{jj}}\)</span></strong>: This whole term is the <strong>standard
error</strong> of the coefficient <span
class="math inline">\(\hat{\beta}_j\)</span>. It measures the precision
of your estimate. A smaller standard error means a narrower, more
precise interval. 这整个项是系数 <span
class="math inline">\(\hat{\beta}_j\)</span>
的<strong>标准误差</strong>。它衡量您估计的精度。标准误差越小，区间越窄，精度越高。</li>
</ul></li>
</ul>
<h4 id="confidence-interval-for-the-mean-response-平均响应的置信区间">2.
Confidence Interval for the Mean Response 平均响应的置信区间</h4>
<p>This interval addresses the uncertainty about the location of the
regression line itself. 这个区间解决了回归线本身位置的不确定性。</p>
<ul>
<li><strong>The Question It Answers:</strong> “For a house with 3
bedrooms and 2 bathrooms, what is the plausible range for the
<em>average</em> selling price of <em>all such houses</em>?”
<strong>它回答的问题</strong>：“对于一栋有 3 间卧室和 2
间浴室的房子，<em>所有此类房屋</em>的<em>平均</em>售价的合理范围是多少？”</li>
<li><strong>The Formula:</strong> <span
class="math inline">\(\hat{\boldsymbol{\beta}}^T \mathbf{x} \pm
t_{n-p-1}(\alpha/2)s\sqrt{\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}}\)</span>
<ul>
<li><strong><span class="math inline">\(\hat{\boldsymbol{\beta}}^T
\mathbf{x}\)</span></strong>: This is your point prediction, <span
class="math inline">\(\hat{y}\)</span>, for the given input vector
<strong>x</strong>. 这是给定输入向量 <strong>x</strong> 的点预测 <span
class="math inline">\(\hat{y}\)</span>。</li>
<li><strong><span
class="math inline">\(s\sqrt{\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}}\)</span></strong>:
This is the standard error of the mean response. Its value depends on
how far the input vector <strong>x</strong> is from the center of the
data. This means the confidence interval is narrowest near the average
of your data and gets wider as you move toward the extremes.
这是平均响应的标准误差。其值取决于输入向量 <strong>x</strong>
距离数据中心的距离。这意味着置信区间在数据平均值附近最窄，并且随着接近极值而变宽。</li>
</ul></li>
</ul>
<h4
id="prediction-interval-for-an-individual-response-单个响应的预测区间">3.
Prediction Interval for an Individual Response 单个响应的预测区间</h4>
<p>This is the most comprehensive interval and is often the most useful
for making real-world predictions.
这是最全面的区间，通常对于进行实际预测最有用。</p>
<ul>
<li><strong>The Question It Answers:</strong> “I want to predict the
selling price for <em>one specific house</em> that has 3 bedrooms and 2
bathrooms. What is a plausible price range for this <em>single
house</em>?”</li>
<li><strong>The Formula:</strong> <span
class="math inline">\(\hat{\boldsymbol{\beta}}^T \mathbf{x} \pm
t_{n-p-1}(\alpha/2)s\sqrt{1 +
\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}}\)</span></li>
<li><strong>The Key Difference:</strong> Notice the formula is identical
to the one above, except for the <strong><code>1 + ...</code></strong>
inside the square root. This “1” is critically important. It accounts
for the second source of uncertainty.
<strong>请注意，该公式与上面的公式完全相同，只是平方根中的</strong><code>1 + ...</code>**不同。这个“1”至关重要。它解释了第二个不确定性来源。
<ol type="1">
<li><strong>Uncertainty in the model:</strong> We are not perfectly
certain about the true location of the regression line. This is captured
by the <span
class="math inline">\(\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}\)</span>
term. **我们无法完全确定回归线的真实位置。这可以通过 <span
class="math inline">\(\mathbf{x}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}\)</span>
项来捕捉。</li>
<li><strong>Uncertainty in the individual data point:</strong> Even if
we knew the true regression line perfectly, individual data points would
still be scattered around it due to random error (<span
class="math inline">\(\epsilon\)</span>). The “1” in the formula
accounts for this irreducible, random error of a single observation.
即使我们完全了解真实的回归线，由于随机误差 (<span
class="math inline">\(\epsilon\)</span>)，单个数据点仍然会散布在其周围。公式中的“1”解释了单个观测值中这种不可约的随机误差。</li>
</ol></li>
</ul>
<p>Because it accounts for both types of uncertainty, the
<strong>prediction interval is always wider than the confidence interval
for the mean</strong>.
由于它同时考虑了两种不确定性，因此<strong>预测区间</strong>总是比均值的置信区间更宽。</p>
<h4 id="the-core-difference-an-analogy-一个类比">The Core Difference: An
Analogy 一个类比</h4>
<ul>
<li><p><strong>Confidence Interval (Mean) 均值置信区间:</strong> Like
predicting the <strong>average</strong> arrival time for a specific
flight that runs every day. After observing it for a year, you can
predict the average very accurately (e.g., 10:05 AM ± 2 minutes).
就像预测每天特定航班的<strong>平均</strong>到达时间。经过一年的观察，您可以非常准确地预测平均值（例如，上午
10:05 ± 2 分钟）。</p></li>
<li><p><strong>Prediction Interval (Individual) 个体预测区间:</strong>
Like predicting the arrival time for that same flight on <strong>one
specific day</strong> next week. You have to account for the uncertainty
in the average <em>plus</em> the potential for random, one-time events
like weather or air traffic delays. Your prediction must be wider to be
safe (e.g., 10:05 AM ± 15 minutes).
就像预测同一航班下周<strong>某一天</strong>的到达时间。您必须考虑平均值的不确定性，以及*可能出现的随机、一次性事件，例如天气或空中交通延误。您的预测范围必须更广才能安全（例如，上午
10:05 ± 15 分钟）。</p></li>
</ul>
<h1 id="construct-different-types-of-intervals-1">12.construct different
types of intervals</h1>
<p><img src="/imgs/5054C3/ANOVA1.png">
<img src="/imgs/5054C3/ANOVA2.png"></p>
<ul>
<li><strong>内容</strong>:</li>
</ul>
<p>These slides explain <strong>Analysis of Variance (ANOVA)</strong>, a
method used in regression to break down the total variability in your
data to test if your model is statistically significant as a
whole.这些幻灯片讲解了<strong>方差分析
(ANOVA)</strong>，这是一种用于回归分析的方法，用于分解数据中的总变异性，以检验模型整体是否具有统计显著性。</p>
<h3 id="summary-5">Summary</h3>
<p>The core idea is to decompose the total variation in the response
variable (<strong>Total Sum of Squares, SS_total</strong>) into two
parts: the variation that is explained by your regression model
(<strong>Regression Sum of Squares, SS_reg</strong>) and the variation
that is left unexplained (<strong>Error Sum of Squares,
SS_error</strong>).
其核心思想是将响应变量的总变异（<strong>总平方和，SS_total</strong>）分解为两部分：回归模型可以解释的变异（<strong>回归平方和，SS_reg</strong>）和未解释的变异（<strong>误差平方和，SS_error</strong>）。</p>
<p>By comparing the size of the explained variation to the unexplained
variation using an <strong>F-statistic</strong>, we can formally test
the hypothesis that our model has predictive power. This entire process
is neatly organized in an <strong>ANOVA table</strong>.
通过使用<strong>F
统计量</strong>比较已解释变异与未解释变异的大小，我们可以正式检验模型具有预测能力的假设。整个过程都整齐地组织在<strong>方差分析表</strong>中。</p>
<h3 id="deeper-dive-into-concepts-and-connections">Deeper Dive into
Concepts and Connections</h3>
<h4
id="the-decomposition-of-variances-the-core-equation-方差分解核心方程">1.
The Decomposition of Variances (The Core Equation)
方差分解（核心方程）</h4>
<p>The first slide starts with the fundamental equation of ANOVA, which
stems directly from the geometric properties of least squares:
第一张幻灯片以方差分析的基本方程开头，该方程直接源于最小二乘的几何性质：</p>
<p><span class="math display">\[SS_{total} = SS_{reg} +
SS_{error}\]</span></p>
<ul>
<li><strong>SS_total (Total Sum of Squares):</strong> <span
class="math inline">\(\sum(y_i - \bar{y})^2\)</span>
<ul>
<li><strong>Concept:</strong> This measures the <strong>total
variation</strong> in your response variable, <code>y</code>. Imagine
you didn’t have a model and your only prediction for any <code>y</code>
was its overall average, <code>ȳ</code>. SS_total is the total squared
error of this simple “mean-only” model. It represents the total amount
of variation you are trying to explain.
这测量的是响应变量“y”的<strong>总变异</strong>。假设你没有模型，你对任何“y”的唯一预测是它的整体平均值“ȳ”。SS_total
是这个简单的“仅均值”模型的总平方误差。它代表了你试图解释的变异总量。</li>
</ul></li>
<li><strong>SS_reg (Regression Sum of Squares):</strong> <span
class="math inline">\(\sum(\hat{y}_i - \bar{y})^2\)</span>
<ul>
<li><strong>Concept:</strong> This measures the <strong>explained
variation</strong>. It’s the amount of variation in <code>y</code> that
is captured by your regression model. It calculates the difference
between your model’s predictions (<code>ŷ</code>) and the simple average
(<code>ȳ</code>). A large SS_reg means your model’s predictions are a
big improvement over just guessing the average.
<strong>它衡量</strong>解释变异**。它是回归模型捕捉到的 y
的变异量。它计算模型预测值（“ŷ”）与简单平均值（“ȳ”）之间的差异。较大的
SS_reg 意味着模型的预测结果比仅仅猜测平均值有显著改善。</li>
</ul></li>
<li><strong>SS_error (Error Sum of Squares):</strong> <span
class="math inline">\(\sum(y_i - \hat{y}_i)^2\)</span>
<ul>
<li><strong>Concept:</strong> This measures the <strong>unexplained
variation</strong> (also called the Residual Sum of Squares). It’s the
amount of variation your model <em>failed</em> to capture. It’s the sum
of the squared differences between the actual data (<code>y</code>) and
your model’s predictions (<code>ŷ</code>).
<strong>它衡量</strong>未解释变异**（也称为残差平方和）。它是模型<em>未能</em>捕捉到的变异量。它是实际数据
(<code>y</code>) 与模型预测值 (<code>ŷ</code>) 之间平方差之和。</li>
</ul></li>
</ul>
<p>The <strong>R-squared</strong> value is a direct consequence of this
decomposition. It’s the proportion of the total variance that is
explained by the model: <strong>R 平方</strong>
值是这种分解的直接结果。它是模型解释的总方差的比例：</p>
<p><span class="math display">\[R^2 =
\frac{SS_{reg}}{SS_{total}}\]</span></p>
<h4 id="the-anova-table-and-the-f-test">2. The ANOVA Table and the
F-test</h4>
<p>方差分析表和 F 检验 The second slide organizes these sums of squares
to perform a formal hypothesis test.
第二张幻灯片整理了这些平方和，以进行正式的假设检验。</p>
<ul>
<li><strong>The Question:</strong> “Is there <em>any</em> relationship
between my set of predictors and the response variable?” or “Is my model
better than nothing?”
“我的预测变量集和响应变量之间是否存在<em>任何</em>关系？”或“我的模型比没有模型好吗？”</li>
<li><strong>The Hypotheses:</strong>
<ul>
<li><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>):</strong> <span
class="math inline">\(\beta_1 = \beta_2 = \dots = \beta_p = 0\)</span>.
(None of the predictors have a relationship with the response; the model
is useless). <strong>零假设 (<span
class="math inline">\(H_0\)</span>)</strong>：<span
class="math inline">\(\beta_1 = \beta_2 = \dots = \beta_p = 0\)</span>。
（所有预测变量都与响应变量无关；该模型毫无用处）。</li>
<li><strong>Alternative Hypothesis (<span
class="math inline">\(H_1\)</span>):</strong> At least one <span
class="math inline">\(\beta_j\)</span> is not zero. (The model has some
predictive value). <strong>备择假设 (<span
class="math inline">\(H_1\)</span>)：</strong>至少有一个 <span
class="math inline">\(\beta_j\)</span>
不为零。（该模型具有一定的预测值）。</li>
</ul></li>
</ul>
<p>To test this, we can’t just compare the raw SS values, because they
depend on the number of data points and predictors. We need to normalize
them. 为了验证这一点，我们不能仅仅比较原始的 SS
值，因为它们取决于数据点和预测变量的数量。我们需要对它们进行归一化。</p>
<ul>
<li><strong>Mean Squares (MS):</strong> This is the “average” variation.
We calculate it by dividing the Sum of Squares by its <strong>degrees of
freedom (df)</strong>.
<strong>这是“平均”变异。我们通过将平方和除以其</strong>自由度 (df)**
来计算它。
<ul>
<li><strong>MS_reg</strong> = <span class="math inline">\(SS_{reg} /
p\)</span>. This is the average explained variation <em>per
predictor</em>. 这是<em>每个预测变量</em>的平均解释变异。</li>
<li><strong>MS_error</strong> = <span class="math inline">\(SS_{error} /
(n - p - 1)\)</span>. This is the average unexplained variation, which
is our estimate of the error variance, <span
class="math inline">\(s^2\)</span>. 这是平均未解释变异，即我们对误差方差
<span class="math inline">\(s^2\)</span> 的估计值。</li>
</ul></li>
</ul>
<h4 id="the-connection-the-f-statistic-联系f-统计量">3. The Connection:
The F-statistic 联系：F 统计量</h4>
<p>The <strong>F-statistic</strong> is the key that connects everything.
It’s the ratio of the two mean squares: <strong>F
统计量</strong>是连接一切的关键。它是两个均方的比值： <span
class="math display">\[F = \frac{\text{Mean Squared
Regression}}{\text{Mean Squared Error}} =
\frac{MS_{reg}}{MS_{error}}\]</span></p>
<ul>
<li><strong>Intuitive Meaning:</strong> The F-statistic is a ratio of
the <strong>average explained variation</strong> to the <strong>average
unexplained variation</strong>. F
统计量是<strong>平均解释变异</strong>与<strong>平均未解释变异</strong>的比值。
<ul>
<li>If your model is useless (<span class="math inline">\(H_0\)</span>
is true), the explained variation should be about the same as the
random, unexplained variation. The F-statistic will be close to 1.
如果你的模型无效（H_0$
为真），则解释变异应该与随机的未解释变异大致相同。F 统计量接近 1。</li>
<li>If your model is useful (<span class="math inline">\(H_1\)</span> is
true), the explained variation should be significantly larger than the
unexplained variation. The F-statistic will be much greater than 1.
如果你的模型有效（H_1$ 为真），则解释变异应该显著大于未解释变异。 F
统计量将远大于 1。</li>
</ul></li>
</ul>
<p>We compare our calculated F-statistic to an
<strong>F-distribution</strong> to get a <strong>p-value</strong>. A
small p-value (&lt; 0.05) provides strong evidence to reject the null
hypothesis and conclude that your model, as a whole, is statistically
significant. 我们将计算出的 F 统计量与<strong>F
分布</strong>进行比较，得出<strong>p 值</strong>。较小的 p 值（&lt;
0.05）可以提供强有力的证据来拒绝零假设，并得出您的模型整体具有统计显著性的结论。</p>
<h1 id="construct-different-types-of-intervals-2">12.construct different
types of intervals</h1>
<p><img src="/imgs/5054C3/Gauss_Markov1.png">
<img src="/imgs/5054C3/Gauss_Markov2.png"></p>
<ul>
<li><strong>内容</strong>: These slides explain the <strong>Gauss-Markov
theorem</strong>, a cornerstone result in statistics that establishes
why the Least Squares Estimator (LSE) is considered the gold standard
for fitting linear models under a specific set of assumptions.
这些幻灯片解释了<strong>高斯-马尔可夫定理</strong>，这是统计学中的一个基石性成果，它阐明了为什么最小二乘估计量
(LSE) 被认为是在特定假设条件下拟合线性模型的黄金标准。</li>
</ul>
<h3 id="summary-6">Summary</h3>
<p>The slides argue for the superiority of the Least Squares Estimator
(LSE) by highlighting its key properties: it’s easy to compute,
consistent, and efficient. This culminates in the <strong>Gauss-Markov
Theorem</strong>, which proves that LSE is <strong>BLUE</strong>: the
<strong>B</strong>est <strong>L</strong>inear <strong>U</strong>nbiased
<strong>E</strong>stimator. This means that among all estimators that
are both linear and unbiased, the LSE is the “best” because it has the
smallest possible variance, making it the most precise. The second slide
provides the key steps for the mathematical proof of this important
theorem. 这些幻灯片通过强调最小二乘估计量 (LSE)
的关键特性来论证其优越性：易于计算、一致性高且高效。最终得出了<strong>高斯-马尔可夫定理</strong>，该定理证明了
LSE
是<strong>BLUE</strong>：<strong>最佳</strong>线性<strong>无偏</strong>估计量。这意味着在所有线性且无偏的估计量中，LSE
是“最佳”的，因为它具有最小的方差，因此精度最高。第二张幻灯片提供了这一重要定理的数学证明的关键步骤。</p>
<h3 id="deeper-dive-into-the-concepts">Deeper Dive into the
Concepts</h3>
<h4 id="properties-of-lse-slide-1-局部正交估计-lse-的性质">Properties of
LSE (Slide 1) 局部正交估计 (LSE) 的性质</h4>
<ul>
<li><strong>Easy Computation易于计算:</strong> The LSE has a direct,
closed-form solution called the Normal Equation (<span
class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>). You can
calculate it directly without needing complex iterative algorithms.</li>
<li><strong>Consistency一致性:</strong> As your sample size gets larger
and larger, the LSE estimate (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>) is guaranteed
to get closer and closer to the true population value (<span
class="math inline">\(\boldsymbol{\beta}\)</span>). With enough data, it
will find the truth. 随着样本量越来越大，LSE 估计值 (<span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>)
必然会越来越接近真实的总体值 (<span
class="math inline">\(\boldsymbol{\beta}\)</span>)。只要有足够的数据，它就能找到真相。</li>
<li><strong>Efficiency效率:</strong> An efficient estimator is the one
with the lowest possible variance. This means its estimates are the most
precise and least spread out.
高效的估计器是方差尽可能低的估计器。这意味着它的估计值最精确，且分布最均匀。</li>
<li><strong>BLUE (Best Linear Unbiased
Estimator)BLUE（最佳线性无偏估计器）:</strong> This acronym elegantly
summarizes the Gauss-Markov theorem.
这个缩写完美地概括了高斯-马尔可夫定理。
<ul>
<li><strong>Linear:</strong> The estimator is a linear function of the
response variable <strong>y</strong>. We can write it as <span
class="math inline">\(\hat{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span> for some matrix <strong>A</strong>.
估计器是响应变量<strong>y</strong>的线性函数。对于某个矩阵<strong>A</strong>，我们可以将其写成
<span class="math inline">\(\hat{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span>。</li>
<li><strong>Unbiased:</strong> The estimator does not systematically
overestimate or underestimate the true parameter. On average, its
expected value is the true value: <span
class="math inline">\(E[\hat{\boldsymbol{\beta}}] =
\boldsymbol{\beta}\)</span>.
估计器不会系统性地高估或低估真实参数。平均而言，其预期值即为真实值：<span
class="math inline">\(E[\hat{\boldsymbol{\beta}}] =
\boldsymbol{\beta}\)</span>。</li>
<li><strong>Best:</strong> It has the <strong>minimum variance</strong>
of all possible linear unbiased estimators. It’s the most precise and
reliable estimator in its class.
在所有可能的线性无偏估计器中，它的<strong>方差</strong>最小。它是同类中最精确、最可靠的估计器。</li>
</ul></li>
</ul>
<h4 id="the-gauss-markov-theorem-高斯-马尔可夫定理">The Gauss-Markov
Theorem 高斯-马尔可夫定理</h4>
<p>The theorem provides the theoretical justification for using OLS.
该定理为使用最小二乘法 (OLS) 提供了理论依据。 * <strong>The Core
Idea:</strong> You could invent many different ways to estimate the
coefficients of a linear model. As long as your proposed methods are
both linear and unbiased, the Gauss-Markov theorem guarantees that none
of them will be more precise than the standard least squares method. LSE
gives the “sharpest” possible estimates.
你可以发明许多不同的方法来估计线性模型的系数。只要你提出的方法是线性的且无偏的，高斯-马尔可夫定理就能保证，它们都不会比标准最小二乘法更精确。最小二乘法
(LSE) 给出了“最精确”的估计值。</p>
<ul>
<li><strong>The Logic of the Proof (Slide 2) 证明逻辑:</strong> The
proof is a clever comparison of variances. **该证明巧妙地比较了方差。
<ol type="1">
<li>It starts by defining <strong>any</strong> other linear unbiased
estimator as <span class="math inline">\(\tilde{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span>.
首先，将<strong>任何</strong>其他线性无偏估计量定义为 <span
class="math inline">\(\tilde{\boldsymbol{\beta}} =
\mathbf{A}\mathbf{y}\)</span>。</li>
<li>It uses the “unbiased” property to force a condition on the matrix
<strong>A</strong>, which ultimately leads to the insight that
<strong>A</strong> can be written in terms of the LSE matrix plus some
other matrix <strong>D</strong>, where <strong>DX = 0</strong>.
它利用“无偏”性质对矩阵<strong>A</strong>强制施加一个条件，最终得出<strong>A</strong>可以写成LSE矩阵加上另一个矩阵<strong>D</strong>，其中<strong>DX
= 0</strong>。</li>
<li>It then calculates the variance of this other estimator, which turns
out to be: <span class="math display">\[Var(\tilde{\boldsymbol{\beta}})
= Var(\text{LSE}) + \text{a non-negative term involving }
\mathbf{D}\]</span> 然后计算另一个估计量的方差，结果为： <span
class="math display">\[Var(\tilde{\boldsymbol{\beta}}) = Var(\text{LSE})
+ \text{一个包含 } \mathbf{D} 的非负项\]</span></li>
<li>Since the variance of any other linear unbiased estimator is the
variance of the LSE <em>plus</em> something non-negative, the variance
of the LSE must be the smallest possible value.
由于任何其他线性无偏估计量的方差都是LSE的方差<em>加上</em>一个非负项，因此LSE的方差必须是最小的可能值。</li>
</ol></li>
</ul>
<h3 id="further-understandings-beyond-the-slides">Further Understandings
Beyond the Slides</h3>
<h4 id="what-are-the-required-assumptions需要哪些假设">1. What are the
required assumptions?需要哪些假设？</h4>
<p>The Gauss-Markov theorem is powerful, but it’s not magic. It only
holds if a set of assumptions about the model’s errors (<span
class="math inline">\(\epsilon\)</span>) are met:
高斯-马尔可夫定理虽然强大，但并非魔法。它仅在满足以下关于模型误差 (<span
class="math inline">\(\epsilon\)</span>) 的假设时成立： * <strong>Zero
Mean零均值:</strong> The average of the errors is zero (<span
class="math inline">\(E[\epsilon] = 0\)</span>). 误差的平均值为零 (<span
class="math inline">\(E[\epsilon] = 0\)</span>)。 * <strong>Constant
Variance (Homoscedasticity)恒定方差（同方差性）:</strong> The errors
have the same variance, <span class="math inline">\(\sigma^2\)</span>,
at all levels of the predictors.
<strong>在预测变量的各个水平上，误差具有相同的方差 <span
class="math inline">\(\sigma^2\)</span>。 * </strong>Uncorrelated
Errors不相关误差:** The error for one observation is not correlated with
the error for another. 一个观测值的误差与另一个观测值的误差不相关。 *
<strong>No Perfect Multicollinearity非完全多重共线性:</strong> The
predictor variables are not perfectly linearly related.
预测变量并非完全线性相关。</p>
<p><strong>Crucially, the Gauss-Markov theorem does NOT require the
errors to be normally distributed.</strong> The normality assumption is
only needed later for constructing confidence intervals and conducting
t-tests and F-tests.
至关重要的是，高斯-马尔可夫定理并不要求误差服从正态分布。**正态性假设仅在构建置信区间以及进行
t 检验和 F 检验时需要。</p>
<h4
id="when-is-lse-not-the-best-the-bias-variance-tradeoff-什么时候-lse-不是最佳选择-偏差-方差权衡">2.
When is LSE NOT the Best? (The Bias-Variance Tradeoff) 什么时候 LSE
不是最佳选择？ （偏差-方差权衡）</h4>
<p>While LSE is the best <em>unbiased</em> estimator, sometimes we can
get better predictive performance by accepting a little bit of bias in
exchange for a large reduction in variance. This is the core idea behind
modern regularization methods: 虽然 LSE
是最好的<em>无偏</em>估计器，但有时我们可以通过接受少量偏差来大幅降低方差，从而获得更好的预测性能。这是现代正则化方法背后的核心思想：
* <strong>Ridge Regression and LASSO岭回归和 LASSO:</strong> These are
popular techniques that produce <em>biased</em> estimates of the
coefficients. However, by introducing this small amount of bias, they
can often produce models with a lower overall error (Mean Squared Error)
than LSE, especially when predictors are highly correlated.
这些是产生<em>有偏</em>系数估计的流行技术。然而，通过引入少量偏差，它们通常可以生成比
LSE
具有更低总体误差（均方误差）的模型，尤其是在预测变量高度相关的情况下。
Therefore, while LSE is the theoretical champion in the world of
unbiased estimators, in the world of predictive modeling, methods that
intentionally introduce bias can sometimes be superior. 因此，虽然 LSE
是无偏估计领域的理论冠军，但在预测模型领域，有意引入偏差的方法有时会更胜一筹。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/16/2025_9_15%E4%BC%9A%E8%AE%AE%E7%BA%AA%E8%A6%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/16/2025_9_15%E4%BC%9A%E8%AE%AE%E7%BA%AA%E8%A6%81/" class="post-title-link" itemprop="url">MEETING - AI4Chemistry conference notes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-09-16 01:00:00" itemprop="dateCreated datePublished" datetime="2025-09-16T01:00:00+08:00">2025-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-21 06:46:20" itemprop="dateModified" datetime="2025-09-21T06:46:20+08:00">2025-09-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/meeting-notes/" itemprop="url" rel="index"><span itemprop="name">meeting notes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Meeting Notes</p>
<h1 id="九月份规划">1. 九月份规划：</h1>
<ul>
<li><strong>内容</strong>:</li>
</ul>
<h2 id="经典论文模型复现">1.1 经典论文模型复现：</h2>
<ul>
<li><strong>1.1.1</strong> <a
target="_blank" rel="noopener" href="https://www.biorxiv.org/content/10.1101/2025.08.06.668973v2">MetaAI-3D蛋白质结构对比学习</a>
<a target="_blank" rel="noopener" href="https://github.com/kalifadan/FusionProt">github</a></li>
<li><strong>1.1.2</strong> <a
target="_blank" rel="noopener" href="https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-024-02030-9">DrugDAGT</a>
<a target="_blank" rel="noopener" href="https://github.com/codejiajia/DrugDAGT">github</a> <a
target="_blank" rel="noopener" href="https://zenodo.org/records/6568518#.YrtTKHbMK38">!!! orbnet
qm9的graph based的feature</a> <a
target="_blank" rel="noopener" href="https://zenodo.org/records/6568518#.YrtTKHbMK38">github</a></li>
<li><strong>1.1.3 GCL &amp; GCFORMER</strong> <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.13902">经典对比学习论文</a> <a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=u6FuiKzT1K">!! Graph contrastive
learning former - NIPS2024</a> <a
target="_blank" rel="noopener" href="https://github.com/JHL-HUST/GCFormer">github</a></li>
<li><strong>GCL</strong> <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.13902">Graph Contrastive Learning with
Augmentations - NIPS</a> <a
target="_blank" rel="noopener" href="https://github.com/Shen-Lab/GraphCL">github</a></li>
</ul>
<h2 id="数据集">1.2 数据集：</h2>
<p><a
target="_blank" rel="noopener" href="https://quantum-machine.org/datasets/">quantum-machine-QM9</a> <a
target="_blank" rel="noopener" href="https://github.com/bigdata-ustc/QM9nano4USTC">中科大实验课做了个QM9数据集的demo</a></p>
<ul>
<li><p><strong>闭壳层分子数据集</strong></p></li>
<li><p><strong>分子规模</strong>：含 13.4
万个稳定的小分子有机物</p></li>
<li><p><strong>元素组成</strong>：仅包含
H（氢）、C（碳）、N（氮）、O（氧）、F（氟）5 种元素</p></li>
<li><p><strong>计算理论水平</strong>：所有分子属性基于DFT（密度泛函理论）/B3LYP
泛函 / 6-31G (2df,p) 基组计算</p></li>
<li><p><strong>包含属性</strong>：偶极矩、HOMO（最高占据分子轨道）能量、LUMO（最低未占据分子轨道）能量、0K
内能、298.15K 内能等，是闭壳层分子性质预测的基准数据集</p></li>
<li><p><strong>训练输入</strong>：闭壳层下的量子化学矩阵，即 Fock
矩阵（F）、密度矩阵（P）、哈密顿矩阵（H）、重叠矩阵（S），构成向量 T
的闭壳层形式（因闭壳层自旋对称，无需区分 α、β 自旋，故
T=[F,P,H,S]）</p></li>
<li><p><strong>特征本质</strong>：这些矩阵编码了分子的电子结构信息（如轨道间相互作用、电子密度分布），是
OrbNet-Equi 学习 “分子结构 - 能量” 映射的核心依据</p></li>
<li><p><strong>训练输出</strong>：QM9 中的0K
内能作为核心训练目标输出，0K
内能是分子势能面（PES）计算的核心属性，直接关联分子稳定性与反应能垒预测，相比其他属性（如偶极矩），能量是闭壳层与开壳层系统共有的关键指标，便于后续扩展到开壳层能量预测。</p></li>
</ul>
<h2 id="复现方式">1.3 复现方式：</h2>
<p>数据小型化复现： setp1: 5k bonds and edges</p>
<p>setp2: 10k bonds and edges</p>
<p>setp3: 20k bonds and edges</p>
<h2 id="两种数据空间一个待理解的概念open-shell">1.4
两种数据空间一个待理解的概念Open-shell：</h2>
<ul>
<li><strong>AO (Atomic
Orbital)</strong>：<strong>原子轨道</strong>。</li>
<li><strong>MO (Molecular
Orbital)</strong>：<strong>分子轨道</strong>。</li>
<li><strong>Open-shell</strong>：<strong>开壳层组态</strong>。</li>
</ul>
<h4 id="ao-atomic-orbital---原子轨道层面-以原子为中心的模型">1. AO
(Atomic Orbital) - 原子轨道层面 / 以原子为中心的模型</h4>
<p>将分子看作是原子（节点）和化学键（边）构成的图。模型学习每个原子以及其周围局部环境的representation，预测整个分子的性质。</p>
<ul>
<li><p><strong>key</strong>:
分子的性质是由其组成原子以及原子间的相互作用决定的。</p></li>
<li><p><strong>输入</strong>:
原子的坐标、原子类型、以及原子间的距离或键合关系。</p></li>
<li><p><strong>方式</strong>: <strong>消息传递图神经网络 (Message
Passing Neural Network, MPNN)</strong>
。每个原子（节点）从其邻居原子那里接收“消息”（信息），更新自己的状态（特征向量）。过程会重复多次（对应图神经网络的多个GCL层），信息可以在整个分子中传播。</p>
<ul>
<li><strong>EGNN (E(n) Equivariant Graph Neural Network)</strong>:
典型的以原子为中心的模型。<strong>等变性
(Equivariance)</strong>，旋转或移动整个分子时，模型内部学习到的原子表示也会相应地旋转或移动，最终预测的能量等标量属性保持不变。符合物理规律，性能出色。它直接在原子的3D坐标上进行操作。</li>
<li><strong>OrbNet</strong>:
<strong>数据来源</strong>采用半经验方法（GFN1-xTB）生成量子化学矩阵，显著降低了计算成本，同时保留了关键物理信息,支持数千原子规模的分子模拟。
闭壳层（Closed-shell）与开壳层（Open-shell）系统的区别：闭壳层电子自旋全配对（仅需考虑空间自由度），开壳层含未配对电子（需同时考虑空间和自旋自由度），开壳层在自由基、反应中间体等场景的关键意义。
<ul>
<li><strong>key</strong>:
基于原子轨道（AO）特征（自洽场（SCF）收敛过程中的量子化学矩阵）预测分子能量。</li>
<li><strong>特征表示</strong>: 采用对称适配原子轨道（SAAO）基组，将 AO
特征编码为图结构数据。</li>
<li><strong>模型架构</strong>:
基于图神经网络（GNN），解码输出张量并求和得到分子能量。</li>
</ul></li>
</ul></li>
</ul>
<h4 id="mo-molecular-orbital---分子轨道层面-以分子为整体的模型">2. MO
(Molecular Orbital) - 分子轨道层面 / 以分子为整体的模型</h4>
<p>直接学习或预测整个分子的全局属性，分子整体电子结构相关的属性。分子轨道本身就是由所有原子轨道线性组合而成的，描述了电子在整个分子中的运动状态。</p>
<ul>
<li><strong>key</strong>:
直接对分子的全局特征或其电子结构的宏观表现（如轨道能级）进行建模。</li>
<li><strong>典型输入</strong>: 整个分子的描述符（例如分子指纹
fingerprint），或者直接将分子结构作为输入来预测分子轨道的性质。</li>
<li><strong>工作方式</strong>:
这类模型可能不完全依赖于原子间的消息传递，而是旨在直接构建一个从分子到其全局属性的映射。例如，预测分子的最高占据分子轨道
(HOMO) 和最低未占据分子轨道 (LUMO) 的能量。</li>
</ul>
<h4 id="open-shell---开壳层组态">3. Open-shell - 开壳层组态</h4>
<p><a
target="_blank" rel="noopener" href="https://www.zhihu.com/question/620638044?write">Open-shell</a></p>
<p>随便看看的一篇ICML-2024 <a
target="_blank" rel="noopener" href="https://openreview.net/forum?id=XC9IoAsyEN">ICML-WORKSHOP-2024</a></p>
<p><a
target="_blank" rel="noopener" href="https://www.nature.com/articles/s41524-022-00863-y">NPJ-2022</a></p>
<p><a
target="_blank" rel="noopener" href="https://www.pnas.org/doi/abs/10.1073/pnas.2205221119">PNAS-2022
OrbNet-Equi</a> <a
target="_blank" rel="noopener" href="https://zenodo.org/records/6568518#.YrtTKHbMK38">!!! orbnet
qm9的graph based的feature</a></p>
<h1 id="十月份规划">2. 十月份规划：</h1>
<h2 id="what-we-need-to-do">2.1 What we need to do?</h2>
<h2 id="我们需要分析ao-和-mo-的表现">我们需要分析AO 和 MO 的表现。</h2>
<p>我们不确定MO和AO的variability的差异，是由EGNN还是GPR带来的
他们的information不一样。</p>
<p>Learning curve - learnability图</p>
<p>我们需要通过对比学习找到 AO 和 MO 的 similarity。</p>
<p>理想化结果：
我们希望AO通过对比学习达到MO的程度，我们希望对比学习对AO更有用。</p>
<p>我们希望达到GCL + AO</p>
<p>AO从物理意义上更本质，MO的性质更好。</p>
<p>Final goal Inverse design 需要生成 AO</p>
<h2 id="linear-combination-of-atomic-orbitals">2.2 (Linear Combination
of Atomic Orbitals)</h2>
<p><strong>LCAO</strong> <strong>原子轨道线性组合 (Linear Combination of
Atomic Orbitals)</strong>。</p>
<ul>
<li><p><strong>key</strong>:
分子的复杂行为（由分子轨道MO描述）可以近似地通过其组成原子的更简单的行为（由原子轨道AO描述）来构建。一个<strong>分子轨道
(MO)</strong> 可以表示为多个<strong>原子轨道 (AO)</strong>
的加权和。</p></li>
<li><p><strong>数学形式</strong>: 一个分子轨道 <span
class="math inline">\(\Psi_{MO}\)</span>，它可以表示为： <span
class="math display">\[\Psi_{MO} = c_1\phi_1 + c_2\phi_2 + \dots +
c_n\phi_n = \sum_{i=1}^{n} c_i\phi_i\]</span> 其中：</p>
<ul>
<li><span class="math inline">\(\Psi_{MO}\)</span>
是一个分子轨道波函数。</li>
<li><span class="math inline">\(\phi_i\)</span> 是第 <span
class="math inline">\(i\)</span> 个原子的原子轨道波函数。</li>
<li><span class="math inline">\(c_i\)</span>
是每个原子轨道的<strong>组合系数
(coefficient)</strong>，它是一个权重值，表示该原子轨道对这个分子轨道的贡献大小。系数通过求解薛定谔方程（通常使用Hartree-Fock等近似方法）得到的。</li>
</ul></li>
<li><p><strong>ex</strong>:
氢分子（H₂）。有两个氢原子，每个氢原子有一个1s原子轨道（<span
class="math inline">\(\phi_A\)</span> 和 <span
class="math inline">\(\phi_B\)</span>）。这两个原子轨道可以通过两种方式线性组合，形成两个分子轨道：</p>
<ol type="1">
<li><strong>成键轨道 (Bonding MO)</strong>: <span
class="math inline">\(\Psi_{\sigma} = c_A\phi_A +
c_B\phi_B\)</span>。电子处于这个轨道时，会主要分布在两个原子核之间，形成稳定的化学键。能量比原来的AO更低。</li>
<li><strong>反键轨道 (Antibonding MO)</strong>: <span
class="math inline">\(\Psi_{\sigma^*} = c&#39;_A\phi_A -
c&#39;_B\phi_B\)</span>。电子处于这个轨道时，会主要分布在原子核的外侧，排斥两个原子核，不利于成键。能量比原来的AO更高。</li>
</ol></li>
</ul>
<h3 id="localization-分子轨道局域化">2.3 Localization
(分子轨道局域化)</h3>
<p>分子轨道（MOs），尤其是通过标准计算方法（如Hartree-Fock）直接求解出来的，通常是<strong>离域的
(delocalized)</strong>。这意味着每个MO都可能扩展到整个分子，由分子中几乎所有原子的AOs贡献构成。例如，在苯环中，计算出的π电子MO会均匀地分布在六个碳原子上。</p>
<p><strong>分子轨道局域化 (Localization of Molecular Orbitals)</strong>
就是一个数学变换过程，它将这些离域的MOs转化为一组新的<strong>局域化分子轨道
(Localized Molecular Orbitals, LMOs)</strong>。</p>
<ul>
<li><strong>核心目标</strong>:
在不改变分子整体波函数和总能量的前提下，将分子轨道尽可能地限制在空间中的一小块区域内。</li>
<li><strong>变换结果</strong>:
<ul>
<li>离域的成键轨道 <span class="math inline">\(\rightarrow\)</span>
对应于特定 <strong>化学键</strong>
的局域轨道（例如C-H键，C=C双键）。</li>
<li>离域的非键轨道 <span class="math inline">\(\rightarrow\)</span>
对应于特定原子上的 <strong>孤对电子 (lone pair)</strong> 或
<strong>内层电子</strong>。</li>
</ul></li>
<li><strong>局域化</strong>:
<ol type="1">
<li><strong>化学直观性</strong>:
LMOs提供了清晰的化学图像，便于理解和分析化学成键情况。</li>
</ol></li>
</ul>
<h3 id="lcao-和-mo-的关系">3. LCAO 和 MO 的关系</h3>
<ol type="1">
<li><strong>LCAO是构建MO的方法</strong>:
LCAO是用于近似计算和表示分子轨道（MO）的数学框架。我们假设MO可以由一组已知的基函数（即原子轨道AO）线性组合而成。</li>
<li><strong>MO是LCAO方法的结果</strong>:
通过LCAO方法，结合量子力学变分原理求解薛定谔方程，我们最终得到了一系列分子轨道（MOs）的具体形式（即每个AO的贡献系数<span
class="math inline">\(c_i\)</span>）以及它们的能量。</li>
</ol>
<p><strong>原子轨道 (AO) [输入]</strong> <span
class="math inline">\(\xrightarrow{\text{LCAO方法 [过程/框架]}}\)</span>
<strong>分子轨道 (MO) [输出/结果]</strong></p>
<h3 id="高斯过程回归-gaussian-process-regression-gpr">4. 高斯过程回归
(Gaussian Process Regression, GPR)</h3>
<p><strong>高斯过程回归 (GPR)</strong>
是一种基于贝叶斯思想的非参数回归方法。它在处理小样本、高维度、需要不确定性估计的复杂回归问题时特别有效。</p>
<h4 id="key">key</h4>
<p><strong>GPR的核心是直接对函数本身进行建模</strong>。它假设我们想要建模的目标函数
<span class="math inline">\(f(x)\)</span> 是一个服从<strong>高斯过程
(Gaussian Process, GP)</strong> 的随机函数。</p>
<ul>
<li><strong>高斯过程 (GP)</strong>
一个高斯过程是无穷多个随机变量的集合，其中任意有限个随机变量的组合都服从一个联合高斯分布。
一个GP定义了一个关于<strong>函数的分布 (a distribution over
functions)</strong>。当我们从这个GP中“采样”时，我们得到的不是一个数值，而是一整个函数。</li>
</ul>
<p>一个高斯过程完全由两部分定义： 1. <strong>均值函数 (Mean Function)
<span class="math inline">\(m(x)\)</span></strong>:
定义了函数分布的“期望”或“中心趋势”。通常为了简化，会假设均值为零。 2.
<strong>协方差函数 (Covariance Function) 或 核函数 (Kernel) <span
class="math inline">\(k(x, x&#39;)\)</span></strong>:
定义了函数在不同输入点 <span class="math inline">\(x\)</span> 和 <span
class="math inline">\(x&#39;\)</span>
处的值之间的“相关性”或“相似性”。如果 <span
class="math inline">\(x\)</span> 和 <span
class="math inline">\(x&#39;\)</span> 很接近，核函数的值就很大，意味着
<span class="math inline">\(f(x)\)</span> 和 <span
class="math inline">\(f(x&#39;)\)</span>
的值会很相似。这编码了我们对函数平滑性的先验信念。</p>
<h4 id="gpr-工作">GPR 工作</h4>
<p>GPR的工作流程：</p>
<p><strong>第一步：定义先验分布 (Prior Distribution)</strong>
在看到任何训练数据之前，我们首先根据先验知识选择一个均值函数（通常为0）和一个核函数（例如常用的<strong>径向基函数核/RBF核</strong>）。这个GP定义了一个函数的先验分布，包含了我们能想到的所有“可能”的函数。</p>
<p><strong>第二步：计算后验分布 (Posterior Distribution)</strong>
当我们得到一组训练数据 <span class="math inline">\((X_{train},
Y_{train})\)</span>
后，我们利用贝叶斯定理来更新我们的函数分布。我们从先验分布中“筛选”掉那些与训练数据不符的函数，得到一个<strong>后验分布
(Posterior Distribution)</strong>。</p>
<p>这个后验分布仍然是一个高斯过程，其均值和协方差有解析解（可以直接计算出来），不需要复杂的迭代优化。</p>
<h4 id="进行预测">进行预测</h4>
<p>对于一个新的测试点 <span
class="math inline">\(x_{test}\)</span>，我们想预测对应的 <span
class="math inline">\(y_{test}\)</span>。在后验分布下，<span
class="math inline">\(y_{test}\)</span>
的预测值服从一个一维高斯分布，这个分布有： 1. <strong>预测均值
(Predicted Mean)</strong>: 这就是我们对 <span
class="math inline">\(y_{test}\)</span>
的最佳点估计。它是由训练数据点的加权平均计算得出的，权重由核函数决定。
2. <strong>预测方差 (Predicted Variance)</strong>:
这衡量了我们对预测结果的<strong>不确定性</strong>。在靠近训练数据点的地方，方差会很小（预测很自信）；在远离训练数据点的未知区域，方差会很大（预测很不确定）。</p>
<h1 id="后续规划">3. 后续规划：</h1>
<p>IF AO 的学习表现比 MO 要好 我们将会聚焦于 AO （Atomic representation
vs atomic orbital）</p>
<ol type="1">
<li><strong>AO one body decomposition</strong></li>
<li><strong>MO two body decomposition</strong></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/15/Pandoc_Deployment/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/15/Pandoc_Deployment/" class="post-title-link" itemprop="url">BLOGS - Pandoc Deployment</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-09-15 10:00:00" itemprop="dateCreated datePublished" datetime="2025-09-15T10:00:00+08:00">2025-09-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-19 19:24:55" itemprop="dateModified" datetime="2025-09-19T19:24:55+08:00">2025-09-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">技术</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Download <a
target="_blank" rel="noopener" href="https://github.com/jgm/pandoc/releases/tag/3.8">Pandoc</a>!</p>
<p><strong>pandoc-3.8-windows-x86_64.msi</strong></p>
<h2
id="问题主要为了解决默认的next渲染器无法渲染复杂公式的问题">【问题】主要为了解决默认的Next渲染器无法渲染复杂公式的问题</h2>
<h3
id="step1在系统变量中找到path点击编辑">Step1:在系统变量中找到<strong>Path→点击编辑</strong></h3>
<h3
id="step2点击新建输入pandoc.exe的父目录路径c点击确定">Step2:点击新建→输入pandoc.exe的父目录路径（C:）→点击确定</h3>
<h3 id="step3重启终端">Step3:重启终端</h3>
<h3
id="step4安装pandoc-3.8-windows-x86_64.msi">Step4:安装pandoc-3.8-windows-x86_64.msi</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pandoc --version</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm list --depth=0 | Select-String <span class="string">&quot;renderer&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-markdown-it --save</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm list --depth=0 | Select-String <span class="string">&quot;renderer&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://blog.csdn.net/gitblog_00216/article/details/141763934">Ref</a></p>
<h3
id="step5在你所在的博客头加入必要的引入">Step5:在你所在的博客头加入必要的引入</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mathjax: <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3
id="step6在d_ailab_hkust_machine_learning的_config.yaml文件中确保你的pandoc路径能被找到">Step6:在D:_AILab_HKUST_Machine_Learning的_config.yaml文件中确保你的Pandoc路径能被找到</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ pandoc:</span><br><span class="line">$ pandoc_path: <span class="string">&quot;C:/Users/Aprine/AppData/Local/Pandoc/pandoc.exe&quot;</span> <span class="comment"># </span></span><br><span class="line">$ args:</span><br><span class="line">$   - <span class="string">&quot;--mathjax&quot;</span></span><br></pre></td></tr></table></figure>
<h3
id="step7在d_ailab_hkust_machine_learning_config.yaml文件中确保你的math信息配置正确">Step7:在D:_AILab_HKUST_Machine_Learning_config.yaml文件中确保你的math信息配置正确</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ mathjax:</span><br><span class="line">$   <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">$   <span class="comment"># See: https://mhchem.github.io/MathJax-mhchem/</span></span><br><span class="line">$   mhchem: <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3
id="step8修改你的head文件的基础格式">Step8:修改你的head文件的基础格式</h3>
<p>More info: <a
target="_blank" rel="noopener" href="https://blog.csdn.net/ALexander_Monster/article/details/105717091">Ref</a></p>
<h3 id="push-new-blog">Push new blog</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$   hexo g -d</span><br></pre></td></tr></table></figure>
<p>对比损失函数（InfoNCE/NT-Xent Loss）定义为： <span
class="math inline">\(\mathcal{L}_{\text{q}} = -\log \underbrace{\left(
\frac{\exp\left( \mathbf{q} \cdot \mathbf{k}^{+} / \tau
\right)}{\exp\left( \mathbf{q} \cdot \mathbf{k}^{+} / \tau \right) +
\sum\limits_{i=1}^{N} \exp\left( \mathbf{q} \cdot \mathbf{k}_{i}^{-} /
\tau \right)} \right)}_{\text{Softmax 概率}}\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
