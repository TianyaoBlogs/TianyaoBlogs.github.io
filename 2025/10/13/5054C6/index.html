<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianyaoblogs.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-6 Lecturer: Prof.XIA DONG 1. Linear Model Selection and Regularization çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ– Summary of Core Concepts Chapter 6: Linear Model Selection and Regularization, focusing specifica">
<meta property="og:type" content="article">
<meta property="og:title" content="MSDM 5054 - Statistical Machine Learning-L6">
<meta property="og:url" content="https://tianyaoblogs.github.io/2025/10/13/5054C6/index.html">
<meta property="og:site_name" content="TianyaoBlogs">
<meta property="og:description" content="ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-6 Lecturer: Prof.XIA DONG 1. Linear Model Selection and Regularization çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ– Summary of Core Concepts Chapter 6: Linear Model Selection and Regularization, focusing specifica">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-13T13:00:00.000Z">
<meta property="article:modified_time" content="2025-10-19T19:21:23.759Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://tianyaoblogs.github.io/2025/10/13/5054C6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>MSDM 5054 - Statistical Machine Learning-L6 | TianyaoBlogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TianyaoBlogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/13/5054C6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MSDM 5054 - Statistical Machine Learning-L6
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-10-13 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-13T21:00:00+08:00">2025-10-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-10-20 03:21:23" itemprop="dateModified" datetime="2025-10-20T03:21:23+08:00">2025-10-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-6</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1
id="linear-model-selection-and-regularization-çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–">1.
Linear Model Selection and Regularization çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–</h1>
<h2 id="summary-of-core-concepts">Summary of Core Concepts</h2>
<p><strong>Chapter 6: Linear Model Selection and
Regularization</strong>, focusing specifically on <strong>Section 6.1:
Subset Selection</strong>.
<strong>ç¬¬å…­ç« ï¼šçº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–</strong>ï¼Œ<strong>6.1èŠ‚ï¼šå­é›†é€‰æ‹©</strong></p>
<ul>
<li><p><strong>The Problem:</strong> You have a dataset with many
potential predictor variables (features). If you include all of them
(like <strong>Model 1</strong> with <span
class="math inline">\(p\)</span> predictors in slide
<code>...221320.png</code>), you risk including â€œnoiseâ€ variables. These
irrelevant features can decrease model accuracy (overfitting) and make
the model difficult to interpret.
æ•°æ®é›†åŒ…å«è®¸å¤šæ½œåœ¨çš„é¢„æµ‹å˜é‡ï¼ˆç‰¹å¾ï¼‰ã€‚å¦‚æœåŒ…å«æ‰€æœ‰è¿™äº›å˜é‡ï¼ˆä¾‹å¦‚å¹»ç¯ç‰‡â€œâ€¦221320.pngâ€ä¸­å¸¦æœ‰<span
class="math inline">\(p\)</span>ä¸ªé¢„æµ‹å˜é‡çš„<strong>æ¨¡å‹1</strong>ï¼‰ï¼Œåˆ™å¯èƒ½ä¼šåŒ…å«â€œå™ªå£°â€å˜é‡ã€‚è¿™äº›ä¸ç›¸å…³çš„ç‰¹å¾ä¼šé™ä½æ¨¡å‹çš„å‡†ç¡®ç‡ï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼Œå¹¶ä½¿æ¨¡å‹éš¾ä»¥è§£é‡Šã€‚</p></li>
<li><p><strong>The Goal:</strong> Identify a smaller subset of variables
that are truly related to the response. This creates a simpler, more
interpretable, and often more accurate model (like <strong>Model
2</strong> with <span class="math inline">\(q\)</span> predictors).
æ‰¾å‡ºä¸€ä¸ªä¸å“åº”çœŸæ­£ç›¸å…³çš„è¾ƒå°å˜é‡å­é›†ã€‚è¿™å°†åˆ›å»ºä¸€ä¸ªæ›´ç®€å•ã€æ›´æ˜“äºè§£é‡Šä¸”é€šå¸¸æ›´å‡†ç¡®çš„æ¨¡å‹ï¼ˆä¾‹å¦‚å¸¦æœ‰<span
class="math inline">\(q\)</span>ä¸ªé¢„æµ‹å˜é‡çš„<strong>æ¨¡å‹2</strong>ï¼‰ã€‚</p></li>
<li><p><strong>The Main Method Discussed: Best Subset
Selection</strong></p></li>
<li><p><strong>ä¸»è¦è®¨è®ºçš„æ–¹æ³•ï¼šæœ€ä½³å­é›†é€‰æ‹©</strong> This is an
<em>exhaustive search</em> algorithm. It checks <em>every possible
combination</em> of predictors to find the â€œbestâ€ model. With <span
class="math inline">\(p\)</span> variables, this means checking <span
class="math inline">\(2^p\)</span> total models.
è¿™æ˜¯ä¸€ç§<em>ç©·ä¸¾æœç´¢</em>ç®—æ³•ã€‚å®ƒæ£€æŸ¥<em>æ‰€æœ‰å¯èƒ½çš„é¢„æµ‹å˜é‡ç»„åˆ</em>ï¼Œä»¥æ‰¾åˆ°â€œæœ€ä½³â€æ¨¡å‹ã€‚å¯¹äº
<span class="math inline">\(p\)</span> ä¸ªå˜é‡ï¼Œè¿™æ„å‘³ç€éœ€è¦æ£€æŸ¥æ€»å…±
<span class="math inline">\(2^p\)</span> ä¸ªæ¨¡å‹ã€‚</p>
<p>The algorithm (from slide <code>...221333.png</code>) works in three
steps:</p>
<ol type="1">
<li><p><strong>Step 1:</strong> Fit the â€œnull modelâ€ <span
class="math inline">\(M_0\)</span>, which has no predictors (it just
predicts the average of the response). æ‹Ÿåˆâ€œç©ºæ¨¡å‹â€<span
class="math inline">\(M_0\)</span>ï¼Œå®ƒæ²¡æœ‰é¢„æµ‹å˜é‡ï¼ˆå®ƒåªé¢„æµ‹å“åº”çš„å¹³å‡å€¼ï¼‰ã€‚</p></li>
<li><p><strong>Step 2:</strong> For each <span
class="math inline">\(k\)</span> (from 1 to <span
class="math inline">\(p\)</span>):</p>
<ul>
<li><p>Fit all <span class="math inline">\(\binom{p}{k}\)</span> models
that contain exactly <span class="math inline">\(k\)</span> predictors.
(e.g., fit all models with 1 predictor, then all models with 2
predictors, etc.).</p></li>
<li><p>æ‹Ÿåˆæ‰€æœ‰åŒ…å« <span class="math inline">\(k\)</span> ä¸ªé¢„æµ‹å˜é‡çš„
<span class="math inline">\(\binom{p}{k}\)</span>
ä¸ªæ¨¡å‹ã€‚ï¼ˆä¾‹å¦‚ï¼Œå…ˆæ‹Ÿåˆæ‰€æœ‰åŒ…å« 1 ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œç„¶åæ‹Ÿåˆæ‰€æœ‰åŒ…å« 2
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œç­‰ç­‰ï¼‰ã€‚</p></li>
<li><p>From this group, select the single best model <em>for that size
<span class="math inline">\(k\)</span></em>. This â€œbestâ€ model is the
one with the highest <strong><span
class="math inline">\(R^2\)</span></strong> (or lowest
<strong>RSS</strong> - Residual Sum of Squares) on the <em>training
data</em>. Call this model <span
class="math inline">\(M_k\)</span>.</p></li>
<li><p>ä»è¿™ç»„ä¸­ï¼Œé€‰æ‹© <em>å¯¹äºè¯¥è§„æ¨¡ <span
class="math inline">\(k\)</span></em> çš„æœ€ä½³æ¨¡å‹ã€‚è¿™ä¸ªâ€œæœ€ä½³â€æ¨¡å‹æ˜¯åœ¨
<em>è®­ç»ƒæ•°æ®</em> ä¸Šå…·æœ‰æœ€é«˜ <strong><span
class="math inline">\(R^2\)</span></strong>ï¼ˆæˆ–æœ€ä½ <strong>RSS</strong>
- æ®‹å·®å¹³æ–¹å’Œï¼‰çš„æ¨¡å‹ã€‚å°†æ­¤æ¨¡å‹ç§°ä¸º <span
class="math inline">\(M_k\)</span>ã€‚</p></li>
</ul></li>
<li><p><strong>Step 3:</strong> You now have <span
class="math inline">\(p+1\)</span> models: <span
class="math inline">\(M_0, M_1, \dots, M_p\)</span>. You must select the
single best one from this list. To do this, you <strong>cannot</strong>
use training <span class="math inline">\(R^2\)</span> (as it will always
pick the biggest model <span class="math inline">\(M_p\)</span>).
Instead, you must use a metric that estimates <em>test error</em>, such
as: <strong>ç°åœ¨ä½ æœ‰ <span class="math inline">\(p+1\)</span>
ä¸ªæ¨¡å‹ï¼š<span class="math inline">\(M_0, M_1, \dots,
M_p\)</span>ã€‚ä½ å¿…é¡»ä»åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä½³æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œä½ </strong>ä¸èƒ½**ä½¿ç”¨è®­ç»ƒ
<span class="math inline">\(R^2\)</span>ï¼ˆå› ä¸ºå®ƒæ€»æ˜¯ä¼šé€‰æ‹©æœ€å¤§çš„æ¨¡å‹
<span
class="math inline">\(M_p\)</span>ï¼‰ã€‚ç›¸åï¼Œä½ å¿…é¡»ä½¿ç”¨ä¸€ä¸ªèƒ½å¤Ÿä¼°è®¡<em>æµ‹è¯•è¯¯å·®</em>çš„æŒ‡æ ‡ï¼Œä¾‹å¦‚ï¼š</p>
<ul>
<li><strong>Cross-Validation (CV) äº¤å‰éªŒè¯ (CV)</strong> (This is what
the Python code uses)</li>
<li><strong>AIC</strong> (Akaike Information Criterion
èµ¤æ± ä¿¡æ¯å‡†åˆ™)</li>
<li><strong>BIC</strong> (Bayesian Information Criterion
è´å¶æ–¯ä¿¡æ¯å‡†åˆ™)</li>
<li><strong>Adjusted <span class="math inline">\(R^2\)</span> è°ƒæ•´åçš„
<span class="math inline">\(R^2\)</span></strong></li>
</ul></li>
</ol></li>
<li><p><strong>Key Takeaway:</strong> The slides show this â€œsubset
selectionâ€ concept can be applied <em>beyond</em> linear models. The
Python code demonstrates this by applying best subset selection to a
<strong>K-Nearest Neighbors (KNN) Regressor</strong>, a non-linear
model.â€œå­é›†é€‰æ‹©â€çš„æ¦‚å¿µå¯ä»¥åº”ç”¨äºçº¿æ€§æ¨¡å‹<em>ä¹‹å¤–</em>ã€‚</p></li>
</ul>
<h2
id="mathematical-understanding-key-questions-æ•°å­¦ç†è§£ä¸å…³é”®é—®é¢˜">Mathematical
Understanding &amp; Key Questions æ•°å­¦ç†è§£ä¸å…³é”®é—®é¢˜</h2>
<p>This section directly answers the questions posed on your slides.</p>
<h3 id="how-to-compare-which-model-is-better">How to compare which model
is better?</h3>
<p>(From slides <code>...221320.png</code> and
<code>...221326.png</code>)</p>
<p>You cannot use <strong>training error</strong> (like <span
class="math inline">\(R^2\)</span> or RSS) to compare models with
<em>different numbers of predictors</em>. A model with more predictors
will almost always have a better <em>training</em> score, even if those
extra predictors are just noise. This is called
<strong>overfitting</strong>. ä¸èƒ½ä½¿ç”¨<strong>è®­ç»ƒè¯¯å·®</strong>ï¼ˆä¾‹å¦‚
<span class="math inline">\(R^2\)</span> æˆ–
RSSï¼‰æ¥æ¯”è¾ƒå…·æœ‰<em>ä¸åŒæ•°é‡é¢„æµ‹å˜é‡</em>çš„æ¨¡å‹ã€‚å…·æœ‰æ›´å¤šé¢„æµ‹å˜é‡çš„æ¨¡å‹å‡ ä¹æ€»æ˜¯å…·æœ‰æ›´å¥½çš„<em>è®­ç»ƒ</em>åˆ†æ•°ï¼Œå³ä½¿è¿™äº›é¢å¤–çš„é¢„æµ‹å˜é‡åªæ˜¯å™ªå£°ã€‚è¿™è¢«ç§°ä¸º<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚</p>
<p>To compare models of different sizes (like Model 1 vs.Â Model 2, or
<span class="math inline">\(M_2\)</span> vs.Â <span
class="math inline">\(M_5\)</span>), you <strong>must</strong> use a
method that estimates <strong>test error</strong> (how the model
performs on new, unseen data). The slides mention:
è¦æ¯”è¾ƒä¸åŒå¤§å°çš„æ¨¡å‹ï¼ˆä¾‹å¦‚æ¨¡å‹ 1 ä¸æ¨¡å‹ 2ï¼Œæˆ– <span
class="math inline">\(M_2\)</span> ä¸ <span
class="math inline">\(M_5\)</span>ï¼‰ï¼Œæ‚¨<strong>å¿…é¡»</strong>ä½¿ç”¨ä¸€ç§ä¼°ç®—<strong>æµ‹è¯•è¯¯å·®</strong>ï¼ˆæ¨¡å‹åœ¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ä¸Šçš„è¡¨ç°ï¼‰çš„æ–¹æ³•ã€‚</p>
<ul>
<li><p><strong>Cross-Validation (CV):</strong> This is the gold
standard. You split your data into â€œfolds,â€ train the model on some
folds, and test it on the remaining fold. You repeat this and average
the test scores. The model with the best (e.g., lowest) average CV error
is chosen.
å°†æ•°æ®åˆ†æˆâ€œæŠ˜å â€ï¼Œåœ¨ä¸€äº›æŠ˜å ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨å‰©ä½™çš„æŠ˜å ä¸Šæµ‹è¯•æ¨¡å‹ã€‚é‡å¤æ­¤æ“ä½œå¹¶å–æµ‹è¯•åˆ†æ•°çš„å¹³å‡å€¼ã€‚é€‰æ‹©å¹³å‡
CV è¯¯å·®æœ€å°ï¼ˆä¾‹å¦‚ï¼Œæœ€å°ï¼‰çš„æ¨¡å‹ã€‚</p></li>
<li><p><strong>AIC &amp; BIC:</strong> These are mathematical
adjustments to the training error (like RSS) that add a <em>penalty</em>
for having more predictors. They balance model <em>fit</em> with model
<em>complexity</em>. è¿™äº›æ˜¯å¯¹è®­ç»ƒè¯¯å·®ï¼ˆå¦‚
RSSï¼‰çš„æ•°å­¦è°ƒæ•´ï¼Œä¼šå› é¢„æµ‹å˜é‡è¾ƒå¤šè€Œå¢åŠ <em>æƒ©ç½š</em>ã€‚å®ƒä»¬å¹³è¡¡äº†æ¨¡å‹<em>æ‹Ÿåˆåº¦</em>å’Œæ¨¡å‹<em>å¤æ‚åº¦</em>ã€‚</p></li>
</ul>
<h3 id="why-use-r2-in-step-2">Why use <span
class="math inline">\(R^2\)</span> in Step 2?</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p>In Step 2, you are only comparing models <strong>of the same
size</strong> (i.e., all models that have exactly <span
class="math inline">\(k\)</span> predictors). For models with the same
number of parameters, a higher <span class="math inline">\(R^2\)</span>
(or lower RSS) on the training data directly corresponds to a better
fit. You donâ€™t need to penalize for complexity because all models being
compared <em>have the same complexity</em>.
åªæ¯”è¾ƒ<strong>å¤§å°ç›¸åŒ</strong>çš„æ¨¡å‹ï¼ˆå³æ‰€æœ‰æ°å¥½å…·æœ‰ <span
class="math inline">\(k\)</span>
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼‰ã€‚å¯¹äºå‚æ•°æ•°é‡ç›¸åŒçš„æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®ä¸Šæ›´é«˜çš„ <span
class="math inline">\(R^2\)</span>ï¼ˆæˆ–æ›´ä½çš„
RSSï¼‰ç›´æ¥å¯¹åº”ç€æ›´å¥½çš„æ‹Ÿåˆåº¦ã€‚æ‚¨ä¸éœ€è¦å¯¹å¤æ‚åº¦è¿›è¡Œæƒ©ç½šï¼Œå› ä¸ºæ‰€æœ‰è¢«æ¯”è¾ƒçš„æ¨¡å‹<em>éƒ½å…·æœ‰ç›¸åŒçš„å¤æ‚åº¦</em>ã€‚</p>
<h3 id="why-cant-we-use-training-error-in-step-3">Why canâ€™t we use
training error in Step 3?</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p>In Step 3, you are comparing models <strong>of different
sizes</strong> (<span class="math inline">\(M_0\)</span> vs.Â <span
class="math inline">\(M_1\)</span> vs.Â <span
class="math inline">\(M_2\)</span>, etc.). As you add predictors, the
training <span class="math inline">\(R^2\)</span> will <em>always</em>
go up (or stay the same), and the training RSS will <em>always</em> go
down (or stay the same). If you used <span
class="math inline">\(R^2\)</span> to pick the best model in Step 3, you
would <em>always</em> pick the most complex model <span
class="math inline">\(M_p\)</span>, which is almost certainly overfit.
å°†æ¯”è¾ƒ<strong>ä¸åŒå¤§å°</strong>çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ <span
class="math inline">\(M_0\)</span> vs.Â <span
class="math inline">\(M_1\)</span> vs.Â <span
class="math inline">\(M_2\)</span> ç­‰ï¼‰ã€‚éšç€æ‚¨æ·»åŠ é¢„æµ‹å˜é‡ï¼Œè®­ç»ƒ <span
class="math inline">\(R^2\)</span>
å°†<em>å§‹ç»ˆ</em>ä¸Šå‡ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ï¼Œè€Œè®­ç»ƒ RSS
å°†<em>å§‹ç»ˆ</em>ä¸‹é™ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ã€‚å¦‚æœæ‚¨åœ¨æ­¥éª¤ 3 ä¸­ä½¿ç”¨ <span
class="math inline">\(R^2\)</span>
æ¥é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼Œé‚£ä¹ˆæ‚¨<em>å§‹ç»ˆ</em>ä¼šé€‰æ‹©æœ€å¤æ‚çš„æ¨¡å‹ <span
class="math inline">\(M_p\)</span>ï¼Œè€Œè¯¥æ¨¡å‹å‡ ä¹è‚¯å®šä¼šè¿‡æ‹Ÿåˆã€‚</p>
<p>Therefore, you <em>must</em> use a metric that estimates test error
(like CV) or penalizes for complexity (like AIC, BIC, or Adjusted <span
class="math inline">\(R^2\)</span>) to find the right balance between
fit and simplicity. å› æ­¤ï¼Œæ‚¨<em>å¿…é¡»</em>ä½¿ç”¨ä¸€ä¸ªå¯ä»¥ä¼°ç®—æµ‹è¯•è¯¯å·®ï¼ˆä¾‹å¦‚
CVï¼‰æˆ–æƒ©ç½šå¤æ‚åº¦ï¼ˆä¾‹å¦‚ AICã€BIC æˆ–è°ƒæ•´åçš„ <span
class="math inline">\(R^2\)</span>ï¼‰çš„æŒ‡æ ‡æ¥æ‰¾åˆ°æ‹Ÿåˆåº¦å’Œç®€å•æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚</p>
<h2 id="code-analysis">Code Analysis</h2>
<p>The Python code (slides <code>...221249.jpg</code> and
<code>...221303.jpg</code>) implements the <strong>Best Subset
Selection</strong> algorithm using <strong>KNN Regression</strong>.</p>
<h3 id="key-functions">Key Functions</h3>
<ul>
<li><code>main()</code>:
<ol type="1">
<li><strong>Loads Data:</strong> Reads the <code>Credit.csv</code>
file.</li>
<li><strong>Preprocesses Data:</strong>
<ul>
<li>Converts categorical features (â€˜Genderâ€™, â€˜Studentâ€™, â€˜Marriedâ€™,
â€˜Ethnicityâ€™) into numerical ones (dummy variables).
å°†åˆ†ç±»ç‰¹å¾ï¼ˆâ€œæ€§åˆ«â€ã€â€œå­¦ç”Ÿâ€ã€â€œå·²å©šâ€ã€â€œç§æ—â€ï¼‰è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾ï¼ˆè™šæ‹Ÿå˜é‡ï¼‰ã€‚</li>
<li>Creates the feature matrix <code>X</code> and target variable
<code>y</code> (â€˜Balanceâ€™). åˆ›å»ºç‰¹å¾çŸ©é˜µ <code>X</code> å’Œç›®æ ‡å˜é‡
<code>y</code>ï¼ˆâ€œä½™é¢â€ï¼‰ã€‚</li>
<li><strong>Scales</strong> the features using
<code>StandardScaler</code>. This is crucial for KNN, which is sensitive
to the scale of features. ç”¨ <code>StandardScaler</code>
å¯¹ç‰¹å¾è¿›è¡Œ<strong>ç¼©æ”¾</strong>ã€‚è¿™å¯¹äº KNN
è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¯¹ç‰¹å¾çš„ç¼©æ”¾éå¸¸æ•æ„Ÿã€‚</li>
</ul></li>
<li><strong>Adds Noise (in the second example):</strong> Slide
<code>...221303.jpg</code> shows code that <em>adds 20 new â€œnoisyâ€
columns</em> to the data. This is to test if the selection algorithm is
smart enough to ignore them. å‘æ•°æ®ä¸­æ·»åŠ  20
ä¸ªæ–°çš„â€œå™ªå£°â€åˆ—çš„ä»£ç ã€‚è¿™æ˜¯ä¸ºäº†æµ‹è¯•é€‰æ‹©ç®—æ³•æ˜¯å¦è¶³å¤Ÿæ™ºèƒ½ï¼Œèƒ½å¤Ÿå¿½ç•¥å®ƒä»¬ã€‚</li>
<li><strong>Runs Selection:</strong> Calls
<code>best_subset_selection_parallel</code> to do the main work.</li>
<li><strong>Prints Results:</strong> Finds the best subset (lowest
error) and prints the top 20 best-performing subsets.
æ‰¾åˆ°æœ€ä½³å­é›†ï¼ˆè¯¯å·®æœ€å°ï¼‰ï¼Œå¹¶æ‰“å°å‡ºè¡¨ç°æœ€ä½³çš„å‰ 20 ä¸ªå­é›†ã€‚</li>
<li><strong>Final Evaluation:</strong> It re-trains a KNN model on
<em>only</em> the best subset and calculates the final cross-validated
RMSE. ä»…åŸºäºæœ€ä½³å­é›†é‡æ–°è®­ç»ƒ KNN æ¨¡å‹ï¼Œå¹¶è®¡ç®—æœ€ç»ˆçš„äº¤å‰éªŒè¯ RMSEã€‚</li>
</ol></li>
<li><code>evaluate_subset(subset, ...)</code>:
<ul>
<li>This is the â€œworkerâ€ function. Itâ€™s called for <em>every single</em>
possible subset.</li>
<li>It takes a <code>subset</code> (a list of feature names, e.g.,
<code>['Income', 'Limit']</code>).</li>
<li>It creates a new <code>X_subset</code> containing <em>only</em>
those columns.</li>
<li>It runs 5-fold cross-validation (<code>cross_val_score</code>) on a
KNN model using this <code>X_subset</code>.</li>
<li>It uses <code>'neg_mean_squared_error'</code> as the metric. This is
negative MSE; a <em>higher</em> score (closer to 0) is better.
å®ƒä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„â€œX_subsetâ€<em>ï¼Œä»…åŒ…å«è¿™äº›åˆ—ã€‚ å®ƒä¼šä½¿ç”¨æ­¤â€œX_subsetâ€åœ¨
KNN æ¨¡å‹ä¸Šè¿è¡Œ 5 å€äº¤å‰éªŒè¯ï¼ˆâ€œcross_val_scoreâ€ï¼‰ã€‚
å®ƒä½¿ç”¨â€œneg_mean_squared_errorâ€ä½œä¸ºåº¦é‡æ ‡å‡†ã€‚è¿™æ˜¯è´Ÿ
MSEï¼›</em>æ›´é«˜*çš„åˆ†æ•°ï¼ˆè¶Šæ¥è¿‘ 0ï¼‰è¶Šå¥½ã€‚</li>
<li>It returns the subset and its average CV score.</li>
</ul></li>
<li><code>best_subset_selection_parallel(model, ...)</code>:
<ul>
<li>This is the â€œmanagerâ€ function.è¿™æ˜¯â€œç®¡ç†å™¨â€å‡½æ•°ã€‚</li>
<li>It iterates from <code>k=1</code> up to the total number of
features.å®ƒä»â€œk=1â€è¿­ä»£åˆ°ç‰¹å¾æ€»æ•°ã€‚</li>
<li>For each <code>k</code>, it generates <em>all combinations</em> of
features of that size (this is the <span
class="math inline">\(\binom{p}{k}\)</span> part).
å¯¹äºæ¯ä¸ªâ€œkâ€ï¼Œå®ƒä¼šç”Ÿæˆè¯¥å¤§å°çš„ç‰¹å¾çš„<em>æ‰€æœ‰ç»„åˆ</em>ï¼ˆè¿™æ˜¯ <span
class="math inline">\(\binom{p}{k}\)</span> éƒ¨åˆ†ï¼‰ã€‚</li>
<li>It uses <code>Parallel</code> and <code>delayed</code> (from
<code>joblib</code>) to run <code>evaluate_subset</code> for all these
combinations <em>in parallel</em>, speeding up the process
significantly. å®ƒä½¿ç”¨ <code>Parallel</code> å’Œ
<code>delayed</code>ï¼ˆæ¥è‡ª
<code>joblib</code>ï¼‰å¯¹æ‰€æœ‰è¿™äº›ç»„åˆ<em>å¹¶è¡Œ</em>è¿è¡Œ
<code>evaluate_subset</code>ï¼Œä»è€Œæ˜¾è‘—åŠ å¿«äº†å¤„ç†é€Ÿåº¦ã€‚</li>
<li>It collects all the results and returns
them.å®ƒæ”¶é›†æ‰€æœ‰ç»“æœå¹¶è¿”å›ã€‚</li>
</ul></li>
</ul>
<h3 id="analysis-of-the-output">Analysis of the Output</h3>
<ul>
<li><strong>Slide <code>...221255.png</code> (Original Data):</strong>
<ul>
<li>The code runs subset selection on the original dataset.</li>
<li>The â€œTop 20 Best Feature Subsetsâ€ are shown. The CV scores are
negative (they are <code>neg_mean_squared_error</code>), so the scores
<em>closest to zero</em> (smallest magnitude) are best.</li>
<li>The <strong>Best feature subset</strong> is found to be
<code>('Income', 'Limit', 'Rating', 'Student')</code>.</li>
<li>The final cross-validated RMSE for this model is
<strong>105.41</strong>.</li>
</ul></li>
<li><strong>Slide <code>...221309.png</code> (Data with 20 Noisy
Variables):</strong>
<ul>
<li>The code is re-run after adding 20 useless â€œNoisyâ€ features.</li>
<li>The algorithm <em>still</em> works. It correctly identifies that the
â€œNoisyâ€ variables are useless.</li>
<li>The <strong>Best feature subset</strong> is now
<code>('Income', 'Limit', 'Student')</code>. (Note: â€˜Ratingâ€™ was
dropped, likely because itâ€™s highly correlated with â€˜Limitâ€™, and the
noisy data made the simpler model perform slightly better in CV).</li>
<li>The final RMSE is <strong>114.94</strong>. This is <em>higher</em>
than the original 105.41, which is expectedâ€”the presence of so many
noise variables makes the selection problem harder, but the final model
is still good and, most importantly, <em>it successfully excluded all 20
noisy features</em>. æœ€ç»ˆçš„ RMSE ä¸º <strong>114.94</strong>ã€‚è¿™æ¯”æœ€åˆçš„
105.41<em>æ›´é«˜</em>ï¼Œè¿™æ˜¯é¢„æœŸçš„â€”â€”å¦‚æ­¤å¤šçš„å™ªå£°å˜é‡çš„å­˜åœ¨ä½¿å¾—é€‰æ‹©é—®é¢˜æ›´åŠ å›°éš¾ï¼Œä½†æœ€ç»ˆæ¨¡å‹ä»ç„¶å¾ˆå¥½ï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œ<em>å®ƒæˆåŠŸåœ°æ’é™¤äº†æ‰€æœ‰
20 ä¸ªå™ªå£°ç‰¹å¾</em>ã€‚</li>
</ul></li>
</ul>
<h2 id="conceptual-overview-the-why">Conceptual Overview: The â€œWhyâ€</h2>
<p>Slides cover <strong>Chapter 6: Linear Model Selection and
Regularization</strong>, which is all about a fundamental trade-off in
machine learning: the <strong>bias-variance trade-off</strong>.
è¯¥éƒ¨åˆ†ä¸»è¦è®¨è®ºæœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªåŸºæœ¬æƒè¡¡ï¼š<strong>åå·®-æ–¹å·®æƒè¡¡</strong>ã€‚</p>
<ul>
<li><p><strong>The Problem (Slide <code>...221320.png</code>):</strong>
Imagine you have a dataset with 50 predictors (<span
class="math inline">\(p=50\)</span>). You want to predict a response
<span class="math inline">\(y\)</span>. å‡è®¾ä½ æœ‰ä¸€ä¸ªåŒ…å« 50
ä¸ªé¢„æµ‹å˜é‡ï¼ˆp=50ï¼‰çš„æ•°æ®é›†ã€‚ä½ æƒ³è¦é¢„æµ‹å“åº” <span
class="math inline">\(y\)</span>ã€‚</p>
<ul>
<li><strong>Model 1 (Full Model):</strong> You use all 50 predictors.
This model is very <strong>flexible</strong>. It will fit the
<em>training data</em> extremely well, resulting in a low
<strong>bias</strong>. However, itâ€™s highly likely that many of those 50
predictors are just â€œnoiseâ€ (random, unrelated variables). By fitting to
this noise, the model will be <strong>overfit</strong>. When you show it
new, unseen data (the <em>test data</em>), it will perform poorly. This
is called <strong>high variance</strong>. ä½ ä½¿ç”¨äº†æ‰€æœ‰ 50
ä¸ªé¢„æµ‹å˜é‡ã€‚è¿™ä¸ªæ¨¡å‹éå¸¸<strong>çµæ´»</strong>ã€‚å®ƒèƒ½å¾ˆå¥½åœ°æ‹Ÿåˆ<em>è®­ç»ƒæ•°æ®</em>ï¼Œä»è€Œäº§ç”Ÿè¾ƒä½çš„<strong>åå·®</strong>ã€‚ç„¶è€Œï¼Œè¿™
50
ä¸ªé¢„æµ‹å˜é‡ä¸­å¾ˆå¯èƒ½æœ‰å¾ˆå¤šåªæ˜¯â€œå™ªå£°â€ï¼ˆéšæœºçš„ã€ä¸ç›¸å…³çš„å˜é‡ï¼‰ã€‚ç”±äºæ‹Ÿåˆè¿™äº›å™ªå£°ï¼Œæ¨¡å‹ä¼š<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚å½“ä½ å‘å®ƒå±•ç¤ºæ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ï¼ˆ<em>æµ‹è¯•æ•°æ®</em>ï¼‰æ—¶ï¼Œå®ƒçš„è¡¨ç°ä¼šå¾ˆå·®ã€‚è¿™è¢«ç§°ä¸º<strong>é«˜æ–¹å·®</strong>ã€‚</li>
<li><strong>Model 2 (Subset Model):</strong> You intelligently select
only the 3 predictors (<span class="math inline">\(q=3\)</span>) that
are <em>actually</em> related to <span class="math inline">\(y\)</span>.
This model is less flexible. It wonâ€™t fit the <em>training data</em> as
perfectly as Model 1 (it has higher <strong>bias</strong>). But, because
itâ€™s <em>not</em> fitting the noise, it will generalize much better to
new data. It will have a much lower <strong>variance</strong>, and thus
a lower overall <em>test error</em>. ä½ æ™ºèƒ½åœ°åªé€‰æ‹©ä¸ <span
class="math inline">\(y\)</span> <em>çœŸæ­£</em>ç›¸å…³çš„ 3 ä¸ªé¢„æµ‹å˜é‡ (<span
class="math inline">\(q=3\)</span>)ã€‚è¿™ä¸ªæ¨¡å‹çš„çµæ´»æ€§è¾ƒå·®ã€‚å®ƒå¯¹
<em>è®­ç»ƒæ•°æ®</em> çš„æ‹Ÿåˆåº¦ä¸å¦‚æ¨¡å‹ 1
å®Œç¾ï¼ˆå®ƒçš„<strong>åå·®</strong>æ›´é«˜ï¼‰ã€‚ä½†æ˜¯ï¼Œç”±äºå®ƒå¯¹å™ªå£°çš„æ‹Ÿåˆåº¦æ›´é«˜ï¼Œå› æ­¤å¯¹æ–°æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ä¼šæ›´å¥½ã€‚å®ƒçš„<strong>æ–¹å·®</strong>ä¼šæ›´ä½ï¼Œå› æ­¤æ€»ä½“çš„<em>æµ‹è¯•è¯¯å·®</em>ä¹Ÿä¼šæ›´ä½ã€‚</li>
</ul></li>
<li><p><strong>The Goal:</strong> The goal is to find the model that has
the <strong>lowest test error</strong>. We need a formal method to
<em>find</em> the best subset (like Model 2) without just guessing.
<strong>ç›®æ ‡æ˜¯æ‰¾åˆ°</strong>æµ‹è¯•è¯¯å·®**æœ€ä½çš„æ¨¡å‹ã€‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ­£å¼çš„æ–¹æ³•æ¥<em>æ‰¾åˆ°</em>æœ€ä½³å­é›†ï¼ˆä¾‹å¦‚æ¨¡å‹
2ï¼‰ï¼Œè€Œä¸æ˜¯ä»…ä»…é çŒœæµ‹ã€‚</p></li>
<li><p><strong>Two Main Strategies (Slide
<code>...221314.png</code>):</strong></p>
<ol type="1">
<li><p><strong>Subset Selection (Section 6.1):</strong> This is what
weâ€™re focused on. Itâ€™s an â€œall-or-nothingâ€ approach. You either
<em>keep</em> a variable in the model or you <em>discard</em> it
completely. The â€œBest Subset Selectionâ€ algorithm is the most extreme,
â€œbrute-forceâ€ way to do this.
æ˜¯æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹ã€‚è¿™æ˜¯ä¸€ç§â€œå…¨æœ‰æˆ–å…¨æ— â€çš„æ–¹æ³•ã€‚ä½ è¦ä¹ˆåœ¨æ¨¡å‹ä¸­â€œä¿ç•™â€ä¸€ä¸ªå˜é‡ï¼Œè¦ä¹ˆâ€œå½»åº•ä¸¢å¼ƒâ€å®ƒã€‚â€œæœ€ä½³å­é›†é€‰æ‹©â€ç®—æ³•æ˜¯æœ€æç«¯ã€æœ€â€œæš´åŠ›â€çš„åšæ³•ã€‚</p></li>
<li><p><strong>Shrinkage/Regularization (Section 6.2):</strong> This is
a more subtle approach (e.g., Ridge Regression, LASSO). Instead of
discarding variables, you <em>keep all <span
class="math inline">\(p\)</span> variables</em> but add a penalty to the
model that â€œshrinksâ€ the coefficients (<span
class="math inline">\(\beta\)</span>) of the useless variables towards
zero.
è¿™æ˜¯ä¸€ç§æ›´å·§å¦™çš„æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œå²­å›å½’ã€LASSOï¼‰ã€‚ä½ ä¸æ˜¯ä¸¢å¼ƒå˜é‡ï¼Œè€Œæ˜¯<em>ä¿ç•™æ‰€æœ‰
<span class="math inline">\(p\)</span>
ä¸ªå˜é‡</em>ï¼Œä½†ä¼šç»™æ¨¡å‹æ·»åŠ ä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œå°†æ— ç”¨å˜é‡çš„ç³»æ•°ï¼ˆ<span
class="math inline">\(\beta\)</span>ï¼‰â€œæ”¶ç¼©â€åˆ°é›¶ã€‚</p></li>
</ol></li>
</ul>
<h2 id="questions">Questions ğŸ¯</h2>
<h3 id="q1-how-to-compare-which-model-is-better">Q1: â€œHow to compare
which model is better?â€</h3>
<p>(From slides <code>...221320.png</code> and
<code>...221326.png</code>)</p>
<p>This is the most important question. You <strong>cannot</strong> use
metrics based on <em>training data</em> (like <span
class="math inline">\(R^2\)</span> or RSS - Residual Sum of Squares) to
compare models with <em>different numbers of predictors</em>.
è¿™æ˜¯æœ€é‡è¦çš„é—®é¢˜ã€‚æ‚¨<strong>ä¸èƒ½</strong>ä½¿ç”¨åŸºäº<em>è®­ç»ƒæ•°æ®</em>çš„æŒ‡æ ‡ï¼ˆä¾‹å¦‚
R^2 æˆ– RSS - æ®‹å·®å¹³æ–¹å’Œï¼‰æ¥æ¯”è¾ƒå…·æœ‰<em>ä¸åŒæ•°é‡é¢„æµ‹å˜é‡</em>çš„æ¨¡å‹ã€‚</p>
<ul>
<li><p><strong>The Trap:</strong> A model with more predictors will
<em>always</em> have a higher <span class="math inline">\(R^2\)</span>
(or lower RSS) on the data it was trained on. <span
class="math inline">\(R^2\)</span> will <em>always</em> increase as you
add variables, even if they are pure noise. If you used <span
class="math inline">\(R^2\)</span> to compare a 3-predictor model to a
10-predictor model, the 10-predictor model would <em>always</em> look
better on paper, even if itâ€™s terribly overfit.
å…·æœ‰æ›´å¤šé¢„æµ‹å˜é‡çš„æ¨¡å‹åœ¨å…¶è®­ç»ƒæ•°æ®ä¸Š<em>æ€»æ˜¯</em>å…·æœ‰æ›´é«˜çš„
R^2ï¼ˆæˆ–æ›´ä½çš„ RSSï¼‰ã€‚éšç€å˜é‡çš„å¢åŠ ï¼ŒR^2
ä¼š<em>æ€»æ˜¯</em>å¢åŠ ï¼Œå³ä½¿è¿™äº›å˜é‡æ˜¯çº¯å™ªå£°ã€‚å¦‚æœæ‚¨ä½¿ç”¨ R^2 æ¥æ¯”è¾ƒ 3
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹å’Œ 10 ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œé‚£ä¹ˆ 10
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹åœ¨çº¸é¢ä¸Š<em>æ€»æ˜¯</em>çœ‹èµ·æ¥æ›´å¥½ï¼Œå³ä½¿å®ƒä¸¥é‡è¿‡æ‹Ÿåˆã€‚</p></li>
<li><p><strong>The Correct Way:</strong> You must use a metric that
estimates the <strong>test error</strong>. The slides and code show two
ways:æ‚¨å¿…é¡»ä½¿ç”¨ä¸€ä¸ªèƒ½å¤Ÿä¼°è®¡<strong>æµ‹è¯•è¯¯å·®</strong>çš„æŒ‡æ ‡ã€‚</p>
<ol type="1">
<li><strong>Cross-Validation (CV):</strong> This is the method used in
your Python code. It works by:
<ul>
<li>Splitting your training data into <span
class="math inline">\(k\)</span> â€œfoldsâ€ (e.g., 5 folds).
å°†è®­ç»ƒæ•°æ®æ‹†åˆ†æˆ <span class="math inline">\(k\)</span> ä¸ªâ€œæŠ˜å â€ï¼ˆä¾‹å¦‚ 5
ä¸ªæŠ˜å ï¼‰ã€‚</li>
<li>Training the model on 4 folds and testing it on the 5th fold.
ä½¿ç”¨å…¶ä¸­ 4 ä¸ªæŠ˜å è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ç¬¬ 5 ä¸ªæŠ˜å è¿›è¡Œæµ‹è¯•ã€‚</li>
<li>Repeating this 5 times, so each fold gets to be the test set once.
é‡å¤æ­¤æ“ä½œ 5 æ¬¡ï¼Œä½¿æ¯ä¸ªæŠ˜å éƒ½ä½œä¸ºæµ‹è¯•é›†ä¸€æ¬¡ã€‚</li>
<li>Averaging the 5 test errors. å¯¹ 5 ä¸ªæµ‹è¯•è¯¯å·®æ±‚å¹³å‡å€¼ã€‚ This gives
you a robust estimate of how your model will perform on <em>unseen
data</em>. You then choose the model with the best (lowest) average CV
error.
è¿™å¯ä»¥è®©ä½ å¯¹æ¨¡å‹åœ¨<em>æœªè§æ•°æ®</em>ä¸Šçš„è¡¨ç°æœ‰ä¸€ä¸ªç¨³å¥çš„ä¼°è®¡ã€‚ç„¶åï¼Œä½ å¯ä»¥é€‰æ‹©å¹³å‡
CV è¯¯å·®æœ€å°ï¼ˆæœ€ä½³ï¼‰çš„æ¨¡å‹ã€‚</li>
</ul></li>
<li><strong>Mathematical Adjustments (AIC, BIC, Adjusted <span
class="math inline">\(R^2\)</span>):</strong> These are formulas that
take the training error (like RSS) and add a <em>penalty</em> for each
predictor (<span class="math inline">\(k\)</span>) you add.
<ul>
<li><span class="math inline">\(AIC \approx RSS +
2k\sigma^2\)</span></li>
<li><span class="math inline">\(BIC \approx RSS +
\log(n)k\sigma^2\)</span> A model with more predictors (larger <span
class="math inline">\(k\)</span>) gets a bigger penalty. To be chosen, a
more complex model must <em>significantly</em> improve the RSS to
overcome this penalty. é¢„æµ‹å˜é‡è¶Šå¤šï¼ˆk
è¶Šå¤§ï¼‰çš„æ¨¡å‹ï¼Œæƒ©ç½šè¶Šå¤§ã€‚è¦è¢«é€‰ä¸­ï¼Œæ›´å¤æ‚çš„æ¨¡å‹å¿…é¡»<em>æ˜¾è‘—</em>æå‡ RSS
ä»¥å…‹æœæ­¤æƒ©ç½šã€‚</li>
</ul></li>
</ol></li>
</ul>
<h3 id="q2-why-using-r2-for-step-2">Q2: â€œWhy using <span
class="math inline">\(R^2\)</span> for step 2?â€</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p><strong>Step 2</strong> of the â€œBest Subset Selectionâ€ algorithm
says: â€œFor <span class="math inline">\(k = 1, \dots, p\)</span>: Fit all
<span class="math inline">\(\binom{p}{k}\)</span> modelsâ€¦ Pick the best
model, that with the largest <span class="math inline">\(R^2\)</span>, â€¦
and call it <span class="math inline">\(M_k\)</span>.â€ â€œå¯¹äº <span
class="math inline">\(k = 1, \dots, p\)</span>ï¼šæ‹Ÿåˆæ‰€æœ‰ <span
class="math inline">\(\binom{p}{k}\)</span> ä¸ªæ¨¡å‹â€¦â€¦é€‰æ‹©å…·æœ‰æœ€å¤§ <span
class="math inline">\(R^2\)</span> çš„æœ€ä½³æ¨¡å‹â€¦â€¦å¹¶å°†å…¶å‘½åä¸º <span
class="math inline">\(M_k\)</span>ã€‚â€</p>
<ul>
<li><strong>The Reason:</strong> In Step 2, you are <em>only</em>
comparing models <strong>of the same size</strong>. For example, when
<span class="math inline">\(k=3\)</span>, you are comparing all possible
3-predictor models: æ­¥éª¤ 2
ä¸­ï¼Œæ‚¨<em>ä»…</em>æ¯”è¾ƒ**ç›¸åŒå¤§å°çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå½“ <span
class="math inline">\(k=3\)</span> æ—¶ï¼Œæ‚¨å°†æ¯”è¾ƒæ‰€æœ‰å¯èƒ½çš„ 3
é¢„æµ‹å˜é‡æ¨¡å‹ï¼š
<ul>
<li>Model A: (<span class="math inline">\(X_1, X_2, X_3\)</span>)</li>
<li>Model B: (<span class="math inline">\(X_1, X_2, X_4\)</span>)</li>
<li>Model C: (<span class="math inline">\(X_1, X_3, X_5\)</span>)</li>
<li>â€¦and so on.</li>
</ul>
Since all these models have the <em>exact same complexity</em> (they all
have <span class="math inline">\(k=3\)</span> predictors), there is no
risk of unfairly favoring a more complex model. Therefore, you are free
to use a training metric like <span class="math inline">\(R^2\)</span>
(or RSS). The model with the highest <span
class="math inline">\(R^2\)</span> is, by definition, the one that
<em>best fits the training data</em> for that specific size <span
class="math inline">\(k\)</span>.
ç”±äºæ‰€æœ‰è¿™äº›æ¨¡å‹éƒ½å…·æœ‰<em>å®Œå…¨ç›¸åŒçš„å¤æ‚åº¦</em>ï¼ˆå®ƒä»¬éƒ½å…·æœ‰ <span
class="math inline">\(k=3\)</span>
ä¸ªé¢„æµ‹å˜é‡ï¼‰ï¼Œå› æ­¤ä¸å­˜åœ¨ä¸å…¬å¹³åœ°åå‘æ›´å¤æ‚æ¨¡å‹çš„é£é™©ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥è‡ªç”±ä½¿ç”¨åƒ
<span class="math inline">\(R^2\)</span>ï¼ˆæˆ–
RSSï¼‰è¿™æ ·çš„è®­ç»ƒæŒ‡æ ‡ã€‚æ ¹æ®å®šä¹‰ï¼Œå…·æœ‰æœ€é«˜ <span
class="math inline">\(R^2\)</span> çš„æ¨¡å‹å°±æ˜¯åœ¨ç‰¹å®šå¤§å° <span
class="math inline">\(k\)</span>
ä¸‹<em>ä¸è®­ç»ƒæ•°æ®æ‹Ÿåˆåº¦</em>æœ€é«˜çš„æ¨¡å‹ã€‚</li>
</ul>
<h3
id="q3-cannot-use-training-error-in-step-3.-why-not-æ­¥éª¤-3-ä¸­ä¸èƒ½ä½¿ç”¨è®­ç»ƒè¯¯å·®-ä¸ºä»€ä¹ˆ">Q3:
â€œCannot use training error in Step 3.â€ Why not? â€œæ­¥éª¤ 3
ä¸­ä¸èƒ½ä½¿ç”¨è®­ç»ƒè¯¯å·®ã€‚â€ ä¸ºä»€ä¹ˆï¼Ÿ</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p><strong>Step 3</strong> says: â€œSelect a single best model from <span
class="math inline">\(M_0, M_1, \dots, M_p\)</span> by cross validation,
AIC, or BIC.â€â€œé€šè¿‡äº¤å‰éªŒè¯ã€AIC æˆ– BICï¼Œä» <span
class="math inline">\(M_0ã€M_1ã€\dotsã€M_p\)</span>
ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä½³æ¨¡å‹ã€‚â€</p>
<ul>
<li><p><strong>The Reason:</strong> In Step 3, you are now comparing
models <strong>of different sizes</strong>. You are comparing the best
1-predictor model (<span class="math inline">\(M_1\)</span>) vs.Â the
best 2-predictor model (<span class="math inline">\(M_2\)</span>)
vs.Â the best 3-predictor model (<span
class="math inline">\(M_3\)</span>), and so on, all the way up to <span
class="math inline">\(M_p\)</span>. åœ¨æ­¥éª¤ 3
ä¸­ï¼Œæ‚¨æ­£åœ¨æ¯”è¾ƒ<strong>ä¸åŒå¤§å°</strong>çš„æ¨¡å‹ã€‚æ‚¨æ­£åœ¨æ¯”è¾ƒæœ€ä½³çš„å•é¢„æµ‹æ¨¡å‹
(<span class="math inline">\(M_1\)</span>)ã€æœ€ä½³çš„åŒé¢„æµ‹æ¨¡å‹ (<span
class="math inline">\(M_2\)</span>) å’Œæœ€ä½³çš„ä¸‰é¢„æµ‹æ¨¡å‹ (<span
class="math inline">\(M_3\)</span>)ï¼Œä¾æ­¤ç±»æ¨ï¼Œç›´åˆ° <span
class="math inline">\(M_p\)</span>ã€‚</p>
<p>As explained in Q1, if you used a training error metric like <span
class="math inline">\(R^2\)</span> here, the <span
class="math inline">\(R^2\)</span> would just keep going up, and you
would <em>always</em> select the largest, most complex model, <span
class="math inline">\(M_p\)</span>. This completely defeats the purpose
of model selection. å¦‚é—®é¢˜ 1 æ‰€è¿°ï¼Œå¦‚æœæ‚¨åœ¨æ­¤å¤„ä½¿ç”¨åƒ <span
class="math inline">\(R^2\)</span> è¿™æ ·çš„è®­ç»ƒè¯¯å·®æŒ‡æ ‡ï¼Œé‚£ä¹ˆ <span
class="math inline">\(R^2\)</span>
ä¼šæŒç»­ä¸Šå‡ï¼Œå¹¶ä¸”æ‚¨<em>æ€»æ˜¯</em>ä¼šé€‰æ‹©æœ€å¤§ã€æœ€å¤æ‚çš„æ¨¡å‹ <span
class="math inline">\(M_p\)</span>ã€‚è¿™å®Œå…¨è¿èƒŒäº†æ¨¡å‹é€‰æ‹©çš„ç›®çš„ã€‚</p>
<p>Therefore, in Step 3, you <em>must</em> use a method that estimates
<strong>test error</strong> (like Cross-Validation) or one that
<strong>penalizes for complexity</strong> (like AIC or BIC) to find the
â€œsweet spotâ€ model that balances fit and simplicity. å› æ­¤ï¼Œåœ¨æ­¥éª¤ 3
ä¸­ï¼Œæ‚¨<em>å¿…é¡»</em>ä½¿ç”¨ä¸€ç§ä¼°ç®—<strong>æµ‹è¯•è¯¯å·®</strong>çš„æ–¹æ³•ï¼ˆä¾‹å¦‚äº¤å‰éªŒè¯ï¼‰æˆ–<strong>æƒ©ç½šå¤æ‚æ€§</strong>çš„æ–¹æ³•ï¼ˆä¾‹å¦‚
AIC æˆ–
BICï¼‰ï¼Œä»¥æ‰¾åˆ°åœ¨æ‹Ÿåˆåº¦å’Œç®€å•æ€§ä¹‹é—´å–å¾—å¹³è¡¡çš„â€œæœ€ä½³ç‚¹â€æ¨¡å‹ã€‚</p></li>
</ul>
<h2 id="mathematical-deep-dive">Mathematical Deep Dive ğŸ§®</h2>
<ul>
<li><strong><span class="math inline">\(Y = \beta_0 + \beta_1X_1 + \dots
+ \beta_pX_p + \epsilon\)</span>:</strong> The full linear model. The
goal of subset selection is to find a subset of <span
class="math inline">\(X_j\)</span>â€™s where <span
class="math inline">\(\beta_j \neq 0\)</span> and set all other <span
class="math inline">\(\beta\)</span>â€™s to 0.
å®Œæ•´çš„çº¿æ€§æ¨¡å‹ã€‚å­é›†é€‰æ‹©çš„ç›®æ ‡æ˜¯æ‰¾åˆ° <span
class="math inline">\(X_j\)</span> çš„ä¸€ä¸ªå­é›†ï¼Œå…¶ä¸­ $_j ç­‰äº
0ï¼Œå¹¶å°†æ‰€æœ‰å…¶ä»– <span class="math inline">\(\beta\)</span> è®¾ç½®ä¸º
0ã€‚</li>
<li><strong><span class="math inline">\(2^p\)</span>
combinations:</strong> (Slide <code>...221333.png</code>) This is the
total number of models you have to check. For each of the <span
class="math inline">\(p\)</span> variables, you have two choices: either
it is <strong>IN</strong> the model or it is
<strong>OUT</strong>.è¿™æ˜¯ä½ éœ€è¦æ£€æŸ¥çš„æ¨¡å‹æ€»æ•°ã€‚å¯¹äºæ¯ä¸ª <span
class="math inline">\(p\)</span>
ä¸ªå˜é‡ï¼Œä½ æœ‰ä¸¤ä¸ªé€‰æ‹©ï¼šè¦ä¹ˆå®ƒåœ¨æ¨¡å‹<strong>å†…éƒ¨</strong>ï¼Œè¦ä¹ˆå®ƒåœ¨æ¨¡å‹<strong>å¤–éƒ¨</strong>ã€‚
<ul>
<li>Example: <span class="math inline">\(p=3\)</span> (variables <span
class="math inline">\(X_1, X_2, X_3\)</span>)</li>
<li>The <span class="math inline">\(2^3 = 8\)</span> possible models
are:
<ol type="1">
<li>{} (The null model, <span class="math inline">\(M_0\)</span>)</li>
<li>{ <span class="math inline">\(X_1\)</span> }</li>
<li>{ <span class="math inline">\(X_2\)</span> }</li>
<li>{ <span class="math inline">\(X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_2\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_2, X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_2, X_3\)</span> } (The full
model, <span class="math inline">\(M_3\)</span>)</li>
</ol></li>
<li>This is why this method is called an <strong>â€œexhaustive
searchâ€</strong>. It literally checks every single one. For <span
class="math inline">\(p=20\)</span>, <span
class="math inline">\(2^{20}\)</span> is over a million
models!è¿™å°±æ˜¯è¯¥æ–¹æ³•è¢«ç§°ä¸º<strong>â€œç©·ä¸¾æœç´¢â€</strong>çš„åŸå› ã€‚å®ƒå®é™…ä¸Šä¼šæ£€æŸ¥æ¯ä¸€ä¸ªæ¨¡å‹ã€‚å¯¹äº
<span class="math inline">\(p=20\)</span>ï¼Œ<span
class="math inline">\(2^{20}\)</span> å°±è¶…è¿‡ä¸€ç™¾ä¸‡ä¸ªæ¨¡å‹ï¼</li>
</ul></li>
<li><strong><span class="math inline">\(\binom{p}{k} =
\frac{p!}{k!(p-k)!}\)</span>:</strong> (Slide
<code>...221333.png</code>) This is the â€œcombinationsâ€ formula. It tells
you <em>how many</em> models you fit <em>in Step 2</em> for a specific
<span
class="math inline">\(k\)</span>.è¿™æ˜¯â€œç»„åˆâ€å…¬å¼ã€‚å®ƒå‘Šè¯‰ä½ ï¼Œå¯¹äºç‰¹å®šçš„
<span class="math inline">\(k\)</span>ï¼Œ<em>åœ¨æ­¥éª¤ 2</em>ä¸­ï¼Œä½ æ‹Ÿåˆäº†
<em>å¤šå°‘</em> ä¸ªæ¨¡å‹ã€‚
<ul>
<li>Example: <span class="math inline">\(p=10\)</span> total
predictors.</li>
<li>For <span class="math inline">\(k=1\)</span>: You fit <span
class="math inline">\(\binom{10}{1} = 10\)</span> models.</li>
<li>For <span class="math inline">\(k=2\)</span>: You fit <span
class="math inline">\(\binom{10}{2} = \frac{10 \times 9}{2 \times 1} =
45\)</span> models.</li>
<li>For <span class="math inline">\(k=3\)</span>: You fit <span
class="math inline">\(\binom{10}{3} = \frac{10 \times 9 \times 8}{3
\times 2 \times 1} = 120\)</span> models.</li>
<li>â€¦and so on. The sum of all these <span
class="math inline">\(\binom{p}{k}\)</span> from <span
class="math inline">\(k=0\)</span> to <span
class="math inline">\(k=p\)</span> equals <span
class="math inline">\(2^p\)</span>.</li>
</ul></li>
</ul>
<h2 id="detailed-code-analysis">Detailed Code Analysis ğŸ’»</h2>
<p>Your slides show Python code that applies the <strong>Best Subset
Selection algorithm</strong> to a <strong>KNN Regressor</strong>. This
is a great example of how the <em>selection algorithm</em> is
independent of the <em>model type</em> (as mentioned in slide
<code>...221314.png</code>).</p>
<h3 id="key-functions-1">Key Functions</h3>
<ul>
<li><strong><code>main()</code></strong>
<ol type="1">
<li><strong>Load &amp; Preprocess:</strong> Reads
<code>Credit.csv</code>. The most important step here is converting
categorical text (like â€˜Maleâ€™/â€˜Femaleâ€™) into numbers (1/0).</li>
<li><strong>Scale Data:</strong> <code>scaler = StandardScaler()</code>
and <code>X_scaled = scaler.fit_transform(X)</code>.
<ul>
<li><strong>WHY?</strong> This is <strong>CRITICAL</strong> for KNN. KNN
works by measuring distance. If â€˜Incomeâ€™ (e.g., 50,000) is on a vastly
different scale than â€˜Cardsâ€™ (e.g., 3), the â€˜Incomeâ€™ feature will
completely dominate the distance calculation, making â€˜Cardsâ€™ irrelevant.
Scaling resizes all features to have a mean of 0 and standard deviation
of 1, so they all contribute fairly.</li>
</ul></li>
<li><strong>Handle Noisy Data (Slide
<code>...221303.jpg</code>):</strong> This version of the code
<em>intentionally</em> adds 20 columns of useless, random numbers. This
is a test to see if the algorithm is smart enough to ignore them.</li>
<li><strong>Run Selection:</strong>
<code>results_df = best_subset_selection_parallel(...)</code>. This
function does all the heavy lifting (explained next).</li>
<li><strong>Find Best Model:</strong>
<code>results_df.sort_values(by='CV_Score', ascending=False)</code>.
<ul>
<li><strong>WHY <code>ascending=False</code>?</strong> The code uses the
metric <code>'neg_mean_squared_error'</code>. This is MSE, but negative
(e.g., -15000). A <em>better</em> model has an error closer to 0 (e.g.,
-10000). Since -10000 is <em>greater than</em> -15000, you sort in
descending (high-to-low) order to put the best models at the top.</li>
</ul></li>
<li><strong>Final Evaluation (Step 3):</strong>
<code>final_scores = cross_val_score(knn, X_best, y, ...)</code>
<ul>
<li>This is the implementation of Step 3. It takes <em>only</em> the
single best subset (<code>X_best</code>) and runs a <em>new</em>
cross-validation on it. This gives a final, unbiased estimate of how
good that one model is.</li>
</ul></li>
<li><strong>Print RMSE:</strong>
<code>final_rmse = np.sqrt(-final_scores)</code>. It converts the
negative MSE back into a positive RMSE (Root Mean Squared Error), which
is in the same units as the target <span
class="math inline">\(y\)</span> (in this case, â€˜Balanceâ€™ in
dollars).</li>
</ol></li>
<li><strong><code>best_subset_selection_parallel(model, ...)</code></strong>
<ol type="1">
<li>This is the â€œmanagerâ€ function. It implements the loop from Step
2.</li>
<li><code>for k in range(1, n_features + 1):</code> This is the loop
â€œFor <span class="math inline">\(k = 1, \dots, p\)</span>â€.</li>
<li><code>subsets = list(combinations(feature_names, k))</code>: This
generates the <span class="math inline">\(\binom{p}{k}\)</span>
combinations for the current <span
class="math inline">\(k\)</span>.</li>
<li><code>results = Parallel(n_jobs=n_jobs)(...)</code>: This is a
non-core, â€œspeed-upâ€ command. It uses the <code>joblib</code> library to
run the evaluations on all your computerâ€™s CPU cores at once (in
parallel). Without this, checking millions of models would take
days.</li>
<li><code>subset_scores = ... [delayed(evaluate_subset)(...) ...]</code>
This line farms out the <em>actual work</em> to the
<code>evaluate_subset</code> function for every single subset.</li>
</ol></li>
<li><strong><code>evaluate_subset(subset, ...)</code></strong>
<ol type="1">
<li>This is the â€œworkerâ€ function. It gets called thousands or millions
of times.</li>
<li>Its job is to evaluate <em>one single subset</em> (e.g.,
<code>('Income', 'Limit', 'Student')</code>).</li>
<li><code>X_subset = X[list(subset)]</code>: It slices the data to get
<em>only</em> these columns.</li>
<li><code>scores = cross_val_score(model, X_subset, ...)</code>:
<strong>This is the most important line.</strong> It takes the subset
and performs a full 5-fold cross-validation on it.</li>
<li><code>return (subset, np.mean(scores))</code>: It returns the subset
and its average CV score.</li>
</ol></li>
</ul>
<h3 id="summary-of-outputs-slides-...221255.png-...221309.png">Summary
of Outputs (Slides <code>...221255.png</code> &amp;
<code>...221309.png</code>)</h3>
<ul>
<li><strong>Original Data (Slide <code>...221255.png</code>):</strong>
<ul>
<li><strong>Best Subset:</strong>
<code>('Income', 'Limit', 'Rating', 'Student')</code></li>
<li><strong>Final RMSE:</strong> ~105.4</li>
</ul></li>
<li><strong>Data with 20 â€œNoisyâ€ Variables (Slide
<code>...221309.png</code>):</strong>
<ul>
<li><strong>Best Subset:</strong>
<code>('Income', 'Limit', 'Student')</code></li>
<li><strong>Result:</strong> The algorithm <em>successfully</em>
identified that all 20 â€œNoisyâ€ variables were useless and
<strong>excluded every single one of them</strong> from the best
models.</li>
<li><strong>Final RMSE:</strong> ~114.9</li>
<li><strong>Key Takeaway:</strong> The RMSE is slightly higher, which
makes sense because the selection problem was much harder. But the
<em>method worked perfectly</em>. It filtered all the â€œnoiseâ€ and found
a simple, powerful model, just as the theory on slide
<code>...221320.png</code> predicted.</li>
</ul></li>
</ul>
<h1
id="the-core-problem-training-error-vs.-test-error-æ ¸å¿ƒé—®é¢˜è®­ç»ƒè¯¯å·®-vs.-æµ‹è¯•è¯¯å·®">2.
The Core Problem: Training Error vs.Â Test Error æ ¸å¿ƒé—®é¢˜ï¼šè®­ç»ƒè¯¯å·®
vs.Â æµ‹è¯•è¯¯å·®</h1>
<p>The central theme of these slides is finding the â€œbestâ€ model. The
problem is that a model with more predictors (more complex) will
<em>always</em> fit the data it was trained on better. This is a trap.
å¯»æ‰¾â€œæœ€ä½³â€æ¨¡å‹ã€‚é—®é¢˜åœ¨äºï¼Œé¢„æµ‹å› å­è¶Šå¤šï¼ˆè¶Šå¤æ‚ï¼‰çš„æ¨¡å‹<em>æ€»æ˜¯</em>èƒ½æ›´å¥½åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚è¿™æ˜¯ä¸€ä¸ªé™·é˜±ã€‚</p>
<ul>
<li><strong>Training Error:</strong> How well the model fits the data we
used to build it. <strong><span class="math inline">\(R^2\)</span> and
<span class="math inline">\(RSS\)</span> measure this.</strong>
æ¨¡å‹ä¸æˆ‘ä»¬æ„å»ºæ¨¡å‹æ—¶æ‰€ç”¨æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ã€‚<strong><span
class="math inline">\(R^2\)</span> å’Œ <span
class="math inline">\(RSS\)</span> è¡¡é‡äº†è¿™ä¸€ç‚¹ã€‚</strong></li>
<li><strong>Test Error:</strong> How well the model predicts new, unseen
data. This is what we <em>actually</em> care about. A model that is too
complex (e.g., has 10 predictors when only 3 are useful) will have low
training error but very high test error. This is called
<strong>overfitting</strong>.
æ¨¡å‹é¢„æµ‹æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®çš„å‡†ç¡®ç¨‹åº¦ã€‚è¿™æ‰æ˜¯æˆ‘ä»¬<em>çœŸæ­£</em>å…³å¿ƒçš„ã€‚è¿‡äºå¤æ‚çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œæœ‰
10 ä¸ªé¢„æµ‹å› å­ï¼Œä½†åªæœ‰ 3
ä¸ªæœ‰ç”¨ï¼‰çš„è®­ç»ƒè¯¯å·®ä¼šå¾ˆä½ï¼Œä½†æµ‹è¯•è¯¯å·®ä¼šå¾ˆé«˜ã€‚è¿™è¢«ç§°ä¸º<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚</li>
</ul>
<p>The goal is to choose a model that has the lowest <em>test
error</em>. The metrics below (Adjusted <span
class="math inline">\(R^2\)</span>, AIC, BIC) are all attempts to
<em>estimate</em> this test error without having to actually collect new
data. They do this by adding a <strong>penalty</strong> for complexity.
ç›®æ ‡æ˜¯é€‰æ‹©ä¸€ä¸ªå…·æœ‰æœ€ä½<em>æµ‹è¯•è¯¯å·®</em>çš„æ¨¡å‹ã€‚ä»¥ä¸‹æŒ‡æ ‡ï¼ˆè°ƒæ•´åçš„ <span
class="math inline">\(R^2\)</span>ã€AICã€BICï¼‰éƒ½æ˜¯åœ¨æ— éœ€å®é™…æ”¶é›†æ–°æ•°æ®çš„æƒ…å†µä¸‹å°è¯•<em>ä¼°è®¡</em>æ­¤æµ‹è¯•è¯¯å·®ã€‚ä»–ä»¬é€šè¿‡å¢åŠ <strong>å¤æ‚åº¦æƒ©ç½š</strong>æ¥å®ç°è¿™ä¸€ç‚¹ã€‚</p>
<h2 id="basic-metrics-measures-of-fit">Basic Metrics (Measures of
Fit)</h2>
<p>These formulas from slide 13 describe how well a model fits the
<em>training data</em>.</p>
<h3 id="residue-error-æ®‹å·®è¯¯å·®">Residue (Error) æ®‹å·®ï¼ˆè¯¯å·®ï¼‰</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\epsilon}_i = y_i - \hat{y}_i = y_i -
\hat{\beta}_0 - \sum_{j=1}^{p} \hat{\beta}_j x_{ij}\)</span></li>
<li><strong>Concept:</strong> This is the most basic building block.
Itâ€™s the difference between the <em>actual</em> observed value (<span
class="math inline">\(y_i\)</span>) and the value your model
<em>predicted</em> (<span class="math inline">\(\hat{y}_i\)</span>). It
is the â€œerrorâ€ for a single data point.
è¿™æ˜¯æœ€åŸºæœ¬çš„æ„å»ºå—ã€‚å®ƒæ˜¯<em>å®é™…</em>è§‚æµ‹å€¼ (<span
class="math inline">\(y_i\)</span>) ä¸æ¨¡å‹*é¢„æµ‹å€¼ (<span
class="math inline">\(\hat{y}_i\)</span>)
ä¹‹é—´çš„å·®å€¼ã€‚å®ƒæ˜¯å•ä¸ªæ•°æ®ç‚¹çš„â€œè¯¯å·®â€ã€‚</li>
</ul>
<h3 id="residual-sum-of-squares-rss-æ®‹å·®å¹³æ–¹å’Œ-rss">Residual Sum of
Squares (RSS) æ®‹å·®å¹³æ–¹å’Œ (RSS)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(RSS =
\sum_{i=1}^{n} \hat{\epsilon}_i^2\)</span></li>
<li><strong>Concept:</strong> This is the overall measure of model
error. You square all the individual errors (residues) to make them
positive and then add them all up.
è¿™æ˜¯æ¨¡å‹è¯¯å·®çš„æ€»ä½“åº¦é‡ã€‚å°†æ‰€æœ‰å•ä¸ªè¯¯å·®ï¼ˆæ®‹å·®ï¼‰å¹³æ–¹ï¼Œä½¿å…¶ä¸ºæ­£ï¼Œç„¶åå°†å®ƒä»¬å…¨éƒ¨ç›¸åŠ ã€‚</li>
<li><strong>Goal:</strong> The entire process of linear regression
(called â€œOrdinary Least Squaresâ€) is designed to find the <span
class="math inline">\(\hat{\beta}\)</span> coefficients that make this
<strong>RSS value as small as possible</strong>.
æ•´ä¸ªçº¿æ€§å›å½’è¿‡ç¨‹ï¼ˆç§°ä¸ºâ€œæ™®é€šæœ€å°äºŒä¹˜æ³•â€ï¼‰æ—¨åœ¨æ‰¾åˆ°ä½¿<strong>RSS
å€¼å°½å¯èƒ½å°</strong>çš„ <span class="math inline">\(\hat{\beta}\)</span>
ä¸ªç³»æ•°ã€‚</li>
<li><strong>The Flaw ç¼ºé™·:</strong> <span
class="math inline">\(RSS\)</span> will <em>always</em> decrease (or
stay the same) as you add more predictors (<span
class="math inline">\(p\)</span>). A model with all 10 predictors will
have a lower <span class="math inline">\(RSS\)</span> than a model with
9, even if that 10th predictor is useless. Therefore, <span
class="math inline">\(RSS\)</span> is useless for choosing
<em>between</em> models of different sizes. éšç€é¢„æµ‹å˜é‡ (<span
class="math inline">\(p\)</span>) çš„å¢åŠ ï¼Œ<span
class="math inline">\(RSS\)</span>
æ€»æ˜¯ä¼šå‡å°ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ã€‚ä¸€ä¸ªåŒ…å«æ‰€æœ‰ 10 ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹çš„ <span
class="math inline">\(RSS\)</span> ä¼šä½äºä¸€ä¸ªåŒ…å« 9
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œå³ä½¿ç¬¬ 10 ä¸ªé¢„æµ‹å˜é‡æ¯«æ— ç”¨å¤„ã€‚å› æ­¤ï¼Œ<span
class="math inline">\(RSS\)</span>
å¯¹äºåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¹‹é—´è¿›è¡Œé€‰æ‹©æ¯«æ— ç”¨å¤„ã€‚</li>
</ul>
<h3 id="r-squared-r2">R-squared (<span
class="math inline">\(R^2\)</span>)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(R^2 = 1 -
\frac{SS_{error}}{SS_{total}} = 1 - \frac{RSS}{\sum_{i=1}^{n} (y_i -
\bar{y})^2}\)</span></li>
<li><strong>Concept:</strong> This metric reframes <span
class="math inline">\(RSS\)</span> into a more interpretable
percentage.æ­¤æŒ‡æ ‡å°† <span class="math inline">\(RSS\)</span>
é‡æ–°å®šä¹‰ä¸ºæ›´æ˜“äºè§£é‡Šçš„ç™¾åˆ†æ¯”ã€‚
<ul>
<li><span class="math inline">\(SS_{total}\)</span> (the denominator)
represents the <em>total variance</em> of the data. Itâ€™s the error you
would get if your â€œmodelâ€ was just guessing the average value (<span
class="math inline">\(\bar{y}\)</span>) for every single observation.
ï¼ˆåˆ†æ¯ï¼‰è¡¨ç¤ºæ•°æ®çš„<em>æ€»æ–¹å·®</em>ã€‚å¦‚æœä½ çš„â€œæ¨¡å‹â€åªæ˜¯çŒœæµ‹æ¯ä¸ªè§‚æµ‹å€¼çš„å¹³å‡å€¼
(<span
class="math inline">\(\bar{y}\)</span>)ï¼Œé‚£ä¹ˆä½ å°±ä¼šå¾—åˆ°è¿™ä¸ªè¯¯å·®ã€‚</li>
<li><span class="math inline">\(SS_{error}\)</span> (the <span
class="math inline">\(RSS\)</span>) is the error <em>after</em> using
your model. æ˜¯â€œæ¨¡å‹è§£é‡Šçš„æ€»æ–¹å·®çš„æ¯”ä¾‹â€ã€‚ <span
class="math inline">\(R^2\)</span> ä¸º 0.75
æ„å‘³ç€ä½ çš„æ¨¡å‹å¯ä»¥è§£é‡Šå“åº”å˜é‡ 75% çš„å˜å¼‚ã€‚</li>
<li><span class="math inline">\(R^2\)</span> is the â€œproportion of total
variance explained by the model.â€ An <span
class="math inline">\(R^2\)</span> of 0.75 means your model can explain
75% of the variation in the response variable.</li>
</ul></li>
<li><strong>The Flaw ç¼ºé™·:</strong> Just like <span
class="math inline">\(RSS\)</span>, <span
class="math inline">\(R^2\)</span> will <em>always</em> increase (or
stay the same) as you add more predictors. This is visually confirmed in
Figure 6.1, where the red line for <span
class="math inline">\(R^2\)</span> only goes up. It will always pick the
most complex model. ä¸ <span class="math inline">\(RSS\)</span>
ä¸€æ ·ï¼Œéšç€é¢„æµ‹å˜é‡çš„å¢åŠ ï¼Œ<span class="math inline">\(R^2\)</span>
ä¼š<em>å§‹ç»ˆ</em>å¢åŠ ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ã€‚å›¾ 6.1 ç›´è§‚åœ°è¯å®äº†è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­ <span
class="math inline">\(R^2\)</span>
çš„çº¢çº¿åªä¼šä¸Šå‡ã€‚å®ƒæ€»æ˜¯ä¼šé€‰æ‹©æœ€å¤æ‚çš„æ¨¡å‹ã€‚</li>
</ul>
<h2
id="advanced-metrics-for-model-selection-é«˜çº§æŒ‡æ ‡ç”¨äºæ¨¡å‹é€‰æ‹©">Advanced
Metrics (For Model Selection) é«˜çº§æŒ‡æ ‡ï¼ˆç”¨äºæ¨¡å‹é€‰æ‹©ï¼‰</h2>
<p>These metrics â€œfixâ€ the flaw of <span
class="math inline">\(R^2\)</span> by including a penalty for the number
of predictors.</p>
<h3 id="adjusted-r2">Adjusted <span
class="math inline">\(R^2\)</span></h3>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
  \text{Adjusted } R^2 = 1 - \frac{RSS / (n - p - 1)}{SS_{total} / (n -
1)}
  \]</span></li>
<li><strong>Mathematical Concept:</strong> This formula replaces the
â€œSum of Squaresâ€ (<span class="math inline">\(SS\)</span>) with â€œMean
Squaresâ€ (<span class="math inline">\(MS\)</span>).
<ul>
<li><span class="math inline">\(MS_{error} =
\frac{RSS}{n-p-1}\)</span></li>
<li><span class="math inline">\(MS_{total} =
\frac{SS_{total}}{n-1}\)</span></li>
</ul></li>
<li><strong>The â€œPenaltyâ€ Explained:</strong> The penalty is
<strong>degrees of freedom</strong>.
<ul>
<li><span class="math inline">\(n\)</span> = number of data points.</li>
<li><span class="math inline">\(p\)</span> = number of predictors.</li>
<li>The term <span class="math inline">\(n-p-1\)</span> is the degrees
of freedom for the residuals. You start with <span
class="math inline">\(n\)</span> data points, but you â€œuse upâ€ one
degree of freedom to estimate the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) and <span
class="math inline">\(p\)</span> more to estimate the <span
class="math inline">\(p\)</span> slopes.</li>
</ul></li>
<li><strong>How it Works:</strong>
<ol type="1">
<li>When you add a new predictor (increase <span
class="math inline">\(p\)</span>), <span
class="math inline">\(RSS\)</span> goes down, which makes the numerator
(<span class="math inline">\(MS_{error}\)</span>) smaller.</li>
<li>â€¦But, increasing <span class="math inline">\(p\)</span>
<em>also</em> decreases the denominator (<span
class="math inline">\(n-p-1\)</span>), which makes the numerator (<span
class="math inline">\(MS_{error}\)</span>) <em>larger</em>.</li>
</ol>
<ul>
<li>This creates a â€œtug-of-war.â€ If the new predictor is
<strong>useful</strong>, it will drop <span
class="math inline">\(RSS\)</span> a lot, and Adjusted <span
class="math inline">\(R^2\)</span> will <strong>increase</strong>. If
the new predictor is <strong>useless</strong>, <span
class="math inline">\(RSS\)</span> will barely change, and the penalty
from decreasing the denominator will win, causing Adjusted <span
class="math inline">\(R^2\)</span> to <strong>decrease</strong>.</li>
</ul></li>
<li><strong>Goal:</strong> You select the model with the
<strong>highest</strong> Adjusted <span
class="math inline">\(R^2\)</span>.</li>
</ul>
<h3 id="akaike-information-criterion-aic">Akaike Information Criterion
(AIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Concept Breakdown:</strong>
<ul>
<li><span class="math inline">\(\ell(\hat{\theta})\)</span>: This is the
<strong>Maximized Likelihood Function</strong>.
<ul>
<li>The <strong>Likelihood Function</strong> <span
class="math inline">\(\ell(\theta)\)</span> asks: â€œGiven a set of model
parameters <span class="math inline">\(\theta\)</span>, how probable is
the data we observed?â€</li>
<li>The <strong>Maximum Likelihood Estimate (MLE)</strong> <span
class="math inline">\(\hat{\theta}\)</span> is the specific set of
parameters (the <span class="math inline">\(\hat{\beta}\)</span>â€™s) that
<em>maximizes</em> this probability.</li>
</ul></li>
<li><span class="math inline">\(\log \ell(\hat{\theta})\)</span>: The
<strong>log-likelihood</strong>. This is just a number that represents
the <em>best possible fit</em> the model can achieve for the data. A
higher number is a better fit.</li>
<li><span class="math inline">\(-2 \log \ell(\hat{\theta})\)</span>:
This is the <strong>Deviance</strong>. Since a higher log-likelihood is
better, a <em>lower</em> deviance is better. This term measures
<strong>poorness-of-fit</strong>.</li>
<li><span class="math inline">\(d\)</span>: The number of parameters
estimated by the model. (e.g., <span class="math inline">\(p\)</span>
predictors + 1 intercept).</li>
<li><span class="math inline">\(2d\)</span>: This is the <strong>Penalty
Term</strong>.</li>
</ul></li>
<li><strong>How it Works:</strong> <span class="math inline">\(AIC =
(\text{Poorness-of-Fit}) + (\text{Complexity Penalty})\)</span>. As you
add predictors, the fit gets better (the deviance term goes down), but
the penalty term (<span class="math inline">\(2d\)</span>) goes up.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> AIC.</li>
</ul>
<h3 id="bayesian-information-criterion-bic">Bayesian Information
Criterion (BIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(BIC =
-2 \log \ell(\hat{\theta}) + \log(n)d\)</span></li>
<li><strong>Concept:</strong> This is mathematically identical to AIC,
but the penalty term is different.
<ul>
<li><strong>AIC Penalty:</strong> <span
class="math inline">\(2d\)</span></li>
<li><strong>BIC Penalty:</strong> <span
class="math inline">\(\log(n)d\)</span></li>
</ul></li>
<li><strong>Comparison:</strong>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations
in your dataset.</li>
<li>As long as your dataset has 8 or more observations (<span
class="math inline">\(n \ge 8\)</span>), <span
class="math inline">\(\log(n)\)</span> will be greater than 2.</li>
<li>This means <strong>BIC applies a much harsher penalty for
complexity</strong> than AIC.</li>
</ul></li>
<li><strong>Consequence:</strong> BIC will tend to choose
<em>simpler</em> models (fewer predictors) than AIC.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> BIC.</li>
</ul>
<h2 id="the-deeper-theory-why-aic-works">The Deeper Theory: Why AIC
Works</h2>
<p>Slide 27 (â€œUnderstanding AICâ€) gives the deep mathematical
justification.</p>
<ul>
<li><strong>Goal:</strong> We have a <em>true</em>, unknown process
<span class="math inline">\(p\)</span> that generates our data. We are
creating a model <span class="math inline">\(\hat{p}_j\)</span>. We want
our model to be as â€œcloseâ€ to the truth as possible.</li>
<li><strong>Kullback-Leibler (K-L) Distance:</strong> This is a function
<span class="math inline">\(K(p, \hat{p}_j)\)</span> that measures the
â€œinformation lostâ€ when you use your model <span
class="math inline">\(\hat{p}_j\)</span> to approximate the truth <span
class="math inline">\(p\)</span>. You want to <em>minimize</em> this
distance.</li>
<li><strong>The Math:</strong>
<ol type="1">
<li><span class="math inline">\(K(p, \hat{p}_j) = \int p(y) \log \left(
\frac{p(y)}{\hat{p}_j(y)} \right) dy\)</span></li>
<li>This splits into: <span class="math inline">\(K(p, \hat{p}_j) =
\underbrace{\int p(y) \log(p(y)) dy}_{\text{Constant}} -
\underbrace{\int p(y) \log(\hat{p}_j(y)) dy}_{\text{This is what we need
to maximize}}\)</span></li>
</ol></li>
<li><strong>The Problem:</strong> We canâ€™t calculate that second term
because it requires knowing the <em>true</em> function <span
class="math inline">\(p\)</span>.</li>
<li><strong>Akaikeâ€™s Insight:</strong> Akaike proved that the
log-likelihood we <em>can</em> calculate, <span
class="math inline">\(\log \ell(\hat{\theta})\)</span>, is a
<em>biased</em> estimator of that target. He also proved that the bias
is approximately <span class="math inline">\(-d\)</span>.</li>
<li><strong>The Solution:</strong> An <em>unbiased</em> estimate of the
target is <span class="math inline">\(\log \ell(\hat{\theta}) -
d\)</span>.</li>
<li><strong>Final Step:</strong> For historical and statistical reasons,
he multiplied this by <span class="math inline">\(-2\)</span> to create
the final AIC formula.</li>
<li><strong>Conclusion:</strong> AIC is not just a random formula. It is
a carefully derived estimate of how much information your model loses
compared to the â€œtruthâ€ (i.e., its expected performance on new
data).</li>
</ul>
<h2 id="aicbic-for-linear-regression">AIC/BIC for Linear Regression</h2>
<p>Slide 26 shows how these general formulas simplify for linear
regression (assuming normal, Gaussian errors).</p>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Linear Regression Formula:</strong> <span
class="math inline">\(AIC = \frac{1}{n\hat{\sigma}^2}(RSS +
2d\hat{\sigma}^2)\)</span></li>
</ul>
<p><strong>Key Insight:</strong> For linear regression, the
â€œpoorness-of-fitâ€ term (<span class="math inline">\(-2 \log
\ell(\hat{\theta})\)</span>) is <em>directly proportional to</em> the
<span class="math inline">\(RSS\)</span>.</p>
<p>This makes it much easier to understand. You can just think of the
formulas as: * <strong>AIC <span class="math inline">\(\approx\)</span>
<span class="math inline">\(RSS + 2d\hat{\sigma}^2\)</span></strong> *
<strong>BIC <span class="math inline">\(\approx\)</span> <span
class="math inline">\(RSS + \log(n)d\hat{\sigma}^2\)</span></strong></p>
<p>(Here <span class="math inline">\(\hat{\sigma}^2\)</span> is an
estimate of the error variance, which can often be treated as a
constant).</p>
<p>This clearly shows the trade-off: We want a model with a low
<strong><span class="math inline">\(RSS\)</span></strong> (good fit) and
a low <strong><span class="math inline">\(d\)</span></strong> (low
complexity). These two goals are in direct competition.</p>
<p><strong>Mallowâ€™s <span class="math inline">\(C_p\)</span>:</strong>
The slide notes that <span class="math inline">\(C_p\)</span> is
equivalent to AIC for linear regression. The <span
class="math inline">\(C_p\)</span> formula is <span
class="math inline">\(C_p = \frac{1}{n}(RSS +
2d\hat{\sigma}^2_{full})\)</span>, where <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> is the error
variance estimated from the <em>full</em> model. Since <span
class="math inline">\(n\)</span> and <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> are constants,
minimizing <span class="math inline">\(C_p\)</span> is mathematically
identical to minimizing <span class="math inline">\(RSS +
2d\hat{\sigma}^2_{full}\)</span>, which is the same logic as AIC. #
3.</p>
<h1 id="section">4.</h1>
<h1 id="section-1">5.</h1>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/10/06/5054C5/" rel="prev" title="MSDM 5054 - Statistical Machine Learning-L5">
      <i class="fa fa-chevron-left"></i> MSDM 5054 - Statistical Machine Learning-L5
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#linear-model-selection-and-regularization-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.</span> <span class="nav-text">1.
Linear Model Selection and Regularization çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#summary-of-core-concepts"><span class="nav-number">1.1.</span> <span class="nav-text">Summary of Core Concepts</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mathematical-understanding-key-questions-%E6%95%B0%E5%AD%A6%E7%90%86%E8%A7%A3%E4%B8%8E%E5%85%B3%E9%94%AE%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.</span> <span class="nav-text">Mathematical
Understanding &amp; Key Questions æ•°å­¦ç†è§£ä¸å…³é”®é—®é¢˜</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#how-to-compare-which-model-is-better"><span class="nav-number">1.2.1.</span> <span class="nav-text">How to compare which model
is better?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-use-r2-in-step-2"><span class="nav-number">1.2.2.</span> <span class="nav-text">Why use \(R^2\) in Step 2?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-cant-we-use-training-error-in-step-3"><span class="nav-number">1.2.3.</span> <span class="nav-text">Why canâ€™t we use
training error in Step 3?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-analysis"><span class="nav-number">1.3.</span> <span class="nav-text">Code Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#key-functions"><span class="nav-number">1.3.1.</span> <span class="nav-text">Key Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#analysis-of-the-output"><span class="nav-number">1.3.2.</span> <span class="nav-text">Analysis of the Output</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#conceptual-overview-the-why"><span class="nav-number">1.4.</span> <span class="nav-text">Conceptual Overview: The â€œWhyâ€</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#questions"><span class="nav-number">1.5.</span> <span class="nav-text">Questions ğŸ¯</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#q1-how-to-compare-which-model-is-better"><span class="nav-number">1.5.1.</span> <span class="nav-text">Q1: â€œHow to compare
which model is better?â€</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q2-why-using-r2-for-step-2"><span class="nav-number">1.5.2.</span> <span class="nav-text">Q2: â€œWhy using \(R^2\) for step 2?â€</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q3-cannot-use-training-error-in-step-3.-why-not-%E6%AD%A5%E9%AA%A4-3-%E4%B8%AD%E4%B8%8D%E8%83%BD%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE-%E4%B8%BA%E4%BB%80%E4%B9%88"><span class="nav-number">1.5.3.</span> <span class="nav-text">Q3:
â€œCannot use training error in Step 3.â€ Why not? â€œæ­¥éª¤ 3
ä¸­ä¸èƒ½ä½¿ç”¨è®­ç»ƒè¯¯å·®ã€‚â€ ä¸ºä»€ä¹ˆï¼Ÿ</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mathematical-deep-dive"><span class="nav-number">1.6.</span> <span class="nav-text">Mathematical Deep Dive ğŸ§®</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#detailed-code-analysis"><span class="nav-number">1.7.</span> <span class="nav-text">Detailed Code Analysis ğŸ’»</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#key-functions-1"><span class="nav-number">1.7.1.</span> <span class="nav-text">Key Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary-of-outputs-slides-...221255.png-...221309.png"><span class="nav-number">1.7.2.</span> <span class="nav-text">Summary
of Outputs (Slides ...221255.png &amp;
...221309.png)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#the-core-problem-training-error-vs.-test-error-%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE-vs.-%E6%B5%8B%E8%AF%95%E8%AF%AF%E5%B7%AE"><span class="nav-number">2.</span> <span class="nav-text">2.
The Core Problem: Training Error vs.Â Test Error æ ¸å¿ƒé—®é¢˜ï¼šè®­ç»ƒè¯¯å·®
vs.Â æµ‹è¯•è¯¯å·®</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#basic-metrics-measures-of-fit"><span class="nav-number">2.1.</span> <span class="nav-text">Basic Metrics (Measures of
Fit)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#residue-error-%E6%AE%8B%E5%B7%AE%E8%AF%AF%E5%B7%AE"><span class="nav-number">2.1.1.</span> <span class="nav-text">Residue (Error) æ®‹å·®ï¼ˆè¯¯å·®ï¼‰</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#residual-sum-of-squares-rss-%E6%AE%8B%E5%B7%AE%E5%B9%B3%E6%96%B9%E5%92%8C-rss"><span class="nav-number">2.1.2.</span> <span class="nav-text">Residual Sum of
Squares (RSS) æ®‹å·®å¹³æ–¹å’Œ (RSS)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#r-squared-r2"><span class="nav-number">2.1.3.</span> <span class="nav-text">R-squared (\(R^2\))</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#advanced-metrics-for-model-selection-%E9%AB%98%E7%BA%A7%E6%8C%87%E6%A0%87%E7%94%A8%E4%BA%8E%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="nav-number">2.2.</span> <span class="nav-text">Advanced
Metrics (For Model Selection) é«˜çº§æŒ‡æ ‡ï¼ˆç”¨äºæ¨¡å‹é€‰æ‹©ï¼‰</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#adjusted-r2"><span class="nav-number">2.2.1.</span> <span class="nav-text">Adjusted \(R^2\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#akaike-information-criterion-aic"><span class="nav-number">2.2.2.</span> <span class="nav-text">Akaike Information Criterion
(AIC)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bayesian-information-criterion-bic"><span class="nav-number">2.2.3.</span> <span class="nav-text">Bayesian Information
Criterion (BIC)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-deeper-theory-why-aic-works"><span class="nav-number">2.3.</span> <span class="nav-text">The Deeper Theory: Why AIC
Works</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aicbic-for-linear-regression"><span class="nav-number">2.4.</span> <span class="nav-text">AIC&#x2F;BIC for Linear Regression</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#section"><span class="nav-number">3.</span> <span class="nav-text">4.</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#section-1"><span class="nav-number">4.</span> <span class="nav-text">5.</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> å¼ºåŠ›é©±åŠ¨
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
