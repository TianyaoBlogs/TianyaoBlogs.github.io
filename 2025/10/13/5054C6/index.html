<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianyaoblogs.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-6 Lecturer: Prof.XIA DONG 1. Linear Model Selection and Regularization çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ– Summary of Core Concepts Chapter 6: Linear Model Selection and Regularization, focusing specifica">
<meta property="og:type" content="article">
<meta property="og:title" content="MSDM 5054 - Statistical Machine Learning-L6">
<meta property="og:url" content="https://tianyaoblogs.github.io/2025/10/13/5054C6/index.html">
<meta property="og:site_name" content="TianyaoBlogs">
<meta property="og:description" content="ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-6 Lecturer: Prof.XIA DONG 1. Linear Model Selection and Regularization çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ– Summary of Core Concepts Chapter 6: Linear Model Selection and Regularization, focusing specifica">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-13T13:00:00.000Z">
<meta property="article:modified_time" content="2025-10-19T19:45:06.469Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://tianyaoblogs.github.io/2025/10/13/5054C6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>MSDM 5054 - Statistical Machine Learning-L6 | TianyaoBlogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TianyaoBlogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/13/5054C6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MSDM 5054 - Statistical Machine Learning-L6
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-10-13 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-13T21:00:00+08:00">2025-10-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-10-20 03:45:06" itemprop="dateModified" datetime="2025-10-20T03:45:06+08:00">2025-10-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-6</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1
id="linear-model-selection-and-regularization-çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–">1.
Linear Model Selection and Regularization çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–</h1>
<h2 id="summary-of-core-concepts">Summary of Core Concepts</h2>
<p><strong>Chapter 6: Linear Model Selection and
Regularization</strong>, focusing specifically on <strong>Section 6.1:
Subset Selection</strong>.
<strong>ç¬¬å…­ç« ï¼šçº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–</strong>ï¼Œ<strong>6.1èŠ‚ï¼šå­é›†é€‰æ‹©</strong></p>
<ul>
<li><p><strong>The Problem:</strong> You have a dataset with many
potential predictor variables (features). If you include all of them
(like <strong>Model 1</strong> with <span
class="math inline">\(p\)</span> predictors in slide
<code>...221320.png</code>), you risk including â€œnoiseâ€ variables. These
irrelevant features can decrease model accuracy (overfitting) and make
the model difficult to interpret.
æ•°æ®é›†åŒ…å«è®¸å¤šæ½œåœ¨çš„é¢„æµ‹å˜é‡ï¼ˆç‰¹å¾ï¼‰ã€‚å¦‚æœåŒ…å«æ‰€æœ‰è¿™äº›å˜é‡ï¼ˆä¾‹å¦‚å¹»ç¯ç‰‡â€œâ€¦221320.pngâ€ä¸­å¸¦æœ‰<span
class="math inline">\(p\)</span>ä¸ªé¢„æµ‹å˜é‡çš„<strong>æ¨¡å‹1</strong>ï¼‰ï¼Œåˆ™å¯èƒ½ä¼šåŒ…å«â€œå™ªå£°â€å˜é‡ã€‚è¿™äº›ä¸ç›¸å…³çš„ç‰¹å¾ä¼šé™ä½æ¨¡å‹çš„å‡†ç¡®ç‡ï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼Œå¹¶ä½¿æ¨¡å‹éš¾ä»¥è§£é‡Šã€‚</p></li>
<li><p><strong>The Goal:</strong> Identify a smaller subset of variables
that are truly related to the response. This creates a simpler, more
interpretable, and often more accurate model (like <strong>Model
2</strong> with <span class="math inline">\(q\)</span> predictors).
æ‰¾å‡ºä¸€ä¸ªä¸å“åº”çœŸæ­£ç›¸å…³çš„è¾ƒå°å˜é‡å­é›†ã€‚è¿™å°†åˆ›å»ºä¸€ä¸ªæ›´ç®€å•ã€æ›´æ˜“äºè§£é‡Šä¸”é€šå¸¸æ›´å‡†ç¡®çš„æ¨¡å‹ï¼ˆä¾‹å¦‚å¸¦æœ‰<span
class="math inline">\(q\)</span>ä¸ªé¢„æµ‹å˜é‡çš„<strong>æ¨¡å‹2</strong>ï¼‰ã€‚</p></li>
<li><p><strong>The Main Method Discussed: Best Subset
Selection</strong></p></li>
<li><p><strong>ä¸»è¦è®¨è®ºçš„æ–¹æ³•ï¼šæœ€ä½³å­é›†é€‰æ‹©</strong> This is an
<em>exhaustive search</em> algorithm. It checks <em>every possible
combination</em> of predictors to find the â€œbestâ€ model. With <span
class="math inline">\(p\)</span> variables, this means checking <span
class="math inline">\(2^p\)</span> total models.
è¿™æ˜¯ä¸€ç§<em>ç©·ä¸¾æœç´¢</em>ç®—æ³•ã€‚å®ƒæ£€æŸ¥<em>æ‰€æœ‰å¯èƒ½çš„é¢„æµ‹å˜é‡ç»„åˆ</em>ï¼Œä»¥æ‰¾åˆ°â€œæœ€ä½³â€æ¨¡å‹ã€‚å¯¹äº
<span class="math inline">\(p\)</span> ä¸ªå˜é‡ï¼Œè¿™æ„å‘³ç€éœ€è¦æ£€æŸ¥æ€»å…±
<span class="math inline">\(2^p\)</span> ä¸ªæ¨¡å‹ã€‚</p>
<p>The algorithm (from slide <code>...221333.png</code>) works in three
steps:</p>
<ol type="1">
<li><p><strong>Step 1:</strong> Fit the â€œnull modelâ€ <span
class="math inline">\(M_0\)</span>, which has no predictors (it just
predicts the average of the response). æ‹Ÿåˆâ€œç©ºæ¨¡å‹â€<span
class="math inline">\(M_0\)</span>ï¼Œå®ƒæ²¡æœ‰é¢„æµ‹å˜é‡ï¼ˆå®ƒåªé¢„æµ‹å“åº”çš„å¹³å‡å€¼ï¼‰ã€‚</p></li>
<li><p><strong>Step 2:</strong> For each <span
class="math inline">\(k\)</span> (from 1 to <span
class="math inline">\(p\)</span>):</p>
<ul>
<li><p>Fit all <span class="math inline">\(\binom{p}{k}\)</span> models
that contain exactly <span class="math inline">\(k\)</span> predictors.
(e.g., fit all models with 1 predictor, then all models with 2
predictors, etc.).</p></li>
<li><p>æ‹Ÿåˆæ‰€æœ‰åŒ…å« <span class="math inline">\(k\)</span> ä¸ªé¢„æµ‹å˜é‡çš„
<span class="math inline">\(\binom{p}{k}\)</span>
ä¸ªæ¨¡å‹ã€‚ï¼ˆä¾‹å¦‚ï¼Œå…ˆæ‹Ÿåˆæ‰€æœ‰åŒ…å« 1 ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œç„¶åæ‹Ÿåˆæ‰€æœ‰åŒ…å« 2
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œç­‰ç­‰ï¼‰ã€‚</p></li>
<li><p>From this group, select the single best model <em>for that size
<span class="math inline">\(k\)</span></em>. This â€œbestâ€ model is the
one with the highest <strong><span
class="math inline">\(R^2\)</span></strong> (or lowest
<strong>RSS</strong> - Residual Sum of Squares) on the <em>training
data</em>. Call this model <span
class="math inline">\(M_k\)</span>.</p></li>
<li><p>ä»è¿™ç»„ä¸­ï¼Œé€‰æ‹© <em>å¯¹äºè¯¥è§„æ¨¡ <span
class="math inline">\(k\)</span></em> çš„æœ€ä½³æ¨¡å‹ã€‚è¿™ä¸ªâ€œæœ€ä½³â€æ¨¡å‹æ˜¯åœ¨
<em>è®­ç»ƒæ•°æ®</em> ä¸Šå…·æœ‰æœ€é«˜ <strong><span
class="math inline">\(R^2\)</span></strong>ï¼ˆæˆ–æœ€ä½ <strong>RSS</strong>
- æ®‹å·®å¹³æ–¹å’Œï¼‰çš„æ¨¡å‹ã€‚å°†æ­¤æ¨¡å‹ç§°ä¸º <span
class="math inline">\(M_k\)</span>ã€‚</p></li>
</ul></li>
<li><p><strong>Step 3:</strong> You now have <span
class="math inline">\(p+1\)</span> models: <span
class="math inline">\(M_0, M_1, \dots, M_p\)</span>. You must select the
single best one from this list. To do this, you <strong>cannot</strong>
use training <span class="math inline">\(R^2\)</span> (as it will always
pick the biggest model <span class="math inline">\(M_p\)</span>).
Instead, you must use a metric that estimates <em>test error</em>, such
as: <strong>ç°åœ¨ä½ æœ‰ <span class="math inline">\(p+1\)</span>
ä¸ªæ¨¡å‹ï¼š<span class="math inline">\(M_0, M_1, \dots,
M_p\)</span>ã€‚ä½ å¿…é¡»ä»åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä½³æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œä½ </strong>ä¸èƒ½**ä½¿ç”¨è®­ç»ƒ
<span class="math inline">\(R^2\)</span>ï¼ˆå› ä¸ºå®ƒæ€»æ˜¯ä¼šé€‰æ‹©æœ€å¤§çš„æ¨¡å‹
<span
class="math inline">\(M_p\)</span>ï¼‰ã€‚ç›¸åï¼Œä½ å¿…é¡»ä½¿ç”¨ä¸€ä¸ªèƒ½å¤Ÿä¼°è®¡<em>æµ‹è¯•è¯¯å·®</em>çš„æŒ‡æ ‡ï¼Œä¾‹å¦‚ï¼š</p>
<ul>
<li><strong>Cross-Validation (CV) äº¤å‰éªŒè¯ (CV)</strong> (This is what
the Python code uses)</li>
<li><strong>AIC</strong> (Akaike Information Criterion
èµ¤æ± ä¿¡æ¯å‡†åˆ™)</li>
<li><strong>BIC</strong> (Bayesian Information Criterion
è´å¶æ–¯ä¿¡æ¯å‡†åˆ™)</li>
<li><strong>Adjusted <span class="math inline">\(R^2\)</span> è°ƒæ•´åçš„
<span class="math inline">\(R^2\)</span></strong></li>
</ul></li>
</ol></li>
<li><p><strong>Key Takeaway:</strong> The slides show this â€œsubset
selectionâ€ concept can be applied <em>beyond</em> linear models. The
Python code demonstrates this by applying best subset selection to a
<strong>K-Nearest Neighbors (KNN) Regressor</strong>, a non-linear
model.â€œå­é›†é€‰æ‹©â€çš„æ¦‚å¿µå¯ä»¥åº”ç”¨äºçº¿æ€§æ¨¡å‹<em>ä¹‹å¤–</em>ã€‚</p></li>
</ul>
<h2
id="mathematical-understanding-key-questions-æ•°å­¦ç†è§£ä¸å…³é”®é—®é¢˜">Mathematical
Understanding &amp; Key Questions æ•°å­¦ç†è§£ä¸å…³é”®é—®é¢˜</h2>
<p>This section directly answers the questions posed on your slides.</p>
<h3 id="how-to-compare-which-model-is-better">How to compare which model
is better?</h3>
<p>(From slides <code>...221320.png</code> and
<code>...221326.png</code>)</p>
<p>You cannot use <strong>training error</strong> (like <span
class="math inline">\(R^2\)</span> or RSS) to compare models with
<em>different numbers of predictors</em>. A model with more predictors
will almost always have a better <em>training</em> score, even if those
extra predictors are just noise. This is called
<strong>overfitting</strong>. ä¸èƒ½ä½¿ç”¨<strong>è®­ç»ƒè¯¯å·®</strong>ï¼ˆä¾‹å¦‚
<span class="math inline">\(R^2\)</span> æˆ–
RSSï¼‰æ¥æ¯”è¾ƒå…·æœ‰<em>ä¸åŒæ•°é‡é¢„æµ‹å˜é‡</em>çš„æ¨¡å‹ã€‚å…·æœ‰æ›´å¤šé¢„æµ‹å˜é‡çš„æ¨¡å‹å‡ ä¹æ€»æ˜¯å…·æœ‰æ›´å¥½çš„<em>è®­ç»ƒ</em>åˆ†æ•°ï¼Œå³ä½¿è¿™äº›é¢å¤–çš„é¢„æµ‹å˜é‡åªæ˜¯å™ªå£°ã€‚è¿™è¢«ç§°ä¸º<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚</p>
<p>To compare models of different sizes (like Model 1 vs.Â Model 2, or
<span class="math inline">\(M_2\)</span> vs.Â <span
class="math inline">\(M_5\)</span>), you <strong>must</strong> use a
method that estimates <strong>test error</strong> (how the model
performs on new, unseen data). The slides mention:
è¦æ¯”è¾ƒä¸åŒå¤§å°çš„æ¨¡å‹ï¼ˆä¾‹å¦‚æ¨¡å‹ 1 ä¸æ¨¡å‹ 2ï¼Œæˆ– <span
class="math inline">\(M_2\)</span> ä¸ <span
class="math inline">\(M_5\)</span>ï¼‰ï¼Œæ‚¨<strong>å¿…é¡»</strong>ä½¿ç”¨ä¸€ç§ä¼°ç®—<strong>æµ‹è¯•è¯¯å·®</strong>ï¼ˆæ¨¡å‹åœ¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ä¸Šçš„è¡¨ç°ï¼‰çš„æ–¹æ³•ã€‚</p>
<ul>
<li><p><strong>Cross-Validation (CV):</strong> This is the gold
standard. You split your data into â€œfolds,â€ train the model on some
folds, and test it on the remaining fold. You repeat this and average
the test scores. The model with the best (e.g., lowest) average CV error
is chosen.
å°†æ•°æ®åˆ†æˆâ€œæŠ˜å â€ï¼Œåœ¨ä¸€äº›æŠ˜å ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨å‰©ä½™çš„æŠ˜å ä¸Šæµ‹è¯•æ¨¡å‹ã€‚é‡å¤æ­¤æ“ä½œå¹¶å–æµ‹è¯•åˆ†æ•°çš„å¹³å‡å€¼ã€‚é€‰æ‹©å¹³å‡
CV è¯¯å·®æœ€å°ï¼ˆä¾‹å¦‚ï¼Œæœ€å°ï¼‰çš„æ¨¡å‹ã€‚</p></li>
<li><p><strong>AIC &amp; BIC:</strong> These are mathematical
adjustments to the training error (like RSS) that add a <em>penalty</em>
for having more predictors. They balance model <em>fit</em> with model
<em>complexity</em>. è¿™äº›æ˜¯å¯¹è®­ç»ƒè¯¯å·®ï¼ˆå¦‚
RSSï¼‰çš„æ•°å­¦è°ƒæ•´ï¼Œä¼šå› é¢„æµ‹å˜é‡è¾ƒå¤šè€Œå¢åŠ <em>æƒ©ç½š</em>ã€‚å®ƒä»¬å¹³è¡¡äº†æ¨¡å‹<em>æ‹Ÿåˆåº¦</em>å’Œæ¨¡å‹<em>å¤æ‚åº¦</em>ã€‚</p></li>
</ul>
<h3 id="why-use-r2-in-step-2">Why use <span
class="math inline">\(R^2\)</span> in Step 2?</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p>In Step 2, you are only comparing models <strong>of the same
size</strong> (i.e., all models that have exactly <span
class="math inline">\(k\)</span> predictors). For models with the same
number of parameters, a higher <span class="math inline">\(R^2\)</span>
(or lower RSS) on the training data directly corresponds to a better
fit. You donâ€™t need to penalize for complexity because all models being
compared <em>have the same complexity</em>.
åªæ¯”è¾ƒ<strong>å¤§å°ç›¸åŒ</strong>çš„æ¨¡å‹ï¼ˆå³æ‰€æœ‰æ°å¥½å…·æœ‰ <span
class="math inline">\(k\)</span>
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼‰ã€‚å¯¹äºå‚æ•°æ•°é‡ç›¸åŒçš„æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®ä¸Šæ›´é«˜çš„ <span
class="math inline">\(R^2\)</span>ï¼ˆæˆ–æ›´ä½çš„
RSSï¼‰ç›´æ¥å¯¹åº”ç€æ›´å¥½çš„æ‹Ÿåˆåº¦ã€‚æ‚¨ä¸éœ€è¦å¯¹å¤æ‚åº¦è¿›è¡Œæƒ©ç½šï¼Œå› ä¸ºæ‰€æœ‰è¢«æ¯”è¾ƒçš„æ¨¡å‹<em>éƒ½å…·æœ‰ç›¸åŒçš„å¤æ‚åº¦</em>ã€‚</p>
<h3 id="why-cant-we-use-training-error-in-step-3">Why canâ€™t we use
training error in Step 3?</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p>In Step 3, you are comparing models <strong>of different
sizes</strong> (<span class="math inline">\(M_0\)</span> vs.Â <span
class="math inline">\(M_1\)</span> vs.Â <span
class="math inline">\(M_2\)</span>, etc.). As you add predictors, the
training <span class="math inline">\(R^2\)</span> will <em>always</em>
go up (or stay the same), and the training RSS will <em>always</em> go
down (or stay the same). If you used <span
class="math inline">\(R^2\)</span> to pick the best model in Step 3, you
would <em>always</em> pick the most complex model <span
class="math inline">\(M_p\)</span>, which is almost certainly overfit.
å°†æ¯”è¾ƒ<strong>ä¸åŒå¤§å°</strong>çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ <span
class="math inline">\(M_0\)</span> vs.Â <span
class="math inline">\(M_1\)</span> vs.Â <span
class="math inline">\(M_2\)</span> ç­‰ï¼‰ã€‚éšç€æ‚¨æ·»åŠ é¢„æµ‹å˜é‡ï¼Œè®­ç»ƒ <span
class="math inline">\(R^2\)</span>
å°†<em>å§‹ç»ˆ</em>ä¸Šå‡ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ï¼Œè€Œè®­ç»ƒ RSS
å°†<em>å§‹ç»ˆ</em>ä¸‹é™ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ã€‚å¦‚æœæ‚¨åœ¨æ­¥éª¤ 3 ä¸­ä½¿ç”¨ <span
class="math inline">\(R^2\)</span>
æ¥é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼Œé‚£ä¹ˆæ‚¨<em>å§‹ç»ˆ</em>ä¼šé€‰æ‹©æœ€å¤æ‚çš„æ¨¡å‹ <span
class="math inline">\(M_p\)</span>ï¼Œè€Œè¯¥æ¨¡å‹å‡ ä¹è‚¯å®šä¼šè¿‡æ‹Ÿåˆã€‚</p>
<p>Therefore, you <em>must</em> use a metric that estimates test error
(like CV) or penalizes for complexity (like AIC, BIC, or Adjusted <span
class="math inline">\(R^2\)</span>) to find the right balance between
fit and simplicity. å› æ­¤ï¼Œæ‚¨<em>å¿…é¡»</em>ä½¿ç”¨ä¸€ä¸ªå¯ä»¥ä¼°ç®—æµ‹è¯•è¯¯å·®ï¼ˆä¾‹å¦‚
CVï¼‰æˆ–æƒ©ç½šå¤æ‚åº¦ï¼ˆä¾‹å¦‚ AICã€BIC æˆ–è°ƒæ•´åçš„ <span
class="math inline">\(R^2\)</span>ï¼‰çš„æŒ‡æ ‡æ¥æ‰¾åˆ°æ‹Ÿåˆåº¦å’Œç®€å•æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚</p>
<h2 id="code-analysis">Code Analysis</h2>
<p>The Python code (slides <code>...221249.jpg</code> and
<code>...221303.jpg</code>) implements the <strong>Best Subset
Selection</strong> algorithm using <strong>KNN Regression</strong>.</p>
<h3 id="key-functions">Key Functions</h3>
<ul>
<li><code>main()</code>:
<ol type="1">
<li><strong>Loads Data:</strong> Reads the <code>Credit.csv</code>
file.</li>
<li><strong>Preprocesses Data:</strong>
<ul>
<li>Converts categorical features (â€˜Genderâ€™, â€˜Studentâ€™, â€˜Marriedâ€™,
â€˜Ethnicityâ€™) into numerical ones (dummy variables).
å°†åˆ†ç±»ç‰¹å¾ï¼ˆâ€œæ€§åˆ«â€ã€â€œå­¦ç”Ÿâ€ã€â€œå·²å©šâ€ã€â€œç§æ—â€ï¼‰è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾ï¼ˆè™šæ‹Ÿå˜é‡ï¼‰ã€‚</li>
<li>Creates the feature matrix <code>X</code> and target variable
<code>y</code> (â€˜Balanceâ€™). åˆ›å»ºç‰¹å¾çŸ©é˜µ <code>X</code> å’Œç›®æ ‡å˜é‡
<code>y</code>ï¼ˆâ€œä½™é¢â€ï¼‰ã€‚</li>
<li><strong>Scales</strong> the features using
<code>StandardScaler</code>. This is crucial for KNN, which is sensitive
to the scale of features. ç”¨ <code>StandardScaler</code>
å¯¹ç‰¹å¾è¿›è¡Œ<strong>ç¼©æ”¾</strong>ã€‚è¿™å¯¹äº KNN
è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¯¹ç‰¹å¾çš„ç¼©æ”¾éå¸¸æ•æ„Ÿã€‚</li>
</ul></li>
<li><strong>Adds Noise (in the second example):</strong> Slide
<code>...221303.jpg</code> shows code that <em>adds 20 new â€œnoisyâ€
columns</em> to the data. This is to test if the selection algorithm is
smart enough to ignore them. å‘æ•°æ®ä¸­æ·»åŠ  20
ä¸ªæ–°çš„â€œå™ªå£°â€åˆ—çš„ä»£ç ã€‚è¿™æ˜¯ä¸ºäº†æµ‹è¯•é€‰æ‹©ç®—æ³•æ˜¯å¦è¶³å¤Ÿæ™ºèƒ½ï¼Œèƒ½å¤Ÿå¿½ç•¥å®ƒä»¬ã€‚</li>
<li><strong>Runs Selection:</strong> Calls
<code>best_subset_selection_parallel</code> to do the main work.</li>
<li><strong>Prints Results:</strong> Finds the best subset (lowest
error) and prints the top 20 best-performing subsets.
æ‰¾åˆ°æœ€ä½³å­é›†ï¼ˆè¯¯å·®æœ€å°ï¼‰ï¼Œå¹¶æ‰“å°å‡ºè¡¨ç°æœ€ä½³çš„å‰ 20 ä¸ªå­é›†ã€‚</li>
<li><strong>Final Evaluation:</strong> It re-trains a KNN model on
<em>only</em> the best subset and calculates the final cross-validated
RMSE. ä»…åŸºäºæœ€ä½³å­é›†é‡æ–°è®­ç»ƒ KNN æ¨¡å‹ï¼Œå¹¶è®¡ç®—æœ€ç»ˆçš„äº¤å‰éªŒè¯ RMSEã€‚</li>
</ol></li>
<li><code>evaluate_subset(subset, ...)</code>:
<ul>
<li>This is the â€œworkerâ€ function. Itâ€™s called for <em>every single</em>
possible subset.</li>
<li>It takes a <code>subset</code> (a list of feature names, e.g.,
<code>['Income', 'Limit']</code>).</li>
<li>It creates a new <code>X_subset</code> containing <em>only</em>
those columns.</li>
<li>It runs 5-fold cross-validation (<code>cross_val_score</code>) on a
KNN model using this <code>X_subset</code>.</li>
<li>It uses <code>'neg_mean_squared_error'</code> as the metric. This is
negative MSE; a <em>higher</em> score (closer to 0) is better.
å®ƒä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„â€œX_subsetâ€<em>ï¼Œä»…åŒ…å«è¿™äº›åˆ—ã€‚ å®ƒä¼šä½¿ç”¨æ­¤â€œX_subsetâ€åœ¨
KNN æ¨¡å‹ä¸Šè¿è¡Œ 5 å€äº¤å‰éªŒè¯ï¼ˆâ€œcross_val_scoreâ€ï¼‰ã€‚
å®ƒä½¿ç”¨â€œneg_mean_squared_errorâ€ä½œä¸ºåº¦é‡æ ‡å‡†ã€‚è¿™æ˜¯è´Ÿ
MSEï¼›</em>æ›´é«˜*çš„åˆ†æ•°ï¼ˆè¶Šæ¥è¿‘ 0ï¼‰è¶Šå¥½ã€‚</li>
<li>It returns the subset and its average CV score.</li>
</ul></li>
<li><code>best_subset_selection_parallel(model, ...)</code>:
<ul>
<li>This is the â€œmanagerâ€ function.è¿™æ˜¯â€œç®¡ç†å™¨â€å‡½æ•°ã€‚</li>
<li>It iterates from <code>k=1</code> up to the total number of
features.å®ƒä»â€œk=1â€è¿­ä»£åˆ°ç‰¹å¾æ€»æ•°ã€‚</li>
<li>For each <code>k</code>, it generates <em>all combinations</em> of
features of that size (this is the <span
class="math inline">\(\binom{p}{k}\)</span> part).
å¯¹äºæ¯ä¸ªâ€œkâ€ï¼Œå®ƒä¼šç”Ÿæˆè¯¥å¤§å°çš„ç‰¹å¾çš„<em>æ‰€æœ‰ç»„åˆ</em>ï¼ˆè¿™æ˜¯ <span
class="math inline">\(\binom{p}{k}\)</span> éƒ¨åˆ†ï¼‰ã€‚</li>
<li>It uses <code>Parallel</code> and <code>delayed</code> (from
<code>joblib</code>) to run <code>evaluate_subset</code> for all these
combinations <em>in parallel</em>, speeding up the process
significantly. å®ƒä½¿ç”¨ <code>Parallel</code> å’Œ
<code>delayed</code>ï¼ˆæ¥è‡ª
<code>joblib</code>ï¼‰å¯¹æ‰€æœ‰è¿™äº›ç»„åˆ<em>å¹¶è¡Œ</em>è¿è¡Œ
<code>evaluate_subset</code>ï¼Œä»è€Œæ˜¾è‘—åŠ å¿«äº†å¤„ç†é€Ÿåº¦ã€‚</li>
<li>It collects all the results and returns
them.å®ƒæ”¶é›†æ‰€æœ‰ç»“æœå¹¶è¿”å›ã€‚</li>
</ul></li>
</ul>
<h3 id="analysis-of-the-output">Analysis of the Output</h3>
<ul>
<li><strong>Slide <code>...221255.png</code> (Original Data):</strong>
<ul>
<li>The code runs subset selection on the original dataset.</li>
<li>The â€œTop 20 Best Feature Subsetsâ€ are shown. The CV scores are
negative (they are <code>neg_mean_squared_error</code>), so the scores
<em>closest to zero</em> (smallest magnitude) are best.</li>
<li>The <strong>Best feature subset</strong> is found to be
<code>('Income', 'Limit', 'Rating', 'Student')</code>.</li>
<li>The final cross-validated RMSE for this model is
<strong>105.41</strong>.</li>
</ul></li>
<li><strong>Slide <code>...221309.png</code> (Data with 20 Noisy
Variables):</strong>
<ul>
<li>The code is re-run after adding 20 useless â€œNoisyâ€ features.</li>
<li>The algorithm <em>still</em> works. It correctly identifies that the
â€œNoisyâ€ variables are useless.</li>
<li>The <strong>Best feature subset</strong> is now
<code>('Income', 'Limit', 'Student')</code>. (Note: â€˜Ratingâ€™ was
dropped, likely because itâ€™s highly correlated with â€˜Limitâ€™, and the
noisy data made the simpler model perform slightly better in CV).</li>
<li>The final RMSE is <strong>114.94</strong>. This is <em>higher</em>
than the original 105.41, which is expectedâ€”the presence of so many
noise variables makes the selection problem harder, but the final model
is still good and, most importantly, <em>it successfully excluded all 20
noisy features</em>. æœ€ç»ˆçš„ RMSE ä¸º <strong>114.94</strong>ã€‚è¿™æ¯”æœ€åˆçš„
105.41<em>æ›´é«˜</em>ï¼Œè¿™æ˜¯é¢„æœŸçš„â€”â€”å¦‚æ­¤å¤šçš„å™ªå£°å˜é‡çš„å­˜åœ¨ä½¿å¾—é€‰æ‹©é—®é¢˜æ›´åŠ å›°éš¾ï¼Œä½†æœ€ç»ˆæ¨¡å‹ä»ç„¶å¾ˆå¥½ï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œ<em>å®ƒæˆåŠŸåœ°æ’é™¤äº†æ‰€æœ‰
20 ä¸ªå™ªå£°ç‰¹å¾</em>ã€‚</li>
</ul></li>
</ul>
<h2 id="conceptual-overview-the-why">Conceptual Overview: The â€œWhyâ€</h2>
<p>Slides cover <strong>Chapter 6: Linear Model Selection and
Regularization</strong>, which is all about a fundamental trade-off in
machine learning: the <strong>bias-variance trade-off</strong>.
è¯¥éƒ¨åˆ†ä¸»è¦è®¨è®ºæœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªåŸºæœ¬æƒè¡¡ï¼š<strong>åå·®-æ–¹å·®æƒè¡¡</strong>ã€‚</p>
<ul>
<li><p><strong>The Problem (Slide <code>...221320.png</code>):</strong>
Imagine you have a dataset with 50 predictors (<span
class="math inline">\(p=50\)</span>). You want to predict a response
<span class="math inline">\(y\)</span>. å‡è®¾ä½ æœ‰ä¸€ä¸ªåŒ…å« 50
ä¸ªé¢„æµ‹å˜é‡ï¼ˆp=50ï¼‰çš„æ•°æ®é›†ã€‚ä½ æƒ³è¦é¢„æµ‹å“åº” <span
class="math inline">\(y\)</span>ã€‚</p>
<ul>
<li><strong>Model 1 (Full Model):</strong> You use all 50 predictors.
This model is very <strong>flexible</strong>. It will fit the
<em>training data</em> extremely well, resulting in a low
<strong>bias</strong>. However, itâ€™s highly likely that many of those 50
predictors are just â€œnoiseâ€ (random, unrelated variables). By fitting to
this noise, the model will be <strong>overfit</strong>. When you show it
new, unseen data (the <em>test data</em>), it will perform poorly. This
is called <strong>high variance</strong>. ä½ ä½¿ç”¨äº†æ‰€æœ‰ 50
ä¸ªé¢„æµ‹å˜é‡ã€‚è¿™ä¸ªæ¨¡å‹éå¸¸<strong>çµæ´»</strong>ã€‚å®ƒèƒ½å¾ˆå¥½åœ°æ‹Ÿåˆ<em>è®­ç»ƒæ•°æ®</em>ï¼Œä»è€Œäº§ç”Ÿè¾ƒä½çš„<strong>åå·®</strong>ã€‚ç„¶è€Œï¼Œè¿™
50
ä¸ªé¢„æµ‹å˜é‡ä¸­å¾ˆå¯èƒ½æœ‰å¾ˆå¤šåªæ˜¯â€œå™ªå£°â€ï¼ˆéšæœºçš„ã€ä¸ç›¸å…³çš„å˜é‡ï¼‰ã€‚ç”±äºæ‹Ÿåˆè¿™äº›å™ªå£°ï¼Œæ¨¡å‹ä¼š<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚å½“ä½ å‘å®ƒå±•ç¤ºæ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ï¼ˆ<em>æµ‹è¯•æ•°æ®</em>ï¼‰æ—¶ï¼Œå®ƒçš„è¡¨ç°ä¼šå¾ˆå·®ã€‚è¿™è¢«ç§°ä¸º<strong>é«˜æ–¹å·®</strong>ã€‚</li>
<li><strong>Model 2 (Subset Model):</strong> You intelligently select
only the 3 predictors (<span class="math inline">\(q=3\)</span>) that
are <em>actually</em> related to <span class="math inline">\(y\)</span>.
This model is less flexible. It wonâ€™t fit the <em>training data</em> as
perfectly as Model 1 (it has higher <strong>bias</strong>). But, because
itâ€™s <em>not</em> fitting the noise, it will generalize much better to
new data. It will have a much lower <strong>variance</strong>, and thus
a lower overall <em>test error</em>. ä½ æ™ºèƒ½åœ°åªé€‰æ‹©ä¸ <span
class="math inline">\(y\)</span> <em>çœŸæ­£</em>ç›¸å…³çš„ 3 ä¸ªé¢„æµ‹å˜é‡ (<span
class="math inline">\(q=3\)</span>)ã€‚è¿™ä¸ªæ¨¡å‹çš„çµæ´»æ€§è¾ƒå·®ã€‚å®ƒå¯¹
<em>è®­ç»ƒæ•°æ®</em> çš„æ‹Ÿåˆåº¦ä¸å¦‚æ¨¡å‹ 1
å®Œç¾ï¼ˆå®ƒçš„<strong>åå·®</strong>æ›´é«˜ï¼‰ã€‚ä½†æ˜¯ï¼Œç”±äºå®ƒå¯¹å™ªå£°çš„æ‹Ÿåˆåº¦æ›´é«˜ï¼Œå› æ­¤å¯¹æ–°æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ä¼šæ›´å¥½ã€‚å®ƒçš„<strong>æ–¹å·®</strong>ä¼šæ›´ä½ï¼Œå› æ­¤æ€»ä½“çš„<em>æµ‹è¯•è¯¯å·®</em>ä¹Ÿä¼šæ›´ä½ã€‚</li>
</ul></li>
<li><p><strong>The Goal:</strong> The goal is to find the model that has
the <strong>lowest test error</strong>. We need a formal method to
<em>find</em> the best subset (like Model 2) without just guessing.
<strong>ç›®æ ‡æ˜¯æ‰¾åˆ°</strong>æµ‹è¯•è¯¯å·®**æœ€ä½çš„æ¨¡å‹ã€‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ­£å¼çš„æ–¹æ³•æ¥<em>æ‰¾åˆ°</em>æœ€ä½³å­é›†ï¼ˆä¾‹å¦‚æ¨¡å‹
2ï¼‰ï¼Œè€Œä¸æ˜¯ä»…ä»…é çŒœæµ‹ã€‚</p></li>
<li><p><strong>Two Main Strategies (Slide
<code>...221314.png</code>):</strong></p>
<ol type="1">
<li><p><strong>Subset Selection (Section 6.1):</strong> This is what
weâ€™re focused on. Itâ€™s an â€œall-or-nothingâ€ approach. You either
<em>keep</em> a variable in the model or you <em>discard</em> it
completely. The â€œBest Subset Selectionâ€ algorithm is the most extreme,
â€œbrute-forceâ€ way to do this.
æ˜¯æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹ã€‚è¿™æ˜¯ä¸€ç§â€œå…¨æœ‰æˆ–å…¨æ— â€çš„æ–¹æ³•ã€‚ä½ è¦ä¹ˆåœ¨æ¨¡å‹ä¸­â€œä¿ç•™â€ä¸€ä¸ªå˜é‡ï¼Œè¦ä¹ˆâ€œå½»åº•ä¸¢å¼ƒâ€å®ƒã€‚â€œæœ€ä½³å­é›†é€‰æ‹©â€ç®—æ³•æ˜¯æœ€æç«¯ã€æœ€â€œæš´åŠ›â€çš„åšæ³•ã€‚</p></li>
<li><p><strong>Shrinkage/Regularization (Section 6.2):</strong> This is
a more subtle approach (e.g., Ridge Regression, LASSO). Instead of
discarding variables, you <em>keep all <span
class="math inline">\(p\)</span> variables</em> but add a penalty to the
model that â€œshrinksâ€ the coefficients (<span
class="math inline">\(\beta\)</span>) of the useless variables towards
zero.
è¿™æ˜¯ä¸€ç§æ›´å·§å¦™çš„æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œå²­å›å½’ã€LASSOï¼‰ã€‚ä½ ä¸æ˜¯ä¸¢å¼ƒå˜é‡ï¼Œè€Œæ˜¯<em>ä¿ç•™æ‰€æœ‰
<span class="math inline">\(p\)</span>
ä¸ªå˜é‡</em>ï¼Œä½†ä¼šç»™æ¨¡å‹æ·»åŠ ä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œå°†æ— ç”¨å˜é‡çš„ç³»æ•°ï¼ˆ<span
class="math inline">\(\beta\)</span>ï¼‰â€œæ”¶ç¼©â€åˆ°é›¶ã€‚</p></li>
</ol></li>
</ul>
<h2 id="questions">Questions ğŸ¯</h2>
<h3 id="q1-how-to-compare-which-model-is-better">Q1: â€œHow to compare
which model is better?â€</h3>
<p>(From slides <code>...221320.png</code> and
<code>...221326.png</code>)</p>
<p>This is the most important question. You <strong>cannot</strong> use
metrics based on <em>training data</em> (like <span
class="math inline">\(R^2\)</span> or RSS - Residual Sum of Squares) to
compare models with <em>different numbers of predictors</em>.
è¿™æ˜¯æœ€é‡è¦çš„é—®é¢˜ã€‚æ‚¨<strong>ä¸èƒ½</strong>ä½¿ç”¨åŸºäº<em>è®­ç»ƒæ•°æ®</em>çš„æŒ‡æ ‡ï¼ˆä¾‹å¦‚
R^2 æˆ– RSS - æ®‹å·®å¹³æ–¹å’Œï¼‰æ¥æ¯”è¾ƒå…·æœ‰<em>ä¸åŒæ•°é‡é¢„æµ‹å˜é‡</em>çš„æ¨¡å‹ã€‚</p>
<ul>
<li><p><strong>The Trap:</strong> A model with more predictors will
<em>always</em> have a higher <span class="math inline">\(R^2\)</span>
(or lower RSS) on the data it was trained on. <span
class="math inline">\(R^2\)</span> will <em>always</em> increase as you
add variables, even if they are pure noise. If you used <span
class="math inline">\(R^2\)</span> to compare a 3-predictor model to a
10-predictor model, the 10-predictor model would <em>always</em> look
better on paper, even if itâ€™s terribly overfit.
å…·æœ‰æ›´å¤šé¢„æµ‹å˜é‡çš„æ¨¡å‹åœ¨å…¶è®­ç»ƒæ•°æ®ä¸Š<em>æ€»æ˜¯</em>å…·æœ‰æ›´é«˜çš„
R^2ï¼ˆæˆ–æ›´ä½çš„ RSSï¼‰ã€‚éšç€å˜é‡çš„å¢åŠ ï¼ŒR^2
ä¼š<em>æ€»æ˜¯</em>å¢åŠ ï¼Œå³ä½¿è¿™äº›å˜é‡æ˜¯çº¯å™ªå£°ã€‚å¦‚æœæ‚¨ä½¿ç”¨ R^2 æ¥æ¯”è¾ƒ 3
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹å’Œ 10 ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œé‚£ä¹ˆ 10
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹åœ¨çº¸é¢ä¸Š<em>æ€»æ˜¯</em>çœ‹èµ·æ¥æ›´å¥½ï¼Œå³ä½¿å®ƒä¸¥é‡è¿‡æ‹Ÿåˆã€‚</p></li>
<li><p><strong>The Correct Way:</strong> You must use a metric that
estimates the <strong>test error</strong>. The slides and code show two
ways:æ‚¨å¿…é¡»ä½¿ç”¨ä¸€ä¸ªèƒ½å¤Ÿä¼°è®¡<strong>æµ‹è¯•è¯¯å·®</strong>çš„æŒ‡æ ‡ã€‚</p>
<ol type="1">
<li><strong>Cross-Validation (CV):</strong> This is the method used in
your Python code. It works by:
<ul>
<li>Splitting your training data into <span
class="math inline">\(k\)</span> â€œfoldsâ€ (e.g., 5 folds).
å°†è®­ç»ƒæ•°æ®æ‹†åˆ†æˆ <span class="math inline">\(k\)</span> ä¸ªâ€œæŠ˜å â€ï¼ˆä¾‹å¦‚ 5
ä¸ªæŠ˜å ï¼‰ã€‚</li>
<li>Training the model on 4 folds and testing it on the 5th fold.
ä½¿ç”¨å…¶ä¸­ 4 ä¸ªæŠ˜å è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ç¬¬ 5 ä¸ªæŠ˜å è¿›è¡Œæµ‹è¯•ã€‚</li>
<li>Repeating this 5 times, so each fold gets to be the test set once.
é‡å¤æ­¤æ“ä½œ 5 æ¬¡ï¼Œä½¿æ¯ä¸ªæŠ˜å éƒ½ä½œä¸ºæµ‹è¯•é›†ä¸€æ¬¡ã€‚</li>
<li>Averaging the 5 test errors. å¯¹ 5 ä¸ªæµ‹è¯•è¯¯å·®æ±‚å¹³å‡å€¼ã€‚ This gives
you a robust estimate of how your model will perform on <em>unseen
data</em>. You then choose the model with the best (lowest) average CV
error.
è¿™å¯ä»¥è®©ä½ å¯¹æ¨¡å‹åœ¨<em>æœªè§æ•°æ®</em>ä¸Šçš„è¡¨ç°æœ‰ä¸€ä¸ªç¨³å¥çš„ä¼°è®¡ã€‚ç„¶åï¼Œä½ å¯ä»¥é€‰æ‹©å¹³å‡
CV è¯¯å·®æœ€å°ï¼ˆæœ€ä½³ï¼‰çš„æ¨¡å‹ã€‚</li>
</ul></li>
<li><strong>Mathematical Adjustments (AIC, BIC, Adjusted <span
class="math inline">\(R^2\)</span>):</strong> These are formulas that
take the training error (like RSS) and add a <em>penalty</em> for each
predictor (<span class="math inline">\(k\)</span>) you add.
<ul>
<li><span class="math inline">\(AIC \approx RSS +
2k\sigma^2\)</span></li>
<li><span class="math inline">\(BIC \approx RSS +
\log(n)k\sigma^2\)</span> A model with more predictors (larger <span
class="math inline">\(k\)</span>) gets a bigger penalty. To be chosen, a
more complex model must <em>significantly</em> improve the RSS to
overcome this penalty. é¢„æµ‹å˜é‡è¶Šå¤šï¼ˆk
è¶Šå¤§ï¼‰çš„æ¨¡å‹ï¼Œæƒ©ç½šè¶Šå¤§ã€‚è¦è¢«é€‰ä¸­ï¼Œæ›´å¤æ‚çš„æ¨¡å‹å¿…é¡»<em>æ˜¾è‘—</em>æå‡ RSS
ä»¥å…‹æœæ­¤æƒ©ç½šã€‚</li>
</ul></li>
</ol></li>
</ul>
<h3 id="q2-why-using-r2-for-step-2">Q2: â€œWhy using <span
class="math inline">\(R^2\)</span> for step 2?â€</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p><strong>Step 2</strong> of the â€œBest Subset Selectionâ€ algorithm
says: â€œFor <span class="math inline">\(k = 1, \dots, p\)</span>: Fit all
<span class="math inline">\(\binom{p}{k}\)</span> modelsâ€¦ Pick the best
model, that with the largest <span class="math inline">\(R^2\)</span>, â€¦
and call it <span class="math inline">\(M_k\)</span>.â€ â€œå¯¹äº <span
class="math inline">\(k = 1, \dots, p\)</span>ï¼šæ‹Ÿåˆæ‰€æœ‰ <span
class="math inline">\(\binom{p}{k}\)</span> ä¸ªæ¨¡å‹â€¦â€¦é€‰æ‹©å…·æœ‰æœ€å¤§ <span
class="math inline">\(R^2\)</span> çš„æœ€ä½³æ¨¡å‹â€¦â€¦å¹¶å°†å…¶å‘½åä¸º <span
class="math inline">\(M_k\)</span>ã€‚â€</p>
<ul>
<li><strong>The Reason:</strong> In Step 2, you are <em>only</em>
comparing models <strong>of the same size</strong>. For example, when
<span class="math inline">\(k=3\)</span>, you are comparing all possible
3-predictor models: æ­¥éª¤ 2
ä¸­ï¼Œæ‚¨<em>ä»…</em>æ¯”è¾ƒ**ç›¸åŒå¤§å°çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå½“ <span
class="math inline">\(k=3\)</span> æ—¶ï¼Œæ‚¨å°†æ¯”è¾ƒæ‰€æœ‰å¯èƒ½çš„ 3
é¢„æµ‹å˜é‡æ¨¡å‹ï¼š
<ul>
<li>Model A: (<span class="math inline">\(X_1, X_2, X_3\)</span>)</li>
<li>Model B: (<span class="math inline">\(X_1, X_2, X_4\)</span>)</li>
<li>Model C: (<span class="math inline">\(X_1, X_3, X_5\)</span>)</li>
<li>â€¦and so on.</li>
</ul>
Since all these models have the <em>exact same complexity</em> (they all
have <span class="math inline">\(k=3\)</span> predictors), there is no
risk of unfairly favoring a more complex model. Therefore, you are free
to use a training metric like <span class="math inline">\(R^2\)</span>
(or RSS). The model with the highest <span
class="math inline">\(R^2\)</span> is, by definition, the one that
<em>best fits the training data</em> for that specific size <span
class="math inline">\(k\)</span>.
ç”±äºæ‰€æœ‰è¿™äº›æ¨¡å‹éƒ½å…·æœ‰<em>å®Œå…¨ç›¸åŒçš„å¤æ‚åº¦</em>ï¼ˆå®ƒä»¬éƒ½å…·æœ‰ <span
class="math inline">\(k=3\)</span>
ä¸ªé¢„æµ‹å˜é‡ï¼‰ï¼Œå› æ­¤ä¸å­˜åœ¨ä¸å…¬å¹³åœ°åå‘æ›´å¤æ‚æ¨¡å‹çš„é£é™©ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥è‡ªç”±ä½¿ç”¨åƒ
<span class="math inline">\(R^2\)</span>ï¼ˆæˆ–
RSSï¼‰è¿™æ ·çš„è®­ç»ƒæŒ‡æ ‡ã€‚æ ¹æ®å®šä¹‰ï¼Œå…·æœ‰æœ€é«˜ <span
class="math inline">\(R^2\)</span> çš„æ¨¡å‹å°±æ˜¯åœ¨ç‰¹å®šå¤§å° <span
class="math inline">\(k\)</span>
ä¸‹<em>ä¸è®­ç»ƒæ•°æ®æ‹Ÿåˆåº¦</em>æœ€é«˜çš„æ¨¡å‹ã€‚</li>
</ul>
<h3
id="q3-cannot-use-training-error-in-step-3.-why-not-æ­¥éª¤-3-ä¸­ä¸èƒ½ä½¿ç”¨è®­ç»ƒè¯¯å·®-ä¸ºä»€ä¹ˆ">Q3:
â€œCannot use training error in Step 3.â€ Why not? â€œæ­¥éª¤ 3
ä¸­ä¸èƒ½ä½¿ç”¨è®­ç»ƒè¯¯å·®ã€‚â€ ä¸ºä»€ä¹ˆï¼Ÿ</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p><strong>Step 3</strong> says: â€œSelect a single best model from <span
class="math inline">\(M_0, M_1, \dots, M_p\)</span> by cross validation,
AIC, or BIC.â€â€œé€šè¿‡äº¤å‰éªŒè¯ã€AIC æˆ– BICï¼Œä» <span
class="math inline">\(M_0ã€M_1ã€\dotsã€M_p\)</span>
ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä½³æ¨¡å‹ã€‚â€</p>
<ul>
<li><p><strong>The Reason:</strong> In Step 3, you are now comparing
models <strong>of different sizes</strong>. You are comparing the best
1-predictor model (<span class="math inline">\(M_1\)</span>) vs.Â the
best 2-predictor model (<span class="math inline">\(M_2\)</span>)
vs.Â the best 3-predictor model (<span
class="math inline">\(M_3\)</span>), and so on, all the way up to <span
class="math inline">\(M_p\)</span>. åœ¨æ­¥éª¤ 3
ä¸­ï¼Œæ‚¨æ­£åœ¨æ¯”è¾ƒ<strong>ä¸åŒå¤§å°</strong>çš„æ¨¡å‹ã€‚æ‚¨æ­£åœ¨æ¯”è¾ƒæœ€ä½³çš„å•é¢„æµ‹æ¨¡å‹
(<span class="math inline">\(M_1\)</span>)ã€æœ€ä½³çš„åŒé¢„æµ‹æ¨¡å‹ (<span
class="math inline">\(M_2\)</span>) å’Œæœ€ä½³çš„ä¸‰é¢„æµ‹æ¨¡å‹ (<span
class="math inline">\(M_3\)</span>)ï¼Œä¾æ­¤ç±»æ¨ï¼Œç›´åˆ° <span
class="math inline">\(M_p\)</span>ã€‚</p>
<p>As explained in Q1, if you used a training error metric like <span
class="math inline">\(R^2\)</span> here, the <span
class="math inline">\(R^2\)</span> would just keep going up, and you
would <em>always</em> select the largest, most complex model, <span
class="math inline">\(M_p\)</span>. This completely defeats the purpose
of model selection. å¦‚é—®é¢˜ 1 æ‰€è¿°ï¼Œå¦‚æœæ‚¨åœ¨æ­¤å¤„ä½¿ç”¨åƒ <span
class="math inline">\(R^2\)</span> è¿™æ ·çš„è®­ç»ƒè¯¯å·®æŒ‡æ ‡ï¼Œé‚£ä¹ˆ <span
class="math inline">\(R^2\)</span>
ä¼šæŒç»­ä¸Šå‡ï¼Œå¹¶ä¸”æ‚¨<em>æ€»æ˜¯</em>ä¼šé€‰æ‹©æœ€å¤§ã€æœ€å¤æ‚çš„æ¨¡å‹ <span
class="math inline">\(M_p\)</span>ã€‚è¿™å®Œå…¨è¿èƒŒäº†æ¨¡å‹é€‰æ‹©çš„ç›®çš„ã€‚</p>
<p>Therefore, in Step 3, you <em>must</em> use a method that estimates
<strong>test error</strong> (like Cross-Validation) or one that
<strong>penalizes for complexity</strong> (like AIC or BIC) to find the
â€œsweet spotâ€ model that balances fit and simplicity. å› æ­¤ï¼Œåœ¨æ­¥éª¤ 3
ä¸­ï¼Œæ‚¨<em>å¿…é¡»</em>ä½¿ç”¨ä¸€ç§ä¼°ç®—<strong>æµ‹è¯•è¯¯å·®</strong>çš„æ–¹æ³•ï¼ˆä¾‹å¦‚äº¤å‰éªŒè¯ï¼‰æˆ–<strong>æƒ©ç½šå¤æ‚æ€§</strong>çš„æ–¹æ³•ï¼ˆä¾‹å¦‚
AIC æˆ–
BICï¼‰ï¼Œä»¥æ‰¾åˆ°åœ¨æ‹Ÿåˆåº¦å’Œç®€å•æ€§ä¹‹é—´å–å¾—å¹³è¡¡çš„â€œæœ€ä½³ç‚¹â€æ¨¡å‹ã€‚</p></li>
</ul>
<h2 id="mathematical-deep-dive">Mathematical Deep Dive ğŸ§®</h2>
<ul>
<li><strong><span class="math inline">\(Y = \beta_0 + \beta_1X_1 + \dots
+ \beta_pX_p + \epsilon\)</span>:</strong> The full linear model. The
goal of subset selection is to find a subset of <span
class="math inline">\(X_j\)</span>â€™s where <span
class="math inline">\(\beta_j \neq 0\)</span> and set all other <span
class="math inline">\(\beta\)</span>â€™s to 0.
å®Œæ•´çš„çº¿æ€§æ¨¡å‹ã€‚å­é›†é€‰æ‹©çš„ç›®æ ‡æ˜¯æ‰¾åˆ° <span
class="math inline">\(X_j\)</span> çš„ä¸€ä¸ªå­é›†ï¼Œå…¶ä¸­ $_j ç­‰äº
0ï¼Œå¹¶å°†æ‰€æœ‰å…¶ä»– <span class="math inline">\(\beta\)</span> è®¾ç½®ä¸º
0ã€‚</li>
<li><strong><span class="math inline">\(2^p\)</span>
combinations:</strong> (Slide <code>...221333.png</code>) This is the
total number of models you have to check. For each of the <span
class="math inline">\(p\)</span> variables, you have two choices: either
it is <strong>IN</strong> the model or it is
<strong>OUT</strong>.è¿™æ˜¯ä½ éœ€è¦æ£€æŸ¥çš„æ¨¡å‹æ€»æ•°ã€‚å¯¹äºæ¯ä¸ª <span
class="math inline">\(p\)</span>
ä¸ªå˜é‡ï¼Œä½ æœ‰ä¸¤ä¸ªé€‰æ‹©ï¼šè¦ä¹ˆå®ƒåœ¨æ¨¡å‹<strong>å†…éƒ¨</strong>ï¼Œè¦ä¹ˆå®ƒåœ¨æ¨¡å‹<strong>å¤–éƒ¨</strong>ã€‚
<ul>
<li>Example: <span class="math inline">\(p=3\)</span> (variables <span
class="math inline">\(X_1, X_2, X_3\)</span>)</li>
<li>The <span class="math inline">\(2^3 = 8\)</span> possible models
are:
<ol type="1">
<li>{} (The null model, <span class="math inline">\(M_0\)</span>)</li>
<li>{ <span class="math inline">\(X_1\)</span> }</li>
<li>{ <span class="math inline">\(X_2\)</span> }</li>
<li>{ <span class="math inline">\(X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_2\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_2, X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_2, X_3\)</span> } (The full
model, <span class="math inline">\(M_3\)</span>)</li>
</ol></li>
<li>This is why this method is called an <strong>â€œexhaustive
searchâ€</strong>. It literally checks every single one. For <span
class="math inline">\(p=20\)</span>, <span
class="math inline">\(2^{20}\)</span> is over a million
models!è¿™å°±æ˜¯è¯¥æ–¹æ³•è¢«ç§°ä¸º<strong>â€œç©·ä¸¾æœç´¢â€</strong>çš„åŸå› ã€‚å®ƒå®é™…ä¸Šä¼šæ£€æŸ¥æ¯ä¸€ä¸ªæ¨¡å‹ã€‚å¯¹äº
<span class="math inline">\(p=20\)</span>ï¼Œ<span
class="math inline">\(2^{20}\)</span> å°±è¶…è¿‡ä¸€ç™¾ä¸‡ä¸ªæ¨¡å‹ï¼</li>
</ul></li>
<li><strong><span class="math inline">\(\binom{p}{k} =
\frac{p!}{k!(p-k)!}\)</span>:</strong> (Slide
<code>...221333.png</code>) This is the â€œcombinationsâ€ formula. It tells
you <em>how many</em> models you fit <em>in Step 2</em> for a specific
<span
class="math inline">\(k\)</span>.è¿™æ˜¯â€œç»„åˆâ€å…¬å¼ã€‚å®ƒå‘Šè¯‰ä½ ï¼Œå¯¹äºç‰¹å®šçš„
<span class="math inline">\(k\)</span>ï¼Œ<em>åœ¨æ­¥éª¤ 2</em>ä¸­ï¼Œä½ æ‹Ÿåˆäº†
<em>å¤šå°‘</em> ä¸ªæ¨¡å‹ã€‚
<ul>
<li>Example: <span class="math inline">\(p=10\)</span> total
predictors.</li>
<li>For <span class="math inline">\(k=1\)</span>: You fit <span
class="math inline">\(\binom{10}{1} = 10\)</span> models.</li>
<li>For <span class="math inline">\(k=2\)</span>: You fit <span
class="math inline">\(\binom{10}{2} = \frac{10 \times 9}{2 \times 1} =
45\)</span> models.</li>
<li>For <span class="math inline">\(k=3\)</span>: You fit <span
class="math inline">\(\binom{10}{3} = \frac{10 \times 9 \times 8}{3
\times 2 \times 1} = 120\)</span> models.</li>
<li>â€¦and so on. The sum of all these <span
class="math inline">\(\binom{p}{k}\)</span> from <span
class="math inline">\(k=0\)</span> to <span
class="math inline">\(k=p\)</span> equals <span
class="math inline">\(2^p\)</span>.</li>
</ul></li>
</ul>
<h2 id="detailed-code-analysis">Detailed Code Analysis ğŸ’»</h2>
<p>Your slides show Python code that applies the <strong>Best Subset
Selection algorithm</strong> to a <strong>KNN Regressor</strong>. This
is a great example of how the <em>selection algorithm</em> is
independent of the <em>model type</em> (as mentioned in slide
<code>...221314.png</code>).</p>
<h3 id="key-functions-1">Key Functions</h3>
<ul>
<li><strong><code>main()</code></strong>
<ol type="1">
<li><strong>Load &amp; Preprocess:</strong> Reads
<code>Credit.csv</code>. The most important step here is converting
categorical text (like â€˜Maleâ€™/â€˜Femaleâ€™) into numbers (1/0).</li>
<li><strong>Scale Data:</strong> <code>scaler = StandardScaler()</code>
and <code>X_scaled = scaler.fit_transform(X)</code>.
<ul>
<li><strong>WHY?</strong> This is <strong>CRITICAL</strong> for KNN. KNN
works by measuring distance. If â€˜Incomeâ€™ (e.g., 50,000) is on a vastly
different scale than â€˜Cardsâ€™ (e.g., 3), the â€˜Incomeâ€™ feature will
completely dominate the distance calculation, making â€˜Cardsâ€™ irrelevant.
Scaling resizes all features to have a mean of 0 and standard deviation
of 1, so they all contribute fairly.</li>
</ul></li>
<li><strong>Handle Noisy Data (Slide
<code>...221303.jpg</code>):</strong> This version of the code
<em>intentionally</em> adds 20 columns of useless, random numbers. This
is a test to see if the algorithm is smart enough to ignore them.</li>
<li><strong>Run Selection:</strong>
<code>results_df = best_subset_selection_parallel(...)</code>. This
function does all the heavy lifting (explained next).</li>
<li><strong>Find Best Model:</strong>
<code>results_df.sort_values(by='CV_Score', ascending=False)</code>.
<ul>
<li><strong>WHY <code>ascending=False</code>?</strong> The code uses the
metric <code>'neg_mean_squared_error'</code>. This is MSE, but negative
(e.g., -15000). A <em>better</em> model has an error closer to 0 (e.g.,
-10000). Since -10000 is <em>greater than</em> -15000, you sort in
descending (high-to-low) order to put the best models at the top.</li>
</ul></li>
<li><strong>Final Evaluation (Step 3):</strong>
<code>final_scores = cross_val_score(knn, X_best, y, ...)</code>
<ul>
<li>This is the implementation of Step 3. It takes <em>only</em> the
single best subset (<code>X_best</code>) and runs a <em>new</em>
cross-validation on it. This gives a final, unbiased estimate of how
good that one model is.</li>
</ul></li>
<li><strong>Print RMSE:</strong>
<code>final_rmse = np.sqrt(-final_scores)</code>. It converts the
negative MSE back into a positive RMSE (Root Mean Squared Error), which
is in the same units as the target <span
class="math inline">\(y\)</span> (in this case, â€˜Balanceâ€™ in
dollars).</li>
</ol></li>
<li><strong><code>best_subset_selection_parallel(model, ...)</code></strong>
<ol type="1">
<li>This is the â€œmanagerâ€ function. It implements the loop from Step
2.</li>
<li><code>for k in range(1, n_features + 1):</code> This is the loop
â€œFor <span class="math inline">\(k = 1, \dots, p\)</span>â€.</li>
<li><code>subsets = list(combinations(feature_names, k))</code>: This
generates the <span class="math inline">\(\binom{p}{k}\)</span>
combinations for the current <span
class="math inline">\(k\)</span>.</li>
<li><code>results = Parallel(n_jobs=n_jobs)(...)</code>: This is a
non-core, â€œspeed-upâ€ command. It uses the <code>joblib</code> library to
run the evaluations on all your computerâ€™s CPU cores at once (in
parallel). Without this, checking millions of models would take
days.</li>
<li><code>subset_scores = ... [delayed(evaluate_subset)(...) ...]</code>
This line farms out the <em>actual work</em> to the
<code>evaluate_subset</code> function for every single subset.</li>
</ol></li>
<li><strong><code>evaluate_subset(subset, ...)</code></strong>
<ol type="1">
<li>This is the â€œworkerâ€ function. It gets called thousands or millions
of times.</li>
<li>Its job is to evaluate <em>one single subset</em> (e.g.,
<code>('Income', 'Limit', 'Student')</code>).</li>
<li><code>X_subset = X[list(subset)]</code>: It slices the data to get
<em>only</em> these columns.</li>
<li><code>scores = cross_val_score(model, X_subset, ...)</code>:
<strong>This is the most important line.</strong> It takes the subset
and performs a full 5-fold cross-validation on it.</li>
<li><code>return (subset, np.mean(scores))</code>: It returns the subset
and its average CV score.</li>
</ol></li>
</ul>
<h3 id="summary-of-outputs-slides-...221255.png-...221309.png">Summary
of Outputs (Slides <code>...221255.png</code> &amp;
<code>...221309.png</code>)</h3>
<ul>
<li><strong>Original Data (Slide <code>...221255.png</code>):</strong>
<ul>
<li><strong>Best Subset:</strong>
<code>('Income', 'Limit', 'Rating', 'Student')</code></li>
<li><strong>Final RMSE:</strong> ~105.4</li>
</ul></li>
<li><strong>Data with 20 â€œNoisyâ€ Variables (Slide
<code>...221309.png</code>):</strong>
<ul>
<li><strong>Best Subset:</strong>
<code>('Income', 'Limit', 'Student')</code></li>
<li><strong>Result:</strong> The algorithm <em>successfully</em>
identified that all 20 â€œNoisyâ€ variables were useless and
<strong>excluded every single one of them</strong> from the best
models.</li>
<li><strong>Final RMSE:</strong> ~114.9</li>
<li><strong>Key Takeaway:</strong> The RMSE is slightly higher, which
makes sense because the selection problem was much harder. But the
<em>method worked perfectly</em>. It filtered all the â€œnoiseâ€ and found
a simple, powerful model, just as the theory on slide
<code>...221320.png</code> predicted.</li>
</ul></li>
</ul>
<h1
id="the-core-problem-training-error-vs.-test-error-æ ¸å¿ƒé—®é¢˜è®­ç»ƒè¯¯å·®-vs.-æµ‹è¯•è¯¯å·®">2.
The Core Problem: Training Error vs.Â Test Error æ ¸å¿ƒé—®é¢˜ï¼šè®­ç»ƒè¯¯å·®
vs.Â æµ‹è¯•è¯¯å·®</h1>
<p>The central theme of these slides is finding the â€œbestâ€ model. The
problem is that a model with more predictors (more complex) will
<em>always</em> fit the data it was trained on better. This is a trap.
å¯»æ‰¾â€œæœ€ä½³â€æ¨¡å‹ã€‚é—®é¢˜åœ¨äºï¼Œé¢„æµ‹å› å­è¶Šå¤šï¼ˆè¶Šå¤æ‚ï¼‰çš„æ¨¡å‹<em>æ€»æ˜¯</em>èƒ½æ›´å¥½åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚è¿™æ˜¯ä¸€ä¸ªé™·é˜±ã€‚</p>
<ul>
<li><strong>Training Error:</strong> How well the model fits the data we
used to build it. <strong><span class="math inline">\(R^2\)</span> and
<span class="math inline">\(RSS\)</span> measure this.</strong>
æ¨¡å‹ä¸æˆ‘ä»¬æ„å»ºæ¨¡å‹æ—¶æ‰€ç”¨æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ã€‚<strong><span
class="math inline">\(R^2\)</span> å’Œ <span
class="math inline">\(RSS\)</span> è¡¡é‡äº†è¿™ä¸€ç‚¹ã€‚</strong></li>
<li><strong>Test Error:</strong> How well the model predicts new, unseen
data. This is what we <em>actually</em> care about. A model that is too
complex (e.g., has 10 predictors when only 3 are useful) will have low
training error but very high test error. This is called
<strong>overfitting</strong>.
æ¨¡å‹é¢„æµ‹æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®çš„å‡†ç¡®ç¨‹åº¦ã€‚è¿™æ‰æ˜¯æˆ‘ä»¬<em>çœŸæ­£</em>å…³å¿ƒçš„ã€‚è¿‡äºå¤æ‚çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œæœ‰
10 ä¸ªé¢„æµ‹å› å­ï¼Œä½†åªæœ‰ 3
ä¸ªæœ‰ç”¨ï¼‰çš„è®­ç»ƒè¯¯å·®ä¼šå¾ˆä½ï¼Œä½†æµ‹è¯•è¯¯å·®ä¼šå¾ˆé«˜ã€‚è¿™è¢«ç§°ä¸º<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚</li>
</ul>
<p>The goal is to choose a model that has the lowest <em>test
error</em>. The metrics below (Adjusted <span
class="math inline">\(R^2\)</span>, AIC, BIC) are all attempts to
<em>estimate</em> this test error without having to actually collect new
data. They do this by adding a <strong>penalty</strong> for complexity.
ç›®æ ‡æ˜¯é€‰æ‹©ä¸€ä¸ªå…·æœ‰æœ€ä½<em>æµ‹è¯•è¯¯å·®</em>çš„æ¨¡å‹ã€‚ä»¥ä¸‹æŒ‡æ ‡ï¼ˆè°ƒæ•´åçš„ <span
class="math inline">\(R^2\)</span>ã€AICã€BICï¼‰éƒ½æ˜¯åœ¨æ— éœ€å®é™…æ”¶é›†æ–°æ•°æ®çš„æƒ…å†µä¸‹å°è¯•<em>ä¼°è®¡</em>æ­¤æµ‹è¯•è¯¯å·®ã€‚ä»–ä»¬é€šè¿‡å¢åŠ <strong>å¤æ‚åº¦æƒ©ç½š</strong>æ¥å®ç°è¿™ä¸€ç‚¹ã€‚</p>
<h2 id="basic-metrics-measures-of-fit">Basic Metrics (Measures of
Fit)</h2>
<p>These formulas from slide 13 describe how well a model fits the
<em>training data</em>.</p>
<h3 id="residue-error-æ®‹å·®è¯¯å·®">Residue (Error) æ®‹å·®ï¼ˆè¯¯å·®ï¼‰</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\epsilon}_i = y_i - \hat{y}_i = y_i -
\hat{\beta}_0 - \sum_{j=1}^{p} \hat{\beta}_j x_{ij}\)</span></li>
<li><strong>Concept:</strong> This is the most basic building block.
Itâ€™s the difference between the <em>actual</em> observed value (<span
class="math inline">\(y_i\)</span>) and the value your model
<em>predicted</em> (<span class="math inline">\(\hat{y}_i\)</span>). It
is the â€œerrorâ€ for a single data point.
è¿™æ˜¯æœ€åŸºæœ¬çš„æ„å»ºå—ã€‚å®ƒæ˜¯<em>å®é™…</em>è§‚æµ‹å€¼ (<span
class="math inline">\(y_i\)</span>) ä¸æ¨¡å‹*é¢„æµ‹å€¼ (<span
class="math inline">\(\hat{y}_i\)</span>)
ä¹‹é—´çš„å·®å€¼ã€‚å®ƒæ˜¯å•ä¸ªæ•°æ®ç‚¹çš„â€œè¯¯å·®â€ã€‚</li>
</ul>
<h3 id="residual-sum-of-squares-rss-æ®‹å·®å¹³æ–¹å’Œ-rss">Residual Sum of
Squares (RSS) æ®‹å·®å¹³æ–¹å’Œ (RSS)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(RSS =
\sum_{i=1}^{n} \hat{\epsilon}_i^2\)</span></li>
<li><strong>Concept:</strong> This is the overall measure of model
error. You square all the individual errors (residues) to make them
positive and then add them all up.
è¿™æ˜¯æ¨¡å‹è¯¯å·®çš„æ€»ä½“åº¦é‡ã€‚å°†æ‰€æœ‰å•ä¸ªè¯¯å·®ï¼ˆæ®‹å·®ï¼‰å¹³æ–¹ï¼Œä½¿å…¶ä¸ºæ­£ï¼Œç„¶åå°†å®ƒä»¬å…¨éƒ¨ç›¸åŠ ã€‚</li>
<li><strong>Goal:</strong> The entire process of linear regression
(called â€œOrdinary Least Squaresâ€) is designed to find the <span
class="math inline">\(\hat{\beta}\)</span> coefficients that make this
<strong>RSS value as small as possible</strong>.
æ•´ä¸ªçº¿æ€§å›å½’è¿‡ç¨‹ï¼ˆç§°ä¸ºâ€œæ™®é€šæœ€å°äºŒä¹˜æ³•â€ï¼‰æ—¨åœ¨æ‰¾åˆ°ä½¿<strong>RSS
å€¼å°½å¯èƒ½å°</strong>çš„ <span class="math inline">\(\hat{\beta}\)</span>
ä¸ªç³»æ•°ã€‚</li>
<li><strong>The Flaw ç¼ºé™·:</strong> <span
class="math inline">\(RSS\)</span> will <em>always</em> decrease (or
stay the same) as you add more predictors (<span
class="math inline">\(p\)</span>). A model with all 10 predictors will
have a lower <span class="math inline">\(RSS\)</span> than a model with
9, even if that 10th predictor is useless. Therefore, <span
class="math inline">\(RSS\)</span> is useless for choosing
<em>between</em> models of different sizes. éšç€é¢„æµ‹å˜é‡ (<span
class="math inline">\(p\)</span>) çš„å¢åŠ ï¼Œ<span
class="math inline">\(RSS\)</span>
æ€»æ˜¯ä¼šå‡å°ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ã€‚ä¸€ä¸ªåŒ…å«æ‰€æœ‰ 10 ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹çš„ <span
class="math inline">\(RSS\)</span> ä¼šä½äºä¸€ä¸ªåŒ…å« 9
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œå³ä½¿ç¬¬ 10 ä¸ªé¢„æµ‹å˜é‡æ¯«æ— ç”¨å¤„ã€‚å› æ­¤ï¼Œ<span
class="math inline">\(RSS\)</span>
å¯¹äºåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¹‹é—´è¿›è¡Œé€‰æ‹©æ¯«æ— ç”¨å¤„ã€‚</li>
</ul>
<h3 id="r-squared-r2">R-squared (<span
class="math inline">\(R^2\)</span>)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(R^2 = 1 -
\frac{SS_{error}}{SS_{total}} = 1 - \frac{RSS}{\sum_{i=1}^{n} (y_i -
\bar{y})^2}\)</span></li>
<li><strong>Concept:</strong> This metric reframes <span
class="math inline">\(RSS\)</span> into a more interpretable
percentage.æ­¤æŒ‡æ ‡å°† <span class="math inline">\(RSS\)</span>
é‡æ–°å®šä¹‰ä¸ºæ›´æ˜“äºè§£é‡Šçš„ç™¾åˆ†æ¯”ã€‚
<ul>
<li><span class="math inline">\(SS_{total}\)</span> (the denominator)
represents the <em>total variance</em> of the data. Itâ€™s the error you
would get if your â€œmodelâ€ was just guessing the average value (<span
class="math inline">\(\bar{y}\)</span>) for every single observation.
ï¼ˆåˆ†æ¯ï¼‰è¡¨ç¤ºæ•°æ®çš„<em>æ€»æ–¹å·®</em>ã€‚å¦‚æœä½ çš„â€œæ¨¡å‹â€åªæ˜¯çŒœæµ‹æ¯ä¸ªè§‚æµ‹å€¼çš„å¹³å‡å€¼
(<span
class="math inline">\(\bar{y}\)</span>)ï¼Œé‚£ä¹ˆä½ å°±ä¼šå¾—åˆ°è¿™ä¸ªè¯¯å·®ã€‚</li>
<li><span class="math inline">\(SS_{error}\)</span> (the <span
class="math inline">\(RSS\)</span>) is the error <em>after</em> using
your model. æ˜¯â€œæ¨¡å‹è§£é‡Šçš„æ€»æ–¹å·®çš„æ¯”ä¾‹â€ã€‚ <span
class="math inline">\(R^2\)</span> ä¸º 0.75
æ„å‘³ç€ä½ çš„æ¨¡å‹å¯ä»¥è§£é‡Šå“åº”å˜é‡ 75% çš„å˜å¼‚ã€‚</li>
<li><span class="math inline">\(R^2\)</span> is the â€œproportion of total
variance explained by the model.â€ An <span
class="math inline">\(R^2\)</span> of 0.75 means your model can explain
75% of the variation in the response variable.</li>
</ul></li>
<li><strong>The Flaw ç¼ºé™·:</strong> Just like <span
class="math inline">\(RSS\)</span>, <span
class="math inline">\(R^2\)</span> will <em>always</em> increase (or
stay the same) as you add more predictors. This is visually confirmed in
Figure 6.1, where the red line for <span
class="math inline">\(R^2\)</span> only goes up. It will always pick the
most complex model. ä¸ <span class="math inline">\(RSS\)</span>
ä¸€æ ·ï¼Œéšç€é¢„æµ‹å˜é‡çš„å¢åŠ ï¼Œ<span class="math inline">\(R^2\)</span>
ä¼š<em>å§‹ç»ˆ</em>å¢åŠ ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ã€‚å›¾ 6.1 ç›´è§‚åœ°è¯å®äº†è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­ <span
class="math inline">\(R^2\)</span>
çš„çº¢çº¿åªä¼šä¸Šå‡ã€‚å®ƒæ€»æ˜¯ä¼šé€‰æ‹©æœ€å¤æ‚çš„æ¨¡å‹ã€‚</li>
</ul>
<h2
id="advanced-metrics-for-model-selection-é«˜çº§æŒ‡æ ‡ç”¨äºæ¨¡å‹é€‰æ‹©">Advanced
Metrics (For Model Selection) é«˜çº§æŒ‡æ ‡ï¼ˆç”¨äºæ¨¡å‹é€‰æ‹©ï¼‰</h2>
<p>These metrics â€œfixâ€ the flaw of <span
class="math inline">\(R^2\)</span> by including a penalty for the number
of predictors.</p>
<h3 id="adjusted-r2">Adjusted <span
class="math inline">\(R^2\)</span></h3>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
  \text{Adjusted } R^2 = 1 - \frac{RSS / (n - p - 1)}{SS_{total} / (n -
1)}
  \]</span></li>
<li><strong>Mathematical Concept:</strong> This formula replaces the
â€œSum of Squaresâ€ (<span class="math inline">\(SS\)</span>) with â€œMean
Squaresâ€ (<span class="math inline">\(MS\)</span>).
<ul>
<li><span class="math inline">\(MS_{error} =
\frac{RSS}{n-p-1}\)</span></li>
<li><span class="math inline">\(MS_{total} =
\frac{SS_{total}}{n-1}\)</span></li>
</ul></li>
<li><strong>The â€œPenaltyâ€ Explained:</strong> The penalty is
<strong>degrees of freedom</strong>.
<ul>
<li><span class="math inline">\(n\)</span> = number of data points.</li>
<li><span class="math inline">\(p\)</span> = number of predictors.</li>
<li>The term <span class="math inline">\(n-p-1\)</span> is the degrees
of freedom for the residuals. You start with <span
class="math inline">\(n\)</span> data points, but you â€œuse upâ€ one
degree of freedom to estimate the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) and <span
class="math inline">\(p\)</span> more to estimate the <span
class="math inline">\(p\)</span> slopes.</li>
</ul></li>
<li><strong>How it Works:</strong>
<ol type="1">
<li>When you add a new predictor (increase <span
class="math inline">\(p\)</span>), <span
class="math inline">\(RSS\)</span> goes down, which makes the numerator
(<span class="math inline">\(MS_{error}\)</span>) smaller.</li>
<li>â€¦But, increasing <span class="math inline">\(p\)</span>
<em>also</em> decreases the denominator (<span
class="math inline">\(n-p-1\)</span>), which makes the numerator (<span
class="math inline">\(MS_{error}\)</span>) <em>larger</em>.</li>
</ol>
<ul>
<li>This creates a â€œtug-of-war.â€ If the new predictor is
<strong>useful</strong>, it will drop <span
class="math inline">\(RSS\)</span> a lot, and Adjusted <span
class="math inline">\(R^2\)</span> will <strong>increase</strong>. If
the new predictor is <strong>useless</strong>, <span
class="math inline">\(RSS\)</span> will barely change, and the penalty
from decreasing the denominator will win, causing Adjusted <span
class="math inline">\(R^2\)</span> to <strong>decrease</strong>.</li>
</ul></li>
<li><strong>Goal:</strong> You select the model with the
<strong>highest</strong> Adjusted <span
class="math inline">\(R^2\)</span>.</li>
</ul>
<h3 id="akaike-information-criterion-aic">Akaike Information Criterion
(AIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Concept Breakdown:</strong>
<ul>
<li><span class="math inline">\(\ell(\hat{\theta})\)</span>: This is the
<strong>Maximized Likelihood Function</strong>.
<ul>
<li>The <strong>Likelihood Function</strong> <span
class="math inline">\(\ell(\theta)\)</span> asks: â€œGiven a set of model
parameters <span class="math inline">\(\theta\)</span>, how probable is
the data we observed?â€</li>
<li>The <strong>Maximum Likelihood Estimate (MLE)</strong> <span
class="math inline">\(\hat{\theta}\)</span> is the specific set of
parameters (the <span class="math inline">\(\hat{\beta}\)</span>â€™s) that
<em>maximizes</em> this probability.</li>
</ul></li>
<li><span class="math inline">\(\log \ell(\hat{\theta})\)</span>: The
<strong>log-likelihood</strong>. This is just a number that represents
the <em>best possible fit</em> the model can achieve for the data. A
higher number is a better fit.</li>
<li><span class="math inline">\(-2 \log \ell(\hat{\theta})\)</span>:
This is the <strong>Deviance</strong>. Since a higher log-likelihood is
better, a <em>lower</em> deviance is better. This term measures
<strong>poorness-of-fit</strong>.</li>
<li><span class="math inline">\(d\)</span>: The number of parameters
estimated by the model. (e.g., <span class="math inline">\(p\)</span>
predictors + 1 intercept).</li>
<li><span class="math inline">\(2d\)</span>: This is the <strong>Penalty
Term</strong>.</li>
</ul></li>
<li><strong>How it Works:</strong> <span class="math inline">\(AIC =
(\text{Poorness-of-Fit}) + (\text{Complexity Penalty})\)</span>. As you
add predictors, the fit gets better (the deviance term goes down), but
the penalty term (<span class="math inline">\(2d\)</span>) goes up.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> AIC.</li>
</ul>
<h3 id="bayesian-information-criterion-bic">Bayesian Information
Criterion (BIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(BIC =
-2 \log \ell(\hat{\theta}) + \log(n)d\)</span></li>
<li><strong>Concept:</strong> This is mathematically identical to AIC,
but the penalty term is different.
<ul>
<li><strong>AIC Penalty:</strong> <span
class="math inline">\(2d\)</span></li>
<li><strong>BIC Penalty:</strong> <span
class="math inline">\(\log(n)d\)</span></li>
</ul></li>
<li><strong>Comparison:</strong>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations
in your dataset.</li>
<li>As long as your dataset has 8 or more observations (<span
class="math inline">\(n \ge 8\)</span>), <span
class="math inline">\(\log(n)\)</span> will be greater than 2.</li>
<li>This means <strong>BIC applies a much harsher penalty for
complexity</strong> than AIC.</li>
</ul></li>
<li><strong>Consequence:</strong> BIC will tend to choose
<em>simpler</em> models (fewer predictors) than AIC.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> BIC.</li>
</ul>
<h2 id="the-deeper-theory-why-aic-works">The Deeper Theory: Why AIC
Works</h2>
<p>Slide 27 (â€œUnderstanding AICâ€) gives the deep mathematical
justification.</p>
<ul>
<li><strong>Goal:</strong> We have a <em>true</em>, unknown process
<span class="math inline">\(p\)</span> that generates our data. We are
creating a model <span class="math inline">\(\hat{p}_j\)</span>. We want
our model to be as â€œcloseâ€ to the truth as possible.</li>
<li><strong>Kullback-Leibler (K-L) Distance:</strong> This is a function
<span class="math inline">\(K(p, \hat{p}_j)\)</span> that measures the
â€œinformation lostâ€ when you use your model <span
class="math inline">\(\hat{p}_j\)</span> to approximate the truth <span
class="math inline">\(p\)</span>. You want to <em>minimize</em> this
distance.</li>
<li><strong>The Math:</strong>
<ol type="1">
<li><span class="math inline">\(K(p, \hat{p}_j) = \int p(y) \log \left(
\frac{p(y)}{\hat{p}_j(y)} \right) dy\)</span></li>
<li>This splits into: <span class="math inline">\(K(p, \hat{p}_j) =
\underbrace{\int p(y) \log(p(y)) dy}_{\text{Constant}} -
\underbrace{\int p(y) \log(\hat{p}_j(y)) dy}_{\text{This is what we need
to maximize}}\)</span></li>
</ol></li>
<li><strong>The Problem:</strong> We canâ€™t calculate that second term
because it requires knowing the <em>true</em> function <span
class="math inline">\(p\)</span>.</li>
<li><strong>Akaikeâ€™s Insight:</strong> Akaike proved that the
log-likelihood we <em>can</em> calculate, <span
class="math inline">\(\log \ell(\hat{\theta})\)</span>, is a
<em>biased</em> estimator of that target. He also proved that the bias
is approximately <span class="math inline">\(-d\)</span>.</li>
<li><strong>The Solution:</strong> An <em>unbiased</em> estimate of the
target is <span class="math inline">\(\log \ell(\hat{\theta}) -
d\)</span>.</li>
<li><strong>Final Step:</strong> For historical and statistical reasons,
he multiplied this by <span class="math inline">\(-2\)</span> to create
the final AIC formula.</li>
<li><strong>Conclusion:</strong> AIC is not just a random formula. It is
a carefully derived estimate of how much information your model loses
compared to the â€œtruthâ€ (i.e., its expected performance on new
data).</li>
</ul>
<h2 id="aicbic-for-linear-regression">AIC/BIC for Linear Regression</h2>
<p>Slide 26 shows how these general formulas simplify for linear
regression (assuming normal, Gaussian errors).</p>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Linear Regression Formula:</strong> <span
class="math inline">\(AIC = \frac{1}{n\hat{\sigma}^2}(RSS +
2d\hat{\sigma}^2)\)</span></li>
</ul>
<p><strong>Key Insight:</strong> For linear regression, the
â€œpoorness-of-fitâ€ term (<span class="math inline">\(-2 \log
\ell(\hat{\theta})\)</span>) is <em>directly proportional to</em> the
<span class="math inline">\(RSS\)</span>.</p>
<p>This makes it much easier to understand. You can just think of the
formulas as: * <strong>AIC <span class="math inline">\(\approx\)</span>
<span class="math inline">\(RSS + 2d\hat{\sigma}^2\)</span></strong> *
<strong>BIC <span class="math inline">\(\approx\)</span> <span
class="math inline">\(RSS + \log(n)d\hat{\sigma}^2\)</span></strong></p>
<p>(Here <span class="math inline">\(\hat{\sigma}^2\)</span> is an
estimate of the error variance, which can often be treated as a
constant).</p>
<p>This clearly shows the trade-off: We want a model with a low
<strong><span class="math inline">\(RSS\)</span></strong> (good fit) and
a low <strong><span class="math inline">\(d\)</span></strong> (low
complexity). These two goals are in direct competition.</p>
<p><strong>Mallowâ€™s <span class="math inline">\(C_p\)</span>:</strong>
The slide notes that <span class="math inline">\(C_p\)</span> is
equivalent to AIC for linear regression. The <span
class="math inline">\(C_p\)</span> formula is <span
class="math inline">\(C_p = \frac{1}{n}(RSS +
2d\hat{\sigma}^2_{full})\)</span>, where <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> is the error
variance estimated from the <em>full</em> model. Since <span
class="math inline">\(n\)</span> and <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> are constants,
minimizing <span class="math inline">\(C_p\)</span> is mathematically
identical to minimizing <span class="math inline">\(RSS +
2d\hat{\sigma}^2_{full}\)</span>, which is the same logic as AIC.</p>
<p>Here is a detailed breakdown of the mathematical formulas and
concepts from your slides.</p>
<h2 id="the-core-problem-training-error-vs.-test-error">The Core
Problem: Training Error vs.Â Test Error</h2>
<p>The central theme of these slides is finding the â€œbestâ€ model. The
problem is that a model with more predictors (more complex) will
<em>always</em> fit the data it was trained on better. This is a
trap.</p>
<ul>
<li><strong>Training Error:</strong> How well the model fits the data we
used to build it. <strong><span class="math inline">\(R^2\)</span> and
<span class="math inline">\(RSS\)</span> measure this.</strong></li>
<li><strong>Test Error:</strong> How well the model predicts new, unseen
data. This is what we <em>actually</em> care about. A model that is too
complex (e.g., has 10 predictors when only 3 are useful) will have low
training error but very high test error. This is called
<strong>overfitting</strong>.</li>
</ul>
<p>The goal is to choose a model that has the lowest <em>test
error</em>. The metrics below (Adjusted <span
class="math inline">\(R^2\)</span>, AIC, BIC) are all attempts to
<em>estimate</em> this test error without having to actually collect new
data. They do this by adding a <strong>penalty</strong> for
complexity.</p>
<h2 id="basic-metrics-measures-of-fit-1">Basic Metrics (Measures of
Fit)</h2>
<p>These formulas from slide 13 describe how well a model fits the
<em>training data</em>.</p>
<h3 id="residue-error">Residue (Error)</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\epsilon}_i = y_i - \hat{y}_i = y_i -
\hat{\beta}_0 - \sum_{j=1}^{p} \hat{\beta}_j x_{ij}\)</span></li>
<li><strong>Concept:</strong> This is the most basic building block.
Itâ€™s the difference between the <em>actual</em> observed value (<span
class="math inline">\(y_i\)</span>) and the value your model
<em>predicted</em> (<span class="math inline">\(\hat{y}_i\)</span>). It
is the â€œerrorâ€ for a single data point.</li>
</ul>
<h3 id="residual-sum-of-squares-rss">Residual Sum of Squares (RSS)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(RSS =
\sum_{i=1}^{n} \hat{\epsilon}_i^2\)</span></li>
<li><strong>Concept:</strong> This is the overall measure of model
error. You square all the individual errors (residues) to make them
positive and then add them all up.</li>
<li><strong>Goal:</strong> The entire process of linear regression
(called â€œOrdinary Least Squaresâ€) is designed to find the <span
class="math inline">\(\hat{\beta}\)</span> coefficients that make this
<strong>RSS value as small as possible</strong>.</li>
<li><strong>The Flaw:</strong> <span class="math inline">\(RSS\)</span>
will <em>always</em> decrease (or stay the same) as you add more
predictors (<span class="math inline">\(p\)</span>). A model with all 10
predictors will have a lower <span class="math inline">\(RSS\)</span>
than a model with 9, even if that 10th predictor is useless. Therefore,
<span class="math inline">\(RSS\)</span> is useless for choosing
<em>between</em> models of different sizes.</li>
</ul>
<h3 id="r-squared-r2-1">R-squared (<span
class="math inline">\(R^2\)</span>)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(R^2 = 1 -
\frac{SS_{error}}{SS_{total}} = 1 - \frac{RSS}{\sum_{i=1}^{n} (y_i -
\bar{y})^2}\)</span></li>
<li><strong>Concept:</strong> This metric reframes <span
class="math inline">\(RSS\)</span> into a more interpretable percentage.
<ul>
<li><span class="math inline">\(SS_{total}\)</span> (the denominator)
represents the <em>total variance</em> of the data. Itâ€™s the error you
would get if your â€œmodelâ€ was just guessing the average value (<span
class="math inline">\(\bar{y}\)</span>) for every single
observation.</li>
<li><span class="math inline">\(SS_{error}\)</span> (the <span
class="math inline">\(RSS\)</span>) is the error <em>after</em> using
your model.</li>
<li><span class="math inline">\(R^2\)</span> is the â€œproportion of total
variance explained by the model.â€ An <span
class="math inline">\(R^2\)</span> of 0.75 means your model can explain
75% of the variation in the response variable.</li>
</ul></li>
<li><strong>The Flaw:</strong> Just like <span
class="math inline">\(RSS\)</span>, <span
class="math inline">\(R^2\)</span> will <em>always</em> increase (or
stay the same) as you add more predictors. This is visually confirmed in
Figure 6.1, where the red line for <span
class="math inline">\(R^2\)</span> only goes up. It will always pick the
most complex model.</li>
</ul>
<h2 id="advanced-metrics-for-model-selection">Advanced Metrics (For
Model Selection)</h2>
<p>These metrics â€œfixâ€ the flaw of <span
class="math inline">\(R^2\)</span> by including a penalty for the number
of predictors.</p>
<h3 id="adjusted-r2-1">Adjusted <span
class="math inline">\(R^2\)</span></h3>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
  \text{Adjusted } R^2 = 1 - \frac{RSS / (n - p - 1)}{SS_{total} / (n -
1)}
  \]</span></li>
<li><strong>Mathematical Concept:</strong> This formula replaces the
â€œSum of Squaresâ€ (<span class="math inline">\(SS\)</span>) with â€œMean
Squaresâ€ (<span class="math inline">\(MS\)</span>).
<ul>
<li><span class="math inline">\(MS_{error} =
\frac{RSS}{n-p-1}\)</span></li>
<li><span class="math inline">\(MS_{total} =
\frac{SS_{total}}{n-1}\)</span></li>
</ul></li>
<li><strong>The â€œPenaltyâ€ Explained:</strong> The penalty is
<strong>degrees of freedom</strong>.
<ul>
<li><span class="math inline">\(n\)</span> = number of data points.</li>
<li><span class="math inline">\(p\)</span> = number of predictors.</li>
<li>The term <span class="math inline">\(n-p-1\)</span> is the degrees
of freedom for the residuals. You start with <span
class="math inline">\(n\)</span> data points, but you â€œuse upâ€ one
degree of freedom to estimate the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) and <span
class="math inline">\(p\)</span> more to estimate the <span
class="math inline">\(p\)</span> slopes.</li>
</ul></li>
<li><strong>How it Works:</strong>
<ol type="1">
<li>When you add a new predictor (increase <span
class="math inline">\(p\)</span>), <span
class="math inline">\(RSS\)</span> goes down, which makes the numerator
(<span class="math inline">\(MS_{error}\)</span>) smaller.</li>
<li>â€¦But, increasing <span class="math inline">\(p\)</span>
<em>also</em> decreases the denominator (<span
class="math inline">\(n-p-1\)</span>), which makes the numerator (<span
class="math inline">\(MS_{error}\)</span>) <em>larger</em>.</li>
</ol>
<ul>
<li>This creates a â€œtug-of-war.â€ If the new predictor is
<strong>useful</strong>, it will drop <span
class="math inline">\(RSS\)</span> a lot, and Adjusted <span
class="math inline">\(R^2\)</span> will <strong>increase</strong>. If
the new predictor is <strong>useless</strong>, <span
class="math inline">\(RSS\)</span> will barely change, and the penalty
from decreasing the denominator will win, causing Adjusted <span
class="math inline">\(R^2\)</span> to <strong>decrease</strong>.</li>
</ul></li>
<li><strong>Goal:</strong> You select the model with the
<strong>highest</strong> Adjusted <span
class="math inline">\(R^2\)</span>.</li>
</ul>
<h3 id="akaike-information-criterion-aic-1">Akaike Information Criterion
(AIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Concept Breakdown:</strong>
<ul>
<li><span class="math inline">\(\ell(\hat{\theta})\)</span>: This is the
<strong>Maximized Likelihood Function</strong>.
<ul>
<li>The <strong>Likelihood Function</strong> <span
class="math inline">\(\ell(\theta)\)</span> asks: â€œGiven a set of model
parameters <span class="math inline">\(\theta\)</span>, how probable is
the data we observed?â€</li>
<li>The <strong>Maximum Likelihood Estimate (MLE)</strong> <span
class="math inline">\(\hat{\theta}\)</span> is the specific set of
parameters (the <span class="math inline">\(\hat{\beta}\)</span>â€™s) that
<em>maximizes</em> this probability.</li>
</ul></li>
<li><span class="math inline">\(\log \ell(\hat{\theta})\)</span>: The
<strong>log-likelihood</strong>. This is just a number that represents
the <em>best possible fit</em> the model can achieve for the data. A
higher number is a better fit.</li>
<li><span class="math inline">\(-2 \log \ell(\hat{\theta})\)</span>:
This is the <strong>Deviance</strong>. Since a higher log-likelihood is
better, a <em>lower</em> deviance is better. This term measures
<strong>poorness-of-fit</strong>.</li>
<li><span class="math inline">\(d\)</span>: The number of parameters
estimated by the model. (e.g., <span class="math inline">\(p\)</span>
predictors + 1 intercept).</li>
<li><span class="math inline">\(2d\)</span>: This is the <strong>Penalty
Term</strong>.</li>
</ul></li>
<li><strong>How it Works:</strong> <span class="math inline">\(AIC =
(\text{Poorness-of-Fit}) + (\text{Complexity Penalty})\)</span>. As you
add predictors, the fit gets better (the deviance term goes down), but
the penalty term (<span class="math inline">\(2d\)</span>) goes up.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> AIC.</li>
</ul>
<h3 id="bayesian-information-criterion-bic-1">Bayesian Information
Criterion (BIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(BIC =
-2 \log \ell(\hat{\theta}) + \log(n)d\)</span></li>
<li><strong>Concept:</strong> This is mathematically identical to AIC,
but the penalty term is different.
<ul>
<li><strong>AIC Penalty:</strong> <span
class="math inline">\(2d\)</span></li>
<li><strong>BIC Penalty:</strong> <span
class="math inline">\(\log(n)d\)</span></li>
</ul></li>
<li><strong>Comparison:</strong>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations
in your dataset.</li>
<li>As long as your dataset has 8 or more observations (<span
class="math inline">\(n \ge 8\)</span>), <span
class="math inline">\(\log(n)\)</span> will be greater than 2.</li>
<li>This means <strong>BIC applies a much harsher penalty for
complexity</strong> than AIC.</li>
</ul></li>
<li><strong>Consequence:</strong> BIC will tend to choose
<em>simpler</em> models (fewer predictors) than AIC.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> BIC.</li>
</ul>
<h2 id="the-deeper-theory-why-aic-works-1">The Deeper Theory: Why AIC
Works</h2>
<p>Slide 27 (â€œUnderstanding AICâ€) gives the deep mathematical
justification.</p>
<ul>
<li><strong>Goal:</strong> We have a <em>true</em>, unknown process
<span class="math inline">\(p\)</span> that generates our data. We are
creating a model <span class="math inline">\(\hat{p}_j\)</span>. We want
our model to be as â€œcloseâ€ to the truth as possible.</li>
<li><strong>Kullback-Leibler (K-L) Distance:</strong> This is a function
<span class="math inline">\(K(p, \hat{p}_j)\)</span> that measures the
â€œinformation lostâ€ when you use your model <span
class="math inline">\(\hat{p}_j\)</span> to approximate the truth <span
class="math inline">\(p\)</span>. You want to <em>minimize</em> this
distance.</li>
<li><strong>The Math:</strong>
<ol type="1">
<li><span class="math inline">\(K(p, \hat{p}_j) = \int p(y) \log \left(
\frac{p(y)}{\hat{p}_j(y)} \right) dy\)</span></li>
<li>This splits into: <span class="math inline">\(K(p, \hat{p}_j) =
\underbrace{\int p(y) \log(p(y)) dy}_{\text{Constant}} -
\underbrace{\int p(y) \log(\hat{p}_j(y)) dy}_{\text{This is what we need
to maximize}}\)</span></li>
</ol></li>
<li><strong>The Problem:</strong> We canâ€™t calculate that second term
because it requires knowing the <em>true</em> function <span
class="math inline">\(p\)</span>.</li>
<li><strong>Akaikeâ€™s Insight:</strong> Akaike proved that the
log-likelihood we <em>can</em> calculate, <span
class="math inline">\(\log \ell(\hat{\theta})\)</span>, is a
<em>biased</em> estimator of that target. He also proved that the bias
is approximately <span class="math inline">\(-d\)</span>.</li>
<li><strong>The Solution:</strong> An <em>unbiased</em> estimate of the
target is <span class="math inline">\(\log \ell(\hat{\theta}) -
d\)</span>.</li>
<li><strong>Final Step:</strong> For historical and statistical reasons,
he multiplied this by <span class="math inline">\(-2\)</span> to create
the final AIC formula.</li>
<li><strong>Conclusion:</strong> AIC is not just a random formula. It is
a carefully derived estimate of how much information your model loses
compared to the â€œtruthâ€ (i.e., its expected performance on new
data).</li>
</ul>
<h2 id="aicbic-for-linear-regression-1">AIC/BIC for Linear
Regression</h2>
<p>Slide 26 shows how these general formulas simplify for linear
regression (assuming normal, Gaussian errors).</p>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Linear Regression Formula:</strong> <span
class="math inline">\(AIC = \frac{1}{n\hat{\sigma}^2}(RSS +
2d\hat{\sigma}^2)\)</span></li>
</ul>
<p><strong>Key Insight:</strong> For linear regression, the
â€œpoorness-of-fitâ€ term (<span class="math inline">\(-2 \log
\ell(\hat{\theta})\)</span>) is <em>directly proportional to</em> the
<span class="math inline">\(RSS\)</span>.</p>
<p>This makes it much easier to understand. You can just think of the
formulas as: * <strong>AIC <span class="math inline">\(\approx\)</span>
<span class="math inline">\(RSS + 2d\hat{\sigma}^2\)</span></strong> *
<strong>BIC <span class="math inline">\(\approx\)</span> <span
class="math inline">\(RSS + \log(n)d\hat{\sigma}^2\)</span></strong></p>
<p>(Here <span class="math inline">\(\hat{\sigma}^2\)</span> is an
estimate of the error variance, which can often be treated as a
constant).</p>
<p>This clearly shows the trade-off: We want a model with a low
<strong><span class="math inline">\(RSS\)</span></strong> (good fit) and
a low <strong><span class="math inline">\(d\)</span></strong> (low
complexity). These two goals are in direct competition.</p>
<p><strong>Mallowâ€™s <span class="math inline">\(C_p\)</span>:</strong>
The slide notes that <span class="math inline">\(C_p\)</span> is
equivalent to AIC for linear regression. The <span
class="math inline">\(C_p\)</span> formula is <span
class="math inline">\(C_p = \frac{1}{n}(RSS +
2d\hat{\sigma}^2_{full})\)</span>, where <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> is the error
variance estimated from the <em>full</em> model. Since <span
class="math inline">\(n\)</span> and <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> are constants,
minimizing <span class="math inline">\(C_p\)</span> is mathematically
identical to minimizing <span class="math inline">\(RSS +
2d\hat{\sigma}^2_{full}\)</span>, which is the same logic as AIC.</p>
<h1 id="variable-selection">3. Variable Selection</h1>
<h2 id="core-concept-the-problem-of-variable-selection">Core Concept:
The Problem of Variable Selection</h2>
<p>In regression, we want to model a response variable <span
class="math inline">\(Y\)</span> using a set of <span
class="math inline">\(p\)</span> predictor variables <span
class="math inline">\(X_1, X_2, ..., X_p\)</span>.</p>
<ul>
<li><p><strong>The â€œKitchen Sinkâ€ Problem:</strong> A common temptation
is to include all available predictors in the model: <span
class="math display">\[Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... +
\beta_pX_p + \epsilon\]</span> This often leads to
<strong>overfitting</strong>. The model may fit the training data well
but will perform poorly on new, unseen data. Itâ€™s also hard to interpret
a model with dozens of predictors.</p></li>
<li><p><strong>The Solution: Subset Selection.</strong> The goal is to
find a smaller subset of the predictors that builds a model that is:</p>
<ol type="1">
<li><strong>Accurate:</strong> Has low prediction error.</li>
<li><strong>Parsimonious:</strong> Uses the fewest predictors
necessary.</li>
<li><strong>Interpretable:</strong> Is simple enough for a human to
understand.</li>
</ol></li>
</ul>
<p>Your slides present two main methods to achieve this: <strong>Best
Subset Selection</strong> and <strong>Forward Stepwise
Selection</strong>.</p>
<h2 id="method-1-best-subset-selection-bss">Method 1: Best Subset
Selection (BSS)</h2>
<p>This is the â€œbrute forceâ€ approach. It considers <em>every single
possible model</em>.</p>
<h3 id="conceptual-algorithm">Conceptual Algorithm</h3>
<ol type="1">
<li>Fit all models with <span class="math inline">\(k=1\)</span>
predictor (there are <span class="math inline">\(p\)</span> of these).
Find the best one (lowest RSS) and call it <span
class="math inline">\(M_1\)</span>.</li>
<li>Fit all models with <span class="math inline">\(k=2\)</span>
predictors (there are <span class="math inline">\(\binom{p}{2}\)</span>
of these). Find the best one and call it <span
class="math inline">\(M_2\)</span>.</li>
<li>â€¦</li>
<li>Fit the one model with <span class="math inline">\(k=p\)</span>
predictors (the full model), <span
class="math inline">\(M_p\)</span>.</li>
<li>You now have a list of <span class="math inline">\(p\)</span> â€œbestâ€
models: <span class="math inline">\(M_1, M_2, ..., M_p\)</span>.</li>
<li>Use a selection criterion (like <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>BIC</strong>,
<strong>AIC</strong>, or <strong><span
class="math inline">\(C_p\)</span></strong>) to choose the single best
model from this list.</li>
</ol>
<h3
id="mathematical-computational-cost-from-slide-225641.png">Mathematical
&amp; Computational Cost (from slide <code>225641.png</code>)</h3>
<ul>
<li>For each predictor, there are two possibilities: itâ€™s either
<strong>IN</strong> the model or <strong>OUT</strong>.</li>
<li>With <span class="math inline">\(p\)</span> predictors, the total
number of models to test is <span class="math inline">\(2 \times 2
\times ... \times 2\)</span> (<span class="math inline">\(p\)</span>
times).</li>
<li><strong>Total Models = <span
class="math inline">\(2^p\)</span></strong></li>
<li>This is a â€œcombinatorial explosion.â€ As the slide notes, if <span
class="math inline">\(p=20\)</span>, <span class="math inline">\(2^{20}
= 1,048,576\)</span> models. This is computationally infeasible for
large <span class="math inline">\(p\)</span>.</li>
</ul>
<h2 id="method-2-forward-stepwise-selection-fss">Method 2: Forward
Stepwise Selection (FSS)</h2>
<p>This is a â€œgreedyâ€ algorithm. Itâ€™s an efficient alternative to BSS
that does <em>not</em> test every model.</p>
<h3
id="conceptual-algorithm-from-slides-225645.png-225648.png">Conceptual
Algorithm (from slides <code>225645.png</code> &amp;
<code>225648.png</code>)</h3>
<ul>
<li><p><strong>Step 1:</strong> Start with the <strong>null
model</strong>, <span class="math inline">\(M_0\)</span>, which has no
predictors. <span class="math display">\[M_0: Y = \beta_0 +
\epsilon\]</span> The prediction is just the sample mean of <span
class="math inline">\(Y\)</span>.</p></li>
<li><p><strong>Step 2 (Iterative):</strong></p>
<ul>
<li><strong>For <span class="math inline">\(k=0\)</span> (to get <span
class="math inline">\(M_1\)</span>):</strong> Fit all <span
class="math inline">\(p\)</span> models that add <em>one</em> predictor
to <span class="math inline">\(M_0\)</span>. Choose the best one (lowest
<strong>RSS</strong> or highest <strong><span
class="math inline">\(R^2\)</span></strong>). This is <span
class="math inline">\(M_1\)</span>. Letâ€™s say it contains <span
class="math inline">\(X_1\)</span>.</li>
<li><strong>For <span class="math inline">\(k=1\)</span> (to get <span
class="math inline">\(M_2\)</span>):</strong> <em>Keep</em> <span
class="math inline">\(X_1\)</span> in the model. Fit all <span
class="math inline">\(p-1\)</span> models that add <em>one more</em>
predictor to <span class="math inline">\(M_1\)</span> (e.g., <span
class="math inline">\(M_1+X_2\)</span>, <span
class="math inline">\(M_1+X_3\)</span>, â€¦). Choose the best of these.
This is <span class="math inline">\(M_2\)</span>.</li>
<li><strong>Repeat:</strong> Continue this process, adding one variable
at a time, until all <span class="math inline">\(p\)</span> predictors
are in the model <span class="math inline">\(M_p\)</span>.</li>
</ul></li>
<li><p><strong>Step 3:</strong> You now have a sequence of <span
class="math inline">\(p+1\)</span> models: <span
class="math inline">\(M_0, M_1, ..., M_p\)</span>. Choose the single
best model from this sequence using <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>AIC</strong>,
<strong>BIC</strong>, or <strong><span
class="math inline">\(C_p\)</span></strong>.</p></li>
</ul>
<h3
id="mathematical-computational-cost-from-slide-225651.png">Mathematical
&amp; Computational Cost (from slide <code>225651.png</code>)</h3>
<ul>
<li>To find <span class="math inline">\(M_1\)</span>, you fit <span
class="math inline">\(p\)</span> models.</li>
<li>To find <span class="math inline">\(M_2\)</span>, you fit <span
class="math inline">\(p-1\)</span> models.</li>
<li>To find <span class="math inline">\(M_p\)</span>, you fit <span
class="math inline">\(1\)</span> model.</li>
<li>The null model <span class="math inline">\(M_0\)</span> is 1
model.</li>
<li><strong>Total Models = <span class="math inline">\(1 +
\sum_{k=0}^{p-1} (p-k) = 1 + p + (p-1) + ... + 1 = 1 +
\frac{p(p+1)}{2}\)</span></strong></li>
<li>As the slide notes, if <span class="math inline">\(p=20\)</span>,
this is only <span class="math inline">\(1 + 20(21)/2 = 211\)</span>
models. This is vastly more efficient than BSS.</li>
<li><strong>Key weakness:</strong> The method is â€œgreedy.â€ If it adds
<span class="math inline">\(X_1\)</span> in Step 1, it can
<em>never</em> be removed. Itâ€™s possible the true best 2-variable model
is <span class="math inline">\((X_2, X_3)\)</span>, but if FSS chose
<span class="math inline">\(X_1\)</span> as the best 1-variable model,
it will never find <span class="math inline">\((X_2, X_3)\)</span>.</li>
</ul>
<h2 id="how-to-choose-the-best-model-the-criteria">4. How to Choose the
â€œBestâ€ Model: The Criteria</h2>
<p>You canâ€™t use <strong>RSS</strong> or <strong><span
class="math inline">\(R^2\)</span></strong> to compare models with
<em>different numbers of predictors</em> (<span
class="math inline">\(k\)</span>). This is because RSS always decreases
(and <span class="math inline">\(R^2\)</span> always increases) as you
add more variables. You <em>must</em> use a criterion that penalizes
complexity.</p>
<ul>
<li><p><strong>RSS (Residual Sum of Squares):</strong> Goal is to
<strong>minimize</strong>. <span class="math display">\[RSS =
\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span> Good for comparing models
<em>of the same size <span
class="math inline">\(k\)</span></em>.</p></li>
<li><p><strong>Adjusted R-squared (<span class="math inline">\(Adj.
R^2\)</span>):</strong> Goal is to <strong>maximize</strong>. <span
class="math display">\[Adj. R^2 = 1 -
\frac{(1-R^2)(n-1)}{n-p-1}\]</span> This â€œadjustsâ€ <span
class="math inline">\(R^2\)</span> by adding a penalty for having more
predictors (<span class="math inline">\(p\)</span>). Adding a useless
predictor will make <span class="math inline">\(Adj. R^2\)</span> go
down.</p></li>
<li><p><strong>Mallowâ€™s <span
class="math inline">\(C_p\)</span>:</strong> Goal is to
<strong>minimize</strong>. <span class="math display">\[C_p \approx
\frac{1}{n}(RSS + 2p\hat{\sigma}^2)\]</span> Here, <span
class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the
error variance from the <em>full model</em> (with all <span
class="math inline">\(p\)</span> predictors). A good model will have
<span class="math inline">\(C_p \approx p\)</span>.</p></li>
<li><p><strong>AIC (Akaike Information Criterion) &amp; BIC (Bayesian
Information Criterion):</strong> Goal is to <strong>minimize</strong>.
<span class="math display">\[AIC = 2p - 2\ln(\hat{L})\]</span> <span
class="math display">\[BIC = p\ln(n) - 2\ln(\hat{L})\]</span> Here,
<span class="math inline">\(\hat{L}\)</span> is the maximized likelihood
of the model. You donâ€™t need to calculate this by hand; software
provides it.</p>
<ul>
<li><strong>Key difference:</strong> BICâ€™s penalty for <span
class="math inline">\(p\)</span> is <span
class="math inline">\(p\ln(n)\)</span>, while AICâ€™s is <span
class="math inline">\(2p\)</span>. Since <span
class="math inline">\(\ln(n)\)</span> is almost always <span
class="math inline">\(&gt; 2\)</span> (for <span
class="math inline">\(n&gt;7\)</span>), <strong>BIC applies a much
heavier penalty for complexity</strong>.</li>
<li>This means <strong>BIC tends to choose smaller, more parsimonious
models</strong> than AIC or <span class="math inline">\(Adj.
R^2\)</span>.</li>
</ul></li>
</ul>
<h2 id="python-code-analysis-slide-225546.jpg">5. Python Code Analysis
(Slide <code>225546.jpg</code>)</h2>
<p>This slide shows the Python code for <strong>Best Subset
Selection</strong> (BSS).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations <span class="comment"># &lt;-- This is the BSS engine</span></span><br></pre></td></tr></table></figure>
<h3 id="block-1-load-the-credit-dataset">Block 1: Load the Credit
dataset</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Load the Credit dataset</span></span><br><span class="line">Credit = pd.read_csv(<span class="string">&#x27;Credit.csv&#x27;</span>)</span><br><span class="line">Credit[<span class="string">&#x27;ID&#x27;</span>] = Credit[<span class="string">&#x27;ID&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">(num_samples, num_predictors) = Credit.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert categorical text data to numerical (dummy variables)</span></span><br><span class="line">Credit[<span class="string">&#x27;Gender&#x27;</span>] = Credit[<span class="string">&#x27;Gender&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Male&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Female&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Student&#x27;</span>] = Credit[<span class="string">&#x27;Student&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Yes&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;No&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Married&#x27;</span>] = Credit[<span class="string">&#x27;Married&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Yes&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;No&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Ethnicity&#x27;</span>] = Credit[<span class="string">&#x27;Ethnicity&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Asian&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Caucasian&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;African American&#x27;</span>: <span class="number">0</span>&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>pd.read_csv</code></strong>: Reads the data into a
<code>pandas</code> DataFrame.</li>
<li><strong><code>.map()</code></strong>: This is a crucial
preprocessing step. Regression models require numbers, not text like
â€˜Yesâ€™ or â€˜Maleâ€™. This line converts those strings into <code>1</code>s
and <code>0</code>s.</li>
</ul>
<h3 id="block-2-plot-scatterplot-matrix">Block 2: Plot scatterplot
matrix</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. Plot scatterplot matrix</span></span><br><span class="line">selected_columns = [<span class="string">&#x27;Balance&#x27;</span>, <span class="string">&#x27;Education&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Cards&#x27;</span>, <span class="string">&#x27;Rating&#x27;</span>, <span class="string">&#x27;Limit&#x27;</span>, <span class="string">&#x27;Income&#x27;</span>]</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&quot;ticks&quot;</span>)</span><br><span class="line">sns.pairplot(Credit[selected_columns], diag_kind=<span class="string">&#x27;kde&#x27;</span>)</span><br><span class="line">plt.suptitle(<span class="string">&#x27;Scatterplot Matrix&#x27;</span>, y=<span class="number">1.02</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>sns.pairplot</code></strong>: A powerful visualization
from the <code>seaborn</code> library. The resulting plot (right side of
the slide) is a grid.
<ul>
<li><strong>Diagonal plots (kde)</strong>: Show the distribution (Kernel
Density Estimate) of a single variable (e.g., â€˜Balanceâ€™ is skewed
right).</li>
<li><strong>Off-diagonal plots (scatter)</strong>: Show the relationship
between two variables (e.g., â€˜Limitâ€™ and â€˜Ratingâ€™ are almost perfectly
linear). This helps you visually spot potentially strong
predictors.</li>
</ul></li>
</ul>
<h3 id="block-3-best-subset-selection">Block 3: Best Subset
Selection</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Best Subset Selection</span></span><br><span class="line"><span class="comment"># (This code is incomplete on the slide, I&#x27;ll fill in the logic)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define target and predictors</span></span><br><span class="line">target = <span class="string">&#x27;Balance&#x27;</span></span><br><span class="line">predictors = [col <span class="keyword">for</span> col <span class="keyword">in</span> Credit.columns <span class="keyword">if</span> col != target] </span><br><span class="line">nvmax = <span class="number">10</span> <span class="comment"># Max number of predictors to test (up to 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize lists to store model statistics</span></span><br><span class="line">model_stats = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over number of predictors from 1 to nvmax</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, nvmax + <span class="number">1</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Generate all possible combinations of predictors of size k</span></span><br><span class="line">    <span class="comment"># This is the core of BSS</span></span><br><span class="line">    <span class="keyword">for</span> subset <span class="keyword">in</span> <span class="built_in">list</span>(combinations(predictors, k)):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the design matrix (X)</span></span><br><span class="line">        X_subset = Credit[<span class="built_in">list</span>(subset)]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Add a constant (intercept) term to the model</span></span><br><span class="line">        <span class="comment"># Y = B0 + B1*X1 -&gt; statsmodels needs B0 to be added manually</span></span><br><span class="line">        X_subset_const = sm.add_constant(X_subset)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the target variable (y)</span></span><br><span class="line">        y_target = Credit[target]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Fit the Ordinary Least Squares (OLS) model</span></span><br><span class="line">        model = sm.OLS(y_target, X_subset_const).fit()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate RSS</span></span><br><span class="line">        RSS = ((model.resid) ** <span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (The full code would also calculate R-squared, Adj. R-sq, BIC, etc. here)</span></span><br><span class="line">        <span class="comment"># model_stats.append(&#123;&#x27;k&#x27;: k, &#x27;subset&#x27;: subset, &#x27;RSS&#x27;: RSS, ...&#125;)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>for k in range(1, nvmax + 1)</code></strong>: This is
the <em>outer</em> loop that iterates from <span
class="math inline">\(k=1\)</span> (1 predictor) to <span
class="math inline">\(k=10\)</span> (10 predictors).</li>
<li><strong><code>list(combinations(predictors, k))</code></strong>:
This is the <em>inner</em> loop and the <strong>most important
line</strong>. The <code>itertools.combinations</code> function is a
highly efficient way to generate all unique subsets.
<ul>
<li>When <span class="math inline">\(k=1\)</span>, it returns
<code>[('Income',), ('Limit',), ('Rating',), ...]</code>.</li>
<li>When <span class="math inline">\(k=2\)</span>, it returns
<code>[('Income', 'Limit'), ('Income', 'Rating'), ('Limit', 'Rating'), ...]</code>.</li>
<li>This is what generates the <span class="math inline">\(2^p\)</span>
(or in this case, <span class="math inline">\(\sum_{k=1}^{10}
\binom{p}{k}\)</span>) models to test.</li>
</ul></li>
<li><strong><code>sm.add_constant(X_subset)</code></strong>: Your
regression equation is <span class="math inline">\(Y = \beta_0 +
\beta_1X_1\)</span>. The <span class="math inline">\(X_1\)</span> is
your <code>X_subset</code>. The <code>sm.add_constant</code> function
adds a column of <code>1</code>s to your data, which allows the
<code>statsmodels</code> library to estimate the <span
class="math inline">\(\beta_0\)</span> (intercept) term.</li>
<li><strong><code>sm.OLS(y_target, X_subset_const).fit()</code></strong>:
This fits the Ordinary Least Squares (OLS) model, which finds the <span
class="math inline">\(\beta\)</span> coefficients that <strong>minimize
the RSS</strong>.</li>
<li><strong><code>model.resid</code></strong>: This attribute of the
fitted model contains the residuals (<span class="math inline">\(e_i =
y_i - \hat{y}_i\)</span>) for each data point.</li>
<li><strong><code>((model.resid) ** 2).sum()</code></strong>: This line
is the direct code implementation of the formula <span
class="math inline">\(RSS = \sum e_i^2\)</span>.</li>
</ul>
<h2 id="synthesizing-the-results-the-plots">Synthesizing the Results
(The Plots)</h2>
<p>After running the BSS code, you get the data used in the plots and
the table.</p>
<ul>
<li><p><strong>Image <code>225550.png</code> (Adjusted
R-squared)</strong></p>
<ul>
<li><strong>Goal:</strong> Maximize.</li>
<li><strong>What it shows:</strong> The gray dots are <em>all</em> the
models tested for each <span class="math inline">\(k\)</span>. The red
line connects the single <em>best</em> model for each <span
class="math inline">\(k\)</span>.</li>
<li><strong>Conclusion:</strong> The plot shows a sharp â€œelbow.â€ The
<span class="math inline">\(Adj. R^2\)</span> increases dramatically up
to <span class="math inline">\(k=4\)</span>, then increases very slowly.
The maximum is around <span class="math inline">\(k=6\)</span> or <span
class="math inline">\(k=7\)</span>, but the gain after <span
class="math inline">\(k=4\)</span> is minimal.</li>
</ul></li>
<li><p><strong>Image <code>225554.png</code> (BIC)</strong></p>
<ul>
<li><strong>Goal:</strong> Minimize.</li>
<li><strong>What it shows:</strong> BIC heavily penalizes
complexity.</li>
<li><strong>Conclusion:</strong> The plot shows a very clear minimum.
The BIC value plummets from <span class="math inline">\(k=2\)</span> to
<span class="math inline">\(k=3\)</span> and hits its lowest point at
<strong><span class="math inline">\(k=4\)</span></strong>. After <span
class="math inline">\(k=4\)</span>, the penalty for adding more
variables is <em>larger</em> than the benefit in model fit, so the BIC
score starts to rise. This is a very strong vote for the 4-predictor
model.</li>
</ul></li>
<li><p><strong>Image <code>225635.png</code> (Mallowâ€™s <span
class="math inline">\(C_p\)</span>)</strong></p>
<ul>
<li><strong>Goal:</strong> Minimize.</li>
<li><strong>What it shows:</strong> A very similar story to BIC.</li>
<li><strong>Conclusion:</strong> The <span
class="math inline">\(C_p\)</span> value drops significantly and hits
its minimum at <strong><span
class="math inline">\(k=4\)</span></strong>.</li>
</ul></li>
<li><p><strong>Image <code>225638.png</code> (Summary
Table)</strong></p>
<ul>
<li>This is the <strong>most important image</strong> for the final
conclusion. It summarizes the red line from all the plots.</li>
<li>Look at the row for <code>Num_Predictors = 4</code>. The predictors
are <strong>(Income, Limit, Cards, Student)</strong>.</li>
<li>Now look at the columns for <code>BIC</code> and <code>Cp</code>.
<ul>
<li><strong>BIC:</strong> <code>4841.615607</code>. This is the lowest
value in the entire <code>BIC</code> column (the value at <span
class="math inline">\(k=3\)</span> is <code>4865.352851</code>).</li>
<li><strong>Cp:</strong> <code>7.122228</code>. This is also the lowest
value in the <code>Cp</code> column.</li>
</ul></li>
<li>The <code>Adj_R_squared</code> at <span
class="math inline">\(k=4\)</span> is <code>0.953580</code>, which is
very close to its maximum of <code>~0.954</code> at <span
class="math inline">\(k=7-10\)</span>.</li>
</ul></li>
</ul>
<p><strong>Final Conclusion:</strong> All three â€œpenalizedâ€ criteria
(Adjusted <span class="math inline">\(R^2\)</span>, BIC, and <span
class="math inline">\(C_p\)</span>) point to the same conclusion. While
<span class="math inline">\(Adj. R^2\)</span> is a bit ambiguous,
<strong>BIC and <span class="math inline">\(C_p\)</span> provide a clear
signal that the best, most parsimonious model is the 4-predictor model
using <code>Income</code>, <code>Limit</code>, <code>Cards</code>, and
<code>Student</code></strong>.</p>
<h1 id="subset-selection">4. Subset Selection</h1>
<h2 id="summary-of-subset-selection">Summary of Subset Selection</h2>
<p>These slides introduce <strong>subset selection</strong>, a process
in statistical learning used to identify the best subset of predictors
(variables) for a regression model. The goal is to find a model that has
low prediction error and avoids overfitting by excluding irrelevant
variables.</p>
<p>The slides cover two main â€œgreedyâ€ (stepwise) algorithms and the
criteria used to select the final best model.</p>
<h2 id="stepwise-selection-algorithms">Stepwise Selection
Algorithms</h2>
<p>Instead of testing all <span class="math inline">\(2^p\)</span>
possible models (which is â€œbest subset selectionâ€ and computationally
unfeasible), stepwise methods build a single path of models.</p>
<h3 id="forward-stepwise-selection">Forward Stepwise Selection</h3>
<p>This is an <strong>additive</strong> (bottom-up) approach:</p>
<ol type="1">
<li><strong>Start</strong> with the null model (no predictors).</li>
<li><strong>Find</strong> the best 1-variable model (the one that gives
the lowest Residual Sum of Squares, or RSS).</li>
<li><strong>Add</strong> the single variable that, when added to the
current model, results in the <em>new</em> best model (lowest RSS).</li>
<li><strong>Repeat</strong> this process until all <span
class="math inline">\(p\)</span> predictors are in the model.</li>
<li>This generates a sequence of <span
class="math inline">\(p+1\)</span> models, from <span
class="math inline">\(\mathcal{M}_0\)</span> to <span
class="math inline">\(\mathcal{M}_p\)</span>.</li>
</ol>
<h3 id="backward-stepwise-selection">Backward Stepwise Selection</h3>
<p>This is a <strong>subtractive</strong> (top-down) approach:</p>
<ol type="1">
<li><strong>Start</strong> with the full model containing all <span
class="math inline">\(p\)</span> predictors.</li>
<li><strong>Find</strong> the best <span
class="math inline">\((p-1)\)</span>-variable model by <em>removing</em>
the single variable that results in the <em>lowest RSS</em> (or highest
<span class="math inline">\(R^2\)</span>). This variable is considered
the least significant.</li>
<li><strong>Remove</strong> the next variable that, when removed from
the current best model, gives the new best model.</li>
<li><strong>Repeat</strong> until only the null model remains.</li>
<li>This also generates a sequence of <span
class="math inline">\(p+1\)</span> models.</li>
</ol>
<h4 id="pros-and-cons-backward-selection">Pros and Cons (Backward
Selection)</h4>
<ul>
<li><strong>Pro:</strong> Computationally efficient compared to best
subset. It fits <span class="math inline">\(1 + \sum_{k=0}^{p-1}(p-k) =
\mathbf{1 + p(p+1)/2}\)</span> models, which is much less than <span
class="math inline">\(2^p\)</span>. (e.g., for <span
class="math inline">\(p=20\)</span>, itâ€™s 211 models vs.Â &gt;1
million).</li>
<li><strong>Con:</strong> <strong>Cannot be used if <span
class="math inline">\(p &gt; n\)</span></strong> (more predictors than
observations), because the initial full model cannot be fit.</li>
<li><strong>Con (for both):</strong> These methods are
<strong>greedy</strong>. A variable added in forward selection is
<em>never removed</em>, and a variable removed in backward selection is
<em>never added back</em>. This means they are not guaranteed to find
the true best model.</li>
</ul>
<h2 id="choosing-the-final-best-model">Choosing the Final Best
Model</h2>
<p>Both forward and backward selection give you a set of candidate
models (e.g., the best 1-variable model, best 2-variable model, etc.).
You must then choose the <em>single best</em> one. The slides show two
main approaches:</p>
<h3 id="a.-direct-error-estimation">A. Direct Error Estimation</h3>
<p>Use a validation set or cross-validation (CV) to estimate the test
error for each model (e.g., the 1-variable, 2-variableâ€¦ models).
<strong>Choose the model with the lowest estimated test
error.</strong></p>
<h3 id="b.-adjusted-metrics-penalizing-for-complexity">B. Adjusted
Metrics (Penalizing for Complexity)</h3>
<p>Standard RSS and <span class="math inline">\(R^2\)</span> will always
improve as you add variables, leading to overfitting. Instead, use
metrics that <em>penalize</em> the model for having too many
predictors.</p>
<ul>
<li><p><strong>Mallowsâ€™ <span
class="math inline">\(C_p\)</span>:</strong> An estimate of test Mean
Squared Error (MSE). <span class="math display">\[C_p = \frac{1}{n} (RSS
+ 2d\hat{\sigma}^2)\]</span> (where <span
class="math inline">\(d\)</span> is the number of predictors, and <span
class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the
error variance). <strong>You want to find the model with the
<em>minimum</em> <span
class="math inline">\(C_p\)</span>.</strong></p></li>
<li><p><strong>BIC (Bayesian Information Criterion):</strong> <span
class="math display">\[BIC = \frac{1}{n} (RSS +
\log(n)d\hat{\sigma}^2)\]</span> BICâ€™s penalty <span
class="math inline">\(\log(n)\)</span> is stronger than <span
class="math inline">\(C_p\)</span>â€™s (or AICâ€™s) penalty of <span
class="math inline">\(2\)</span>, so it tends to select <em>smaller</em>
(more parsimonious) models. <strong>You want to find the model with the
<em>minimum</em> BIC.</strong></p></li>
<li><p><strong>Adjusted <span
class="math inline">\(R^2\)</span>:</strong> <span
class="math display">\[R^2_{adj} = 1 -
\frac{RSS/(n-d-1)}{TSS/(n-1)}\]</span> (where <span
class="math inline">\(TSS\)</span> is the Total Sum of Squares). Unlike
<span class="math inline">\(R^2\)</span>, this metric can decrease if
adding a variable doesnâ€™t help enough. <strong>You want to find the
model with the <em>maximum</em> Adjusted <span
class="math inline">\(R^2\)</span>.</strong></p></li>
</ul>
<h2 id="python-code-understanding">Python Code Understanding</h2>
<p>The slides use the <code>regsubsets()</code> function from the
<code>leaps</code> package in <strong>R</strong>.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># R Code from slides</span></span><br><span class="line">library<span class="punctuation">(</span>leaps<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Forward Selection</span></span><br><span class="line">regfit.fwd <span class="operator">&lt;-</span> regsubsets<span class="punctuation">(</span>Balance<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Credit<span class="punctuation">,</span> method<span class="operator">=</span><span class="string">&quot;forward&quot;</span><span class="punctuation">,</span> nvmax<span class="operator">=</span><span class="number">11</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Backward Selection</span></span><br><span class="line">regfit.bwd <span class="operator">&lt;-</span> regsubsets<span class="punctuation">(</span>Balance<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Credit<span class="punctuation">,</span> method<span class="operator">=</span><span class="string">&quot;backward&quot;</span><span class="punctuation">,</span> nvmax<span class="operator">=</span><span class="number">11</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>In <strong>Python</strong>, the standard tool for this is
<code>SequentialFeatureSelector</code> from
<strong><code>scikit-learn</code></strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SequentialFeatureSelector</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Credit&#x27; is a pandas DataFrame with &#x27;Balance&#x27; as the target</span></span><br><span class="line">X = Credit.drop(<span class="string">&#x27;Balance&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = Credit[<span class="string">&#x27;Balance&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the linear regression estimator</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Forward Selection ---</span></span><br><span class="line"><span class="comment"># direction=&#x27;forward&#x27; starts with 0 features and adds them</span></span><br><span class="line"><span class="comment"># To get the best 4-variable model, for example:</span></span><br><span class="line">sfs_forward = SequentialFeatureSelector(</span><br><span class="line">    model,</span><br><span class="line">    n_features_to_select=<span class="number">4</span>,</span><br><span class="line">    direction=<span class="string">&#x27;forward&#x27;</span>,</span><br><span class="line">    cv=<span class="literal">None</span> <span class="comment"># Or use cross-validation, e.g., cv=10</span></span><br><span class="line">)</span><br><span class="line">sfs_forward.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Forward selection best 4 features:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(sfs_forward.get_feature_names_out())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Backward Selection ---</span></span><br><span class="line"><span class="comment"># direction=&#x27;backward&#x27; starts with all features and removes them</span></span><br><span class="line">sfs_backward = SequentialFeatureSelector(</span><br><span class="line">    model,</span><br><span class="line">    n_features_to_select=<span class="number">4</span>,</span><br><span class="line">    direction=<span class="string">&#x27;backward&#x27;</span>,</span><br><span class="line">    cv=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line">sfs_backward.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nBackward selection best 4 features:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(sfs_backward.get_feature_names_out())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: To replicate the plots, you would loop this process,</span></span><br><span class="line"><span class="comment"># changing &#x27;n_features_to_select&#x27; from 1 to p,</span></span><br><span class="line"><span class="comment"># record the model scores (e.g., RSS, AIC, BIC) at each step,</span></span><br><span class="line"><span class="comment"># and then plot the results.</span></span><br></pre></td></tr></table></figure>
<h2 id="important-images">Important Images</h2>
<ol type="1">
<li><p><strong>Slide <code>...230014.png</code> (Forward Selection
Plots) &amp; <code>...230036.png</code> (Backward Selection
Plots):</strong></p>
<ul>
<li><strong>What they are:</strong> These <span class="math inline">\(2
\times 2\)</span> plot grids are the most important visuals. They show
<strong>Residual Sum of Squares (RSS)</strong>, <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>BIC</strong>, and
<strong>Mallowsâ€™ <span class="math inline">\(C_p\)</span></strong>
plotted against the <em>Number of Variables</em>.</li>
<li><strong>Why theyâ€™re important:</strong> They are the
<strong>decision-making tool</strong>. You use these plots to choose the
best model.
<ul>
<li>You look for the â€œelbowâ€ or <strong>minimum</strong> value for BIC
and <span class="math inline">\(C_p\)</span>.</li>
<li>You look for the â€œpeakâ€ or <strong>maximum</strong> value for
Adjusted <span class="math inline">\(R^2\)</span>.</li>
<li>(RSS is not used for selection as it always decreases).</li>
</ul></li>
</ul></li>
<li><p><strong>Slide <code>...230040.png</code> (Find the best
model):</strong></p>
<ul>
<li><strong>What it is:</strong> This slide shows a close-up of the
<span class="math inline">\(C_p\)</span>, BIC, and Adjusted <span
class="math inline">\(R^2\)</span> plots, with the â€œbestâ€ model (the
min/max) marked with a blue â€˜xâ€™.</li>
<li><strong>Why itâ€™s important:</strong> It explicitly states the
selection criteria. The text highlights that BIC suggests a 4-variable
model, while the other two are â€œrather flatâ€ after 4, making the choice
less obvious but pointing to a simple model.</li>
</ul></li>
<li><p><strong>Slide <code>...230045.png</code> (BIC vs.Â Validation
vs.Â CV):</strong></p>
<ul>
<li><strong>What it is:</strong> This shows three plots for selecting
the best model using different criteria: BIC, Validation Set Error, and
Cross-Validation Error.</li>
<li><strong>Why itâ€™s important:</strong> It shows that <strong>different
selection criteria can lead to different â€œbestâ€ models</strong>. Here,
BIC (a mathematical adjustment) picks a 4-variable model, while
validation and CV (direct error estimation) both pick a 6-variable
model.</li>
</ul></li>
</ol>
<p>The slides use the <code>Credit</code> dataset to demonstrate two key
tasks: 1. <strong>Running</strong> different subset selection algorithms
(forward, backward, best). 2. <strong>Using</strong> various statistical
metrics (BIC, <span class="math inline">\(C_p\)</span>, CV error) to
choose the single best model.</p>
<h2 id="comparing-selection-algorithms-the-path">Comparing Selection
Algorithms (The Path)</h2>
<p>This part of the example compares the <em>sequence</em> of models
selected by â€œForward Stepwiseâ€ selection versus â€œBest Subsetâ€
selection.</p>
<p><strong>Key Result (from Table 6.1):</strong></p>
<p>This table is the most important result for comparing the
algorithms.</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Variables</th>
<th style="text-align: left;">Best Subset</th>
<th style="text-align: left;">Forward Stepwise</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>one</strong></td>
<td style="text-align: left;"><code>rating</code></td>
<td style="text-align: left;"><code>rating</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>two</strong></td>
<td style="text-align: left;"><code>rating</code>,
<code>income</code></td>
<td style="text-align: left;"><code>rating</code>,
<code>income</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>three</strong></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>four</strong></td>
<td style="text-align: left;"><code>cards</code>, <code>income</code>,
<code>student</code>, <code>limit</code></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code>, <code>limit</code></td>
</tr>
</tbody>
</table>
<p><strong>Summary of this result:</strong></p>
<ul>
<li><strong>Identical for 1, 2, and 3 variables:</strong> Both methods
agree on the best one-variable model (<code>rating</code>), the best
two-variable model (<code>rating</code>, <code>income</code>), and the
best three-variable model (<code>rating</code>, <code>income</code>,
<code>student</code>).</li>
<li><strong>They Diverge at 4 variables:</strong>
<ul>
<li><strong>Forward selection</strong> is <em>greedy</em>. It started
with <code>rating</code>, <code>income</code>, <code>student</code> and
was â€œstuckâ€ with them. It then added <code>limit</code>, as that was the
best variable to <em>add</em> to its existing 3-variable model.</li>
<li><strong>Best subset selection</strong> is <em>not</em> greedy. It
tests all possible 4-variable combinations. It discovered that the model
<code>cards</code>, <code>income</code>, <code>student</code>,
<code>limit</code> has a slightly lower RSS than the model forward
selection found.</li>
</ul></li>
<li><strong>Main Takeaway:</strong> This demonstrates the limitation of
a greedy algorithm. Forward selection missed the â€œtrueâ€ best 4-variable
model because it was locked into its previous choices and couldnâ€™t â€œswap
outâ€ <code>rating</code> for <code>cards</code>.</li>
</ul>
<h2 id="choosing-the-single-best-model-the-destination">Choosing the
Single Best Model (The Destination)</h2>
<p>This is the most critical part of the analysis. After running a
selection algorithm (like forward, backward, or best subset), you get a
list of the â€œbestâ€ models for each size (best 1-variable, best
2-variable, etc.). Now you must decide: <strong>is the best model the
4-variable one, the 6-variable one, or another?</strong></p>
<p>The slides show several plots to help make this decision, all plotted
against the â€œNumber of Predictors.â€</p>
<p><strong>Summary of Plot Results:</strong></p>
<p>Hereâ€™s what each plot tells you:</p>
<ul>
<li><strong>Residual Sum of Squares (RSS)</strong> (e.g., in slide
<code>...230014.png</code>, top-left)
<ul>
<li><strong>What it shows:</strong> RSS <em>always</em> decreases as you
add more variables. It drops sharply until 4 variables, then flattens
out.</li>
<li><strong>Conclusion:</strong> This plot is <strong>not useful for
picking the best model</strong> because it will always pick the full
model, which is overfit. Itâ€™s only used to see the diminishing returns
of adding new variables.</li>
</ul></li>
<li><strong>Adjusted <span class="math inline">\(R^2\)</span></strong>
(e.g., in slide <code>...230040.png</code>, right)
<ul>
<li><strong>What it shows:</strong> This metric penalizes adding useless
variables. The plot rises quickly, then flattens, peaking at its
<strong>maximum value around 6 or 7 variables</strong>.</li>
<li><strong>Conclusion:</strong> This metric suggests a <strong>6 or
7-variable model</strong>.</li>
</ul></li>
<li><strong>Mallowsâ€™ <span class="math inline">\(C_p\)</span></strong>
(e.g., in slide <code>...230040.png</code>, left)
<ul>
<li><strong>What it shows:</strong> This is an estimate of test error.
We want the model with the <strong>minimum <span
class="math inline">\(C_p\)</span></strong>. The plot drops to a low
value at 4 variables and stays low, with its absolute minimum around
<strong>6 or 7 variables</strong>.</li>
<li><strong>Conclusion:</strong> This metric also suggests a <strong>6
or 7-variable model</strong>.</li>
</ul></li>
<li><strong>BIC (Bayesian Information Criterion)</strong> (e.g., in
slide <code>...230040.png</code>, center)
<ul>
<li><strong>What it shows:</strong> This is another estimate of test
error, but it has a <em>stronger penalty</em> for model complexity. The
plot shows a clear â€œUâ€ shape, reaching its <strong>minimum value at 4
variables</strong> and then <em>increasing</em> afterward.</li>
<li><strong>Conclusion:</strong> This metric strongly suggests a
<strong>4-variable model</strong>.</li>
</ul></li>
<li><strong>Validation Set &amp; Cross-Validation (CV) Error</strong>
(Slide <code>...230045.png</code>)
<ul>
<li><strong>What it shows:</strong> These plots show the <em>direct</em>
estimate of test error (not a mathematical adjustment like BIC or <span
class="math inline">\(C_p\)</span>). Both the validation set error and
the 10-fold CV error show a â€œUâ€ shape.</li>
<li><strong>Conclusion:</strong> Both methods reach their
<strong>minimum error at 6 variables</strong>. This is considered a very
reliable result.</li>
</ul></li>
</ul>
<h2 id="final-summary-of-results">Final Summary of Results</h2>
<p>The analysis of the <code>Credit</code> dataset reveals two strong
candidates for the â€œbestâ€ model, depending on your goal:</p>
<ol type="1">
<li><p><strong>The 6-Variable Model:</strong> This model is supported by
the <strong>Adjusted <span class="math inline">\(R^2\)</span></strong>,
<strong>Mallowsâ€™ <span class="math inline">\(C_p\)</span></strong>, and
(most importantly) the <strong>Validation Set</strong> and
<strong>10-fold Cross-Validation</strong> results. These metrics all
indicate that the 6-variable model has the <strong>lowest prediction
error</strong> on new data.</p></li>
<li><p><strong>The 4-Variable Model:</strong> This model is supported by
<strong>BIC</strong>. Because BIC penalizes complexity more heavily, it
selects a simpler (more <em>parsimonious</em>) model.</p></li>
</ol>
<p><strong>Overall Conclusion:</strong> If your primary goal is
<strong>maximum predictive accuracy</strong>, you should choose the
<strong>6-variable model</strong>. If your goal is a <strong>simpler,
more interpretable model</strong> that is still very good (and avoids
any risk of overfitting), the <strong>4-variable model</strong> is an
excellent choice.</p>
<h1
id="two-main-strategies-for-controlling-model-complexity-in-linear-regression">5.
Two main strategies for controlling model complexity in linear
regression</h1>
<p>This presentation covers two main strategies for controlling model
complexity in linear regression: <strong>Subset Selection</strong>
(choosing <em>which</em> variables to include) and <strong>Shrinkage
Methods</strong> (keeping all variables but <em>reducing the impact</em>
of their coefficients).</p>
<h2 id="subset-selection-1">Subset Selection</h2>
<p>This method involves selecting a subset of the <span
class="math inline">\(p\)</span> total predictors to use in the
model.</p>
<h3 id="key-concepts-formulas">Key Concepts &amp; Formulas</h3>
<ul>
<li><p><strong>The Model:</strong> The standard linear regression model
is represented in matrix form: <span class="math display">\[\mathbf{y} =
\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\]</span> The goal
of subset selection is to find a coefficient vector <span
class="math inline">\(\boldsymbol{\beta}\)</span> that is
<strong>sparse</strong>, meaning it has many zero entries.</p></li>
<li><p><strong>Forward Selection:</strong> This is a <em>greedy
algorithm</em> that starts with an empty model and iteratively adds the
single predictor that most improves the fit.</p></li>
<li><p><strong>Theoretical Guarantee:</strong> Can forward selection
find the <em>true</em> sparse set of variables?</p>
<ul>
<li>Yes, <em>if</em> the predictors are not strongly correlated.</li>
<li>This is quantified by the <strong>Mutual Coherence
Condition</strong>. Assuming the predictors <span
class="math inline">\(\mathbf{x}_i\)</span> are normalized, the method
is guaranteed to work if: <span class="math display">\[\mu = \max_{i
\neq j} |\langle \mathbf{x}_i, \mathbf{x}_j \rangle| &lt; \frac{1}{2s -
1}\]</span> where <span class="math inline">\(s\)</span> is the number
of true non-zero coefficients and <span class="math inline">\(\langle
\mathbf{x}_i, \mathbf{x}_j \rangle\)</span> represents the correlation
between predictors.</li>
</ul></li>
</ul>
<h3 id="practical-application-finding-the-best-model-size">Practical
Application: Finding the Best Model Size</h3>
<p>How do you know whether to choose a model with 3, 4, or 5 variables?
You use <strong>Cross-Validation (CV)</strong>.</p>
<ul>
<li><p><strong>Important Image:</strong> The plot titled â€œ10-fold CVâ€
(from the first slide) is the most important visual. It plots the
estimated test error (CV Error) on the y-axis against the number of
variables in the model on the x-axis.</p></li>
<li><p><strong>The â€œOne Standard Deviation Ruleâ€:</strong> Looking at
the plot, the error drops sharply and then flattens. The absolute
minimum error might be at 6 variables, but itâ€™s only slightly better
than the 3-variable model.</p>
<ol type="1">
<li>Find the model with the <em>lowest</em> CV error.</li>
<li>Calculate the standard error for that error estimate.</li>
<li>Select the <strong>simplest model</strong> (fewest variables) whose
error is <em>within one standard deviation</em> of the minimum.</li>
<li>This follows <strong>Occamâ€™s razor</strong>: choose the simplest
explanation (model) that fits the data well enough. In the example
given, this rule selects the 3-variable model.</li>
</ol></li>
</ul>
<h3 id="code-interpretation-r-vs.-python">Code Interpretation (R
vs.Â Python)</h3>
<p>The R code in the first slide performs this 10-fold CV manually for
forward selection:</p>
<ol type="1">
<li>It loops from <code>p = 1</code> to <code>10</code> (model
sizes).</li>
<li>Inside the loop, it identifies the <code>p</code> variables chosen
by a pre-computed forward selection model
(<code>regfit.fwd</code>).</li>
<li>It fits a new model (<code>glm.fit</code>) using <em>only</em> those
<code>p</code> variables.</li>
<li>It runs 10-fold CV (<code>cv.glm</code>) on <em>that specific
model</em> to get its test error.</li>
<li>It stores the error in <code>CV10.err[p]</code>.</li>
<li>Finally, it plots the results.</li>
</ol>
<p><strong>In Python (with <code>scikit-learn</code>):</strong> This
entire process is often automated.</p>
<ul>
<li>You would use <code>sklearn.feature_selection.RFECV</code>
(Recursive Feature Elimination with Cross-Validation).</li>
<li><code>RFECV</code> automatically performs cross-validation to find
the optimal number of features, effectively producing the same plot and
result as the R code.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Conceptual Python equivalent for finding the best model size</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFECV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"></span><br><span class="line"><span class="comment"># X, y = load_your_data()</span></span><br><span class="line">X, y = make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">10</span>, n_informative=<span class="number">3</span>, noise=<span class="number">10</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">estimator = LinearRegression()</span><br><span class="line"><span class="comment"># RFECV will test models with 1 feature, 2 features, etc.,</span></span><br><span class="line"><span class="comment"># and use cross-validation (cv=10) to find the best number.</span></span><br><span class="line">selector = RFECV(estimator, step=<span class="number">1</span>, cv=<span class="number">10</span>, scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>)</span><br><span class="line">selector = selector.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal number of features: <span class="subst">&#123;selector.n_features_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># You can plot selector.cv_results_[&#x27;mean_test_score&#x27;] to get the CV curve</span></span><br></pre></td></tr></table></figure>
<h2 id="shrinkage-methods-regularization">Shrinkage Methods
(Regularization)</h2>
<p>Instead of explicitly removing variables, shrinkage methods keep all
<span class="math inline">\(p\)</span> variables but <em>shrink</em>
their coefficients <span class="math inline">\(\beta_j\)</span> towards
zero.</p>
<h3 id="ridge-regression">Ridge Regression</h3>
<p>Ridge regression is a prime example of a shrinkage method.</p>
<ul>
<li><p><strong>Objective Function:</strong> It finds the coefficients
<span class="math inline">\(\boldsymbol{\beta}\)</span> that minimize a
new quantity: <span class="math display">\[\underbrace{\sum_{i=1}^{n}
(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2}_{\text{RSS (Goodness
of Fit)}} + \underbrace{\lambda \sum_{j=1}^{p}
\beta_j^2}_{\text{$\ell_2$ Penalty (Shrinkage)}}\]</span></p></li>
<li><p><strong>The <span class="math inline">\(\lambda\)</span> Tuning
Parameter:</strong> This parameter controls the strength of the
penalty:</p>
<ul>
<li><strong>If <span class="math inline">\(\lambda =
0\)</span>:</strong> The penalty term disappears. Ridge regression is
identical to standard Ordinary Least Squares (OLS).</li>
<li><strong>If <span class="math inline">\(\lambda \to
\infty\)</span>:</strong> The penalty is â€œinfinitelyâ€ strong. To
minimize the function, all coefficients <span
class="math inline">\(\beta_j\)</span> (for <span
class="math inline">\(j=1...p\)</span>) are forced to be zero. The model
becomes an intercept-only model.</li>
<li><strong>Note:</strong> The intercept <span
class="math inline">\(\beta_0\)</span> is <em>not penalized</em>.</li>
</ul></li>
<li><p><strong>The Bias-Variance Trade-off:</strong> This is the core
concept of regularization.</p>
<ul>
<li>Standard OLS has low bias but can have high variance (it
overfits).</li>
<li>Ridge regression adds a <em>small amount of bias</em> (the
coefficients are â€œwrongâ€ on purpose) to <strong>significantly reduce the
modelâ€™s variance</strong>.</li>
<li>This trade-off often leads to a model with a lower overall test
error.</li>
</ul></li>
<li><p><strong>Matrix Solution:</strong> The discussion slide asks â€œWhat
is the solution?â€. While OLS has the solution <span
class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>, the Ridge
solution is: <span class="math display">\[\hat{\boldsymbol{\beta}}^R =
(\mathbf{X}^T\mathbf{X} + \lambda
\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</span> where <span
class="math inline">\(\mathbf{I}\)</span> is the identity matrix. The
<span class="math inline">\(\lambda \mathbf{I}\)</span> term adds a
â€œridgeâ€ to the diagonal, making the matrix invertible even if <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is singular (which
happens if <span class="math inline">\(p &gt; n\)</span> or predictors
are collinear).</p></li>
</ul>
<h3 id="an-essential-step-standardization">An Essential Step:
Standardization</h3>
<ul>
<li><strong>Problem:</strong> The <span
class="math inline">\(\ell_2\)</span> penalty <span
class="math inline">\(\lambda \sum \beta_j^2\)</span> is applied equally
to all coefficients. If predictor <span
class="math inline">\(x_1\)</span> (e.g., house size in sq-ft) is on a
much larger scale than <span class="math inline">\(x_2\)</span> (e.g.,
number of rooms), its coefficient <span
class="math inline">\(\beta_1\)</span> will naturally be much smaller
than <span class="math inline">\(\beta_2\)</span>. The penalty will
unfairly punish <span class="math inline">\(\beta_2\)</span> more.</li>
<li><strong>Solution:</strong> You <strong>must standardize</strong>
your inputs <em>before</em> fitting a Ridge model.</li>
<li><strong>Formula:</strong> For each predictor <span
class="math inline">\(X_j\)</span>, all its observations <span
class="math inline">\(x_{ij}\)</span> are rescaled: <span
class="math display">\[\tilde{x}_{ij} = \frac{x_{ij} -
\bar{x}_j}{\sigma_j}\]</span> (where <span
class="math inline">\(\bar{x}_j\)</span> is the mean of the predictor
and <span class="math inline">\(\sigma_j\)</span> is its standard
deviation). This puts all predictors on a common scale (mean=0,
std=1).</li>
</ul>
<p><strong>In Python (with <code>scikit-learn</code>):</strong></p>
<ul>
<li>You use <code>sklearn.preprocessing.StandardScaler</code> to
standardize your data.</li>
<li>You use <code>sklearn.linear_model.Ridge</code> to fit the
model.</li>
<li>You use <code>sklearn.linear_model.RidgeCV</code> to automatically
find the best value for <span class="math inline">\(\lambda\)</span>
(called <code>alpha</code> in scikit-learn) using cross-validation.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Conceptual Python code for Ridge Regression</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># X, y = load_your_data()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a pipeline that first standardizes the data,</span></span><br><span class="line"><span class="comment"># then fits a Ridge model.</span></span><br><span class="line"><span class="comment"># RidgeCV tests a range of alphas (lambdas) automatically.</span></span><br><span class="line">model = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    RidgeCV(alphas=[<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">100.0</span>], scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best alpha (lambda): <span class="subst">&#123;model.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].alpha_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model coefficients: <span class="subst">&#123;model.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].coef_&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="subset-selection-2">Subset Selection</h2>
<p>This section is about choosing <em>which</em> predictors (variables)
to include in your linear model. The main idea is to find a â€œsparseâ€
model (one with few variables) that performs well.</p>
<h3 id="the-model-and-the-goal">The Model and The Goal</h3>
<ul>
<li><strong>Slide: â€œForward selection in Linear
Regressionâ€</strong></li>
<li><strong>Formula:</strong> The standard linear regression model is
<span class="math inline">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} +
\boldsymbol{\epsilon}\)</span>
<ul>
<li><span class="math inline">\(\mathbf{y}\)</span> is the <span
class="math inline">\(n \times 1\)</span> vector of outcomes.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is the <span
class="math inline">\(n \times (p+1)\)</span> matrix of predictors (with
a leading column of 1s for the intercept).</li>
<li><span class="math inline">\(\boldsymbol{\beta}\)</span> is the <span
class="math inline">\((p+1) \times 1\)</span> vector of coefficients
(<span class="math inline">\(\beta_0, \beta_1, ...,
\beta_p\)</span>).</li>
<li><span class="math inline">\(\boldsymbol{\epsilon}\)</span> is the
<span class="math inline">\(n \times 1\)</span> vector of irreducible
error.</li>
</ul></li>
<li><strong>Key Question:</strong> â€œIf <span
class="math inline">\(\boldsymbol{\beta}\)</span> is sparse with at most
<span class="math inline">\(s\)</span> non-zero entries, can forward
selection find those variables?â€
<ul>
<li><strong>Sparse</strong> means most coefficients are zero.</li>
<li><strong>Forward Selection</strong> is a <em>greedy algorithm</em>:
<ol type="1">
<li>Start with no variables.</li>
<li>Add the one variable that gives the best fit.</li>
<li>Add the <em>next</em> best variable to the existing model.</li>
<li>Repeat until you have a model with <span
class="math inline">\(s\)</span> variables.</li>
</ol></li>
<li>The slide suggests the answer is <strong>yes</strong>, but only
under certain conditions.</li>
</ul></li>
</ul>
<h3 id="the-condition-for-success">The Condition for Success</h3>
<ul>
<li><strong>Slide: â€œOrthogonal Matching Pursuitâ€</strong></li>
<li><strong>Key Concept:</strong> Forward selection can provably find
the correct variables if those variables are not strongly
correlated.</li>
<li><strong>Formula:</strong> This is formalized by the <strong>Mutual
Coherence Condition</strong>: <span class="math display">\[\mu = \max_{i
\neq j} |\langle \mathbf{x}_i, \mathbf{x}_j \rangle| &lt; \frac{1}{2s -
1}\]</span>
<ul>
<li><strong>What it means:</strong>
<ul>
<li><code>assuming $\mathbf&#123;x&#125;_i$'s are normalized</code> means weâ€™ve
scaled them to have a length of 1.</li>
<li><span class="math inline">\(\langle \mathbf{x}_i, \mathbf{x}_j
\rangle\)</span> is the dot product, which is just their
<strong>correlation</strong> since they are normalized.</li>
<li><span class="math inline">\(\mu\)</span> (mu) is the <strong>largest
absolute correlation</strong> you can find between any two
<em>different</em> predictors.</li>
<li><span class="math inline">\(s\)</span> is the true number of
important variables.</li>
</ul></li>
<li><strong>In English:</strong> If the maximum correlation between any
of your predictors is less than this threshold, the greedy forward
selection algorithm is guaranteed to find the true, sparse set of
variables.</li>
</ul></li>
</ul>
<h3 id="how-to-choose-the-model-size-practice">How to Choose the Model
Size (Practice)</h3>
<p>The theory is nice, but in practice, you donâ€™t know <span
class="math inline">\(s\)</span>. How many variables should you
pick?</p>
<ul>
<li><p><strong>Slide: â€œ10-fold CV Errorsâ€</strong></p></li>
<li><p><strong>This is the most important practical slide for this
section.</strong></p></li>
<li><p><strong>What the plot shows:</strong></p>
<ul>
<li><strong>X-axis:</strong> â€œNumber of Variablesâ€ (from 1 to 10).</li>
<li><strong>Y-axis:</strong> â€œCV Errorâ€ (the 10-fold cross-validated
Mean Squared Error).</li>
<li><strong>The Curve:</strong> The error drops very fast as we add the
first 2-3 variables. Then, it flattens out. Adding more than 3 variables
doesnâ€™t really help much.</li>
</ul></li>
<li><p><strong>Slide: â€œThe one standard deviation
ruleâ€</strong></p></li>
<li><p>This rule helps you pick the â€œbestâ€ model from the CV plot.</p>
<ol type="1">
<li>Find the model with the absolute <em>minimum</em> CV error (in the
plot, this looks to be around 6 or 7 variables).</li>
<li>Calculate the standard error of that minimum CV error.</li>
<li>Draw a â€œtoleranceâ€ line at
<code>(minimum error) + (one standard error)</code>.</li>
<li>Choose the <strong>simplest model</strong> (fewest variables) whose
CV error is <em>below</em> this tolerance line.</li>
</ol>
<!-- end list -->
<ul>
<li>The slide states this rule â€œgives the model with 3 variableâ€ for
this example. This is because the 3-variable model is much simpler than
the 6-variable one, and its error is â€œgood enoughâ€ (within one standard
deviation of the minimum). This is an application of <strong>Occamâ€™s
razor</strong>.</li>
</ul></li>
</ul>
<h3 id="code-r-vs.-python">Code: R vs.Â Python</h3>
<p>The R code on the â€œ10-fold CV Errorsâ€ slide generates that exact
plot.</p>
<ul>
<li><p><strong>R Code Explained:</strong></p>
<ul>
<li><code>library(boot)</code>: Loads the cross-validation library.</li>
<li><code>CV10.err=rep(0,10)</code>: Creates an empty vector to store
the 10 error scores.</li>
<li><code>for(p in 1:10)</code>: A loop that will test model sizes from
1 to 10.</li>
<li><code>x&lt;-which(summary(regfit.fwd)$which[p,])</code>: Gets the
<em>names</em> of the <span class="math inline">\(p\)</span> variables
chosen by a pre-run forward selection (<code>regfit.fwd</code>).</li>
<li><code>glm.fit=glm(Balance~.,data=newCred)</code>: Fits a model using
<em>only</em> those <span class="math inline">\(p\)</span>
variables.</li>
<li><code>cv.err=cv.glm(newCred,glm.fit,K=10)</code>: Performs 10-fold
CV on <em>that specific <span class="math inline">\(p\)</span>-variable
model</em>.</li>
<li><code>CV10.err[p]&lt;-cv.err$delta[1]</code>: Stores the CV
error.</li>
<li><code>plot(...)</code>: Plots the 10 errors against the 10 model
sizes.</li>
</ul></li>
<li><p><strong>Python Equivalent (Conceptual):</strong></p>
<ul>
<li>In <code>scikit-learn</code>, this process is often automated. You
wouldnâ€™t write the CV loop yourself.</li>
<li>You would use <code>sklearn.feature_selection.RFECV</code>
(Recursive Feature Elimination with Cross-Validation). This tool
automatically wraps a model (like <code>LinearRegression</code>),
performs cross-validation, and finds the optimal number of features,
effectively producing the same plot and result.</li>
</ul></li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- Python equivalent for 6.1 ---</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFECV</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="comment"># Assume X and y are your data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create a pipeline</span></span><br><span class="line"><span class="comment"># (Note: It&#x27;s good practice to scale, even for OLS, if you&#x27;re comparing)</span></span><br><span class="line">pipeline = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    LinearRegression()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create the RFECV (Recursive Feature Elimination w/ CV) object</span></span><br><span class="line"><span class="comment"># This is an *alternative* to forward selection, but serves the same purpose</span></span><br><span class="line"><span class="comment"># It will test models with 1, 2, 3... features using 10-fold CV</span></span><br><span class="line">feature_selector = RFECV(</span><br><span class="line">    estimator=pipeline, </span><br><span class="line">    min_features_to_select=<span class="number">1</span>, </span><br><span class="line">    step=<span class="number">1</span>, </span><br><span class="line">    cv=<span class="number">10</span>, </span><br><span class="line">    scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span> <span class="comment"># We want to minimize error</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit it</span></span><br><span class="line">feature_selector.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal number of features found: <span class="subst">&#123;feature_selector.n_features_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You could then plot feature_selector.cv_results_[&#x27;mean_test_score&#x27;]</span></span><br><span class="line"><span class="comment"># to replicate the R plot.</span></span><br></pre></td></tr></table></figure>
<h2 id="shrinkage-methods-by-regularization">Shrinkage Methods by
Regularization</h2>
<p>This is a different approach. Instead of <em>removing</em> variables,
we keep all <span class="math inline">\(p\)</span> variables but
<em>shrink</em> their coefficients <span
class="math inline">\(\beta_j\)</span> towards 0.</p>
<h3 id="ridge-regression-the-core-idea">Ridge Regression: The Core
Idea</h3>
<ul>
<li><strong>Slide: â€œRidge regressionâ€</strong></li>
<li><strong>Formula:</strong> Ridge regression minimizes a new objective
function: <span class="math display">\[\min_{\boldsymbol{\beta}} \left(
\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 +
\lambda \sum_{j=1}^{p} \beta_j^2 \right)\]</span>
<ul>
<li><strong>Term 1: <span class="math inline">\(\text{RSS}\)</span>
(Residual Sum of Squares).</strong> This is the original OLS â€œgoodness
of fitâ€ term. We want this to be small.</li>
<li><strong>Term 2: <span class="math inline">\(\lambda \sum
\beta_j^2\)</span>.</strong> This is the <strong><span
class="math inline">\(\ell_2\)</span> penalty</strong> or â€œshrinkage
penaltyâ€. It adds a â€œcostâ€ for having large coefficients.</li>
</ul></li>
<li><strong>The <span class="math inline">\(\lambda\)</span> (lambda)
Parameter:</strong>
<ul>
<li>This is the <strong>tuning parameter</strong> that controls the
trade-off between fit and simplicity.</li>
<li><code>$\lambda = 0$</code>: No penalty. The objective is just to
minimize RSS. The solution <span
class="math inline">\(\hat{\boldsymbol{\beta}}^R\)</span> is identical
to the OLS solution <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</li>
<li><code>$\lambda = \infty$</code>: Infinite penalty. The only way to
minimize the cost is to make all <span class="math inline">\(\beta_j =
0\)</span> (for <span class="math inline">\(j \ge 1\)</span>). The model
becomes an intercept-only model.</li>
<li><code>Large $\lambda$</code>: Heavy penalty, more shrinkage.</li>
<li><strong>Crucial Note:</strong> The intercept <span
class="math inline">\(\beta_0\)</span> is <strong>not
penalized</strong>. This is because <span
class="math inline">\(\beta_0\)</span> just represents the mean of <span
class="math inline">\(y\)</span> when all <span
class="math inline">\(x\)</span>â€™s are 0; shrinking it makes no
sense.</li>
</ul></li>
</ul>
<h3 id="the-need-for-standardization">The Need for Standardization</h3>
<ul>
<li><strong>Slide: â€œStandardize the inputsâ€</strong></li>
<li><strong>Problem:</strong> The penalty <span
class="math inline">\(\lambda \sum \beta_j^2\)</span> is applied to all
coefficients. But what if <span class="math inline">\(x_1\)</span> is
â€œhouse size in sq-ftâ€ (values 1000-5000) and <span
class="math inline">\(x_2\)</span> is â€œnumber of bedroomsâ€ (values 1-5)?
<ul>
<li>The coefficient <span class="math inline">\(\beta_1\)</span> for
house size will naturally be <em>tiny</em>, while the coefficient <span
class="math inline">\(\beta_2\)</span> for bedrooms will be
<em>large</em>, even if they are equally important.</li>
<li>Ridge regression would unfairly and heavily penalize <span
class="math inline">\(\beta_2\)</span> while barely touching <span
class="math inline">\(\beta_1\)</span>.</li>
</ul></li>
<li><strong>Solution:</strong> You <strong>must</strong> standardize all
predictors <em>before</em> fitting a Ridge model.</li>
<li><strong>Formula:</strong> For each observation <span
class="math inline">\(i\)</span> of each predictor <span
class="math inline">\(j\)</span>: <span
class="math display">\[\tilde{x}_{ij} = \frac{x_{ij} -
\bar{x}_j}{\sqrt{(1/n) \sum_{i=1}^{n} (x_{ij} - \bar{x}_j)^2}}\]</span>
<ul>
<li>This formula rescales every predictor to have a mean of 0 and a
standard deviation of 1.</li>
<li>Now, all coefficients <span class="math inline">\(\beta_j\)</span>
are on a â€œlevel playing fieldâ€ and can be penalized fairly.</li>
</ul></li>
</ul>
<h3 id="answering-the-discussion-questions">Answering the Discussion
Questions</h3>
<ul>
<li><strong>Slide: â€œDISCUSSIONâ€</strong>
<ul>
<li><code>What is the solution of Ridge regression?</code></li>
<li><code>What is the bias and the variance?</code></li>
</ul></li>
</ul>
<h4 id="what-is-the-solution-of-ridge-regression">1. What is the
solution of Ridge regression?</h4>
<p>The solution can be written in matrix form, which is very
elegant.</p>
<ul>
<li><p><strong>Standard OLS Solution:</strong> The coefficients <span
class="math inline">\(\hat{\boldsymbol{\beta}}^{\text{OLS}}\)</span>
that minimize RSS are found by: <span
class="math display">\[\hat{\boldsymbol{\beta}}^{\text{OLS}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p></li>
<li><p><strong>Ridge Regression Solution:</strong> The coefficients
<span class="math inline">\(\hat{\boldsymbol{\beta}}^{R}\)</span> that
minimize the Ridge objective are: <span
class="math display">\[\hat{\boldsymbol{\beta}}^{R} =
(\mathbf{X}^T\mathbf{X} + \lambda
\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><strong>Explanation:</strong>
<ul>
<li><span class="math inline">\(\mathbf{I}\)</span> is the
<strong>identity matrix</strong> (a matrix of 1s on the diagonal, 0s
everywhere else).</li>
<li>By adding <span class="math inline">\(\lambda\mathbf{I}\)</span>, we
are adding a positive value <span class="math inline">\(\lambda\)</span>
to the <em>diagonal</em> of the <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> matrix.</li>
<li>This addition <strong>stabilizes</strong> the matrix. <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> might not be
invertible (if <span class="math inline">\(p &gt; n\)</span> or if
predictors are perfectly collinear), but <span
class="math inline">\((\mathbf{X}^T\mathbf{X} + \lambda
\mathbf{I})\)</span> is <em>always</em> invertible for <span
class="math inline">\(\lambda &gt; 0\)</span>.</li>
<li>This addition is what mathematically â€œshrinksâ€ the coefficients
toward zero.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="what-is-the-bias-and-the-variance">2. What is the bias and the
variance?</h4>
<p>This is the <strong>most important concept</strong> in
regularization. Itâ€™s the <strong>bias-variance trade-off</strong>.</p>
<ul>
<li><p><strong>Standard OLS (where <span
class="math inline">\(\lambda=0\)</span>):</strong></p>
<ul>
<li><strong>Bias: Low.</strong> The OLS estimator is
<strong>unbiased</strong>, meaning that if you took many samples and fit
many OLS models, their average <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> would be the
<em>true</em> <span
class="math inline">\(\boldsymbol{\beta}\)</span>.</li>
<li><strong>Variance: High.</strong> The OLS solution can be
<em>highly</em> sensitive to the training data. If you change a few data
points, the coefficients can swing wildly. This is especially true if
<span class="math inline">\(p\)</span> is large or predictors are
correlated. This â€œsensitivityâ€ is high variance, which leads to
<strong>overfitting</strong>.</li>
</ul></li>
<li><p><strong>Ridge Regression (where <span
class="math inline">\(\lambda &gt; 0\)</span>):</strong></p>
<ul>
<li><strong>Bias: High(er).</strong> Ridge regression is a
<strong>biased</strong> estimator. By adding the penalty, we are
<em>purposefully</em> pulling the coefficients away from the OLS
solution and towards zero. The average <span
class="math inline">\(\hat{\boldsymbol{\beta}}^R\)</span> from many
samples will <em>not</em> equal the true <span
class="math inline">\(\boldsymbol{\beta}\)</span>. We have
<em>introduced</em> bias into our model.</li>
<li><strong>Variance: Low(er).</strong> In exchange for this bias, we
get a massive <em>reduction in variance</em>. The <span
class="math inline">\(\lambda\mathbf{I}\)</span> term stabilizes the
solution. The coefficients wonâ€™t change wildly even if the training data
changes. The model is more robust and less sensitive.</li>
</ul></li>
</ul>
<p><strong>The Trade-off:</strong> The total expected test error of a
model is: <span class="math inline">\(\text{Error} = \text{Bias}^2 +
\text{Variance} + \text{Irreducible Error}\)</span></p>
<p>By using Ridge regression, we <em>increase</em> the <span
class="math inline">\(\text{Bias}^2\)</span> term a little, but we
<em>decrease</em> the <span
class="math inline">\(\text{Variance}\)</span> term a lot. The goal is
to find a <span class="math inline">\(\lambda\)</span> where the
<em>total error</em> is minimized. Ridge regression reduces variance
<em>at the cost of</em> increased bias.</p>
<h3 id="python-equivalent-for-6.2">Python Equivalent for 6.2</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- Python equivalent for 6.2 ---</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="comment"># Assume X and y are your data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create a pipeline that AUTOMATICALLY</span></span><br><span class="line"><span class="comment">#    - Standardizes the data</span></span><br><span class="line"><span class="comment">#    - Fits a Ridge Regression model</span></span><br><span class="line"><span class="comment">#    - Uses Cross-Validation to find the BEST lambda (alpha in scikit-learn)</span></span><br><span class="line">alphas_to_test = [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">100.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># RidgeCV handles everything for us</span></span><br><span class="line">pipeline = make_pipeline(</span><br><span class="line">    StandardScaler(),</span><br><span class="line">    RidgeCV(alphas=alphas_to_test, scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>, cv=<span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit the pipeline</span></span><br><span class="line">pipeline.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Get the results</span></span><br><span class="line">best_lambda = pipeline.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].alpha_</span><br><span class="line">ridge_coefficients = pipeline.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].coef_</span><br><span class="line">intercept = pipeline.named_steps[<span class="string">&#x27;ridgecv&#x27;</span>].intercept_</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha) found by CV: <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model intercept (beta_0): <span class="subst">&#123;intercept&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Model coefficients (beta_j): <span class="subst">&#123;ridge_coefficients&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="the-why-of-ridge-regression">6. The â€œWhyâ€ of Ridge
Regression</h1>
<h2 id="core-concepts-the-why-of-ridge-regression">Core Concepts: The
â€œWhyâ€ of Ridge Regression</h2>
<p>Your slides explain that ridge regression is a â€œshrinkage methodâ€
designed to solve a major problem with standard Ordinary Least Squares
(OLS) regression: <strong>high variance</strong>.</p>
<h3 id="the-bias-variance-tradeoff-slide-3">The Bias-Variance Tradeoff
(Slide 3)</h3>
<p>This is the most important theoretical concept. In prediction, the
total error (Mean Squared Error, or MSE) of a model is composed of three
parts: <span class="math inline">\(\text{Error} = \text{Variance} +
\text{Bias}^2 + \text{Irreducible Error}\)</span></p>
<ul>
<li><strong>Ordinary Least Squares (OLS):</strong> Aims to be unbiased
(low bias). However, when you have many predictors (<span
class="math inline">\(p\)</span>), especially if they are correlated, or
if <span class="math inline">\(p\)</span> is large compared to the
number of samples <span class="math inline">\(n\)</span> (<span
class="math inline">\(p \approx n\)</span> or <span
class="math inline">\(p &gt; n\)</span>), the OLS model becomes highly
<em>unstable</em>. A small change in the training data can cause the
coefficients to change wildly. This is <strong>high variance</strong>.
(See Slide 6, â€œRemarksâ€).</li>
<li><strong>Ridge Regression:</strong> By adding a penalty, ridge
<em>intentionally</em> introduces a small amount of
<strong>bias</strong> (it pulls coefficients away from their â€œtrueâ€ OLS
values). In return, it achieves a <em>massive</em> reduction in
<strong>variance</strong>.</li>
</ul>
<p>As <strong>Slide 3</strong> shows:</p>
<ul>
<li>The <strong>green line (Variance)</strong> starts very high for low
<span class="math inline">\(\lambda\)</span> (left side) and drops
quickly.</li>
<li>The <strong>black line (Squared Bias)</strong> starts at zero (for
OLS at <span class="math inline">\(\lambda=0\)</span>) and slowly
increases as <span class="math inline">\(\lambda\)</span> grows.</li>
<li>The <strong>purple line (Test MSE)</strong> is the sum of the two.
Itâ€™s U-shaped. The goal of ridge is to find the <span
class="math inline">\(\lambda\)</span> (marked by the â€˜xâ€™) at the
<em>bottom</em> of this â€œU,â€ which gives the lowest possible total
error.</li>
</ul>
<h3 id="why-is-it-called-ridge-the-3d-spatial-meaning-slide-5">Why Is It
Called â€œRidgeâ€? The 3D Spatial Meaning (Slide 5)</h3>
<p>This slide explains the problem of <strong>collinearity</strong> and
the origin of the name.</p>
<ul>
<li><strong>Left Plot (Least Squares):</strong> Imagine a model with two
correlated predictors, <span class="math inline">\(\beta_1\)</span> and
<span class="math inline">\(\beta_2\)</span>. The y-axis (SS1) is the
error (RSS). Because the predictors are correlated, there isnâ€™t one
single â€œpointâ€ that is the minimum. Instead, thereâ€™s a long, flat
<em>valley</em> or <em>trough</em> (marked â€œunstableâ€). Many different
combinations of <span class="math inline">\(\beta_1\)</span> and <span
class="math inline">\(\beta_2\)</span> along this valley give a
similarly low error. The OLS solution is unstable because it can pick
<em>any</em> point in this flat-bottomed valley.</li>
<li><strong>Right Plot (Ridge):</strong> The ridge objective function
adds a penalty term: <span class="math inline">\(\lambda(\beta_1^2 +
\beta_2^2)\)</span>. This penalty term, by itself, is a perfect circular
bowl centered at (0,0). When you add this â€œbowlâ€ to the OLS â€œvalley,â€ it
<em>stabilizes</em> the function. It pulls the minimum towards (0,0) and
creates a single, stable, well-defined minimum.</li>
<li><strong>The â€œRidgeâ€ Name:</strong> The penalty <span
class="math inline">\(\lambda\mathbf{I}\)</span> (from the matrix
formula) adds a â€œridgeâ€ of values to the diagonal of the <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> matrix, which
geometrically turns the unstable flat valley into a stable bowl.</li>
</ul>
<h2 id="mathematical-formulas">Mathematical Formulas</h2>
<p>The key difference between OLS and Ridge is the function they try to
minimize.</p>
<ol type="1">
<li><p><strong>OLS Objective Function:</strong> Minimize the Residual
Sum of Squares (RSS). <span class="math display">\[\text{RSS} =
\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}
\right)^2\]</span></p></li>
<li><p><strong>Ridge Objective Function (Slide 6):</strong> Minimize the
RSS <em>plus</em> an L2 penalty term. <span
class="math display">\[\text{Minimize: } \left[ \sum_{i=1}^{n} \left(
y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 \right] +
\lambda \sum_{j=1}^{p} \beta_j^2\]</span></p>
<ul>
<li><span class="math inline">\(\lambda\)</span> is the <strong>tuning
parameter</strong> controlling the penalty strength.</li>
<li><span class="math inline">\(\sum_{j=1}^{p} \beta_j^2\)</span> is the
<strong>L2-norm</strong> (squared) of the coefficients. It penalizes
large coefficients.</li>
</ul></li>
<li><p><strong>L2 Norm (Slide 1):</strong> The L2 norm of a vector <span
class="math inline">\(\mathbf{a}\)</span> is its standard Euclidean
length. The plot on Slide 1 uses this to show the <em>total
magnitude</em> of the ridge coefficients. <span
class="math display">\[\|\mathbf{a}\|_2 = \sqrt{\sum_{j=1}^p
a_j^2}\]</span></p></li>
<li><p><strong>Matrix Solution (Slide 6):</strong> This is the
â€œclosed-formâ€ solution for the ridge coefficients <span
class="math inline">\(\hat{\beta}^R\)</span>. <span
class="math display">\[\hat{\beta}^R = (\mathbf{X}^T\mathbf{X} +
\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{I}\)</span> is the identity
matrix.</li>
<li>The term <span class="math inline">\(\lambda\mathbf{I}\)</span> is
what stabilizes the <span
class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> matrix, making it
invertible even if itâ€™s singular (due to <span class="math inline">\(p
&gt; n\)</span> or collinearity).</li>
</ul></li>
</ol>
<h2 id="walkthrough-of-the-credit-data-example-all-slides">Walkthrough
of the â€œCredit Dataâ€ Example (All Slides)</h2>
<p>Here is the logical story of the R code, from start to finish.</p>
<h3 id="step-1-data-preparation-slide-8">Step 1: Data Preparation (Slide
8)</h3>
<ul>
<li><code>x=scale(model.matrix(Balance~., Credit)[,-1])</code>
<ul>
<li><code>model.matrix(...)</code> creates the predictor matrix
<code>x</code>.</li>
<li><code>scale(...)</code> is <strong>critically important</strong>. It
standardizes all predictors to have a mean of 0 and a standard deviation
of 1. This is necessary because the ridge penalty <span
class="math inline">\(\lambda \sum \beta_j^2\)</span> is
<em>unit-dependent</em>. If <code>Income</code> (in 10,000s) and
<code>Cards</code> (1-10) were unscaled, the penalty would unfairly
crush the <code>Income</code> coefficient. Scaling puts all predictors
on a level playing field.</li>
</ul></li>
<li><code>y=Credit$Balance</code>
<ul>
<li>This sets the <code>y</code> (target) variable.</li>
</ul></li>
</ul>
<h3 id="step-2-fit-the-ridge-model-slide-8">Step 2: Fit the Ridge Model
(Slide 8)</h3>
<ul>
<li><code>grid=10^seq(4,-2,length=100)</code>
<ul>
<li>This creates a <em>grid</em> of 100 <span
class="math inline">\(\lambda\)</span> values to test, ranging from
<span class="math inline">\(10^4\)</span> (a huge penalty) down to <span
class="math inline">\(10^{-2}\)</span> (a tiny penalty).</li>
</ul></li>
<li><code>ridge.mod=glmnet(x,y,alpha=0,lambda=grid)</code>
<ul>
<li>This is the main command. It fits a <em>separate</em> ridge model
for <em>every single <span class="math inline">\(\lambda\)</span></em>
in the <code>grid</code>.</li>
<li><code>alpha=0</code> is the specific command that tells
<code>glmnet</code> to perform <strong>Ridge Regression</strong>.
(Setting <code>alpha=1</code> would be LASSO).</li>
</ul></li>
<li><code>coef(ridge.mod)[,50]</code>
<ul>
<li>This inspects the model. It pulls out the vector of coefficients for
the 50th <span class="math inline">\(\lambda\)</span> in the grid (which
is <span class="math inline">\(\lambda=10.72\)</span>).</li>
</ul></li>
</ul>
<h3
id="step-3-visualize-the-coefficient-solution-path-slides-1-4-9">Step 3:
Visualize the Coefficient â€œSolution Pathâ€ (Slides 1, 4, 9)</h3>
<p>These plots all show the same thing: how the coefficients change as
<span class="math inline">\(\lambda\)</span> changes.</p>
<ul>
<li><strong>Slide 9 Plot:</strong> This plots the standardized
coefficients for 4 predictors (<code>Income</code>, <code>Limit</code>,
<code>Rating</code>, <code>Student</code>) against the <em>index</em> (1
to 100). Index 1 (left) is the largest <span
class="math inline">\(\lambda\)</span>, and index 100 (right) is the
smallest <span class="math inline">\(\lambda\)</span> (closest to OLS).
You can see the coefficients â€œgrowâ€ from 0 as the penalty (<span
class="math inline">\(\lambda\)</span>) gets smaller.</li>
<li><strong>Slide 1 (Left Plot):</strong> This is the <em>same plot</em>
as Slide 9, but more professional. It plots the coefficients against
<span class="math inline">\(\lambda\)</span> on a log scale. You can
clearly see all coefficients (gray lines) being â€œshrunkâ€ toward zero as
<span class="math inline">\(\lambda\)</span> increases (moves right).
The key predictors (<code>Income</code>, <code>Rating</code>, etc.) are
highlighted.</li>
<li><strong>Slide 1 (Right Plot):</strong> This is the <em>exact same
data</em> again, but with a different x-axis: <span
class="math inline">\(\|\hat{\beta}_\lambda^R\|_2 /
\|\hat{\beta}\|_2\)</span>.
<ul>
<li><strong>1.0</strong> on the right means <span
class="math inline">\(\lambda=0\)</span>. The ratio of the ridge norm to
the OLS norm is 1 (they are the same).</li>
<li><strong>0.0</strong> on the left means <span
class="math inline">\(\lambda=\infty\)</span>. The ridge coefficients
are all 0, so their norm is 0.</li>
<li>This axis shows the â€œfractionâ€ of the full OLS coefficient magnitude
that the model is using.</li>
</ul></li>
<li><strong>Slide 4 Plot:</strong> This plots the <em>total L2 norm</em>
of <em>all</em> coefficients (<span
class="math inline">\(\|\hat{\beta}_\lambda^R\|_2\)</span>) against the
index. As the index goes from 1 to 100 (i.e., <span
class="math inline">\(\lambda\)</span> gets smaller), the total
magnitude of the coefficients gets larger, which is exactly what we
expect.</li>
</ul>
<h3
id="step-4-find-the-best-lambda-using-cross-validation-slides-4-7">Step
4: Find the <em>Best</em> <span class="math inline">\(\lambda\)</span>
using Cross-Validation (Slides 4 &amp; 7)</h3>
<p>We have 100 models. Which one is best?</p>
<ul>
<li><p><strong>The â€œManualâ€ Way (Slide 4):</strong></p>
<ul>
<li>The code splits the data into a <code>train</code> and
<code>test</code> set.</li>
<li>It fits a model <em>only</em> on the <code>train</code> set.</li>
<li>It tests two <span class="math inline">\(\lambda\)</span> values:
<ul>
<li><code>s=4</code>: Gives a test MSE of <code>10293.33</code>.</li>
<li><code>s=10</code>: Gives a test MSE of <code>168981.1</code> (much
worse!).</li>
</ul></li>
<li>This shows that <span class="math inline">\(\lambda=4\)</span> is
better than <span class="math inline">\(\lambda=10\)</span>, but we
donâ€™t know if itâ€™s the <em>best</em>.</li>
</ul></li>
<li><p><strong>The â€œAutomaticâ€ Way (Slide 7):</strong></p>
<ul>
<li><code>cv.out=cv.glmnet(x[train,], y[train], alpha=0)</code></li>
<li>This runs <strong>10-fold Cross-Validation</strong> on the training
set. It automatically splits the training set into 10 â€œfolds,â€ trains on
9, tests on 1, and repeats this 10 times for <em>every <span
class="math inline">\(\lambda\)</span></em>.</li>
<li><strong>The Plot:</strong> The plot on this slide is the result. It
shows the average MSE (y-axis) for each <span
class="math inline">\(\log(\lambda)\)</span> (x-axis). This is the
<em>real-data version</em> of the theoretical purple curve from Slide
3.</li>
<li><code>bestlam=cv.out$lambda.min</code></li>
<li>This command finds the <span class="math inline">\(\lambda\)</span>
at the <em>very bottom</em> of the U-shaped curve. The output shows
<code>bestlam</code> is <strong>41.6</strong>.</li>
<li><code>ridge.pred=predict(ridge.mod, s=bestlam, newx=x[test,])</code></li>
<li>Now, we use this <em>one best <span
class="math inline">\(\lambda\)</span></em> to make predictions on our
held-out <code>test</code> set.</li>
<li><code>mean((ridge.pred-y.test)^2)</code></li>
<li>The final, reliable test MSE is <strong>16129.68</strong>. This is
our best estimate of how the model will perform on new, unseen
data.</li>
</ul></li>
</ul>
<h2 id="python-scikit-learn-equivalents">Python
(<code>scikit-learn</code>) Equivalents</h2>
<p>Here is how you would perform the entire R workflow from your slides
in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. Load and Prepare Data (like Slide 8) ---</span></span><br><span class="line"><span class="comment"># Assuming &#x27;Credit&#x27; is a pandas DataFrame</span></span><br><span class="line"><span class="comment"># X = Credit.drop(&#x27;Balance&#x27;, axis=1)</span></span><br><span class="line"><span class="comment"># y = Credit[&#x27;Balance&#x27;]</span></span><br><span class="line"><span class="comment"># ... (need to handle categorical variables first, e.g., with pd.get_dummies) ...</span></span><br><span class="line"><span class="comment"># For this example, let&#x27;s assume X and y are already loaded and numeric.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize the predictors (CRITICAL)</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. Train/Test Split (like Slide 4) ---</span></span><br><span class="line"><span class="comment"># test_size=0.5 and random_state=1 mimic the R code</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X_scaled, y, test_size=<span class="number">0.5</span>, random_state=<span class="number">1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 3. Find Best Lambda (alpha) with Cross-Validation (like Slide 7) ---</span></span><br><span class="line"><span class="comment"># Create the same log-spaced grid of lambdas (sklearn calls it &#x27;alpha&#x27;)</span></span><br><span class="line">lambda_grid = np.logspace(<span class="number">4</span>, -<span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># RidgeCV performs cross-validation to find the best alpha</span></span><br><span class="line"><span class="comment"># cv=10 matches the 10-fold CV</span></span><br><span class="line"><span class="comment"># store_cv_values=True is needed to plot the CV error curve</span></span><br><span class="line">cv_model = RidgeCV(alphas=lambda_grid, store_cv_values=<span class="literal">True</span>, cv=<span class="number">10</span>)</span><br><span class="line">cv_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best lambda found</span></span><br><span class="line">best_lambda = cv_model.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha) found by CV: <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the CV error curve (like Slide 7 plot)</span></span><br><span class="line"><span class="comment"># cv_model.cv_values_ has shape (n_samples, n_alphas)</span></span><br><span class="line"><span class="comment"># We need to average over the samples for each alpha</span></span><br><span class="line">mse_path = np.mean(cv_model.cv_values_, axis=<span class="number">0</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(np.log10(cv_model.alphas_), mse_path, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Log(lambda)&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Mean Squared Error&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Cross-Validation Error Path&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 4. Evaluate on Test Set (like Slide 7) ---</span></span><br><span class="line"><span class="comment"># &#x27;cv_model&#x27; is already refit on the full training set using the best_lambda</span></span><br><span class="line">test_pred = cv_model.predict(X_test)</span><br><span class="line">final_test_mse = mean_squared_error(y_test, test_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Final Test MSE with best lambda: <span class="subst">&#123;final_test_mse&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 5. Get Final Coefficients (like Slide 7, bottom) ---</span></span><br><span class="line"><span class="comment"># The coefficients from the CV-trained model:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Intercept: <span class="subst">&#123;cv_model.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Coefficients:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> coef, feature <span class="keyword">in</span> <span class="built_in">zip</span>(cv_model.coef_, X.columns):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;  <span class="subst">&#123;feature&#125;</span>: <span class="subst">&#123;coef&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 6. Plot the Solution Path (like Slide 1) ---</span></span><br><span class="line"><span class="comment"># To do this, we fit a Ridge model for each lambda and store the coefficients</span></span><br><span class="line">coefs = []</span><br><span class="line"><span class="keyword">for</span> lam <span class="keyword">in</span> lambda_grid:</span><br><span class="line">    model = Ridge(alpha=lam)</span><br><span class="line">    model.fit(X_scaled, y)  <span class="comment"># Fit on all data</span></span><br><span class="line">    coefs.append(model.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(np.log10(lambda_grid), coefs)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Log(lambda)&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Standardized Coefficients&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Ridge Solution Path&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="shrinkage-methods-regularization-1">7. Shrinkage Methods
(Regularization)</h1>
<p>These slides cover <strong>Shrinkage Methods</strong>, also known as
<strong>Regularization</strong>, which are techniques used to improve on
the standard least squares model, particularly when dealing with many
variables or multicollinearity. The main focus is on
<strong>LASSO</strong> regression.</p>
<h2 id="key-mathematical-formulas">Key Mathematical Formulas</h2>
<p>The slides present two main, but equivalent, ways to formulate these
methods.</p>
<h3 id="penalized-formulation-slide-1">1. Penalized Formulation (Slide
1)</h3>
<p>This is the most common formulation. The goal is to minimize a
function that is a combination of the <strong>Residual Sum of Squares
(RSS)</strong> and a <strong>penalty term</strong>. The penalty
discourages large coefficients.</p>
<ul>
<li><strong>LASSO (Least Absolute Shrinkage and Selection
Operator):</strong> The goal is to find coefficients (<span
class="math inline">\(\beta_0, \beta_j\)</span>) that minimize: <span
class="math display">\[\sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p}
\beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j|\]</span>
<ul>
<li><strong>Penalty:</strong> The <span
class="math inline">\(L_1\)</span> norm (<span
class="math inline">\(\|\beta\|_1\)</span>), which is the sum of the
<em>absolute values</em> of the coefficients.</li>
<li><strong>Key Property:</strong> This penalty can force some
coefficients to be <strong>exactly zero</strong>, effectively performing
automatic variable selection.</li>
</ul></li>
</ul>
<h3 id="constrained-formulation-slide-2">2. Constrained Formulation
(Slide 2)</h3>
<p>This alternative formulation minimizes the RSS <em>subject to a
constraint</em> (a â€œbudgetâ€) on the size of the coefficients.</p>
<ul>
<li><p><strong>For Lasso:</strong> Minimize RSS subject to: <span
class="math display">\[\sum_{j=1}^{p} |\beta_j| \le s\]</span> (The sum
of the absolute values of the coefficients must be less than some budget
<span class="math inline">\(s\)</span>.)</p></li>
<li><p><strong>For Ridge:</strong> Minimize RSS subject to: <span
class="math display">\[\sum_{j=1}^{p} \beta_j^2 \le s\]</span> (The sum
of the <em>squares</em> of the coefficients (<span
class="math inline">\(L_2\)</span> norm) must be less than <span
class="math inline">\(s\)</span>.)</p></li>
</ul>
<p><strong>Equivalence (Slide 3):</strong> For any penalty value <span
class="math inline">\(\lambda\)</span> used in the first formulation,
there is a corresponding budget <span class="math inline">\(s\)</span>
in the second formulation that will give the exact same set of
coefficients. <span class="math inline">\(\lambda\)</span> and <span
class="math inline">\(s\)</span> are inversely related: a large <span
class="math inline">\(\lambda\)</span> (high penalty) corresponds to a
small <span class="math inline">\(s\)</span> (small budget).</p>
<h2 id="important-plots-and-interpretation">Important Plots and
Interpretation</h2>
<p>Your slides show the two most important plots for understanding and
using LASSO.</p>
<h3 id="the-cross-validation-cv-plot-slide-5">1. The Cross-Validation
(CV) Plot (Slide 5)</h3>
<p>This plot is crucial for <strong>choosing the best tuning parameter
(<span class="math inline">\(\lambda\)</span>)</strong>.</p>
<ul>
<li><strong>X-axis:</strong> <span
class="math inline">\(\text{Log}(\lambda)\)</span>. This is the penalty
strength.
<ul>
<li><strong>Right side (high <span
class="math inline">\(\lambda\)</span>):</strong> High penalty, simple
model (many coefficients are 0), high bias, high Mean-Squared Error
(MSE).</li>
<li><strong>Left side (low <span
class="math inline">\(\lambda\)</span>):</strong> Low penalty, complex
model (like standard linear regression), high variance, MSE starts to
increase (overfitting).</li>
</ul></li>
<li><strong>Y-axis:</strong> Mean-Squared Error (MSE) from
cross-validation.</li>
<li><strong>Goal:</strong> Find the <span
class="math inline">\(\lambda\)</span> at the <strong>bottom of the â€œUâ€
shape</strong>, which gives the <em>lowest</em> MSE. This is the optimal
trade-off between bias and variance. The top axis shows how many
variables are included in the model at each <span
class="math inline">\(\lambda\)</span>.</li>
</ul>
<h3 id="the-coefficient-path-plot-slide-6">2. The Coefficient Path Plot
(Slide 6)</h3>
<p>This plot is the best visualization for <strong>understanding what
LASSO does</strong>.</p>
<ul>
<li><strong>Left Plot (vs.Â <span
class="math inline">\(\lambda\)</span>):</strong>
<ul>
<li><strong>X-axis:</strong> The penalty strength <span
class="math inline">\(\lambda\)</span>.</li>
<li><strong>Y-axis:</strong> The standardized value of each
coefficient.</li>
<li><strong>How to read it:</strong> Start from the
<strong>right</strong> (high <span
class="math inline">\(\lambda\)</span>). All coefficients are 0. As you
move <strong>left</strong>, <span class="math inline">\(\lambda\)</span>
<em>decreases</em>, and the penalty is relaxed. Variables â€œenterâ€ the
model one by one (their coefficients become non-zero). You can see that
â€˜Ratingâ€™, â€˜Incomeâ€™, and â€˜Studentâ€™ are the most important variables, as
they are the first to become non-zero.</li>
</ul></li>
<li><strong>Right Plot (vs.Â <span class="math inline">\(L_1\)</span>
Norm Ratio):</strong>
<ul>
<li>This shows the exact same information as the left plot, but the
x-axis is reversed and rescaled. An axis value of 0.0 means full penalty
(all <span class="math inline">\(\beta=0\)</span>), and 1.0 means no
penalty.</li>
</ul></li>
</ul>
<h2 id="code-understanding-r-to-python">Code Understanding (R to
Python)</h2>
<p>The slides use the <code>glmnet</code> package in R. The equivalent
and most popular library in Python is <strong>scikit-learn</strong>.</p>
<h3 id="finding-the-best-lambda-cv">1. Finding the Best <span
class="math inline">\(\lambda\)</span> (CV)</h3>
<p>The R code <code>cv.out=cv.glmnet(x[train,],y[train],alpha=1)</code>
performs cross-validation to find the best <span
class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><strong>Python Equivalent:</strong> Use <code>LassoCV</code>. It
does the same thing: tests many <span
class="math inline">\(\lambda\)</span> values (called
<code>alphas</code> in scikit-learn) and picks the best one.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the LassoCV object</span></span><br><span class="line"><span class="comment"># cv=5 means 5-fold cross-validation</span></span><br><span class="line">lasso_cv = LassoCV(cv=<span class="number">5</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model to the training data</span></span><br><span class="line">lasso_cv.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best lambda (called alpha_ in sklearn)</span></span><br><span class="line">best_lambda = lasso_cv.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha): <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the MSEs</span></span><br><span class="line"><span class="comment"># This is what&#x27;s plotted in the CV plot</span></span><br><span class="line"><span class="built_in">print</span>(lasso_cv.mse_path_)</span><br></pre></td></tr></table></figure>
<h3 id="fitting-with-the-best-lambda-and-getting-coefficients">2.
Fitting with the Best <span class="math inline">\(\lambda\)</span> and
Getting Coefficients</h3>
<p>The R code
<code>lasso.coef=predict(out,type="coefficients",s=bestlam)</code> gets
the coefficients for the best <span
class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><strong>Python Equivalent:</strong> The <code>LassoCV</code> object
is <em>already</em> refitted on the full training data using the best
<span class="math inline">\(\lambda\)</span>. You can also fit a new
<code>Lasso</code> model with that specific <span
class="math inline">\(\lambda\)</span>.</li>
</ul>
<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Option 1: Use the already-fitted LassoCV object ---</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Coefficients from LassoCV:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(lasso_cv.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on the test set</span></span><br><span class="line">y_pred = lasso_cv.predict(X_test)</span><br><span class="line">test_mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Test MSE: <span class="subst">&#123;test_mse&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Option 2: Fit a new Lasso model with the best lambda ---</span></span><br><span class="line">final_lasso = Lasso(alpha=best_lambda)</span><br><span class="line">final_lasso.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get coefficients (Slide 7 shows this)</span></span><br><span class="line"><span class="comment"># Note how some are 0!</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nCoefficients from new Lasso model:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(final_lasso.coef_)</span><br></pre></td></tr></table></figure>
<h2 id="the-core-problem-two-equivalent-formulas">The Core Problem: Two
Equivalent Formulas</h2>
<p>The slides show two ways of writing the <em>same problem</em>.
Understanding this equivalence is key.</p>
<h3 id="formulation-1-the-penalized-method-slides-1-4">Formulation 1:
The Penalized Method (Slides 1 &amp; 4)</h3>
<ul>
<li><p><strong>Formula:</strong> <span
class="math display">\[\min_{\beta} \left( \sum_{i=1}^{n} (y_i -
\mathbf{x}_i^T \beta)^2 + \lambda \|\beta\|_1 \right)\]</span></p>
<ul>
<li><strong><span class="math inline">\(\sum (y_i - \mathbf{x}_i^T
\beta)^2\)</span></strong>: This is the normal <strong>Residual Sum of
Squares (RSS)</strong>. We want to make this small (fit the data
well).</li>
<li><strong><span class="math inline">\(\lambda
\|\beta\|_1\)</span></strong>: This is the <strong><span
class="math inline">\(L_1\)</span> penalty</strong>.
<ul>
<li><span class="math inline">\(\|\beta\|_1 = \sum_{j=1}^{p}
|\beta_j|\)</span> is the sum of the absolute values of the
coefficients.</li>
<li><span class="math inline">\(\lambda\)</span> (lambda) is a tuning
parameter. Think of it as a <strong>â€œpenalty knobâ€</strong>.</li>
</ul></li>
</ul></li>
<li><p><strong>How to think about <span
class="math inline">\(\lambda\)</span></strong>:</p>
<ul>
<li><strong>If <span class="math inline">\(\lambda =
0\)</span>:</strong> There is no penalty. This is just standard Ordinary
Least Squares (OLS) regression. The model will likely overfit.</li>
<li><strong>If <span class="math inline">\(\lambda\)</span> is
<em>small</em>:</strong> Thereâ€™s a small penalty. Coefficients will
shrink a <em>little</em> bit.</li>
<li><strong>If <span class="math inline">\(\lambda\)</span> is <em>very
large</em>:</strong> The penalty is severe. The <em>only</em> way to
make the penalty term small is to make the coefficients (<span
class="math inline">\(\beta\)</span>) themselves small. The model will
eventually shrink all coefficients to <strong>exactly 0</strong>.</li>
</ul></li>
</ul>
<h3 id="formulation-2-the-constrained-method-slides-2-3">Formulation 2:
The Constrained Method (Slides 2 &amp; 3)</h3>
<ul>
<li><p><strong>Formula:</strong> <span
class="math display">\[\min_{\beta} \sum_{i=1}^{n} (y_i - \mathbf{x}_i^T
\beta)^2 \quad \text{subject to} \quad \|\beta\|_1 \le
s\]</span></p></li>
<li><p><strong>How to think about <span
class="math inline">\(s\)</span></strong>:</p>
<ul>
<li>This says: â€œFind the best-fitting model (minimize RSS) <em>but</em>
you have a limited <strong>â€˜budgetâ€™ <span
class="math inline">\(s\)</span></strong> for the total size of your
coefficients.â€</li>
<li><strong>If <span class="math inline">\(s\)</span> is <em>very
large</em>:</strong> The budget is huge. This constraint does nothing.
You get the standard OLS solution.</li>
<li><strong>If <span class="math inline">\(s\)</span> is
<em>small</em>:</strong> The budget is tight. You <em>must</em> shrink
your coefficients to stay under the budget <span
class="math inline">\(s\)</span>. To get the best fit, the model will be
forced to set unimportant coefficients to 0 and only â€œspendâ€ its budget
on the most important variables.</li>
</ul></li>
</ul>
<p><strong>The Equivalence:</strong> These two forms are equivalent. For
any <span class="math inline">\(\lambda\)</span> you pick, thereâ€™s a
corresponding budget <span class="math inline">\(s\)</span> that gives
the <em>exact same solution</em>.</p>
<ul>
<li>High <span class="math inline">\(\lambda\)</span> (strong penalty)
<span class="math inline">\(\iff\)</span> Small <span
class="math inline">\(s\)</span> (tight budget)</li>
<li>Low <span class="math inline">\(\lambda\)</span> (weak penalty)
<span class="math inline">\(\iff\)</span> Large <span
class="math inline">\(s\)</span> (loose budget)</li>
</ul>
<p>This equivalence is why you see plots with both <span
class="math inline">\(\lambda\)</span> and <span
class="math inline">\(L_1\)</span> Norm on the x-axis. They are just two
different ways of looking at the same â€œpenaltyâ€ spectrum.</p>
<h2 id="detailed-plot-code-analysis">Detailed Plot &amp; Code
Analysis</h2>
<p>Letâ€™s look at the plots and code, which answer the practical
questions: <strong>(1)</strong> How do we pick the <em>best</em> <span
class="math inline">\(\lambda\)</span>? and <strong>(2)</strong> What
does LASSO <em>do</em> to the coefficients?</p>
<h3 id="question-1-how-to-pick-the-best-lambda-slide-5">Question 1: How
to pick the best <span class="math inline">\(\lambda\)</span>? (Slide
5)</h3>
<p>This is the <strong>Cross-Validation (CV) Plot</strong>. Its one and
only job is to help you find the optimal <span
class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><strong>R Code:</strong>
<code>cv.out=cv.glmnet(x[train,],y[train],alpha=1)</code>
<ul>
<li><code>cv.glmnet</code>: This R function <em>automatically</em> does
K-fold cross-validation. <code>alpha=1</code> explicitly tells it to use
<strong>LASSO</strong> (alpha=0 would be Ridge).</li>
<li>It tries a whole range of <span
class="math inline">\(\lambda\)</span> values, calculates the
Mean-Squared Error (MSE) for each, and stores the results in
<code>cv.out</code>.</li>
</ul></li>
<li><strong>Plot Analysis:</strong>
<ul>
<li><strong>X-axis:</strong> <span
class="math inline">\(\text{Log}(\lambda)\)</span>. The penalty
strength. <strong>Right = High Penalty</strong> (simple model),
<strong>Left = Low Penalty</strong> (complex model).</li>
<li><strong>Y-axis:</strong> Mean-Squared Error (MSE). <strong>Lower is
better.</strong></li>
<li><strong>Red Dots:</strong> The average MSE for each <span
class="math inline">\(\lambda\)</span>.</li>
<li><strong>Gray Bars:</strong> The error bars (standard error).</li>
<li><strong>The â€œUâ€ Shape:</strong> This is the classic
<strong>bias-variance trade-off</strong>.
<ul>
<li><strong>Right Side (High <span
class="math inline">\(\lambda\)</span>):</strong> The model is <em>too
simple</em> (too many coefficients are 0). Itâ€™s â€œunderfitting.â€ The
error is high (high bias).</li>
<li><strong>Left Side (High <span
class="math inline">\(\lambda\)</span>):</strong> The model is <em>too
complex</em> (low penalty, like OLS). Itâ€™s â€œoverfittingâ€ the training
data. The error on new data is high (high variance).</li>
<li><strong>Bottom of the â€œUâ€:</strong> This is the â€œsweet spot.â€ The
<span class="math inline">\(\lambda\)</span> at the very bottom (marked
by the left vertical dotted line) gives the <strong>lowest possible
MSE</strong>. This is <code>lambda.min</code>.</li>
</ul></li>
</ul></li>
</ul>
<p><strong>Answer:</strong> You pick the <span
class="math inline">\(\lambda\)</span> that corresponds to the lowest
point on this graph.</p>
<h3 id="question-2-what-does-lasso-do-slides-5-6-7">Question 2: What
does LASSO <em>do</em>? (Slides 5, 6, 7)</h3>
<p>These slides all show the <em>effect</em> of LASSO.</p>
<p><strong>A. The Coefficient Path Plots (Slides 5 &amp; 6)</strong></p>
<p>These plots visualize how coefficients change. They show the <em>same
information</em> just with different x-axes.</p>
<ul>
<li><strong>Left Plot (Slide 6) vs.Â <span
class="math inline">\(\lambda\)</span>:</strong>
<ul>
<li><strong>How to read:</strong> Read from <strong>RIGHT to
LEFT</strong>.</li>
<li>At the far right (<span class="math inline">\(\lambda\)</span> is
large), all coefficients are 0.</li>
<li>As you move left, <span class="math inline">\(\lambda\)</span> gets
smaller, and the penalty is relaxed. Variables â€œenterâ€ the model one by
one as their coefficients become non-zero.</li>
<li>You can see â€˜Ratingâ€™ (red-dashed), â€˜Studentâ€™ (black-solid), and
â€˜Incomeâ€™ (blue-dotted) are the first to enter, suggesting they are the
most important predictors.</li>
</ul></li>
<li><strong>Right Plot (Slide 6) vs.Â <span
class="math inline">\(L_1\)</span> Norm Ratio:</strong>
<ul>
<li>This is the <em>same plot</em>, just flipped and rescaled. The
x-axis is <span class="math inline">\(\|\hat{\beta}_\lambda\|_1 /
\|\hat{\beta}_{OLS}\|_1\)</span>.</li>
<li><strong>How to read:</strong> Read from <strong>LEFT to
RIGHT</strong>.</li>
<li><strong>At 0.0:</strong> This is a â€œ0% budgetâ€ (like <span
class="math inline">\(s=0\)</span> or <span
class="math inline">\(\lambda=\infty\)</span>). All coefficients are
0.</li>
<li><strong>At 1.0:</strong> This is a â€œ100% budgetâ€ (like <span
class="math inline">\(s=\infty\)</span> or <span
class="math inline">\(\lambda=0\)</span>). This is the full OLS
model.</li>
<li>This view clearly shows the coefficients â€œgrowingâ€ from 0 as their
â€œbudgetâ€ (<span class="math inline">\(L_1\)</span> Norm) increases.</li>
</ul></li>
</ul>
<p><strong>B. The Code Output (Slide 7) - This is the most important
â€œanswerâ€</strong></p>
<p>This slide <em>explicitly demonstrates</em> variable selection by
comparing the coefficients from two different <span
class="math inline">\(\lambda\)</span> values.</p>
<ul>
<li><p><strong>First Block (The â€œOptimalâ€ Model):</strong></p>
<ul>
<li><code>bestlam.cv &lt;- cv.out$lambda.min</code>: This gets the <span
class="math inline">\(\lambda\)</span> from the bottom of the â€œUâ€ in the
CV plot.</li>
<li><code>lasso.conf &lt;- predict(out,type="coefficients",s=bestlam.cv)[1:12,]</code>:
This gets the coefficients using that <em>best</em> <span
class="math inline">\(\lambda\)</span>.</li>
<li><code>lasso.conf[lasso.conf!=0]</code>: This R command filters the
list to show <em>only the non-zero coefficients</em>.</li>
<li><strong>Result:</strong> The optimal model <em>still keeps 10
variables</em> (â€˜Incomeâ€™, â€˜Limitâ€™, â€˜Ratingâ€™, etc.). It has shrunk them,
but it hasnâ€™t set many to 0.</li>
</ul></li>
<li><p><strong>Second Block (The â€œHigh Penaltyâ€ Model):</strong></p>
<ul>
<li>The slide text says â€œif we choose a larger regularization
parameter.â€ Here, theyâ€™ve picked an arbitrary <em>larger</em> value,
<code>s=10</code>. (Note: Râ€™s <code>predict.glmnet</code> can be
confusing; <code>s=10</code> here means <span
class="math inline">\(\lambda=10\)</span>).</li>
<li><code>lasso.conf &lt;- predict(out,type="coefficients",s=10)[1:12,]</code>:
This gets the coefficients using a <em>stronger penalty</em> (<span
class="math inline">\(\lambda=10\)</span>).</li>
<li><code>lasso.conf[lasso.conf!=0]</code>: Again, show only the
non-zero coefficients.</li>
<li><strong>Result:</strong> Look! The list is much shorter. The
coefficients for â€˜Ageâ€™, â€˜Educationâ€™, â€˜GenderFemaleâ€™, â€˜MarriedYesâ€™, and
â€˜Ethnicityâ€™ are <em>all gone</em> (shrunk to 0.000000). The model has
decided these are not important enough to â€œspendâ€ budget on.</li>
</ul></li>
</ul>
<p><strong>Conclusion:</strong> LASSO performs <strong>automatic
variable selection</strong>. By increasing <span
class="math inline">\(\lambda\)</span>, you create a
<strong>sparser</strong> (simpler) model. Slide 7 is the concrete
proof.</p>
<h2 id="python-equivalents-in-more-detail">Python Equivalents (in more
detail)</h2>
<p>Here is how you would replicate the <em>entire</em> workflow from the
slides in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso, LassoCV, lasso_path</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Assume X_train, y_train, X_test, y_test are loaded ---</span></span><br><span class="line"><span class="comment"># Example: </span></span><br><span class="line"><span class="comment"># data = pd.read_csv(&#x27;Credit.csv&#x27;)</span></span><br><span class="line"><span class="comment"># X = pd.get_dummies(data.drop([&#x27;ID&#x27;, &#x27;Balance&#x27;], axis=1), drop_first=True)</span></span><br><span class="line"><span class="comment"># y = data[&#x27;Balance&#x27;]</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># It&#x27;s CRITICAL to scale data before regularization</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train_scaled = scaler.fit_transform(X_train)</span><br><span class="line">X_test_scaled = scaler.transform(X_test)</span><br><span class="line">feature_names = X.columns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Replicate the CV Plot (Slide 5: ...000200.png)</span></span><br><span class="line"><span class="comment"># LassoCV does what cv.glmnet does: finds the best lambda (alpha)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Running LassoCV to find best lambda (alpha)...&quot;</span>)</span><br><span class="line"><span class="comment"># &#x27;alphas&#x27; is the list of lambdas to try. We can let it choose automatically.</span></span><br><span class="line"><span class="comment"># cv=10 means 10-fold cross-validation.</span></span><br><span class="line">lasso_cv = LassoCV(cv=<span class="number">10</span>, random_state=<span class="number">1</span>, max_iter=<span class="number">10000</span>)</span><br><span class="line">lasso_cv.fit(X_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The best lambda found</span></span><br><span class="line">best_lambda = lasso_cv.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best lambda (alpha) found: <span class="subst">&#123;best_lambda&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Plotting the CV (MSE vs. Log(Lambda)) ---</span></span><br><span class="line"><span class="comment"># This recreates the R plot</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"><span class="comment"># lasso_cv.mse_path_ is a (n_alphas, n_folds) array of MSEs</span></span><br><span class="line"><span class="comment"># We take the mean across the folds (axis=1)</span></span><br><span class="line">mean_mses = np.mean(lasso_cv.mse_path_, axis=<span class="number">1</span>)</span><br><span class="line">log_lambdas = np.log10(lasso_cv.alphas_)</span><br><span class="line"></span><br><span class="line">plt.plot(log_lambdas, mean_mses, <span class="string">&#x27;r.-&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Log(Lambda / Alpha)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Mean-Squared Error&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;LASSO Cross-Validation Path (Replicating R Plot)&#x27;</span>)</span><br><span class="line"><span class="comment"># Plot a vertical line at the best lambda</span></span><br><span class="line">plt.axvline(np.log10(best_lambda), linestyle=<span class="string">&#x27;--&#x27;</span>, color=<span class="string">&#x27;k&#x27;</span>, label=<span class="string">f&#x27;Best Lambda (alpha) = <span class="subst">&#123;best_lambda:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.gca().invert_xaxis() <span class="comment"># High lambda is on the right in R plot</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Replicate the Coefficient Path Plot (Slide 6: ...000206.png)</span></span><br><span class="line"><span class="comment"># We can use the lasso_path function, or just use the CV object</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The lasso_cv object already calculated the paths!</span></span><br><span class="line">coefs = lasso_cv.path(X_train_scaled, y_train, alphas=lasso_cv.alphas_)[<span class="number">1</span>].T</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X_train_scaled.shape[<span class="number">1</span>]):</span><br><span class="line">    plt.plot(log_lambdas, coefs[:, i], label=feature_names[i])</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Log(Lambda / Alpha)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Standardized Coefficients&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;LASSO Coefficient Path (Replicating R Plot)&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">plt.gca().invert_xaxis()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Replicate the Code Output (Slide 7: ...000202.png)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Replicating R Output ---&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- First Block: Coefficients with BEST lambda ---</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Coefficients using best lambda (alpha = <span class="subst">&#123;best_lambda:<span class="number">.4</span>f&#125;</span>):&quot;</span>)</span><br><span class="line"><span class="comment"># The lasso_cv object is already fitted with the best lambda</span></span><br><span class="line">best_coefs = lasso_cv.coef_</span><br><span class="line">coef_series_best = pd.Series(best_coefs, index=feature_names)</span><br><span class="line"><span class="comment"># This is like R&#x27;s `lasso.conf[lasso.conf != 0]`</span></span><br><span class="line"><span class="built_in">print</span>(coef_series_best[coef_series_best != <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Second Block: Coefficients with a LARGER lambda ---</span></span><br><span class="line"><span class="comment"># Let&#x27;s pick a larger lambda, e.g., 10 (like the slide)</span></span><br><span class="line">large_lambda = <span class="number">10</span> </span><br><span class="line">lasso_high_penalty = Lasso(alpha=large_lambda)</span><br><span class="line">lasso_high_penalty.fit(X_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nCoefficients using larger lambda (alpha = <span class="subst">&#123;large_lambda&#125;</span>):&quot;</span>)</span><br><span class="line">high_pen_coefs = lasso_high_penalty.coef_</span><br><span class="line">coef_series_high = pd.Series(high_pen_coefs, index=feature_names)</span><br><span class="line"><span class="comment"># This is the second R command: `lasso.conf[lasso.conf != 0]`</span></span><br><span class="line"><span class="built_in">print</span>(coef_series_high[coef_series_high != <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Final Prediction ---</span></span><br><span class="line"><span class="comment"># This is R&#x27;s `mean((lasso.pred-y.test)^2)`</span></span><br><span class="line">y_pred = lasso_cv.predict(X_test_scaled)</span><br><span class="line">test_mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nTest MSE using best lambda: <span class="subst">&#123;test_mse:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="the-game-of-regularization">The â€œGameâ€ of Regularization</h3>
<p>First, letâ€™s understand what these plots are showing. This is a â€œmapâ€
of a constrained optimization problem.</p>
<ul>
<li><strong>The Red Ellipses (RSS Contours):</strong> Think of these as
contour lines on a topographic map.
<ul>
<li><strong>The Center (<span
class="math inline">\(\hat{\beta}\)</span>):</strong> This point is the
â€œbottom of the valley.â€ It represents the <em>perfect</em>,
unconstrained solutionâ€”the standard Ordinary Least Squares (OLS)
coefficients. This point has the lowest possible Residual Sum of Squares
(RSS), or error.</li>
<li><strong>The Lines:</strong> Every point on a single red ellipse has
the <em>exact same</em> RSS. As the ellipses get bigger (moving away
from the center <span class="math inline">\(\hat{\beta}\)</span>), the
error gets higher.</li>
</ul></li>
<li><strong>The Blue Shaded Area (Constraint Region):</strong> This is
the â€œruleâ€ of the game.
<ul>
<li>This is our â€œbudget.â€ We are <em>only allowed</em> to pick a
solution (<span class="math inline">\(\beta_1, \beta_2\)</span>) from
<em>inside or on the boundary</em> of this blue shape.</li>
<li><strong>LASSO:</strong> The constraint is <span
class="math inline">\(|\beta_1| + |\beta_2| \le s\)</span>. This
equation forms a <strong>diamond</strong> (or a rotated square).</li>
<li><strong>Ridge:</strong> The constraint is <span
class="math inline">\(\beta_1^2 + \beta_2^2 \le s\)</span>. This
equation forms a <strong>circle</strong>.</li>
</ul></li>
<li><strong>The Goal:</strong> Find the â€œbestâ€ point that is <em>inside
the blue area</em>.
<ul>
<li>The â€œbestâ€ point is the one with the lowest possible error
(RSS).</li>
<li>Geometrically, this means we start at the center (<span
class="math inline">\(\hat{\beta}\)</span>) and expand our ellipse
outward. The <em>very first point</em> where the ellipse
<strong>touches</strong> the blue constraint region is our
solution.</li>
</ul></li>
</ul>
<h3 id="why-lasso-performs-variable-selection-the-diamond">Why LASSO
Performs Variable Selection (The Diamond) ğŸ¯</h3>
<p>This is the most important concept. Look at the LASSO diagrams.</p>
<ul>
<li><strong>The Shape:</strong> The LASSO constraint is a
<strong>diamond</strong>.</li>
<li><strong>The Key Feature:</strong> This diamond has <strong>sharp
corners</strong> (vertices). And most importantly, these corners lie
<strong>exactly on the axes</strong>.
<ul>
<li>The top corner is at <span class="math inline">\((\beta_1=0,
\beta_2=s)\)</span>.</li>
<li>The right corner is at <span class="math inline">\((\beta_1=s,
\beta_2=0)\)</span>.</li>
</ul></li>
<li><strong>The â€œCollisionâ€:</strong> Now, imagine the red ellipses
(representing our error) expanding from the OLS solution (<span
class="math inline">\(\hat{\beta}\)</span>). They will almost always
â€œhitâ€ the blue diamond at one of its <strong>sharp corners</strong>.
<ul>
<li>Look at your textbook diagram (slide <code>...000304.png</code>).
The ellipse clearly makes contact with the diamond at the top corner,
where <span class="math inline">\(\beta_1 = 0\)</span>.</li>
<li>Look at your example (slide <code>...000259.jpg</code>). The center
of the ellipses is at (4, 0.1). The closest point on the diamond that
the expanding ellipses will hit is the corner at (2, 0). At this
solution, <strong><span class="math inline">\(y\)</span> is exactly
0</strong>.</li>
</ul></li>
</ul>
<p><strong>Conclusion:</strong> Because the <span
class="math inline">\(L_1\)</span> â€œdiamondâ€ has corners on the axes,
the optimal solution is very likely to land on one of them. When it
does, the coefficient for the <em>other</em> axis is set to
<strong>exactly zero</strong>. This is the <strong>variable selection
property</strong>.</p>
<h3 id="why-ridge-regression-only-shrinks-the-circle">Why Ridge
Regression Only Shrinks (The Circle) ğŸ¤</h3>
<p>Now, look at the Ridge regression diagram.</p>
<ul>
<li><strong>The Shape:</strong> The Ridge constraint is a
<strong>circle</strong>.</li>
<li><strong>The Key Feature:</strong> A circle is perfectly smooth and
has <strong>no corners</strong>.</li>
<li><strong>The â€œCollisionâ€:</strong> Imagine the same ellipses
expanding and hitting the blue circle. The contact point will be a
<em>tangent</em> point.
<ul>
<li>Because the circle is round, this tangent point can be
<em>anywhere</em> on its circumference.</li>
<li>It is <em>extremely unlikely</em> that the contact point will be
exactly on an axis (e.g., at <span class="math inline">\((\beta_1=0,
\beta_2=s)\)</span>). This would only happen if the OLS solution <span
class="math inline">\(\hat{\beta}\)</span> was <em>already</em>
perfectly aligned with that axis.</li>
</ul></li>
<li><strong>Conclusion:</strong> The Ridge solution will find a point
where <em>both</em> <span class="math inline">\(\beta_1\)</span> and
<span class="math inline">\(\beta_2\)</span> are non-zero. The
coefficients are â€œshrunkâ€ (pulled in from <span
class="math inline">\(\hat{\beta}\)</span> towards the origin), but they
<strong>never become zero</strong>. This is why Ridge is called a
â€œshrinkageâ€ method, but not a â€œvariable selectionâ€ method.</li>
</ul>
<h3 id="summary-diamond-vs.-circle">Summary: Diamond vs.Â Circle</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">LASSO (<span
class="math inline">\(L_1\)</span> Norm)</th>
<th style="text-align: left;">Ridge (<span
class="math inline">\(L_2\)</span> Norm)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Constraint Shape</strong></td>
<td style="text-align: left;"><strong>Diamond</strong> (or
hyper-rhombus)</td>
<td style="text-align: left;"><strong>Circle</strong> (or
hypersphere)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Key Feature</strong></td>
<td style="text-align: left;"><strong>Sharp corners</strong> on the
axes</td>
<td style="text-align: left;"><strong>Smooth curve</strong> with no
corners</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Geometric Solution</strong></td>
<td style="text-align: left;">Ellipses hit the
<strong>corners</strong></td>
<td style="text-align: left;">Ellipses hit a <strong>smooth
part</strong></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Result</strong></td>
<td style="text-align: left;">Forces some coefficients to
<strong>exactly 0</strong></td>
<td style="text-align: left;">Shrinks all coefficients <em>towards</em>
0</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Name</strong></td>
<td style="text-align: left;"><strong>Variable Selection</strong></td>
<td style="text-align: left;"><strong>Shrinkage</strong></td>
</tr>
</tbody>
</table>
<p>The â€œspace meaningâ€ is that the <strong>sharp corners of the <span
class="math inline">\(L_1\)</span> diamond are what make variable
selection possible</strong>. The smooth circle of the <span
class="math inline">\(L_2\)</span> norm does not have these corners and
thus cannot force coefficients to zero.</p>
<h1 id="shrinkage-methods-lasso-vs.-ridge">8. Shrinkage Methods (Lasso
vs.Â Ridge)</h1>
<h2 id="core-concept-shrinkage-methods">Core Concept: Shrinkage
Methods</h2>
<p>Both <strong>Ridge (L2)</strong> and <strong>Lasso (L1)</strong> are
regularization techniques used to improve upon standard <strong>Ordinary
Least Squares (OLS)</strong> regression.</p>
<p>Their main goal is to manage the <strong>bias-variance
tradeoff</strong>. OLS often has low bias but very high variance,
especially when you have many predictors (<span
class="math inline">\(p\)</span>) or when predictors are correlated.
Ridge and Lasso improve prediction accuracy by <em>shrinking</em> the
regression coefficients towards zero. This adds a small amount of bias
but significantly <em>reduces</em> the variance, leading to a lower
overall Test Mean Squared Error (MSE).</p>
<h2 id="the-key-difference-math-how-they-shrink">The Key Difference:
Math &amp; How They Shrink</h2>
<p>The slides show that the two methods use different penalties, which
leads to very different mathematical forms and practical outcomes.</p>
<ul>
<li><strong>Ridge Regression (L2 Penalty):</strong> Minimizes <span
class="math inline">\(RSS + \lambda \sum_{j=1}^{p}
\beta_j^2\)</span></li>
<li><strong>Lasso Regression (L1 Penalty):</strong> Minimizes <span
class="math inline">\(RSS + \lambda \sum_{j=1}^{p}
|\beta_j|\)</span></li>
</ul>
<p>Slide 80 provides the exact formulas for their coefficient estimates
in a simple, orthogonal case (where predictors are independent):</p>
<h3 id="ridge-regression-proportional-shrinkage">Ridge Regression
(Proportional Shrinkage)</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\beta}_j^R = \hat{\beta}_j^{LSE} / (1 +
\lambda)\)</span></li>
<li><strong>What this means:</strong> Ridge <em>shrinks</em> every least
squares coefficient by a proportional amount. It will make coefficients
<em>smaller</em>, but it will <strong>never set them to exactly
zero</strong> (unless <span class="math inline">\(\lambda\)</span> is
<span class="math inline">\(\infty\)</span>).</li>
</ul>
<h3 id="lasso-regression-soft-thresholding">Lasso Regression
(Soft-Thresholding)</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\beta}_j^L =
\text{sign}(\hat{\beta}_j^{LSE})(|\hat{\beta}_j^{LSE}| -
\lambda/2)_+\)</span></li>
<li><strong>What this means:</strong> This is a â€œsoft-thresholdingâ€
operator.
<ul>
<li>If the original coefficient <span
class="math inline">\(\hat{\beta}_j^{LSE}\)</span> is small (its
absolute value is less than <span
class="math inline">\(\lambda/2\)</span>), Lasso <strong>sets it to
exactly zero</strong>.</li>
<li>If the coefficient is large, Lasso subtracts <span
class="math inline">\(\lambda/2\)</span> from its absolute value,
shrinking it towards zero.</li>
</ul></li>
<li><strong>Key Property:</strong> Because of this, Lasso performs
<strong>automatic feature selection</strong> by eliminating
predictors.</li>
</ul>
<h2 id="important-images-explained">Important Images Explained</h2>
<h3 id="most-important-figure-6.10-slide-82">Most Important: Figure 6.10
(Slide 82)</h3>
<p>This is the best visual for understanding the <em>mathematical
difference</em> from the formulas above.</p>
<ul>
<li><strong>Left (Ridge):</strong> The red line shows the Ridge estimate
vs.Â the OLS estimate. Itâ€™s a straight, diagonal line with a slope less
than 1. It shrinks everything <em>proportionally</em>.</li>
<li><strong>Right (Lasso):</strong> The red line shows the Lasso
estimate. Itâ€™s â€œflatâ€ at zero for a range, showing it <strong>sets small
coefficients to zero</strong>. Then, it slopes up, but itâ€™s shifted (it
shrinks the large coefficients by a fixed amount).</li>
</ul>
<h3 id="scenario-1-figure-6.8-slide-76">Scenario 1: Figure 6.8 (Slide
76)</h3>
<p>This plot shows what happens when <strong>all 45 predictors are truly
related to the response</strong>.</p>
<ul>
<li><strong>Result (Slide 77):</strong> <strong>Ridge performs slightly
better</strong> (has a lower minimum MSE, shown by the dotted purple
line).</li>
<li><strong>Why:</strong> Lassoâ€™s assumption (that some coefficients are
zero) is <em>wrong</em> in this case. By forcing some relevant
predictors to zero, it adds too much bias. Ridge, by just
<em>shrinking</em> all of them, finds a better balance.</li>
</ul>
<h3 id="scenario-2-figure-6.9-slide-78">Scenario 2: Figure 6.9 (Slide
78)</h3>
<p>This plot shows the <em>opposite</em> scenario: <strong>only 2 out of
45 predictors are truly related</strong> (a â€œsparseâ€ model).</p>
<ul>
<li><strong>Result:</strong> <strong>Lasso performs much better</strong>
(its solid purple line has a much lower minimum MSE).</li>
<li><strong>Why:</strong> Lassoâ€™s assumption is <em>correct</em>. It
successfully sets the 43 â€œnoiseâ€ predictors to zero, which dramatically
reduces variance, while correctly keeping the 2 important ones.</li>
</ul>
<h2 id="python-code-understanding-1">Python &amp; Code
Understanding</h2>
<p>The slides donâ€™t contain Python code, but they describe the exact
concepts you would use, primarily in <code>scikit-learn</code>.</p>
<ul>
<li><p><strong>Implementing Ridge &amp; Lasso:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, Lasso, RidgeCV, LassoCV</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># It&#x27;s crucial to scale data before regularization</span></span><br><span class="line"><span class="comment"># alpha is the same as the Î» (lambda) in your slides</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Ridge ---</span></span><br><span class="line"><span class="comment"># The math for Ridge is a &quot;closed-form solution&quot; (Slide 80)</span></span><br><span class="line"><span class="comment"># ridge_model = make_pipeline(StandardScaler(), Ridge(alpha=1.0))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Lasso ---</span></span><br><span class="line"><span class="comment"># Lasso requires a numerical solver (like coordinate descent)</span></span><br><span class="line"><span class="comment"># lasso_model = make_pipeline(StandardScaler(), Lasso(alpha=0.1))</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>The Soft-Thresholding Formula:</strong> The math from
Slide 80, <span class="math inline">\(\text{sign}(y)(|y| -
\lambda/2)_+\)</span>, is the core operation in the â€œcoordinate descentâ€
algorithm used to solve Lasso. You could write it in Python/Numpy:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">soft_threshold</span>(<span class="params">x, lambda_val</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;Implements the Lasso soft-thresholding formula.&quot;&quot;&quot;</span></span><br><span class="line">  <span class="keyword">return</span> np.sign(x) * np.maximum(<span class="number">0</span>, np.<span class="built_in">abs</span>(x) - (lambda_val / <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example:</span></span><br><span class="line"><span class="comment"># ols_coefficient = 1.5</span></span><br><span class="line"><span class="comment"># threshold = 4.0</span></span><br><span class="line"><span class="comment"># lasso_coefficient = soft_threshold(ols_coefficient, threshold) </span></span><br><span class="line"><span class="comment"># print(lasso_coefficient) # Output: 0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ols_coefficient = 3.0</span></span><br><span class="line"><span class="comment"># threshold = 4.0</span></span><br><span class="line"><span class="comment"># lasso_coefficient = soft_threshold(ols_coefficient, threshold) </span></span><br><span class="line"><span class="comment"># print(lasso_coefficient) # Output: 1.0 (it was 3.0, shrunk by 4/2 = 2)</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>Choosing <span class="math inline">\(\lambda\)</span>
(alpha):</strong> Slide 79 says to â€œUse cross validation to determine
which one has better prediction.â€ In <code>scikit-learn</code>, this is
done for you with <code>RidgeCV</code> and <code>LassoCV</code>, which
automatically test a range of <code>alpha</code> values.</p></li>
</ul>
<h2 id="summary-lasso-vs.-ridge">Summary: Lasso vs.Â Ridge</h2>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Ridge (L2)</th>
<th style="text-align: left;">Lasso (L1)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Penalty</strong></td>
<td style="text-align: left;"><span class="math inline">\(L_2\)</span>
norm: <span class="math inline">\(\lambda \sum \beta_j^2\)</span></td>
<td style="text-align: left;"><span class="math inline">\(L_1\)</span>
norm: <span class="math inline">\(\lambda \sum |\beta_j|\)</span></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Coefficient
Shrinkage</strong></td>
<td style="text-align: left;">Proportional; shrinks all coefficients,
but never to <em>exactly</em> zero.</td>
<td style="text-align: left;">Soft-thresholding; can force coefficients
to be <em>exactly</em> zero.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Feature Selection?</strong></td>
<td style="text-align: left;">No</td>
<td style="text-align: left;"><strong>Yes</strong>, this is its main
advantage.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Interpretability</strong></td>
<td style="text-align: left;">Less interpretable (keeps all <span
class="math inline">\(p\)</span> variables).</td>
<td style="text-align: left;">More interpretable (produces a â€œsparseâ€
model with fewer variables).</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Best Used Whenâ€¦</strong></td>
<td style="text-align: left;">â€¦most predictors are useful. (e.g., Slide
76: 45/45 relevant).</td>
<td style="text-align: left;">â€¦many predictors are â€œnoiseâ€ and only a
few are strong. (e.g., Slide 78: 2/45 relevant).</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Computation</strong></td>
<td style="text-align: left;">Has a simple, closed-form solution.</td>
<td style="text-align: left;">Requires numerical optimization (e.g.,
coordinate descent).</td>
</tr>
</tbody>
</table>
<h1 id="shrinkage-methods-ridge-lasso">9. Shrinkage Methods (Ridge &amp;
LASSO)</h1>
<h2 id="summary-of-shrinkage-methods-ridge-lasso">Summary of Shrinkage
Methods (Ridge &amp; LASSO)</h2>
<p>These slides introduce <strong>shrinkage methods</strong>, also known
as <strong>regularization</strong>, a technique used in regression (like
linear regression) to improve model performance. The main idea is to add
a <em>penalty</em> to the modelâ€™s loss function to â€œshrinkâ€ the size of
the coefficients. This helps to reduce model variance and prevent
overfitting, especially when you have many features.</p>
<p>The two main methods discussed are <strong>Ridge Regression</strong>
(<span class="math inline">\(L_2\)</span> penalty) and
<strong>LASSO</strong> (<span class="math inline">\(L_1\)</span>
penalty).</p>
<h2 id="key-mathematical-formulas-1">Key Mathematical Formulas</h2>
<ol type="1">
<li><p><strong>Standard Linear Model:</strong> The problem starts with
the standard linear regression model (from slide 1):</p>
<p><span class="math display">\[
\]</span>$$\mathbf{y} = \mathbf{X}\beta + \epsilon</p>
<p><span class="math display">\[
\]</span>$$ * <span class="math inline">\(\mathbf{y}\)</span> is the
<span class="math inline">\(n \times 1\)</span> vector of observed
outcomes.</p>
<ul>
<li><span class="math inline">\(\mathbf{X}\)</span> is the <span
class="math inline">\(n \times p\)</span> matrix of <span
class="math inline">\(p\)</span> predictor features for <span
class="math inline">\(n\)</span> observations.</li>
<li><span class="math inline">\(\beta\)</span> is the <span
class="math inline">\(p \times 1\)</span> vector of coefficients (what
we want to find).</li>
<li><span class="math inline">\(\epsilon\)</span> is the <span
class="math inline">\(n \times 1\)</span> vector of random errors.</li>
<li>The goal of standard â€œOrdinary Least Squaresâ€ (OLS) regression is to
find the <span class="math inline">\(\beta\)</span> that minimizes the
loss: <span class="math inline">\(\|\mathbf{X}\beta -
\mathbf{y}\|^2_2\)</span>.</li>
</ul></li>
<li><p><strong>LASSO (L1 Regularization):</strong> LASSO (Least Absolute
Shrinkage and Selection Operator) adds a penalty based on the
<em>absolute value</em> of the coefficients (the <span
class="math inline">\(L_1\)</span>-norm). This is the key formula from
slide 1:</p>
<p><span class="math display">\[
\]</span>$$\hat{\beta}(\lambda) \leftarrow \arg \min_{\beta} \left(
|\mathbf{X}\beta - \mathbf{y}|^2_2 + \lambda|\beta|_1 \right)</p>
<p><span class="math display">\[
\]</span>$$ * <span class="math inline">\(\|\beta\|_1 = \sum_{j=1}^{p}
|\beta_j|\)</span></p>
<ul>
<li><span class="math inline">\(\lambda\)</span> (lambda) is the
<strong>tuning parameter</strong> that controls the strength of the
penalty. A larger <span class="math inline">\(\lambda\)</span> means
more shrinkage.</li>
<li><strong>Key Property (Variable Selection):</strong> The <span
class="math inline">\(L_1\)</span> penalty can force some coefficients
(<span class="math inline">\(\beta_j\)</span>) to become <strong>exactly
zero</strong>. This means LASSO simultaneously performs <em>feature
selection</em> by automatically removing irrelevant predictors.</li>
<li><strong>Support (Slide 1):</strong> The question â€œCan it recover the
support of <span class="math inline">\(\beta\)</span>?â€ is asking if
LASSO can correctly identify the set of true non-zero coefficients
(defined as <span class="math inline">\(S := \{j : \beta_j \neq
0\}\)</span>).</li>
</ul></li>
<li><p><strong>Ridge Regression (L2 Regularization):</strong> Ridge
regression (mentioned on slide 2, shown on slide 3) adds a penalty based
on the <em>squared value</em> of the coefficients (the <span
class="math inline">\(L_2\)</span>-norm).</p>
<p><span class="math display">\[
\]</span>$$\hat{\beta}(\lambda) \leftarrow \arg \min_{\beta} \left(
|\mathbf{X}\beta - \mathbf{y}|^2_2 + \lambda|\beta|^2_2 \right)</p>
<p><span class="math display">\[
\]</span>$$ * <span class="math inline">\(\|\beta\|^2_2 = \sum_{j=1}^{p}
\beta_j^2\)</span></p>
<ul>
<li><strong>Key Property (Shrinkage):</strong> The <span
class="math inline">\(L_2\)</span> penalty <em>shrinks</em> coefficients
<em>towards</em> zero but <strong>never</strong> sets them to
<em>exactly</em> zero (unless <span class="math inline">\(\lambda =
\infty\)</span>). It is effective at handling multicollinearity.</li>
</ul></li>
</ol>
<h2 id="important-images-concepts">Important Images &amp; Concepts</h2>
<p>The most important images are the plots from slides 3 and 4. They
illustrate the two most critical concepts: <strong>how to choose <span
class="math inline">\(\lambda\)</span></strong> and <strong>what the
penalty does to the coefficients</strong>.</p>
<h3 id="tuning-parameter-selection-slides-3-4-left-plots">Tuning
Parameter Selection (Slides 3 &amp; 4, Left Plots)</h3>
<ul>
<li><strong>Problem:</strong> How do you find the <em>best</em> value
for <span class="math inline">\(\lambda\)</span>?</li>
<li><strong>Solution:</strong> <strong>Cross-Validation (CV)</strong>.
The slides show 10-fold CV.</li>
<li><strong>What the Plots Show:</strong> The left plots on slides 3 and
4 show the <strong>Cross-Validation Error</strong> (like MSE) for
different values of the penalty.
<ul>
<li>The x-axis represents the penalty strength (either <span
class="math inline">\(\lambda\)</span> itself or a related measure like
the shrinkage ratio <span
class="math inline">\(\|\hat{\beta}_\lambda\|_1 /
\|\hat{\beta}\|_1\)</span>).</li>
<li>The y-axis is the prediction error.</li>
<li>The curve is typically <strong>U-shaped</strong>. The vertical
dashed line marks the <strong>minimum</strong> of this curve. This
minimum point corresponds to the <strong>optimal <span
class="math inline">\(\lambda\)</span></strong>, which provides the best
balance between bias and variance, leading to the best-performing model
on unseen data.</li>
</ul></li>
</ul>
<h3 id="coefficient-paths-slides-3-4-right-plots">Coefficient Paths
(Slides 3 &amp; 4, Right Plots)</h3>
<p>These â€œtraceâ€ plots are crucial for understanding the difference
between Ridge and LASSO. They show how the value of each coefficient
(y-axis) changes as the penalty strength (x-axis) changes.</p>
<ul>
<li><strong>Slide 3 (Ridge):</strong> As <span
class="math inline">\(\lambda\)</span> increases (moving right), all
coefficient values are smoothly shrunk <em>towards</em> zero, but none
of them actually hit zero.</li>
<li><strong>Slide 4 (LASSO):</strong> As the penalty increases (moving
from right to left, as the ratio <span class="math inline">\(s\)</span>
goes from 1.0 to 0.0), you can see coefficients â€œdrop offâ€ and become
<strong>exactly zero</strong> one by one. The model with the optimal
<span class="math inline">\(\lambda\)</span> (vertical line) has
selected only a few non-zero coefficients (the pink and teal lines),
while all the grey lines have been set to zero. This is <em>feature
selection</em> in action.</li>
</ul>
<h2 id="key-discussion-points-slide-2">Key Discussion Points (Slide
2)</h2>
<ul>
<li><strong>Non-linear models:</strong> You can apply these methods to
non-linear models by first creating non-linear features (e.g., <span
class="math inline">\(x_1^2\)</span>, <span
class="math inline">\(x_2^2\)</span>, <span class="math inline">\(x_1
\cdot x_2\)</span>) and then feeding them into a LASSO or Ridge model.
The regularization will then select which of these linear <em>or</em>
non-linear terms are important.</li>
<li><strong>Correlated Features (Multicollinearity):</strong> The
question â€œIf <span class="math inline">\(x_j \approx x_k\)</span>, how
does LASSO behave?â€ is a key weakness of LASSO.
<ul>
<li><strong>LASSO:</strong> Tends to <em>arbitrarily</em> select one of
the correlated features and set the others to zero. This can make the
model unstable.</li>
<li><strong>Ridge:</strong> Tends to shrink the coefficients of
correlated features <em>together</em>, giving them similar (but smaller)
values.</li>
<li><strong>Elastic Net</strong> (not shown) is a hybrid of Ridge and
LASSO that is often used to get the best of both worlds: it can select
groups of correlated variables.</li>
</ul></li>
</ul>
<h2 id="python-code-understanding-using-scikit-learn">Python Code
Understanding (using <code>scikit-learn</code>)</h2>
<p>Here is how you would implement these concepts in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso, Ridge, LassoCV, RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Assume you have your data ---</span></span><br><span class="line"><span class="comment"># X: your feature matrix (e.g., shape 100, 20)</span></span><br><span class="line"><span class="comment"># y: your target vector (e.g., shape 100,)</span></span><br><span class="line"><span class="comment"># X, y = ... load your data ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. It&#x27;s crucial to scale your data before regularization</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Find the optimal lambda (alpha) using Cross-Validation</span></span><br><span class="line"><span class="comment"># scikit-learn uses &#x27;alpha&#x27; instead of &#x27;lambda&#x27; for the tuning parameter.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- For LASSO ---</span></span><br><span class="line"><span class="comment"># LassoCV automatically performs cross-validation (e.g., cv=10)</span></span><br><span class="line"><span class="comment"># to find the best alpha.</span></span><br><span class="line">lasso_cv_model = LassoCV(cv=<span class="number">10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">lasso_cv_model.fit(X_scaled, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best alpha (lambda)</span></span><br><span class="line">best_alpha_lasso = lasso_cv_model.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal alpha (lambda) for LASSO: <span class="subst">&#123;best_alpha_lasso&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the final coefficients</span></span><br><span class="line">lasso_coeffs = lasso_cv_model.coef_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;LASSO coefficients: <span class="subst">&#123;lasso_coeffs&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># You will see that many of these are exactly 0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- For Ridge ---</span></span><br><span class="line"><span class="comment"># RidgeCV works similarly. It&#x27;s often good to test alphas on a log scale.</span></span><br><span class="line">ridge_alphas = np.logspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>) <span class="comment"># 100 values from 0.001 to 1000</span></span><br><span class="line">ridge_cv_model = RidgeCV(alphas=ridge_alphas, store_cv_values=<span class="literal">True</span>)</span><br><span class="line">ridge_cv_model.fit(X_scaled, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the best alpha (lambda)</span></span><br><span class="line">best_alpha_ridge = ridge_cv_model.alpha_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Optimal alpha (lambda) for Ridge: <span class="subst">&#123;best_alpha_ridge&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the final coefficients</span></span><br><span class="line">ridge_coeffs = ridge_cv_model.coef_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ridge coefficients: <span class="subst">&#123;ridge_coeffs&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># You will see these are small, but not exactly zero.</span></span><br></pre></td></tr></table></figure>
<h2 id="bias-variance-tradeoff">Bias-variance tradeoff</h2>
<h2 id="key-mathematical-formulas-concepts">Key Mathematical Formulas
&amp; Concepts</h2>
<h3 id="lasso-sign-consistency">LASSO: Sign Consistency</h3>
<p>This is the â€œidealâ€ scenario for LASSO. Sign consistency means that,
with enough data, the LASSO model not only selects the <em>correct</em>
set of features (it recovers the â€œsupportâ€ <span
class="math inline">\(S\)</span>) but also correctly identifies the
<em>sign</em> (positive or negative) of their coefficients.</p>
<ul>
<li><p><strong>The Goal (Slide 1):</strong></p>
<p><span class="math display">\[
\]</span>$$\text{sign}(\hat{\beta}(\lambda)) = \text{sign}(\beta)</p>
<p><span class="math display">\[
\]</span>$$This means the signs of our <em>estimated</em> coefficients
<span class="math inline">\(\hat{\beta}(\lambda)\)</span> match the
signs of the <em>true</em> underlying coefficients <span
class="math inline">\(\beta\)</span>.</p></li>
<li><p><strong>The â€œIrrepresentable Conditionâ€ (Slide 1):</strong> This
is the mathematical guarantee required for LASSO to achieve sign
consistency.</p>
<p><span class="math display">\[
\]</span>$$|\mathbf{X}_{S<sup>c}</sup>\top \mathbf{X}_S
(\mathbf{X}_S^\top \mathbf{X}<em>S)^{-1}
\text{sign}(\beta_S)|</em>\infty &lt; 1</p>
<p><span class="math display">\[
\]</span>$$ * <strong>Plain English:</strong> This formula is a complex
way of saying: <strong>The irrelevant features (<span
class="math inline">\(\mathbf{X}_{S^c}\)</span>) cannot be too strongly
correlated with the true, relevant features (<span
class="math inline">\(\mathbf{X}_S\)</span>).</strong></p>
<ul>
<li>If an irrelevant feature is very similar (highly correlated) to a
true feature, LASSO can get â€œconfusedâ€ and might pick the wrong one, or
its estimate will be unstable. This condition fails.</li>
</ul></li>
</ul>
<h3 id="ridge-regression-the-bias-variance-tradeoff">Ridge Regression:
The Bias-Variance Tradeoff</h3>
<ul>
<li><p><strong>The Formula (Slide 3):</strong></p>
<p><span class="math display">\[
\]</span>$$\hat{\beta}<em>{\text{ridge}}(\lambda) \leftarrow \arg
\min</em>{\beta} \left( |\mathbf{y} - \mathbf{X}\beta|^2 +
\lambda|\beta|^2 \right)</p>
<p><span class="math display">\[
\]</span>$$<em>(Note: This is the <span
class="math inline">\(L_2\)</span> penalty, so <span
class="math inline">\(\|\beta\|^2 = \sum
\beta_j^2\)</span>)</em></p></li>
<li><p><strong>The Problem it Solves: Collinearity (Slide 2)</strong>
When features are strongly correlated (e.g., <span
class="math inline">\(x_i \approx x_j\)</span>), regular methods
fail:</p>
<ul>
<li><strong>LSE (OLS):</strong> Fails because the matrix <span
class="math inline">\(\mathbf{X}^\top \mathbf{X}\)</span> is
â€œnon-invertibleâ€ (or singular), so the math for the solution <span
class="math inline">\(\hat{\beta} = (\mathbf{X}^\top
\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}\)</span> breaks down.</li>
<li><strong>LASSO:</strong> Fails because the <strong>Irrepresentable
Condition</strong> is violated. LASSO will tend to <em>arbitrarily</em>
pick one of the correlated features and set the others to zero.</li>
</ul></li>
<li><p><strong>The Ridge Solution (Slide 3):</strong></p>
<ol type="1">
<li><strong>Always has a solution:</strong> Adding the <span
class="math inline">\(\lambda\)</span> penalty makes the matrix math
work, even if <span class="math inline">\(\mathbf{X}^\top
\mathbf{X}\)</span> is non-invertible.</li>
<li><strong>Groups variables:</strong> This is the key takeaway. Instead
of arbitrarily picking one feature, <strong>Ridge tends to shrink the
coefficients of collinear variables <em>together</em></strong>.</li>
<li><strong>Bias-Variance Tradeoff:</strong> Ridge <em>introduces
bias</em> into the estimates (they are â€œwrongâ€ on purpose) to
<em>massively reduce variance</em> (they are more stable and less
sensitive to the specific training data). This trade-off usually leads
to a much lower overall error (Mean Squared Error).</li>
</ol></li>
</ul>
<h2 id="important-images-key-takeaways">Important Images &amp; Key
Takeaways</h2>
<ol type="1">
<li><p><strong>Slide 2 (Collinearity Failures):</strong> This is the
most important â€œproblemâ€ slide. It clearly explains <em>why</em> you
canâ€™t always use standard LSE or LASSO. The fact that all three methods
(LSE, LASSO, Forward Selection) fail with strong collinearity motivates
the need for Ridge.</p></li>
<li><p><strong>Slide 3 (Ridge Properties):</strong> This is the most
important â€œsolutionâ€ slide. The two most critical points are:</p>
<ul>
<li><code>Always unique solution for Î» &gt; 0</code></li>
<li><code>Collinear variables tend to be grouped!</code> (This is the
â€œfixâ€ for the problem on Slide 2).</li>
</ul></li>
</ol>
<h2 id="python-code-understanding-2">Python Code Understanding</h2>
<p>Letâ€™s demonstrate the <strong>key difference</strong> (Slide 3) in
how LASSO and Ridge handle collinear features.</p>
<p>We will create two features, <code>x1</code> and <code>x2</code>,
that are nearly identical.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso, Ridge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Create a dataset with 2 strongly correlated features</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line"><span class="comment"># x1: a standard feature</span></span><br><span class="line">x1 = np.random.randn(n_samples)</span><br><span class="line"><span class="comment"># x2: almost identical to x1</span></span><br><span class="line">x2 = x1 + <span class="number">0.01</span> * np.random.randn(n_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine into our feature matrix X</span></span><br><span class="line">X = np.c_[x1, x2]</span><br><span class="line"></span><br><span class="line"><span class="comment"># y: The target variable (let&#x27;s say y = 2*x1 + 2*x2)</span></span><br><span class="line">y = <span class="number">2</span> * x1 + <span class="number">2</span> * x2 + np.random.randn(n_samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit LASSO (alpha is the same as lambda)</span></span><br><span class="line"><span class="comment"># We use a moderate alpha</span></span><br><span class="line">lasso_model = Lasso(alpha=<span class="number">1.0</span>)</span><br><span class="line">lasso_model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit Ridge (alpha is the same as lambda)</span></span><br><span class="line">ridge_model = Ridge(alpha=<span class="number">1.0</span>)</span><br><span class="line">ridge_model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Compare the coefficients</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- Results for Correlated Features ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;True Coefficients: [2.0, 2.0]&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;LASSO Coefficients: <span class="subst">&#123;np.<span class="built_in">round</span>(lasso_model.coef_, <span class="number">2</span>)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ridge Coefficients: <span class="subst">&#123;np.<span class="built_in">round</span>(ridge_model.coef_, <span class="number">2</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="example-output">Example Output:</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--- Results for Correlated Features ---</span><br><span class="line">True Coefficients: [2.0, 2.0]</span><br><span class="line">LASSO Coefficients: [3.89 0.  ]</span><br><span class="line">Ridge Coefficients: [1.95 1.94]</span><br></pre></td></tr></table></figure>
<h3 id="code-explanation">Code Explanation:</h3>
<ul>
<li><strong>LASSO:</strong> As predicted by the slides, LASSO failed to
find the true model. It <em>arbitrarily</em> picked <code>x1</code>,
gave it a large coefficient, and <strong>set <code>x2</code> to
zero</strong>. This is unstable and not what we wanted.</li>
<li><strong>Ridge:</strong> As predicted by Slide 3, Ridge handled the
collinearity perfectly. It identified that both <code>x1</code> and
<code>x2</code> were important and <strong>â€œgroupedâ€ them</strong> by
assigning them nearly identical, stable coefficients (1.95 and 1.94),
which are very close to the true values of 2.0.</li>
</ul>
<h1 id="elastic-net">10. Elastic Net</h1>
<h2 id="overall-summary">Overall Summary</h2>
<p>These slides introduce <strong>Elastic Net</strong>, a modern
regression method that solves the major weaknesses of its two
predecessors, <strong>Ridge</strong> and <strong>LASSO</strong>
regression.</p>
<ul>
<li><strong>Ridge</strong> is good for <strong>collinearity</strong>
(correlated features) but canâ€™t do <strong>variable selection</strong>
(it canâ€™t set any featureâ€™s coefficient to <em>exactly</em> zero).</li>
<li><strong>LASSO</strong> is good for <strong>variable
selection</strong> (it creates <em>sparse</em> models by setting
coefficients to zero) but behaves <strong>unstably</strong> when
features are correlated (it tends to randomly pick one and discard the
others).</li>
</ul>
<p><strong>Elastic Net</strong> combines the L1 penalty of LASSO and the
L2 penalty of Ridge. The result is a single, flexible model that:</p>
<ol type="1">
<li>Performs <strong>variable selection</strong> (like LASSO).</li>
<li>Handles <strong>correlated features</strong> stably by grouping them
together (like Ridge).</li>
<li>Can select more features than samples (<span class="math inline">\(p
&gt; n\)</span>), which LASSO cannot do.</li>
</ol>
<h3 id="slide-1-the-definition-and-formula-file-...020245.png">Slide 1:
The Definition and Formula (File: <code>...020245.png</code>)</h3>
<p>This slide explains <em>why</em> Elastic Net was created and defines
it <em>mathematically</em>.</p>
<ul>
<li><strong>The Problem:</strong> It states the exact trade-off:
<ul>
<li>â€œRidge regression can handle collinearity, but cannot perform
variable selection;â€</li>
<li>â€œLASSO can perform variable selection, but performs poorly when
collinearity;â€</li>
</ul></li>
<li><strong>The Solution (The Formula):</strong> The core of the method
is this optimization formula: <span
class="math display">\[\hat{\beta}_{eNet}(\lambda, \alpha) \leftarrow
\arg \min_{\beta} \left( \underbrace{\|\mathbf{y} -
\mathbf{X}\beta\|^2}_{\text{Loss}} + \lambda \left(
\underbrace{\alpha\|\beta\|_1}_{\text{L1 Penalty}} +
\underbrace{\frac{1-\alpha}{2}\|\beta\|_2^2}_{\text{L2 Penalty}} \right)
\right)\]</span></li>
<li><strong>Breaking Down the Formula:</strong>
<ul>
<li><strong><span class="math inline">\(\|\mathbf{y} -
\mathbf{X}\beta\|^2\)</span></strong>: This is the standard â€œResidual
Sum of Squaresâ€ (RSS). We want to find coefficients (<span
class="math inline">\(\beta\)</span>) that make the modelâ€™s predictions
(<span class="math inline">\(X\beta\)</span>) as close as possible to
the true values (<span class="math inline">\(y\)</span>).</li>
<li><strong><span class="math inline">\(\lambda\)</span>
(Lambda)</strong>: This is the <strong>master knob</strong> for
<em>total regularization strength</em>. A larger <span
class="math inline">\(\lambda\)</span> means a bigger penalty, which
â€œshrinksâ€ all coefficients more.</li>
<li><strong><span class="math inline">\(\alpha\)</span>
(Alpha)</strong>: This is the <strong>mixing parameter</strong> that
balances L1 and L2. This is the key innovation.
<ul>
<li><strong><span
class="math inline">\(\alpha\|\beta\|_1\)</span></strong>: This is the
<strong>L1 (LASSO)</strong> part. It forces weak coefficients to become
exactly zero, thus selecting variables.</li>
<li><strong><span
class="math inline">\(\frac{1-\alpha}{2}\|\beta\|_2^2\)</span></strong>:
This is the <strong>L2 (Ridge)</strong> part. It shrinks all
coefficients and, crucially, encourages correlated features to have
similar coefficients (the grouping effect).</li>
</ul></li>
</ul></li>
<li><strong>The Special Cases:</strong>
<ul>
<li>If <strong><span class="math inline">\(\alpha = 0\)</span></strong>,
the L1 term vanishes, and the model becomes pure <strong>Ridge
Regression</strong>.</li>
<li>If <strong><span class="math inline">\(\alpha = 1\)</span></strong>,
the L2 term vanishes, and the model becomes pure <strong>LASSO
Regression</strong>.</li>
<li>If <strong><span class="math inline">\(0 &lt; \alpha &lt;
1\)</span></strong>, you get <strong>Elastic Net</strong>, which
â€œencourages grouping of correlated variablesâ€ <em>and</em> â€œcan perform
variable selection.â€</li>
</ul></li>
</ul>
<h3
id="slide-2-the-intuition-and-the-grouping-effect-file-...020249.jpg">Slide
2: The Intuition and The Grouping Effect (File:
<code>...020249.jpg</code>)</h3>
<p>This slide gives you the <em>visual intuition</em> and the
<em>practical proof</em> of why Elastic Net works. It has two parts.</p>
<h4 id="part-1-the-three-graphs-geometric-intuition">Part 1: The Three
Graphs (Geometric Intuition)</h4>
<p>These graphs show the <em>constraint region</em> (the shaded shape)
for each penalty. The model tries to find the best coefficients (<span
class="math inline">\(\theta_{opt}\)</span>), and the final solution
(the green dot) is the first point where the cost function (the blue
ellipses) â€œtouchesâ€ the constraint region.</p>
<ul>
<li><strong>L1 Norm (LASSO):</strong> The region is a
<strong>diamond</strong>. Because of its <strong>sharp corners</strong>,
the ellipses are very likely to hit a corner first. At a corner, one of
the coefficients (e.g., <span class="math inline">\(\theta_1\)</span>)
is zero. This is a visual explanation of how LASSO creates
<strong>sparsity</strong> (variable selection).</li>
<li><strong>L2 Norm (Ridge):</strong> The region is a
<strong>circle</strong>. It has <strong>no corners</strong>. The
ellipses will hit a â€œsmoothâ€ point on the circle, shrinking both
coefficients (<span class="math inline">\(\theta_1\)</span> and <span
class="math inline">\(\theta_2\)</span>) but not setting either to zero.
This is <strong>weight sharing</strong>.</li>
<li><strong>L1 + L2 (Elastic Net):</strong> The region is a
<strong>â€œrounded squareâ€</strong>. Itâ€™s the perfect compromise.
<ul>
<li>It has â€œcornersâ€ (like LASSO) so it can still set coefficients to
zero.</li>
<li>It has â€œcurved edgesâ€ (like Ridge) so itâ€™s more stable and handles
correlated variables by finding a solution on an edge rather than a
single sharp corner.</li>
</ul></li>
</ul>
<h4 id="part-2-the-formula-the-grouping-effect">Part 2: The Formula (The
Grouping Effect)</h4>
<p>The text at the bottom explains Elastic Netâ€™s â€œgrouping effect.â€</p>
<ul>
<li><strong>The Implication:</strong> â€œIf <span
class="math inline">\(x_j \approx x_k\)</span>, then <span
class="math inline">\(\hat{\beta}_j \approx
\hat{\beta}_k\)</span>.â€</li>
<li><strong>Meaning:</strong> If two features (<span
class="math inline">\(x_j\)</span> and <span
class="math inline">\(x_k\)</span>) are highly correlated (their values
are very similar), Elastic Net will force their <em>coefficients</em>
(<span class="math inline">\(\hat{\beta}_j\)</span> and <span
class="math inline">\(\hat{\beta}_k\)</span>) to also be very
similar.</li>
<li><strong>Why this is good:</strong> This is the <em>opposite</em> of
LASSO. LASSO would be unstable and might arbitrarily set <span
class="math inline">\(\hat{\beta}_j\)</span> to a large value and <span
class="math inline">\(\hat{\beta}_k\)</span> to zero. Elastic Net
â€œgroupsâ€ them: it will either keep <em>both</em> in the model with
similar importance, or it will shrink <em>both</em> of them out of the
model together. This is a much more stable and realistic result.</li>
<li><strong>The Warning:</strong> â€œLASSO may be unstable in this case!â€
This directly highlights the problem that Elastic Net solves.</li>
</ul>
<h3 id="slide-3-the-feature-comparison-table-file-...020255.png">Slide
3: The Feature Comparison Table (File: <code>...020255.png</code>)</h3>
<p>This table is your â€œcheat sheetâ€ for choosing the right model. It
compares Ridge, LASSO, and Elastic Net on all their key properties.</p>
<ul>
<li><strong>Penalty:</strong> Shows the L2, L1, and combined
penalties.</li>
<li><strong>Sparsity:</strong> Can the model set coefficients to 0?
<ul>
<li>Ridge: <strong>No âŒ</strong></li>
<li>LASSO: <strong>Yes âœ…</strong></li>
<li>Elastic Net: <strong>Yes âœ…</strong></li>
</ul></li>
<li><strong>Variable Selection:</strong> This is a <em>crucial</em> row.
<ul>
<li>LASSO: <strong>Yes âœ…</strong>, BUT it has a major limitation: if
you have more features than samples (<span class="math inline">\(p &gt;
n\)</span>), LASSO can select <em>at most</em> <span
class="math inline">\(n\)</span> features.</li>
<li>Elastic Net: <strong>Yes âœ…</strong>, and it <strong>can select more
than <span class="math inline">\(n\)</span> variables</strong>. This
makes it the clear choice for â€œwideâ€ data problems (e.g., in genomics,
where <span class="math inline">\(p=20,000\)</span> features and <span
class="math inline">\(n=100\)</span> samples).</li>
</ul></li>
<li><strong>Grouping Effect:</strong> How does it handle correlated
features?
<ul>
<li>Ridge: <strong>Strong âœ…</strong></li>
<li>LASSO: <strong>Weak âŒ</strong> (it â€œpicks oneâ€)</li>
<li>Elastic Net: <strong>Strong âœ…</strong></li>
</ul></li>
<li><strong>Solution Uniqueness:</strong> Is the answer stable?
<ul>
<li>Ridge: <strong>Always âœ…</strong></li>
<li>LASSO: <strong>No âŒ</strong> (not if <span
class="math inline">\(X\)</span> is â€œrank-deficient,â€ e.g., <span
class="math inline">\(p &gt; n\)</span> or correlated features)</li>
<li>Elastic Net: <strong>Always âœ…</strong> (as long as <span
class="math inline">\(\alpha &lt; 1\)</span>, the Ridge component
guarantees a unique, stable solution).</li>
</ul></li>
<li><strong>Use Case:</strong> When should you use each?
<ul>
<li><strong>Ridge:</strong> For prediction, especially with
<strong>multicollinearity</strong>.</li>
<li><strong>LASSO:</strong> For <strong>interpretability</strong> and
creating <strong>sparse models</strong> (when you think only a few
features matter).</li>
<li><strong>Elastic Net:</strong> The best all-arounder. Use it for
<strong>correlated predictors</strong>, when <strong><span
class="math inline">\(p \gg n\)</span></strong>, or when you need both
<strong>sparsity + stability</strong>.</li>
</ul></li>
</ul>
<h3 id="code-understanding-python-scikit-learn">Code Understanding
(Python <code>scikit-learn</code>)</h3>
<p>When you use this in Python, be aware of a common confusion in the
parameter names:</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Concept (from your slides)</th>
<th style="text-align: left;"><code>scikit-learn</code> Parameter</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong><span
class="math inline">\(\lambda\)</span></strong> (Lambda)</td>
<td style="text-align: left;"><code>alpha</code></td>
<td style="text-align: left;">The <strong>overall strength</strong> of
regularization.</td>
</tr>
<tr>
<td style="text-align: left;"><strong><span
class="math inline">\(\alpha\)</span></strong> (Alpha)</td>
<td style="text-align: left;"><code>l1_ratio</code></td>
<td style="text-align: left;">The <strong>mixing parameter</strong>
between L1 and L2.</td>
</tr>
</tbody>
</table>
<p><strong>Example:</strong> An <code>l1_ratio</code> of <code>0</code>
is Ridge. An <code>l1_ratio</code> of <code>1</code> is LASSO. An
<code>l1_ratio</code> of <code>0.5</code> is a 50/50 mix.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet, ElasticNetCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Initialize a specific model</span></span><br><span class="line"><span class="comment"># This uses 0.5 for lambda (slide&#x27;s alpha) and 0.1 for lambda (slide&#x27;s lambda)</span></span><br><span class="line">model = ElasticNet(alpha=<span class="number">0.1</span>, l1_ratio=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. A much better way: Find the best parameters automatically</span></span><br><span class="line"><span class="comment"># This will test l1_ratios of 0.1, 0.5, and 0.9</span></span><br><span class="line"><span class="comment"># and automatically find the best &#x27;alpha&#x27; (strength) for each.</span></span><br><span class="line">cv_model = ElasticNetCV(</span><br><span class="line">    l1_ratio=[<span class="number">.1</span>, <span class="number">.5</span>, <span class="number">.9</span>],</span><br><span class="line">    cv=<span class="number">5</span>  <span class="comment"># 5-fold cross-validation</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit the model to your data (X_train, y_train)</span></span><br><span class="line"><span class="comment"># cv_model.fit(X_train, y_train)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. See the best parameters it found</span></span><br><span class="line"><span class="comment"># print(f&quot;Best l1_ratio (slide&#x27;s alpha): &#123;cv_model.l1_ratio_&#125;&quot;)</span></span><br><span class="line"><span class="comment"># print(f&quot;Best alpha (slide&#x27;s lambda): &#123;cv_model.alpha_&#125;&quot;)</span></span><br></pre></td></tr></table></figure>
<h1 id="high-dimensional-data-analysis">11. High-Dimensional Data
Analysis</h1>
<h2 id="the-core-problem-large-p-small-n">The Core Problem: Large <span
class="math inline">\(p\)</span>, Small <span
class="math inline">\(n\)</span></h2>
<p>The slides introduce the challenge of high-dimensional data, which is
defined by having <strong>many more features (predictors) <span
class="math inline">\(p\)</span> than observations (samples) <span
class="math inline">\(n\)</span></strong>. This is often written as
<strong><span class="math inline">\(p \gg n\)</span></strong>.</p>
<ul>
<li><strong>Example:</strong> Predicting blood pressure (the response
<span class="math inline">\(y\)</span>) using millions of genetic
markers (SNPs) as features <span class="math inline">\(X\)</span>, but
only having data from a few hundred patients.</li>
<li><strong>Troubles:</strong>
<ul>
<li><strong>Overfitting:</strong> Models become â€œtoo flexibleâ€ and learn
the noise in the training data, rather than the true underlying
pattern.</li>
<li><strong>Non-Unique Solution:</strong> When <span
class="math inline">\(p &gt; n\)</span>, the standard least squares
linear regression model doesnâ€™t even have a unique solution.</li>
<li><strong>Misleading Metrics:</strong> This leads to a common symptom:
a very small <strong>training error</strong> (or high <span
class="math inline">\(R^2\)</span>) but a very large <strong>test
error</strong>.</li>
</ul></li>
</ul>
<h2 id="most-important-image-the-overfitting-trap-figure-6.23">Most
Important Image: The Overfitting Trap (Figure 6.23)</h2>
<p>Figure 6.23 (from the first uploaded image) is the most critical
visual for understanding the <em>problem</em>. It shows what happens
when you add features (variables) that are <em>completely unrelated</em>
to the outcome.</p>
<ul>
<li><strong>Left Plot (RÂ²):</strong> The <span
class="math inline">\(R^2\)</span> on the training data increases
towards 1. This <em>looks</em> like a perfect fit.</li>
<li><strong>Center Plot (Training MSE):</strong> The Mean Squared Error
on the <em>training</em> data decreases to 0. This also <em>looks</em>
perfect.</li>
<li><strong>Right Plot (Test MSE):</strong> The Mean Squared Error on
the <em>test</em> data (new, unseen data) explodes. This reveals the
model is garbage and has just memorized the training set.</li>
</ul>
<p>âš ï¸ <strong>This is the key takeaway:</strong> In high dimensions,
<span class="math inline">\(R^2\)</span> and training MSE are
<strong>useless</strong> and <strong>misleading</strong> metrics for
model quality.</p>
<h2 id="the-solution-regularization-model-selection">The Solution:
Regularization &amp; Model Selection</h2>
<p>To combat overfitting, we must use <strong>less flexible
models</strong>. The main strategy is <strong>regularization</strong>
(also called shrinkage), which involves adding a penalty term to the
cost function to â€œshrinkâ€ the model coefficients (<span
class="math inline">\(\beta\)</span>).</p>
<h3 id="mathematical-formulas-python-code">Mathematical Formulas &amp;
Python Code ğŸ</h3>
<p>The standard <strong>Least Squares</strong> cost function you try to
minimize is: <span class="math display">\[\text{RSS} = \sum_{i=1}^n
\left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j\right)^2 \quad
\text{or} \quad \|y - X\beta\|^2_2\]</span> This fails when <span
class="math inline">\(p &gt; n\)</span>. The solutions modify this:</p>
<h4 id="a.-ridge-regression-l_2-penalty">A. Ridge Regression (<span
class="math inline">\(L_2\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> Shrinks all coefficients towards zero, but
never <em>to</em> zero. Itâ€™s good when many features are related to the
outcome.</li>
<li><strong>Math Formula:</strong> <span
class="math display">\[\text{Minimize: } \left( \|y - X\beta\|^2_2 +
\lambda \sum_{j=1}^p \beta_j^2 \right)\]</span>
<ul>
<li>The <span class="math inline">\(\lambda \sum_{j=1}^p
\beta_j^2\)</span> is the <strong><span
class="math inline">\(L_2\)</span> penalty</strong>.</li>
<li><span class="math inline">\(\lambda\)</span> (lambda) is a
<em>tuning parameter</em> that controls the penalty strength. A larger
<span class="math inline">\(\lambda\)</span> means more shrinkage.</li>
</ul></li>
<li><strong>Python (Scikit-learn):</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># alpha is the lambda (Î») tuning parameter</span></span><br><span class="line"><span class="comment"># We find the best alpha using cross-validation</span></span><br><span class="line">ridge_model = Ridge(alpha=<span class="number">1.0</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">ridge_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate using test error (e.g., MSE on test set)</span></span><br><span class="line"><span class="comment"># NOT with training R-squared</span></span><br><span class="line">test_score = ridge_model.score(X_test, y_test) </span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="b.-the-lasso-l_1-penalty">B. The Lasso (<span
class="math inline">\(L_1\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> This is a very important method. The <span
class="math inline">\(L_1\)</span> penalty can force coefficients to be
<strong>exactly zero</strong>. This means Lasso performs
<strong>automatic feature selection</strong>, creating a <em>sparse</em>
model.</li>
<li><strong>Math Formula:</strong> <span
class="math display">\[\text{Minimize: } \left( \|y - X\beta\|^2_2 +
\lambda \sum_{j=1}^p |\beta_j| \right)\]</span>
<ul>
<li>The <span class="math inline">\(\lambda \sum_{j=1}^p
|\beta_j|\)</span> is the <strong><span
class="math inline">\(L_1\)</span> penalty</strong>.</li>
<li>Again, <span class="math inline">\(\lambda\)</span> is the tuning
parameter.</li>
</ul></li>
<li><strong>Python (Scikit-learn):</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"></span><br><span class="line"><span class="comment"># alpha is the lambda (Î») tuning parameter</span></span><br><span class="line">lasso_model = Lasso(alpha=<span class="number">0.1</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">lasso_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The model automatically selects features</span></span><br><span class="line"><span class="comment"># Coefficients that are zero were &#x27;dropped&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(lasso_model.coef_) </span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="c.-other-methods">C. Other Methods</h4>
<p>The slides also mention:</p>
<ul>
<li><strong>Forward Stepwise Selection:</strong> A different approach
where you start with no features and add them one by one, picking the
one that improves the model most (based on a criterion like
cross-validation error).</li>
<li><strong>Principal Components Regression (PCR):</strong> A
dimensionality reduction technique.</li>
</ul>
<h2 id="the-curse-of-dimensionality-figure-6.24">The Curse of
Dimensionality (Figure 6.24)</h2>
<p>This example (Figures 6.24 and its description) shows a more subtle
problem.</p>
<ul>
<li><strong>Setup:</strong> A model with <span
class="math inline">\(n=100\)</span> observations and 20 <em>true</em>
features.</li>
<li><strong>Plots:</strong> They test Lasso by adding more and more
<em>irrelevant</em> features:
<ul>
<li><strong><span class="math inline">\(p=20\)</span> (Left):</strong>
Lasso performs well. The lowest test MSE is found with minimal
regularization.</li>
<li><strong><span class="math inline">\(p=50\)</span> (Center):</strong>
Lasso still works well, but it needs more regularization (a smaller
â€œDegrees of Freedomâ€) to filter out the 30 junk features.</li>
<li><strong><span class="math inline">\(p=2000\)</span>
(Right):</strong> This is the <strong>curse of dimensionality</strong>.
Even with a good method like Lasso, the 1,980 irrelevant features add so
much noise that the model <strong>performs poorly regardless</strong> of
the tuning parameter. The true signal is â€œlost in the noise.â€</li>
</ul></li>
</ul>
<h2 id="summary-cautions-for-p-n">Summary: Cautions for <span
class="math inline">\(p &gt; n\)</span></h2>
<p>The final slide gives the most important rules to follow:</p>
<ol type="1">
<li><strong>Beware Extreme Multicollinearity:</strong> When <span
class="math inline">\(p &gt; n\)</span>, your features are
mathematically guaranteed to be linearly related, which breaks standard
regression.</li>
<li><strong>Donâ€™t Overstate Results:</strong> A model you find (e.g.,
with Lasso) is just <em>one</em> of many potentially good models.</li>
<li><strong>ğŸš« DO NOT USE</strong> training <span
class="math inline">\(R^2\)</span>, <span
class="math inline">\(p\)</span>-values, or training MSE to justify your
model. As Figure 6.23 showed, they are misleading.</li>
<li><strong>âœ… DO USE</strong> <strong>test error</strong> and
<strong>cross-validation error</strong> to choose your model and assess
its performance.</li>
</ol>
<h2 id="the-core-problem-p-gg-n-the-troubles-slide">The Core Problem:
<span class="math inline">\(p \gg n\)</span> (The â€œTroublesâ€ Slide)</h2>
<p>This slide (filename: <code>...020259.png</code>) sets up the entire
problem. The issue isnâ€™t just â€œoverfittingâ€; itâ€™s a fundamental
mathematical breakdown of standard methods.</p>
<ul>
<li><strong>â€œLarge <span class="math inline">\(p\)</span> makes our
linear regression model too flexibleâ€</strong>: This is an
understatement. It leads to a problem called an <strong>underdetermined
system</strong>.</li>
<li><strong>â€œIf <span class="math inline">\(p &gt; n\)</span>, the LSE
is not even uniquely determinedâ€</strong>: This is the most important
technical point.
<ul>
<li><strong>Mathematical Reason:</strong> The standard solution for
Ordinary Least Squares (OLS) is <span class="math inline">\(\hat{\beta}
= (X^T X)^{-1} X^T y\)</span>.</li>
<li><span class="math inline">\(X\)</span> is the data matrix with <span
class="math inline">\(n\)</span> rows (observations) and <span
class="math inline">\(p\)</span> columns (features).</li>
<li>The matrix <span class="math inline">\(X^T X\)</span> has dimensions
<span class="math inline">\(p \times p\)</span>.</li>
<li>When <span class="math inline">\(p &gt; n\)</span>, the <span
class="math inline">\(X^T X\)</span> matrix is
<strong>singular</strong>, which means its determinant is zero and it
<strong>cannot be inverted</strong>. The <span
class="math inline">\((X^T X)^{-1}\)</span> term does not exist.</li>
<li><strong>â€œExtreme multicollinearityâ€</strong> (from slide
<code>...020744.png</code>) is the direct cause. When <span
class="math inline">\(p &gt; n\)</span>, the columns of <span
class="math inline">\(X\)</span> (the features) are <em>guaranteed</em>
to be linearly dependent. There are infinite combinations of the
features that can explain the data.</li>
</ul></li>
</ul>
<h2 id="the-simplest-example-n2-figure-6.22">The Simplest Example: <span
class="math inline">\(n=2\)</span> (Figure 6.22)</h2>
<p>This slide (filename: <code>...020728.png</code>) is the
<em>perfect</em> illustration of the â€œnot uniquely determinedâ€
problem.</p>
<ul>
<li><strong>Left Plot (Low-D):</strong> Many points (<span
class="math inline">\(n\)</span>), only two parameters (<span
class="math inline">\(p=2\)</span>: intercept <span
class="math inline">\(\beta_0\)</span> and slope <span
class="math inline">\(\beta_1\)</span>). The line is a â€œbest fitâ€ that
balances the errors. The training error (RSS) is non-zero.</li>
<li><strong>Right Plot (High-D):</strong> We have <span
class="math inline">\(n=2\)</span> observations and <span
class="math inline">\(p=2\)</span> parameters.
<ul>
<li>You have two equations (one for each point) and two unknowns (<span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>).</li>
<li>The model has <em>exactly</em> enough flexibility to pass
<em>perfectly</em> through both points.</li>
<li>The result is <strong>zero training error</strong>.</li>
<li>This â€œperfectâ€ fit is an illusion. If you got a <em>new</em> data
point, this line would almost certainly be a terrible predictor. This is
the essence of overfitting.</li>
</ul></li>
</ul>
<h2 id="the-consequence-misleading-metrics-figure-6.23">The Consequence:
Misleading Metrics (Figure 6.23)</h2>
<p>This slide (filename: <code>...020730.png</code>) scales up the
problem from <span class="math inline">\(n=2\)</span> to <span
class="math inline">\(n=20\)</span> and shows <em>why</em> you must be
cautious.</p>
<ul>
<li><strong>The Setup:</strong> <span
class="math inline">\(n=20\)</span> observations. We start with 1
feature and add more and more <em>irrelevant, junk</em> features.</li>
<li><strong>Left Plot (<span
class="math inline">\(R^2\)</span>):</strong> The <span
class="math inline">\(R^2\)</span> on the training data steadily
increases towards 1 as we add features. This is because, by pure chance,
each new junk feature can explain a tiny bit more of the noise in the
training set.</li>
<li><strong>Center Plot (Training MSE):</strong> The training error
drops to 0. This is the same as the <span
class="math inline">\(n=2\)</span> plot. Once the number of features
(<span class="math inline">\(p\)</span>) gets close to the number of
observations (<span class="math inline">\(n=20\)</span>), the model can
perfectly fit the 20 data points, even if the features are random
noise.</li>
<li><strong>Right Plot (Test MSE):</strong> This is the â€œtruth.â€ The
<em>actual</em> error on new, unseen data gets worse and worse. By
adding noise features, we are just â€œmemorizingâ€ the training set, and
our modelâ€™s ability to generalize is destroyed.</li>
<li><strong>Key Lesson:</strong> (from slide <code>...020744.png</code>)
This is why you must <strong>â€œAvoid usingâ€¦ <span
class="math inline">\(p\)</span>-values, <span
class="math inline">\(R^2\)</span>, or other traditional measures of
model on training as evidence of good fit.â€</strong> They are guaranteed
to lie to you when <span class="math inline">\(p &gt; n\)</span>.</li>
</ul>
<h2 id="the-solutions-the-deal-with-slide">The Solutions (The â€œDeal
withâ€¦â€ Slide)</h2>
<p>This slide (filename: <code>...020734.png</code>) lists the
strategies to fix this. The core idea is <strong>regularization</strong>
(or shrinkage). We add a â€œpenaltyâ€ to the cost function to stop the
<span class="math inline">\(\beta\)</span> coefficients from getting too
large or too numerous.</p>
<h4 id="a.-ridge-regression-l_2-penalty-1">A. Ridge Regression (<span
class="math inline">\(L_2\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> Keeps all <span
class="math inline">\(p\)</span> features, but shrinks their
coefficients. Itâ€™s excellent for handling multicollinearity.</li>
<li><strong>Math:</strong> <span class="math inline">\(\text{Minimize: }
\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j\right)^2 +
\lambda \sum_{j=1}^p \beta_j^2\)</span>
<ul>
<li>The first part is the standard RSS.</li>
<li>The <span class="math inline">\(\lambda \sum \beta_j^2\)</span> is
the <strong><span class="math inline">\(L_2\)</span> penalty</strong>.
It punishes large coefficient values.</li>
</ul></li>
<li><strong><span class="math inline">\(\lambda\)</span>
(Lambda):</strong> This is the <strong>tuning parameter</strong>.
<ul>
<li>If <span class="math inline">\(\lambda=0\)</span>, itâ€™s just OLS
(which fails).</li>
<li>If <span class="math inline">\(\lambda \to \infty\)</span>, all
<span class="math inline">\(\beta\)</span>â€™s are shrunk to 0.</li>
<li>The right <span class="math inline">\(\lambda\)</span> is chosen via
<strong>cross-validation</strong>.</li>
</ul></li>
</ul>
<h4 id="b.-the-lasso-l_1-penalty-1">B. The Lasso (<span
class="math inline">\(L_1\)</span> Penalty)</h4>
<ul>
<li><strong>Concept:</strong> This is often preferred because it
performs <strong>automatic feature selection</strong>. It shrinks many
coefficients to be <strong>exactly zero</strong>.</li>
<li><strong>Math:</strong> <span class="math inline">\(\text{Minimize: }
\sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j\right)^2 +
\lambda \sum_{j=1}^p |\beta_j|\)</span>
<ul>
<li>The <span class="math inline">\(\lambda \sum |\beta_j|\)</span> is
the <strong><span class="math inline">\(L_1\)</span> penalty</strong>.
This absolute value penalty is what allows coefficients to become
exactly 0.</li>
</ul></li>
<li><strong>Benefit:</strong> The final model is <em>sparse</em> (e.g.,
it might say â€œout of 2,000 features, only these 15 matterâ€).</li>
</ul>
<h4 id="c.-tuning-parameter-choice-the-real-work">C. Tuning Parameter
Choice (The <em>Real</em> Work)</h4>
<p>How do you pick the best <span
class="math inline">\(\lambda\)</span>? You must use the data you have.
The slides mention this and â€œcross validation errorâ€ (from
<code>...020744.png</code>).</p>
<ul>
<li><strong>Python Code (Scikit-learn):</strong> You donâ€™t just guess
<code>alpha</code> (which is <span
class="math inline">\(\lambda\)</span> in scikit-learn). You use a tool
like <code>LassoCV</code> or <code>GridSearchCV</code> to find the best
one. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a high-dimensional dataset</span></span><br><span class="line">X, y = make_regression(n_samples=<span class="number">100</span>, n_features=<span class="number">500</span>, n_informative=<span class="number">10</span>, noise=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># LassoCV automatically performs cross-validation to find the best alpha (lambda)</span></span><br><span class="line"><span class="comment"># cv=10 means 10-fold cross-validation</span></span><br><span class="line">lasso_cv_model = LassoCV(cv=<span class="number">10</span>, random_state=<span class="number">0</span>, max_iter=<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the model</span></span><br><span class="line">lasso_cv_model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is the best lambda (alpha) it found:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best alpha (lambda): <span class="subst">&#123;lasso_cv_model.alpha_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can now see the coefficients</span></span><br><span class="line"><span class="comment"># Most of the 500 coefficients will be 0.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of non-zero features: <span class="subst">&#123;np.<span class="built_in">sum</span>(lasso_cv_model.coef_ != <span class="number">0</span>)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="a-final-warning-the-curse-of-dimensionality-figure-6.24">A Final
Warning: The Curse of Dimensionality (Figure 6.24)</h2>
<p>This final set of slides (filenames: <code>...020738.png</code> and
<code>...020741.jpg</code>) provides a crucial, subtle warning:
<strong>Regularization is not magic.</strong></p>
<ul>
<li><strong>The Setup:</strong> <span
class="math inline">\(n=100\)</span> observations. There are <strong>20
real features</strong> that truly affect the response.</li>
<li><strong>The Experiment:</strong> They run Lasso three times, adding
more and more <em>noise</em> features:
<ul>
<li><strong>Left Plot (<span
class="math inline">\(p=20\)</span>):</strong> All 20 features are real.
The lowest test MSE is found with minimal regularization (high â€œDegrees
of Freedom,â€ meaning many non-zero coefficients). This makes sense; you
want to keep all 20 real features.</li>
<li><strong>Center Plot (<span
class="math inline">\(p=50\)</span>):</strong> Now we have 20 real
features + 30 noise features. Lasso still works! The best model is found
with more regularization (fewer â€œDegrees of Freedomâ€). Lasso
successfully â€œzeroed outâ€ many of the 30 noise features.</li>
<li><strong>Right Plot (<span
class="math inline">\(p=2000\)</span>):</strong> This is the
<strong>curse of dimensionality</strong>. We have 20 real features +
1980 noise features. The <em>noise</em> has completely overwhelmed the
<em>signal</em>. <strong>Lasso fails.</strong> The test MSE is high
<em>no matter what</em> tuning parameter you choose. The model cannot
distinguish the 20 real features from the 1980 junk ones.</li>
</ul></li>
</ul>
<p><strong>Final Takeaway:</strong> Even with advanced methods like
Lasso, if your <span class="math inline">\(p \gg n\)</span> problem is
<em>too</em> extreme (i.S. the signal-to-noise ratio is too low), it may
be impossible to build a good predictive model.</p>
<h2 id="the-goal-collaborative-filtering">The Goal: â€œCollaborative
Filteringâ€</h2>
<p>The first slide (<code>...021218.png</code>) uses the term
<strong>Collaborative Filtering</strong>. This is the key concept. The
model â€œcollaboratesâ€ by using the ratings of <em>all</em> users to fill
in the blanks for a <em>single</em> user.</p>
<ul>
<li><strong>How it works:</strong> The model assumes your â€œtasteâ€
(vector <span class="math inline">\(\mathbf{u}_i\)</span>) can be
described as a combination of <span class="math inline">\(r\)</span>
â€œlatent featuresâ€ (e.g., <span class="math inline">\(r=3\)</span>: %
action, % comedy, % drama). It <em>also</em> assumes each movie (vector
<span class="math inline">\(\mathbf{v}_j\)</span>) has a profile on
these same features.</li>
<li>Your predicted rating for a movie is the dot product of your taste
vector and the movieâ€™s feature vector.</li>
<li>The model finds the best â€œtasteâ€ vectors <span
class="math inline">\(\mathbf{U}\)</span> and â€œmovieâ€ vectors <span
class="math inline">\(\mathbf{V}\)</span> that explain all the known
ratings <em>simultaneously</em>. Itâ€™s collaborative because Leeâ€™s
ratings help define the features of â€œBullet Trainâ€ (<span
class="math inline">\(\mathbf{v}_2\)</span>), which in turn helps
predict Yangâ€™s rating for that same movie.</li>
</ul>
<h2 id="the-hard-problem-and-its-2-flavors">The Hard Problem (and its 2
Flavors)</h2>
<p>The second slide (<code>...021222.png</code>) presents the intuitive,
but computationally <em>very</em> hard, way to frame the problem.</p>
<h4 id="detail-1-noise-vs.-no-noise">Detail 1: Noise vs.Â No Noise</h4>
<p>The slide shows <span class="math inline">\(\mathbf{Y} = \mathbf{M} +
\mathbf{E}\)</span>. This is critical. * <span
class="math inline">\(\mathbf{M}\)</span> is the â€œtrue,â€ â€œclean,â€
underlying low-rank matrix of everyoneâ€™s â€œtrueâ€ preferences. * <span
class="math inline">\(\mathbf{E}\)</span> is a matrix of random noise.
(e.g., your true rating is 4.3, but you entered a 4; or you were in a
bad mood and rated a 3). * <span
class="math inline">\(\mathbf{Y}\)</span> is the <em>noisy data</em> we
actually observe.</p>
<p>Because of this noise, we donâ€™t expect to find a matrix <span
class="math inline">\(\mathbf{N}\)</span> that <em>perfectly</em>
matches our data. Instead, we try to find a low-rank <span
class="math inline">\(\mathbf{N}\)</span> that is <em>as close as
possible</em>. This leads to the formula: <span
class="math display">\[\underset{\text{rank}(\mathbf{N}) \le
r}{\text{minimize}} \quad \left\| \mathcal{P}_{\mathcal{O}}(\mathbf{Y} -
\mathbf{N}) \right\|_{\text{F}}^2\]</span> This says: â€œFind a matrix
<span class="math inline">\(\mathbf{N}\)</span> (of rank <span
class="math inline">\(r\)</span> or less) that minimizes the sum of
squared errors <em>only on the ratings we observed</em> (<span
class="math inline">\(\mathcal{O}\)</span>).â€</p>
<h4
id="detail-2-why-is-textrankmathbfn-le-r-a-non-convex-constraint">Detail
2: Why is <span class="math inline">\(\text{rank}(\mathbf{N}) \le
r\)</span> a â€œNon-convex constraintâ€?</h4>
<p>This is the â€œdifficult to optimizeâ€ part. A convex problem is
(simplistically) one with a single valley, making it easy to find the
single lowest point. A non-convex problem has many local valleys, and an
algorithm can get stuck in a â€œpretty goodâ€ valley instead of the â€œbestâ€
one.</p>
<p>The rank constraint is non-convex. For example, the average of two
rank-1 matrices is <em>not</em> necessarily a rank-1 matrix (it could be
rank-2). This lack of a â€œsmooth valleyâ€ property makes the problem
NP-hard.</p>
<h4 id="detail-3-the-number-of-parameters-rd_1-d_2">Detail 3: The Number
of Parameters: <span class="math inline">\(r(d_1 + d_2)\)</span></h4>
<p>The slide asks, â€œhow many entries are needed?â€ The answer is based on
the number of unknown parameters. * A rank-<span
class="math inline">\(r\)</span> matrix <span
class="math inline">\(\mathbf{M}\)</span> can be factored into <span
class="math inline">\(\mathbf{U}\)</span> (which is <span
class="math inline">\(d_1 \times r\)</span>) and <span
class="math inline">\(\mathbf{V}^T\)</span> (which is <span
class="math inline">\(r \times d_2\)</span>). * The number of entries in
<span class="math inline">\(\mathbf{U}\)</span> is <span
class="math inline">\(d_1 \times r\)</span>. * The number of entries in
<span class="math inline">\(\mathbf{V}\)</span> is <span
class="math inline">\(d_2 \times r\)</span>. * Total â€œunknownsâ€ to solve
for: <span class="math inline">\(d_1 r + d_2 r = r(d_1 + d_2)\)</span>.
* This means we must have <em>at least</em> <span
class="math inline">\(r(d_1 + d_2)\)</span> observed ratings to have any
hope of uniquely solving for <span
class="math inline">\(\mathbf{U}\)</span> and <span
class="math inline">\(\mathbf{V}\)</span>. If our number of observations
<span class="math inline">\(|\mathcal{O}|\)</span> is less than this,
the problem is hopelessly underdetermined.</p>
<h2 id="the-magic-solution-convex-relaxation">The â€œMagicâ€ Solution:
Convex Relaxation</h2>
<p>The final slide (<code>...021225.png</code>) presents the
groundbreaking solution from CandÃ¨s and Recht. This solution cleverly
<em>changes the problem</em> to one that is convex and solvable.</p>
<h4
id="detail-1-the-l1-norm-analogy-this-is-the-most-important-concept">Detail
1: The L1-Norm Analogy (This is the most important concept)</h4>
<p>This is the key to understanding <em>why</em> this works.</p>
<ul>
<li><strong>In Vectors (Lasso):</strong>
<ul>
<li><strong>Hard Problem:</strong> Find the <em>sparsest</em> vector
<span class="math inline">\(\beta\)</span> (fewest non-zeros). This is
<span class="math inline">\(L_0\)</span> norm, <span
class="math inline">\(\text{minimize } \|\beta\|_0\)</span>. This is
non-convex.</li>
<li><strong>Easy Problem:</strong> Minimize the <span
class="math inline">\(L_1\)</span> norm, <span
class="math inline">\(\text{minimize } \|\beta\|_1 = \sum
|\beta_j|\)</span>. This is convex, and itâ€™s a â€œrelaxationâ€ that
<em>also</em> produces sparse solutions.</li>
</ul></li>
<li><strong>In Matrices (Matrix Completion):</strong>
<ul>
<li><strong>Hard Problem:</strong> Find the <em>lowest-rank</em> matrix
<span class="math inline">\(\mathbf{X}\)</span>. Rank is the number of
non-zero singular values. This is <span
class="math inline">\(\text{minimize } \text{rank}(\mathbf{X})\)</span>.
This is non-convex.</li>
<li><strong>Easy Problem:</strong> Minimize the <strong>Nuclear
Norm</strong>, <span class="math inline">\(\text{minimize }
\|\mathbf{X}\|_* = \sum \sigma_i(\mathbf{X})\)</span> (where <span
class="math inline">\(\sigma_i\)</span> are the singular values). This
is convex, and itâ€™s the â€œmatrix equivalentâ€ of the <span
class="math inline">\(L_1\)</span> norm. Itâ€™s a relaxation that
<em>also</em> produces low-rank solutions.</li>
</ul></li>
</ul>
<h4 id="detail-2-noiseless-vs.-noisy-again">Detail 2: Noiseless
vs.Â Noisy (Again)</h4>
<p>Notice the <em>constraint</em> in this new problem: <span
class="math display">\[\text{Minimize } \quad \|\mathbf{X}\|_*\]</span>
<span class="math display">\[\text{Subject to } \quad X_{ij} = M_{ij},
\quad (i, j) \in \mathcal{O}\]</span></p>
<p>This formulation is for the <strong>noiseless</strong> case. It
assumes the <span class="math inline">\(M_{ij}\)</span> we observed are
<em>perfectly accurate</em>. It demands that our solution <span
class="math inline">\(\mathbf{X}\)</span> <em>exactly matches</em> the
known ratings. This is different from the optimization problem on the
previous slide, which just tried to get <em>close</em> to the noisy data
<span class="math inline">\(\mathbf{Y}\)</span>.</p>
<p>(In practice, you solve a noisy-aware version that combines both
ideas, but the slide shows the original, â€œexact completionâ€
problem.)</p>
<h4 id="detail-3-the-guarantee-what-the-math-at-the-bottom-means">Detail
3: The Guarantee (What the math at the bottom means)</h4>
<p><span class="math display">\[\text{If } \mathcal{O} \text{ is
randomly sampled and } |\mathcal{O}| \gg r(d_1+d_2)\log(d_1+d_2),
\text{... then the solution is unique and } \mathbf{M}
\text{...}\]</span></p>
<p>This is the punchline. The CandÃ¨s paper <em>proved</em> that if you
have <em>enough</em> (but still very few) <em>randomly</em> sampled
ratings, solving this easy convex problem (minimizing the nuclear norm)
will <em>magically give you the exact, true, low-rank matrix <span
class="math inline">\(\mathbf{M}\)</span></em>.</p>
<ul>
<li><strong><span class="math inline">\(|\mathcal{O}| \gg
r(d_1+d_2)\)</span></strong>: This part makes sense. We need <em>at
least</em> as many observations as our <span
class="math inline">\(r(d_1+d_2)\)</span> degrees of freedom.</li>
<li><strong><span class="math inline">\(\log(d_1+d_2)\)</span></strong>:
This â€œlogâ€ factor is the â€œpriceâ€ we pay for not knowing <em>where</em>
the information is. Itâ€™s an astonishingly small price.</li>
<li><strong>Example:</strong> For a 1,000,000 user x 10,000 movie matrix
(like Netflix) with <span class="math inline">\(r=10\)</span>, you donâ€™t
need <span class="math inline">\(\approx 10^{10}\)</span> ratings. You
need a number closer to <span class="math inline">\(10 \times (10^6 +
10^4) \times \log(\dots)\)</span>, which is <em>dramatically</em>
smaller. This is why this method is practical.</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/10/06/5054C5/" rel="prev" title="MSDM 5054 - Statistical Machine Learning-L5">
      <i class="fa fa-chevron-left"></i> MSDM 5054 - Statistical Machine Learning-L5
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/10/22/5120c7-1/" rel="next" title="PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W7-1">
      PHYS 5120 - Computational Energy Materials and Electronic Structure Simulations-W7-1 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#linear-model-selection-and-regularization-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.</span> <span class="nav-text">1.
Linear Model Selection and Regularization çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#summary-of-core-concepts"><span class="nav-number">1.1.</span> <span class="nav-text">Summary of Core Concepts</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mathematical-understanding-key-questions-%E6%95%B0%E5%AD%A6%E7%90%86%E8%A7%A3%E4%B8%8E%E5%85%B3%E9%94%AE%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.</span> <span class="nav-text">Mathematical
Understanding &amp; Key Questions æ•°å­¦ç†è§£ä¸å…³é”®é—®é¢˜</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#how-to-compare-which-model-is-better"><span class="nav-number">1.2.1.</span> <span class="nav-text">How to compare which model
is better?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-use-r2-in-step-2"><span class="nav-number">1.2.2.</span> <span class="nav-text">Why use \(R^2\) in Step 2?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-cant-we-use-training-error-in-step-3"><span class="nav-number">1.2.3.</span> <span class="nav-text">Why canâ€™t we use
training error in Step 3?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-analysis"><span class="nav-number">1.3.</span> <span class="nav-text">Code Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#key-functions"><span class="nav-number">1.3.1.</span> <span class="nav-text">Key Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#analysis-of-the-output"><span class="nav-number">1.3.2.</span> <span class="nav-text">Analysis of the Output</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#conceptual-overview-the-why"><span class="nav-number">1.4.</span> <span class="nav-text">Conceptual Overview: The â€œWhyâ€</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#questions"><span class="nav-number">1.5.</span> <span class="nav-text">Questions ğŸ¯</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#q1-how-to-compare-which-model-is-better"><span class="nav-number">1.5.1.</span> <span class="nav-text">Q1: â€œHow to compare
which model is better?â€</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q2-why-using-r2-for-step-2"><span class="nav-number">1.5.2.</span> <span class="nav-text">Q2: â€œWhy using \(R^2\) for step 2?â€</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q3-cannot-use-training-error-in-step-3.-why-not-%E6%AD%A5%E9%AA%A4-3-%E4%B8%AD%E4%B8%8D%E8%83%BD%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE-%E4%B8%BA%E4%BB%80%E4%B9%88"><span class="nav-number">1.5.3.</span> <span class="nav-text">Q3:
â€œCannot use training error in Step 3.â€ Why not? â€œæ­¥éª¤ 3
ä¸­ä¸èƒ½ä½¿ç”¨è®­ç»ƒè¯¯å·®ã€‚â€ ä¸ºä»€ä¹ˆï¼Ÿ</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mathematical-deep-dive"><span class="nav-number">1.6.</span> <span class="nav-text">Mathematical Deep Dive ğŸ§®</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#detailed-code-analysis"><span class="nav-number">1.7.</span> <span class="nav-text">Detailed Code Analysis ğŸ’»</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#key-functions-1"><span class="nav-number">1.7.1.</span> <span class="nav-text">Key Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary-of-outputs-slides-...221255.png-...221309.png"><span class="nav-number">1.7.2.</span> <span class="nav-text">Summary
of Outputs (Slides ...221255.png &amp;
...221309.png)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#the-core-problem-training-error-vs.-test-error-%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE-vs.-%E6%B5%8B%E8%AF%95%E8%AF%AF%E5%B7%AE"><span class="nav-number">2.</span> <span class="nav-text">2.
The Core Problem: Training Error vs.Â Test Error æ ¸å¿ƒé—®é¢˜ï¼šè®­ç»ƒè¯¯å·®
vs.Â æµ‹è¯•è¯¯å·®</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#basic-metrics-measures-of-fit"><span class="nav-number">2.1.</span> <span class="nav-text">Basic Metrics (Measures of
Fit)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#residue-error-%E6%AE%8B%E5%B7%AE%E8%AF%AF%E5%B7%AE"><span class="nav-number">2.1.1.</span> <span class="nav-text">Residue (Error) æ®‹å·®ï¼ˆè¯¯å·®ï¼‰</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#residual-sum-of-squares-rss-%E6%AE%8B%E5%B7%AE%E5%B9%B3%E6%96%B9%E5%92%8C-rss"><span class="nav-number">2.1.2.</span> <span class="nav-text">Residual Sum of
Squares (RSS) æ®‹å·®å¹³æ–¹å’Œ (RSS)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#r-squared-r2"><span class="nav-number">2.1.3.</span> <span class="nav-text">R-squared (\(R^2\))</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#advanced-metrics-for-model-selection-%E9%AB%98%E7%BA%A7%E6%8C%87%E6%A0%87%E7%94%A8%E4%BA%8E%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="nav-number">2.2.</span> <span class="nav-text">Advanced
Metrics (For Model Selection) é«˜çº§æŒ‡æ ‡ï¼ˆç”¨äºæ¨¡å‹é€‰æ‹©ï¼‰</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#adjusted-r2"><span class="nav-number">2.2.1.</span> <span class="nav-text">Adjusted \(R^2\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#akaike-information-criterion-aic"><span class="nav-number">2.2.2.</span> <span class="nav-text">Akaike Information Criterion
(AIC)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bayesian-information-criterion-bic"><span class="nav-number">2.2.3.</span> <span class="nav-text">Bayesian Information
Criterion (BIC)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-deeper-theory-why-aic-works"><span class="nav-number">2.3.</span> <span class="nav-text">The Deeper Theory: Why AIC
Works</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aicbic-for-linear-regression"><span class="nav-number">2.4.</span> <span class="nav-text">AIC&#x2F;BIC for Linear Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-core-problem-training-error-vs.-test-error"><span class="nav-number">2.5.</span> <span class="nav-text">The Core
Problem: Training Error vs.Â Test Error</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#basic-metrics-measures-of-fit-1"><span class="nav-number">2.6.</span> <span class="nav-text">Basic Metrics (Measures of
Fit)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#residue-error"><span class="nav-number">2.6.1.</span> <span class="nav-text">Residue (Error)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#residual-sum-of-squares-rss"><span class="nav-number">2.6.2.</span> <span class="nav-text">Residual Sum of Squares (RSS)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#r-squared-r2-1"><span class="nav-number">2.6.3.</span> <span class="nav-text">R-squared (\(R^2\))</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#advanced-metrics-for-model-selection"><span class="nav-number">2.7.</span> <span class="nav-text">Advanced Metrics (For
Model Selection)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#adjusted-r2-1"><span class="nav-number">2.7.1.</span> <span class="nav-text">Adjusted \(R^2\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#akaike-information-criterion-aic-1"><span class="nav-number">2.7.2.</span> <span class="nav-text">Akaike Information Criterion
(AIC)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bayesian-information-criterion-bic-1"><span class="nav-number">2.7.3.</span> <span class="nav-text">Bayesian Information
Criterion (BIC)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-deeper-theory-why-aic-works-1"><span class="nav-number">2.8.</span> <span class="nav-text">The Deeper Theory: Why AIC
Works</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aicbic-for-linear-regression-1"><span class="nav-number">2.9.</span> <span class="nav-text">AIC&#x2F;BIC for Linear
Regression</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#variable-selection"><span class="nav-number">3.</span> <span class="nav-text">3. Variable Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#core-concept-the-problem-of-variable-selection"><span class="nav-number">3.1.</span> <span class="nav-text">Core Concept:
The Problem of Variable Selection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#method-1-best-subset-selection-bss"><span class="nav-number">3.2.</span> <span class="nav-text">Method 1: Best Subset
Selection (BSS)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#conceptual-algorithm"><span class="nav-number">3.2.1.</span> <span class="nav-text">Conceptual Algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mathematical-computational-cost-from-slide-225641.png"><span class="nav-number">3.2.2.</span> <span class="nav-text">Mathematical
&amp; Computational Cost (from slide 225641.png)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#method-2-forward-stepwise-selection-fss"><span class="nav-number">3.3.</span> <span class="nav-text">Method 2: Forward
Stepwise Selection (FSS)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#conceptual-algorithm-from-slides-225645.png-225648.png"><span class="nav-number">3.3.1.</span> <span class="nav-text">Conceptual
Algorithm (from slides 225645.png &amp;
225648.png)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mathematical-computational-cost-from-slide-225651.png"><span class="nav-number">3.3.2.</span> <span class="nav-text">Mathematical
&amp; Computational Cost (from slide 225651.png)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#how-to-choose-the-best-model-the-criteria"><span class="nav-number">3.4.</span> <span class="nav-text">4. How to Choose the
â€œBestâ€ Model: The Criteria</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python-code-analysis-slide-225546.jpg"><span class="nav-number">3.5.</span> <span class="nav-text">5. Python Code Analysis
(Slide 225546.jpg)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#block-1-load-the-credit-dataset"><span class="nav-number">3.5.1.</span> <span class="nav-text">Block 1: Load the Credit
dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#block-2-plot-scatterplot-matrix"><span class="nav-number">3.5.2.</span> <span class="nav-text">Block 2: Plot scatterplot
matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#block-3-best-subset-selection"><span class="nav-number">3.5.3.</span> <span class="nav-text">Block 3: Best Subset
Selection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#synthesizing-the-results-the-plots"><span class="nav-number">3.6.</span> <span class="nav-text">Synthesizing the Results
(The Plots)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#subset-selection"><span class="nav-number">4.</span> <span class="nav-text">4. Subset Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#summary-of-subset-selection"><span class="nav-number">4.1.</span> <span class="nav-text">Summary of Subset Selection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#stepwise-selection-algorithms"><span class="nav-number">4.2.</span> <span class="nav-text">Stepwise Selection
Algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#forward-stepwise-selection"><span class="nav-number">4.2.1.</span> <span class="nav-text">Forward Stepwise Selection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#backward-stepwise-selection"><span class="nav-number">4.2.2.</span> <span class="nav-text">Backward Stepwise Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#pros-and-cons-backward-selection"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">Pros and Cons (Backward
Selection)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#choosing-the-final-best-model"><span class="nav-number">4.3.</span> <span class="nav-text">Choosing the Final Best
Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#a.-direct-error-estimation"><span class="nav-number">4.3.1.</span> <span class="nav-text">A. Direct Error Estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#b.-adjusted-metrics-penalizing-for-complexity"><span class="nav-number">4.3.2.</span> <span class="nav-text">B. Adjusted
Metrics (Penalizing for Complexity)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python-code-understanding"><span class="nav-number">4.4.</span> <span class="nav-text">Python Code Understanding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#important-images"><span class="nav-number">4.5.</span> <span class="nav-text">Important Images</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#comparing-selection-algorithms-the-path"><span class="nav-number">4.6.</span> <span class="nav-text">Comparing Selection
Algorithms (The Path)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#choosing-the-single-best-model-the-destination"><span class="nav-number">4.7.</span> <span class="nav-text">Choosing the
Single Best Model (The Destination)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#final-summary-of-results"><span class="nav-number">4.8.</span> <span class="nav-text">Final Summary of Results</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#two-main-strategies-for-controlling-model-complexity-in-linear-regression"><span class="nav-number">5.</span> <span class="nav-text">5.
Two main strategies for controlling model complexity in linear
regression</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#subset-selection-1"><span class="nav-number">5.1.</span> <span class="nav-text">Subset Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#key-concepts-formulas"><span class="nav-number">5.1.1.</span> <span class="nav-text">Key Concepts &amp; Formulas</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#practical-application-finding-the-best-model-size"><span class="nav-number">5.1.2.</span> <span class="nav-text">Practical
Application: Finding the Best Model Size</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code-interpretation-r-vs.-python"><span class="nav-number">5.1.3.</span> <span class="nav-text">Code Interpretation (R
vs.Â Python)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shrinkage-methods-regularization"><span class="nav-number">5.2.</span> <span class="nav-text">Shrinkage Methods
(Regularization)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ridge-regression"><span class="nav-number">5.2.1.</span> <span class="nav-text">Ridge Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#an-essential-step-standardization"><span class="nav-number">5.2.2.</span> <span class="nav-text">An Essential Step:
Standardization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#subset-selection-2"><span class="nav-number">5.3.</span> <span class="nav-text">Subset Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-model-and-the-goal"><span class="nav-number">5.3.1.</span> <span class="nav-text">The Model and The Goal</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-condition-for-success"><span class="nav-number">5.3.2.</span> <span class="nav-text">The Condition for Success</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#how-to-choose-the-model-size-practice"><span class="nav-number">5.3.3.</span> <span class="nav-text">How to Choose the Model
Size (Practice)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code-r-vs.-python"><span class="nav-number">5.3.4.</span> <span class="nav-text">Code: R vs.Â Python</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#shrinkage-methods-by-regularization"><span class="nav-number">5.4.</span> <span class="nav-text">Shrinkage Methods by
Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ridge-regression-the-core-idea"><span class="nav-number">5.4.1.</span> <span class="nav-text">Ridge Regression: The Core
Idea</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-need-for-standardization"><span class="nav-number">5.4.2.</span> <span class="nav-text">The Need for Standardization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#answering-the-discussion-questions"><span class="nav-number">5.4.3.</span> <span class="nav-text">Answering the Discussion
Questions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#what-is-the-solution-of-ridge-regression"><span class="nav-number">5.4.3.1.</span> <span class="nav-text">1. What is the
solution of Ridge regression?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#what-is-the-bias-and-the-variance"><span class="nav-number">5.4.3.2.</span> <span class="nav-text">2. What is the bias and the
variance?</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#python-equivalent-for-6.2"><span class="nav-number">5.4.4.</span> <span class="nav-text">Python Equivalent for 6.2</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#the-why-of-ridge-regression"><span class="nav-number">6.</span> <span class="nav-text">6. The â€œWhyâ€ of Ridge
Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#core-concepts-the-why-of-ridge-regression"><span class="nav-number">6.1.</span> <span class="nav-text">Core Concepts: The
â€œWhyâ€ of Ridge Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-bias-variance-tradeoff-slide-3"><span class="nav-number">6.1.1.</span> <span class="nav-text">The Bias-Variance Tradeoff
(Slide 3)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-is-it-called-ridge-the-3d-spatial-meaning-slide-5"><span class="nav-number">6.1.2.</span> <span class="nav-text">Why Is It
Called â€œRidgeâ€? The 3D Spatial Meaning (Slide 5)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mathematical-formulas"><span class="nav-number">6.2.</span> <span class="nav-text">Mathematical Formulas</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#walkthrough-of-the-credit-data-example-all-slides"><span class="nav-number">6.3.</span> <span class="nav-text">Walkthrough
of the â€œCredit Dataâ€ Example (All Slides)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#step-1-data-preparation-slide-8"><span class="nav-number">6.3.1.</span> <span class="nav-text">Step 1: Data Preparation (Slide
8)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#step-2-fit-the-ridge-model-slide-8"><span class="nav-number">6.3.2.</span> <span class="nav-text">Step 2: Fit the Ridge Model
(Slide 8)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#step-3-visualize-the-coefficient-solution-path-slides-1-4-9"><span class="nav-number">6.3.3.</span> <span class="nav-text">Step 3:
Visualize the Coefficient â€œSolution Pathâ€ (Slides 1, 4, 9)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#step-4-find-the-best-lambda-using-cross-validation-slides-4-7"><span class="nav-number">6.3.4.</span> <span class="nav-text">Step
4: Find the Best \(\lambda\)
using Cross-Validation (Slides 4 &amp; 7)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python-scikit-learn-equivalents"><span class="nav-number">6.4.</span> <span class="nav-text">Python
(scikit-learn) Equivalents</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#shrinkage-methods-regularization-1"><span class="nav-number">7.</span> <span class="nav-text">7. Shrinkage Methods
(Regularization)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#key-mathematical-formulas"><span class="nav-number">7.1.</span> <span class="nav-text">Key Mathematical Formulas</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#penalized-formulation-slide-1"><span class="nav-number">7.1.1.</span> <span class="nav-text">1. Penalized Formulation (Slide
1)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#constrained-formulation-slide-2"><span class="nav-number">7.1.2.</span> <span class="nav-text">2. Constrained Formulation
(Slide 2)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#important-plots-and-interpretation"><span class="nav-number">7.2.</span> <span class="nav-text">Important Plots and
Interpretation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-cross-validation-cv-plot-slide-5"><span class="nav-number">7.2.1.</span> <span class="nav-text">1. The Cross-Validation
(CV) Plot (Slide 5)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-coefficient-path-plot-slide-6"><span class="nav-number">7.2.2.</span> <span class="nav-text">2. The Coefficient Path Plot
(Slide 6)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-understanding-r-to-python"><span class="nav-number">7.3.</span> <span class="nav-text">Code Understanding (R to
Python)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#finding-the-best-lambda-cv"><span class="nav-number">7.3.1.</span> <span class="nav-text">1. Finding the Best \(\lambda\) (CV)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fitting-with-the-best-lambda-and-getting-coefficients"><span class="nav-number">7.3.2.</span> <span class="nav-text">2.
Fitting with the Best \(\lambda\) and
Getting Coefficients</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-core-problem-two-equivalent-formulas"><span class="nav-number">7.4.</span> <span class="nav-text">The Core Problem: Two
Equivalent Formulas</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#formulation-1-the-penalized-method-slides-1-4"><span class="nav-number">7.4.1.</span> <span class="nav-text">Formulation 1:
The Penalized Method (Slides 1 &amp; 4)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#formulation-2-the-constrained-method-slides-2-3"><span class="nav-number">7.4.2.</span> <span class="nav-text">Formulation 2:
The Constrained Method (Slides 2 &amp; 3)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#detailed-plot-code-analysis"><span class="nav-number">7.5.</span> <span class="nav-text">Detailed Plot &amp; Code
Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#question-1-how-to-pick-the-best-lambda-slide-5"><span class="nav-number">7.5.1.</span> <span class="nav-text">Question 1: How
to pick the best \(\lambda\)? (Slide
5)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#question-2-what-does-lasso-do-slides-5-6-7"><span class="nav-number">7.5.2.</span> <span class="nav-text">Question 2: What
does LASSO do? (Slides 5, 6, 7)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python-equivalents-in-more-detail"><span class="nav-number">7.6.</span> <span class="nav-text">Python Equivalents (in more
detail)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-game-of-regularization"><span class="nav-number">7.6.1.</span> <span class="nav-text">The â€œGameâ€ of Regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-lasso-performs-variable-selection-the-diamond"><span class="nav-number">7.6.2.</span> <span class="nav-text">Why LASSO
Performs Variable Selection (The Diamond) ğŸ¯</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-ridge-regression-only-shrinks-the-circle"><span class="nav-number">7.6.3.</span> <span class="nav-text">Why Ridge
Regression Only Shrinks (The Circle) ğŸ¤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary-diamond-vs.-circle"><span class="nav-number">7.6.4.</span> <span class="nav-text">Summary: Diamond vs.Â Circle</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#shrinkage-methods-lasso-vs.-ridge"><span class="nav-number">8.</span> <span class="nav-text">8. Shrinkage Methods (Lasso
vs.Â Ridge)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#core-concept-shrinkage-methods"><span class="nav-number">8.1.</span> <span class="nav-text">Core Concept: Shrinkage
Methods</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-key-difference-math-how-they-shrink"><span class="nav-number">8.2.</span> <span class="nav-text">The Key Difference:
Math &amp; How They Shrink</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ridge-regression-proportional-shrinkage"><span class="nav-number">8.2.1.</span> <span class="nav-text">Ridge Regression
(Proportional Shrinkage)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lasso-regression-soft-thresholding"><span class="nav-number">8.2.2.</span> <span class="nav-text">Lasso Regression
(Soft-Thresholding)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#important-images-explained"><span class="nav-number">8.3.</span> <span class="nav-text">Important Images Explained</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#most-important-figure-6.10-slide-82"><span class="nav-number">8.3.1.</span> <span class="nav-text">Most Important: Figure 6.10
(Slide 82)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#scenario-1-figure-6.8-slide-76"><span class="nav-number">8.3.2.</span> <span class="nav-text">Scenario 1: Figure 6.8 (Slide
76)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#scenario-2-figure-6.9-slide-78"><span class="nav-number">8.3.3.</span> <span class="nav-text">Scenario 2: Figure 6.9 (Slide
78)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python-code-understanding-1"><span class="nav-number">8.4.</span> <span class="nav-text">Python &amp; Code
Understanding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#summary-lasso-vs.-ridge"><span class="nav-number">8.5.</span> <span class="nav-text">Summary: Lasso vs.Â Ridge</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#shrinkage-methods-ridge-lasso"><span class="nav-number">9.</span> <span class="nav-text">9. Shrinkage Methods (Ridge &amp;
LASSO)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#summary-of-shrinkage-methods-ridge-lasso"><span class="nav-number">9.1.</span> <span class="nav-text">Summary of Shrinkage
Methods (Ridge &amp; LASSO)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#key-mathematical-formulas-1"><span class="nav-number">9.2.</span> <span class="nav-text">Key Mathematical Formulas</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#important-images-concepts"><span class="nav-number">9.3.</span> <span class="nav-text">Important Images &amp; Concepts</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tuning-parameter-selection-slides-3-4-left-plots"><span class="nav-number">9.3.1.</span> <span class="nav-text">Tuning
Parameter Selection (Slides 3 &amp; 4, Left Plots)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#coefficient-paths-slides-3-4-right-plots"><span class="nav-number">9.3.2.</span> <span class="nav-text">Coefficient Paths
(Slides 3 &amp; 4, Right Plots)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#key-discussion-points-slide-2"><span class="nav-number">9.4.</span> <span class="nav-text">Key Discussion Points (Slide
2)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python-code-understanding-using-scikit-learn"><span class="nav-number">9.5.</span> <span class="nav-text">Python Code
Understanding (using scikit-learn)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bias-variance-tradeoff"><span class="nav-number">9.6.</span> <span class="nav-text">Bias-variance tradeoff</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#key-mathematical-formulas-concepts"><span class="nav-number">9.7.</span> <span class="nav-text">Key Mathematical Formulas
&amp; Concepts</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lasso-sign-consistency"><span class="nav-number">9.7.1.</span> <span class="nav-text">LASSO: Sign Consistency</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ridge-regression-the-bias-variance-tradeoff"><span class="nav-number">9.7.2.</span> <span class="nav-text">Ridge Regression:
The Bias-Variance Tradeoff</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#important-images-key-takeaways"><span class="nav-number">9.8.</span> <span class="nav-text">Important Images &amp; Key
Takeaways</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python-code-understanding-2"><span class="nav-number">9.9.</span> <span class="nav-text">Python Code Understanding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#example-output"><span class="nav-number">9.9.1.</span> <span class="nav-text">Example Output:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code-explanation"><span class="nav-number">9.9.2.</span> <span class="nav-text">Code Explanation:</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#elastic-net"><span class="nav-number">10.</span> <span class="nav-text">10. Elastic Net</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#overall-summary"><span class="nav-number">10.1.</span> <span class="nav-text">Overall Summary</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#slide-1-the-definition-and-formula-file-...020245.png"><span class="nav-number">10.1.1.</span> <span class="nav-text">Slide 1:
The Definition and Formula (File: ...020245.png)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#slide-2-the-intuition-and-the-grouping-effect-file-...020249.jpg"><span class="nav-number">10.1.2.</span> <span class="nav-text">Slide
2: The Intuition and The Grouping Effect (File:
...020249.jpg)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#part-1-the-three-graphs-geometric-intuition"><span class="nav-number">10.1.2.1.</span> <span class="nav-text">Part 1: The Three
Graphs (Geometric Intuition)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#part-2-the-formula-the-grouping-effect"><span class="nav-number">10.1.2.2.</span> <span class="nav-text">Part 2: The Formula (The
Grouping Effect)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#slide-3-the-feature-comparison-table-file-...020255.png"><span class="nav-number">10.1.3.</span> <span class="nav-text">Slide
3: The Feature Comparison Table (File: ...020255.png)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code-understanding-python-scikit-learn"><span class="nav-number">10.1.4.</span> <span class="nav-text">Code Understanding
(Python scikit-learn)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#high-dimensional-data-analysis"><span class="nav-number">11.</span> <span class="nav-text">11. High-Dimensional Data
Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#the-core-problem-large-p-small-n"><span class="nav-number">11.1.</span> <span class="nav-text">The Core Problem: Large \(p\), Small \(n\)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#most-important-image-the-overfitting-trap-figure-6.23"><span class="nav-number">11.2.</span> <span class="nav-text">Most
Important Image: The Overfitting Trap (Figure 6.23)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-solution-regularization-model-selection"><span class="nav-number">11.3.</span> <span class="nav-text">The Solution:
Regularization &amp; Model Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#mathematical-formulas-python-code"><span class="nav-number">11.3.1.</span> <span class="nav-text">Mathematical Formulas &amp;
Python Code ğŸ</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#a.-ridge-regression-l_2-penalty"><span class="nav-number">11.3.1.1.</span> <span class="nav-text">A. Ridge Regression (\(L_2\) Penalty)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#b.-the-lasso-l_1-penalty"><span class="nav-number">11.3.1.2.</span> <span class="nav-text">B. The Lasso (\(L_1\) Penalty)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#c.-other-methods"><span class="nav-number">11.3.1.3.</span> <span class="nav-text">C. Other Methods</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-curse-of-dimensionality-figure-6.24"><span class="nav-number">11.4.</span> <span class="nav-text">The Curse of
Dimensionality (Figure 6.24)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#summary-cautions-for-p-n"><span class="nav-number">11.5.</span> <span class="nav-text">Summary: Cautions for \(p &gt; n\)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-core-problem-p-gg-n-the-troubles-slide"><span class="nav-number">11.6.</span> <span class="nav-text">The Core Problem:
\(p \gg n\) (The â€œTroublesâ€ Slide)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-simplest-example-n2-figure-6.22"><span class="nav-number">11.7.</span> <span class="nav-text">The Simplest Example: \(n&#x3D;2\) (Figure 6.22)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-consequence-misleading-metrics-figure-6.23"><span class="nav-number">11.8.</span> <span class="nav-text">The Consequence:
Misleading Metrics (Figure 6.23)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-solutions-the-deal-with-slide"><span class="nav-number">11.9.</span> <span class="nav-text">The Solutions (The â€œDeal
withâ€¦â€ Slide)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#a.-ridge-regression-l_2-penalty-1"><span class="nav-number">11.9.0.1.</span> <span class="nav-text">A. Ridge Regression (\(L_2\) Penalty)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#b.-the-lasso-l_1-penalty-1"><span class="nav-number">11.9.0.2.</span> <span class="nav-text">B. The Lasso (\(L_1\) Penalty)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#c.-tuning-parameter-choice-the-real-work"><span class="nav-number">11.9.0.3.</span> <span class="nav-text">C. Tuning Parameter
Choice (The Real Work)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#a-final-warning-the-curse-of-dimensionality-figure-6.24"><span class="nav-number">11.10.</span> <span class="nav-text">A Final
Warning: The Curse of Dimensionality (Figure 6.24)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-goal-collaborative-filtering"><span class="nav-number">11.11.</span> <span class="nav-text">The Goal: â€œCollaborative
Filteringâ€</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-hard-problem-and-its-2-flavors"><span class="nav-number">11.12.</span> <span class="nav-text">The Hard Problem (and its 2
Flavors)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#detail-1-noise-vs.-no-noise"><span class="nav-number">11.12.0.1.</span> <span class="nav-text">Detail 1: Noise vs.Â No Noise</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#detail-2-why-is-textrankmathbfn-le-r-a-non-convex-constraint"><span class="nav-number">11.12.0.2.</span> <span class="nav-text">Detail
2: Why is \(\text{rank}(\mathbf{N}) \le
r\) a â€œNon-convex constraintâ€?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#detail-3-the-number-of-parameters-rd_1-d_2"><span class="nav-number">11.12.0.3.</span> <span class="nav-text">Detail 3: The Number
of Parameters: \(r(d_1 + d_2)\)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-magic-solution-convex-relaxation"><span class="nav-number">11.13.</span> <span class="nav-text">The â€œMagicâ€ Solution:
Convex Relaxation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#detail-1-the-l1-norm-analogy-this-is-the-most-important-concept"><span class="nav-number">11.13.0.1.</span> <span class="nav-text">Detail
1: The L1-Norm Analogy (This is the most important concept)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#detail-2-noiseless-vs.-noisy-again"><span class="nav-number">11.13.0.2.</span> <span class="nav-text">Detail 2: Noiseless
vs.Â Noisy (Again)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#detail-3-the-guarantee-what-the-math-at-the-bottom-means"><span class="nav-number">11.13.0.3.</span> <span class="nav-text">Detail
3: The Guarantee (What the math at the bottom means)</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> å¼ºåŠ›é©±åŠ¨
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
