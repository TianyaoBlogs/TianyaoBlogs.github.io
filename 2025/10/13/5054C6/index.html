<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianyaoblogs.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-6 Lecturer: Prof.XIA DONG 1. Linear Model Selection and Regularization çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ– Summary of Core Concepts Chapter 6: Linear Model Selection and Regularization, focusing specifica">
<meta property="og:type" content="article">
<meta property="og:title" content="MSDM 5054 - Statistical Machine Learning-L6">
<meta property="og:url" content="https://tianyaoblogs.github.io/2025/10/13/5054C6/index.html">
<meta property="og:site_name" content="TianyaoBlogs">
<meta property="og:description" content="ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-6 Lecturer: Prof.XIA DONG 1. Linear Model Selection and Regularization çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ– Summary of Core Concepts Chapter 6: Linear Model Selection and Regularization, focusing specifica">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-13T13:00:00.000Z">
<meta property="article:modified_time" content="2025-10-19T19:30:26.883Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://tianyaoblogs.github.io/2025/10/13/5054C6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>MSDM 5054 - Statistical Machine Learning-L6 | TianyaoBlogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TianyaoBlogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/13/5054C6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MSDM 5054 - Statistical Machine Learning-L6
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-10-13 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-13T21:00:00+08:00">2025-10-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-10-20 03:30:26" itemprop="dateModified" datetime="2025-10-20T03:30:26+08:00">2025-10-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-6</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1
id="linear-model-selection-and-regularization-çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–">1.
Linear Model Selection and Regularization çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–</h1>
<h2 id="summary-of-core-concepts">Summary of Core Concepts</h2>
<p><strong>Chapter 6: Linear Model Selection and
Regularization</strong>, focusing specifically on <strong>Section 6.1:
Subset Selection</strong>.
<strong>ç¬¬å…­ç« ï¼šçº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–</strong>ï¼Œ<strong>6.1èŠ‚ï¼šå­é›†é€‰æ‹©</strong></p>
<ul>
<li><p><strong>The Problem:</strong> You have a dataset with many
potential predictor variables (features). If you include all of them
(like <strong>Model 1</strong> with <span
class="math inline">\(p\)</span> predictors in slide
<code>...221320.png</code>), you risk including â€œnoiseâ€ variables. These
irrelevant features can decrease model accuracy (overfitting) and make
the model difficult to interpret.
æ•°æ®é›†åŒ…å«è®¸å¤šæ½œåœ¨çš„é¢„æµ‹å˜é‡ï¼ˆç‰¹å¾ï¼‰ã€‚å¦‚æœåŒ…å«æ‰€æœ‰è¿™äº›å˜é‡ï¼ˆä¾‹å¦‚å¹»ç¯ç‰‡â€œâ€¦221320.pngâ€ä¸­å¸¦æœ‰<span
class="math inline">\(p\)</span>ä¸ªé¢„æµ‹å˜é‡çš„<strong>æ¨¡å‹1</strong>ï¼‰ï¼Œåˆ™å¯èƒ½ä¼šåŒ…å«â€œå™ªå£°â€å˜é‡ã€‚è¿™äº›ä¸ç›¸å…³çš„ç‰¹å¾ä¼šé™ä½æ¨¡å‹çš„å‡†ç¡®ç‡ï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼Œå¹¶ä½¿æ¨¡å‹éš¾ä»¥è§£é‡Šã€‚</p></li>
<li><p><strong>The Goal:</strong> Identify a smaller subset of variables
that are truly related to the response. This creates a simpler, more
interpretable, and often more accurate model (like <strong>Model
2</strong> with <span class="math inline">\(q\)</span> predictors).
æ‰¾å‡ºä¸€ä¸ªä¸å“åº”çœŸæ­£ç›¸å…³çš„è¾ƒå°å˜é‡å­é›†ã€‚è¿™å°†åˆ›å»ºä¸€ä¸ªæ›´ç®€å•ã€æ›´æ˜“äºè§£é‡Šä¸”é€šå¸¸æ›´å‡†ç¡®çš„æ¨¡å‹ï¼ˆä¾‹å¦‚å¸¦æœ‰<span
class="math inline">\(q\)</span>ä¸ªé¢„æµ‹å˜é‡çš„<strong>æ¨¡å‹2</strong>ï¼‰ã€‚</p></li>
<li><p><strong>The Main Method Discussed: Best Subset
Selection</strong></p></li>
<li><p><strong>ä¸»è¦è®¨è®ºçš„æ–¹æ³•ï¼šæœ€ä½³å­é›†é€‰æ‹©</strong> This is an
<em>exhaustive search</em> algorithm. It checks <em>every possible
combination</em> of predictors to find the â€œbestâ€ model. With <span
class="math inline">\(p\)</span> variables, this means checking <span
class="math inline">\(2^p\)</span> total models.
è¿™æ˜¯ä¸€ç§<em>ç©·ä¸¾æœç´¢</em>ç®—æ³•ã€‚å®ƒæ£€æŸ¥<em>æ‰€æœ‰å¯èƒ½çš„é¢„æµ‹å˜é‡ç»„åˆ</em>ï¼Œä»¥æ‰¾åˆ°â€œæœ€ä½³â€æ¨¡å‹ã€‚å¯¹äº
<span class="math inline">\(p\)</span> ä¸ªå˜é‡ï¼Œè¿™æ„å‘³ç€éœ€è¦æ£€æŸ¥æ€»å…±
<span class="math inline">\(2^p\)</span> ä¸ªæ¨¡å‹ã€‚</p>
<p>The algorithm (from slide <code>...221333.png</code>) works in three
steps:</p>
<ol type="1">
<li><p><strong>Step 1:</strong> Fit the â€œnull modelâ€ <span
class="math inline">\(M_0\)</span>, which has no predictors (it just
predicts the average of the response). æ‹Ÿåˆâ€œç©ºæ¨¡å‹â€<span
class="math inline">\(M_0\)</span>ï¼Œå®ƒæ²¡æœ‰é¢„æµ‹å˜é‡ï¼ˆå®ƒåªé¢„æµ‹å“åº”çš„å¹³å‡å€¼ï¼‰ã€‚</p></li>
<li><p><strong>Step 2:</strong> For each <span
class="math inline">\(k\)</span> (from 1 to <span
class="math inline">\(p\)</span>):</p>
<ul>
<li><p>Fit all <span class="math inline">\(\binom{p}{k}\)</span> models
that contain exactly <span class="math inline">\(k\)</span> predictors.
(e.g., fit all models with 1 predictor, then all models with 2
predictors, etc.).</p></li>
<li><p>æ‹Ÿåˆæ‰€æœ‰åŒ…å« <span class="math inline">\(k\)</span> ä¸ªé¢„æµ‹å˜é‡çš„
<span class="math inline">\(\binom{p}{k}\)</span>
ä¸ªæ¨¡å‹ã€‚ï¼ˆä¾‹å¦‚ï¼Œå…ˆæ‹Ÿåˆæ‰€æœ‰åŒ…å« 1 ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œç„¶åæ‹Ÿåˆæ‰€æœ‰åŒ…å« 2
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œç­‰ç­‰ï¼‰ã€‚</p></li>
<li><p>From this group, select the single best model <em>for that size
<span class="math inline">\(k\)</span></em>. This â€œbestâ€ model is the
one with the highest <strong><span
class="math inline">\(R^2\)</span></strong> (or lowest
<strong>RSS</strong> - Residual Sum of Squares) on the <em>training
data</em>. Call this model <span
class="math inline">\(M_k\)</span>.</p></li>
<li><p>ä»è¿™ç»„ä¸­ï¼Œé€‰æ‹© <em>å¯¹äºè¯¥è§„æ¨¡ <span
class="math inline">\(k\)</span></em> çš„æœ€ä½³æ¨¡å‹ã€‚è¿™ä¸ªâ€œæœ€ä½³â€æ¨¡å‹æ˜¯åœ¨
<em>è®­ç»ƒæ•°æ®</em> ä¸Šå…·æœ‰æœ€é«˜ <strong><span
class="math inline">\(R^2\)</span></strong>ï¼ˆæˆ–æœ€ä½ <strong>RSS</strong>
- æ®‹å·®å¹³æ–¹å’Œï¼‰çš„æ¨¡å‹ã€‚å°†æ­¤æ¨¡å‹ç§°ä¸º <span
class="math inline">\(M_k\)</span>ã€‚</p></li>
</ul></li>
<li><p><strong>Step 3:</strong> You now have <span
class="math inline">\(p+1\)</span> models: <span
class="math inline">\(M_0, M_1, \dots, M_p\)</span>. You must select the
single best one from this list. To do this, you <strong>cannot</strong>
use training <span class="math inline">\(R^2\)</span> (as it will always
pick the biggest model <span class="math inline">\(M_p\)</span>).
Instead, you must use a metric that estimates <em>test error</em>, such
as: <strong>ç°åœ¨ä½ æœ‰ <span class="math inline">\(p+1\)</span>
ä¸ªæ¨¡å‹ï¼š<span class="math inline">\(M_0, M_1, \dots,
M_p\)</span>ã€‚ä½ å¿…é¡»ä»åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä½³æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œä½ </strong>ä¸èƒ½**ä½¿ç”¨è®­ç»ƒ
<span class="math inline">\(R^2\)</span>ï¼ˆå› ä¸ºå®ƒæ€»æ˜¯ä¼šé€‰æ‹©æœ€å¤§çš„æ¨¡å‹
<span
class="math inline">\(M_p\)</span>ï¼‰ã€‚ç›¸åï¼Œä½ å¿…é¡»ä½¿ç”¨ä¸€ä¸ªèƒ½å¤Ÿä¼°è®¡<em>æµ‹è¯•è¯¯å·®</em>çš„æŒ‡æ ‡ï¼Œä¾‹å¦‚ï¼š</p>
<ul>
<li><strong>Cross-Validation (CV) äº¤å‰éªŒè¯ (CV)</strong> (This is what
the Python code uses)</li>
<li><strong>AIC</strong> (Akaike Information Criterion
èµ¤æ± ä¿¡æ¯å‡†åˆ™)</li>
<li><strong>BIC</strong> (Bayesian Information Criterion
è´å¶æ–¯ä¿¡æ¯å‡†åˆ™)</li>
<li><strong>Adjusted <span class="math inline">\(R^2\)</span> è°ƒæ•´åçš„
<span class="math inline">\(R^2\)</span></strong></li>
</ul></li>
</ol></li>
<li><p><strong>Key Takeaway:</strong> The slides show this â€œsubset
selectionâ€ concept can be applied <em>beyond</em> linear models. The
Python code demonstrates this by applying best subset selection to a
<strong>K-Nearest Neighbors (KNN) Regressor</strong>, a non-linear
model.â€œå­é›†é€‰æ‹©â€çš„æ¦‚å¿µå¯ä»¥åº”ç”¨äºçº¿æ€§æ¨¡å‹<em>ä¹‹å¤–</em>ã€‚</p></li>
</ul>
<h2
id="mathematical-understanding-key-questions-æ•°å­¦ç†è§£ä¸å…³é”®é—®é¢˜">Mathematical
Understanding &amp; Key Questions æ•°å­¦ç†è§£ä¸å…³é”®é—®é¢˜</h2>
<p>This section directly answers the questions posed on your slides.</p>
<h3 id="how-to-compare-which-model-is-better">How to compare which model
is better?</h3>
<p>(From slides <code>...221320.png</code> and
<code>...221326.png</code>)</p>
<p>You cannot use <strong>training error</strong> (like <span
class="math inline">\(R^2\)</span> or RSS) to compare models with
<em>different numbers of predictors</em>. A model with more predictors
will almost always have a better <em>training</em> score, even if those
extra predictors are just noise. This is called
<strong>overfitting</strong>. ä¸èƒ½ä½¿ç”¨<strong>è®­ç»ƒè¯¯å·®</strong>ï¼ˆä¾‹å¦‚
<span class="math inline">\(R^2\)</span> æˆ–
RSSï¼‰æ¥æ¯”è¾ƒå…·æœ‰<em>ä¸åŒæ•°é‡é¢„æµ‹å˜é‡</em>çš„æ¨¡å‹ã€‚å…·æœ‰æ›´å¤šé¢„æµ‹å˜é‡çš„æ¨¡å‹å‡ ä¹æ€»æ˜¯å…·æœ‰æ›´å¥½çš„<em>è®­ç»ƒ</em>åˆ†æ•°ï¼Œå³ä½¿è¿™äº›é¢å¤–çš„é¢„æµ‹å˜é‡åªæ˜¯å™ªå£°ã€‚è¿™è¢«ç§°ä¸º<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚</p>
<p>To compare models of different sizes (like Model 1 vs.Â Model 2, or
<span class="math inline">\(M_2\)</span> vs.Â <span
class="math inline">\(M_5\)</span>), you <strong>must</strong> use a
method that estimates <strong>test error</strong> (how the model
performs on new, unseen data). The slides mention:
è¦æ¯”è¾ƒä¸åŒå¤§å°çš„æ¨¡å‹ï¼ˆä¾‹å¦‚æ¨¡å‹ 1 ä¸æ¨¡å‹ 2ï¼Œæˆ– <span
class="math inline">\(M_2\)</span> ä¸ <span
class="math inline">\(M_5\)</span>ï¼‰ï¼Œæ‚¨<strong>å¿…é¡»</strong>ä½¿ç”¨ä¸€ç§ä¼°ç®—<strong>æµ‹è¯•è¯¯å·®</strong>ï¼ˆæ¨¡å‹åœ¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ä¸Šçš„è¡¨ç°ï¼‰çš„æ–¹æ³•ã€‚</p>
<ul>
<li><p><strong>Cross-Validation (CV):</strong> This is the gold
standard. You split your data into â€œfolds,â€ train the model on some
folds, and test it on the remaining fold. You repeat this and average
the test scores. The model with the best (e.g., lowest) average CV error
is chosen.
å°†æ•°æ®åˆ†æˆâ€œæŠ˜å â€ï¼Œåœ¨ä¸€äº›æŠ˜å ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨å‰©ä½™çš„æŠ˜å ä¸Šæµ‹è¯•æ¨¡å‹ã€‚é‡å¤æ­¤æ“ä½œå¹¶å–æµ‹è¯•åˆ†æ•°çš„å¹³å‡å€¼ã€‚é€‰æ‹©å¹³å‡
CV è¯¯å·®æœ€å°ï¼ˆä¾‹å¦‚ï¼Œæœ€å°ï¼‰çš„æ¨¡å‹ã€‚</p></li>
<li><p><strong>AIC &amp; BIC:</strong> These are mathematical
adjustments to the training error (like RSS) that add a <em>penalty</em>
for having more predictors. They balance model <em>fit</em> with model
<em>complexity</em>. è¿™äº›æ˜¯å¯¹è®­ç»ƒè¯¯å·®ï¼ˆå¦‚
RSSï¼‰çš„æ•°å­¦è°ƒæ•´ï¼Œä¼šå› é¢„æµ‹å˜é‡è¾ƒå¤šè€Œå¢åŠ <em>æƒ©ç½š</em>ã€‚å®ƒä»¬å¹³è¡¡äº†æ¨¡å‹<em>æ‹Ÿåˆåº¦</em>å’Œæ¨¡å‹<em>å¤æ‚åº¦</em>ã€‚</p></li>
</ul>
<h3 id="why-use-r2-in-step-2">Why use <span
class="math inline">\(R^2\)</span> in Step 2?</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p>In Step 2, you are only comparing models <strong>of the same
size</strong> (i.e., all models that have exactly <span
class="math inline">\(k\)</span> predictors). For models with the same
number of parameters, a higher <span class="math inline">\(R^2\)</span>
(or lower RSS) on the training data directly corresponds to a better
fit. You donâ€™t need to penalize for complexity because all models being
compared <em>have the same complexity</em>.
åªæ¯”è¾ƒ<strong>å¤§å°ç›¸åŒ</strong>çš„æ¨¡å‹ï¼ˆå³æ‰€æœ‰æ°å¥½å…·æœ‰ <span
class="math inline">\(k\)</span>
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼‰ã€‚å¯¹äºå‚æ•°æ•°é‡ç›¸åŒçš„æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®ä¸Šæ›´é«˜çš„ <span
class="math inline">\(R^2\)</span>ï¼ˆæˆ–æ›´ä½çš„
RSSï¼‰ç›´æ¥å¯¹åº”ç€æ›´å¥½çš„æ‹Ÿåˆåº¦ã€‚æ‚¨ä¸éœ€è¦å¯¹å¤æ‚åº¦è¿›è¡Œæƒ©ç½šï¼Œå› ä¸ºæ‰€æœ‰è¢«æ¯”è¾ƒçš„æ¨¡å‹<em>éƒ½å…·æœ‰ç›¸åŒçš„å¤æ‚åº¦</em>ã€‚</p>
<h3 id="why-cant-we-use-training-error-in-step-3">Why canâ€™t we use
training error in Step 3?</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p>In Step 3, you are comparing models <strong>of different
sizes</strong> (<span class="math inline">\(M_0\)</span> vs.Â <span
class="math inline">\(M_1\)</span> vs.Â <span
class="math inline">\(M_2\)</span>, etc.). As you add predictors, the
training <span class="math inline">\(R^2\)</span> will <em>always</em>
go up (or stay the same), and the training RSS will <em>always</em> go
down (or stay the same). If you used <span
class="math inline">\(R^2\)</span> to pick the best model in Step 3, you
would <em>always</em> pick the most complex model <span
class="math inline">\(M_p\)</span>, which is almost certainly overfit.
å°†æ¯”è¾ƒ<strong>ä¸åŒå¤§å°</strong>çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ <span
class="math inline">\(M_0\)</span> vs.Â <span
class="math inline">\(M_1\)</span> vs.Â <span
class="math inline">\(M_2\)</span> ç­‰ï¼‰ã€‚éšç€æ‚¨æ·»åŠ é¢„æµ‹å˜é‡ï¼Œè®­ç»ƒ <span
class="math inline">\(R^2\)</span>
å°†<em>å§‹ç»ˆ</em>ä¸Šå‡ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ï¼Œè€Œè®­ç»ƒ RSS
å°†<em>å§‹ç»ˆ</em>ä¸‹é™ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ã€‚å¦‚æœæ‚¨åœ¨æ­¥éª¤ 3 ä¸­ä½¿ç”¨ <span
class="math inline">\(R^2\)</span>
æ¥é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼Œé‚£ä¹ˆæ‚¨<em>å§‹ç»ˆ</em>ä¼šé€‰æ‹©æœ€å¤æ‚çš„æ¨¡å‹ <span
class="math inline">\(M_p\)</span>ï¼Œè€Œè¯¥æ¨¡å‹å‡ ä¹è‚¯å®šä¼šè¿‡æ‹Ÿåˆã€‚</p>
<p>Therefore, you <em>must</em> use a metric that estimates test error
(like CV) or penalizes for complexity (like AIC, BIC, or Adjusted <span
class="math inline">\(R^2\)</span>) to find the right balance between
fit and simplicity. å› æ­¤ï¼Œæ‚¨<em>å¿…é¡»</em>ä½¿ç”¨ä¸€ä¸ªå¯ä»¥ä¼°ç®—æµ‹è¯•è¯¯å·®ï¼ˆä¾‹å¦‚
CVï¼‰æˆ–æƒ©ç½šå¤æ‚åº¦ï¼ˆä¾‹å¦‚ AICã€BIC æˆ–è°ƒæ•´åçš„ <span
class="math inline">\(R^2\)</span>ï¼‰çš„æŒ‡æ ‡æ¥æ‰¾åˆ°æ‹Ÿåˆåº¦å’Œç®€å•æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚</p>
<h2 id="code-analysis">Code Analysis</h2>
<p>The Python code (slides <code>...221249.jpg</code> and
<code>...221303.jpg</code>) implements the <strong>Best Subset
Selection</strong> algorithm using <strong>KNN Regression</strong>.</p>
<h3 id="key-functions">Key Functions</h3>
<ul>
<li><code>main()</code>:
<ol type="1">
<li><strong>Loads Data:</strong> Reads the <code>Credit.csv</code>
file.</li>
<li><strong>Preprocesses Data:</strong>
<ul>
<li>Converts categorical features (â€˜Genderâ€™, â€˜Studentâ€™, â€˜Marriedâ€™,
â€˜Ethnicityâ€™) into numerical ones (dummy variables).
å°†åˆ†ç±»ç‰¹å¾ï¼ˆâ€œæ€§åˆ«â€ã€â€œå­¦ç”Ÿâ€ã€â€œå·²å©šâ€ã€â€œç§æ—â€ï¼‰è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾ï¼ˆè™šæ‹Ÿå˜é‡ï¼‰ã€‚</li>
<li>Creates the feature matrix <code>X</code> and target variable
<code>y</code> (â€˜Balanceâ€™). åˆ›å»ºç‰¹å¾çŸ©é˜µ <code>X</code> å’Œç›®æ ‡å˜é‡
<code>y</code>ï¼ˆâ€œä½™é¢â€ï¼‰ã€‚</li>
<li><strong>Scales</strong> the features using
<code>StandardScaler</code>. This is crucial for KNN, which is sensitive
to the scale of features. ç”¨ <code>StandardScaler</code>
å¯¹ç‰¹å¾è¿›è¡Œ<strong>ç¼©æ”¾</strong>ã€‚è¿™å¯¹äº KNN
è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¯¹ç‰¹å¾çš„ç¼©æ”¾éå¸¸æ•æ„Ÿã€‚</li>
</ul></li>
<li><strong>Adds Noise (in the second example):</strong> Slide
<code>...221303.jpg</code> shows code that <em>adds 20 new â€œnoisyâ€
columns</em> to the data. This is to test if the selection algorithm is
smart enough to ignore them. å‘æ•°æ®ä¸­æ·»åŠ  20
ä¸ªæ–°çš„â€œå™ªå£°â€åˆ—çš„ä»£ç ã€‚è¿™æ˜¯ä¸ºäº†æµ‹è¯•é€‰æ‹©ç®—æ³•æ˜¯å¦è¶³å¤Ÿæ™ºèƒ½ï¼Œèƒ½å¤Ÿå¿½ç•¥å®ƒä»¬ã€‚</li>
<li><strong>Runs Selection:</strong> Calls
<code>best_subset_selection_parallel</code> to do the main work.</li>
<li><strong>Prints Results:</strong> Finds the best subset (lowest
error) and prints the top 20 best-performing subsets.
æ‰¾åˆ°æœ€ä½³å­é›†ï¼ˆè¯¯å·®æœ€å°ï¼‰ï¼Œå¹¶æ‰“å°å‡ºè¡¨ç°æœ€ä½³çš„å‰ 20 ä¸ªå­é›†ã€‚</li>
<li><strong>Final Evaluation:</strong> It re-trains a KNN model on
<em>only</em> the best subset and calculates the final cross-validated
RMSE. ä»…åŸºäºæœ€ä½³å­é›†é‡æ–°è®­ç»ƒ KNN æ¨¡å‹ï¼Œå¹¶è®¡ç®—æœ€ç»ˆçš„äº¤å‰éªŒè¯ RMSEã€‚</li>
</ol></li>
<li><code>evaluate_subset(subset, ...)</code>:
<ul>
<li>This is the â€œworkerâ€ function. Itâ€™s called for <em>every single</em>
possible subset.</li>
<li>It takes a <code>subset</code> (a list of feature names, e.g.,
<code>['Income', 'Limit']</code>).</li>
<li>It creates a new <code>X_subset</code> containing <em>only</em>
those columns.</li>
<li>It runs 5-fold cross-validation (<code>cross_val_score</code>) on a
KNN model using this <code>X_subset</code>.</li>
<li>It uses <code>'neg_mean_squared_error'</code> as the metric. This is
negative MSE; a <em>higher</em> score (closer to 0) is better.
å®ƒä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„â€œX_subsetâ€<em>ï¼Œä»…åŒ…å«è¿™äº›åˆ—ã€‚ å®ƒä¼šä½¿ç”¨æ­¤â€œX_subsetâ€åœ¨
KNN æ¨¡å‹ä¸Šè¿è¡Œ 5 å€äº¤å‰éªŒè¯ï¼ˆâ€œcross_val_scoreâ€ï¼‰ã€‚
å®ƒä½¿ç”¨â€œneg_mean_squared_errorâ€ä½œä¸ºåº¦é‡æ ‡å‡†ã€‚è¿™æ˜¯è´Ÿ
MSEï¼›</em>æ›´é«˜*çš„åˆ†æ•°ï¼ˆè¶Šæ¥è¿‘ 0ï¼‰è¶Šå¥½ã€‚</li>
<li>It returns the subset and its average CV score.</li>
</ul></li>
<li><code>best_subset_selection_parallel(model, ...)</code>:
<ul>
<li>This is the â€œmanagerâ€ function.è¿™æ˜¯â€œç®¡ç†å™¨â€å‡½æ•°ã€‚</li>
<li>It iterates from <code>k=1</code> up to the total number of
features.å®ƒä»â€œk=1â€è¿­ä»£åˆ°ç‰¹å¾æ€»æ•°ã€‚</li>
<li>For each <code>k</code>, it generates <em>all combinations</em> of
features of that size (this is the <span
class="math inline">\(\binom{p}{k}\)</span> part).
å¯¹äºæ¯ä¸ªâ€œkâ€ï¼Œå®ƒä¼šç”Ÿæˆè¯¥å¤§å°çš„ç‰¹å¾çš„<em>æ‰€æœ‰ç»„åˆ</em>ï¼ˆè¿™æ˜¯ <span
class="math inline">\(\binom{p}{k}\)</span> éƒ¨åˆ†ï¼‰ã€‚</li>
<li>It uses <code>Parallel</code> and <code>delayed</code> (from
<code>joblib</code>) to run <code>evaluate_subset</code> for all these
combinations <em>in parallel</em>, speeding up the process
significantly. å®ƒä½¿ç”¨ <code>Parallel</code> å’Œ
<code>delayed</code>ï¼ˆæ¥è‡ª
<code>joblib</code>ï¼‰å¯¹æ‰€æœ‰è¿™äº›ç»„åˆ<em>å¹¶è¡Œ</em>è¿è¡Œ
<code>evaluate_subset</code>ï¼Œä»è€Œæ˜¾è‘—åŠ å¿«äº†å¤„ç†é€Ÿåº¦ã€‚</li>
<li>It collects all the results and returns
them.å®ƒæ”¶é›†æ‰€æœ‰ç»“æœå¹¶è¿”å›ã€‚</li>
</ul></li>
</ul>
<h3 id="analysis-of-the-output">Analysis of the Output</h3>
<ul>
<li><strong>Slide <code>...221255.png</code> (Original Data):</strong>
<ul>
<li>The code runs subset selection on the original dataset.</li>
<li>The â€œTop 20 Best Feature Subsetsâ€ are shown. The CV scores are
negative (they are <code>neg_mean_squared_error</code>), so the scores
<em>closest to zero</em> (smallest magnitude) are best.</li>
<li>The <strong>Best feature subset</strong> is found to be
<code>('Income', 'Limit', 'Rating', 'Student')</code>.</li>
<li>The final cross-validated RMSE for this model is
<strong>105.41</strong>.</li>
</ul></li>
<li><strong>Slide <code>...221309.png</code> (Data with 20 Noisy
Variables):</strong>
<ul>
<li>The code is re-run after adding 20 useless â€œNoisyâ€ features.</li>
<li>The algorithm <em>still</em> works. It correctly identifies that the
â€œNoisyâ€ variables are useless.</li>
<li>The <strong>Best feature subset</strong> is now
<code>('Income', 'Limit', 'Student')</code>. (Note: â€˜Ratingâ€™ was
dropped, likely because itâ€™s highly correlated with â€˜Limitâ€™, and the
noisy data made the simpler model perform slightly better in CV).</li>
<li>The final RMSE is <strong>114.94</strong>. This is <em>higher</em>
than the original 105.41, which is expectedâ€”the presence of so many
noise variables makes the selection problem harder, but the final model
is still good and, most importantly, <em>it successfully excluded all 20
noisy features</em>. æœ€ç»ˆçš„ RMSE ä¸º <strong>114.94</strong>ã€‚è¿™æ¯”æœ€åˆçš„
105.41<em>æ›´é«˜</em>ï¼Œè¿™æ˜¯é¢„æœŸçš„â€”â€”å¦‚æ­¤å¤šçš„å™ªå£°å˜é‡çš„å­˜åœ¨ä½¿å¾—é€‰æ‹©é—®é¢˜æ›´åŠ å›°éš¾ï¼Œä½†æœ€ç»ˆæ¨¡å‹ä»ç„¶å¾ˆå¥½ï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œ<em>å®ƒæˆåŠŸåœ°æ’é™¤äº†æ‰€æœ‰
20 ä¸ªå™ªå£°ç‰¹å¾</em>ã€‚</li>
</ul></li>
</ul>
<h2 id="conceptual-overview-the-why">Conceptual Overview: The â€œWhyâ€</h2>
<p>Slides cover <strong>Chapter 6: Linear Model Selection and
Regularization</strong>, which is all about a fundamental trade-off in
machine learning: the <strong>bias-variance trade-off</strong>.
è¯¥éƒ¨åˆ†ä¸»è¦è®¨è®ºæœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªåŸºæœ¬æƒè¡¡ï¼š<strong>åå·®-æ–¹å·®æƒè¡¡</strong>ã€‚</p>
<ul>
<li><p><strong>The Problem (Slide <code>...221320.png</code>):</strong>
Imagine you have a dataset with 50 predictors (<span
class="math inline">\(p=50\)</span>). You want to predict a response
<span class="math inline">\(y\)</span>. å‡è®¾ä½ æœ‰ä¸€ä¸ªåŒ…å« 50
ä¸ªé¢„æµ‹å˜é‡ï¼ˆp=50ï¼‰çš„æ•°æ®é›†ã€‚ä½ æƒ³è¦é¢„æµ‹å“åº” <span
class="math inline">\(y\)</span>ã€‚</p>
<ul>
<li><strong>Model 1 (Full Model):</strong> You use all 50 predictors.
This model is very <strong>flexible</strong>. It will fit the
<em>training data</em> extremely well, resulting in a low
<strong>bias</strong>. However, itâ€™s highly likely that many of those 50
predictors are just â€œnoiseâ€ (random, unrelated variables). By fitting to
this noise, the model will be <strong>overfit</strong>. When you show it
new, unseen data (the <em>test data</em>), it will perform poorly. This
is called <strong>high variance</strong>. ä½ ä½¿ç”¨äº†æ‰€æœ‰ 50
ä¸ªé¢„æµ‹å˜é‡ã€‚è¿™ä¸ªæ¨¡å‹éå¸¸<strong>çµæ´»</strong>ã€‚å®ƒèƒ½å¾ˆå¥½åœ°æ‹Ÿåˆ<em>è®­ç»ƒæ•°æ®</em>ï¼Œä»è€Œäº§ç”Ÿè¾ƒä½çš„<strong>åå·®</strong>ã€‚ç„¶è€Œï¼Œè¿™
50
ä¸ªé¢„æµ‹å˜é‡ä¸­å¾ˆå¯èƒ½æœ‰å¾ˆå¤šåªæ˜¯â€œå™ªå£°â€ï¼ˆéšæœºçš„ã€ä¸ç›¸å…³çš„å˜é‡ï¼‰ã€‚ç”±äºæ‹Ÿåˆè¿™äº›å™ªå£°ï¼Œæ¨¡å‹ä¼š<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚å½“ä½ å‘å®ƒå±•ç¤ºæ–°çš„ã€æœªè§è¿‡çš„æ•°æ®ï¼ˆ<em>æµ‹è¯•æ•°æ®</em>ï¼‰æ—¶ï¼Œå®ƒçš„è¡¨ç°ä¼šå¾ˆå·®ã€‚è¿™è¢«ç§°ä¸º<strong>é«˜æ–¹å·®</strong>ã€‚</li>
<li><strong>Model 2 (Subset Model):</strong> You intelligently select
only the 3 predictors (<span class="math inline">\(q=3\)</span>) that
are <em>actually</em> related to <span class="math inline">\(y\)</span>.
This model is less flexible. It wonâ€™t fit the <em>training data</em> as
perfectly as Model 1 (it has higher <strong>bias</strong>). But, because
itâ€™s <em>not</em> fitting the noise, it will generalize much better to
new data. It will have a much lower <strong>variance</strong>, and thus
a lower overall <em>test error</em>. ä½ æ™ºèƒ½åœ°åªé€‰æ‹©ä¸ <span
class="math inline">\(y\)</span> <em>çœŸæ­£</em>ç›¸å…³çš„ 3 ä¸ªé¢„æµ‹å˜é‡ (<span
class="math inline">\(q=3\)</span>)ã€‚è¿™ä¸ªæ¨¡å‹çš„çµæ´»æ€§è¾ƒå·®ã€‚å®ƒå¯¹
<em>è®­ç»ƒæ•°æ®</em> çš„æ‹Ÿåˆåº¦ä¸å¦‚æ¨¡å‹ 1
å®Œç¾ï¼ˆå®ƒçš„<strong>åå·®</strong>æ›´é«˜ï¼‰ã€‚ä½†æ˜¯ï¼Œç”±äºå®ƒå¯¹å™ªå£°çš„æ‹Ÿåˆåº¦æ›´é«˜ï¼Œå› æ­¤å¯¹æ–°æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ä¼šæ›´å¥½ã€‚å®ƒçš„<strong>æ–¹å·®</strong>ä¼šæ›´ä½ï¼Œå› æ­¤æ€»ä½“çš„<em>æµ‹è¯•è¯¯å·®</em>ä¹Ÿä¼šæ›´ä½ã€‚</li>
</ul></li>
<li><p><strong>The Goal:</strong> The goal is to find the model that has
the <strong>lowest test error</strong>. We need a formal method to
<em>find</em> the best subset (like Model 2) without just guessing.
<strong>ç›®æ ‡æ˜¯æ‰¾åˆ°</strong>æµ‹è¯•è¯¯å·®**æœ€ä½çš„æ¨¡å‹ã€‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ­£å¼çš„æ–¹æ³•æ¥<em>æ‰¾åˆ°</em>æœ€ä½³å­é›†ï¼ˆä¾‹å¦‚æ¨¡å‹
2ï¼‰ï¼Œè€Œä¸æ˜¯ä»…ä»…é çŒœæµ‹ã€‚</p></li>
<li><p><strong>Two Main Strategies (Slide
<code>...221314.png</code>):</strong></p>
<ol type="1">
<li><p><strong>Subset Selection (Section 6.1):</strong> This is what
weâ€™re focused on. Itâ€™s an â€œall-or-nothingâ€ approach. You either
<em>keep</em> a variable in the model or you <em>discard</em> it
completely. The â€œBest Subset Selectionâ€ algorithm is the most extreme,
â€œbrute-forceâ€ way to do this.
æ˜¯æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹ã€‚è¿™æ˜¯ä¸€ç§â€œå…¨æœ‰æˆ–å…¨æ— â€çš„æ–¹æ³•ã€‚ä½ è¦ä¹ˆåœ¨æ¨¡å‹ä¸­â€œä¿ç•™â€ä¸€ä¸ªå˜é‡ï¼Œè¦ä¹ˆâ€œå½»åº•ä¸¢å¼ƒâ€å®ƒã€‚â€œæœ€ä½³å­é›†é€‰æ‹©â€ç®—æ³•æ˜¯æœ€æç«¯ã€æœ€â€œæš´åŠ›â€çš„åšæ³•ã€‚</p></li>
<li><p><strong>Shrinkage/Regularization (Section 6.2):</strong> This is
a more subtle approach (e.g., Ridge Regression, LASSO). Instead of
discarding variables, you <em>keep all <span
class="math inline">\(p\)</span> variables</em> but add a penalty to the
model that â€œshrinksâ€ the coefficients (<span
class="math inline">\(\beta\)</span>) of the useless variables towards
zero.
è¿™æ˜¯ä¸€ç§æ›´å·§å¦™çš„æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œå²­å›å½’ã€LASSOï¼‰ã€‚ä½ ä¸æ˜¯ä¸¢å¼ƒå˜é‡ï¼Œè€Œæ˜¯<em>ä¿ç•™æ‰€æœ‰
<span class="math inline">\(p\)</span>
ä¸ªå˜é‡</em>ï¼Œä½†ä¼šç»™æ¨¡å‹æ·»åŠ ä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œå°†æ— ç”¨å˜é‡çš„ç³»æ•°ï¼ˆ<span
class="math inline">\(\beta\)</span>ï¼‰â€œæ”¶ç¼©â€åˆ°é›¶ã€‚</p></li>
</ol></li>
</ul>
<h2 id="questions">Questions ğŸ¯</h2>
<h3 id="q1-how-to-compare-which-model-is-better">Q1: â€œHow to compare
which model is better?â€</h3>
<p>(From slides <code>...221320.png</code> and
<code>...221326.png</code>)</p>
<p>This is the most important question. You <strong>cannot</strong> use
metrics based on <em>training data</em> (like <span
class="math inline">\(R^2\)</span> or RSS - Residual Sum of Squares) to
compare models with <em>different numbers of predictors</em>.
è¿™æ˜¯æœ€é‡è¦çš„é—®é¢˜ã€‚æ‚¨<strong>ä¸èƒ½</strong>ä½¿ç”¨åŸºäº<em>è®­ç»ƒæ•°æ®</em>çš„æŒ‡æ ‡ï¼ˆä¾‹å¦‚
R^2 æˆ– RSS - æ®‹å·®å¹³æ–¹å’Œï¼‰æ¥æ¯”è¾ƒå…·æœ‰<em>ä¸åŒæ•°é‡é¢„æµ‹å˜é‡</em>çš„æ¨¡å‹ã€‚</p>
<ul>
<li><p><strong>The Trap:</strong> A model with more predictors will
<em>always</em> have a higher <span class="math inline">\(R^2\)</span>
(or lower RSS) on the data it was trained on. <span
class="math inline">\(R^2\)</span> will <em>always</em> increase as you
add variables, even if they are pure noise. If you used <span
class="math inline">\(R^2\)</span> to compare a 3-predictor model to a
10-predictor model, the 10-predictor model would <em>always</em> look
better on paper, even if itâ€™s terribly overfit.
å…·æœ‰æ›´å¤šé¢„æµ‹å˜é‡çš„æ¨¡å‹åœ¨å…¶è®­ç»ƒæ•°æ®ä¸Š<em>æ€»æ˜¯</em>å…·æœ‰æ›´é«˜çš„
R^2ï¼ˆæˆ–æ›´ä½çš„ RSSï¼‰ã€‚éšç€å˜é‡çš„å¢åŠ ï¼ŒR^2
ä¼š<em>æ€»æ˜¯</em>å¢åŠ ï¼Œå³ä½¿è¿™äº›å˜é‡æ˜¯çº¯å™ªå£°ã€‚å¦‚æœæ‚¨ä½¿ç”¨ R^2 æ¥æ¯”è¾ƒ 3
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹å’Œ 10 ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œé‚£ä¹ˆ 10
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹åœ¨çº¸é¢ä¸Š<em>æ€»æ˜¯</em>çœ‹èµ·æ¥æ›´å¥½ï¼Œå³ä½¿å®ƒä¸¥é‡è¿‡æ‹Ÿåˆã€‚</p></li>
<li><p><strong>The Correct Way:</strong> You must use a metric that
estimates the <strong>test error</strong>. The slides and code show two
ways:æ‚¨å¿…é¡»ä½¿ç”¨ä¸€ä¸ªèƒ½å¤Ÿä¼°è®¡<strong>æµ‹è¯•è¯¯å·®</strong>çš„æŒ‡æ ‡ã€‚</p>
<ol type="1">
<li><strong>Cross-Validation (CV):</strong> This is the method used in
your Python code. It works by:
<ul>
<li>Splitting your training data into <span
class="math inline">\(k\)</span> â€œfoldsâ€ (e.g., 5 folds).
å°†è®­ç»ƒæ•°æ®æ‹†åˆ†æˆ <span class="math inline">\(k\)</span> ä¸ªâ€œæŠ˜å â€ï¼ˆä¾‹å¦‚ 5
ä¸ªæŠ˜å ï¼‰ã€‚</li>
<li>Training the model on 4 folds and testing it on the 5th fold.
ä½¿ç”¨å…¶ä¸­ 4 ä¸ªæŠ˜å è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ç¬¬ 5 ä¸ªæŠ˜å è¿›è¡Œæµ‹è¯•ã€‚</li>
<li>Repeating this 5 times, so each fold gets to be the test set once.
é‡å¤æ­¤æ“ä½œ 5 æ¬¡ï¼Œä½¿æ¯ä¸ªæŠ˜å éƒ½ä½œä¸ºæµ‹è¯•é›†ä¸€æ¬¡ã€‚</li>
<li>Averaging the 5 test errors. å¯¹ 5 ä¸ªæµ‹è¯•è¯¯å·®æ±‚å¹³å‡å€¼ã€‚ This gives
you a robust estimate of how your model will perform on <em>unseen
data</em>. You then choose the model with the best (lowest) average CV
error.
è¿™å¯ä»¥è®©ä½ å¯¹æ¨¡å‹åœ¨<em>æœªè§æ•°æ®</em>ä¸Šçš„è¡¨ç°æœ‰ä¸€ä¸ªç¨³å¥çš„ä¼°è®¡ã€‚ç„¶åï¼Œä½ å¯ä»¥é€‰æ‹©å¹³å‡
CV è¯¯å·®æœ€å°ï¼ˆæœ€ä½³ï¼‰çš„æ¨¡å‹ã€‚</li>
</ul></li>
<li><strong>Mathematical Adjustments (AIC, BIC, Adjusted <span
class="math inline">\(R^2\)</span>):</strong> These are formulas that
take the training error (like RSS) and add a <em>penalty</em> for each
predictor (<span class="math inline">\(k\)</span>) you add.
<ul>
<li><span class="math inline">\(AIC \approx RSS +
2k\sigma^2\)</span></li>
<li><span class="math inline">\(BIC \approx RSS +
\log(n)k\sigma^2\)</span> A model with more predictors (larger <span
class="math inline">\(k\)</span>) gets a bigger penalty. To be chosen, a
more complex model must <em>significantly</em> improve the RSS to
overcome this penalty. é¢„æµ‹å˜é‡è¶Šå¤šï¼ˆk
è¶Šå¤§ï¼‰çš„æ¨¡å‹ï¼Œæƒ©ç½šè¶Šå¤§ã€‚è¦è¢«é€‰ä¸­ï¼Œæ›´å¤æ‚çš„æ¨¡å‹å¿…é¡»<em>æ˜¾è‘—</em>æå‡ RSS
ä»¥å…‹æœæ­¤æƒ©ç½šã€‚</li>
</ul></li>
</ol></li>
</ul>
<h3 id="q2-why-using-r2-for-step-2">Q2: â€œWhy using <span
class="math inline">\(R^2\)</span> for step 2?â€</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p><strong>Step 2</strong> of the â€œBest Subset Selectionâ€ algorithm
says: â€œFor <span class="math inline">\(k = 1, \dots, p\)</span>: Fit all
<span class="math inline">\(\binom{p}{k}\)</span> modelsâ€¦ Pick the best
model, that with the largest <span class="math inline">\(R^2\)</span>, â€¦
and call it <span class="math inline">\(M_k\)</span>.â€ â€œå¯¹äº <span
class="math inline">\(k = 1, \dots, p\)</span>ï¼šæ‹Ÿåˆæ‰€æœ‰ <span
class="math inline">\(\binom{p}{k}\)</span> ä¸ªæ¨¡å‹â€¦â€¦é€‰æ‹©å…·æœ‰æœ€å¤§ <span
class="math inline">\(R^2\)</span> çš„æœ€ä½³æ¨¡å‹â€¦â€¦å¹¶å°†å…¶å‘½åä¸º <span
class="math inline">\(M_k\)</span>ã€‚â€</p>
<ul>
<li><strong>The Reason:</strong> In Step 2, you are <em>only</em>
comparing models <strong>of the same size</strong>. For example, when
<span class="math inline">\(k=3\)</span>, you are comparing all possible
3-predictor models: æ­¥éª¤ 2
ä¸­ï¼Œæ‚¨<em>ä»…</em>æ¯”è¾ƒ**ç›¸åŒå¤§å°çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå½“ <span
class="math inline">\(k=3\)</span> æ—¶ï¼Œæ‚¨å°†æ¯”è¾ƒæ‰€æœ‰å¯èƒ½çš„ 3
é¢„æµ‹å˜é‡æ¨¡å‹ï¼š
<ul>
<li>Model A: (<span class="math inline">\(X_1, X_2, X_3\)</span>)</li>
<li>Model B: (<span class="math inline">\(X_1, X_2, X_4\)</span>)</li>
<li>Model C: (<span class="math inline">\(X_1, X_3, X_5\)</span>)</li>
<li>â€¦and so on.</li>
</ul>
Since all these models have the <em>exact same complexity</em> (they all
have <span class="math inline">\(k=3\)</span> predictors), there is no
risk of unfairly favoring a more complex model. Therefore, you are free
to use a training metric like <span class="math inline">\(R^2\)</span>
(or RSS). The model with the highest <span
class="math inline">\(R^2\)</span> is, by definition, the one that
<em>best fits the training data</em> for that specific size <span
class="math inline">\(k\)</span>.
ç”±äºæ‰€æœ‰è¿™äº›æ¨¡å‹éƒ½å…·æœ‰<em>å®Œå…¨ç›¸åŒçš„å¤æ‚åº¦</em>ï¼ˆå®ƒä»¬éƒ½å…·æœ‰ <span
class="math inline">\(k=3\)</span>
ä¸ªé¢„æµ‹å˜é‡ï¼‰ï¼Œå› æ­¤ä¸å­˜åœ¨ä¸å…¬å¹³åœ°åå‘æ›´å¤æ‚æ¨¡å‹çš„é£é™©ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥è‡ªç”±ä½¿ç”¨åƒ
<span class="math inline">\(R^2\)</span>ï¼ˆæˆ–
RSSï¼‰è¿™æ ·çš„è®­ç»ƒæŒ‡æ ‡ã€‚æ ¹æ®å®šä¹‰ï¼Œå…·æœ‰æœ€é«˜ <span
class="math inline">\(R^2\)</span> çš„æ¨¡å‹å°±æ˜¯åœ¨ç‰¹å®šå¤§å° <span
class="math inline">\(k\)</span>
ä¸‹<em>ä¸è®­ç»ƒæ•°æ®æ‹Ÿåˆåº¦</em>æœ€é«˜çš„æ¨¡å‹ã€‚</li>
</ul>
<h3
id="q3-cannot-use-training-error-in-step-3.-why-not-æ­¥éª¤-3-ä¸­ä¸èƒ½ä½¿ç”¨è®­ç»ƒè¯¯å·®-ä¸ºä»€ä¹ˆ">Q3:
â€œCannot use training error in Step 3.â€ Why not? â€œæ­¥éª¤ 3
ä¸­ä¸èƒ½ä½¿ç”¨è®­ç»ƒè¯¯å·®ã€‚â€ ä¸ºä»€ä¹ˆï¼Ÿ</h3>
<p>(From slide <code>...221333.png</code>)</p>
<p><strong>Step 3</strong> says: â€œSelect a single best model from <span
class="math inline">\(M_0, M_1, \dots, M_p\)</span> by cross validation,
AIC, or BIC.â€â€œé€šè¿‡äº¤å‰éªŒè¯ã€AIC æˆ– BICï¼Œä» <span
class="math inline">\(M_0ã€M_1ã€\dotsã€M_p\)</span>
ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä½³æ¨¡å‹ã€‚â€</p>
<ul>
<li><p><strong>The Reason:</strong> In Step 3, you are now comparing
models <strong>of different sizes</strong>. You are comparing the best
1-predictor model (<span class="math inline">\(M_1\)</span>) vs.Â the
best 2-predictor model (<span class="math inline">\(M_2\)</span>)
vs.Â the best 3-predictor model (<span
class="math inline">\(M_3\)</span>), and so on, all the way up to <span
class="math inline">\(M_p\)</span>. åœ¨æ­¥éª¤ 3
ä¸­ï¼Œæ‚¨æ­£åœ¨æ¯”è¾ƒ<strong>ä¸åŒå¤§å°</strong>çš„æ¨¡å‹ã€‚æ‚¨æ­£åœ¨æ¯”è¾ƒæœ€ä½³çš„å•é¢„æµ‹æ¨¡å‹
(<span class="math inline">\(M_1\)</span>)ã€æœ€ä½³çš„åŒé¢„æµ‹æ¨¡å‹ (<span
class="math inline">\(M_2\)</span>) å’Œæœ€ä½³çš„ä¸‰é¢„æµ‹æ¨¡å‹ (<span
class="math inline">\(M_3\)</span>)ï¼Œä¾æ­¤ç±»æ¨ï¼Œç›´åˆ° <span
class="math inline">\(M_p\)</span>ã€‚</p>
<p>As explained in Q1, if you used a training error metric like <span
class="math inline">\(R^2\)</span> here, the <span
class="math inline">\(R^2\)</span> would just keep going up, and you
would <em>always</em> select the largest, most complex model, <span
class="math inline">\(M_p\)</span>. This completely defeats the purpose
of model selection. å¦‚é—®é¢˜ 1 æ‰€è¿°ï¼Œå¦‚æœæ‚¨åœ¨æ­¤å¤„ä½¿ç”¨åƒ <span
class="math inline">\(R^2\)</span> è¿™æ ·çš„è®­ç»ƒè¯¯å·®æŒ‡æ ‡ï¼Œé‚£ä¹ˆ <span
class="math inline">\(R^2\)</span>
ä¼šæŒç»­ä¸Šå‡ï¼Œå¹¶ä¸”æ‚¨<em>æ€»æ˜¯</em>ä¼šé€‰æ‹©æœ€å¤§ã€æœ€å¤æ‚çš„æ¨¡å‹ <span
class="math inline">\(M_p\)</span>ã€‚è¿™å®Œå…¨è¿èƒŒäº†æ¨¡å‹é€‰æ‹©çš„ç›®çš„ã€‚</p>
<p>Therefore, in Step 3, you <em>must</em> use a method that estimates
<strong>test error</strong> (like Cross-Validation) or one that
<strong>penalizes for complexity</strong> (like AIC or BIC) to find the
â€œsweet spotâ€ model that balances fit and simplicity. å› æ­¤ï¼Œåœ¨æ­¥éª¤ 3
ä¸­ï¼Œæ‚¨<em>å¿…é¡»</em>ä½¿ç”¨ä¸€ç§ä¼°ç®—<strong>æµ‹è¯•è¯¯å·®</strong>çš„æ–¹æ³•ï¼ˆä¾‹å¦‚äº¤å‰éªŒè¯ï¼‰æˆ–<strong>æƒ©ç½šå¤æ‚æ€§</strong>çš„æ–¹æ³•ï¼ˆä¾‹å¦‚
AIC æˆ–
BICï¼‰ï¼Œä»¥æ‰¾åˆ°åœ¨æ‹Ÿåˆåº¦å’Œç®€å•æ€§ä¹‹é—´å–å¾—å¹³è¡¡çš„â€œæœ€ä½³ç‚¹â€æ¨¡å‹ã€‚</p></li>
</ul>
<h2 id="mathematical-deep-dive">Mathematical Deep Dive ğŸ§®</h2>
<ul>
<li><strong><span class="math inline">\(Y = \beta_0 + \beta_1X_1 + \dots
+ \beta_pX_p + \epsilon\)</span>:</strong> The full linear model. The
goal of subset selection is to find a subset of <span
class="math inline">\(X_j\)</span>â€™s where <span
class="math inline">\(\beta_j \neq 0\)</span> and set all other <span
class="math inline">\(\beta\)</span>â€™s to 0.
å®Œæ•´çš„çº¿æ€§æ¨¡å‹ã€‚å­é›†é€‰æ‹©çš„ç›®æ ‡æ˜¯æ‰¾åˆ° <span
class="math inline">\(X_j\)</span> çš„ä¸€ä¸ªå­é›†ï¼Œå…¶ä¸­ $_j ç­‰äº
0ï¼Œå¹¶å°†æ‰€æœ‰å…¶ä»– <span class="math inline">\(\beta\)</span> è®¾ç½®ä¸º
0ã€‚</li>
<li><strong><span class="math inline">\(2^p\)</span>
combinations:</strong> (Slide <code>...221333.png</code>) This is the
total number of models you have to check. For each of the <span
class="math inline">\(p\)</span> variables, you have two choices: either
it is <strong>IN</strong> the model or it is
<strong>OUT</strong>.è¿™æ˜¯ä½ éœ€è¦æ£€æŸ¥çš„æ¨¡å‹æ€»æ•°ã€‚å¯¹äºæ¯ä¸ª <span
class="math inline">\(p\)</span>
ä¸ªå˜é‡ï¼Œä½ æœ‰ä¸¤ä¸ªé€‰æ‹©ï¼šè¦ä¹ˆå®ƒåœ¨æ¨¡å‹<strong>å†…éƒ¨</strong>ï¼Œè¦ä¹ˆå®ƒåœ¨æ¨¡å‹<strong>å¤–éƒ¨</strong>ã€‚
<ul>
<li>Example: <span class="math inline">\(p=3\)</span> (variables <span
class="math inline">\(X_1, X_2, X_3\)</span>)</li>
<li>The <span class="math inline">\(2^3 = 8\)</span> possible models
are:
<ol type="1">
<li>{} (The null model, <span class="math inline">\(M_0\)</span>)</li>
<li>{ <span class="math inline">\(X_1\)</span> }</li>
<li>{ <span class="math inline">\(X_2\)</span> }</li>
<li>{ <span class="math inline">\(X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_2\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_2, X_3\)</span> }</li>
<li>{ <span class="math inline">\(X_1, X_2, X_3\)</span> } (The full
model, <span class="math inline">\(M_3\)</span>)</li>
</ol></li>
<li>This is why this method is called an <strong>â€œexhaustive
searchâ€</strong>. It literally checks every single one. For <span
class="math inline">\(p=20\)</span>, <span
class="math inline">\(2^{20}\)</span> is over a million
models!è¿™å°±æ˜¯è¯¥æ–¹æ³•è¢«ç§°ä¸º<strong>â€œç©·ä¸¾æœç´¢â€</strong>çš„åŸå› ã€‚å®ƒå®é™…ä¸Šä¼šæ£€æŸ¥æ¯ä¸€ä¸ªæ¨¡å‹ã€‚å¯¹äº
<span class="math inline">\(p=20\)</span>ï¼Œ<span
class="math inline">\(2^{20}\)</span> å°±è¶…è¿‡ä¸€ç™¾ä¸‡ä¸ªæ¨¡å‹ï¼</li>
</ul></li>
<li><strong><span class="math inline">\(\binom{p}{k} =
\frac{p!}{k!(p-k)!}\)</span>:</strong> (Slide
<code>...221333.png</code>) This is the â€œcombinationsâ€ formula. It tells
you <em>how many</em> models you fit <em>in Step 2</em> for a specific
<span
class="math inline">\(k\)</span>.è¿™æ˜¯â€œç»„åˆâ€å…¬å¼ã€‚å®ƒå‘Šè¯‰ä½ ï¼Œå¯¹äºç‰¹å®šçš„
<span class="math inline">\(k\)</span>ï¼Œ<em>åœ¨æ­¥éª¤ 2</em>ä¸­ï¼Œä½ æ‹Ÿåˆäº†
<em>å¤šå°‘</em> ä¸ªæ¨¡å‹ã€‚
<ul>
<li>Example: <span class="math inline">\(p=10\)</span> total
predictors.</li>
<li>For <span class="math inline">\(k=1\)</span>: You fit <span
class="math inline">\(\binom{10}{1} = 10\)</span> models.</li>
<li>For <span class="math inline">\(k=2\)</span>: You fit <span
class="math inline">\(\binom{10}{2} = \frac{10 \times 9}{2 \times 1} =
45\)</span> models.</li>
<li>For <span class="math inline">\(k=3\)</span>: You fit <span
class="math inline">\(\binom{10}{3} = \frac{10 \times 9 \times 8}{3
\times 2 \times 1} = 120\)</span> models.</li>
<li>â€¦and so on. The sum of all these <span
class="math inline">\(\binom{p}{k}\)</span> from <span
class="math inline">\(k=0\)</span> to <span
class="math inline">\(k=p\)</span> equals <span
class="math inline">\(2^p\)</span>.</li>
</ul></li>
</ul>
<h2 id="detailed-code-analysis">Detailed Code Analysis ğŸ’»</h2>
<p>Your slides show Python code that applies the <strong>Best Subset
Selection algorithm</strong> to a <strong>KNN Regressor</strong>. This
is a great example of how the <em>selection algorithm</em> is
independent of the <em>model type</em> (as mentioned in slide
<code>...221314.png</code>).</p>
<h3 id="key-functions-1">Key Functions</h3>
<ul>
<li><strong><code>main()</code></strong>
<ol type="1">
<li><strong>Load &amp; Preprocess:</strong> Reads
<code>Credit.csv</code>. The most important step here is converting
categorical text (like â€˜Maleâ€™/â€˜Femaleâ€™) into numbers (1/0).</li>
<li><strong>Scale Data:</strong> <code>scaler = StandardScaler()</code>
and <code>X_scaled = scaler.fit_transform(X)</code>.
<ul>
<li><strong>WHY?</strong> This is <strong>CRITICAL</strong> for KNN. KNN
works by measuring distance. If â€˜Incomeâ€™ (e.g., 50,000) is on a vastly
different scale than â€˜Cardsâ€™ (e.g., 3), the â€˜Incomeâ€™ feature will
completely dominate the distance calculation, making â€˜Cardsâ€™ irrelevant.
Scaling resizes all features to have a mean of 0 and standard deviation
of 1, so they all contribute fairly.</li>
</ul></li>
<li><strong>Handle Noisy Data (Slide
<code>...221303.jpg</code>):</strong> This version of the code
<em>intentionally</em> adds 20 columns of useless, random numbers. This
is a test to see if the algorithm is smart enough to ignore them.</li>
<li><strong>Run Selection:</strong>
<code>results_df = best_subset_selection_parallel(...)</code>. This
function does all the heavy lifting (explained next).</li>
<li><strong>Find Best Model:</strong>
<code>results_df.sort_values(by='CV_Score', ascending=False)</code>.
<ul>
<li><strong>WHY <code>ascending=False</code>?</strong> The code uses the
metric <code>'neg_mean_squared_error'</code>. This is MSE, but negative
(e.g., -15000). A <em>better</em> model has an error closer to 0 (e.g.,
-10000). Since -10000 is <em>greater than</em> -15000, you sort in
descending (high-to-low) order to put the best models at the top.</li>
</ul></li>
<li><strong>Final Evaluation (Step 3):</strong>
<code>final_scores = cross_val_score(knn, X_best, y, ...)</code>
<ul>
<li>This is the implementation of Step 3. It takes <em>only</em> the
single best subset (<code>X_best</code>) and runs a <em>new</em>
cross-validation on it. This gives a final, unbiased estimate of how
good that one model is.</li>
</ul></li>
<li><strong>Print RMSE:</strong>
<code>final_rmse = np.sqrt(-final_scores)</code>. It converts the
negative MSE back into a positive RMSE (Root Mean Squared Error), which
is in the same units as the target <span
class="math inline">\(y\)</span> (in this case, â€˜Balanceâ€™ in
dollars).</li>
</ol></li>
<li><strong><code>best_subset_selection_parallel(model, ...)</code></strong>
<ol type="1">
<li>This is the â€œmanagerâ€ function. It implements the loop from Step
2.</li>
<li><code>for k in range(1, n_features + 1):</code> This is the loop
â€œFor <span class="math inline">\(k = 1, \dots, p\)</span>â€.</li>
<li><code>subsets = list(combinations(feature_names, k))</code>: This
generates the <span class="math inline">\(\binom{p}{k}\)</span>
combinations for the current <span
class="math inline">\(k\)</span>.</li>
<li><code>results = Parallel(n_jobs=n_jobs)(...)</code>: This is a
non-core, â€œspeed-upâ€ command. It uses the <code>joblib</code> library to
run the evaluations on all your computerâ€™s CPU cores at once (in
parallel). Without this, checking millions of models would take
days.</li>
<li><code>subset_scores = ... [delayed(evaluate_subset)(...) ...]</code>
This line farms out the <em>actual work</em> to the
<code>evaluate_subset</code> function for every single subset.</li>
</ol></li>
<li><strong><code>evaluate_subset(subset, ...)</code></strong>
<ol type="1">
<li>This is the â€œworkerâ€ function. It gets called thousands or millions
of times.</li>
<li>Its job is to evaluate <em>one single subset</em> (e.g.,
<code>('Income', 'Limit', 'Student')</code>).</li>
<li><code>X_subset = X[list(subset)]</code>: It slices the data to get
<em>only</em> these columns.</li>
<li><code>scores = cross_val_score(model, X_subset, ...)</code>:
<strong>This is the most important line.</strong> It takes the subset
and performs a full 5-fold cross-validation on it.</li>
<li><code>return (subset, np.mean(scores))</code>: It returns the subset
and its average CV score.</li>
</ol></li>
</ul>
<h3 id="summary-of-outputs-slides-...221255.png-...221309.png">Summary
of Outputs (Slides <code>...221255.png</code> &amp;
<code>...221309.png</code>)</h3>
<ul>
<li><strong>Original Data (Slide <code>...221255.png</code>):</strong>
<ul>
<li><strong>Best Subset:</strong>
<code>('Income', 'Limit', 'Rating', 'Student')</code></li>
<li><strong>Final RMSE:</strong> ~105.4</li>
</ul></li>
<li><strong>Data with 20 â€œNoisyâ€ Variables (Slide
<code>...221309.png</code>):</strong>
<ul>
<li><strong>Best Subset:</strong>
<code>('Income', 'Limit', 'Student')</code></li>
<li><strong>Result:</strong> The algorithm <em>successfully</em>
identified that all 20 â€œNoisyâ€ variables were useless and
<strong>excluded every single one of them</strong> from the best
models.</li>
<li><strong>Final RMSE:</strong> ~114.9</li>
<li><strong>Key Takeaway:</strong> The RMSE is slightly higher, which
makes sense because the selection problem was much harder. But the
<em>method worked perfectly</em>. It filtered all the â€œnoiseâ€ and found
a simple, powerful model, just as the theory on slide
<code>...221320.png</code> predicted.</li>
</ul></li>
</ul>
<h1
id="the-core-problem-training-error-vs.-test-error-æ ¸å¿ƒé—®é¢˜è®­ç»ƒè¯¯å·®-vs.-æµ‹è¯•è¯¯å·®">2.
The Core Problem: Training Error vs.Â Test Error æ ¸å¿ƒé—®é¢˜ï¼šè®­ç»ƒè¯¯å·®
vs.Â æµ‹è¯•è¯¯å·®</h1>
<p>The central theme of these slides is finding the â€œbestâ€ model. The
problem is that a model with more predictors (more complex) will
<em>always</em> fit the data it was trained on better. This is a trap.
å¯»æ‰¾â€œæœ€ä½³â€æ¨¡å‹ã€‚é—®é¢˜åœ¨äºï¼Œé¢„æµ‹å› å­è¶Šå¤šï¼ˆè¶Šå¤æ‚ï¼‰çš„æ¨¡å‹<em>æ€»æ˜¯</em>èƒ½æ›´å¥½åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚è¿™æ˜¯ä¸€ä¸ªé™·é˜±ã€‚</p>
<ul>
<li><strong>Training Error:</strong> How well the model fits the data we
used to build it. <strong><span class="math inline">\(R^2\)</span> and
<span class="math inline">\(RSS\)</span> measure this.</strong>
æ¨¡å‹ä¸æˆ‘ä»¬æ„å»ºæ¨¡å‹æ—¶æ‰€ç”¨æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ã€‚<strong><span
class="math inline">\(R^2\)</span> å’Œ <span
class="math inline">\(RSS\)</span> è¡¡é‡äº†è¿™ä¸€ç‚¹ã€‚</strong></li>
<li><strong>Test Error:</strong> How well the model predicts new, unseen
data. This is what we <em>actually</em> care about. A model that is too
complex (e.g., has 10 predictors when only 3 are useful) will have low
training error but very high test error. This is called
<strong>overfitting</strong>.
æ¨¡å‹é¢„æµ‹æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®çš„å‡†ç¡®ç¨‹åº¦ã€‚è¿™æ‰æ˜¯æˆ‘ä»¬<em>çœŸæ­£</em>å…³å¿ƒçš„ã€‚è¿‡äºå¤æ‚çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œæœ‰
10 ä¸ªé¢„æµ‹å› å­ï¼Œä½†åªæœ‰ 3
ä¸ªæœ‰ç”¨ï¼‰çš„è®­ç»ƒè¯¯å·®ä¼šå¾ˆä½ï¼Œä½†æµ‹è¯•è¯¯å·®ä¼šå¾ˆé«˜ã€‚è¿™è¢«ç§°ä¸º<strong>è¿‡æ‹Ÿåˆ</strong>ã€‚</li>
</ul>
<p>The goal is to choose a model that has the lowest <em>test
error</em>. The metrics below (Adjusted <span
class="math inline">\(R^2\)</span>, AIC, BIC) are all attempts to
<em>estimate</em> this test error without having to actually collect new
data. They do this by adding a <strong>penalty</strong> for complexity.
ç›®æ ‡æ˜¯é€‰æ‹©ä¸€ä¸ªå…·æœ‰æœ€ä½<em>æµ‹è¯•è¯¯å·®</em>çš„æ¨¡å‹ã€‚ä»¥ä¸‹æŒ‡æ ‡ï¼ˆè°ƒæ•´åçš„ <span
class="math inline">\(R^2\)</span>ã€AICã€BICï¼‰éƒ½æ˜¯åœ¨æ— éœ€å®é™…æ”¶é›†æ–°æ•°æ®çš„æƒ…å†µä¸‹å°è¯•<em>ä¼°è®¡</em>æ­¤æµ‹è¯•è¯¯å·®ã€‚ä»–ä»¬é€šè¿‡å¢åŠ <strong>å¤æ‚åº¦æƒ©ç½š</strong>æ¥å®ç°è¿™ä¸€ç‚¹ã€‚</p>
<h2 id="basic-metrics-measures-of-fit">Basic Metrics (Measures of
Fit)</h2>
<p>These formulas from slide 13 describe how well a model fits the
<em>training data</em>.</p>
<h3 id="residue-error-æ®‹å·®è¯¯å·®">Residue (Error) æ®‹å·®ï¼ˆè¯¯å·®ï¼‰</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\epsilon}_i = y_i - \hat{y}_i = y_i -
\hat{\beta}_0 - \sum_{j=1}^{p} \hat{\beta}_j x_{ij}\)</span></li>
<li><strong>Concept:</strong> This is the most basic building block.
Itâ€™s the difference between the <em>actual</em> observed value (<span
class="math inline">\(y_i\)</span>) and the value your model
<em>predicted</em> (<span class="math inline">\(\hat{y}_i\)</span>). It
is the â€œerrorâ€ for a single data point.
è¿™æ˜¯æœ€åŸºæœ¬çš„æ„å»ºå—ã€‚å®ƒæ˜¯<em>å®é™…</em>è§‚æµ‹å€¼ (<span
class="math inline">\(y_i\)</span>) ä¸æ¨¡å‹*é¢„æµ‹å€¼ (<span
class="math inline">\(\hat{y}_i\)</span>)
ä¹‹é—´çš„å·®å€¼ã€‚å®ƒæ˜¯å•ä¸ªæ•°æ®ç‚¹çš„â€œè¯¯å·®â€ã€‚</li>
</ul>
<h3 id="residual-sum-of-squares-rss-æ®‹å·®å¹³æ–¹å’Œ-rss">Residual Sum of
Squares (RSS) æ®‹å·®å¹³æ–¹å’Œ (RSS)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(RSS =
\sum_{i=1}^{n} \hat{\epsilon}_i^2\)</span></li>
<li><strong>Concept:</strong> This is the overall measure of model
error. You square all the individual errors (residues) to make them
positive and then add them all up.
è¿™æ˜¯æ¨¡å‹è¯¯å·®çš„æ€»ä½“åº¦é‡ã€‚å°†æ‰€æœ‰å•ä¸ªè¯¯å·®ï¼ˆæ®‹å·®ï¼‰å¹³æ–¹ï¼Œä½¿å…¶ä¸ºæ­£ï¼Œç„¶åå°†å®ƒä»¬å…¨éƒ¨ç›¸åŠ ã€‚</li>
<li><strong>Goal:</strong> The entire process of linear regression
(called â€œOrdinary Least Squaresâ€) is designed to find the <span
class="math inline">\(\hat{\beta}\)</span> coefficients that make this
<strong>RSS value as small as possible</strong>.
æ•´ä¸ªçº¿æ€§å›å½’è¿‡ç¨‹ï¼ˆç§°ä¸ºâ€œæ™®é€šæœ€å°äºŒä¹˜æ³•â€ï¼‰æ—¨åœ¨æ‰¾åˆ°ä½¿<strong>RSS
å€¼å°½å¯èƒ½å°</strong>çš„ <span class="math inline">\(\hat{\beta}\)</span>
ä¸ªç³»æ•°ã€‚</li>
<li><strong>The Flaw ç¼ºé™·:</strong> <span
class="math inline">\(RSS\)</span> will <em>always</em> decrease (or
stay the same) as you add more predictors (<span
class="math inline">\(p\)</span>). A model with all 10 predictors will
have a lower <span class="math inline">\(RSS\)</span> than a model with
9, even if that 10th predictor is useless. Therefore, <span
class="math inline">\(RSS\)</span> is useless for choosing
<em>between</em> models of different sizes. éšç€é¢„æµ‹å˜é‡ (<span
class="math inline">\(p\)</span>) çš„å¢åŠ ï¼Œ<span
class="math inline">\(RSS\)</span>
æ€»æ˜¯ä¼šå‡å°ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ã€‚ä¸€ä¸ªåŒ…å«æ‰€æœ‰ 10 ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹çš„ <span
class="math inline">\(RSS\)</span> ä¼šä½äºä¸€ä¸ªåŒ…å« 9
ä¸ªé¢„æµ‹å˜é‡çš„æ¨¡å‹ï¼Œå³ä½¿ç¬¬ 10 ä¸ªé¢„æµ‹å˜é‡æ¯«æ— ç”¨å¤„ã€‚å› æ­¤ï¼Œ<span
class="math inline">\(RSS\)</span>
å¯¹äºåœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¹‹é—´è¿›è¡Œé€‰æ‹©æ¯«æ— ç”¨å¤„ã€‚</li>
</ul>
<h3 id="r-squared-r2">R-squared (<span
class="math inline">\(R^2\)</span>)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(R^2 = 1 -
\frac{SS_{error}}{SS_{total}} = 1 - \frac{RSS}{\sum_{i=1}^{n} (y_i -
\bar{y})^2}\)</span></li>
<li><strong>Concept:</strong> This metric reframes <span
class="math inline">\(RSS\)</span> into a more interpretable
percentage.æ­¤æŒ‡æ ‡å°† <span class="math inline">\(RSS\)</span>
é‡æ–°å®šä¹‰ä¸ºæ›´æ˜“äºè§£é‡Šçš„ç™¾åˆ†æ¯”ã€‚
<ul>
<li><span class="math inline">\(SS_{total}\)</span> (the denominator)
represents the <em>total variance</em> of the data. Itâ€™s the error you
would get if your â€œmodelâ€ was just guessing the average value (<span
class="math inline">\(\bar{y}\)</span>) for every single observation.
ï¼ˆåˆ†æ¯ï¼‰è¡¨ç¤ºæ•°æ®çš„<em>æ€»æ–¹å·®</em>ã€‚å¦‚æœä½ çš„â€œæ¨¡å‹â€åªæ˜¯çŒœæµ‹æ¯ä¸ªè§‚æµ‹å€¼çš„å¹³å‡å€¼
(<span
class="math inline">\(\bar{y}\)</span>)ï¼Œé‚£ä¹ˆä½ å°±ä¼šå¾—åˆ°è¿™ä¸ªè¯¯å·®ã€‚</li>
<li><span class="math inline">\(SS_{error}\)</span> (the <span
class="math inline">\(RSS\)</span>) is the error <em>after</em> using
your model. æ˜¯â€œæ¨¡å‹è§£é‡Šçš„æ€»æ–¹å·®çš„æ¯”ä¾‹â€ã€‚ <span
class="math inline">\(R^2\)</span> ä¸º 0.75
æ„å‘³ç€ä½ çš„æ¨¡å‹å¯ä»¥è§£é‡Šå“åº”å˜é‡ 75% çš„å˜å¼‚ã€‚</li>
<li><span class="math inline">\(R^2\)</span> is the â€œproportion of total
variance explained by the model.â€ An <span
class="math inline">\(R^2\)</span> of 0.75 means your model can explain
75% of the variation in the response variable.</li>
</ul></li>
<li><strong>The Flaw ç¼ºé™·:</strong> Just like <span
class="math inline">\(RSS\)</span>, <span
class="math inline">\(R^2\)</span> will <em>always</em> increase (or
stay the same) as you add more predictors. This is visually confirmed in
Figure 6.1, where the red line for <span
class="math inline">\(R^2\)</span> only goes up. It will always pick the
most complex model. ä¸ <span class="math inline">\(RSS\)</span>
ä¸€æ ·ï¼Œéšç€é¢„æµ‹å˜é‡çš„å¢åŠ ï¼Œ<span class="math inline">\(R^2\)</span>
ä¼š<em>å§‹ç»ˆ</em>å¢åŠ ï¼ˆæˆ–ä¿æŒä¸å˜ï¼‰ã€‚å›¾ 6.1 ç›´è§‚åœ°è¯å®äº†è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­ <span
class="math inline">\(R^2\)</span>
çš„çº¢çº¿åªä¼šä¸Šå‡ã€‚å®ƒæ€»æ˜¯ä¼šé€‰æ‹©æœ€å¤æ‚çš„æ¨¡å‹ã€‚</li>
</ul>
<h2
id="advanced-metrics-for-model-selection-é«˜çº§æŒ‡æ ‡ç”¨äºæ¨¡å‹é€‰æ‹©">Advanced
Metrics (For Model Selection) é«˜çº§æŒ‡æ ‡ï¼ˆç”¨äºæ¨¡å‹é€‰æ‹©ï¼‰</h2>
<p>These metrics â€œfixâ€ the flaw of <span
class="math inline">\(R^2\)</span> by including a penalty for the number
of predictors.</p>
<h3 id="adjusted-r2">Adjusted <span
class="math inline">\(R^2\)</span></h3>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
  \text{Adjusted } R^2 = 1 - \frac{RSS / (n - p - 1)}{SS_{total} / (n -
1)}
  \]</span></li>
<li><strong>Mathematical Concept:</strong> This formula replaces the
â€œSum of Squaresâ€ (<span class="math inline">\(SS\)</span>) with â€œMean
Squaresâ€ (<span class="math inline">\(MS\)</span>).
<ul>
<li><span class="math inline">\(MS_{error} =
\frac{RSS}{n-p-1}\)</span></li>
<li><span class="math inline">\(MS_{total} =
\frac{SS_{total}}{n-1}\)</span></li>
</ul></li>
<li><strong>The â€œPenaltyâ€ Explained:</strong> The penalty is
<strong>degrees of freedom</strong>.
<ul>
<li><span class="math inline">\(n\)</span> = number of data points.</li>
<li><span class="math inline">\(p\)</span> = number of predictors.</li>
<li>The term <span class="math inline">\(n-p-1\)</span> is the degrees
of freedom for the residuals. You start with <span
class="math inline">\(n\)</span> data points, but you â€œuse upâ€ one
degree of freedom to estimate the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) and <span
class="math inline">\(p\)</span> more to estimate the <span
class="math inline">\(p\)</span> slopes.</li>
</ul></li>
<li><strong>How it Works:</strong>
<ol type="1">
<li>When you add a new predictor (increase <span
class="math inline">\(p\)</span>), <span
class="math inline">\(RSS\)</span> goes down, which makes the numerator
(<span class="math inline">\(MS_{error}\)</span>) smaller.</li>
<li>â€¦But, increasing <span class="math inline">\(p\)</span>
<em>also</em> decreases the denominator (<span
class="math inline">\(n-p-1\)</span>), which makes the numerator (<span
class="math inline">\(MS_{error}\)</span>) <em>larger</em>.</li>
</ol>
<ul>
<li>This creates a â€œtug-of-war.â€ If the new predictor is
<strong>useful</strong>, it will drop <span
class="math inline">\(RSS\)</span> a lot, and Adjusted <span
class="math inline">\(R^2\)</span> will <strong>increase</strong>. If
the new predictor is <strong>useless</strong>, <span
class="math inline">\(RSS\)</span> will barely change, and the penalty
from decreasing the denominator will win, causing Adjusted <span
class="math inline">\(R^2\)</span> to <strong>decrease</strong>.</li>
</ul></li>
<li><strong>Goal:</strong> You select the model with the
<strong>highest</strong> Adjusted <span
class="math inline">\(R^2\)</span>.</li>
</ul>
<h3 id="akaike-information-criterion-aic">Akaike Information Criterion
(AIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Concept Breakdown:</strong>
<ul>
<li><span class="math inline">\(\ell(\hat{\theta})\)</span>: This is the
<strong>Maximized Likelihood Function</strong>.
<ul>
<li>The <strong>Likelihood Function</strong> <span
class="math inline">\(\ell(\theta)\)</span> asks: â€œGiven a set of model
parameters <span class="math inline">\(\theta\)</span>, how probable is
the data we observed?â€</li>
<li>The <strong>Maximum Likelihood Estimate (MLE)</strong> <span
class="math inline">\(\hat{\theta}\)</span> is the specific set of
parameters (the <span class="math inline">\(\hat{\beta}\)</span>â€™s) that
<em>maximizes</em> this probability.</li>
</ul></li>
<li><span class="math inline">\(\log \ell(\hat{\theta})\)</span>: The
<strong>log-likelihood</strong>. This is just a number that represents
the <em>best possible fit</em> the model can achieve for the data. A
higher number is a better fit.</li>
<li><span class="math inline">\(-2 \log \ell(\hat{\theta})\)</span>:
This is the <strong>Deviance</strong>. Since a higher log-likelihood is
better, a <em>lower</em> deviance is better. This term measures
<strong>poorness-of-fit</strong>.</li>
<li><span class="math inline">\(d\)</span>: The number of parameters
estimated by the model. (e.g., <span class="math inline">\(p\)</span>
predictors + 1 intercept).</li>
<li><span class="math inline">\(2d\)</span>: This is the <strong>Penalty
Term</strong>.</li>
</ul></li>
<li><strong>How it Works:</strong> <span class="math inline">\(AIC =
(\text{Poorness-of-Fit}) + (\text{Complexity Penalty})\)</span>. As you
add predictors, the fit gets better (the deviance term goes down), but
the penalty term (<span class="math inline">\(2d\)</span>) goes up.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> AIC.</li>
</ul>
<h3 id="bayesian-information-criterion-bic">Bayesian Information
Criterion (BIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(BIC =
-2 \log \ell(\hat{\theta}) + \log(n)d\)</span></li>
<li><strong>Concept:</strong> This is mathematically identical to AIC,
but the penalty term is different.
<ul>
<li><strong>AIC Penalty:</strong> <span
class="math inline">\(2d\)</span></li>
<li><strong>BIC Penalty:</strong> <span
class="math inline">\(\log(n)d\)</span></li>
</ul></li>
<li><strong>Comparison:</strong>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations
in your dataset.</li>
<li>As long as your dataset has 8 or more observations (<span
class="math inline">\(n \ge 8\)</span>), <span
class="math inline">\(\log(n)\)</span> will be greater than 2.</li>
<li>This means <strong>BIC applies a much harsher penalty for
complexity</strong> than AIC.</li>
</ul></li>
<li><strong>Consequence:</strong> BIC will tend to choose
<em>simpler</em> models (fewer predictors) than AIC.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> BIC.</li>
</ul>
<h2 id="the-deeper-theory-why-aic-works">The Deeper Theory: Why AIC
Works</h2>
<p>Slide 27 (â€œUnderstanding AICâ€) gives the deep mathematical
justification.</p>
<ul>
<li><strong>Goal:</strong> We have a <em>true</em>, unknown process
<span class="math inline">\(p\)</span> that generates our data. We are
creating a model <span class="math inline">\(\hat{p}_j\)</span>. We want
our model to be as â€œcloseâ€ to the truth as possible.</li>
<li><strong>Kullback-Leibler (K-L) Distance:</strong> This is a function
<span class="math inline">\(K(p, \hat{p}_j)\)</span> that measures the
â€œinformation lostâ€ when you use your model <span
class="math inline">\(\hat{p}_j\)</span> to approximate the truth <span
class="math inline">\(p\)</span>. You want to <em>minimize</em> this
distance.</li>
<li><strong>The Math:</strong>
<ol type="1">
<li><span class="math inline">\(K(p, \hat{p}_j) = \int p(y) \log \left(
\frac{p(y)}{\hat{p}_j(y)} \right) dy\)</span></li>
<li>This splits into: <span class="math inline">\(K(p, \hat{p}_j) =
\underbrace{\int p(y) \log(p(y)) dy}_{\text{Constant}} -
\underbrace{\int p(y) \log(\hat{p}_j(y)) dy}_{\text{This is what we need
to maximize}}\)</span></li>
</ol></li>
<li><strong>The Problem:</strong> We canâ€™t calculate that second term
because it requires knowing the <em>true</em> function <span
class="math inline">\(p\)</span>.</li>
<li><strong>Akaikeâ€™s Insight:</strong> Akaike proved that the
log-likelihood we <em>can</em> calculate, <span
class="math inline">\(\log \ell(\hat{\theta})\)</span>, is a
<em>biased</em> estimator of that target. He also proved that the bias
is approximately <span class="math inline">\(-d\)</span>.</li>
<li><strong>The Solution:</strong> An <em>unbiased</em> estimate of the
target is <span class="math inline">\(\log \ell(\hat{\theta}) -
d\)</span>.</li>
<li><strong>Final Step:</strong> For historical and statistical reasons,
he multiplied this by <span class="math inline">\(-2\)</span> to create
the final AIC formula.</li>
<li><strong>Conclusion:</strong> AIC is not just a random formula. It is
a carefully derived estimate of how much information your model loses
compared to the â€œtruthâ€ (i.e., its expected performance on new
data).</li>
</ul>
<h2 id="aicbic-for-linear-regression">AIC/BIC for Linear Regression</h2>
<p>Slide 26 shows how these general formulas simplify for linear
regression (assuming normal, Gaussian errors).</p>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Linear Regression Formula:</strong> <span
class="math inline">\(AIC = \frac{1}{n\hat{\sigma}^2}(RSS +
2d\hat{\sigma}^2)\)</span></li>
</ul>
<p><strong>Key Insight:</strong> For linear regression, the
â€œpoorness-of-fitâ€ term (<span class="math inline">\(-2 \log
\ell(\hat{\theta})\)</span>) is <em>directly proportional to</em> the
<span class="math inline">\(RSS\)</span>.</p>
<p>This makes it much easier to understand. You can just think of the
formulas as: * <strong>AIC <span class="math inline">\(\approx\)</span>
<span class="math inline">\(RSS + 2d\hat{\sigma}^2\)</span></strong> *
<strong>BIC <span class="math inline">\(\approx\)</span> <span
class="math inline">\(RSS + \log(n)d\hat{\sigma}^2\)</span></strong></p>
<p>(Here <span class="math inline">\(\hat{\sigma}^2\)</span> is an
estimate of the error variance, which can often be treated as a
constant).</p>
<p>This clearly shows the trade-off: We want a model with a low
<strong><span class="math inline">\(RSS\)</span></strong> (good fit) and
a low <strong><span class="math inline">\(d\)</span></strong> (low
complexity). These two goals are in direct competition.</p>
<p><strong>Mallowâ€™s <span class="math inline">\(C_p\)</span>:</strong>
The slide notes that <span class="math inline">\(C_p\)</span> is
equivalent to AIC for linear regression. The <span
class="math inline">\(C_p\)</span> formula is <span
class="math inline">\(C_p = \frac{1}{n}(RSS +
2d\hat{\sigma}^2_{full})\)</span>, where <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> is the error
variance estimated from the <em>full</em> model. Since <span
class="math inline">\(n\)</span> and <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> are constants,
minimizing <span class="math inline">\(C_p\)</span> is mathematically
identical to minimizing <span class="math inline">\(RSS +
2d\hat{\sigma}^2_{full}\)</span>, which is the same logic as AIC.</p>
<p>Here is a detailed breakdown of the mathematical formulas and
concepts from your slides.</p>
<h2 id="the-core-problem-training-error-vs.-test-error">The Core
Problem: Training Error vs.Â Test Error</h2>
<p>The central theme of these slides is finding the â€œbestâ€ model. The
problem is that a model with more predictors (more complex) will
<em>always</em> fit the data it was trained on better. This is a
trap.</p>
<ul>
<li><strong>Training Error:</strong> How well the model fits the data we
used to build it. <strong><span class="math inline">\(R^2\)</span> and
<span class="math inline">\(RSS\)</span> measure this.</strong></li>
<li><strong>Test Error:</strong> How well the model predicts new, unseen
data. This is what we <em>actually</em> care about. A model that is too
complex (e.g., has 10 predictors when only 3 are useful) will have low
training error but very high test error. This is called
<strong>overfitting</strong>.</li>
</ul>
<p>The goal is to choose a model that has the lowest <em>test
error</em>. The metrics below (Adjusted <span
class="math inline">\(R^2\)</span>, AIC, BIC) are all attempts to
<em>estimate</em> this test error without having to actually collect new
data. They do this by adding a <strong>penalty</strong> for
complexity.</p>
<h2 id="basic-metrics-measures-of-fit-1">Basic Metrics (Measures of
Fit)</h2>
<p>These formulas from slide 13 describe how well a model fits the
<em>training data</em>.</p>
<h3 id="residue-error">Residue (Error)</h3>
<ul>
<li><strong>Formula:</strong> <span
class="math inline">\(\hat{\epsilon}_i = y_i - \hat{y}_i = y_i -
\hat{\beta}_0 - \sum_{j=1}^{p} \hat{\beta}_j x_{ij}\)</span></li>
<li><strong>Concept:</strong> This is the most basic building block.
Itâ€™s the difference between the <em>actual</em> observed value (<span
class="math inline">\(y_i\)</span>) and the value your model
<em>predicted</em> (<span class="math inline">\(\hat{y}_i\)</span>). It
is the â€œerrorâ€ for a single data point.</li>
</ul>
<h3 id="residual-sum-of-squares-rss">Residual Sum of Squares (RSS)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(RSS =
\sum_{i=1}^{n} \hat{\epsilon}_i^2\)</span></li>
<li><strong>Concept:</strong> This is the overall measure of model
error. You square all the individual errors (residues) to make them
positive and then add them all up.</li>
<li><strong>Goal:</strong> The entire process of linear regression
(called â€œOrdinary Least Squaresâ€) is designed to find the <span
class="math inline">\(\hat{\beta}\)</span> coefficients that make this
<strong>RSS value as small as possible</strong>.</li>
<li><strong>The Flaw:</strong> <span class="math inline">\(RSS\)</span>
will <em>always</em> decrease (or stay the same) as you add more
predictors (<span class="math inline">\(p\)</span>). A model with all 10
predictors will have a lower <span class="math inline">\(RSS\)</span>
than a model with 9, even if that 10th predictor is useless. Therefore,
<span class="math inline">\(RSS\)</span> is useless for choosing
<em>between</em> models of different sizes.</li>
</ul>
<h3 id="r-squared-r2-1">R-squared (<span
class="math inline">\(R^2\)</span>)</h3>
<ul>
<li><strong>Formula:</strong> <span class="math inline">\(R^2 = 1 -
\frac{SS_{error}}{SS_{total}} = 1 - \frac{RSS}{\sum_{i=1}^{n} (y_i -
\bar{y})^2}\)</span></li>
<li><strong>Concept:</strong> This metric reframes <span
class="math inline">\(RSS\)</span> into a more interpretable percentage.
<ul>
<li><span class="math inline">\(SS_{total}\)</span> (the denominator)
represents the <em>total variance</em> of the data. Itâ€™s the error you
would get if your â€œmodelâ€ was just guessing the average value (<span
class="math inline">\(\bar{y}\)</span>) for every single
observation.</li>
<li><span class="math inline">\(SS_{error}\)</span> (the <span
class="math inline">\(RSS\)</span>) is the error <em>after</em> using
your model.</li>
<li><span class="math inline">\(R^2\)</span> is the â€œproportion of total
variance explained by the model.â€ An <span
class="math inline">\(R^2\)</span> of 0.75 means your model can explain
75% of the variation in the response variable.</li>
</ul></li>
<li><strong>The Flaw:</strong> Just like <span
class="math inline">\(RSS\)</span>, <span
class="math inline">\(R^2\)</span> will <em>always</em> increase (or
stay the same) as you add more predictors. This is visually confirmed in
Figure 6.1, where the red line for <span
class="math inline">\(R^2\)</span> only goes up. It will always pick the
most complex model.</li>
</ul>
<h2 id="advanced-metrics-for-model-selection">Advanced Metrics (For
Model Selection)</h2>
<p>These metrics â€œfixâ€ the flaw of <span
class="math inline">\(R^2\)</span> by including a penalty for the number
of predictors.</p>
<h3 id="adjusted-r2-1">Adjusted <span
class="math inline">\(R^2\)</span></h3>
<ul>
<li><strong>Formula:</strong> <span class="math display">\[
  \text{Adjusted } R^2 = 1 - \frac{RSS / (n - p - 1)}{SS_{total} / (n -
1)}
  \]</span></li>
<li><strong>Mathematical Concept:</strong> This formula replaces the
â€œSum of Squaresâ€ (<span class="math inline">\(SS\)</span>) with â€œMean
Squaresâ€ (<span class="math inline">\(MS\)</span>).
<ul>
<li><span class="math inline">\(MS_{error} =
\frac{RSS}{n-p-1}\)</span></li>
<li><span class="math inline">\(MS_{total} =
\frac{SS_{total}}{n-1}\)</span></li>
</ul></li>
<li><strong>The â€œPenaltyâ€ Explained:</strong> The penalty is
<strong>degrees of freedom</strong>.
<ul>
<li><span class="math inline">\(n\)</span> = number of data points.</li>
<li><span class="math inline">\(p\)</span> = number of predictors.</li>
<li>The term <span class="math inline">\(n-p-1\)</span> is the degrees
of freedom for the residuals. You start with <span
class="math inline">\(n\)</span> data points, but you â€œuse upâ€ one
degree of freedom to estimate the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) and <span
class="math inline">\(p\)</span> more to estimate the <span
class="math inline">\(p\)</span> slopes.</li>
</ul></li>
<li><strong>How it Works:</strong>
<ol type="1">
<li>When you add a new predictor (increase <span
class="math inline">\(p\)</span>), <span
class="math inline">\(RSS\)</span> goes down, which makes the numerator
(<span class="math inline">\(MS_{error}\)</span>) smaller.</li>
<li>â€¦But, increasing <span class="math inline">\(p\)</span>
<em>also</em> decreases the denominator (<span
class="math inline">\(n-p-1\)</span>), which makes the numerator (<span
class="math inline">\(MS_{error}\)</span>) <em>larger</em>.</li>
</ol>
<ul>
<li>This creates a â€œtug-of-war.â€ If the new predictor is
<strong>useful</strong>, it will drop <span
class="math inline">\(RSS\)</span> a lot, and Adjusted <span
class="math inline">\(R^2\)</span> will <strong>increase</strong>. If
the new predictor is <strong>useless</strong>, <span
class="math inline">\(RSS\)</span> will barely change, and the penalty
from decreasing the denominator will win, causing Adjusted <span
class="math inline">\(R^2\)</span> to <strong>decrease</strong>.</li>
</ul></li>
<li><strong>Goal:</strong> You select the model with the
<strong>highest</strong> Adjusted <span
class="math inline">\(R^2\)</span>.</li>
</ul>
<h3 id="akaike-information-criterion-aic-1">Akaike Information Criterion
(AIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Concept Breakdown:</strong>
<ul>
<li><span class="math inline">\(\ell(\hat{\theta})\)</span>: This is the
<strong>Maximized Likelihood Function</strong>.
<ul>
<li>The <strong>Likelihood Function</strong> <span
class="math inline">\(\ell(\theta)\)</span> asks: â€œGiven a set of model
parameters <span class="math inline">\(\theta\)</span>, how probable is
the data we observed?â€</li>
<li>The <strong>Maximum Likelihood Estimate (MLE)</strong> <span
class="math inline">\(\hat{\theta}\)</span> is the specific set of
parameters (the <span class="math inline">\(\hat{\beta}\)</span>â€™s) that
<em>maximizes</em> this probability.</li>
</ul></li>
<li><span class="math inline">\(\log \ell(\hat{\theta})\)</span>: The
<strong>log-likelihood</strong>. This is just a number that represents
the <em>best possible fit</em> the model can achieve for the data. A
higher number is a better fit.</li>
<li><span class="math inline">\(-2 \log \ell(\hat{\theta})\)</span>:
This is the <strong>Deviance</strong>. Since a higher log-likelihood is
better, a <em>lower</em> deviance is better. This term measures
<strong>poorness-of-fit</strong>.</li>
<li><span class="math inline">\(d\)</span>: The number of parameters
estimated by the model. (e.g., <span class="math inline">\(p\)</span>
predictors + 1 intercept).</li>
<li><span class="math inline">\(2d\)</span>: This is the <strong>Penalty
Term</strong>.</li>
</ul></li>
<li><strong>How it Works:</strong> <span class="math inline">\(AIC =
(\text{Poorness-of-Fit}) + (\text{Complexity Penalty})\)</span>. As you
add predictors, the fit gets better (the deviance term goes down), but
the penalty term (<span class="math inline">\(2d\)</span>) goes up.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> AIC.</li>
</ul>
<h3 id="bayesian-information-criterion-bic-1">Bayesian Information
Criterion (BIC)</h3>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(BIC =
-2 \log \ell(\hat{\theta}) + \log(n)d\)</span></li>
<li><strong>Concept:</strong> This is mathematically identical to AIC,
but the penalty term is different.
<ul>
<li><strong>AIC Penalty:</strong> <span
class="math inline">\(2d\)</span></li>
<li><strong>BIC Penalty:</strong> <span
class="math inline">\(\log(n)d\)</span></li>
</ul></li>
<li><strong>Comparison:</strong>
<ul>
<li><span class="math inline">\(n\)</span> is the number of observations
in your dataset.</li>
<li>As long as your dataset has 8 or more observations (<span
class="math inline">\(n \ge 8\)</span>), <span
class="math inline">\(\log(n)\)</span> will be greater than 2.</li>
<li>This means <strong>BIC applies a much harsher penalty for
complexity</strong> than AIC.</li>
</ul></li>
<li><strong>Consequence:</strong> BIC will tend to choose
<em>simpler</em> models (fewer predictors) than AIC.</li>
<li><strong>Goal:</strong> You select the model with the
<strong>lowest</strong> BIC.</li>
</ul>
<h2 id="the-deeper-theory-why-aic-works-1">The Deeper Theory: Why AIC
Works</h2>
<p>Slide 27 (â€œUnderstanding AICâ€) gives the deep mathematical
justification.</p>
<ul>
<li><strong>Goal:</strong> We have a <em>true</em>, unknown process
<span class="math inline">\(p\)</span> that generates our data. We are
creating a model <span class="math inline">\(\hat{p}_j\)</span>. We want
our model to be as â€œcloseâ€ to the truth as possible.</li>
<li><strong>Kullback-Leibler (K-L) Distance:</strong> This is a function
<span class="math inline">\(K(p, \hat{p}_j)\)</span> that measures the
â€œinformation lostâ€ when you use your model <span
class="math inline">\(\hat{p}_j\)</span> to approximate the truth <span
class="math inline">\(p\)</span>. You want to <em>minimize</em> this
distance.</li>
<li><strong>The Math:</strong>
<ol type="1">
<li><span class="math inline">\(K(p, \hat{p}_j) = \int p(y) \log \left(
\frac{p(y)}{\hat{p}_j(y)} \right) dy\)</span></li>
<li>This splits into: <span class="math inline">\(K(p, \hat{p}_j) =
\underbrace{\int p(y) \log(p(y)) dy}_{\text{Constant}} -
\underbrace{\int p(y) \log(\hat{p}_j(y)) dy}_{\text{This is what we need
to maximize}}\)</span></li>
</ol></li>
<li><strong>The Problem:</strong> We canâ€™t calculate that second term
because it requires knowing the <em>true</em> function <span
class="math inline">\(p\)</span>.</li>
<li><strong>Akaikeâ€™s Insight:</strong> Akaike proved that the
log-likelihood we <em>can</em> calculate, <span
class="math inline">\(\log \ell(\hat{\theta})\)</span>, is a
<em>biased</em> estimator of that target. He also proved that the bias
is approximately <span class="math inline">\(-d\)</span>.</li>
<li><strong>The Solution:</strong> An <em>unbiased</em> estimate of the
target is <span class="math inline">\(\log \ell(\hat{\theta}) -
d\)</span>.</li>
<li><strong>Final Step:</strong> For historical and statistical reasons,
he multiplied this by <span class="math inline">\(-2\)</span> to create
the final AIC formula.</li>
<li><strong>Conclusion:</strong> AIC is not just a random formula. It is
a carefully derived estimate of how much information your model loses
compared to the â€œtruthâ€ (i.e., its expected performance on new
data).</li>
</ul>
<h2 id="aicbic-for-linear-regression-1">AIC/BIC for Linear
Regression</h2>
<p>Slide 26 shows how these general formulas simplify for linear
regression (assuming normal, Gaussian errors).</p>
<ul>
<li><strong>General Formula:</strong> <span class="math inline">\(AIC =
-2 \log \ell(\hat{\theta}) + 2d\)</span></li>
<li><strong>Linear Regression Formula:</strong> <span
class="math inline">\(AIC = \frac{1}{n\hat{\sigma}^2}(RSS +
2d\hat{\sigma}^2)\)</span></li>
</ul>
<p><strong>Key Insight:</strong> For linear regression, the
â€œpoorness-of-fitâ€ term (<span class="math inline">\(-2 \log
\ell(\hat{\theta})\)</span>) is <em>directly proportional to</em> the
<span class="math inline">\(RSS\)</span>.</p>
<p>This makes it much easier to understand. You can just think of the
formulas as: * <strong>AIC <span class="math inline">\(\approx\)</span>
<span class="math inline">\(RSS + 2d\hat{\sigma}^2\)</span></strong> *
<strong>BIC <span class="math inline">\(\approx\)</span> <span
class="math inline">\(RSS + \log(n)d\hat{\sigma}^2\)</span></strong></p>
<p>(Here <span class="math inline">\(\hat{\sigma}^2\)</span> is an
estimate of the error variance, which can often be treated as a
constant).</p>
<p>This clearly shows the trade-off: We want a model with a low
<strong><span class="math inline">\(RSS\)</span></strong> (good fit) and
a low <strong><span class="math inline">\(d\)</span></strong> (low
complexity). These two goals are in direct competition.</p>
<p><strong>Mallowâ€™s <span class="math inline">\(C_p\)</span>:</strong>
The slide notes that <span class="math inline">\(C_p\)</span> is
equivalent to AIC for linear regression. The <span
class="math inline">\(C_p\)</span> formula is <span
class="math inline">\(C_p = \frac{1}{n}(RSS +
2d\hat{\sigma}^2_{full})\)</span>, where <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> is the error
variance estimated from the <em>full</em> model. Since <span
class="math inline">\(n\)</span> and <span
class="math inline">\(\hat{\sigma}^2_{full}\)</span> are constants,
minimizing <span class="math inline">\(C_p\)</span> is mathematically
identical to minimizing <span class="math inline">\(RSS +
2d\hat{\sigma}^2_{full}\)</span>, which is the same logic as AIC.</p>
<h1 id="variable-selection">3. Variable Selection</h1>
<h2 id="core-concept-the-problem-of-variable-selection">Core Concept:
The Problem of Variable Selection</h2>
<p>In regression, we want to model a response variable <span
class="math inline">\(Y\)</span> using a set of <span
class="math inline">\(p\)</span> predictor variables <span
class="math inline">\(X_1, X_2, ..., X_p\)</span>.</p>
<ul>
<li><p><strong>The â€œKitchen Sinkâ€ Problem:</strong> A common temptation
is to include all available predictors in the model: <span
class="math display">\[Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... +
\beta_pX_p + \epsilon\]</span> This often leads to
<strong>overfitting</strong>. The model may fit the training data well
but will perform poorly on new, unseen data. Itâ€™s also hard to interpret
a model with dozens of predictors.</p></li>
<li><p><strong>The Solution: Subset Selection.</strong> The goal is to
find a smaller subset of the predictors that builds a model that is:</p>
<ol type="1">
<li><strong>Accurate:</strong> Has low prediction error.</li>
<li><strong>Parsimonious:</strong> Uses the fewest predictors
necessary.</li>
<li><strong>Interpretable:</strong> Is simple enough for a human to
understand.</li>
</ol></li>
</ul>
<p>Your slides present two main methods to achieve this: <strong>Best
Subset Selection</strong> and <strong>Forward Stepwise
Selection</strong>.</p>
<h2 id="method-1-best-subset-selection-bss">Method 1: Best Subset
Selection (BSS)</h2>
<p>This is the â€œbrute forceâ€ approach. It considers <em>every single
possible model</em>.</p>
<h3 id="conceptual-algorithm">Conceptual Algorithm</h3>
<ol type="1">
<li>Fit all models with <span class="math inline">\(k=1\)</span>
predictor (there are <span class="math inline">\(p\)</span> of these).
Find the best one (lowest RSS) and call it <span
class="math inline">\(M_1\)</span>.</li>
<li>Fit all models with <span class="math inline">\(k=2\)</span>
predictors (there are <span class="math inline">\(\binom{p}{2}\)</span>
of these). Find the best one and call it <span
class="math inline">\(M_2\)</span>.</li>
<li>â€¦</li>
<li>Fit the one model with <span class="math inline">\(k=p\)</span>
predictors (the full model), <span
class="math inline">\(M_p\)</span>.</li>
<li>You now have a list of <span class="math inline">\(p\)</span> â€œbestâ€
models: <span class="math inline">\(M_1, M_2, ..., M_p\)</span>.</li>
<li>Use a selection criterion (like <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>BIC</strong>,
<strong>AIC</strong>, or <strong><span
class="math inline">\(C_p\)</span></strong>) to choose the single best
model from this list.</li>
</ol>
<h3
id="mathematical-computational-cost-from-slide-225641.png">Mathematical
&amp; Computational Cost (from slide <code>225641.png</code>)</h3>
<ul>
<li>For each predictor, there are two possibilities: itâ€™s either
<strong>IN</strong> the model or <strong>OUT</strong>.</li>
<li>With <span class="math inline">\(p\)</span> predictors, the total
number of models to test is <span class="math inline">\(2 \times 2
\times ... \times 2\)</span> (<span class="math inline">\(p\)</span>
times).</li>
<li><strong>Total Models = <span
class="math inline">\(2^p\)</span></strong></li>
<li>This is a â€œcombinatorial explosion.â€ As the slide notes, if <span
class="math inline">\(p=20\)</span>, <span class="math inline">\(2^{20}
= 1,048,576\)</span> models. This is computationally infeasible for
large <span class="math inline">\(p\)</span>.</li>
</ul>
<h2 id="method-2-forward-stepwise-selection-fss">Method 2: Forward
Stepwise Selection (FSS)</h2>
<p>This is a â€œgreedyâ€ algorithm. Itâ€™s an efficient alternative to BSS
that does <em>not</em> test every model.</p>
<h3
id="conceptual-algorithm-from-slides-225645.png-225648.png">Conceptual
Algorithm (from slides <code>225645.png</code> &amp;
<code>225648.png</code>)</h3>
<ul>
<li><p><strong>Step 1:</strong> Start with the <strong>null
model</strong>, <span class="math inline">\(M_0\)</span>, which has no
predictors. <span class="math display">\[M_0: Y = \beta_0 +
\epsilon\]</span> The prediction is just the sample mean of <span
class="math inline">\(Y\)</span>.</p></li>
<li><p><strong>Step 2 (Iterative):</strong></p>
<ul>
<li><strong>For <span class="math inline">\(k=0\)</span> (to get <span
class="math inline">\(M_1\)</span>):</strong> Fit all <span
class="math inline">\(p\)</span> models that add <em>one</em> predictor
to <span class="math inline">\(M_0\)</span>. Choose the best one (lowest
<strong>RSS</strong> or highest <strong><span
class="math inline">\(R^2\)</span></strong>). This is <span
class="math inline">\(M_1\)</span>. Letâ€™s say it contains <span
class="math inline">\(X_1\)</span>.</li>
<li><strong>For <span class="math inline">\(k=1\)</span> (to get <span
class="math inline">\(M_2\)</span>):</strong> <em>Keep</em> <span
class="math inline">\(X_1\)</span> in the model. Fit all <span
class="math inline">\(p-1\)</span> models that add <em>one more</em>
predictor to <span class="math inline">\(M_1\)</span> (e.g., <span
class="math inline">\(M_1+X_2\)</span>, <span
class="math inline">\(M_1+X_3\)</span>, â€¦). Choose the best of these.
This is <span class="math inline">\(M_2\)</span>.</li>
<li><strong>Repeat:</strong> Continue this process, adding one variable
at a time, until all <span class="math inline">\(p\)</span> predictors
are in the model <span class="math inline">\(M_p\)</span>.</li>
</ul></li>
<li><p><strong>Step 3:</strong> You now have a sequence of <span
class="math inline">\(p+1\)</span> models: <span
class="math inline">\(M_0, M_1, ..., M_p\)</span>. Choose the single
best model from this sequence using <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>AIC</strong>,
<strong>BIC</strong>, or <strong><span
class="math inline">\(C_p\)</span></strong>.</p></li>
</ul>
<h3
id="mathematical-computational-cost-from-slide-225651.png">Mathematical
&amp; Computational Cost (from slide <code>225651.png</code>)</h3>
<ul>
<li>To find <span class="math inline">\(M_1\)</span>, you fit <span
class="math inline">\(p\)</span> models.</li>
<li>To find <span class="math inline">\(M_2\)</span>, you fit <span
class="math inline">\(p-1\)</span> models.</li>
<li>To find <span class="math inline">\(M_p\)</span>, you fit <span
class="math inline">\(1\)</span> model.</li>
<li>The null model <span class="math inline">\(M_0\)</span> is 1
model.</li>
<li><strong>Total Models = <span class="math inline">\(1 +
\sum_{k=0}^{p-1} (p-k) = 1 + p + (p-1) + ... + 1 = 1 +
\frac{p(p+1)}{2}\)</span></strong></li>
<li>As the slide notes, if <span class="math inline">\(p=20\)</span>,
this is only <span class="math inline">\(1 + 20(21)/2 = 211\)</span>
models. This is vastly more efficient than BSS.</li>
<li><strong>Key weakness:</strong> The method is â€œgreedy.â€ If it adds
<span class="math inline">\(X_1\)</span> in Step 1, it can
<em>never</em> be removed. Itâ€™s possible the true best 2-variable model
is <span class="math inline">\((X_2, X_3)\)</span>, but if FSS chose
<span class="math inline">\(X_1\)</span> as the best 1-variable model,
it will never find <span class="math inline">\((X_2, X_3)\)</span>.</li>
</ul>
<h2 id="how-to-choose-the-best-model-the-criteria">4. How to Choose the
â€œBestâ€ Model: The Criteria</h2>
<p>You canâ€™t use <strong>RSS</strong> or <strong><span
class="math inline">\(R^2\)</span></strong> to compare models with
<em>different numbers of predictors</em> (<span
class="math inline">\(k\)</span>). This is because RSS always decreases
(and <span class="math inline">\(R^2\)</span> always increases) as you
add more variables. You <em>must</em> use a criterion that penalizes
complexity.</p>
<ul>
<li><p><strong>RSS (Residual Sum of Squares):</strong> Goal is to
<strong>minimize</strong>. <span class="math display">\[RSS =
\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span> Good for comparing models
<em>of the same size <span
class="math inline">\(k\)</span></em>.</p></li>
<li><p><strong>Adjusted R-squared (<span class="math inline">\(Adj.
R^2\)</span>):</strong> Goal is to <strong>maximize</strong>. <span
class="math display">\[Adj. R^2 = 1 -
\frac{(1-R^2)(n-1)}{n-p-1}\]</span> This â€œadjustsâ€ <span
class="math inline">\(R^2\)</span> by adding a penalty for having more
predictors (<span class="math inline">\(p\)</span>). Adding a useless
predictor will make <span class="math inline">\(Adj. R^2\)</span> go
down.</p></li>
<li><p><strong>Mallowâ€™s <span
class="math inline">\(C_p\)</span>:</strong> Goal is to
<strong>minimize</strong>. <span class="math display">\[C_p \approx
\frac{1}{n}(RSS + 2p\hat{\sigma}^2)\]</span> Here, <span
class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the
error variance from the <em>full model</em> (with all <span
class="math inline">\(p\)</span> predictors). A good model will have
<span class="math inline">\(C_p \approx p\)</span>.</p></li>
<li><p><strong>AIC (Akaike Information Criterion) &amp; BIC (Bayesian
Information Criterion):</strong> Goal is to <strong>minimize</strong>.
<span class="math display">\[AIC = 2p - 2\ln(\hat{L})\]</span> <span
class="math display">\[BIC = p\ln(n) - 2\ln(\hat{L})\]</span> Here,
<span class="math inline">\(\hat{L}\)</span> is the maximized likelihood
of the model. You donâ€™t need to calculate this by hand; software
provides it.</p>
<ul>
<li><strong>Key difference:</strong> BICâ€™s penalty for <span
class="math inline">\(p\)</span> is <span
class="math inline">\(p\ln(n)\)</span>, while AICâ€™s is <span
class="math inline">\(2p\)</span>. Since <span
class="math inline">\(\ln(n)\)</span> is almost always <span
class="math inline">\(&gt; 2\)</span> (for <span
class="math inline">\(n&gt;7\)</span>), <strong>BIC applies a much
heavier penalty for complexity</strong>.</li>
<li>This means <strong>BIC tends to choose smaller, more parsimonious
models</strong> than AIC or <span class="math inline">\(Adj.
R^2\)</span>.</li>
</ul></li>
</ul>
<h2 id="python-code-analysis-slide-225546.jpg">5. Python Code Analysis
(Slide <code>225546.jpg</code>)</h2>
<p>This slide shows the Python code for <strong>Best Subset
Selection</strong> (BSS).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> combinations <span class="comment"># &lt;-- This is the BSS engine</span></span><br></pre></td></tr></table></figure>
<h3 id="block-1-load-the-credit-dataset">Block 1: Load the Credit
dataset</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Load the Credit dataset</span></span><br><span class="line">Credit = pd.read_csv(<span class="string">&#x27;Credit.csv&#x27;</span>)</span><br><span class="line">Credit[<span class="string">&#x27;ID&#x27;</span>] = Credit[<span class="string">&#x27;ID&#x27;</span>].astype(<span class="built_in">str</span>)</span><br><span class="line">(num_samples, num_predictors) = Credit.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert categorical text data to numerical (dummy variables)</span></span><br><span class="line">Credit[<span class="string">&#x27;Gender&#x27;</span>] = Credit[<span class="string">&#x27;Gender&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Male&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Female&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Student&#x27;</span>] = Credit[<span class="string">&#x27;Student&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Yes&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;No&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Married&#x27;</span>] = Credit[<span class="string">&#x27;Married&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Yes&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;No&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">Credit[<span class="string">&#x27;Ethnicity&#x27;</span>] = Credit[<span class="string">&#x27;Ethnicity&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;Asian&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Caucasian&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;African American&#x27;</span>: <span class="number">0</span>&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>pd.read_csv</code></strong>: Reads the data into a
<code>pandas</code> DataFrame.</li>
<li><strong><code>.map()</code></strong>: This is a crucial
preprocessing step. Regression models require numbers, not text like
â€˜Yesâ€™ or â€˜Maleâ€™. This line converts those strings into <code>1</code>s
and <code>0</code>s.</li>
</ul>
<h3 id="block-2-plot-scatterplot-matrix">Block 2: Plot scatterplot
matrix</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. Plot scatterplot matrix</span></span><br><span class="line">selected_columns = [<span class="string">&#x27;Balance&#x27;</span>, <span class="string">&#x27;Education&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;Cards&#x27;</span>, <span class="string">&#x27;Rating&#x27;</span>, <span class="string">&#x27;Limit&#x27;</span>, <span class="string">&#x27;Income&#x27;</span>]</span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&quot;ticks&quot;</span>)</span><br><span class="line">sns.pairplot(Credit[selected_columns], diag_kind=<span class="string">&#x27;kde&#x27;</span>)</span><br><span class="line">plt.suptitle(<span class="string">&#x27;Scatterplot Matrix&#x27;</span>, y=<span class="number">1.02</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>sns.pairplot</code></strong>: A powerful visualization
from the <code>seaborn</code> library. The resulting plot (right side of
the slide) is a grid.
<ul>
<li><strong>Diagonal plots (kde)</strong>: Show the distribution (Kernel
Density Estimate) of a single variable (e.g., â€˜Balanceâ€™ is skewed
right).</li>
<li><strong>Off-diagonal plots (scatter)</strong>: Show the relationship
between two variables (e.g., â€˜Limitâ€™ and â€˜Ratingâ€™ are almost perfectly
linear). This helps you visually spot potentially strong
predictors.</li>
</ul></li>
</ul>
<h3 id="block-3-best-subset-selection">Block 3: Best Subset
Selection</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Best Subset Selection</span></span><br><span class="line"><span class="comment"># (This code is incomplete on the slide, I&#x27;ll fill in the logic)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define target and predictors</span></span><br><span class="line">target = <span class="string">&#x27;Balance&#x27;</span></span><br><span class="line">predictors = [col <span class="keyword">for</span> col <span class="keyword">in</span> Credit.columns <span class="keyword">if</span> col != target] </span><br><span class="line">nvmax = <span class="number">10</span> <span class="comment"># Max number of predictors to test (up to 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize lists to store model statistics</span></span><br><span class="line">model_stats = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate over number of predictors from 1 to nvmax</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, nvmax + <span class="number">1</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Generate all possible combinations of predictors of size k</span></span><br><span class="line">    <span class="comment"># This is the core of BSS</span></span><br><span class="line">    <span class="keyword">for</span> subset <span class="keyword">in</span> <span class="built_in">list</span>(combinations(predictors, k)):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the design matrix (X)</span></span><br><span class="line">        X_subset = Credit[<span class="built_in">list</span>(subset)]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Add a constant (intercept) term to the model</span></span><br><span class="line">        <span class="comment"># Y = B0 + B1*X1 -&gt; statsmodels needs B0 to be added manually</span></span><br><span class="line">        X_subset_const = sm.add_constant(X_subset)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the target variable (y)</span></span><br><span class="line">        y_target = Credit[target]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Fit the Ordinary Least Squares (OLS) model</span></span><br><span class="line">        model = sm.OLS(y_target, X_subset_const).fit()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate RSS</span></span><br><span class="line">        RSS = ((model.resid) ** <span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (The full code would also calculate R-squared, Adj. R-sq, BIC, etc. here)</span></span><br><span class="line">        <span class="comment"># model_stats.append(&#123;&#x27;k&#x27;: k, &#x27;subset&#x27;: subset, &#x27;RSS&#x27;: RSS, ...&#125;)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>for k in range(1, nvmax + 1)</code></strong>: This is
the <em>outer</em> loop that iterates from <span
class="math inline">\(k=1\)</span> (1 predictor) to <span
class="math inline">\(k=10\)</span> (10 predictors).</li>
<li><strong><code>list(combinations(predictors, k))</code></strong>:
This is the <em>inner</em> loop and the <strong>most important
line</strong>. The <code>itertools.combinations</code> function is a
highly efficient way to generate all unique subsets.
<ul>
<li>When <span class="math inline">\(k=1\)</span>, it returns
<code>[('Income',), ('Limit',), ('Rating',), ...]</code>.</li>
<li>When <span class="math inline">\(k=2\)</span>, it returns
<code>[('Income', 'Limit'), ('Income', 'Rating'), ('Limit', 'Rating'), ...]</code>.</li>
<li>This is what generates the <span class="math inline">\(2^p\)</span>
(or in this case, <span class="math inline">\(\sum_{k=1}^{10}
\binom{p}{k}\)</span>) models to test.</li>
</ul></li>
<li><strong><code>sm.add_constant(X_subset)</code></strong>: Your
regression equation is <span class="math inline">\(Y = \beta_0 +
\beta_1X_1\)</span>. The <span class="math inline">\(X_1\)</span> is
your <code>X_subset</code>. The <code>sm.add_constant</code> function
adds a column of <code>1</code>s to your data, which allows the
<code>statsmodels</code> library to estimate the <span
class="math inline">\(\beta_0\)</span> (intercept) term.</li>
<li><strong><code>sm.OLS(y_target, X_subset_const).fit()</code></strong>:
This fits the Ordinary Least Squares (OLS) model, which finds the <span
class="math inline">\(\beta\)</span> coefficients that <strong>minimize
the RSS</strong>.</li>
<li><strong><code>model.resid</code></strong>: This attribute of the
fitted model contains the residuals (<span class="math inline">\(e_i =
y_i - \hat{y}_i\)</span>) for each data point.</li>
<li><strong><code>((model.resid) ** 2).sum()</code></strong>: This line
is the direct code implementation of the formula <span
class="math inline">\(RSS = \sum e_i^2\)</span>.</li>
</ul>
<h2 id="synthesizing-the-results-the-plots">Synthesizing the Results
(The Plots)</h2>
<p>After running the BSS code, you get the data used in the plots and
the table.</p>
<ul>
<li><p><strong>Image <code>225550.png</code> (Adjusted
R-squared)</strong></p>
<ul>
<li><strong>Goal:</strong> Maximize.</li>
<li><strong>What it shows:</strong> The gray dots are <em>all</em> the
models tested for each <span class="math inline">\(k\)</span>. The red
line connects the single <em>best</em> model for each <span
class="math inline">\(k\)</span>.</li>
<li><strong>Conclusion:</strong> The plot shows a sharp â€œelbow.â€ The
<span class="math inline">\(Adj. R^2\)</span> increases dramatically up
to <span class="math inline">\(k=4\)</span>, then increases very slowly.
The maximum is around <span class="math inline">\(k=6\)</span> or <span
class="math inline">\(k=7\)</span>, but the gain after <span
class="math inline">\(k=4\)</span> is minimal.</li>
</ul></li>
<li><p><strong>Image <code>225554.png</code> (BIC)</strong></p>
<ul>
<li><strong>Goal:</strong> Minimize.</li>
<li><strong>What it shows:</strong> BIC heavily penalizes
complexity.</li>
<li><strong>Conclusion:</strong> The plot shows a very clear minimum.
The BIC value plummets from <span class="math inline">\(k=2\)</span> to
<span class="math inline">\(k=3\)</span> and hits its lowest point at
<strong><span class="math inline">\(k=4\)</span></strong>. After <span
class="math inline">\(k=4\)</span>, the penalty for adding more
variables is <em>larger</em> than the benefit in model fit, so the BIC
score starts to rise. This is a very strong vote for the 4-predictor
model.</li>
</ul></li>
<li><p><strong>Image <code>225635.png</code> (Mallowâ€™s <span
class="math inline">\(C_p\)</span>)</strong></p>
<ul>
<li><strong>Goal:</strong> Minimize.</li>
<li><strong>What it shows:</strong> A very similar story to BIC.</li>
<li><strong>Conclusion:</strong> The <span
class="math inline">\(C_p\)</span> value drops significantly and hits
its minimum at <strong><span
class="math inline">\(k=4\)</span></strong>.</li>
</ul></li>
<li><p><strong>Image <code>225638.png</code> (Summary
Table)</strong></p>
<ul>
<li>This is the <strong>most important image</strong> for the final
conclusion. It summarizes the red line from all the plots.</li>
<li>Look at the row for <code>Num_Predictors = 4</code>. The predictors
are <strong>(Income, Limit, Cards, Student)</strong>.</li>
<li>Now look at the columns for <code>BIC</code> and <code>Cp</code>.
<ul>
<li><strong>BIC:</strong> <code>4841.615607</code>. This is the lowest
value in the entire <code>BIC</code> column (the value at <span
class="math inline">\(k=3\)</span> is <code>4865.352851</code>).</li>
<li><strong>Cp:</strong> <code>7.122228</code>. This is also the lowest
value in the <code>Cp</code> column.</li>
</ul></li>
<li>The <code>Adj_R_squared</code> at <span
class="math inline">\(k=4\)</span> is <code>0.953580</code>, which is
very close to its maximum of <code>~0.954</code> at <span
class="math inline">\(k=7-10\)</span>.</li>
</ul></li>
</ul>
<p><strong>Final Conclusion:</strong> All three â€œpenalizedâ€ criteria
(Adjusted <span class="math inline">\(R^2\)</span>, BIC, and <span
class="math inline">\(C_p\)</span>) point to the same conclusion. While
<span class="math inline">\(Adj. R^2\)</span> is a bit ambiguous,
<strong>BIC and <span class="math inline">\(C_p\)</span> provide a clear
signal that the best, most parsimonious model is the 4-predictor model
using <code>Income</code>, <code>Limit</code>, <code>Cards</code>, and
<code>Student</code></strong>.</p>
<h1 id="subset-selection">4. Subset Selection</h1>
<h2 id="summary-of-subset-selection">Summary of Subset Selection</h2>
<p>These slides introduce <strong>subset selection</strong>, a process
in statistical learning used to identify the best subset of predictors
(variables) for a regression model. The goal is to find a model that has
low prediction error and avoids overfitting by excluding irrelevant
variables.</p>
<p>The slides cover two main â€œgreedyâ€ (stepwise) algorithms and the
criteria used to select the final best model.</p>
<h2 id="stepwise-selection-algorithms">Stepwise Selection
Algorithms</h2>
<p>Instead of testing all <span class="math inline">\(2^p\)</span>
possible models (which is â€œbest subset selectionâ€ and computationally
unfeasible), stepwise methods build a single path of models.</p>
<h3 id="forward-stepwise-selection">Forward Stepwise Selection</h3>
<p>This is an <strong>additive</strong> (bottom-up) approach:</p>
<ol type="1">
<li><strong>Start</strong> with the null model (no predictors).</li>
<li><strong>Find</strong> the best 1-variable model (the one that gives
the lowest Residual Sum of Squares, or RSS).</li>
<li><strong>Add</strong> the single variable that, when added to the
current model, results in the <em>new</em> best model (lowest RSS).</li>
<li><strong>Repeat</strong> this process until all <span
class="math inline">\(p\)</span> predictors are in the model.</li>
<li>This generates a sequence of <span
class="math inline">\(p+1\)</span> models, from <span
class="math inline">\(\mathcal{M}_0\)</span> to <span
class="math inline">\(\mathcal{M}_p\)</span>.</li>
</ol>
<h3 id="backward-stepwise-selection">Backward Stepwise Selection</h3>
<p>This is a <strong>subtractive</strong> (top-down) approach:</p>
<ol type="1">
<li><strong>Start</strong> with the full model containing all <span
class="math inline">\(p\)</span> predictors.</li>
<li><strong>Find</strong> the best <span
class="math inline">\((p-1)\)</span>-variable model by <em>removing</em>
the single variable that results in the <em>lowest RSS</em> (or highest
<span class="math inline">\(R^2\)</span>). This variable is considered
the least significant.</li>
<li><strong>Remove</strong> the next variable that, when removed from
the current best model, gives the new best model.</li>
<li><strong>Repeat</strong> until only the null model remains.</li>
<li>This also generates a sequence of <span
class="math inline">\(p+1\)</span> models.</li>
</ol>
<h4 id="pros-and-cons-backward-selection">Pros and Cons (Backward
Selection)</h4>
<ul>
<li><strong>Pro:</strong> Computationally efficient compared to best
subset. It fits <span class="math inline">\(1 + \sum_{k=0}^{p-1}(p-k) =
\mathbf{1 + p(p+1)/2}\)</span> models, which is much less than <span
class="math inline">\(2^p\)</span>. (e.g., for <span
class="math inline">\(p=20\)</span>, itâ€™s 211 models vs.Â &gt;1
million).</li>
<li><strong>Con:</strong> <strong>Cannot be used if <span
class="math inline">\(p &gt; n\)</span></strong> (more predictors than
observations), because the initial full model cannot be fit.</li>
<li><strong>Con (for both):</strong> These methods are
<strong>greedy</strong>. A variable added in forward selection is
<em>never removed</em>, and a variable removed in backward selection is
<em>never added back</em>. This means they are not guaranteed to find
the true best model.</li>
</ul>
<h2 id="choosing-the-final-best-model">Choosing the Final Best
Model</h2>
<p>Both forward and backward selection give you a set of candidate
models (e.g., the best 1-variable model, best 2-variable model, etc.).
You must then choose the <em>single best</em> one. The slides show two
main approaches:</p>
<h3 id="a.-direct-error-estimation">A. Direct Error Estimation</h3>
<p>Use a validation set or cross-validation (CV) to estimate the test
error for each model (e.g., the 1-variable, 2-variableâ€¦ models).
<strong>Choose the model with the lowest estimated test
error.</strong></p>
<h3 id="b.-adjusted-metrics-penalizing-for-complexity">B. Adjusted
Metrics (Penalizing for Complexity)</h3>
<p>Standard RSS and <span class="math inline">\(R^2\)</span> will always
improve as you add variables, leading to overfitting. Instead, use
metrics that <em>penalize</em> the model for having too many
predictors.</p>
<ul>
<li><p><strong>Mallowsâ€™ <span
class="math inline">\(C_p\)</span>:</strong> An estimate of test Mean
Squared Error (MSE). <span class="math display">\[C_p = \frac{1}{n} (RSS
+ 2d\hat{\sigma}^2)\]</span> (where <span
class="math inline">\(d\)</span> is the number of predictors, and <span
class="math inline">\(\hat{\sigma}^2\)</span> is an estimate of the
error variance). <strong>You want to find the model with the
<em>minimum</em> <span
class="math inline">\(C_p\)</span>.</strong></p></li>
<li><p><strong>BIC (Bayesian Information Criterion):</strong> <span
class="math display">\[BIC = \frac{1}{n} (RSS +
\log(n)d\hat{\sigma}^2)\]</span> BICâ€™s penalty <span
class="math inline">\(\log(n)\)</span> is stronger than <span
class="math inline">\(C_p\)</span>â€™s (or AICâ€™s) penalty of <span
class="math inline">\(2\)</span>, so it tends to select <em>smaller</em>
(more parsimonious) models. <strong>You want to find the model with the
<em>minimum</em> BIC.</strong></p></li>
<li><p><strong>Adjusted <span
class="math inline">\(R^2\)</span>:</strong> <span
class="math display">\[R^2_{adj} = 1 -
\frac{RSS/(n-d-1)}{TSS/(n-1)}\]</span> (where <span
class="math inline">\(TSS\)</span> is the Total Sum of Squares). Unlike
<span class="math inline">\(R^2\)</span>, this metric can decrease if
adding a variable doesnâ€™t help enough. <strong>You want to find the
model with the <em>maximum</em> Adjusted <span
class="math inline">\(R^2\)</span>.</strong></p></li>
</ul>
<h2 id="python-code-understanding">Python Code Understanding</h2>
<p>The slides use the <code>regsubsets()</code> function from the
<code>leaps</code> package in <strong>R</strong>.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># R Code from slides</span></span><br><span class="line">library<span class="punctuation">(</span>leaps<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Forward Selection</span></span><br><span class="line">regfit.fwd <span class="operator">&lt;-</span> regsubsets<span class="punctuation">(</span>Balance<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Credit<span class="punctuation">,</span> method<span class="operator">=</span><span class="string">&quot;forward&quot;</span><span class="punctuation">,</span> nvmax<span class="operator">=</span><span class="number">11</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Backward Selection</span></span><br><span class="line">regfit.bwd <span class="operator">&lt;-</span> regsubsets<span class="punctuation">(</span>Balance<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Credit<span class="punctuation">,</span> method<span class="operator">=</span><span class="string">&quot;backward&quot;</span><span class="punctuation">,</span> nvmax<span class="operator">=</span><span class="number">11</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>In <strong>Python</strong>, the standard tool for this is
<code>SequentialFeatureSelector</code> from
<strong><code>scikit-learn</code></strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SequentialFeatureSelector</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Credit&#x27; is a pandas DataFrame with &#x27;Balance&#x27; as the target</span></span><br><span class="line">X = Credit.drop(<span class="string">&#x27;Balance&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = Credit[<span class="string">&#x27;Balance&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the linear regression estimator</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Forward Selection ---</span></span><br><span class="line"><span class="comment"># direction=&#x27;forward&#x27; starts with 0 features and adds them</span></span><br><span class="line"><span class="comment"># To get the best 4-variable model, for example:</span></span><br><span class="line">sfs_forward = SequentialFeatureSelector(</span><br><span class="line">    model,</span><br><span class="line">    n_features_to_select=<span class="number">4</span>,</span><br><span class="line">    direction=<span class="string">&#x27;forward&#x27;</span>,</span><br><span class="line">    cv=<span class="literal">None</span> <span class="comment"># Or use cross-validation, e.g., cv=10</span></span><br><span class="line">)</span><br><span class="line">sfs_forward.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Forward selection best 4 features:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(sfs_forward.get_feature_names_out())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Backward Selection ---</span></span><br><span class="line"><span class="comment"># direction=&#x27;backward&#x27; starts with all features and removes them</span></span><br><span class="line">sfs_backward = SequentialFeatureSelector(</span><br><span class="line">    model,</span><br><span class="line">    n_features_to_select=<span class="number">4</span>,</span><br><span class="line">    direction=<span class="string">&#x27;backward&#x27;</span>,</span><br><span class="line">    cv=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line">sfs_backward.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nBackward selection best 4 features:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(sfs_backward.get_feature_names_out())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: To replicate the plots, you would loop this process,</span></span><br><span class="line"><span class="comment"># changing &#x27;n_features_to_select&#x27; from 1 to p,</span></span><br><span class="line"><span class="comment"># record the model scores (e.g., RSS, AIC, BIC) at each step,</span></span><br><span class="line"><span class="comment"># and then plot the results.</span></span><br></pre></td></tr></table></figure>
<h2 id="important-images">Important Images</h2>
<ol type="1">
<li><p><strong>Slide <code>...230014.png</code> (Forward Selection
Plots) &amp; <code>...230036.png</code> (Backward Selection
Plots):</strong></p>
<ul>
<li><strong>What they are:</strong> These <span class="math inline">\(2
\times 2\)</span> plot grids are the most important visuals. They show
<strong>Residual Sum of Squares (RSS)</strong>, <strong>Adjusted <span
class="math inline">\(R^2\)</span></strong>, <strong>BIC</strong>, and
<strong>Mallowsâ€™ <span class="math inline">\(C_p\)</span></strong>
plotted against the <em>Number of Variables</em>.</li>
<li><strong>Why theyâ€™re important:</strong> They are the
<strong>decision-making tool</strong>. You use these plots to choose the
best model.
<ul>
<li>You look for the â€œelbowâ€ or <strong>minimum</strong> value for BIC
and <span class="math inline">\(C_p\)</span>.</li>
<li>You look for the â€œpeakâ€ or <strong>maximum</strong> value for
Adjusted <span class="math inline">\(R^2\)</span>.</li>
<li>(RSS is not used for selection as it always decreases).</li>
</ul></li>
</ul></li>
<li><p><strong>Slide <code>...230040.png</code> (Find the best
model):</strong></p>
<ul>
<li><strong>What it is:</strong> This slide shows a close-up of the
<span class="math inline">\(C_p\)</span>, BIC, and Adjusted <span
class="math inline">\(R^2\)</span> plots, with the â€œbestâ€ model (the
min/max) marked with a blue â€˜xâ€™.</li>
<li><strong>Why itâ€™s important:</strong> It explicitly states the
selection criteria. The text highlights that BIC suggests a 4-variable
model, while the other two are â€œrather flatâ€ after 4, making the choice
less obvious but pointing to a simple model.</li>
</ul></li>
<li><p><strong>Slide <code>...230045.png</code> (BIC vs.Â Validation
vs.Â CV):</strong></p>
<ul>
<li><strong>What it is:</strong> This shows three plots for selecting
the best model using different criteria: BIC, Validation Set Error, and
Cross-Validation Error.</li>
<li><strong>Why itâ€™s important:</strong> It shows that <strong>different
selection criteria can lead to different â€œbestâ€ models</strong>. Here,
BIC (a mathematical adjustment) picks a 4-variable model, while
validation and CV (direct error estimation) both pick a 6-variable
model.</li>
</ul></li>
</ol>
<p>The slides use the <code>Credit</code> dataset to demonstrate two key
tasks: 1. <strong>Running</strong> different subset selection algorithms
(forward, backward, best). 2. <strong>Using</strong> various statistical
metrics (BIC, <span class="math inline">\(C_p\)</span>, CV error) to
choose the single best model.</p>
<h2 id="comparing-selection-algorithms-the-path">Comparing Selection
Algorithms (The Path)</h2>
<p>This part of the example compares the <em>sequence</em> of models
selected by â€œForward Stepwiseâ€ selection versus â€œBest Subsetâ€
selection.</p>
<p><strong>Key Result (from Table 6.1):</strong></p>
<p>This table is the most important result for comparing the
algorithms.</p>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Variables</th>
<th style="text-align: left;">Best Subset</th>
<th style="text-align: left;">Forward Stepwise</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>one</strong></td>
<td style="text-align: left;"><code>rating</code></td>
<td style="text-align: left;"><code>rating</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>two</strong></td>
<td style="text-align: left;"><code>rating</code>,
<code>income</code></td>
<td style="text-align: left;"><code>rating</code>,
<code>income</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>three</strong></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code></td>
</tr>
<tr>
<td style="text-align: left;"><strong>four</strong></td>
<td style="text-align: left;"><code>cards</code>, <code>income</code>,
<code>student</code>, <code>limit</code></td>
<td style="text-align: left;"><code>rating</code>, <code>income</code>,
<code>student</code>, <code>limit</code></td>
</tr>
</tbody>
</table>
<p><strong>Summary of this result:</strong></p>
<ul>
<li><strong>Identical for 1, 2, and 3 variables:</strong> Both methods
agree on the best one-variable model (<code>rating</code>), the best
two-variable model (<code>rating</code>, <code>income</code>), and the
best three-variable model (<code>rating</code>, <code>income</code>,
<code>student</code>).</li>
<li><strong>They Diverge at 4 variables:</strong>
<ul>
<li><strong>Forward selection</strong> is <em>greedy</em>. It started
with <code>rating</code>, <code>income</code>, <code>student</code> and
was â€œstuckâ€ with them. It then added <code>limit</code>, as that was the
best variable to <em>add</em> to its existing 3-variable model.</li>
<li><strong>Best subset selection</strong> is <em>not</em> greedy. It
tests all possible 4-variable combinations. It discovered that the model
<code>cards</code>, <code>income</code>, <code>student</code>,
<code>limit</code> has a slightly lower RSS than the model forward
selection found.</li>
</ul></li>
<li><strong>Main Takeaway:</strong> This demonstrates the limitation of
a greedy algorithm. Forward selection missed the â€œtrueâ€ best 4-variable
model because it was locked into its previous choices and couldnâ€™t â€œswap
outâ€ <code>rating</code> for <code>cards</code>.</li>
</ul>
<h2 id="choosing-the-single-best-model-the-destination">Choosing the
Single Best Model (The Destination)</h2>
<p>This is the most critical part of the analysis. After running a
selection algorithm (like forward, backward, or best subset), you get a
list of the â€œbestâ€ models for each size (best 1-variable, best
2-variable, etc.). Now you must decide: <strong>is the best model the
4-variable one, the 6-variable one, or another?</strong></p>
<p>The slides show several plots to help make this decision, all plotted
against the â€œNumber of Predictors.â€</p>
<p><strong>Summary of Plot Results:</strong></p>
<p>Hereâ€™s what each plot tells you:</p>
<ul>
<li><strong>Residual Sum of Squares (RSS)</strong> (e.g., in slide
<code>...230014.png</code>, top-left)
<ul>
<li><strong>What it shows:</strong> RSS <em>always</em> decreases as you
add more variables. It drops sharply until 4 variables, then flattens
out.</li>
<li><strong>Conclusion:</strong> This plot is <strong>not useful for
picking the best model</strong> because it will always pick the full
model, which is overfit. Itâ€™s only used to see the diminishing returns
of adding new variables.</li>
</ul></li>
<li><strong>Adjusted <span class="math inline">\(R^2\)</span></strong>
(e.g., in slide <code>...230040.png</code>, right)
<ul>
<li><strong>What it shows:</strong> This metric penalizes adding useless
variables. The plot rises quickly, then flattens, peaking at its
<strong>maximum value around 6 or 7 variables</strong>.</li>
<li><strong>Conclusion:</strong> This metric suggests a <strong>6 or
7-variable model</strong>.</li>
</ul></li>
<li><strong>Mallowsâ€™ <span class="math inline">\(C_p\)</span></strong>
(e.g., in slide <code>...230040.png</code>, left)
<ul>
<li><strong>What it shows:</strong> This is an estimate of test error.
We want the model with the <strong>minimum <span
class="math inline">\(C_p\)</span></strong>. The plot drops to a low
value at 4 variables and stays low, with its absolute minimum around
<strong>6 or 7 variables</strong>.</li>
<li><strong>Conclusion:</strong> This metric also suggests a <strong>6
or 7-variable model</strong>.</li>
</ul></li>
<li><strong>BIC (Bayesian Information Criterion)</strong> (e.g., in
slide <code>...230040.png</code>, center)
<ul>
<li><strong>What it shows:</strong> This is another estimate of test
error, but it has a <em>stronger penalty</em> for model complexity. The
plot shows a clear â€œUâ€ shape, reaching its <strong>minimum value at 4
variables</strong> and then <em>increasing</em> afterward.</li>
<li><strong>Conclusion:</strong> This metric strongly suggests a
<strong>4-variable model</strong>.</li>
</ul></li>
<li><strong>Validation Set &amp; Cross-Validation (CV) Error</strong>
(Slide <code>...230045.png</code>)
<ul>
<li><strong>What it shows:</strong> These plots show the <em>direct</em>
estimate of test error (not a mathematical adjustment like BIC or <span
class="math inline">\(C_p\)</span>). Both the validation set error and
the 10-fold CV error show a â€œUâ€ shape.</li>
<li><strong>Conclusion:</strong> Both methods reach their
<strong>minimum error at 6 variables</strong>. This is considered a very
reliable result.</li>
</ul></li>
</ul>
<h2 id="final-summary-of-results">Final Summary of Results</h2>
<p>The analysis of the <code>Credit</code> dataset reveals two strong
candidates for the â€œbestâ€ model, depending on your goal:</p>
<ol type="1">
<li><p><strong>The 6-Variable Model:</strong> This model is supported by
the <strong>Adjusted <span class="math inline">\(R^2\)</span></strong>,
<strong>Mallowsâ€™ <span class="math inline">\(C_p\)</span></strong>, and
(most importantly) the <strong>Validation Set</strong> and
<strong>10-fold Cross-Validation</strong> results. These metrics all
indicate that the 6-variable model has the <strong>lowest prediction
error</strong> on new data.</p></li>
<li><p><strong>The 4-Variable Model:</strong> This model is supported by
<strong>BIC</strong>. Because BIC penalizes complexity more heavily, it
selects a simpler (more <em>parsimonious</em>) model.</p></li>
</ol>
<p><strong>Overall Conclusion:</strong> If your primary goal is
<strong>maximum predictive accuracy</strong>, you should choose the
<strong>6-variable model</strong>. If your goal is a <strong>simpler,
more interpretable model</strong> that is still very good (and avoids
any risk of overfitting), the <strong>4-variable model</strong> is an
excellent choice.</p>
<h1 id="section">5.</h1>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/10/06/5054C5/" rel="prev" title="MSDM 5054 - Statistical Machine Learning-L5">
      <i class="fa fa-chevron-left"></i> MSDM 5054 - Statistical Machine Learning-L5
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#linear-model-selection-and-regularization-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.</span> <span class="nav-text">1.
Linear Model Selection and Regularization çº¿æ€§æ¨¡å‹é€‰æ‹©ä¸æ­£åˆ™åŒ–</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#summary-of-core-concepts"><span class="nav-number">1.1.</span> <span class="nav-text">Summary of Core Concepts</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mathematical-understanding-key-questions-%E6%95%B0%E5%AD%A6%E7%90%86%E8%A7%A3%E4%B8%8E%E5%85%B3%E9%94%AE%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.</span> <span class="nav-text">Mathematical
Understanding &amp; Key Questions æ•°å­¦ç†è§£ä¸å…³é”®é—®é¢˜</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#how-to-compare-which-model-is-better"><span class="nav-number">1.2.1.</span> <span class="nav-text">How to compare which model
is better?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-use-r2-in-step-2"><span class="nav-number">1.2.2.</span> <span class="nav-text">Why use \(R^2\) in Step 2?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-cant-we-use-training-error-in-step-3"><span class="nav-number">1.2.3.</span> <span class="nav-text">Why canâ€™t we use
training error in Step 3?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-analysis"><span class="nav-number">1.3.</span> <span class="nav-text">Code Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#key-functions"><span class="nav-number">1.3.1.</span> <span class="nav-text">Key Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#analysis-of-the-output"><span class="nav-number">1.3.2.</span> <span class="nav-text">Analysis of the Output</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#conceptual-overview-the-why"><span class="nav-number">1.4.</span> <span class="nav-text">Conceptual Overview: The â€œWhyâ€</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#questions"><span class="nav-number">1.5.</span> <span class="nav-text">Questions ğŸ¯</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#q1-how-to-compare-which-model-is-better"><span class="nav-number">1.5.1.</span> <span class="nav-text">Q1: â€œHow to compare
which model is better?â€</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q2-why-using-r2-for-step-2"><span class="nav-number">1.5.2.</span> <span class="nav-text">Q2: â€œWhy using \(R^2\) for step 2?â€</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q3-cannot-use-training-error-in-step-3.-why-not-%E6%AD%A5%E9%AA%A4-3-%E4%B8%AD%E4%B8%8D%E8%83%BD%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE-%E4%B8%BA%E4%BB%80%E4%B9%88"><span class="nav-number">1.5.3.</span> <span class="nav-text">Q3:
â€œCannot use training error in Step 3.â€ Why not? â€œæ­¥éª¤ 3
ä¸­ä¸èƒ½ä½¿ç”¨è®­ç»ƒè¯¯å·®ã€‚â€ ä¸ºä»€ä¹ˆï¼Ÿ</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mathematical-deep-dive"><span class="nav-number">1.6.</span> <span class="nav-text">Mathematical Deep Dive ğŸ§®</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#detailed-code-analysis"><span class="nav-number">1.7.</span> <span class="nav-text">Detailed Code Analysis ğŸ’»</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#key-functions-1"><span class="nav-number">1.7.1.</span> <span class="nav-text">Key Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary-of-outputs-slides-...221255.png-...221309.png"><span class="nav-number">1.7.2.</span> <span class="nav-text">Summary
of Outputs (Slides ...221255.png &amp;
...221309.png)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#the-core-problem-training-error-vs.-test-error-%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE-vs.-%E6%B5%8B%E8%AF%95%E8%AF%AF%E5%B7%AE"><span class="nav-number">2.</span> <span class="nav-text">2.
The Core Problem: Training Error vs.Â Test Error æ ¸å¿ƒé—®é¢˜ï¼šè®­ç»ƒè¯¯å·®
vs.Â æµ‹è¯•è¯¯å·®</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#basic-metrics-measures-of-fit"><span class="nav-number">2.1.</span> <span class="nav-text">Basic Metrics (Measures of
Fit)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#residue-error-%E6%AE%8B%E5%B7%AE%E8%AF%AF%E5%B7%AE"><span class="nav-number">2.1.1.</span> <span class="nav-text">Residue (Error) æ®‹å·®ï¼ˆè¯¯å·®ï¼‰</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#residual-sum-of-squares-rss-%E6%AE%8B%E5%B7%AE%E5%B9%B3%E6%96%B9%E5%92%8C-rss"><span class="nav-number">2.1.2.</span> <span class="nav-text">Residual Sum of
Squares (RSS) æ®‹å·®å¹³æ–¹å’Œ (RSS)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#r-squared-r2"><span class="nav-number">2.1.3.</span> <span class="nav-text">R-squared (\(R^2\))</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#advanced-metrics-for-model-selection-%E9%AB%98%E7%BA%A7%E6%8C%87%E6%A0%87%E7%94%A8%E4%BA%8E%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="nav-number">2.2.</span> <span class="nav-text">Advanced
Metrics (For Model Selection) é«˜çº§æŒ‡æ ‡ï¼ˆç”¨äºæ¨¡å‹é€‰æ‹©ï¼‰</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#adjusted-r2"><span class="nav-number">2.2.1.</span> <span class="nav-text">Adjusted \(R^2\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#akaike-information-criterion-aic"><span class="nav-number">2.2.2.</span> <span class="nav-text">Akaike Information Criterion
(AIC)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bayesian-information-criterion-bic"><span class="nav-number">2.2.3.</span> <span class="nav-text">Bayesian Information
Criterion (BIC)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-deeper-theory-why-aic-works"><span class="nav-number">2.3.</span> <span class="nav-text">The Deeper Theory: Why AIC
Works</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aicbic-for-linear-regression"><span class="nav-number">2.4.</span> <span class="nav-text">AIC&#x2F;BIC for Linear Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-core-problem-training-error-vs.-test-error"><span class="nav-number">2.5.</span> <span class="nav-text">The Core
Problem: Training Error vs.Â Test Error</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#basic-metrics-measures-of-fit-1"><span class="nav-number">2.6.</span> <span class="nav-text">Basic Metrics (Measures of
Fit)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#residue-error"><span class="nav-number">2.6.1.</span> <span class="nav-text">Residue (Error)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#residual-sum-of-squares-rss"><span class="nav-number">2.6.2.</span> <span class="nav-text">Residual Sum of Squares (RSS)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#r-squared-r2-1"><span class="nav-number">2.6.3.</span> <span class="nav-text">R-squared (\(R^2\))</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#advanced-metrics-for-model-selection"><span class="nav-number">2.7.</span> <span class="nav-text">Advanced Metrics (For
Model Selection)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#adjusted-r2-1"><span class="nav-number">2.7.1.</span> <span class="nav-text">Adjusted \(R^2\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#akaike-information-criterion-aic-1"><span class="nav-number">2.7.2.</span> <span class="nav-text">Akaike Information Criterion
(AIC)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bayesian-information-criterion-bic-1"><span class="nav-number">2.7.3.</span> <span class="nav-text">Bayesian Information
Criterion (BIC)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-deeper-theory-why-aic-works-1"><span class="nav-number">2.8.</span> <span class="nav-text">The Deeper Theory: Why AIC
Works</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aicbic-for-linear-regression-1"><span class="nav-number">2.9.</span> <span class="nav-text">AIC&#x2F;BIC for Linear
Regression</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#variable-selection"><span class="nav-number">3.</span> <span class="nav-text">3. Variable Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#core-concept-the-problem-of-variable-selection"><span class="nav-number">3.1.</span> <span class="nav-text">Core Concept:
The Problem of Variable Selection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#method-1-best-subset-selection-bss"><span class="nav-number">3.2.</span> <span class="nav-text">Method 1: Best Subset
Selection (BSS)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#conceptual-algorithm"><span class="nav-number">3.2.1.</span> <span class="nav-text">Conceptual Algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mathematical-computational-cost-from-slide-225641.png"><span class="nav-number">3.2.2.</span> <span class="nav-text">Mathematical
&amp; Computational Cost (from slide 225641.png)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#method-2-forward-stepwise-selection-fss"><span class="nav-number">3.3.</span> <span class="nav-text">Method 2: Forward
Stepwise Selection (FSS)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#conceptual-algorithm-from-slides-225645.png-225648.png"><span class="nav-number">3.3.1.</span> <span class="nav-text">Conceptual
Algorithm (from slides 225645.png &amp;
225648.png)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mathematical-computational-cost-from-slide-225651.png"><span class="nav-number">3.3.2.</span> <span class="nav-text">Mathematical
&amp; Computational Cost (from slide 225651.png)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#how-to-choose-the-best-model-the-criteria"><span class="nav-number">3.4.</span> <span class="nav-text">4. How to Choose the
â€œBestâ€ Model: The Criteria</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python-code-analysis-slide-225546.jpg"><span class="nav-number">3.5.</span> <span class="nav-text">5. Python Code Analysis
(Slide 225546.jpg)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#block-1-load-the-credit-dataset"><span class="nav-number">3.5.1.</span> <span class="nav-text">Block 1: Load the Credit
dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#block-2-plot-scatterplot-matrix"><span class="nav-number">3.5.2.</span> <span class="nav-text">Block 2: Plot scatterplot
matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#block-3-best-subset-selection"><span class="nav-number">3.5.3.</span> <span class="nav-text">Block 3: Best Subset
Selection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#synthesizing-the-results-the-plots"><span class="nav-number">3.6.</span> <span class="nav-text">Synthesizing the Results
(The Plots)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#subset-selection"><span class="nav-number">4.</span> <span class="nav-text">4. Subset Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#summary-of-subset-selection"><span class="nav-number">4.1.</span> <span class="nav-text">Summary of Subset Selection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#stepwise-selection-algorithms"><span class="nav-number">4.2.</span> <span class="nav-text">Stepwise Selection
Algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#forward-stepwise-selection"><span class="nav-number">4.2.1.</span> <span class="nav-text">Forward Stepwise Selection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#backward-stepwise-selection"><span class="nav-number">4.2.2.</span> <span class="nav-text">Backward Stepwise Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#pros-and-cons-backward-selection"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">Pros and Cons (Backward
Selection)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#choosing-the-final-best-model"><span class="nav-number">4.3.</span> <span class="nav-text">Choosing the Final Best
Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#a.-direct-error-estimation"><span class="nav-number">4.3.1.</span> <span class="nav-text">A. Direct Error Estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#b.-adjusted-metrics-penalizing-for-complexity"><span class="nav-number">4.3.2.</span> <span class="nav-text">B. Adjusted
Metrics (Penalizing for Complexity)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python-code-understanding"><span class="nav-number">4.4.</span> <span class="nav-text">Python Code Understanding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#important-images"><span class="nav-number">4.5.</span> <span class="nav-text">Important Images</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#comparing-selection-algorithms-the-path"><span class="nav-number">4.6.</span> <span class="nav-text">Comparing Selection
Algorithms (The Path)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#choosing-the-single-best-model-the-destination"><span class="nav-number">4.7.</span> <span class="nav-text">Choosing the
Single Best Model (The Destination)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#final-summary-of-results"><span class="nav-number">4.8.</span> <span class="nav-text">Final Summary of Results</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#section"><span class="nav-number">5.</span> <span class="nav-text">5.</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> å¼ºåŠ›é©±åŠ¨
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
