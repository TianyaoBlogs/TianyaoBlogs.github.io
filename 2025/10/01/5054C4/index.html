<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianyaoblogs.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-4 Lecturer: Prof.XIA DONG 1. What is Classification? Classification is a type of supervised machine learning where the goal is to predict a categorical or qualitative response. Unl">
<meta property="og:type" content="article">
<meta property="og:title" content="MSDM 5054 - Statistical Machine Learning-L4">
<meta property="og:url" content="https://tianyaoblogs.github.io/2025/10/01/5054C4/index.html">
<meta property="og:site_name" content="TianyaoBlogs">
<meta property="og:description" content="ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-4 Lecturer: Prof.XIA DONG 1. What is Classification? Classification is a type of supervised machine learning where the goal is to predict a categorical or qualitative response. Unl">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-10-01T13:00:00.000Z">
<meta property="article:modified_time" content="2025-10-18T15:00:24.287Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://tianyaoblogs.github.io/2025/10/01/5054C4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>MSDM 5054 - Statistical Machine Learning-L4 | TianyaoBlogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TianyaoBlogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/10/01/5054C4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MSDM 5054 - Statistical Machine Learning-L4
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2025-10-01 21:00:00" itemprop="dateCreated datePublished" datetime="2025-10-01T21:00:00+08:00">2025-10-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2025-10-18 23:00:24" itemprop="dateModified" datetime="2025-10-18T23:00:24+08:00">2025-10-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">åˆ†ç±»äº</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>ç»Ÿè®¡æœºå™¨å­¦ä¹ Lecture-4</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="what-is-classification">1. What is Classification?</h1>
<p>Classification is a type of <strong>supervised machine
learning</strong> where the goal is to predict a
<strong>categorical</strong> or qualitative response. Unlike regression
where you predict a continuous numerical value (like a price or
temperature), classification assigns an input to a specific category or
class.
åˆ†ç±»æ˜¯ä¸€ç§<strong>ç›‘ç£å¼æœºå™¨å­¦ä¹ </strong>ï¼Œå…¶ç›®æ ‡æ˜¯é¢„æµ‹<strong>åˆ†ç±»</strong>æˆ–å®šæ€§å“åº”ã€‚ä¸é¢„æµ‹è¿ç»­æ•°å€¼ï¼ˆä¾‹å¦‚ä»·æ ¼æˆ–æ¸©åº¦ï¼‰çš„å›å½’ä¸åŒï¼Œåˆ†ç±»å°†è¾“å…¥åˆ†é…åˆ°ç‰¹å®šçš„ç±»åˆ«æˆ–ç±»åˆ«ã€‚</p>
<p><strong>Key characteristics:</strong></p>
<ul>
<li><p><strong>Goal:</strong> Predict the class of a subject based on
input features.</p></li>
<li><p><strong>Output (Response):</strong> The output is a category,
such as â€˜Yesâ€™/â€˜Noâ€™, â€˜Spamâ€™/â€˜Not Spamâ€™, or
â€˜Highâ€™/â€˜Mediumâ€™/â€˜Lowâ€™.</p></li>
<li><p><strong>Applications:</strong> Common examples include email spam
detectors, medical diagnosis (e.g., virus carrier vs.Â non-carrier), and
fraud detection.</p>
<ul>
<li><strong>ç›®æ ‡</strong>ï¼šæ ¹æ®è¾“å…¥ç‰¹å¾é¢„æµ‹ä¸»é¢˜çš„ç±»åˆ«ã€‚</li>
<li><strong>è¾“å‡ºï¼ˆå“åº”ï¼‰ï¼š</strong>è¾“å‡ºæ˜¯ä¸€ä¸ªç±»åˆ«ï¼Œä¾‹å¦‚â€œæ˜¯â€/â€œå¦â€ã€â€œåƒåœ¾é‚®ä»¶â€/â€œéåƒåœ¾é‚®ä»¶â€æˆ–â€œé«˜â€/â€œä¸­â€/â€œä½â€ã€‚</li>
<li><strong>åº”ç”¨</strong>ï¼šå¸¸è§ç¤ºä¾‹åŒ…æ‹¬åƒåœ¾é‚®ä»¶æ£€æµ‹å™¨ã€åŒ»å­¦è¯Šæ–­ï¼ˆä¾‹å¦‚ï¼Œç—…æ¯’æºå¸¦è€…ä¸éç—…æ¯’æºå¸¦è€…ï¼‰å’Œæ¬ºè¯ˆæ£€æµ‹ã€‚
The example used in the slides is a credit card <strong>Default
dataset</strong>. The goal is to predict whether a customer will
<strong>default</strong> (â€˜Yesâ€™ or â€˜Noâ€™) on their payments based on
their monthly <strong>income</strong> and account
<strong>balance</strong>.</li>
</ul></li>
</ul>
<p>## Why Not Use Linear Regression?ä¸ºä»€ä¹ˆä¸ä½¿ç”¨çº¿æ€§å›å½’ï¼Ÿ</p>
<p>At first, it might seem possible to use linear regression for
classification. For a binary (two-class) problem like the default
dataset, you could code the outcomes as numbers, for example:</p>
<ul>
<li>Default = â€˜Noâ€™ =&gt; <span class="math inline">\(y = 0\)</span></li>
<li>Default = â€˜Yesâ€™ =&gt; <span class="math inline">\(y =
1\)</span></li>
</ul>
<p>You could then fit a standard linear regression model: <span
class="math inline">\(Y \approx \beta_0 + \beta_1 X\)</span>. In this
context, we would interpret the prediction <span
class="math inline">\(\hat{y}\)</span> as the <em>probability</em> of
default, so weâ€™d be modeling <span class="math inline">\(P(Y=1|X) =
\beta_0 + \beta_1 X\)</span>.</p>
<p>However, this approach has two major problems:
ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æœ‰ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š <strong>1. The Output Is Not a
Probability</strong> A linear model can produce outputs that are less
than 0 or greater than 1. This doesnâ€™t make sense for a probability,
which must always be between 0 and 1.</p>
<p>The image below is the most important one for understanding this
issue. The left plot shows a linear regression line fit to the 0/1
default data. You can see the line goes below 0 and would eventually go
above 1 for higher balances. The right plot shows a logistic regression
curve, which always stays between 0 and 1.</p>
<ul>
<li><strong>Left (Linear Regression):</strong> The straight blue line
predicts probabilities &lt; 0 for low balances.</li>
<li><strong>Right (Logistic Regression):</strong> The S-shaped blue
curve correctly constrains the probability output between 0 and 1.</li>
</ul>
<p><strong>2. It Doesnâ€™t Work for Multi-Class Problems</strong> If you
have more than two categories (e.g., â€˜mildâ€™, â€˜moderateâ€™, â€˜severeâ€™), you
might code them as 0, 1, and 2. A linear regression model would
incorrectly assume that the â€œdistanceâ€ between â€˜mildâ€™ and â€˜moderateâ€™ is
the same as the distance between â€˜moderateâ€™ and â€˜severeâ€™, which is
usually not a valid assumption.</p>
<p><strong>1. è¾“å‡ºä¸æ˜¯æ¦‚ç‡</strong> çº¿æ€§æ¨¡å‹å¯ä»¥äº§ç”Ÿå°äº 0 æˆ–å¤§äº 1
çš„è¾“å‡ºã€‚è¿™å¯¹äºæ¦‚ç‡æ¥è¯´æ¯«æ— æ„ä¹‰ï¼Œå› ä¸ºæ¦‚ç‡å¿…é¡»å§‹ç»ˆä»‹äº 0 å’Œ 1 ä¹‹é—´ã€‚</p>
<p>ä¸‹å›¾æ˜¯ç†è§£è¿™ä¸ªé—®é¢˜æœ€é‡è¦çš„å›¾ã€‚å·¦å›¾æ˜¾ç¤ºäº†ä¸ 0/1
é»˜è®¤æ•°æ®æ‹Ÿåˆçš„çº¿æ€§å›å½’çº¿ã€‚æ‚¨å¯ä»¥çœ‹åˆ°ï¼Œè¯¥çº¿ä½äº
0ï¼Œå¹¶ä¸”æœ€ç»ˆä¼šéšç€ä½™é¢çš„å¢åŠ è€Œé«˜äº
1ã€‚å³å›¾æ˜¾ç¤ºäº†é€»è¾‘å›å½’æ›²çº¿ï¼Œå®ƒå§‹ç»ˆä¿æŒåœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚</p>
<ul>
<li><strong>å·¦å›¾ï¼ˆçº¿æ€§å›å½’ï¼‰ï¼š</strong>è“è‰²ç›´çº¿é¢„æµ‹ä½ä½™é¢çš„æ¦‚ç‡å°äº
0ã€‚</li>
<li><strong>å³å›¾ï¼ˆé€»è¾‘å›å½’ï¼‰ï¼š</strong>S
å½¢è“è‰²æ›²çº¿æ­£ç¡®åœ°å°†æ¦‚ç‡è¾“å‡ºé™åˆ¶åœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚</li>
</ul>
<p><strong>2.å®ƒä¸é€‚ç”¨äºå¤šç±»åˆ«é—®é¢˜</strong>
å¦‚æœæ‚¨æœ‰ä¸¤ä¸ªä»¥ä¸Šçš„ç±»åˆ«ï¼ˆä¾‹å¦‚ï¼Œâ€œè½»åº¦â€ã€â€œä¸­åº¦â€ã€â€œé‡åº¦â€ï¼‰ï¼Œæ‚¨å¯èƒ½ä¼šå°†å®ƒä»¬ç¼–ç ä¸º
0ã€1 å’Œ
2ã€‚çº¿æ€§å›å½’æ¨¡å‹ä¼šé”™è¯¯åœ°å‡è®¾â€œè½»åº¦â€å’Œâ€œä¸­åº¦â€ä¹‹é—´çš„â€œè·ç¦»â€ä¸â€œä¸­åº¦â€å’Œâ€œé‡åº¦â€ä¹‹é—´çš„è·ç¦»ç›¸åŒï¼Œè¿™é€šå¸¸ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å‡è®¾ã€‚</p>
<p>## The Solution: Logistic Regression</p>
<p>Instead of modeling the response <span
class="math inline">\(y\)</span> directly, logistic regression models
the <strong>probability</strong> that <span
class="math inline">\(y\)</span> belongs to a particular class. To solve
the issue of the output not being a probability, it uses the
<strong>logistic function</strong> (also known as the sigmoid
function).</p>
<p>This function takes any real-valued input and squeezes it into an
output between 0 and 1.</p>
<p>The formula for the probability in a logistic regression model is:
<span class="math display">\[P(Y=1|X) = \frac{e^{\beta_0 + \beta_1 X}}{1
+ e^{\beta_0 + \beta_1 X}}\]</span> This S-shaped function, shown in the
right-hand plot above, ensures that the output is always a valid
probability. We can then set a threshold (e.g., 0.5) to make the final
class prediction. If <span class="math inline">\(P(Y=1|X) &gt;
0.5\)</span>, we predict â€˜Yesâ€™; otherwise, we predict â€˜Noâ€™.</p>
<p>## è§£å†³æ–¹æ¡ˆï¼šé€»è¾‘å›å½’</p>
<p>é€»è¾‘å›å½’ä¸æ˜¯ç›´æ¥å¯¹å“åº” <span class="math inline">\(y\)</span>
è¿›è¡Œå»ºæ¨¡ï¼Œè€Œæ˜¯å¯¹ <span class="math inline">\(y\)</span>
å±äºç‰¹å®šç±»åˆ«çš„<strong>æ¦‚ç‡</strong>è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¾“å‡ºä¸æ˜¯æ¦‚ç‡çš„é—®é¢˜ï¼Œå®ƒä½¿ç”¨äº†<strong>é€»è¾‘å‡½æ•°</strong>ï¼ˆä¹Ÿç§°ä¸º
S å‹å‡½æ•°ï¼‰ã€‚</p>
<p>æ­¤å‡½æ•°æ¥å—ä»»ä½•å®å€¼è¾“å…¥ï¼Œå¹¶å°†å…¶å‹ç¼©ä¸ºä»‹äº 0 å’Œ 1 ä¹‹é—´çš„è¾“å‡ºã€‚</p>
<p>é€»è¾‘å›å½’æ¨¡å‹ä¸­çš„æ¦‚ç‡å…¬å¼ä¸ºï¼š <span class="math display">\[P(Y=1|X) =
\frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}\]</span>
å¦‚ä¸Šå›¾å³ä¾§æ‰€ç¤ºï¼Œè¿™ä¸ª S
å½¢å‡½æ•°ç¡®ä¿è¾“å‡ºå§‹ç»ˆæ˜¯æœ‰æ•ˆæ¦‚ç‡ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ªé˜ˆå€¼ï¼ˆä¾‹å¦‚
0.5ï¼‰æ¥è¿›è¡Œæœ€ç»ˆçš„ç±»åˆ«é¢„æµ‹ã€‚å¦‚æœ <span class="math inline">\(P(Y=1|X)
&gt; 0.5\)</span>ï¼Œåˆ™é¢„æµ‹â€œæ˜¯â€ï¼›å¦åˆ™ï¼Œé¢„æµ‹â€œå¦â€ã€‚</p>
<p>## Data Visualization &amp; Code in Python</p>
<p>The slides use R to visualize the data. The boxplots are particularly
important because they show which variable is a better predictor.</p>
<ul>
<li><p><strong>Balance vs.Â Default:</strong> The boxplots for balance
show a clear difference. The median balance for those who default
(â€˜Yesâ€™) is much higher than for those who do not (â€˜Noâ€™). This suggests
<strong>balance is a strong predictor</strong>.</p></li>
<li><p><strong>Income vs.Â Default:</strong> The boxplots for income show
a lot of overlap. The median incomes for both groups are very similar.
This suggests <strong>income is a weak predictor</strong>.</p></li>
<li><p><strong>ä½™é¢
vs.Â è¿çº¦</strong>ï¼šä½™é¢çš„ç®±çº¿å›¾æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„å·®å¼‚ã€‚è¿çº¦è€…ï¼ˆâ€œæ˜¯â€ï¼‰çš„ä½™é¢ä¸­ä½æ•°è¿œé«˜äºæœªè¿çº¦è€…ï¼ˆâ€œå¦â€ï¼‰ã€‚è¿™è¡¨æ˜<strong>ä½™é¢æ˜¯ä¸€ä¸ªå¼ºæœ‰åŠ›çš„é¢„æµ‹æŒ‡æ ‡</strong>ã€‚</p></li>
<li><p><strong>æ”¶å…¥
vs.Â è¿çº¦</strong>ï¼šæ”¶å…¥çš„ç®±çº¿å›¾æ˜¾ç¤ºå‡ºå¾ˆå¤§çš„é‡å ã€‚ä¸¤ç»„çš„æ”¶å…¥ä¸­ä½æ•°éå¸¸ç›¸ä¼¼ã€‚è¿™è¡¨æ˜<strong>æ”¶å…¥æ˜¯ä¸€ä¸ªå¼±çš„é¢„æµ‹æŒ‡æ ‡</strong>ã€‚</p></li>
</ul>
<p>Hereâ€™s how you could perform similar analysis and modeling in Python
using <code>seaborn</code> and <code>scikit-learn</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;default_data.csv&#x27; has columns: &#x27;default&#x27; (Yes/No), &#x27;balance&#x27;, &#x27;income&#x27;</span></span><br><span class="line"><span class="comment"># You would load your data like this:</span></span><br><span class="line"><span class="comment"># df = pd.read_csv(&#x27;default_data.csv&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># For demonstration, let&#x27;s create some sample data</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;balance&#x27;</span>: [<span class="number">1200</span>, <span class="number">2100</span>, <span class="number">800</span>, <span class="number">1800</span>, <span class="number">500</span>, <span class="number">1600</span>, <span class="number">2200</span>, <span class="number">1900</span>],</span><br><span class="line">    <span class="string">&#x27;income&#x27;</span>: [<span class="number">45000</span>, <span class="number">60000</span>, <span class="number">30000</span>, <span class="number">55000</span>, <span class="number">25000</span>, <span class="number">48000</span>, <span class="number">70000</span>, <span class="number">65000</span>],</span><br><span class="line">    <span class="string">&#x27;default&#x27;</span>: [<span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>, <span class="string">&#x27;Yes&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. Data Visualization (like the slides) ---</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">14</span>, <span class="number">5</span>))</span><br><span class="line">fig.suptitle(<span class="string">&#x27;Predictor Analysis for Default&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boxplot for Balance</span></span><br><span class="line">sns.boxplot(ax=axes[<span class="number">0</span>], x=<span class="string">&#x27;default&#x27;</span>, y=<span class="string">&#x27;balance&#x27;</span>, data=df)</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&#x27;Balance vs. Default Status&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boxplot for Income</span></span><br><span class="line">sns.boxplot(ax=axes[<span class="number">1</span>], x=<span class="string">&#x27;default&#x27;</span>, y=<span class="string">&#x27;income&#x27;</span>, data=df)</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">&#x27;Income vs. Default Status&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. Logistic Regression Modeling ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert categorical &#x27;default&#x27; column to 0s and 1s</span></span><br><span class="line">df[<span class="string">&#x27;default_encoded&#x27;</span>] = df[<span class="string">&#x27;default&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x == <span class="string">&#x27;Yes&#x27;</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define features (X) and target (y)</span></span><br><span class="line">X = df[[<span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line">y = df[<span class="string">&#x27;default_encoded&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create and train the logistic regression model</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on new data</span></span><br><span class="line"><span class="comment"># For example, a person with a $2000 balance and $50,000 income</span></span><br><span class="line">new_customer = [[<span class="number">2000</span>, <span class="number">50000</span>]]</span><br><span class="line">predicted_prob = model.predict_proba(new_customer)</span><br><span class="line">prediction = model.predict(new_customer)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Customer data: Balance=2000, Income=50000&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Probability of No Default vs. Default: <span class="subst">&#123;predicted_prob&#125;</span>&quot;</span>) <span class="comment"># [[P(No), P(Yes)]]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Final Prediction (0=No, 1=Yes): <span class="subst">&#123;prediction&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="the-mathematical-foundation-of-logistic-regression">2. the
mathematical foundation of logistic regression</h1>
<p>This set of slides explains the mathematical foundation of logistic
regression, how its parameters are estimated using Maximum Likelihood
Estimation (MLE), and how an iterative algorithm called Newton-Raphson
is used to perform this estimation.</p>
<p>é€»è¾‘å›å½’çš„æ•°å­¦åŸºç¡€ã€å¦‚ä½•ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)
ä¼°è®¡å…¶å‚æ•°ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨åä¸º Newton-Raphson çš„è¿­ä»£ç®—æ³•è¿›è¡Œä¼°è®¡ã€‚</p>
<h2
id="the-logistic-regression-model-from-probabilities-to-log-oddsé€»è¾‘å›å½’æ¨¡å‹ä»æ¦‚ç‡åˆ°å¯¹æ•°å‡ ç‡">2.1
The Logistic Regression Model: From Probabilities to
Log-Oddsé€»è¾‘å›å½’æ¨¡å‹ï¼šä»æ¦‚ç‡åˆ°å¯¹æ•°å‡ ç‡</h2>
<p>The core of logistic regression is transforming a linear model into a
valid probability. This is done using the <strong>logistic
function</strong>, also known as the sigmoid function.
é€»è¾‘å›å½’çš„æ ¸å¿ƒæ˜¯å°†çº¿æ€§æ¨¡å‹è½¬æ¢ä¸ºæœ‰æ•ˆçš„æ¦‚ç‡ã€‚è¿™å¯ä»¥é€šè¿‡<strong>é€»è¾‘å‡½æ•°</strong>ï¼ˆä¹Ÿç§°ä¸º
S å‹å‡½æ•°ï¼‰æ¥å®ç°ã€‚ #### <strong>Key Mathematical Formulas</strong></p>
<ol type="1">
<li><p><strong>Probability of Class 1:</strong> The model assumes the
probability of an observation <span
class="math inline">\(\mathbf{x}\)</span> belonging to class 1 is given
by the sigmoid function: <span class="math display">\[
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-\beta^T \mathbf{x})} =
\frac{\exp(\beta^T \mathbf{x})}{1 + \exp(\beta^T \mathbf{x})}
\]</span> This function always outputs a value between 0 and 1, making
it perfect for modeling probabilities.</p></li>
<li><p><strong>Odds:</strong> The odds are the ratio of the probability
of an event happening to the probability of it not happening. <span
class="math display">\[
\text{Odds} = \frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})} = \exp(\beta^T
\mathbf{x})
\]</span></p></li>
<li><p><strong>Log-Odds (Logit):</strong> By taking the natural
logarithm of the odds, we get a linear relationship with the predictors.
This is called the <strong>logit transformation</strong>. <span
class="math display">\[
\text{logit}(P(y=1|\mathbf{x})) =
\log\left(\frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})}\right) = \beta^T
\mathbf{x}
\]</span> This final equation is the heart of the model. It states that
the log-odds of the outcome are a linear function of the predictors.
This provides a great interpretation: a one-unit increase in a predictor
<span class="math inline">\(x_j\)</span> changes the log-odds by <span
class="math inline">\(\beta_j\)</span>.</p></li>
<li><p><strong>ç±»åˆ« 1 çš„æ¦‚ç‡</strong>ï¼šè¯¥æ¨¡å‹å‡è®¾è§‚æµ‹å€¼ <span
class="math inline">\(\mathbf{x}\)</span> å±äºç±»åˆ« 1 çš„æ¦‚ç‡ç”± S
å‹å‡½æ•°ç»™å‡ºï¼š <span class="math display">\[
P(y=1|\mathbf{x}) = \frac{1}{1 + \exp(-\beta^T \mathbf{x})} =
\frac{\exp(\beta^T \mathbf{x})}{1 + \exp(\beta^T \mathbf{x})}
\]</span> æ­¤å‡½æ•°çš„è¾“å‡ºå€¼å§‹ç»ˆä»‹äº 0 å’Œ 1
ä¹‹é—´ï¼Œéå¸¸é€‚åˆç”¨äºæ¦‚ç‡å»ºæ¨¡ã€‚</p></li>
<li><p><strong>å‡ ç‡</strong>ï¼š**å‡ ç‡æ˜¯äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ä¸ä¸å‘ç”Ÿçš„æ¦‚ç‡ä¹‹æ¯”ã€‚
<span class="math display">\[
\text{Odds} = \frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})} = \exp(\beta^T
\mathbf{x})
\]</span></p></li>
<li><p><strong>å¯¹æ•°æ¦‚ç‡
(Logit)</strong>ï¼šé€šè¿‡å¯¹æ¦‚ç‡å–è‡ªç„¶å¯¹æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ¦‚ç‡ä¸é¢„æµ‹å˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚è¿™è¢«ç§°ä¸º<strong>logit
å˜æ¢</strong>ã€‚ <span class="math display">\[
\text{logit}(P(y=1|\mathbf{x})) =
\log\left(\frac{P(y=1|\mathbf{x})}{P(y=0|\mathbf{x})}\right) = \beta^T
\mathbf{x}
\]</span>
æœ€åä¸€ä¸ªæ–¹ç¨‹æ˜¯æ¨¡å‹çš„æ ¸å¿ƒã€‚å®ƒæŒ‡å‡ºç»“æœçš„å¯¹æ•°æ¦‚ç‡æ˜¯é¢„æµ‹å˜é‡çš„çº¿æ€§å‡½æ•°ã€‚è¿™æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„è§£é‡Šï¼šé¢„æµ‹å˜é‡
<span class="math inline">\(x_j\)</span>
æ¯å¢åŠ ä¸€ä¸ªå•ä½ï¼Œå¯¹æ•°æ¦‚ç‡å°±ä¼šæ”¹å˜ <span
class="math inline">\(\beta_j\)</span>ã€‚</p></li>
</ol>
<h2
id="fitting-the-model-maximum-likelihood-estimation-mle-æ‹Ÿåˆæ¨¡å‹æœ€å¤§ä¼¼ç„¶ä¼°è®¡-mle">2.2
Fitting the Model: Maximum Likelihood Estimation (MLE)
æ‹Ÿåˆæ¨¡å‹ï¼šæœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)</h2>
<p>Unlike linear regression, which uses least squares to find the
best-fit line, logistic regression uses <strong>Maximum Likelihood
Estimation (MLE)</strong>. The goal of MLE is to find the parameter
values (the <span class="math inline">\(\beta\)</span> coefficients)
that maximize the probability of observing the actual data that we have.
ä¸ä½¿ç”¨æœ€å°äºŒä¹˜æ³•å¯»æ‰¾æœ€ä½³æ‹Ÿåˆçº¿çš„çº¿æ€§å›å½’ä¸åŒï¼Œé€»è¾‘å›å½’ä½¿ç”¨<strong>æœ€å¤§ä¼¼ç„¶ä¼°è®¡
(MLE)</strong>ã€‚MLE
çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä½¿è§‚æµ‹åˆ°å®é™…æ•°æ®çš„æ¦‚ç‡æœ€å¤§åŒ–çš„å‚æ•°å€¼ï¼ˆ<span
class="math inline">\(\beta\)</span> ç³»æ•°ï¼‰ã€‚</p>
<ol type="1">
<li><p><strong>Likelihood Function:</strong> This is the joint
probability of observing all the data points in our sample. Assuming
each observation is independent, itâ€™s the product of the individual
probabilities:
1.<strong>ä¼¼ç„¶å‡½æ•°</strong>ï¼šè¿™æ˜¯è§‚æµ‹åˆ°æ ·æœ¬ä¸­æ‰€æœ‰æ•°æ®ç‚¹çš„è”åˆæ¦‚ç‡ã€‚å‡è®¾æ¯ä¸ªè§‚æµ‹å€¼éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œå®ƒæ˜¯å„ä¸ªæ¦‚ç‡çš„ä¹˜ç§¯ï¼š
<span class="math display">\[
L(\beta) = \prod_{i=1}^{n} P(y_i|\mathbf{x}_i)
\]</span> A clever way to write this for a binary (0/1) outcome is:
<span class="math display">\[
L(\beta) = \prod_{i=1}^{n} \frac{\exp(y_i \beta^T \mathbf{x}_i)}{1 +
\exp(\beta^T \mathbf{x}_i)}
\]</span></p></li>
<li><p><strong>Log-Likelihood Function:</strong> Products are difficult
to work with mathematically, so we work with the logarithm of the
likelihood, which turns the product into a sum. Maximizing the
log-likelihood is the same as maximizing the likelihood.</p></li>
<li><p><strong>å¯¹æ•°ä¼¼ç„¶å‡½æ•°</strong>ï¼šä¹˜ç§¯åœ¨æ•°å­¦ä¸Šå¾ˆéš¾å¤„ç†ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ä¼¼ç„¶çš„å¯¹æ•°ï¼Œå°†ä¹˜ç§¯è½¬åŒ–ä¸ºå’Œã€‚æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ä¸æœ€å¤§åŒ–ä¼¼ç„¶ç›¸åŒã€‚
<span class="math display">\[
\ell(\beta) = \log(L(\beta)) = \sum_{i=1}^{n} \left[ y_i \beta^T
\mathbf{x}_i - \log(1 + \exp(\beta^T \mathbf{x}_i)) \right]
\]</span> <strong>Key Takeaway:</strong> The slides correctly state that
there is <strong>no explicit formula</strong> to solve for the <span
class="math inline">\(\hat{\beta}\)</span> that maximizes this function.
We must find it using a numerical optimization algorithm.
æ²¡æœ‰<strong>æ˜ç¡®çš„å…¬å¼</strong>æ¥æ±‚è§£æœ€å¤§åŒ–è¯¥å‡½æ•°çš„<span
class="math inline">\(\hat{\beta}\)</span>ã€‚æˆ‘ä»¬å¿…é¡»ä½¿ç”¨æ•°å€¼ä¼˜åŒ–ç®—æ³•æ¥æ‰¾åˆ°å®ƒã€‚</p></li>
</ol>
<h2 id="the-algorithm-newton-raphson-ç®—æ³•ç‰›é¡¿-æ‹‰å¤«æ£®ç®—æ³•">2.3 The
Algorithm: Newton-Raphson ç®—æ³•ï¼šç‰›é¡¿-æ‹‰å¤«æ£®ç®—æ³•</h2>
<p>The slides introduce the <strong>Newton-Raphson algorithm</strong> as
the method to find the optimal <span
class="math inline">\(\hat{\beta}\)</span>. Itâ€™s an efficient iterative
algorithm for finding the roots of a function (i.e., where <span
class="math inline">\(f(x)=0\)</span>).</p>
<p><strong>How does this apply to logistic regression?</strong> To
maximize the log-likelihood function <span
class="math inline">\(\ell(\beta)\)</span>, we need to find the point
where its derivative (gradient) is equal to zero. So, Newton-Raphson is
used to solve <span class="math inline">\(\frac{d\ell(\beta)}{d\beta} =
0\)</span>.</p>
<p>å®ƒæ˜¯ä¸€ç§é«˜æ•ˆçš„è¿­ä»£ç®—æ³•ï¼Œç”¨äºæ±‚å‡½æ•°çš„æ ¹ï¼ˆå³ï¼Œå½“<span
class="math inline">\(f(x)=0\)</span>æ—¶ï¼‰ã€‚</p>
<p><strong>è¿™å¦‚ä½•åº”ç”¨äºé€»è¾‘å›å½’ï¼Ÿ</strong> ä¸ºäº†æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶å‡½æ•° <span
class="math inline">\(\ell(\beta)\)</span>ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°å…¶å¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰ç­‰äºé›¶çš„ç‚¹ã€‚å› æ­¤ï¼Œç‰›é¡¿-æ‹‰å¤«æ£®æ³•ç”¨äºæ±‚è§£
<span class="math inline">\(\frac{d\ell(\beta)}{d\beta} =
0\)</span>ã€‚</p>
<h4 id="the-general-newton-raphson-method"><strong>The General
Newton-Raphson Method</strong></h4>
<p>The algorithm starts with an initial guess, <span
class="math inline">\(x^{old}\)</span>, and iteratively refines it using
the following update rule, which is based on a Taylor series
approximation: <span class="math display">\[
x^{new} = x^{old} - \frac{f(x^{old})}{f&#39;(x^{old})}
\]</span> where <span class="math inline">\(f&#39;(x)\)</span> is the
derivative of <span class="math inline">\(f(x)\)</span>. You repeat this
step until the value of <span class="math inline">\(x\)</span>
converges.</p>
<p>è¯¥ç®—æ³•ä»åˆå§‹ä¼°è®¡ <span class="math inline">\(x^{old}\)</span>
å¼€å§‹ï¼Œå¹¶ä½¿ç”¨ä»¥ä¸‹åŸºäºæ³°å‹’çº§æ•°è¿‘ä¼¼çš„æ›´æ–°è§„åˆ™è¿­ä»£åœ°å¯¹å…¶è¿›è¡Œä¼˜åŒ–ï¼š <span
class="math display">\[
x^{new} = x^{old} - \frac{f(x^{old})}{f&#39;(x^{old})}
\]</span> å…¶ä¸­ <span class="math inline">\(f&#39;(x)\)</span> æ˜¯ <span
class="math inline">\(f(x)\)</span> çš„å¯¼æ•°ã€‚é‡å¤æ­¤æ­¥éª¤ï¼Œç›´åˆ° <span
class="math inline">\(x\)</span> çš„å€¼æ”¶æ•›ã€‚</p>
<h4
id="important-image-newton-raphson-example-x3---4-0"><strong>Important
Image: Newton-Raphson Example (<span class="math inline">\(x^3 - 4 =
0\)</span>)</strong></h4>
<p>[Image showing iterations of Newton-Raphson]</p>
<p>This slide is a great illustration of the algorithmâ€™s power. *
<strong>Goal:</strong> Find <span class="math inline">\(x\)</span> such
that <span class="math inline">\(f(x) = x^3 - 4 = 0\)</span>. *
<strong>Function:</strong> <span class="math inline">\(f(x) = x^3 -
4\)</span> * <strong>Derivative:</strong> <span
class="math inline">\(f&#39;(x) = 3x^2\)</span> * <strong>Update
Rule:</strong> <span class="math inline">\(x^{new} = x^{old} -
\frac{(x^{old})^3 - 4}{3(x^{old})^2}\)</span> Starting with a guess of
<span class="math inline">\(x^{old} = 2\)</span>, the algorithm
converges to the true answer (<span class="math inline">\(4^{1/3}
\approx 1.5874\)</span>) in just 4 steps.</p>
<ul>
<li><strong>ç›®æ ‡</strong>ï¼šæ‰¾åˆ° <span
class="math inline">\(x\)</span>ï¼Œä½¿å¾— <span class="math inline">\(f(x)
= x^3 - 4 = 0\)</span>ã€‚</li>
<li><strong>å‡½æ•°</strong>ï¼š<span class="math inline">\(f(x) = x^3 -
4\)</span></li>
<li><strong>å¯¼æ•°</strong>ï¼š<span class="math inline">\(f&#39;(x) =
3x^2\)</span></li>
<li><strong>æ›´æ–°è§„åˆ™</strong>ï¼š<span class="math inline">\(x^{new} =
x^{old} - \frac{(x^{old})^3 - 4}{3(x^{old})^2}\)</span> ä» <span
class="math inline">\(x^{old} = 2\)</span> çš„çŒœæµ‹å¼€å§‹ï¼Œè¯¥ç®—æ³•ä»…ç”¨ 4
æ­¥å°±æ”¶æ•›åˆ°çœŸå®ç­”æ¡ˆ (<span class="math inline">\(4^{1/3} \approx
1.5874\)</span>)ã€‚</li>
</ul>
<h4 id="code-understanding-python"><strong>Code Understanding
(Python)</strong></h4>
<p>The slides show Python code implementing Newton-Raphson. Letâ€™s break
down the key function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the function we want to find the root of</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) - x*x + <span class="number">3</span> * np.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define its derivative</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f_prime</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.exp(x) - <span class="number">2</span>*x + <span class="number">3</span> * np.cos(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Newton-Raphson method</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">newton_raphson</span>(<span class="params">x0, tol=<span class="number">1e-10</span>, max_iter=<span class="number">100</span></span>):</span><br><span class="line">    x = x0 <span class="comment"># Start with the initial guess</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        fx = f(x)      <span class="comment"># Calculate f(x_old)</span></span><br><span class="line">        fpx = f_prime(x) <span class="comment"># Calculate f&#x27;(x_old)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> fpx == <span class="number">0</span>: <span class="comment"># Cannot divide by zero</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Zero derivative. No solution found.&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># This is the core update rule</span></span><br><span class="line">        x_new = x - fx / fpx</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check if the change is small enough to stop</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(x_new - x) &lt; tol:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Converged to <span class="subst">&#123;x_new&#125;</span> after <span class="subst">&#123;i+<span class="number">1</span>&#125;</span> iterations.&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> x_new</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update x for the next iteration</span></span><br><span class="line">        x = x_new</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Exceeded maximum iterations. No solution found.&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial guess and execution</span></span><br><span class="line">x0 = <span class="number">0.5</span></span><br><span class="line">root = newton_raphson(x0)</span><br></pre></td></tr></table></figure>
<p>The slides show that with a good initial guess
(<code>x0 = 0.5</code>), the algorithm converges quickly. With a bad one
(<code>x0 = 50</code>), it still converges but takes many more steps.
This highlights the importance of the starting point. The slides also
show an implementation of <strong>Gradient Descent</strong>, another
popular optimization algorithm which uses the update rule
<code>x_new = x - learning_rate * gradient</code>.</p>
<h1
id="provide-a-great-case-study-on-logistic-regression-particularly-on-the-important-concept-of-confounding-variables.-heres-a-summary-covering-the-math-code-and-key-insights.">Provide
a great case study on logistic regression, particularly on the important
concept of confounding variables. Hereâ€™s a summary covering the math,
code, and key insights.</h1>
<ol start="3" type="1">
<li><h1 id="core-concept-logistic-regression-æ ¸å¿ƒæ¦‚å¿µé€»è¾‘å›å½’">Core
Concept: Logistic Regression ğŸ“ˆ # æ ¸å¿ƒæ¦‚å¿µï¼šé€»è¾‘å›å½’ ğŸ“ˆ</h1></li>
</ol>
<p>Logistic regression is a statistical method used for <strong>binary
classification</strong>, which means predicting an outcome that can only
be one of two things (e.g., Yes/No, True/False, 1/0).</p>
<p>In this example, the goal is to predict the probability that a
customer will <strong>default</strong> on a loan (Yes or No) based on
factors like their account <code>balance</code>, <code>income</code>,
and whether they are a <code>student</code>.</p>
<p>The core of logistic regression is the <strong>sigmoid (or logistic)
function</strong>, which takes any real-valued number and squishes it to
a value between 0 and 1, representing a probability.</p>
<p><span class="math display">\[
\hat{P}(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_p
X_p)}}
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{P}(Y=1|X)\)</span> is the predicted
probability of the outcome being â€œYesâ€ (e.g., default).</li>
<li><span class="math inline">\(\beta_0\)</span> is the intercept.</li>
<li><span class="math inline">\(\beta_1, ..., \beta_p\)</span> are the
coefficients for each input variable (<span class="math inline">\(X_1,
..., X_p\)</span>). The modelâ€™s job is to find the best values for these
<span class="math inline">\(\beta\)</span> coefficients.</li>
</ul>
<hr />
<p>é€»è¾‘å›å½’æ˜¯ä¸€ç§ç”¨äº<strong>äºŒå…ƒåˆ†ç±»</strong>çš„ç»Ÿè®¡æ–¹æ³•ï¼Œè¿™æ„å‘³ç€é¢„æµ‹ç»“æœåªèƒ½æ˜¯ä¸¤ç§æƒ…å†µä¹‹ä¸€ï¼ˆä¾‹å¦‚ï¼Œæ˜¯/å¦ã€çœŸ/å‡ã€1/0ï¼‰ã€‚</p>
<p>åœ¨æœ¬ä¾‹ä¸­ï¼Œç›®æ ‡æ˜¯æ ¹æ®å®¢æˆ·è´¦æˆ·â€œä½™é¢â€ã€â€œæ”¶å…¥â€ä»¥åŠæ˜¯å¦ä¸ºâ€œå­¦ç”Ÿâ€ç­‰å› ç´ ï¼Œé¢„æµ‹å®¢æˆ·<strong>æ‹–æ¬ </strong>è´·æ¬¾ï¼ˆæ˜¯æˆ–å¦ï¼‰çš„æ¦‚ç‡ã€‚</p>
<p>é€»è¾‘å›å½’çš„æ ¸å¿ƒæ˜¯<strong>Sigmoidï¼ˆæˆ–é€»è¾‘ï¼‰å‡½æ•°</strong>ï¼Œå®ƒå°†ä»»ä½•å®æ•°å‹ç¼©ä¸ºä»‹äº
0 å’Œ 1 ä¹‹é—´çš„å€¼ï¼Œä»¥è¡¨ç¤ºæ¦‚ç‡ã€‚</p>
<p><span class="math display">\[
\hat{P}(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_p
X_p)}}
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{P}(Y=1|X)\)</span>
æ˜¯ç»“æœä¸ºâ€œæ˜¯â€ï¼ˆä¾‹å¦‚ï¼Œé»˜è®¤ï¼‰çš„é¢„æµ‹æ¦‚ç‡ã€‚</li>
<li><span class="math inline">\(\beta_0\)</span> æ˜¯æˆªè·ã€‚</li>
<li><span class="math inline">\(\beta_1, ..., \beta_p\)</span>
æ˜¯æ¯ä¸ªè¾“å…¥å˜é‡ (<span class="math inline">\(X_1, ..., X_p\)</span>)
çš„ç³»æ•°ã€‚æ¨¡å‹çš„ä»»åŠ¡æ˜¯æ‰¾åˆ°è¿™äº› <span class="math inline">\(\beta\)</span>
ç³»æ•°çš„æœ€ä½³å€¼ã€‚</li>
</ul>
<h2 id="how-the-model-learns-mathematical-foundation">3.1 How the Model
â€œLearnsâ€ (Mathematical Foundation)</h2>
<p>The slides show that the modelâ€™s coefficients (<span
class="math inline">\(\beta\)</span>) are found using an algorithm like
<strong>Newton-Raphson</strong>. This is an iterative process to find
the values that <strong>maximize the log-likelihood function</strong>.
Think of this as finding the coefficient values that make the observed
data most
probable.è¿™æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œç”¨äºæŸ¥æ‰¾<strong>æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶å‡½æ•°</strong>çš„å€¼ã€‚å¯ä»¥å°†å…¶è§†ä¸ºæŸ¥æ‰¾ä½¿è§‚æµ‹æ•°æ®æ¦‚ç‡æœ€å¤§çš„ç³»æ•°å€¼ã€‚</p>
<p>The key slide for this is the one titled â€œNewton-Raphson Iterative
Algorithmâ€. It shows the formulas for: * The <strong>Gradient</strong>
(<span class="math inline">\(\nabla\ell\)</span>): The direction of the
steepest ascent of the log-likelihood function. * The
<strong>Hessian</strong> (<span class="math inline">\(H\)</span>): The
curvature of the log-likelihood function.</p>
<ul>
<li><strong>æ¢¯åº¦</strong> (<span
class="math inline">\(\nabla\ell\)</span>)ï¼šå¯¹æ•°ä¼¼ç„¶å‡½æ•°æœ€é™¡ä¸Šå‡çš„æ–¹å‘ã€‚</li>
<li><strong>é»‘æ£®çŸ©é˜µ</strong> (<span
class="math inline">\(H\)</span>)ï¼šå¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„æ›²ç‡ã€‚</li>
</ul>
<p>The updating rule is given by: <span class="math display">\[
\beta^{new} = \beta^{old} - H^{-1}\nabla\ell
\]</span> This formula is used repeatedly until the coefficient values
stop changing significantly, meaning the algorithm has converged to the
best fit. This process is also referred to as <strong>Iteratively
Reweighted Least Squares (IRLS)</strong>.
æ­¤å…¬å¼åå¤ä½¿ç”¨ï¼Œç›´åˆ°ç³»æ•°å€¼ä¸å†å‘ç”Ÿæ˜¾è‘—å˜åŒ–ï¼Œè¿™æ„å‘³ç€ç®—æ³•å·²æ”¶æ•›åˆ°æœ€ä½³æ‹Ÿåˆå€¼ã€‚æ­¤è¿‡ç¨‹ä¹Ÿç§°ä¸º<strong>è¿­ä»£é‡åŠ æƒæœ€å°äºŒä¹˜æ³•
(IRLS)</strong>ã€‚</p>
<hr />
<h2 id="the-puzzle-a-tale-of-two-models">3.2 The Puzzle: A Tale of Two
Models ğŸ•µï¸â€â™‚ï¸</h2>
<p>The most important story in these slides is how the effect of being a
student changes depending on the model. This is a classic example of a
<strong>confounding variable</strong>.</p>
<h4 id="model-1-simple-logistic-regression-default-vs.-student">Model 1:
Simple Logistic Regression (Default vs.Â Student)</h4>
<p>When predicting default using <em>only</em> student status, the model
is: <code>default ~ student</code></p>
<p>From the slides, the coefficients are: * Intercept (<span
class="math inline">\(\beta_0\)</span>): -3.5041 * student[Yes] (<span
class="math inline">\(\beta_1\)</span>): <strong>0.4049</strong>
(positive)</p>
<p>The equation for the log-odds is: <span class="math display">\[
\log\left(\frac{P(\text{default})}{1-P(\text{default})}\right) = -3.5041
+ 0.4049 \times (\text{is\_student})
\]</span></p>
<p><strong>Conclusion:</strong> The positive coefficient (0.4049)
suggests that <strong>students are more likely to default</strong> than
non-students. The slides calculate the probabilities: * <strong>Student
Default Probability:</strong> 4.31% * <strong>Non-Student Default
Probability:</strong> 2.92%</p>
<p>å­¦ç”Ÿèº«ä»½çš„å½±å“å¦‚ä½•æ ¹æ®æ¨¡å‹è€Œå˜åŒ–ã€‚è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„<strong>æ··æ‚å˜é‡</strong>çš„ä¾‹å­ã€‚</p>
<h4 id="æ¨¡å‹-1ç®€å•é€»è¾‘å›å½’è¿çº¦-vs.-å­¦ç”Ÿ">æ¨¡å‹ 1ï¼šç®€å•é€»è¾‘å›å½’ï¼ˆè¿çº¦
vs.Â å­¦ç”Ÿï¼‰</h4>
<p>ä»…ä½¿ç”¨å­¦ç”Ÿèº«ä»½é¢„æµ‹è¿çº¦æ—¶ï¼Œæ¨¡å‹ä¸ºï¼š <code>default ~ student</code></p>
<p>å¹»ç¯ç‰‡ä¸­æ˜¾ç¤ºçš„ç³»æ•°ä¸ºï¼š * æˆªè· (<span
class="math inline">\(\beta_0\)</span>): -3.5041 * å­¦ç”Ÿ[æ˜¯] (<span
class="math inline">\(\beta_1\)</span>):
<strong>0.4049</strong>ï¼ˆæ­£ï¼‰</p>
<p>å¯¹æ•°æ¦‚ç‡å…¬å¼ä¸ºï¼š <span class="math display">\[
\log\left(\frac{P(\text{default})}{1-P(\text{default})}\right) = -3.5041
+ 0.4049 \times (\text{is\_student})
\]</span></p>
<p><strong>ç»“è®º</strong>ï¼šæ­£ç³»æ•° (0.4049)
è¡¨æ˜<strong>å­¦ç”Ÿæ¯”éå­¦ç”Ÿæ›´æœ‰å¯èƒ½è¿çº¦</strong>ã€‚å¹»ç¯ç‰‡è®¡ç®—äº†ä»¥ä¸‹æ¦‚ç‡ï¼š *
<strong>å­¦ç”Ÿè¿çº¦æ¦‚ç‡</strong>ï¼š4.31% *
<strong>éå­¦ç”Ÿè¿çº¦æ¦‚ç‡</strong>ï¼š2.92%</p>
<h2
id="model-2-multiple-logistic-regression-default-vs.-all-variables-æ¨¡å‹-2å¤šå…ƒé€»è¾‘å›å½’è¿çº¦-vs.-æ‰€æœ‰å˜é‡">3.3
Model 2: Multiple Logistic Regression (Default vs.Â All Variables) æ¨¡å‹
2ï¼šå¤šå…ƒé€»è¾‘å›å½’ï¼ˆè¿çº¦ vs.Â æ‰€æœ‰å˜é‡ï¼‰</h2>
<p>When we add <code>balance</code> and <code>income</code> to the
model, it becomes: <code>default ~ student + balance + income</code></p>
<p>From the slides, the new coefficients are: * Intercept (<span
class="math inline">\(\beta_0\)</span>): -10.8690 * balance (<span
class="math inline">\(\beta_1\)</span>): 0.0057 * income (<span
class="math inline">\(\beta_2\)</span>): 0.0030 * student[Yes] (<span
class="math inline">\(\beta_3\)</span>): <strong>-0.6468</strong>
(negative)</p>
<p><strong>The Shocking Twist!</strong> The coefficient for
<code>student[Yes]</code> is now <strong>negative</strong>.</p>
<p><strong>Conclusion:</strong> When we control for balance and income,
<strong>students are actually <em>less</em> likely to default</strong>
than non-students with the same balance and income.</p>
<h4 id="why-the-change-the-confounding-variable-explained">Why the
Change? The Confounding Variable Explained</h4>
<p>The key insight, explained on the slide with multi-colored text
bubbles, is that <strong>students, on average, have higher credit card
balances</strong>.</p>
<ul>
<li>In the simple model, the <code>student</code> variable was
inadvertently capturing the risk associated with having a high
<code>balance</code>. The model mistakenly concluded â€œbeing a student
causes default.â€</li>
<li>In the multiple model, the <code>balance</code> variable properly
accounts for the risk from a high balance. With that effect isolated,
the <code>student</code> variable can show its true, underlying
relationship with default, which is negative.</li>
</ul>
<p>This demonstrates why itâ€™s crucial to consider multiple relevant
variables to avoid drawing incorrect conclusions. <strong>The most
important slides are the ones that present this paradox and its
explanation.</strong></p>
<p><strong>ä»¤äººéœ‡æƒŠçš„è½¬æŠ˜ï¼</strong> <code>student[Yes]</code>
çš„ç³»æ•°ç°åœ¨ä¸º<strong>è´Ÿ</strong>ã€‚</p>
<p><strong>ç»“è®ºï¼š</strong>å½“æˆ‘ä»¬æ§åˆ¶ä½™é¢å’Œæ”¶å…¥æ—¶ï¼Œ<strong>å­¦ç”Ÿå®é™…ä¸Šæ¯”å…·æœ‰ç›¸åŒä½™é¢å’Œæ”¶å…¥çš„éå­¦ç”Ÿæ›´<em>ä½</em>äºè¿çº¦</strong>ã€‚</p>
<h4 id="ä¸ºä»€ä¹ˆä¼šæœ‰å˜åŒ–æ··æ‚å˜é‡è§£é‡Š">ä¸ºä»€ä¹ˆä¼šæœ‰å˜åŒ–ï¼Ÿæ··æ‚å˜é‡è§£é‡Š</h4>
<p>å¹»ç¯ç‰‡ä¸Šç”¨å½©è‰²æ–‡å­—æ°”æ³¡è§£é‡Šäº†å…³é”®çš„è§è§£ï¼Œå³<strong>å­¦ç”Ÿå¹³å‡æ‹¥æœ‰æ›´é«˜çš„ä¿¡ç”¨å¡ä½™é¢</strong>ã€‚</p>
<ul>
<li>åœ¨ç®€å•æ¨¡å‹ä¸­ï¼Œâ€œå­¦ç”Ÿâ€å˜é‡æ— æ„ä¸­æ•æ‰åˆ°äº†é«˜ä½™é¢å¸¦æ¥çš„é£é™©ã€‚è¯¥æ¨¡å‹é”™è¯¯åœ°å¾—å‡ºäº†â€œå­¦ç”Ÿèº«ä»½å¯¼è‡´è¿çº¦â€çš„ç»“è®ºã€‚</li>
<li>åœ¨å¤šå…ƒæ¨¡å‹ä¸­ï¼Œâ€œä½™é¢â€å˜é‡æ­£ç¡®åœ°è§£é‡Šäº†é«˜ä½™é¢å¸¦æ¥çš„é£é™©ã€‚åœ¨åˆ†ç¦»å‡ºè¿™ä¸€å½±å“åï¼Œâ€œå­¦ç”Ÿâ€å˜é‡å¯ä»¥æ˜¾ç¤ºå…¶ä¸è¿çº¦ä¹‹é—´çœŸå®çš„æ½œåœ¨å…³ç³»ï¼Œå³è´Ÿç›¸å…³å…³ç³»ã€‚</li>
</ul>
<p>è¿™è¯´æ˜äº†ä¸ºä»€ä¹ˆè€ƒè™‘å¤šä¸ªç›¸å…³å˜é‡ä»¥é¿å…å¾—å‡ºé”™è¯¯ç»“è®ºè‡³å…³é‡è¦ã€‚</p>
<hr />
<h3 id="code-implementation-r-vs.-python">Code Implementation: R
vs.Â Python</h3>
<p>The slides use Râ€™s <code>glm()</code> (Generalized Linear Model)
function. Hereâ€™s how you would replicate this in Python.</p>
<h4 id="r-code-from-slides">R Code (from slides)</h4>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Simple Model</span></span><br><span class="line">glmod2 <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>default <span class="operator">~</span> student<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> family<span class="operator">=</span>binomial<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>glmod2<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model</span></span><br><span class="line">glmod3 <span class="operator">&lt;-</span> glm<span class="punctuation">(</span>default <span class="operator">~</span> .<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> family<span class="operator">=</span>binomial<span class="punctuation">)</span> <span class="comment"># &#x27;.&#x27; means all other variables</span></span><br><span class="line">summary<span class="punctuation">(</span>glmod3<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<h4 id="python-equivalent">Python Equivalent</h4>
<p>We can use two popular libraries: <code>statsmodels</code> (which
gives R-style summaries) and <code>scikit-learn</code> (the standard for
machine learning).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Default&#x27; is a pandas DataFrame with columns:</span></span><br><span class="line"><span class="comment"># &#x27;default&#x27; (0/1), &#x27;student&#x27; (0/1), &#x27;balance&#x27;, &#x27;income&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Using statsmodels (recommended for interpretation) ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data</span></span><br><span class="line"><span class="comment"># For statsmodels, we need to manually add the intercept</span></span><br><span class="line">X_simple = Default[[<span class="string">&#x27;student&#x27;</span>]]</span><br><span class="line">X_simple = sm.add_constant(X_simple)</span><br><span class="line">y = Default[<span class="string">&#x27;default&#x27;</span>]</span><br><span class="line"></span><br><span class="line">X_multiple = Default[[<span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line">X_multiple = sm.add_constant(X_multiple)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simple Model: default ~ student</span></span><br><span class="line">model_simple = sm.Logit(y, X_simple).fit()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- Simple Model ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_simple.summary())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model: default ~ student + balance + income</span></span><br><span class="line">model_multiple = sm.Logit(y, X_multiple).fit()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Multiple Model ---&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_multiple.summary())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- Using scikit-learn (recommended for prediction tasks) ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the data (scikit-learn adds intercept by default)</span></span><br><span class="line">X_simple_sk = Default[[<span class="string">&#x27;student&#x27;</span>]]</span><br><span class="line">y_sk = Default[<span class="string">&#x27;default&#x27;</span>]</span><br><span class="line"></span><br><span class="line">X_multiple_sk = Default[[<span class="string">&#x27;student&#x27;</span>, <span class="string">&#x27;balance&#x27;</span>, <span class="string">&#x27;income&#x27;</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Simple Model</span></span><br><span class="line">clf_simple = LogisticRegression().fit(X_simple_sk, y_sk)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nSimple Model Intercept (scikit-learn): <span class="subst">&#123;clf_simple.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Simple Model Coefficient (scikit-learn): <span class="subst">&#123;clf_simple.coef_&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple Model</span></span><br><span class="line">clf_multiple = LogisticRegression().fit(X_multiple_sk, y_sk)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nMultiple Model Intercept (scikit-learn): <span class="subst">&#123;clf_multiple.intercept_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Multiple Model Coefficients (scikit-learn): <span class="subst">&#123;clf_multiple.coef_&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1
id="making-predictions-and-the-decision-boundary-è¿›è¡Œé¢„æµ‹å’Œå†³ç­–è¾¹ç•Œ">4
Making Predictions and the Decision Boundary ğŸ¯è¿›è¡Œé¢„æµ‹å’Œå†³ç­–è¾¹ç•Œ</h1>
<p>Once the model is trained (i.e., we have the coefficients <span
class="math inline">\(\hat{\beta}\)</span>), we can make predictions.
ä¸€æ—¦æ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆå³ï¼Œæˆ‘ä»¬æœ‰äº†ç³»æ•° <span
class="math inline">\(\hat{\beta}\)</span>ï¼‰ï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œé¢„æµ‹äº†ã€‚ ##
Math Behind Predictions</p>
<p>The model outputs the <strong>log-odds</strong>, which can be
converted into a probability. A key concept is the <strong>decision
boundary</strong>, which is the threshold where the model is uncertain
(probability = 50%).
æ¨¡å‹è¾“å‡º<strong>å¯¹æ•°æ¦‚ç‡</strong>ï¼Œå®ƒå¯ä»¥è½¬æ¢ä¸ºæ¦‚ç‡ã€‚ä¸€ä¸ªå…³é”®æ¦‚å¿µæ˜¯<strong>å†³ç­–è¾¹ç•Œ</strong>ï¼Œå®ƒæ˜¯æ¨¡å‹ä¸ç¡®å®šçš„é˜ˆå€¼ï¼ˆæ¦‚ç‡
= 50%ï¼‰ã€‚</p>
<ol type="1">
<li><p><strong>The Estimated Odds</strong>: The core output of the
linear part of the model is the exponential of the linear equation,
which gives the odds of the outcome being â€˜Yesâ€™ (or 1).
<strong>ä¼°è®¡æ¦‚ç‡</strong>ï¼šæ¨¡å‹çº¿æ€§éƒ¨åˆ†çš„æ ¸å¿ƒè¾“å‡ºæ˜¯çº¿æ€§æ–¹ç¨‹çš„æŒ‡æ•°ï¼Œå®ƒç»™å‡ºäº†ç»“æœä¸ºâ€œæ˜¯â€ï¼ˆæˆ–
1ï¼‰çš„æ¦‚ç‡ã€‚</p>
<p><span class="math display">\[
\]</span>$$\frac{\hat{P}(y=1|\mathbf{x}_0)}{\hat{P}(y=0|\mathbf{x}_0)} =
\exp(\hat{\beta}^\top \mathbf{x}_0)</p>
<p><span class="math display">\[
\]</span><span class="math display">\[
\]</span></p></li>
<li><p><strong>The Decision Rule</strong>: We classify a new observation
<span class="math inline">\(\mathbf{x}_0\)</span> by comparing its
predicted odds to a threshold <span
class="math inline">\(\delta\)</span>.
<strong>å†³ç­–è§„åˆ™</strong>ï¼šæˆ‘ä»¬é€šè¿‡æ¯”è¾ƒæ–°è§‚æµ‹å€¼ <span
class="math inline">\(\mathbf{x}_0\)</span> çš„é¢„æµ‹æ¦‚ç‡ä¸é˜ˆå€¼ <span
class="math inline">\(\delta\)</span> æ¥å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚</p>
<ul>
<li>Predict <span class="math inline">\(y=1\)</span> if <span
class="math inline">\(\exp(\hat{\beta}^\top \mathbf{x}_0) &gt;
\delta\)</span></li>
<li>Predict <span class="math inline">\(y=0\)</span> if <span
class="math inline">\(\exp(\hat{\beta}^\top \mathbf{x}_0) &lt;
\delta\)</span> A common default is <span
class="math inline">\(\delta=1\)</span>, which means we predict â€˜Yesâ€™ if
the probability is greater than 0.5.</li>
</ul></li>
<li><p><strong>The Linear Boundary</strong>: The decision boundary
itself is where the odds are exactly equal to the threshold. By taking
the logarithm, we see that this boundary is a <strong>linear
equation</strong>. This is why logistic regression is called a
<strong>linear classifier</strong>.
<strong>çº¿æ€§è¾¹ç•Œ</strong>ï¼šå†³ç­–è¾¹ç•Œæœ¬èº«å°±æ˜¯æ¦‚ç‡æ°å¥½ç­‰äºé˜ˆå€¼çš„åœ°æ–¹ã€‚å–å¯¹æ•°åï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸ªè¾¹ç•Œæ˜¯ä¸€ä¸ª<strong>çº¿æ€§æ–¹ç¨‹</strong>ã€‚è¿™å°±æ˜¯é€»è¾‘å›å½’è¢«ç§°ä¸º<strong>çº¿æ€§åˆ†ç±»å™¨</strong>çš„åŸå› ã€‚
<span class="math display">\[
\]</span>$$\hat{\beta}^\top \mathbf{x} = \log(\delta)</p>
<p><span class="math display">\[
\]</span>$$For <span class="math inline">\(\delta=1\)</span>, the
boundary is simply <span class="math inline">\(\hat{\beta}^\top
\mathbf{x} = 0\)</span>.</p></li>
</ol>
<p>This concept is visualized perfectly in the slide titled â€œLinear
Classifier,â€ which shows a straight line neatly separating two classes
of data points.
é¢˜ä¸ºâ€œçº¿æ€§åˆ†ç±»å™¨â€çš„å¹»ç¯ç‰‡å®Œç¾åœ°å±•ç¤ºäº†è¿™ä¸€æ¦‚å¿µï¼Œå®ƒå±•ç¤ºäº†ä¸€æ¡ç›´çº¿ï¼Œå°†ä¸¤ç±»æ•°æ®ç‚¹å·§å¦™åœ°åˆ†éš”å¼€æ¥ã€‚</p>
<h2 id="visualizing-the-confounding-effect">Visualizing the Confounding
Effect</h2>
<p>The most important image in this set is <strong>Figure 4.3</strong>,
as it visually explains the confounding puzzle from the first set of
slides.</p>
<ul>
<li><strong>Right Panel (Boxplots)</strong>: This shows that
<strong>students (Yes) tend to have higher credit card balances</strong>
than non-students (No). This is the source of the confounding.</li>
<li><strong>Left Panel (Default Rates)</strong>:
<ul>
<li>The <strong>dashed lines</strong> show the <em>overall</em> default
rates. The orange line (students) is higher than the blue line
(non-students). This matches our simple model
(<code>default ~ student</code>).</li>
<li>The <strong>solid S-shaped curves</strong> show the probability of
default as a function of balance. For any <em>given</em> balance, the
blue curve (non-students) is slightly higher than the orange curve
(students). This means that <strong>at the same level of debt, students
are <em>less</em> likely to default</strong>. This matches our multiple
regression model
(<code>default ~ student + balance + income</code>).</li>
</ul></li>
</ul>
<p>This single figure brilliantly illustrates how a variable can appear
to have one effect in isolation but the opposite effect when controlling
for a confounding factor. *
<strong>å³ä¾§é¢æ¿ï¼ˆç®±çº¿å›¾ï¼‰</strong>ï¼šè¿™è¡¨æ˜<strong>å­¦ç”Ÿï¼ˆæ˜¯ï¼‰çš„ä¿¡ç”¨å¡ä½™é¢å¾€å¾€é«˜äºéå­¦ç”Ÿï¼ˆå¦ï¼‰ã€‚è¿™å°±æ˜¯æ··æ‚æ•ˆåº”çš„æ ¹æºã€‚
* </strong>å·¦å›¾ï¼ˆè¿çº¦ç‡ï¼‰<strong>ï¼š *
</strong>è™šçº¿<strong>æ˜¾ç¤º<em>æ€»ä½“</em>è¿çº¦ç‡ã€‚æ©™è‰²çº¿ï¼ˆå­¦ç”Ÿï¼‰é«˜äºè“è‰²çº¿ï¼ˆéå­¦ç”Ÿï¼‰ã€‚è¿™ä¸æˆ‘ä»¬çš„ç®€å•æ¨¡å‹ï¼ˆâ€œè¿çº¦
~ å­¦ç”Ÿâ€ï¼‰ç›¸ç¬¦ã€‚ * </strong>S
å½¢å®çº¿<strong>æ˜¾ç¤ºè¿çº¦æ¦‚ç‡ä¸ä½™é¢çš„å…³ç³»ã€‚å¯¹äºä»»ä½•<em>ç»™å®š</em>çš„ä½™é¢ï¼Œè“è‰²æ›²çº¿ï¼ˆéå­¦ç”Ÿï¼‰ç•¥é«˜äºæ©™è‰²æ›²çº¿ï¼ˆå­¦ç”Ÿï¼‰ã€‚è¿™æ„å‘³ç€</strong>åœ¨ç›¸åŒçš„å€ºåŠ¡æ°´å¹³ä¸‹ï¼Œå­¦ç”Ÿè¿çº¦çš„å¯èƒ½æ€§<em>è¾ƒå°</em>ã€‚è¿™ä¸æˆ‘ä»¬çš„å¤šå…ƒå›å½’æ¨¡å‹ï¼ˆâ€œè¿çº¦
~ å­¦ç”Ÿ + ä½™é¢ + æ”¶å…¥â€ï¼‰ç›¸ç¬¦ã€‚</p>
<p>è¿™å¼ å›¾å·§å¦™åœ°è¯´æ˜äº†ä¸ºä»€ä¹ˆä¸€ä¸ªå˜é‡åœ¨å•ç‹¬ä½¿ç”¨æ—¶ä¼¼ä¹ä¼šäº§ç”Ÿä¸€ç§å½±å“ï¼Œä½†åœ¨æ§åˆ¶æ··æ‚å› ç´ åå´ä¼šäº§ç”Ÿç›¸åçš„å½±å“ã€‚</p>
<h2 id="an-important-edge-case-perfect-separation">An Important Edge
Case: Perfect Separation âš ï¸</h2>
<p>What happens if the data can be perfectly separated by a straight
line? å¦‚æœæ•°æ®å¯ä»¥ç”¨ä¸€æ¡ç›´çº¿å®Œç¾åˆ†ç¦»ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ</p>
<p>One might think this is the ideal scenario, but it causes a problem
for the logistic regression algorithm. The model will try to find
coefficients that make the probabilities for each class as close to 1
and 0 as possible. To do this, the magnitude of the coefficients (<span
class="math inline">\(\hat{\beta}\)</span>) must grow infinitely large.
äººä»¬å¯èƒ½è®¤ä¸ºè¿™æ˜¯ç†æƒ³æƒ…å†µï¼Œä½†å®ƒä¼šç»™é€»è¾‘å›å½’ç®—æ³•å¸¦æ¥é—®é¢˜ã€‚æ¨¡å‹ä¼šå°è¯•æ‰¾åˆ°ä½¿æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡å°½å¯èƒ½æ¥è¿‘
1 å’Œ 0 çš„ç³»æ•°ã€‚ä¸ºæ­¤ï¼Œç³»æ•° (<span
class="math inline">\(\hat{\beta}\)</span>) çš„å¤§å°å¿…é¡»æ— é™å¤§ã€‚</p>
<p>The slide â€œNon-convergence for perfectly separated caseâ€ demonstrates
this:</p>
<ul>
<li><p><strong>The Code</strong>: It generates two distinct,
non-overlapping clusters of data points using Pythonâ€™s
<code>scikit-learn</code>.</p></li>
<li><p><strong>Parameter Estimates Graph</strong>: It shows the
<code>Intercept</code>, <code>Coefficient 1</code>, and
<code>Coefficient 2</code> values increasing or decreasing without limit
as the algorithm runs through more iterations. They never converge to a
stable value.</p></li>
<li><p><strong>Decision Boundary Graph</strong>: The decision boundary
itself might look reasonable, but the underlying coefficients are
unstable.</p></li>
<li><p><strong>ä»£ç </strong>ï¼šå®ƒä½¿ç”¨ Python çš„ <code>scikit-learn</code>
ç”Ÿæˆä¸¤ä¸ªä¸åŒçš„ã€ä¸é‡å çš„æ•°æ®ç‚¹èšç±»ã€‚</p></li>
<li><p><strong>å‚æ•°ä¼°è®¡å›¾</strong>ï¼šå®ƒæ˜¾ç¤ºâ€œæˆªè·â€ã€â€œç³»æ•° 1â€å’Œâ€œç³»æ•°
2â€çš„å€¼éšç€ç®—æ³•è¿­ä»£æ¬¡æ•°çš„å¢åŠ æˆ–å‡å°‘è€Œæ— é™å¢å¤§æˆ–å‡å°ã€‚å®ƒä»¬æ°¸è¿œä¸ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªç¨³å®šçš„å€¼ã€‚</p></li>
<li><p><strong>å†³ç­–è¾¹ç•Œå›¾</strong>ï¼šå†³ç­–è¾¹ç•Œæœ¬èº«å¯èƒ½çœ‹èµ·æ¥åˆç†ï¼Œä½†åº•å±‚ç³»æ•°æ˜¯ä¸ç¨³å®šçš„ã€‚</p></li>
</ul>
<p><strong>Key Takeaway</strong>: If your logistic regression model
fails to converge, the first thing you should check for is perfect
separation in your training data.
<strong>å…³é”®è¦ç‚¹</strong>ï¼šå¦‚æœæ‚¨çš„é€»è¾‘å›å½’æ¨¡å‹æœªèƒ½æ”¶æ•›ï¼Œæ‚¨åº”è¯¥æ£€æŸ¥çš„ç¬¬ä¸€ä»¶äº‹å°±æ˜¯è®­ç»ƒæ•°æ®æ˜¯å¦å®Œç¾åˆ†ç¦»ã€‚</p>
<h2 id="code-understanding">Code Understanding</h2>
<p>The slides provide useful code snippets in both R and Python.</p>
<h2 id="r-code-plotting-predictions">R Code (Plotting Predictions)</h2>
<p>This code generates the plot with the two S-shaped curves (one for
students, one for non-students) showing the probability of default as
balance increases.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Create a data frame for prediction with a range of balances</span></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># One version for students, one for non-students</span></span><br><span class="line">Default.st <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>balance<span class="operator">=</span>seq<span class="punctuation">(</span><span class="number">500</span><span class="punctuation">,</span> <span class="number">2500</span><span class="punctuation">,</span> by<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> student<span class="operator">=</span><span class="string">&quot;Yes&quot;</span><span class="punctuation">)</span></span><br><span class="line">Default.nonst <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>balance<span class="operator">=</span>seq<span class="punctuation">(</span><span class="number">500</span><span class="punctuation">,</span> <span class="number">2500</span><span class="punctuation">,</span> by<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> student<span class="operator">=</span><span class="string">&quot;No&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Use the trained multiple regression model (glmod3) to predict probabilities</span></span><br><span class="line">pred.st <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>glmod3<span class="punctuation">,</span> Default.st<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line">pred.nonst <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>glmod3<span class="punctuation">,</span> Default.nonst<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="comment"># Plot the results</span></span><br><span class="line">plot<span class="punctuation">(</span>Default.st<span class="operator">$</span>balance<span class="punctuation">,</span> pred.st<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;l&quot;</span><span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;red&quot;</span><span class="punctuation">,</span> ...<span class="punctuation">)</span> <span class="operator">/</span><span class="operator">/</span> Students</span><br><span class="line">lines<span class="punctuation">(</span>Default.nonst<span class="operator">$</span>balance<span class="punctuation">,</span> pred.nonst<span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;blue&quot;</span><span class="punctuation">,</span> ...<span class="punctuation">)</span> <span class="operator">/</span><span class="operator">/</span> Non<span class="operator">-</span>students</span><br></pre></td></tr></table></figure>
<h4 id="python-code-visualizing-the-decision-boundary">Python Code
(Visualizing the Decision Boundary)</h4>
<p>This Python code uses <code>scikit-learn</code> and
<code>matplotlib</code> to create the plot showing the linear decision
boundary.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Generate synthetic data with two classes</span></span><br><span class="line">X, y = make_classification(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Initialize and fit the logistic regression model</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Create a mesh grid of points to make predictions over the entire plot area</span></span><br><span class="line">xx, yy = np.meshgrid(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Predict the probability for each point on the grid</span></span><br><span class="line">probs = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Plot the decision boundary where the probability is 0.5</span></span><br><span class="line">plt.contour(xx, yy, probs.reshape(xx.shape), levels=[<span class="number">0.5</span>], ...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. Scatter plot the actual data points</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, ...)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="other-important-remarks">Other Important Remarks</h3>
<p>The â€œRemarksâ€ slide briefly mentions some key extensions:</p>
<ul>
<li><p><strong>Probit Model</strong>: An alternative to logistic
regression that uses the cumulative distribution function (CDF) of the
standard normal distribution instead of the sigmoid function. The
results are often very similar.</p></li>
<li><p><strong>Softmax Regression</strong>: An extension of logistic
regression used for multi-class classification (when there are more than
two possible outcomes).</p></li>
<li><p><strong>Probit
æ¨¡å‹</strong>ï¼šé€»è¾‘å›å½’çš„æ›¿ä»£æ–¹æ³•ï¼Œå®ƒä½¿ç”¨æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
(CDF) ä»£æ›¿ S å‹å‡½æ•°ã€‚ç»“æœé€šå¸¸éå¸¸ç›¸ä¼¼ã€‚</p></li>
<li><p><strong>Softmax
å›å½’</strong>ï¼šé€»è¾‘å›å½’çš„æ‰©å±•ï¼Œç”¨äºå¤šç±»åˆ†ç±»ï¼ˆå½“å­˜åœ¨ä¸¤ä¸ªä»¥ä¸Šå¯èƒ½ç»“æœæ—¶ï¼‰ã€‚</p></li>
</ul>
<h1
id="here-is-a-summary-of-the-slides-on-linear-discriminant-analysis-lda-including-the-key-mathematical-formulas-visual-explanations-and-how-to-implement-it-in-python.">5.
Here is a summary of the slides on Linear Discriminant Analysis (LDA),
including the key mathematical formulas, visual explanations, and how to
implement it in Python.</h1>
<h2
id="the-main-idea-classification-using-probabilities-ä½¿ç”¨æ¦‚ç‡è¿›è¡Œåˆ†ç±»">The
Main Idea: Classification Using Probabilities ä½¿ç”¨æ¦‚ç‡è¿›è¡Œåˆ†ç±»</h2>
<p>Linear Discriminant Analysis (LDA) is a classification method. For a
given input <strong>x</strong>, it calculates the probability that
<strong>x</strong> belongs to each class and then assigns
<strong>x</strong> to the class with the <strong>highest
probability</strong>.</p>
<p>It does this using <strong>Bayesâ€™ Theorem</strong>, which provides a
formula for the posterior probability <span class="math inline">\(P(Y=k
| X=x)\)</span>, or the probability that the class is <span
class="math inline">\(k\)</span> given the input <span
class="math inline">\(x\)</span>. çº¿æ€§åˆ¤åˆ«åˆ†æ (LDA)
æ˜¯ä¸€ç§åˆ†ç±»æ–¹æ³•ã€‚å¯¹äºç»™å®šçš„è¾“å…¥ <strong>x</strong>ï¼Œå®ƒè®¡ç®—
<strong>x</strong> å±äºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œç„¶åå°† <strong>x</strong>
åˆ†é…ç»™<strong>æ¦‚ç‡æœ€é«˜</strong>çš„ç±»åˆ«ã€‚</p>
<p>å®ƒä½¿ç”¨<strong>è´å¶æ–¯å®šç†</strong>æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¯¥å®šç†æä¾›äº†åéªŒæ¦‚ç‡
<span class="math inline">\(P(Y=k | X=x)\)</span> çš„å…¬å¼ï¼Œå³ç»™å®šè¾“å…¥
<span class="math inline">\(x\)</span>ï¼Œè¯¥ç±»åˆ«å±äº <span
class="math inline">\(k\)</span> çš„æ¦‚ç‡ã€‚ <span class="math display">\[
p_k(x) = P(Y=k|X=x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)}
\]</span></p>
<ul>
<li><span class="math inline">\(p_k(x)\)</span> is the <strong>posterior
probability</strong> we want to maximize.</li>
<li><span class="math inline">\(\pi_k = P(Y=k)\)</span> is the
<strong>prior probability</strong> of class <span
class="math inline">\(k\)</span> (how common the class is overall).</li>
<li><span class="math inline">\(f_k(x) = f(x|Y=k)\)</span> is the
<strong>class-conditional probability density function</strong> of
observing input <span class="math inline">\(x\)</span> if it belongs to
class <span class="math inline">\(k\)</span>.</li>
</ul>
<p>To classify a new observation <span class="math inline">\(x\)</span>,
we simply find the class <span class="math inline">\(k\)</span> that
makes <span class="math inline">\(p_k(x)\)</span> the largest.
ä¸ºäº†å¯¹æ–°çš„è§‚å¯Ÿå€¼ <span class="math inline">\(x\)</span>
è¿›è¡Œåˆ†ç±»ï¼Œæˆ‘ä»¬åªéœ€æ‰¾åˆ°ä½¿ <span class="math inline">\(p_k(x)\)</span>
æœ€å¤§çš„ç±»åˆ« <span class="math inline">\(k\)</span> å³å¯ã€‚</p>
<hr />
<h2 id="key-assumptions-of-lda">Key Assumptions of LDA</h2>
<p>LDAâ€™s power comes from a specific, simplifying assumption about the
dataâ€™s distribution. LDA
çš„å¼ºå¤§ä¹‹å¤„åœ¨äºå®ƒå¯¹æ•°æ®åˆ†å¸ƒè¿›è¡Œäº†ç‰¹å®šçš„ç®€åŒ–å‡è®¾ã€‚</p>
<ol type="1">
<li><p><strong>Gaussian Distribution:</strong> LDA assumes that the data
within each class <span class="math inline">\(k\)</span> follows a
p-dimensional multivariate normal (or Gaussian) distribution, denoted as
<span class="math inline">\(X|Y=k \sim \mathcal{N}(\mu_k,
\Sigma)\)</span>.</p></li>
<li><p><strong>Common Covariance:</strong> A crucial assumption is that
all classes share the <strong>same covariance matrix</strong> <span
class="math inline">\(\Sigma\)</span>. This means that while the classes
may have different centers (means, <span
class="math inline">\(\mu_k\)</span>), their shape and orientation
(covariance, <span class="math inline">\(\Sigma\)</span>) are
identical.</p></li>
<li><p><strong>é«˜æ–¯åˆ†å¸ƒ</strong>ï¼šLDA å‡è®¾æ¯ä¸ªç±» <span
class="math inline">\(k\)</span> ä¸­çš„æ•°æ®æœä» p
ç»´å¤šå…ƒæ­£æ€ï¼ˆæˆ–é«˜æ–¯ï¼‰åˆ†å¸ƒï¼Œè¡¨ç¤ºä¸º <span class="math inline">\(X|Y=k \sim
\mathcal{N}(\mu_k, \Sigma)\)</span>ã€‚</p></li>
<li><p><strong>å…±åŒåæ–¹å·®</strong>ï¼šä¸€ä¸ªå…³é”®å‡è®¾æ˜¯æ‰€æœ‰ç±»å…±äº«<strong>ç›¸åŒçš„åæ–¹å·®çŸ©é˜µ</strong>
<span
class="math inline">\(\Sigma\)</span>ã€‚è¿™æ„å‘³ç€è™½ç„¶ç±»å¯èƒ½å…·æœ‰ä¸åŒçš„ä¸­å¿ƒï¼ˆå‡å€¼ï¼Œ<span
class="math inline">\(\mu_k\)</span>ï¼‰ï¼Œä½†å®ƒä»¬çš„å½¢çŠ¶å’Œæ–¹å‘ï¼ˆåæ–¹å·®ï¼Œ<span
class="math inline">\(\Sigma\)</span>ï¼‰æ˜¯ç›¸åŒçš„ã€‚</p></li>
</ol>
<p>The probability density function for a class <span
class="math inline">\(k\)</span> is: <span class="math display">\[
f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} \exp \left( -\frac{1}{2}(x
- \mu_k)^T \Sigma^{-1} (x - \mu_k) \right)
\]</span></p>
<p>The image above (from your slide â€œKnowing normal distributionâ€)
illustrates this. The two â€œbellsâ€ have different centers (different
<span class="math inline">\(\mu_k\)</span>) but similar shapes. The one
on the right is â€œtilted,â€ indicating correlation between variables,
which is captured in the shared covariance matrix <span
class="math inline">\(\Sigma\)</span>.
ä¸Šå›¾ï¼ˆæ‘˜è‡ªå¹»ç¯ç‰‡â€œäº†è§£æ­£æ€åˆ†å¸ƒâ€ï¼‰è¯´æ˜äº†è¿™ä¸€ç‚¹ã€‚ä¸¤ä¸ªâ€œé’Ÿâ€å½¢çš„ä¸­å¿ƒä¸åŒï¼ˆ<span
class="math inline">\(\mu_k\)</span>
ä¸åŒï¼‰ï¼Œä½†å½¢çŠ¶ç›¸ä¼¼ã€‚å³è¾¹çš„é’Ÿå½¢â€œå€¾æ–œâ€ï¼Œè¡¨ç¤ºå˜é‡ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œè¿™ä½“ç°åœ¨å…±äº«åæ–¹å·®çŸ©é˜µ
<span class="math inline">\(\Sigma\)</span> ä¸­ã€‚</p>
<hr />
<h2 id="the-math-behind-lda-the-discriminant-function-åˆ¤åˆ«å‡½æ•°">The Math
Behind LDA: The Discriminant Function åˆ¤åˆ«å‡½æ•°</h2>
<p>Since we only need to find the class <span
class="math inline">\(k\)</span> that maximizes the posterior
probability <span class="math inline">\(p_k(x)\)</span>, we can simplify
the math. The denominator in Bayesâ€™ theorem is the same for all classes,
so we only need to maximize the numerator: <span
class="math inline">\(\pi_k f_k(x)\)</span>.
ç”±äºæˆ‘ä»¬åªéœ€è¦æ‰¾åˆ°ä½¿åéªŒæ¦‚ç‡ <span class="math inline">\(p_k(x)\)</span>
æœ€å¤§åŒ–çš„ç±»åˆ« <span
class="math inline">\(k\)</span>ï¼Œå› æ­¤å¯ä»¥ç®€åŒ–æ•°å­¦è®¡ç®—ã€‚è´å¶æ–¯å®šç†ä¸­çš„åˆ†æ¯å¯¹äºæ‰€æœ‰ç±»åˆ«éƒ½æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦æœ€å¤§åŒ–åˆ†å­ï¼š<span
class="math inline">\(\pi_k f_k(x)\)</span>ã€‚ Taking the logarithm
(which doesnâ€™t change which class is maximal) and removing constant
terms gives us the <strong>linear discriminant function</strong>, <span
class="math inline">\(\delta_k(x)\)</span>:
å–å¯¹æ•°ï¼ˆè¿™ä¸ä¼šæ”¹å˜å“ªä¸ªç±»åˆ«æ˜¯æœ€å¤§å€¼ï¼‰å¹¶ç§»é™¤å¸¸æ•°é¡¹ï¼Œå¾—åˆ°<strong>çº¿æ€§åˆ¤åˆ«å‡½æ•°</strong>ï¼Œ<span
class="math inline">\(\delta_k(x)\)</span>ï¼š</p>
<p><span class="math display">\[
\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1}
\mu_k + \log(\pi_k)
\]</span></p>
<p>This function is <strong>linear</strong> in <span
class="math inline">\(x\)</span>, which is why the method is called
<em>Linear</em> Discriminant Analysis. The decision boundary between any
two classes, say class <span class="math inline">\(k\)</span> and class
<span class="math inline">\(l\)</span>, is the set of points where <span
class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>, which defines
a linear hyperplane. è¯¥å‡½æ•°å…³äº <span class="math inline">\(x\)</span>
æ˜¯<strong>çº¿æ€§</strong>çš„ï¼Œå› æ­¤è¯¥æ–¹æ³•è¢«ç§°ä¸º<em>çº¿æ€§</em>åˆ¤åˆ«åˆ†æã€‚ä»»æ„ä¸¤ä¸ªç±»åˆ«ï¼ˆä¾‹å¦‚ç±»åˆ«
<span class="math inline">\(k\)</span> å’Œç±»åˆ« <span
class="math inline">\(l\)</span>ï¼‰ä¹‹é—´çš„å†³ç­–è¾¹ç•Œæ˜¯æ»¡è¶³ <span
class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>
çš„ç‚¹çš„é›†åˆï¼Œè¿™å®šä¹‰äº†ä¸€ä¸ªçº¿æ€§è¶…å¹³é¢ã€‚</p>
<p>The image above (from your â€œGraph of LDAâ€ slide) is very important. *
<strong>Left:</strong> The ellipses show the true 95% probability
contours for three Gaussian classes. The dashed lines are the ideal
Bayes decision boundaries, which are perfectly linear because the
assumption of common covariance holds. * <strong>Right:</strong> This
shows a sample of data points drawn from those distributions. The solid
lines are the LDA decision boundaries calculated from the sample. They
are a very good estimate of the ideal boundaries. ä¸Šå›¾ï¼ˆæ¥è‡ªæ‚¨çš„â€œLDA
å›¾â€å¹»ç¯ç‰‡ï¼‰éå¸¸é‡è¦ã€‚ *
<strong>å·¦å›¾ï¼š</strong>æ¤­åœ†æ˜¾ç¤ºäº†ä¸‰ä¸ªé«˜æ–¯ç±»åˆ«çš„çœŸå® 95%
æ¦‚ç‡è½®å»“ã€‚è™šçº¿æ˜¯ç†æƒ³çš„è´å¶æ–¯å†³ç­–è¾¹ç•Œï¼Œç”±äºå…±åŒåæ–¹å·®å‡è®¾æˆç«‹ï¼Œå› æ­¤å®ƒä»¬æ˜¯å®Œç¾çš„çº¿æ€§ã€‚
*
<strong>å³å›¾ï¼š</strong>è¿™æ˜¾ç¤ºäº†ä»è¿™äº›åˆ†å¸ƒä¸­æŠ½å–çš„æ•°æ®ç‚¹æ ·æœ¬ã€‚å®çº¿æ˜¯æ ¹æ®æ ·æœ¬è®¡ç®—å‡ºçš„
LDA å†³ç­–è¾¹ç•Œã€‚å®ƒä»¬æ˜¯å¯¹ç†æƒ³è¾¹ç•Œçš„éå¸¸å¥½çš„ä¼°è®¡ã€‚ ***</p>
<h2
id="practical-implementation-estimating-the-parameters-å®é™…åº”ç”¨ä¼°è®¡å‚æ•°">Practical
Implementation: Estimating the Parameters å®é™…åº”ç”¨ï¼šä¼°è®¡å‚æ•°</h2>
<p>In a real-world scenario, we donâ€™t know the true parameters (<span
class="math inline">\(\mu_k\)</span>, <span
class="math inline">\(\Sigma\)</span>, <span
class="math inline">\(\pi_k\)</span>). Instead, we
<strong>estimate</strong> them from our training data (<span
class="math inline">\(n\)</span> total samples, with <span
class="math inline">\(n_k\)</span> samples in class <span
class="math inline">\(k\)</span>).
åœ¨å®é™…åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬ä¸çŸ¥é“çœŸæ­£çš„å‚æ•°ï¼ˆ<span
class="math inline">\(\mu_k\)</span>ã€<span
class="math inline">\(\Sigma\)</span>ã€<span
class="math inline">\(\pi_k\)</span>ï¼‰ã€‚ç›¸åï¼Œæˆ‘ä»¬æ ¹æ®è®­ç»ƒæ•°æ®ï¼ˆ<span
class="math inline">\(n\)</span> ä¸ªæ ·æœ¬ï¼Œ<span
class="math inline">\(n_k\)</span> ä¸ªæ ·æœ¬å±äº <span
class="math inline">\(k\)</span> ç±»ï¼‰æ¥<strong>ä¼°è®¡</strong>å®ƒä»¬ã€‚</p>
<ul>
<li><strong>Prior Probability (<span
class="math inline">\(\hat{\pi}_k\)</span>):</strong> The proportion of
training samples in class <span class="math inline">\(k\)</span>. <span
class="math display">\[\hat{\pi}_k = \frac{n_k}{n}\]</span></li>
<li><strong>Class Mean (<span
class="math inline">\(\hat{\mu}_k\)</span>):</strong> The average of the
training samples in class <span class="math inline">\(k\)</span>. <span
class="math display">\[\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i=k}
x_i\]</span></li>
<li><strong>Common Covariance (<span
class="math inline">\(\hat{\Sigma}\)</span>):</strong> A weighted
average of the sample covariance matrices for each class. This is often
called the â€œpooledâ€ covariance. <span
class="math display">\[\hat{\Sigma} = \frac{1}{n-K} \sum_{k=1}^{K}
\sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T\]</span></li>
<li><strong>å…ˆéªŒæ¦‚ç‡ (<span
class="math inline">\(\hat{\pi}_k\)</span>)ï¼š</strong>è®­ç»ƒæ ·æœ¬åœ¨ <span
class="math inline">\(k\)</span> ç±»ä¸­çš„æ¯”ä¾‹ã€‚ <span
class="math display">\[\hat{\pi}_k = \frac{n_k}{n}\]</span></li>
<li><strong>ç±»åˆ«å‡å€¼ (<span
class="math inline">\(\hat{\mu}_k\)</span>)ï¼š</strong>è®­ç»ƒæ ·æœ¬åœ¨ <span
class="math inline">\(k\)</span> ç±»ä¸­çš„å¹³å‡å€¼ã€‚ <span
class="math display">\[\hat{\mu}_k = \frac{1}{n_k} \sum_{i: y_i=k}
x_i\]</span></li>
<li><strong>å…¬å…±åæ–¹å·® (<span
class="math inline">\(\hat{\Sigma}\)</span>)ï¼š</strong>æ¯ä¸ªç±»çš„æ ·æœ¬åæ–¹å·®çŸ©é˜µçš„åŠ æƒå¹³å‡å€¼ã€‚è¿™é€šå¸¸è¢«ç§°ä¸ºâ€œåˆå¹¶â€åæ–¹å·®ã€‚
<span class="math display">\[\hat{\Sigma} = \frac{1}{n-K} \sum_{k=1}^{K}
\sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^T\]</span></li>
</ul>
<p>We then plug these estimates into the discriminant function to get
<span class="math inline">\(\hat{\delta}_k(x)\)</span> and classify a
new observation <span class="math inline">\(x\)</span> to the class with
the largest score. ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›ä¼°è®¡å€¼ä»£å…¥åˆ¤åˆ«å‡½æ•°ï¼Œå¾—åˆ° <span
class="math inline">\(\hat{\delta}_k(x)\)</span>ï¼Œå¹¶å°†æ–°çš„è§‚æµ‹å€¼ <span
class="math inline">\(x\)</span> å½’ç±»åˆ°å¾—åˆ†æœ€é«˜çš„ç±»åˆ«ã€‚ ***</p>
<h2 id="evaluating-performance">Evaluating Performance</h2>
<p>After training the model, we evaluate its performance using a
<strong>confusion matrix</strong>.
è®­ç»ƒæ¨¡å‹åï¼Œæˆ‘ä»¬ä½¿ç”¨<strong>æ··æ·†çŸ©é˜µ</strong>æ¥è¯„ä¼°å…¶æ€§èƒ½ã€‚</p>
<p>This matrix shows the true classes versus the predicted classes. *
<strong>Diagonal elements</strong> (9644, 81) are correct predictions. *
<strong>Off-diagonal elements</strong> (23, 252) are errors.
è¯¥çŸ©é˜µæ˜¾ç¤ºäº†çœŸå®ç±»åˆ«ä¸é¢„æµ‹ç±»åˆ«çš„å¯¹æ¯”ã€‚ * <strong>å¯¹è§’çº¿å…ƒç´ </strong>
(9644, 81) è¡¨ç¤ºæ­£ç¡®é¢„æµ‹ã€‚ * <strong>éå¯¹è§’çº¿å…ƒç´ </strong> (23, 252)
è¡¨ç¤ºé”™è¯¯é¢„æµ‹ã€‚</p>
<p>From this matrix, we can calculate key metrics: * <strong>Overall
Error Rate:</strong> Total incorrect predictions / Total predictions. *
Example: <span class="math inline">\((252 + 23) / 10000 =
2.75\%\)</span> * <strong>Sensitivity (True Positive Rate):</strong>
Correctly predicted positives / Total actual positives. It answers: â€œOf
all the people who actually defaulted, what fraction did we catch?â€ *
Example: <span class="math inline">\(81 / 333 = 24.3\%\)</span>. The
sensitivity is <span class="math inline">\(1 - 75.7\% = 24.3\%\)</span>.
* <strong>Specificity (True Negative Rate):</strong> Correctly predicted
negatives / Total actual negatives. It answers: â€œOf all the people who
did not default, what fraction did we correctly identify?â€ * Example:
<span class="math inline">\(9644 / 9667 = 99.8\%\)</span>. The
specificity is <span class="math inline">\(1 - 0.24\% =
99.8\%\)</span>.</p>
<p>The example in your slides shows a high error rate for â€œdefaultâ€
people (75.7%) because the classes are <strong>unbalanced</strong>â€”there
are far fewer defaulters. This highlights the importance of looking at
class-specific metrics, not just the overall error rate.</p>
<hr />
<h2 id="python-code-understanding">Python Code Understanding</h2>
<p>In Python, you can easily implement LDA using the
<code>scikit-learn</code> library. The code conceptually mirrors the
steps we discussed.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume you have your data X (features) and y (labels)</span></span><br><span class="line"><span class="comment"># X = features (e.g., balance, income)</span></span><br><span class="line"><span class="comment"># y = labels (e.g., 0 for &#x27;no-default&#x27;, 1 for &#x27;default&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Create an instance of the LDA model</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Fit the model to the training data</span></span><br><span class="line"><span class="comment"># This is where the model calculates the estimates:</span></span><br><span class="line"><span class="comment">#  - Prior probabilities (pi_k)</span></span><br><span class="line"><span class="comment">#  - Class means (mu_k)</span></span><br><span class="line"><span class="comment">#  - Pooled covariance matrix (Sigma)</span></span><br><span class="line">lda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Make predictions on new, unseen data</span></span><br><span class="line">predictions = lda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Evaluate the model&#x27;s performance</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Confusion Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, predictions))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, predictions))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>LinearDiscriminantAnalysis()</code> creates the classifier
object.</li>
<li><code>lda.fit(X_train, y_train)</code> is the core training step
where the model learns the <span
class="math inline">\(\hat{\pi}_k\)</span>, <span
class="math inline">\(\hat{\mu}_k\)</span>, and <span
class="math inline">\(\hat{\Sigma}\)</span> parameters from the
data.</li>
<li><code>lda.predict(X_test)</code> uses the learned discriminant
function <span class="math inline">\(\hat{\delta}_k(x)\)</span> to
classify each sample in the test set.</li>
<li><code>confusion_matrix</code> and <code>classification_report</code>
are tools to evaluate the results, just like in the slides.</li>
</ul>
<h1
id="here-is-a-summary-of-the-provided-slides-on-linear-discriminant-analysis-lda-focusing-on-mathematical-concepts-python-code-interpretation-and-key-visuals.">6.
Here is a summary of the provided slides on Linear Discriminant Analysis
(LDA), focusing on mathematical concepts, Python code interpretation,
and key visuals.</h1>
<h2 id="core-concept-lda-for-classification">Core Concept: LDA for
Classification</h2>
<p>Linear Discriminant Analysis (LDA) is a classification method that
models the probability that an observation belongs to a certain class.
It works by finding a linear combination of features that best separates
two or more classes.</p>
<p>The decision is based on <strong>Bayesâ€™ theorem</strong>. For a given
observation with features <span class="math inline">\(X=x\)</span>, LDA
calculates the <strong>posterior probability</strong>, <span
class="math inline">\(p_k(x) = Pr(Y=k|X=x)\)</span>, for each class
<span class="math inline">\(k\)</span>. This is the probability that the
observation belongs to class <span class="math inline">\(k\)</span>
given its features. çº¿æ€§åˆ¤åˆ«åˆ†æ (LDA)
æ˜¯ä¸€ç§åˆ†ç±»æ–¹æ³•ï¼Œå®ƒå¯¹è§‚æµ‹å€¼å±äºæŸä¸ªç±»åˆ«çš„æ¦‚ç‡è¿›è¡Œå»ºæ¨¡ã€‚å®ƒçš„å·¥ä½œåŸç†æ˜¯æ‰¾åˆ°èƒ½å¤Ÿæœ€å¥½åœ°åŒºåˆ†ä¸¤ä¸ªæˆ–å¤šä¸ªç±»åˆ«çš„ç‰¹å¾çš„çº¿æ€§ç»„åˆã€‚</p>
<p>è¯¥å†³ç­–åŸºäº<strong>è´å¶æ–¯å®šç†</strong>ã€‚å¯¹äºç‰¹å¾ä¸º <span
class="math inline">\(X=x\)</span> çš„ç»™å®šè§‚æµ‹å€¼ï¼ŒLDA ä¼šè®¡ç®—æ¯ä¸ªç±»åˆ«
<span class="math inline">\(k\)</span>
çš„<strong>åéªŒæ¦‚ç‡</strong>ï¼Œ<span class="math inline">\(p_k(x) =
Pr(Y=k|X=x)\)</span>ã€‚è¿™æ˜¯ç»™å®šè§‚æµ‹å€¼çš„ç‰¹å¾åï¼Œè¯¥è§‚æµ‹å€¼å±äºç±»åˆ« <span
class="math inline">\(k\)</span> çš„æ¦‚ç‡ã€‚</p>
<p>By default, the Bayes classifier assigns an observation to the class
with the highest posterior probability. For a binary (two-class) problem
like â€˜Yesâ€™ vs.Â â€˜Noâ€™, this means:
é»˜è®¤æƒ…å†µä¸‹ï¼Œè´å¶æ–¯åˆ†ç±»å™¨å°†è§‚æµ‹å€¼åˆ†é…ç»™åéªŒæ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ã€‚å¯¹äºåƒâ€œæ˜¯â€ä¸â€œå¦â€è¿™æ ·çš„äºŒåˆ†ç±»é—®é¢˜ï¼Œè¿™æ„å‘³ç€ï¼š</p>
<ul>
<li>Assign to â€˜Yesâ€™ if <span class="math inline">\(Pr(Y=\text{Yes}|X=x)
&gt; 0.5\)</span></li>
<li>Assign to â€˜Noâ€™ otherwise</li>
</ul>
<h2 id="modifying-the-decision-threshold">Modifying the Decision
Threshold</h2>
<p>The default 0.5 threshold isnâ€™t always optimal. In many real-world
scenarios, the cost of one type of error is much higher than another.
For example, in credit card default prediction: é»˜è®¤çš„ 0.5
é˜ˆå€¼å¹¶éæ€»æ˜¯æœ€ä¼˜çš„ã€‚åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­ï¼Œä¸€ç§é”™è¯¯çš„ä»£ä»·è¿œé«˜äºå¦ä¸€ç§ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¿¡ç”¨å¡è¿çº¦é¢„æµ‹ä¸­ï¼š</p>
<ul>
<li><strong>False Negative:</strong> Incorrectly classifying a person
who will default as someone who wonâ€™t. (The bank loses money).</li>
<li><strong>False Positive:</strong> Incorrectly classifying a person
who wonâ€™t default as someone who will. (The bank loses a potential
customer).</li>
</ul>
<p>A bank might decide that missing a defaulter is much worse than
denying a good customer. To catch more potential defaulters, they can
<strong>lower the probability threshold</strong>.
é“¶è¡Œå¯èƒ½ä¼šè®¤ä¸ºé”™è¿‡ä¸€ä¸ªè¿çº¦è€…æ¯”æ‹’ç»ä¸€ä¸ªä¼˜è´¨å®¢æˆ·æ›´ç³Ÿç³•ã€‚ä¸ºäº†æ•æ‰æ›´å¤šæ½œåœ¨çš„è¿çº¦è€…ï¼Œä»–ä»¬å¯ä»¥<strong>é™ä½æ¦‚ç‡é˜ˆå€¼</strong>ã€‚</p>
<p>A modified rule could be: <span class="math display">\[
Pr(\text{default}=\text{Yes}|X=x) &gt; 0.2
\]</span> This makes the model more â€œsensitiveâ€ to flagging potential
defaulters, even at the cost of misclassifying more non-defaulters.
é™ä½é˜ˆå€¼<strong>ä¼šæé«˜æ•æ„Ÿåº¦</strong>ï¼Œä½†<strong>ä¼šé™ä½ç‰¹å¼‚æ€§</strong>ã€‚</p>
<p>This decision leads to a <strong>trade-off</strong> between two key
performance metrics: * <strong>Sensitivity (True Positive
Rate):</strong> The ability to correctly identify positive cases. (e.g.,
<code>Correctly identified defaulters / Total actual defaulters</code>).
* <strong>Specificity (True Negative Rate):</strong> The ability to
correctly identify negative cases. (e.g.,
<code>Correctly identified non-defaulters / Total actual non-defaulters</code>).</p>
<p>è¿™ä¸€å†³ç­–ä¼šå¯¼è‡´ä¸¤ä¸ªå…³é”®ç»©æ•ˆæŒ‡æ ‡ä¹‹é—´çš„<strong>æƒè¡¡</strong>ï¼š *
<strong>æ•æ„Ÿåº¦ï¼ˆçœŸé˜³æ€§ç‡ï¼‰ï¼š</strong>æ­£ç¡®è¯†åˆ«é˜³æ€§æ¡ˆä¾‹çš„èƒ½åŠ›ã€‚ï¼ˆä¾‹å¦‚ï¼Œâ€œæ­£ç¡®è¯†åˆ«çš„è¿çº¦è€…/å®é™…è¿çº¦è€…æ€»æ•°â€ï¼‰ã€‚
*
<strong>ç‰¹å¼‚æ€§ï¼ˆçœŸé˜´æ€§ç‡ï¼‰ï¼š</strong>æ­£ç¡®è¯†åˆ«é˜´æ€§æ¡ˆä¾‹çš„èƒ½åŠ›ã€‚ï¼ˆä¾‹å¦‚ï¼Œâ€œæ­£ç¡®è¯†åˆ«çš„éè¿çº¦è€…/å®é™…éè¿çº¦è€…æ€»æ•°â€ï¼‰ã€‚</p>
<p>Lowering the threshold <strong>increases sensitivity</strong> but
<strong>decreases specificity</strong>. ## Python Code Explained</p>
<p>The slides show how to implement and adjust LDA using Pythonâ€™s
<code>scikit-learn</code> library.</p>
<h2 id="basic-lda-implementation">Basic LDA Implementation</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the necessary library</span></span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize and train the LDA model</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line">lda_train = lda.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get predictions using the default 0.5 threshold</span></span><br><span class="line">y_pred = lda.predict(X)</span><br></pre></td></tr></table></figure>
<p>This code trains an LDA model and makes predictions using the
standard 50% probability boundary.</p>
<h2 id="adjusting-the-prediction-threshold">Adjusting the Prediction
Threshold</h2>
<p>To use a custom threshold (e.g., 0.2), you donâ€™t use the
<code>.predict()</code> method. Instead, you get the class probabilities
with <code>.predict_proba()</code> and apply the threshold manually.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Get the probabilities for each class</span></span><br><span class="line"><span class="comment"># lda.predict_proba(X) returns an array like [[P(No), P(Yes)], ...]</span></span><br><span class="line"><span class="comment"># We select the second column [:, 1] for the &#x27;Yes&#x27; class probability</span></span><br><span class="line">lda_probs = lda.predict_proba(X)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Define a custom threshold</span></span><br><span class="line">threshold = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Apply the threshold to get new predictions</span></span><br><span class="line"><span class="comment"># This creates a boolean array (True where prob &gt; 0.2, else False)</span></span><br><span class="line"><span class="comment"># We then convert True/False to &#x27;Yes&#x27;/&#x27;No&#x27; labels</span></span><br><span class="line">lda_pred1 = np.where(lda_probs &gt; threshold, <span class="string">&quot;Yes&quot;</span>, <span class="string">&quot;No&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>This is the core technique for tuning the classifierâ€™s behavior to
meet specific business needs, as demonstrated on slides 55 and 56 for
both LDA and Logistic Regression.</p>
<h2 id="important-images-to-understand">Important Images to
Understand</h2>
<ol type="1">
<li><strong>Confusion Matrix (Slide 49):</strong> This table is crucial.
It breaks down the modelâ€™s predictions into True Positives, True
Negatives, False Positives, and False Negatives. All key metrics like
error rate, sensitivity, and specificity are calculated from this
matrix. <strong>æ··æ·†çŸ©é˜µï¼ˆå¹»ç¯ç‰‡
49ï¼‰ï¼š</strong>è¿™å¼ è¡¨è‡³å…³é‡è¦ã€‚å®ƒå°†æ¨¡å‹çš„é¢„æµ‹åˆ†è§£ä¸ºçœŸé˜³æ€§ã€çœŸé˜´æ€§ã€å‡é˜³æ€§å’Œå‡é˜´æ€§ã€‚æ‰€æœ‰å…³é”®æŒ‡æ ‡ï¼Œä¾‹å¦‚é”™è¯¯ç‡ã€çµæ•åº¦å’Œç‰¹å¼‚æ€§ï¼Œéƒ½åŸºäºæ­¤çŸ©é˜µè®¡ç®—å¾—å‡ºã€‚</li>
<li><strong>LDA Decision Boundaries (Slide 51):</strong> This plot
provides a powerful visual intuition. It shows the data points for two
classes and the decision boundary line. The different parallel lines
show how changing the threshold from 0.5 to 0.1 or 0.9 shifts the
boundary, making the model classify more or fewer points into the
minority class. <strong>LDA å†³ç­–è¾¹ç•Œï¼ˆå¹»ç¯ç‰‡
51ï¼‰ï¼š</strong>è¿™å¼ å›¾æä¾›äº†å¼ºå¤§çš„è§†è§‰ç›´è§‚æ€§ã€‚å®ƒå±•ç¤ºäº†ä¸¤ä¸ªç±»åˆ«çš„æ•°æ®ç‚¹å’Œå†³ç­–è¾¹ç•Œçº¿ã€‚ä¸åŒçš„å¹³è¡Œçº¿æ˜¾ç¤ºäº†å°†é˜ˆå€¼ä»
0.5 æ›´æ”¹ä¸º 0.1 æˆ– 0.9
æ—¶è¾¹ç•Œå¦‚ä½•ç§»åŠ¨ï¼Œä»è€Œä½¿æ¨¡å‹å°†æ›´å¤šæˆ–æ›´å°‘çš„ç‚¹å½’å…¥å°‘æ•°ç±»</li>
<li><strong>Error Rate Tradeoff Curve (Slide 53):</strong> This graph is
the most important for understanding the business implication of
changing the threshold. It clearly shows that as the threshold changes,
the error rate for one class goes down while the error rate for the
other goes up. The overall error is minimized at a certain point, but
that may not be the optimal point from a business perspective.
<strong>é”™è¯¯ç‡æƒè¡¡æ›²çº¿ï¼ˆå¹»ç¯ç‰‡
53ï¼‰ï¼š</strong>è¿™å¼ å›¾å¯¹äºç†è§£æ›´æ”¹é˜ˆå€¼çš„ä¸šåŠ¡å«ä¹‰è‡³å…³é‡è¦ã€‚å®ƒæ¸…æ¥šåœ°è¡¨æ˜ï¼Œéšç€é˜ˆå€¼çš„å˜åŒ–ï¼Œä¸€ä¸ªç±»åˆ«çš„é”™è¯¯ç‡ä¸‹é™ï¼Œè€Œå¦ä¸€ä¸ªç±»åˆ«çš„é”™è¯¯ç‡ä¸Šå‡ã€‚æ€»ä½“è¯¯å·®åœ¨æŸä¸ªç‚¹è¾¾åˆ°æœ€å°ï¼Œä½†ä»ä¸šåŠ¡è§’åº¦æ¥çœ‹ï¼Œè¿™å¯èƒ½å¹¶éæœ€ä½³ç‚¹ã€‚</li>
<li><strong>ROC Curve (Slides 54 &amp; 55):</strong> The Receiver
Operating Characteristic (ROC) curve plots Sensitivity vs.Â (1 -
Specificity) for <em>all possible thresholds</em>. An ideal classifier
has a curve that â€œhugsâ€ the top-left corner, indicating high sensitivity
and high specificity. Itâ€™s a standard way to visualize and compare the
overall performance of different classifiers. <strong>ROC æ›²çº¿ï¼ˆå¹»ç¯ç‰‡
54 å’Œ 55ï¼‰ï¼š</strong> æ¥æ”¶è€…æ“ä½œç‰¹æ€§ (ROC)
æ›²çº¿ç»˜åˆ¶äº†<em>æ‰€æœ‰å¯èƒ½é˜ˆå€¼</em>çš„çµæ•åº¦ä¸ï¼ˆ1 -
ç‰¹å¼‚æ€§ï¼‰çš„å…³ç³»ã€‚ç†æƒ³çš„åˆ†ç±»å™¨æ›²çº¿â€œç´§è´´â€å·¦ä¸Šè§’ï¼Œè¡¨ç¤ºé«˜çµæ•åº¦å’Œé«˜ç‰¹å¼‚æ€§ã€‚è¿™æ˜¯å¯è§†åŒ–å’Œæ¯”è¾ƒä¸åŒåˆ†ç±»å™¨æ•´ä½“æ€§èƒ½çš„æ ‡å‡†æ–¹æ³•ã€‚</li>
</ol>
<h1
id="here-is-a-summary-of-the-provided-slides-on-linear-and-quadratic-discriminant-analysis-including-the-key-formulas-python-code-equivalents-and-explanations-of-the-important-concepts.">7.
Here is a summary of the provided slides on Linear and Quadratic
Discriminant Analysis, including the key formulas, Python code
equivalents, and explanations of the important concepts.</h1>
<h2 id="key-goal-classification"><strong>Key Goal:
Classification</strong></h2>
<p>Both <strong>Linear Discriminant Analysis (LDA)</strong> and
<strong>Quadratic Discriminant Analysis (QDA)</strong> are
classification algorithms. Their main goal is to find a decision
boundary to separate different classes (e.g., â€œdefaultâ€ vs.Â â€œnot
defaultâ€) in the data. <strong>çº¿æ€§åˆ¤åˆ«åˆ†æ (LDA)</strong> å’Œ
<strong>äºŒæ¬¡åˆ¤åˆ«åˆ†æ (QDA)</strong>
éƒ½æ˜¯åˆ†ç±»ç®—æ³•ã€‚å®ƒä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªå†³ç­–è¾¹ç•Œæ¥åŒºåˆ†æ•°æ®ä¸­çš„ä¸åŒç±»åˆ«ï¼ˆä¾‹å¦‚ï¼Œâ€œé»˜è®¤â€ä¸â€œéé»˜è®¤â€ï¼‰ã€‚</p>
<h3 id="linear-discriminant-analysis-lda">## Linear Discriminant
Analysis (LDA)</h3>
<p>LDA creates a <strong>linear</strong> decision boundary between
classes. LDA åœ¨ç±»åˆ«ä¹‹é—´åˆ›å»º<strong>çº¿æ€§</strong>å†³ç­–è¾¹ç•Œã€‚</p>
<h4 id="core-idea-fishers-interpretation"><strong>Core Idea (Fisherâ€™s
Interpretation)</strong></h4>
<p>Imagine you have data points for different classes in a 3D space.
Fisherâ€™s idea is to find the best angle to shine a â€œflashlightâ€ on the
data to project its shadow onto a 2D wall (or a 1D line). The â€œbestâ€
projection is the one where the shadows of the different classes are
<strong>as far apart from each other as possible</strong>, while the
shadows within each class are <strong>as tightly packed as
possible</strong>. æƒ³è±¡ä¸€ä¸‹ï¼Œä½ åœ¨ä¸‰ç»´ç©ºé—´ä¸­æ‹¥æœ‰ä¸åŒç±»åˆ«çš„æ•°æ®ç‚¹ã€‚Fisher
çš„æ€æƒ³æ˜¯æ‰¾åˆ°æœ€ä½³è§’åº¦ï¼Œç”¨â€œæ‰‹ç”µç­’â€ç…§å°„æ•°æ®ï¼Œå°†å…¶é˜´å½±æŠ•å°„åˆ°äºŒç»´å¢™å£ï¼ˆæˆ–ä¸€ç»´çº¿ä¸Šï¼‰ã€‚
â€œæœ€ä½³â€æŠ•å½±æ˜¯ä¸åŒç±»åˆ«çš„é˜´å½±<strong>å½¼æ­¤ä¹‹é—´å°½å¯èƒ½è¿œ</strong>ï¼Œè€Œæ¯ä¸ªç±»åˆ«å†…çš„é˜´å½±<strong>å°½å¯èƒ½ç´§å¯†</strong>çš„æŠ•å½±ã€‚</p>
<ul>
<li><strong>Maximize:</strong> The distance between the means of the
projected classes (Between-Class Variance).
æŠ•å½±ç±»åˆ«å‡å€¼ä¹‹é—´çš„è·ç¦»ï¼ˆç±»é—´æ–¹å·®ï¼‰ã€‚</li>
<li><strong>Minimize:</strong> The spread or variance within each
projected class (Within-Class Variance).
æ¯ä¸ªæŠ•å½±ç±»åˆ«å†…çš„æ‰©æ•£æˆ–æ–¹å·®ï¼ˆç±»å†…æ–¹å·®ï¼‰ã€‚ This is the most important
image for understanding the intuition behind LDA. It shows how
projecting the data onto a specific line (defined by vector
<code>w</code>) can make the two classes clearly separable.
è¿™æ˜¯ç†è§£LDAèƒŒåç›´è§‰çš„æœ€é‡è¦å›¾åƒã€‚å®ƒå±•ç¤ºäº†å¦‚ä½•å°†æ•°æ®æŠ•å½±åˆ°ç‰¹å®šç›´çº¿ï¼ˆç”±å‘é‡â€œwâ€å®šä¹‰ï¼‰ä¸Šï¼Œä»è€Œä½¿ä¸¤ä¸ªç±»åˆ«æ¸…æ™°å¯åˆ†ã€‚</li>
</ul>
<h4 id="key-mathematical-formulas"><strong>Key Mathematical
Formulas</strong></h4>
<p>To achieve this, LDA maximizes a ratio called the <strong>Rayleigh
quotient</strong>. LDAæœ€å¤§åŒ–ä¸€ä¸ªç§°ä¸º<strong>ç‘åˆ©å•†</strong>çš„æ¯”ç‡ã€‚</p>
<ol type="1">
<li><strong>Within-Class Covariance (<span
class="math inline">\(\hat{\Sigma}_W\)</span>)</strong>: Measures the
spread of data <em>inside</em> each class. <strong>ç±»å†…åæ–¹å·® (<span
class="math inline">\(\hat{\Sigma}_W\)</span>)</strong>ï¼šè¡¡é‡æ¯ä¸ªç±»åˆ«<em>å†…éƒ¨</em>æ•°æ®çš„æ‰©æ•£ç¨‹åº¦ã€‚
<span class="math display">\[\hat{\Sigma}_W = \frac{1}{n-K}
\sum_{k=1}^{K} \sum_{i: y_i=k} (x_i - \hat{\mu}_k)(x_i -
\hat{\mu}_k)^\top\]</span></li>
<li><strong>Between-Class Covariance (<span
class="math inline">\(\hat{\Sigma}_B\)</span>)</strong>: Measures the
spread <em>between</em> the means of different classes.
<strong>ç±»é—´åæ–¹å·® (<span
class="math inline">\(\hat{\Sigma}_B\)</span>)</strong>ï¼šè¡¡é‡ä¸åŒç±»åˆ«å‡å€¼<em>ä¹‹é—´</em>çš„å·®å¼‚ã€‚
<span class="math display">\[\hat{\Sigma}_B = \sum_{k=1}^{K} n_k
(\hat{\mu}_k - \hat{\mu})(\hat{\mu}_k - \hat{\mu})^\top\]</span></li>
<li><strong>Objective Function</strong>: Find the projection vector
<span class="math inline">\(w\)</span> that maximizes the ratio of
between-class variance to within-class variance.
<strong>ç›®æ ‡å‡½æ•°</strong>ï¼šæ‰¾åˆ°æŠ•å½±å‘é‡ <span
class="math inline">\(w\)</span>ï¼Œä½¿ç±»é—´æ–¹å·®ä¸ç±»å†…æ–¹å·®ä¹‹æ¯”æœ€å¤§åŒ–ã€‚ <span
class="math display">\[\max_w \frac{w^\top \hat{\Sigma}_B w}{w^\top
\hat{\Sigma}_W w}\]</span></li>
</ol>
<h4 id="ldas-main-assumption"><strong>LDAâ€™s Main
Assumption</strong></h4>
<p>The key assumption of LDA is that all classes share the <strong>same
covariance matrix (<span
class="math inline">\(\Sigma\)</span>)</strong>. They can have different
means (<span class="math inline">\(\mu_k\)</span>), but their spread and
orientation must be identical. This assumption is what results in a
linear decision boundary. LDA
çš„å…³é”®å‡è®¾æ˜¯æ‰€æœ‰ç±»åˆ«å…±äº«<strong>ç›¸åŒçš„åæ–¹å·®çŸ©é˜µ (<span
class="math inline">\(\Sigma\)</span>)</strong>ã€‚å®ƒä»¬å¯ä»¥å…·æœ‰ä¸åŒçš„å‡å€¼
(<span
class="math inline">\(\mu_k\)</span>)ï¼Œä½†å®ƒä»¬çš„æ•£åº¦å’Œæ–¹å‘å¿…é¡»ç›¸åŒã€‚æ­£æ˜¯è¿™ä¸€å‡è®¾å¯¼è‡´äº†çº¿æ€§å†³ç­–è¾¹ç•Œã€‚</p>
<h3 id="quadratic-discriminant-analysis-qda">## Quadratic Discriminant
Analysis (QDA)</h3>
<p>QDA is a more flexible extension of LDA that creates a
<strong>quadratic</strong> (curved) decision boundary. QDA æ˜¯ LDA
çš„æ›´çµæ´»çš„æ‰©å±•ï¼Œå®ƒåˆ›å»ºäº†<strong>äºŒæ¬¡</strong>ï¼ˆæ›²çº¿ï¼‰å†³ç­–è¾¹ç•Œã€‚ ####
<strong>Core Idea &amp; Key Assumption</strong></p>
<p>QDA starts with the same principles as LDA but drops the key
assumption. QDA assumes that <strong>each class has its own unique
covariance matrix (<span
class="math inline">\(\Sigma_k\)</span>)</strong>. QDA çš„åŸç†ä¸ LDA
ç›¸åŒï¼Œä½†æ”¾å¼ƒäº†å…³é”®å‡è®¾ã€‚QDA å‡è®¾<strong>æ¯ä¸ªç±»åˆ«éƒ½æœ‰è‡ªå·±ç‹¬ç‰¹çš„åæ–¹å·®çŸ©é˜µ
(<span class="math inline">\(\Sigma_k\)</span>)</strong>ã€‚</p>
<p>This means each class can have its own spread, shape, and
orientation. This additional flexibility allows for a more complex,
curved decision boundary.
è¿™æ„å‘³ç€æ¯ä¸ªç±»åˆ«å¯ä»¥æ‹¥æœ‰è‡ªå·±çš„æ•£åº¦ã€å½¢çŠ¶å’Œæ–¹å‘ã€‚è¿™ç§é¢å¤–çš„çµæ´»æ€§ä½¿å¾—å†³ç­–è¾¹ç•Œæ›´åŠ å¤æ‚ã€æ›²çº¿åŒ–ã€‚</p>
<h4 id="key-mathematical-formula"><strong>Key Mathematical
Formula</strong></h4>
<p>The classification is made using a discrimination function, <span
class="math inline">\(\delta_k(x)\)</span>. We assign a data point <span
class="math inline">\(x\)</span> to the class <span
class="math inline">\(k\)</span> for which <span
class="math inline">\(\delta_k(x)\)</span> is largest. The function for
QDA is: <span class="math display">\[\delta_k(x) = -\frac{1}{2}(x -
\mu_k)^\top \Sigma_k^{-1}(x - \mu_k) - \frac{1}{2}\log(|\Sigma_k|) +
\log \pi_k\]</span> The term containing <span
class="math inline">\(x^\top \Sigma_k^{-1} x\)</span> makes this
function a <strong>quadratic</strong> function of <span
class="math inline">\(x\)</span>.</p>
<h3 id="lda-vs.-qda-the-trade-off">## LDA vs.Â QDA: The Trade-Off</h3>
<p>The choice between LDA and QDA is a classic <strong>bias-variance
trade-off</strong>. åœ¨ LDA å’Œ QDA
ä¹‹é—´è¿›è¡Œé€‰æ‹©æ˜¯å…¸å‹çš„<strong>åå·®-æ–¹å·®æƒè¡¡</strong>ã€‚</p>
<ul>
<li><p><strong>Use LDA when:</strong></p>
<ul>
<li>The assumption of a common covariance matrix is reasonable (the
classes have similar shapes).</li>
<li>You have a small amount of training data, as LDA is less prone to
overfitting.</li>
<li>Simplicity is preferred. LDA is less flexible (high bias) but has
lower variance.</li>
<li>å‡è®¾å…±åŒåæ–¹å·®çŸ©é˜µæ˜¯åˆç†çš„ï¼ˆç±»åˆ«å…·æœ‰ç›¸ä¼¼çš„å½¢çŠ¶ï¼‰ã€‚</li>
<li>è®­ç»ƒæ•°æ®é‡è¾ƒå°‘ï¼Œå› ä¸º LDA ä¸æ˜“è¿‡æ‹Ÿåˆã€‚</li>
<li>ç®€æ´æ˜¯é¦–é€‰ã€‚LDA çµæ´»æ€§è¾ƒå·®ï¼ˆåå·®è¾ƒå¤§ï¼‰ï¼Œä½†æ–¹å·®è¾ƒå°ã€‚</li>
</ul></li>
<li><p><strong>Use QDA when:</strong></p>
<ul>
<li>The classes have clearly different shapes and spreads (different
covariance matrices).</li>
<li>You have a large amount of training data to properly estimate the
separate covariance matrices for each class.</li>
<li>QDA is more flexible (low bias) but can have high variance, meaning
it might overfit on smaller datasets.</li>
<li>ç±»åˆ«å…·æœ‰æ˜æ˜¾ä¸åŒçš„å½¢çŠ¶å’Œåˆ†å¸ƒï¼ˆä¸åŒçš„åæ–¹å·®çŸ©é˜µï¼‰ã€‚</li>
<li>æ‹¥æœ‰å¤§é‡è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥æ­£ç¡®ä¼°è®¡æ¯ä¸ªç±»åˆ«çš„ç‹¬ç«‹åæ–¹å·®çŸ©é˜µã€‚</li>
<li>QDA
æ›´çµæ´»ï¼ˆåå·®è¾ƒå°ï¼‰ï¼Œä½†æ–¹å·®è¾ƒå¤§ï¼Œè¿™æ„å‘³ç€å®ƒå¯èƒ½åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆã€‚
<strong>Rule of Thumb:</strong> If the class variances are equal or
close, LDA is better. Otherwise, QDA is better.
<strong>ç»éªŒæ³•åˆ™ï¼š</strong> å¦‚æœç±»åˆ«æ–¹å·®ç›¸ç­‰æˆ–æ¥è¿‘ï¼Œåˆ™ LDA
æ›´ä½³ã€‚å¦åˆ™ï¼ŒQDA æ›´å¥½ã€‚</li>
</ul></li>
</ul>
<h3 id="code-understanding-python-equivalent">## Code Understanding
(Python Equivalent)</h3>
<p>The slides show code in R. Hereâ€™s how you would perform LDA and
evaluate it in Python using the popular <code>scikit-learn</code>
library.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, accuracy_score, roc_curve, auc</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;df&#x27; is your DataFrame with features and a &#x27;target&#x27; column</span></span><br><span class="line"><span class="comment"># X = df.drop(&#x27;target&#x27;, axis=1)</span></span><br><span class="line"><span class="comment"># y = df[&#x27;target&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Fit an LDA model (equivalent to lda() in R)</span></span><br><span class="line">lda = LinearDiscriminantAnalysis()</span><br><span class="line">lda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Make predictions (equivalent to predict() in R)</span></span><br><span class="line">y_pred_lda = lda.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># To fit a QDA model, the process is identical:</span></span><br><span class="line"><span class="comment"># qda = QuadraticDiscriminantAnalysis()</span></span><br><span class="line"><span class="comment"># qda.fit(X_train, y_train)</span></span><br><span class="line"><span class="comment"># y_pred_qda = qda.predict(X_test)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Create a confusion matrix (equivalent to table())</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;LDA Confusion Matrix:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, y_pred_lda))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Plot the ROC Curve (equivalent to the R code for ROC)</span></span><br><span class="line"><span class="comment"># Get prediction probabilities for the positive class</span></span><br><span class="line">y_pred_proba = lda.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate ROC curve points</span></span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate Area Under the Curve (AUC)</span></span><br><span class="line">roc_auc = auc(fpr, tpr)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(fpr, tpr, color=<span class="string">&#x27;blue&#x27;</span>, lw=<span class="number">2</span>, label=<span class="string">f&#x27;LDA ROC curve (area = <span class="subst">&#123;roc_auc:<span class="number">.2</span>f&#125;</span>)&#x27;</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">&#x27;gray&#x27;</span>, lw=<span class="number">2</span>, linestyle=<span class="string">&#x27;--&#x27;</span>) <span class="comment"># Random guess line</span></span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate (1 - Specificity)&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate (Sensitivity)&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Receiver Operating Characteristic (ROC) Curve&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="understanding-the-roc-curve"><strong>Understanding the ROC
Curve</strong></h4>
<p>The <strong>ROC Curve</strong> is another important image. It helps
you visualize a classifierâ€™s performance across all possible
classification thresholds. <strong>ROC æ›²çº¿</strong>
æ˜¯å¦ä¸€ä¸ªé‡è¦çš„å›¾åƒã€‚å®ƒå¯ä»¥å¸®åŠ©æ‚¨ç›´è§‚åœ°äº†è§£åˆ†ç±»å™¨åœ¨æ‰€æœ‰å¯èƒ½çš„åˆ†ç±»é˜ˆå€¼ä¸‹çš„æ€§èƒ½ã€‚</p>
<ul>
<li>The <strong>Y-axis</strong> is the <strong>True Positive
Rate</strong> (Sensitivity): â€œOf all the actual positives, how many did
we correctly identify?â€</li>
<li>The <strong>X-axis</strong> is the <strong>False Positive
Rate</strong>: â€œOf all the actual negatives, how many did we incorrectly
label as positive?â€</li>
<li>A perfect classifier would have a curve that goes straight up to the
top-left corner (100% TPR, 0% FPR). The diagonal line represents a
random guess. The <strong>Area Under the Curve (AUC)</strong> summarizes
the modelâ€™s performance; a value closer to 1.0 is better.</li>
<li><strong>Y è½´</strong>
è¡¨ç¤º<strong>çœŸé˜³æ€§ç‡</strong>ï¼ˆæ•æ„Ÿåº¦ï¼‰ï¼šâ€œåœ¨æ‰€æœ‰å®é™…çš„é˜³æ€§æ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬æ­£ç¡®è¯†åˆ«äº†å¤šå°‘ä¸ªï¼Ÿâ€</li>
<li><strong>X è½´</strong>
è¡¨ç¤º<strong>å‡é˜³æ€§ç‡</strong>ï¼šâ€œåœ¨æ‰€æœ‰å®é™…çš„é˜´æ€§æ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬é”™è¯¯åœ°å°†å¤šå°‘ä¸ªæ ‡è®°ä¸ºé˜³æ€§ï¼Ÿâ€</li>
<li>ä¸€ä¸ªå®Œç¾çš„åˆ†ç±»å™¨åº”è¯¥æœ‰ä¸€æ¡ç›´çº¿ä¸Šå‡åˆ°å·¦ä¸Šè§’çš„æ›²çº¿ï¼ˆçœŸé˜³æ€§ç‡
100%ï¼Œå‡é˜³æ€§ç‡ 0%ï¼‰ã€‚å¯¹è§’çº¿è¡¨ç¤ºéšæœºçŒœæµ‹ã€‚<strong>æ›²çº¿ä¸‹é¢ç§¯
(AUC)</strong> æ¦‚æ‹¬äº†æ¨¡å‹çš„æ€§èƒ½ï¼›è¯¥å€¼è¶Šæ¥è¿‘ 1.0 è¶Šå¥½ã€‚</li>
</ul>
<h1
id="here-is-a-summary-of-the-provided-slides-on-quadratic-discriminant-analysis-qda-including-the-key-formulas-code-explanations-with-python-equivalents-and-a-guide-to-the-most-important-images.">8.
Here is a summary of the provided slides on Quadratic Discriminant
Analysis (QDA), including the key formulas, code explanations with
Python equivalents, and a guide to the most important images.</h1>
<h3 id="core-concept-qda-vs.-lda">## Core Concept: QDA vs.Â LDA</h3>
<p>The main difference between <strong>Linear Discriminant Analysis
(LDA)</strong> and <strong>Quadratic Discriminant Analysis
(QDA)</strong> lies in their assumptions about the data.
<strong>çº¿æ€§åˆ¤åˆ«åˆ†æ (LDA)</strong> å’Œ <strong>äºŒæ¬¡åˆ¤åˆ«åˆ†æ
(QDA)</strong> çš„ä¸»è¦åŒºåˆ«åœ¨äºå®ƒä»¬å¯¹æ•°æ®çš„å‡è®¾ã€‚ * <strong>LDA</strong>
assumes that all classes share the <strong>same covariance
matrix</strong> (<span class="math inline">\(\Sigma\)</span>). It models
each class as a normal distribution with a different mean (<span
class="math inline">\(\mu_k\)</span>) but the same shape and
orientation. This results in a <em>linear</em> decision boundary between
classes. å‡è®¾æ‰€æœ‰ç±»åˆ«å…±äº«<strong>ç›¸åŒçš„åæ–¹å·®çŸ©é˜µ</strong> (<span
class="math inline">\(\Sigma\)</span>)ã€‚å®ƒå°†æ¯ä¸ªç±»åˆ«å»ºæ¨¡ä¸ºå‡å€¼ä¸åŒ
(<span class="math inline">\(\mu_k\)</span>)
ä½†å½¢çŠ¶å’Œæ–¹å‘ç›¸åŒçš„æ­£æ€åˆ†å¸ƒã€‚è¿™ä¼šå¯¼è‡´ç±»åˆ«ä¹‹é—´å‡ºç° <em>çº¿æ€§</em>
å†³ç­–è¾¹ç•Œã€‚ * <strong>QDA</strong> is more flexible. It assumes that each
class <span class="math inline">\(k\)</span> has its <strong>own,
separate covariance matrix</strong> (<span
class="math inline">\(\Sigma_k\)</span>). This allows each classâ€™s
distribution to have a unique shape, size, and orientation. This
flexibility results in a <em>quadratic</em> decision boundary (like a
parabola, hyperbola, or ellipse). æ›´çµæ´»ã€‚å®ƒå‡è®¾æ¯ä¸ªç±»åˆ« <span
class="math inline">\(k\)</span> éƒ½æœ‰å…¶<strong>ç‹¬ç«‹çš„åæ–¹å·®çŸ©é˜µ</strong>
(<span
class="math inline">\(\Sigma_k\)</span>)ã€‚è¿™ä½¿å¾—æ¯ä¸ªç±»åˆ«çš„åˆ†å¸ƒå…·æœ‰ç‹¬ç‰¹çš„å½¢çŠ¶ã€å¤§å°å’Œæ–¹å‘ã€‚è¿™ç§çµæ´»æ€§å¯¼è‡´äº†<em>äºŒæ¬¡</em>å†³ç­–è¾¹ç•Œï¼ˆç±»ä¼¼äºæŠ›ç‰©çº¿ã€åŒæ›²çº¿æˆ–æ¤­åœ†ï¼‰ã€‚
<strong>Analogy</strong> ğŸ’¡: Imagine youâ€™re drawing boundaries around
different clusters of stars. LDA gives you only straight lines to
separate the clusters. QDA gives you curved lines (circles, ellipses),
which can create a much better fit if the clusters themselves are
elliptical and point in different directions.
æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£åœ¨å›´ç»•ä¸åŒçš„æ˜Ÿå›¢ç»˜åˆ¶è¾¹ç•Œã€‚LDA åªæä¾›ç›´çº¿æ¥åˆ†éš”æ˜Ÿå›¢ã€‚QDA
æä¾›æ›²çº¿ï¼ˆåœ†å½¢ã€æ¤­åœ†å½¢ï¼‰ï¼Œå¦‚æœæ˜Ÿå›¢æœ¬èº«æ˜¯æ¤­åœ†å½¢ä¸”æŒ‡å‘ä¸åŒçš„æ–¹å‘ï¼Œåˆ™å¯ä»¥äº§ç”Ÿæ›´å¥½çš„æ‹Ÿåˆæ•ˆæœã€‚</p>
<h3 id="the-math-behind-qda">## The Math Behind QDA</h3>
<p>QDA classifies a new observation <span
class="math inline">\(x\)</span> to the class <span
class="math inline">\(k\)</span> that has the highest discriminant
score, <span class="math inline">\(\delta_k(x)\)</span>. The formula for
this score is what makes the boundary quadratic. QDA å°†æ–°çš„è§‚æµ‹å€¼ <span
class="math inline">\(x\)</span> å½’ç±»åˆ°å…·æœ‰æœ€é«˜åˆ¤åˆ«åˆ†æ•° <span
class="math inline">\(\delta_k(x)\)</span> çš„ç±» <span
class="math inline">\(k\)</span>
ä¸­ã€‚è¯¥åˆ†æ•°çš„å…¬å¼ä½¿å¾—è¾¹ç•Œå…·æœ‰äºŒæ¬¡é¡¹ã€‚</p>
<p>The discriminant function for class <span
class="math inline">\(k\)</span> is: <span
class="math display">\[\delta_k(x) = -\frac{1}{2}(x - \mu_k)^T
\Sigma_k^{-1}(x - \mu_k) - \frac{1}{2}\log(|\Sigma_k|) +
\log(\pi_k)\]</span></p>
<p>Letâ€™s break it down:</p>
<ul>
<li><span class="math inline">\((x - \mu_k)^T \Sigma_k^{-1}(x -
\mu_k)\)</span>: This is a quadratic term (since it involves <span
class="math inline">\(x^T \Sigma_k^{-1} x\)</span>). It measures the
squared Mahalanobis distance from <span class="math inline">\(x\)</span>
to the class mean <span class="math inline">\(\mu_k\)</span>, scaled by
that classâ€™s specific covariance <span
class="math inline">\(\Sigma_k\)</span>.</li>
<li><span class="math inline">\(\log(|\Sigma_k|)\)</span>: A term that
penalizes classes with larger variance.</li>
<li><span class="math inline">\(\log(\pi_k)\)</span>: The prior
probability of class <span class="math inline">\(k\)</span>. This is our
initial belief about how likely class <span
class="math inline">\(k\)</span> is, before seeing the data.
<ul>
<li><span class="math inline">\((x - \mu_k)^T \Sigma_k^{-1}(x -
\mu_k)\)</span>ï¼šè¿™æ˜¯ä¸€ä¸ªäºŒæ¬¡é¡¹ï¼ˆå› ä¸ºå®ƒæ¶‰åŠ <span
class="math inline">\(x^T \Sigma_k^{-1} x\)</span>ï¼‰ã€‚å®ƒæµ‹é‡ä» <span
class="math inline">\(x\)</span> åˆ°ç±»å‡å€¼ <span
class="math inline">\(\mu_k\)</span>
çš„å¹³æ–¹é©¬æ°è·ç¦»ï¼Œå¹¶æ ¹æ®è¯¥ç±»çš„ç‰¹å®šåæ–¹å·® <span
class="math inline">\(\Sigma_k\)</span> è¿›è¡Œç¼©æ”¾ã€‚</li>
<li><span
class="math inline">\(\log(|\Sigma_k|)\)</span>ï¼šç”¨äºæƒ©ç½šæ–¹å·®è¾ƒå¤§çš„ç±»çš„é¡¹ã€‚</li>
<li><span class="math inline">\(\log(\pi_k)\)</span>ï¼šç±» <span
class="math inline">\(k\)</span> çš„å…ˆéªŒæ¦‚ç‡ã€‚è¿™æ˜¯æˆ‘ä»¬åœ¨çœ‹åˆ°æ•°æ®ä¹‹å‰å¯¹ç±»
<span class="math inline">\(k\)</span> å¯èƒ½æ€§çš„åˆå§‹ä¿¡å¿µã€‚ Because each
class <span class="math inline">\(k\)</span> has its own <span
class="math inline">\(\Sigma_k\)</span>, the quadratic term doesnâ€™t
cancel out when comparing scores between classes, leading to a quadratic
boundary. ç”±äºæ¯ä¸ªç±» <span class="math inline">\(k\)</span> éƒ½æœ‰å…¶è‡ªå·±çš„
<span
class="math inline">\(\Sigma_k\)</span>ï¼Œå› æ­¤åœ¨æ¯”è¾ƒç±»ä¹‹é—´çš„åˆ†æ•°æ—¶ï¼ŒäºŒæ¬¡é¡¹ä¸ä¼šæŠµæ¶ˆï¼Œä»è€Œå¯¼è‡´äºŒæ¬¡è¾¹ç•Œã€‚
<strong>Key Trade-off</strong>:</li>
</ul></li>
<li>If the class variances (<span
class="math inline">\(\Sigma_k\)</span>) are truly different,
<strong>QDA is better</strong>.</li>
<li>If the class variances are similar, <strong>LDA is often
better</strong> because itâ€™s less flexible and less likely to overfit,
especially with a small number of training samples.</li>
<li>å¦‚æœç±»æ–¹å·® (<span class="math inline">\(\Sigma_k\)</span>)
ç¡®å®ä¸åŒï¼Œ<strong>QDA æ›´å¥½</strong>ã€‚</li>
<li>å¦‚æœç±»æ–¹å·®ç›¸ä¼¼ï¼Œ<strong>LDA
é€šå¸¸æ›´å¥½</strong>ï¼Œå› ä¸ºå®ƒçš„çµæ´»æ€§è¾ƒå·®ï¼Œå¹¶ä¸”ä¸å¤ªå¯èƒ½è¿‡æ‹Ÿåˆï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ ·æœ¬æ•°é‡è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚</li>
</ul>
<h3 id="code-implementation-r-and-python">## Code Implementation: R and
Python</h3>
<p>The slides provide R code for fitting a QDA model and evaluating it.
Below is an explanation of the R code and its equivalent in Python using
the popular <code>scikit-learn</code> library.</p>
<h4 id="r-code-from-the-slides">R Code (from the slides)</h4>
<p>The code uses the <code>MASS</code> library for QDA and the
<code>ROCR</code> library for evaluation.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ######## QDA ##########</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Fit the model on the training data</span></span><br><span class="line"><span class="comment"># This formula `Default~.` means &quot;predict &#x27;Default&#x27; using all other variables&quot;.</span></span><br><span class="line">qda.fit.mod2 <span class="operator">&lt;-</span> qda<span class="punctuation">(</span>Default<span class="operator">~</span>.<span class="punctuation">,</span> data<span class="operator">=</span>Default<span class="punctuation">,</span> subset<span class="operator">=</span>train.ids<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Make predictions on the test data</span></span><br><span class="line"><span class="comment"># We are interested in the posterior probabilities for the ROC curve</span></span><br><span class="line">qda.fit.pred3 <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>qda.fit.mod2<span class="punctuation">,</span> Default_test<span class="punctuation">)</span><span class="operator">$</span>posterior<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Evaluate using ROC and AUC</span></span><br><span class="line"><span class="comment"># &#x27;prediction&#x27; and &#x27;performance&#x27; are functions from the ROCR library</span></span><br><span class="line">perf <span class="operator">&lt;-</span> performance<span class="punctuation">(</span>prediction<span class="punctuation">(</span>qda.fit.pred3<span class="punctuation">,</span> Default_test<span class="operator">$</span>Default<span class="punctuation">)</span><span class="punctuation">,</span> <span class="string">&quot;auc&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Get the AUC value</span></span><br><span class="line">auc_value <span class="operator">&lt;-</span> perf<span class="operator">@</span>y.values<span class="punctuation">[[</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">]</span></span><br><span class="line"><span class="comment"># Result from slide: 0.9638683</span></span><br></pre></td></tr></table></figure>
<h4 id="python-equivalent-scikit-learn">Python Equivalent
(<code>scikit-learn</code>)</h4>
<p>Hereâ€™s how you would perform the same steps in Python.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> QuadraticDiscriminantAnalysis</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score, roc_curve</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;Default&#x27; is your DataFrame and &#x27;default&#x27; is the target column</span></span><br><span class="line"><span class="comment"># (preprocessing &#x27;student&#x27; and &#x27;default&#x27; columns to numbers)</span></span><br><span class="line"><span class="comment"># Default[&#x27;default_num&#x27;] = Default[&#x27;default&#x27;].apply(lambda x: 1 if x == &#x27;Yes&#x27; else 0)</span></span><br><span class="line"><span class="comment"># X = Default[[&#x27;balance&#x27;, &#x27;income&#x27;, ...]]</span></span><br><span class="line"><span class="comment"># y = Default[&#x27;default_num&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Split data into training and testing sets</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Initialize and fit the QDA model</span></span><br><span class="line">qda = QuadraticDiscriminantAnalysis()</span><br><span class="line">qda.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Predict probabilities on the test set</span></span><br><span class="line"><span class="comment"># We need the probability of the positive class (&#x27;Yes&#x27;) for the AUC calculation</span></span><br><span class="line">y_pred_proba = qda.predict_proba(X_test)[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Calculate the AUC score</span></span><br><span class="line">auc_score = roc_auc_score(y_test, y_pred_proba)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;AUC Score for QDA: <span class="subst">&#123;auc_score:<span class="number">.7</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can also plot the ROC curve</span></span><br><span class="line"><span class="comment"># fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)</span></span><br><span class="line"><span class="comment"># plt.plot(fpr, tpr)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure>
<h3 id="model-evaluation-roc-and-auc">## Model Evaluation: ROC and
AUC</h3>
<p>The slides correctly emphasize using the <strong>ROC curve</strong>
and the <strong>Area Under the Curve (AUC)</strong> to compare model
performance.</p>
<ul>
<li><p><strong>ROC Curve (Receiver Operating Characteristic)</strong>:
This plot shows how well a model can distinguish between two classes. It
plots the <strong>True Positive Rate</strong> (y-axis) against the
<strong>False Positive Rate</strong> (x-axis) at all possible
classification thresholds. A better model has a curve that is closer to
the top-left corner.</p></li>
<li><p><strong>AUC (Area Under the Curve)</strong>: This is a single
number that summarizes the entire ROC curve.</p>
<ul>
<li><strong>AUC = 1</strong>: Perfect classifier.</li>
<li><strong>AUC = 0.5</strong>: A useless classifier (equivalent to
random guessing).</li>
<li><strong>AUC &gt; 0.7</strong>: Generally considered an acceptable
model.</li>
</ul></li>
<li><p><strong>ROC
æ›²çº¿ï¼ˆæ¥æ”¶è€…æ“ä½œç‰¹å¾ï¼‰</strong>ï¼šæ­¤å›¾æ˜¾ç¤ºäº†æ¨¡å‹åŒºåˆ†ä¸¤ä¸ªç±»åˆ«çš„èƒ½åŠ›ã€‚å®ƒç»˜åˆ¶äº†æ‰€æœ‰å¯èƒ½çš„åˆ†ç±»é˜ˆå€¼ä¸‹çš„
<strong>çœŸé˜³æ€§ç‡</strong>ï¼ˆy è½´ï¼‰ä¸ <strong>å‡é˜³æ€§ç‡</strong>ï¼ˆx
è½´ï¼‰çš„å¯¹æ¯”å›¾ã€‚æ›´å¥½çš„æ¨¡å‹çš„æ›²çº¿è¶Šé è¿‘å·¦ä¸Šè§’ï¼Œæ•ˆæœå°±è¶Šå¥½ã€‚</p>
<ul>
<li><p><strong>AUCï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰</strong>ï¼šè¿™æ˜¯ä¸€ä¸ªæ¦‚æ‹¬æ•´ä¸ª ROC
æ›²çº¿çš„æ•°å€¼ã€‚</p></li>
<li><p><strong>AUC = 1</strong>ï¼šå®Œç¾çš„åˆ†ç±»å™¨ã€‚</p></li>
<li><p><strong>AUC =
0.5</strong>ï¼šæ— ç”¨çš„åˆ†ç±»å™¨ï¼ˆç›¸å½“äºéšæœºçŒœæµ‹ï¼‰ã€‚</p></li>
<li><p><strong>AUC &gt;
0.7</strong>ï¼šé€šå¸¸è¢«è®¤ä¸ºæ˜¯å¯æ¥å—çš„æ¨¡å‹ã€‚</p></li>
</ul></li>
</ul>
<p>The slides show that for the <code>Default</code> dataset,
<strong>LDAâ€™s AUC (0.9647) was slightly higher than QDAâ€™s
(0.9639)</strong>. This suggests that the assumption of a common
covariance matrix (LDA) was a slightly better fit for this particular
test set, possibly because QDAâ€™s extra flexibility wasnâ€™t needed and it
may have slightly overfit the training data.
è¿™è¡¨æ˜ï¼Œå¯¹äºè¿™ä¸ªç‰¹å®šçš„æµ‹è¯•é›†ï¼Œå…¬å…±åæ–¹å·®çŸ©é˜µ (LDA)
çš„å‡è®¾æ‹Ÿåˆåº¦ç•¥é«˜ï¼Œå¯èƒ½æ˜¯å› ä¸º QDA
çš„é¢å¤–çµæ´»æ€§å¹¶éå¿…éœ€ï¼Œå¹¶ä¸”å¯èƒ½å¯¹è®­ç»ƒæ•°æ®ç•¥å¾®è¿‡æ‹Ÿåˆã€‚</p>
<h3 id="key-takeaways-and-important-images">## Key Takeaways and
Important Images</h3>
<h3
id="heres-a-ranking-of-the-most-important-visual-aids-in-your-slides">Hereâ€™s
a ranking of the most important visual aids in your slides:</h3>
<ol type="1">
<li><p><strong>Slide 68/69 (Model Assumption &amp; Formula)</strong>:
These are the <strong>most critical</strong> slides. They present the
core theoretical difference between LDA and QDA and provide the
mathematical foundation (the discriminant function formula).
Understanding these is key to understanding QDA.</p></li>
<li><p><strong>Slide 73 (ROC Comparison)</strong>: This is the most
important image for <strong>practical evaluation</strong>. It visually
compares the performance of LDA and QDA side-by-side, making it easy to
see which one performs better on this specific dataset. The concept of
AUC is introduced here as the method for comparison.</p></li>
<li><p><strong>Slide 71 (Decision Boundaries with Different
Thresholds)</strong>: This is an excellent conceptual image. It shows
how the quadratic decision boundary (the curved lines) separates the
data points. It also illustrates how changing the probability threshold
(from 0.1 to 0.5 to 0.9) shifts the boundary, trading off between
precision and recall.</p></li>
</ol>
<p>Of course. Here is a summary of the remaining slides, which compare
QDA to other popular classification models like Logistic Regression and
K-Nearest Neighbors (KNN).</p>
<hr />
<h3 id="visualizing-the-core-trade-off-lda-vs.-qda">Visualizing the Core
Trade-off: LDA vs.Â QDA</h3>
<p>This is the most important concept in these slides. The choice
between LDA and QDA depends entirely on the underlying structure of your
data.</p>
<p>The slide shows two scenarios: 1. <strong>Left Plot (<span
class="math inline">\(\Sigma_1 = \Sigma_2\)</span>):</strong> When the
true covariance matrices of the classes are the same, the optimal
decision boundary (the Bayes classifier) is a straight line. LDA, which
assumes equal covariances, creates a linear boundary that approximates
this optimal boundary very well. QDAâ€™s flexible, curved boundary is
unnecessarily complex and might overfit the training data. <strong>In
this case, LDA is better.</strong> 2. <strong>Right Plot (<span
class="math inline">\(\Sigma_1 \neq \Sigma_2\)</span>):</strong> When
the true covariance matrices are different, the optimal decision
boundary is a curve. QDAâ€™s quadratic model can capture this
non-linearity much better than LDAâ€™s rigid linear model. <strong>In this
case, QDA is better.</strong></p>
<p>This perfectly illustrates the <strong>bias-variance
tradeoff</strong>. LDA has higher bias (itâ€™s less flexible) but lower
variance. QDA has lower bias (itâ€™s more flexible) but higher
variance.</p>
<hr />
<h3 id="comparing-performance-on-the-default-dataset">Comparing
Performance on the â€œDefaultâ€ Dataset</h3>
<p>The slides compare four different models on the same classification
task. Letâ€™s look at their performance using the <strong>Area Under the
Curve (AUC)</strong>, where a higher score is better.</p>
<ul>
<li><strong>LDA AUC:</strong> 0.9647</li>
<li><strong>QDA AUC:</strong> 0.9639</li>
<li><strong>Logistic Regression AUC:</strong> 0.9645</li>
<li><strong>K-Nearest Neighbors (KNN):</strong> The plot shows test
error vs.Â K. The error is lowest around K=4, but itâ€™s not directly
converted to an AUC score in the slides.</li>
</ul>
<p>Interestingly, for this particular dataset, LDA, QDA, and Logistic
Regression perform almost identically. This suggests that the decision
boundary for this problem is likely very close to linear, meaning the
extra flexibility of QDA isnâ€™t providing much benefit.</p>
<hr />
<h3 id="pros-and-cons-which-model-to-choose">Pros and Cons: Which Model
to Choose?</h3>
<p>The final slide asks for a comparison of the models. Hereâ€™s a summary
of their key characteristics:</p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Decision Boundary</th>
<th style="text-align: left;">Key Pro</th>
<th style="text-align: left;">Key Con</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Logistic Regression</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">Highly interpretable, no strong
assumptions about data distribution.</td>
<td style="text-align: left;">Inflexible; cannot capture non-linear
relationships.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Linear Discriminant Analysis
(LDA)</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">More stable than Logistic Regression when
classes are well-separated.</td>
<td style="text-align: left;">Assumes data is normally distributed with
equal covariance matrices for all classes.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Quadratic Discriminant Analysis
(QDA)</strong></td>
<td style="text-align: left;">Parametric</td>
<td style="text-align: left;">Quadratic (Curved)</td>
<td style="text-align: left;">More flexible than LDA; can model
non-linear boundaries.</td>
<td style="text-align: left;">Requires more data to estimate parameters
and is more prone to overfitting. Assumes normality.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>K-Nearest Neighbors
(KNN)</strong></td>
<td style="text-align: left;">Non-Parametric</td>
<td style="text-align: left;">Highly Non-linear</td>
<td style="text-align: left;">Extremely flexible; makes no assumptions
about the dataâ€™s distribution.</td>
<td style="text-align: left;">Can be slow on large datasets and suffers
from the â€œcurse of dimensionality.â€ Less interpretable.</td>
</tr>
</tbody>
</table>
<h4 id="summary-of-the-comparison">Summary of the Comparison:</h4>
<ul>
<li><strong>Linear Models (Logistic Regression &amp; LDA):</strong>
Choose these for simplicity, interpretability, and when you believe the
relationship between predictors and the class is linear. LDA often
outperforms Logistic Regression if its normality assumptions are
met.</li>
<li><strong>Non-Linear Models (QDA &amp; KNN):</strong> Choose these
when the decision boundary is likely more complex. QDA is a good middle
ground, offering more flexibility than LDA without being as completely
data-driven as KNN. KNN is the most flexible but requires careful tuning
of the parameter K to avoid overfitting or underfitting.</li>
</ul>
<h1
id="here-is-a-more-detailed-slide-by-slide-analysis-of-the-presentation.">9.
Here is a more detailed, slide-by-slide analysis of the
presentation.</h1>
<h2 id="four-classification-methods-comparison-by-simulation">4.6 Four
Classification Methods: Comparison by Simulation</h2>
<p>This section (slides 81-87) introduces four classification methods
and systematically compares their performance on six different simulated
datasets. The goal is to see which method works best under different
conditions (e.g., linear vs.Â non-linear boundaries, normal
vs.Â non-normal data).</p>
<p>The four methods being compared are: * <strong>Logistic
Regression:</strong> A linear method that models the log-odds as a
linear function of the predictors. * <strong>Linear Discriminant
Analysis (LDA):</strong> Another linear method. It also assumes a linear
decision boundary but makes stronger assumptions than logistic
regression (e.g., that data within each class is normally distributed
with a common covariance matrix). * <strong>Quadratic Discriminant
Analysis (QDA):</strong> A non-linear method. It assumes the log-odds
are a <em>quadratic</em> function, which creates a more flexible, curved
decision boundary. It assumes data within each class is normally
distributed, but <em>without</em> a common covariance matrix. *
<strong>K-Nearest Neighbors (KNN):</strong> A non-parametric, highly
flexible method. Two versions are tested: * <strong>KNN-1 (<span
class="math inline">\(K=1\)</span>):</strong> A very flexible (high
variance) model. * <strong>KNN-CV:</strong> A tuned model where the best
<span class="math inline">\(K\)</span> is chosen via
cross-validation.</p>
<p>æ¯”è¾ƒçš„å››ç§æ–¹æ³•æ˜¯ï¼š *
<strong>é€»è¾‘å›å½’</strong>ï¼šä¸€ç§å°†å¯¹æ•°æ¦‚ç‡å»ºæ¨¡ä¸ºé¢„æµ‹å˜é‡çº¿æ€§å‡½æ•°çš„çº¿æ€§æ–¹æ³•ã€‚
* <strong>çº¿æ€§åˆ¤åˆ«åˆ†æ
(LDA)</strong>ï¼šå¦ä¸€ç§çº¿æ€§æ–¹æ³•ã€‚å®ƒä¹Ÿå‡è®¾çº¿æ€§å†³ç­–è¾¹ç•Œï¼Œä½†æ¯”é€»è¾‘å›å½’åšå‡ºæ›´å¼ºçš„å‡è®¾ï¼ˆä¾‹å¦‚ï¼Œæ¯ä¸ªç±»ä¸­çš„æ•°æ®å‘ˆæ­£æ€åˆ†å¸ƒï¼Œä¸”å…·æœ‰å…±åŒçš„åæ–¹å·®çŸ©é˜µï¼‰ã€‚
* <strong>äºŒæ¬¡åˆ¤åˆ«åˆ†æ
(QDA)</strong>ï¼šä¸€ç§éçº¿æ€§æ–¹æ³•ã€‚å®ƒå‡è®¾å¯¹æ•°æ¦‚ç‡ä¸º<em>äºŒæ¬¡</em>å‡½æ•°ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªæ›´çµæ´»ã€æ›´å¼¯æ›²çš„å†³ç­–è¾¹ç•Œã€‚å®ƒå‡è®¾æ¯ä¸ªç±»ä¸­çš„æ•°æ®æœä»æ­£æ€åˆ†å¸ƒï¼Œä½†<em>æ²¡æœ‰</em>å…±åŒçš„åæ–¹å·®çŸ©é˜µã€‚
* <strong>Kæœ€è¿‘é‚»
(KNN)</strong>ï¼šä¸€ç§éå‚æ•°åŒ–ã€é«˜åº¦çµæ´»çš„æ–¹æ³•ã€‚æµ‹è¯•äº†ä¸¤ä¸ªç‰ˆæœ¬ï¼š *
<strong>KNN-1 (<span
class="math inline">\(K=1\)</span>)</strong>ï¼šä¸€ä¸ªéå¸¸çµæ´»ï¼ˆé«˜æ–¹å·®ï¼‰çš„æ¨¡å‹ã€‚
*
<strong>KNN-CV</strong>ï¼šä¸€ä¸ªç»è¿‡è°ƒæ•´çš„æ¨¡å‹ï¼Œé€šè¿‡äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³çš„<span
class="math inline">\(K\)</span>ã€‚</p>
<h3 id="analysis-of-simulation-scenarios">Analysis of Simulation
Scenarios</h3>
<p>The performance is measured by the <strong>test error rate</strong>
(lower is better), shown in the boxplots for each scenario.
æ€§èƒ½é€šè¿‡<strong>æµ‹è¯•é”™è¯¯ç‡</strong>ï¼ˆè¶Šä½è¶Šå¥½ï¼‰æ¥è¡¡é‡ï¼Œæ¯ä¸ªåœºæ™¯çš„ç®±çº¿å›¾éƒ½æ˜¾ç¤ºäº†è¯¥é”™è¯¯ç‡ã€‚</p>
<ul>
<li><strong>Scenario 1 (Slide 82):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>linear</strong> decision boundary.
Data is <strong>normally distributed</strong> with <em>uncorrelated</em>
predictors.</li>
<li><strong>Result:</strong> <strong>LDA and Logistic Regression perform
best</strong>. Their test error rates are low and similar. This is
expected, as the setup perfectly matches their core assumption (linear
boundary). QDA is slightly worse because its extra flexibility (being
quadratic) is unnecessary. KNN-1 is the worst, as its high flexibility
leads to high variance (overfitting).</li>
<li><strong>ç»“æœï¼š</strong> <strong>LDA
å’Œé€»è¾‘å›å½’è¡¨ç°æœ€ä½³</strong>ã€‚å®ƒä»¬çš„æµ‹è¯•é”™è¯¯ç‡è¾ƒä½ä¸”ç›¸ä¼¼ã€‚è¿™æ˜¯æ„æ–™ä¹‹ä¸­çš„ï¼Œå› ä¸ºè®¾ç½®å®Œå…¨ç¬¦åˆå®ƒä»¬çš„æ ¸å¿ƒå‡è®¾ï¼ˆçº¿æ€§è¾¹ç•Œï¼‰ã€‚QDA
ç•¥å·®ï¼Œå› ä¸ºå…¶é¢å¤–çš„çµæ´»æ€§ï¼ˆäºŒæ¬¡æ–¹ï¼‰æ˜¯ä¸å¿…è¦çš„ã€‚KNN-1
æœ€å·®ï¼Œå› ä¸ºå…¶é«˜çµæ´»æ€§å¯¼è‡´æ–¹å·®è¾ƒå¤§ï¼ˆè¿‡æ‹Ÿåˆï¼‰ã€‚</li>
</ul></li>
<li><strong>Scenario 2 (Slide 83):</strong>
<ul>
<li><strong>Setup:</strong> Same as Scenario 1 (<strong>linear</strong>
boundary, <strong>normal</strong> data), but now the two predictors have
a <strong>correlation of 0.5</strong>.</li>
<li><strong>Result:</strong> <strong>Almost no change</strong> from
Scenario 1. <strong>LDA and Logistic Regression are still the
best</strong>. This shows that these linear methods are robust to
correlation between predictors.</li>
<li><strong>ç»“æœï¼š</strong>ä¸åœºæ™¯ 1
ç›¸æ¯”<strong>å‡ ä¹æ²¡æœ‰å˜åŒ–</strong>ã€‚<strong>LDA
å’Œé€»è¾‘å›å½’ä»ç„¶æ˜¯æœ€ä½³</strong>ã€‚è¿™è¡¨æ˜è¿™äº›çº¿æ€§æ–¹æ³•å¯¹é¢„æµ‹å› å­ä¹‹é—´çš„ç›¸å…³æ€§å…·æœ‰é²æ£’æ€§ã€‚</li>
</ul></li>
<li><strong>Scenario 3 (Slide 84):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>linear</strong> decision boundary,
but the data is drawn from a <strong>t-distribution</strong> (which is
non-normal and has â€œheavy tails,â€ or more extreme outliers).</li>
<li><strong>Result:</strong> <strong>Logistic Regression is the clear
winner</strong>. LDAâ€™s performance gets worse because its assumption of
<em>normality</em> is violated by the t-distribution. QDAâ€™s performance
deteriorates significantly due to the non-normality. This highlights a
key difference: logistic regression is more robust to violations of the
normality assumption.</li>
<li><strong>ç»“æœï¼š</strong>é€»è¾‘å›å½’æ˜æ˜¾èƒœå‡º**ã€‚LDA çš„æ€§èƒ½ä¼šå˜å·®ï¼Œå› ä¸º t
åˆ†å¸ƒè¿åäº†å…¶æ­£æ€æ€§å‡è®¾ã€‚QDA
çš„æ€§èƒ½ç”±äºéæ­£æ€æ€§è€Œæ˜¾è‘—ä¸‹é™ã€‚è¿™å‡¸æ˜¾äº†ä¸€ä¸ªå…³é”®åŒºåˆ«ï¼šé€»è¾‘å›å½’å¯¹è¿åæ­£æ€æ€§å‡è®¾çš„æƒ…å†µæ›´ç¨³å¥ã€‚</li>
</ul></li>
<li><strong>Scenario 4 (Slide 85):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>quadratic</strong> decision
boundary. Data is <strong>normally distributed</strong> with different
correlations in each class.</li>
<li><strong>Result:</strong> <strong>QDA is the clear winner</strong> by
a large margin. This setup perfectly matches QDAâ€™s assumption (quadratic
boundary from normal data with different covariance structures). All
other methods (LDA, Logistic, KNN) are linear or not flexible enough, so
they perform poorly.</li>
<li><strong>ç»“æœï¼š</strong>QDA æ˜æ˜¾èƒœå‡º**ï¼Œä¸”é¥é¥é¢†å…ˆã€‚æ­¤è®¾ç½®å®Œå…¨ç¬¦åˆ
QDA
çš„å‡è®¾ï¼ˆæ¥è‡ªå…·æœ‰ä¸åŒåæ–¹å·®ç»“æ„çš„æ­£æ€æ•°æ®çš„äºŒæ¬¡è¾¹ç•Œï¼‰ã€‚æ‰€æœ‰å…¶ä»–æ–¹æ³•ï¼ˆLDAã€Logisticã€KNNï¼‰éƒ½æ˜¯çº¿æ€§çš„æˆ–ä¸å¤Ÿçµæ´»ï¼Œå› æ­¤æ€§èƒ½ä¸ä½³ã€‚</li>
</ul></li>
<li><strong>Scenario 5 (Slide 86):</strong>
<ul>
<li><strong>Setup:</strong> Another <strong>quadratic</strong> boundary,
but generated in a different way (using a logistic function of quadratic
terms).</li>
<li><strong>Result:</strong> <strong>QDA performs best again</strong>,
closely followed by the flexible <strong>KNN-CV</strong>. The linear
methods (LDA, Logistic) have poor performance because they cannot
capture the curve.</li>
<li><strong>ç»“æœï¼šQDA
å†æ¬¡è¡¨ç°æœ€ä½³</strong>ï¼Œç´§éšå…¶åçš„æ˜¯çµæ´»çš„<strong>KNN-CV</strong>ã€‚çº¿æ€§æ–¹æ³•ï¼ˆLDAã€Logisticï¼‰æ€§èƒ½è¾ƒå·®ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•æ•æ‰æ›²çº¿ã€‚</li>
</ul></li>
<li><strong>Scenario 6 (Slide 87):</strong>
<ul>
<li><strong>Setup:</strong> A <strong>complex, non-linear</strong>
decision boundary (more complex than a simple quadratic curve).</li>
<li><strong>Result:</strong> The <strong>flexible KNN-CV method is the
winner</strong>. Its non-parametric nature allows it to approximate the
complex shape. QDA is not flexible <em>enough</em> and performs worse.
This slide highlights the bias-variance trade-off: the overly simple
KNN-1 is the worst, but the <em>tuned</em> KNN-CV is the best.</li>
<li><strong>ç»“æœï¼š</strong>çµæ´»çš„ KNN-CV
æ–¹æ³•èƒœå‡º**ã€‚å…¶éå‚æ•°ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿè¿‘ä¼¼å¤æ‚çš„å½¢çŠ¶ã€‚ QDA
ä¸å¤Ÿçµæ´»ï¼Œæ€§èƒ½è¾ƒå·®ã€‚è¿™å¼ å¹»ç¯ç‰‡é‡ç‚¹ä»‹ç»äº†åå·®-æ–¹å·®æƒè¡¡ï¼šè¿‡äºç®€å•çš„ KNN-1
æœ€å·®ï¼Œè€Œ <em>è°ƒæ•´åçš„</em> KNN-CV æœ€å¥½ã€‚</li>
</ul></li>
</ul>
<h2 id="r-example-on-smarket-data">4.7 R Example on Smarket Data</h2>
<p>This section (slides 88-93) applies Logistic Regression and LDA to
the <code>Smarket</code> dataset from the <code>ISLR</code> package to
predict the stock marketâ€™s <code>Direction</code> (Up or Down).
æœ¬èŠ‚ï¼ˆå¹»ç¯ç‰‡ 88-93ï¼‰å°†é€»è¾‘å›å½’å’Œ LDA
åº”ç”¨äºâ€œISLRâ€åŒ…ä¸­çš„â€œSmarketâ€æ•°æ®é›†ï¼Œä»¥é¢„æµ‹è‚¡å¸‚çš„â€œæ–¹å‘â€ï¼ˆä¸Šæ¶¨æˆ–ä¸‹è·Œï¼‰ã€‚
### Data Preparation (Slides 88, 89, 90)</p>
<ol type="1">
<li><strong>Load Data:</strong> The <code>ISLR</code> library is loaded,
and the <code>Smarket</code> dataset is explored. It contains daily
percentage returns (<code>Lag1</code>â€¦<code>Lag5</code> for the previous
5 days, <code>Today</code>), <code>Volume</code>, and the
<code>Year</code>.</li>
<li><strong>Explore Data:</strong> A correlation matrix
(<code>cor(Smarket[,-9])</code>) is computed, and a plot of
<code>Volume</code> over time is generated.</li>
<li><strong>Split Data:</strong> The data is split into a training set
(Years 2001-2004) and a test set (Year 2005).
<ul>
<li><code>train &lt;- (Year&lt;2005)</code></li>
<li><code>Smarket.2005 &lt;- Smarket[!train,]</code></li>
<li><code>Direction.2005 &lt;- Direction[!train]</code></li>
<li>The test set has 252 observations.</li>
</ul></li>
<li><strong>åŠ è½½æ•°æ®</strong>ï¼šåŠ è½½â€œISLRâ€åº“ï¼Œå¹¶æ¢ç´¢â€œSmarketâ€æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¯æ—¥ç™¾åˆ†æ¯”æ”¶ç›Šç‡ï¼ˆå‰
5 å¤©çš„â€œLag1â€â€¦â€œLag5â€ï¼Œâ€œä»Šæ—¥â€ï¼‰ã€â€œæˆäº¤é‡â€å’Œâ€œå¹´ä»½â€ã€‚</li>
<li><strong>æ¢ç´¢æ•°æ®</strong>ï¼šè®¡ç®—ç›¸å…³çŸ©é˜µ
(<code>cor(Smarket[,-9])</code>)ï¼Œå¹¶ç”Ÿæˆâ€œæˆäº¤é‡â€éšæ—¶é—´å˜åŒ–çš„å›¾è¡¨ã€‚</li>
<li><strong>æ‹†åˆ†æ•°æ®</strong>ï¼šå°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆå¹´ä»½
2001-2004ï¼‰å’Œæµ‹è¯•é›†ï¼ˆå¹´ä»½ 2005ï¼‰ã€‚
<ul>
<li><code>train &lt;- (Year&lt;2005)</code></li>
<li><code>Smarket.2005 &lt;- Smarket[!train,]</code></li>
<li><code>Direction.2005 &lt;- Direction[!train]</code></li>
<li>æµ‹è¯•é›†åŒ…å« 252 ä¸ªè§‚æµ‹å€¼ã€‚</li>
</ul></li>
</ol>
<h3 id="model-1-logistic-regression-all-predictors-slide-90">Model 1:
Logistic Regression (All Predictors) (Slide 90)</h3>
<ul>
<li><strong>Model:</strong> A logistic regression model is fit on the
training data using <em>all</em> predictors.
<ul>
<li><code>glm.fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data=Smarket, family=binomial, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> The model is used to predict the
direction for the 2005 test data.
<ul>
<li><code>glm.probs &lt;- predict(glm.fit, Smarket.2005, type="response")</code></li>
<li>A threshold of 0.5 is used to classify: if <span
class="math inline">\(P(\text{Up}) &gt; 0.5\)</span>, predict â€œUpâ€.</li>
</ul></li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.5198 (or <strong>48.0%
accuracy</strong>).</li>
<li><strong>Conclusion:</strong> This is â€œnot good!â€â€”itâ€™s worse than
flipping a coin. This suggests the model is either too complex or the
predictors are not useful.</li>
</ul></li>
</ul>
<h3 id="model-2-logistic-regression-lag1-lag2-slide-91">Model 2:
Logistic Regression (Lag1 &amp; Lag2) (Slide 91)</h3>
<ul>
<li><strong>Model:</strong> Based on the poor results, a simpler model
is tried, using only <code>Lag1</code> and <code>Lag2</code>.
<ul>
<li><code>glm.fit &lt;- glm(Direction ~ Lag1 + Lag2, data=Smarket, family=binomial, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> Predictions are made on the 2005 test
set.</li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.4404 (or <strong>55.95%
accuracy</strong>). This is an improvement.</li>
<li><strong>Confusion Matrix:</strong> | | True Down | True Up | | :â€” |
:â€” | :â€” | | <strong>Pred Down</strong> | 77 | 69 | | <strong>Pred
Up</strong> | 35 | 71 |</li>
<li><strong>ROC and AUC:</strong> The ROC (Receiver Operating
Characteristic) curve is plotted, and the AUC (Area Under the Curve) is
calculated.</li>
<li><strong>AUC Value:</strong> <strong>0.5584</strong>. This is very
close to 0.5 (which represents a random-chance model), indicating that
the model has very weak predictive power, even though its accuracy is
above 50%.</li>
</ul></li>
</ul>
<h3 id="model-3-lda-lag1-lag2-slide-92">Model 3: LDA (Lag1 &amp; Lag2)
(Slide 92)</h3>
<ul>
<li><strong>Model:</strong> LDA is now performed using the same setup:
<code>Lag1</code> and <code>Lag2</code> as predictors, trained on the
2001-2004 data.
<ul>
<li><code>library(MASS)</code></li>
<li><code>lda.fit &lt;- lda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> Predictions are made on the 2005 test
set.
<ul>
<li><code>lda.pred &lt;- predict(lda.fit, Smarket.2005)</code></li>
</ul></li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Error Rate:</strong> 0.4404 (or <strong>55.95%
accuracy</strong>).</li>
<li><strong>Confusion Matrix:</strong> | | True Down | True Up | | :â€” |
:â€” | :â€” | | <strong>Pred Down</strong> | 77 | 69 | | <strong>Pred
Up</strong> | 35 | 71 |</li>
<li><strong>Observation:</strong> The confusion matrix and accuracy are
<em>identical</em> to the logistic regression model.</li>
</ul></li>
</ul>
<h3 id="final-comparison-slide-93">Final Comparison (Slide 93)</h3>
<ul>
<li><strong>ROC and AUC for LDA:</strong> The ROC curve for the LDA
model is plotted.</li>
<li><strong>AUC Value:</strong> <strong>0.5584</strong>.</li>
<li><strong>Main Conclusion:</strong> As highlighted in the green box,
<strong>â€œLDA has identical performance as Logistic regression!â€</strong>
In this specific practical example, using these two predictors, both
linear methods produce the exact same confusion matrix, the same
accuracy (56%), and the same AUC (0.558). This reinforces the
theoretical idea that both are fitting a linear boundary.</li>
</ul>
<h3 id="æœ€ç»ˆæ¯”è¾ƒå¹»ç¯ç‰‡-93">æœ€ç»ˆæ¯”è¾ƒï¼ˆå¹»ç¯ç‰‡ 93ï¼‰</h3>
<ul>
<li><strong>LDA çš„ ROC å’Œ AUCï¼š</strong>ç»˜åˆ¶äº† LDA æ¨¡å‹çš„ ROC
æ›²çº¿ã€‚</li>
<li><strong>AUC å€¼ï¼š</strong>0.5584**ã€‚</li>
<li><strong>ä¸»è¦ç»“è®ºï¼š</strong>å¦‚ç»¿è‰²æ–¹æ¡†æ‰€ç¤ºï¼Œâ€œLDA çš„æ€§èƒ½ä¸ Logistic
å›å½’ç›¸åŒï¼â€**
åœ¨è¿™ä¸ªå…·ä½“çš„å®é™…ç¤ºä¾‹ä¸­ï¼Œä½¿ç”¨è¿™ä¸¤ä¸ªé¢„æµ‹å˜é‡ï¼Œä¸¤ç§çº¿æ€§æ–¹æ³•éƒ½äº§ç”Ÿäº†å®Œå…¨ç›¸åŒçš„æ··æ·†çŸ©é˜µã€ç›¸åŒçš„å‡†ç¡®ç‡ï¼ˆ56%ï¼‰å’Œç›¸åŒçš„
AUCï¼ˆ0.558ï¼‰ã€‚è¿™å¼ºåŒ–äº†ä¸¤è€…å‡æ‹Ÿåˆçº¿æ€§è¾¹ç•Œçš„ç†è®ºè§‚ç‚¹ã€‚</li>
</ul>
<h2 id="r-example-on-smarket-data-continued">4.7 R Example on Smarket
Data (Continued)</h2>
<p>The previous slides showed that Logistic Regression and Linear
Discriminant Analysis (LDA) had <strong>identical performance</strong>
on the Smarket dataset (using <code>Lag1</code> and <code>Lag2</code>),
both achieving 56% test accuracy and an AUC of 0.558. The analysis now
tests a more flexible method, QDA.</p>
<h3 id="model-3-qda-lag1-lag2-slides-94-95">Model 3: QDA (Lag1 &amp;
Lag2) (Slides 94-95)</h3>
<ul>
<li><strong>Model:</strong> A Quadratic Discriminant Analysis (QDA)
model is fit on the same training data (2001-2004) using only the
<code>Lag1</code> and <code>Lag2</code> predictors.
<ul>
<li><code>qda.fit &lt;- qda(Direction ~ Lag1 + Lag2, data=Smarket, subset=train)</code></li>
</ul></li>
<li><strong>Prediction:</strong> The model is used to predict the market
direction for the 2005 test set.</li>
<li><strong>Results:</strong>
<ul>
<li><strong>Test Accuracy:</strong> The model achieves a test accuracy
of <strong>0.5992 (or 60%)</strong>.</li>
<li><strong>AUC:</strong> The Area Under the Curve (AUC) for the QDA
model is <strong>0.562</strong>.</li>
</ul></li>
<li><strong>Conclusion:</strong> As the slide highlights, <strong>â€œQDA
has better test performance than LDA and Logistic
regression!â€</strong></li>
</ul>
<h3 id="smarket-example-summary">Smarket Example Summary</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Model Type</th>
<th style="text-align: left;">Test Accuracy</th>
<th style="text-align: left;">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Logistic Regression</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">~56%</td>
<td style="text-align: left;">0.558</td>
</tr>
<tr>
<td style="text-align: left;"><strong>LDA</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">~56%</td>
<td style="text-align: left;">0.558</td>
</tr>
<tr>
<td style="text-align: left;"><strong>QDA</strong></td>
<td style="text-align: left;">Quadratic</td>
<td style="text-align: left;"><strong>~60%</strong></td>
<td style="text-align: left;"><strong>0.562</strong></td>
</tr>
</tbody>
</table>
<p>This practical example reinforces the lessons from the simulations
(Section 4.6). The two linear methods (LDA, Logistic) had identical
performance. The more flexible, non-linear QDA model performed better,
suggesting that the true decision boundary between â€œUpâ€ and â€œDownâ€
(based on <code>Lag1</code> and <code>Lag2</code>) is not perfectly
linear.</p>
<h2 id="kernel-lda">4.8 Kernel LDA</h2>
<p>This new section introduces an even more advanced non-linear method,
Kernel LDA.</p>
<h3 id="the-problem-linear-inseparability-slide-97">The Problem: Linear
Inseparability (Slide 97)</h3>
<p>The section starts with a clear visual example. A dataset of two
concentric circles (a â€œdonutâ€ shape) is <strong>linearly
inseparable</strong>. It is impossible to draw a single straight line to
separate the inner (purple) class from the outer (yellow) class.</p>
<h3 id="the-solution-the-kernel-trick-slides-97-99">The Solution: The
Kernel Trick (Slides 97, 99)</h3>
<ol type="1">
<li><strong>Nonlinear Transformation:</strong> The data is â€œliftedâ€ into
a higher-dimensional <em>feature space</em> using a <strong>nonlinear
transformation</strong>, <span class="math inline">\(x \mapsto
\phi(x)\)</span>. In the example on the slide, the 2D data is
transformed, and in this new space, the two classes <em>become</em>
<strong>linearly separable</strong>.</li>
<li><strong>The â€œKernel Trickâ€:</strong> The main idea (from slide 99)
is that we donâ€™t need to explicitly compute this complex transformation
<span class="math inline">\(\phi(x)\)</span>. LDA (based on Fisherâ€™s
approach) only requires inner products of the data points. The â€œkernel
trickâ€ allows us to replace the inner product in the high-dimensional
feature space (<span class="math inline">\(x_i^T x_j\)</span>) with a
simple <strong>kernel function</strong>, <span
class="math inline">\(k(x_i, x_j)\)</span>, computed in the original,
low-dimensional space.
<ul>
<li>An example of such a kernel is the <strong>Gaussian (RBF)
kernel</strong>: <span class="math inline">\(k(x_i, x_j) \propto
e^{-\|x_i - x_j\|^2 / \sigma^2}\)</span>.</li>
</ul></li>
</ol>
<h3 id="academic-foundations-slide-98">Academic Foundations (Slide
98)</h3>
<p>This method is based on foundational academic papers that generalized
linear methods using kernels: * <strong>Fisher discriminant analysis
with kernels</strong> (Mika, 1999) * <strong>Generalized Discriminant
Analysis Using a Kernel Approach</strong> (Baudat, 2000) *
<strong>Kernel principal component analysis</strong> (SchÃ¶lkopf,
1997)</p>
<p>In short, Kernel LDA is an extension of LDA that uses the kernel
trick to find a linear boundary in a high-dimensional feature space,
which corresponds to a highly non-linear boundary in the original
space.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/09/27/QM9/" rel="prev" title="QM9 Dataset">
      <i class="fa fa-chevron-left"></i> QM9 Dataset
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/10/06/5054C5/" rel="next" title="MSDM 5054 - Statistical Machine Learning-L5">
      MSDM 5054 - Statistical Machine Learning-L5 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#what-is-classification"><span class="nav-number">1.</span> <span class="nav-text">1. What is Classification?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#the-mathematical-foundation-of-logistic-regression"><span class="nav-number">2.</span> <span class="nav-text">2. the
mathematical foundation of logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#the-logistic-regression-model-from-probabilities-to-log-odds%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E4%BB%8E%E6%A6%82%E7%8E%87%E5%88%B0%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87"><span class="nav-number">2.1.</span> <span class="nav-text">2.1
The Logistic Regression Model: From Probabilities to
Log-Oddsé€»è¾‘å›å½’æ¨¡å‹ï¼šä»æ¦‚ç‡åˆ°å¯¹æ•°å‡ ç‡</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fitting-the-model-maximum-likelihood-estimation-mle-%E6%8B%9F%E5%90%88%E6%A8%A1%E5%9E%8B%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1-mle"><span class="nav-number">2.2.</span> <span class="nav-text">2.2
Fitting the Model: Maximum Likelihood Estimation (MLE)
æ‹Ÿåˆæ¨¡å‹ï¼šæœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-algorithm-newton-raphson-%E7%AE%97%E6%B3%95%E7%89%9B%E9%A1%BF-%E6%8B%89%E5%A4%AB%E6%A3%AE%E7%AE%97%E6%B3%95"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 The
Algorithm: Newton-Raphson ç®—æ³•ï¼šç‰›é¡¿-æ‹‰å¤«æ£®ç®—æ³•</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#the-general-newton-raphson-method"><span class="nav-number">2.3.0.1.</span> <span class="nav-text">The General
Newton-Raphson Method</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#important-image-newton-raphson-example-x3---4-0"><span class="nav-number">2.3.0.2.</span> <span class="nav-text">Important
Image: Newton-Raphson Example (\(x^3 - 4 &#x3D;
0\))</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#code-understanding-python"><span class="nav-number">2.3.0.3.</span> <span class="nav-text">Code Understanding
(Python)</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#provide-a-great-case-study-on-logistic-regression-particularly-on-the-important-concept-of-confounding-variables.-heres-a-summary-covering-the-math-code-and-key-insights."><span class="nav-number">3.</span> <span class="nav-text">Provide
a great case study on logistic regression, particularly on the important
concept of confounding variables. Hereâ€™s a summary covering the math,
code, and key insights.</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#core-concept-logistic-regression-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-number">4.</span> <span class="nav-text">Core
Concept: Logistic Regression ğŸ“ˆ # æ ¸å¿ƒæ¦‚å¿µï¼šé€»è¾‘å›å½’ ğŸ“ˆ</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#how-the-model-learns-mathematical-foundation"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 How the Model
â€œLearnsâ€ (Mathematical Foundation)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-puzzle-a-tale-of-two-models"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 The Puzzle: A Tale of Two
Models ğŸ•µï¸â€â™‚ï¸</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#model-1-simple-logistic-regression-default-vs.-student"><span class="nav-number">4.2.0.1.</span> <span class="nav-text">Model 1:
Simple Logistic Regression (Default vs.Â Student)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B-1%E7%AE%80%E5%8D%95%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%BF%9D%E7%BA%A6-vs.-%E5%AD%A6%E7%94%9F"><span class="nav-number">4.2.0.2.</span> <span class="nav-text">æ¨¡å‹ 1ï¼šç®€å•é€»è¾‘å›å½’ï¼ˆè¿çº¦
vs.Â å­¦ç”Ÿï¼‰</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#model-2-multiple-logistic-regression-default-vs.-all-variables-%E6%A8%A1%E5%9E%8B-2%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%BF%9D%E7%BA%A6-vs.-%E6%89%80%E6%9C%89%E5%8F%98%E9%87%8F"><span class="nav-number">4.3.</span> <span class="nav-text">3.3
Model 2: Multiple Logistic Regression (Default vs.Â All Variables) æ¨¡å‹
2ï¼šå¤šå…ƒé€»è¾‘å›å½’ï¼ˆè¿çº¦ vs.Â æ‰€æœ‰å˜é‡ï¼‰</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#why-the-change-the-confounding-variable-explained"><span class="nav-number">4.3.0.1.</span> <span class="nav-text">Why the
Change? The Confounding Variable Explained</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E6%9C%89%E5%8F%98%E5%8C%96%E6%B7%B7%E6%9D%82%E5%8F%98%E9%87%8F%E8%A7%A3%E9%87%8A"><span class="nav-number">4.3.0.2.</span> <span class="nav-text">ä¸ºä»€ä¹ˆä¼šæœ‰å˜åŒ–ï¼Ÿæ··æ‚å˜é‡è§£é‡Š</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code-implementation-r-vs.-python"><span class="nav-number">4.3.1.</span> <span class="nav-text">Code Implementation: R
vs.Â Python</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#r-code-from-slides"><span class="nav-number">4.3.1.1.</span> <span class="nav-text">R Code (from slides)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#python-equivalent"><span class="nav-number">4.3.1.2.</span> <span class="nav-text">Python Equivalent</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#making-predictions-and-the-decision-boundary-%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B%E5%92%8C%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C"><span class="nav-number">5.</span> <span class="nav-text">4
Making Predictions and the Decision Boundary ğŸ¯è¿›è¡Œé¢„æµ‹å’Œå†³ç­–è¾¹ç•Œ</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#visualizing-the-confounding-effect"><span class="nav-number">5.1.</span> <span class="nav-text">Visualizing the Confounding
Effect</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#an-important-edge-case-perfect-separation"><span class="nav-number">5.2.</span> <span class="nav-text">An Important Edge
Case: Perfect Separation âš ï¸</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-understanding"><span class="nav-number">5.3.</span> <span class="nav-text">Code Understanding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#r-code-plotting-predictions"><span class="nav-number">5.4.</span> <span class="nav-text">R Code (Plotting Predictions)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#python-code-visualizing-the-decision-boundary"><span class="nav-number">5.4.0.1.</span> <span class="nav-text">Python Code
(Visualizing the Decision Boundary)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#other-important-remarks"><span class="nav-number">5.4.1.</span> <span class="nav-text">Other Important Remarks</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#here-is-a-summary-of-the-slides-on-linear-discriminant-analysis-lda-including-the-key-mathematical-formulas-visual-explanations-and-how-to-implement-it-in-python."><span class="nav-number">6.</span> <span class="nav-text">5.
Here is a summary of the slides on Linear Discriminant Analysis (LDA),
including the key mathematical formulas, visual explanations, and how to
implement it in Python.</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#the-main-idea-classification-using-probabilities-%E4%BD%BF%E7%94%A8%E6%A6%82%E7%8E%87%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB"><span class="nav-number">6.1.</span> <span class="nav-text">The
Main Idea: Classification Using Probabilities ä½¿ç”¨æ¦‚ç‡è¿›è¡Œåˆ†ç±»</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#key-assumptions-of-lda"><span class="nav-number">6.2.</span> <span class="nav-text">Key Assumptions of LDA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-math-behind-lda-the-discriminant-function-%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0"><span class="nav-number">6.3.</span> <span class="nav-text">The Math
Behind LDA: The Discriminant Function åˆ¤åˆ«å‡½æ•°</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#practical-implementation-estimating-the-parameters-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E4%BC%B0%E8%AE%A1%E5%8F%82%E6%95%B0"><span class="nav-number">6.4.</span> <span class="nav-text">Practical
Implementation: Estimating the Parameters å®é™…åº”ç”¨ï¼šä¼°è®¡å‚æ•°</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#evaluating-performance"><span class="nav-number">6.5.</span> <span class="nav-text">Evaluating Performance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python-code-understanding"><span class="nav-number">6.6.</span> <span class="nav-text">Python Code Understanding</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#here-is-a-summary-of-the-provided-slides-on-linear-discriminant-analysis-lda-focusing-on-mathematical-concepts-python-code-interpretation-and-key-visuals."><span class="nav-number">7.</span> <span class="nav-text">6.
Here is a summary of the provided slides on Linear Discriminant Analysis
(LDA), focusing on mathematical concepts, Python code interpretation,
and key visuals.</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#core-concept-lda-for-classification"><span class="nav-number">7.1.</span> <span class="nav-text">Core Concept: LDA for
Classification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#modifying-the-decision-threshold"><span class="nav-number">7.2.</span> <span class="nav-text">Modifying the Decision
Threshold</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#basic-lda-implementation"><span class="nav-number">7.3.</span> <span class="nav-text">Basic LDA Implementation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adjusting-the-prediction-threshold"><span class="nav-number">7.4.</span> <span class="nav-text">Adjusting the Prediction
Threshold</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#important-images-to-understand"><span class="nav-number">7.5.</span> <span class="nav-text">Important Images to
Understand</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#here-is-a-summary-of-the-provided-slides-on-linear-and-quadratic-discriminant-analysis-including-the-key-formulas-python-code-equivalents-and-explanations-of-the-important-concepts."><span class="nav-number">8.</span> <span class="nav-text">7.
Here is a summary of the provided slides on Linear and Quadratic
Discriminant Analysis, including the key formulas, Python code
equivalents, and explanations of the important concepts.</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#key-goal-classification"><span class="nav-number">8.1.</span> <span class="nav-text">Key Goal:
Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#linear-discriminant-analysis-lda"><span class="nav-number">8.1.1.</span> <span class="nav-text">## Linear Discriminant
Analysis (LDA)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#core-idea-fishers-interpretation"><span class="nav-number">8.1.1.1.</span> <span class="nav-text">Core Idea (Fisherâ€™s
Interpretation)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#key-mathematical-formulas"><span class="nav-number">8.1.1.2.</span> <span class="nav-text">Key Mathematical
Formulas</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ldas-main-assumption"><span class="nav-number">8.1.1.3.</span> <span class="nav-text">LDAâ€™s Main
Assumption</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#quadratic-discriminant-analysis-qda"><span class="nav-number">8.1.2.</span> <span class="nav-text">## Quadratic Discriminant
Analysis (QDA)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#key-mathematical-formula"><span class="nav-number">8.1.2.1.</span> <span class="nav-text">Key Mathematical
Formula</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lda-vs.-qda-the-trade-off"><span class="nav-number">8.1.3.</span> <span class="nav-text">## LDA vs.Â QDA: The Trade-Off</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code-understanding-python-equivalent"><span class="nav-number">8.1.4.</span> <span class="nav-text">## Code Understanding
(Python Equivalent)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#understanding-the-roc-curve"><span class="nav-number">8.1.4.1.</span> <span class="nav-text">Understanding the ROC
Curve</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#here-is-a-summary-of-the-provided-slides-on-quadratic-discriminant-analysis-qda-including-the-key-formulas-code-explanations-with-python-equivalents-and-a-guide-to-the-most-important-images."><span class="nav-number">9.</span> <span class="nav-text">8.
Here is a summary of the provided slides on Quadratic Discriminant
Analysis (QDA), including the key formulas, code explanations with
Python equivalents, and a guide to the most important images.</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#core-concept-qda-vs.-lda"><span class="nav-number">9.0.1.</span> <span class="nav-text">## Core Concept: QDA vs.Â LDA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-math-behind-qda"><span class="nav-number">9.0.2.</span> <span class="nav-text">## The Math Behind QDA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code-implementation-r-and-python"><span class="nav-number">9.0.3.</span> <span class="nav-text">## Code Implementation: R and
Python</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#r-code-from-the-slides"><span class="nav-number">9.0.3.1.</span> <span class="nav-text">R Code (from the slides)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#python-equivalent-scikit-learn"><span class="nav-number">9.0.3.2.</span> <span class="nav-text">Python Equivalent
(scikit-learn)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-evaluation-roc-and-auc"><span class="nav-number">9.0.4.</span> <span class="nav-text">## Model Evaluation: ROC and
AUC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#key-takeaways-and-important-images"><span class="nav-number">9.0.5.</span> <span class="nav-text">## Key Takeaways and
Important Images</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#heres-a-ranking-of-the-most-important-visual-aids-in-your-slides"><span class="nav-number">9.0.6.</span> <span class="nav-text">Hereâ€™s
a ranking of the most important visual aids in your slides:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#visualizing-the-core-trade-off-lda-vs.-qda"><span class="nav-number">9.0.7.</span> <span class="nav-text">Visualizing the Core
Trade-off: LDA vs.Â QDA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#comparing-performance-on-the-default-dataset"><span class="nav-number">9.0.8.</span> <span class="nav-text">Comparing
Performance on the â€œDefaultâ€ Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pros-and-cons-which-model-to-choose"><span class="nav-number">9.0.9.</span> <span class="nav-text">Pros and Cons: Which Model
to Choose?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#summary-of-the-comparison"><span class="nav-number">9.0.9.1.</span> <span class="nav-text">Summary of the Comparison:</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#here-is-a-more-detailed-slide-by-slide-analysis-of-the-presentation."><span class="nav-number">10.</span> <span class="nav-text">9.
Here is a more detailed, slide-by-slide analysis of the
presentation.</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#four-classification-methods-comparison-by-simulation"><span class="nav-number">10.1.</span> <span class="nav-text">4.6 Four
Classification Methods: Comparison by Simulation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#analysis-of-simulation-scenarios"><span class="nav-number">10.1.1.</span> <span class="nav-text">Analysis of Simulation
Scenarios</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#r-example-on-smarket-data"><span class="nav-number">10.2.</span> <span class="nav-text">4.7 R Example on Smarket Data</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model-1-logistic-regression-all-predictors-slide-90"><span class="nav-number">10.2.1.</span> <span class="nav-text">Model 1:
Logistic Regression (All Predictors) (Slide 90)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-2-logistic-regression-lag1-lag2-slide-91"><span class="nav-number">10.2.2.</span> <span class="nav-text">Model 2:
Logistic Regression (Lag1 &amp; Lag2) (Slide 91)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-3-lda-lag1-lag2-slide-92"><span class="nav-number">10.2.3.</span> <span class="nav-text">Model 3: LDA (Lag1 &amp; Lag2)
(Slide 92)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#final-comparison-slide-93"><span class="nav-number">10.2.4.</span> <span class="nav-text">Final Comparison (Slide 93)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E7%BB%88%E6%AF%94%E8%BE%83%E5%B9%BB%E7%81%AF%E7%89%87-93"><span class="nav-number">10.2.5.</span> <span class="nav-text">æœ€ç»ˆæ¯”è¾ƒï¼ˆå¹»ç¯ç‰‡ 93ï¼‰</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#r-example-on-smarket-data-continued"><span class="nav-number">10.3.</span> <span class="nav-text">4.7 R Example on Smarket
Data (Continued)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model-3-qda-lag1-lag2-slides-94-95"><span class="nav-number">10.3.1.</span> <span class="nav-text">Model 3: QDA (Lag1 &amp;
Lag2) (Slides 94-95)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#smarket-example-summary"><span class="nav-number">10.3.2.</span> <span class="nav-text">Smarket Example Summary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kernel-lda"><span class="nav-number">10.4.</span> <span class="nav-text">4.8 Kernel LDA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-problem-linear-inseparability-slide-97"><span class="nav-number">10.4.1.</span> <span class="nav-text">The Problem: Linear
Inseparability (Slide 97)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-solution-the-kernel-trick-slides-97-99"><span class="nav-number">10.4.2.</span> <span class="nav-text">The Solution: The
Kernel Trick (Slides 97, 99)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#academic-foundations-slide-98"><span class="nav-number">10.4.3.</span> <span class="nav-text">Academic Foundations (Slide
98)</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> å¼ºåŠ›é©±åŠ¨
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
