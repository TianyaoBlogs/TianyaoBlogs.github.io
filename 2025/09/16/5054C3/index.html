<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tianyaoblogs.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="统计机器学习Lecture-3 Lecturer: Prof.XIA DONG 1. General linear regression model.  ## 1.1 general linear regression model - 内容: general linear regression model. the fundamental equation: \[y_i &#x3D; \b">
<meta property="og:type" content="article">
<meta property="og:title" content="Statistical Machine Learning-L3">
<meta property="og:url" content="https://tianyaoblogs.github.io/2025/09/16/5054C3/index.html">
<meta property="og:site_name" content="TianyaoBlogs">
<meta property="og:description" content="统计机器学习Lecture-3 Lecturer: Prof.XIA DONG 1. General linear regression model.  ## 1.1 general linear regression model - 内容: general linear regression model. the fundamental equation: \[y_i &#x3D; \b">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tianyaoblogs.github.io/imgs/5054C3/General_linear_regression_model.png">
<meta property="og:image" content="https://tianyaoblogs.github.io/imgs/5054C3/Simple_Linear_Regression.png">
<meta property="og:image" content="https://tianyaoblogs.github.io/imgs/5054C3/Statistical_Inference1.png">
<meta property="og:image" content="https://tianyaoblogs.github.io/imgs/5054C3/Statistical_Inference2.png">
<meta property="og:image" content="https://tianyaoblogs.github.io/imgs/5054C3/Multiple_Linear%20Regression1.png">
<meta property="og:image" content="https://tianyaoblogs.github.io/imgs/5054C3/Multiple_Linear%20Regression2.png">
<meta property="og:image" content="https://tianyaoblogs.github.io/imgs/5054C3/matrix_notatio.png">
<meta property="og:image" content="https://tianyaoblogs.github.io/imgs/5054C3/OLS1.png">
<meta property="og:image" content="https://tianyaoblogs.github.io/imgs/5054C3/OLS2.png">
<meta property="article:published_time" content="2025-09-16T13:00:00.000Z">
<meta property="article:modified_time" content="2025-09-18T03:56:06.255Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tianyaoblogs.github.io/imgs/5054C3/General_linear_regression_model.png">

<link rel="canonical" href="https://tianyaoblogs.github.io/2025/09/16/5054C3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Statistical Machine Learning-L3 | TianyaoBlogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">TianyaoBlogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://tianyaoblogs.github.io/2025/09/16/5054C3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianyaoBlogs">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Statistical Machine Learning-L3
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-09-16 21:00:00" itemprop="dateCreated datePublished" datetime="2025-09-16T21:00:00+08:00">2025-09-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-18 11:56:06" itemprop="dateModified" datetime="2025-09-18T11:56:06+08:00">2025-09-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>统计机器学习Lecture-3</p>
<p><a target="_blank" rel="noopener" href="https://www.math.hkust.edu.hk/~madxia/">Lecturer: Prof.XIA
DONG</a></p>
<h1 id="general-linear-regression-model.">1. General linear regression
model.</h1>
<p><img src="/imgs/5054C3/General_linear_regression_model.png" alt="Diagram of a linear regression model">
## 1.1 general linear regression model - <strong>内容</strong>:
<strong>general linear regression model</strong>.</p>
<p>the fundamental equation:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1x_{i1} + \dots +
\beta_px_{ip} + \epsilon_i\]</span></p>
<p>And it correctly identifies the main goal: to <strong>estimate the
parameters</strong> (the coefficients <span
class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span>) from
data so we can make predictions on new data.</p>
<p>核心目标：通过数据<strong>估计参数</strong>（即系数 <span
class="math inline">\(\beta_0, \beta_1, \dots,
\beta_p\)</span>），从而对新数据进行预测。</p>
<h2
id="how-we-actually-find-the-best-values-for-the-β-coefficients-parameter-estimation">1.2
How we actually find the best values for the <span
class="math inline">\(β\)</span> coefficients (parameter
estimation)?:</h2>
<ul>
<li><strong>内容</strong>: We find the best values for the <span
class="math inline">\(\beta\)</span> coefficients by finding the values
that <strong>minimize the overall error</strong> of the model. The most
common and fundamental method for this is called <strong>Ordinary Least
Squares (OLS)</strong>.</li>
</ul>
<h3
id="the-main-method-ordinary-least-squares-ols-普通最小二乘法-ols">##
The Main Method: Ordinary Least Squares (OLS) 普通最小二乘法 (OLS)</h3>
<p>The core idea of OLS is to find the line (or hyperplane in multiple
dimensions) that is as close as possible to all the data points
simultaneously. OLS
的核心思想是找到一条尽可能同时接近所有数据点的直线（或多维超平面）。</p>
<h4 id="define-the-error-residuals-误差">1. Define the Error (Residuals)
误差</h4>
<p>First, we need to define what “error” means. For any single data
point, the error is the difference between the actual value (<span
class="math inline">\(y_i\)</span>) and the value predicted by our model
(<span class="math inline">\(\hat{y}_i\)</span>). This difference is
called the <strong>residual</strong>.
首先，需要定义“误差”的含义。对于任何单个数据点，误差是实际值 (<span
class="math inline">\(y_i\)</span>) 与模型预测值 (<span
class="math inline">\(\hat{y}_i\)</span>)
之间的差值。这个差值称为<strong>残差</strong>。</p>
<p><strong>Residual</strong> = Actual Value - Predicted Value
<strong>残差</strong> = 实际值 - 预测值 <span class="math display">\[e_i
= y_i - \hat{y}_i\]</span></p>
<p>You can visualize residuals as the vertical distance from each data
point to the regression line.
可以将残差可视化为每个数据点到回归线的垂直距离。</p>
<h4
id="the-cost-function-sum-of-squared-residuals-成本函数残差平方和">2.
The Cost Function: Sum of Squared Residuals 成本函数：残差平方和</h4>
<p>We want to make all these residuals as small as possible. We can’t
just add them up, because some are positive and some are negative, and
they would cancel each other out.
所有残差尽可能小。不能简单地将它们相加，因为有些是正数，有些是负数，它们会相互抵消。</p>
<p>So, we square each residual (which makes them all positive) and then
sum them up. This gives us the <strong>Sum of Squared Residuals
(SSR)</strong>, which is our “cost function.”
因此，将每个残差求平方（使它们都为正数），然后将它们相加。这就得到了<strong>残差平方和
(SSR)</strong>，也就是“成本函数”。</p>
<p><span class="math display">\[SSR = \sum_{i=1}^{n} e_i^2 =
\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\]</span></p>
<p>The goal of OLS is simple: <strong>find the values of <span
class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span> that
make this SSR value as small as possible.</strong></p>
<h4
id="solving-for-the-coefficients-the-normal-equation-求解系数正态方程">3.
Solving for the Coefficients: The Normal Equation
求解系数：正态方程</h4>
<p>For linear regression, calculus provides a direct, exact solution to
this minimization problem. By taking the derivative of the SSR function
with respect to each <span class="math inline">\(\beta\)</span>
coefficient and setting it to zero, we can solve for the optimal values.
对于线性回归，微积分为这个最小化问题提供了直接、精确的解。通过对 SSR
函数的每个 <span class="math inline">\(\beta\)</span>
系数求导并将其设为零，就可以求解出最优值。</p>
<p>This process results in a formula known as the <strong>Normal
Equation</strong>, which can be expressed cleanly using matrix algebra:
这个过程会得到一个称为<strong>正态方程</strong>的公式，它可以用矩阵代数清晰地表示出来：</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the
vector of our estimated coefficients.估计系数的向量。</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is a matrix where
each row is an observation and each column is a feature (with an added
column of 1s for the intercept <span
class="math inline">\(\beta_0\)</span>).其中每一行代表一个观测值，每一列代表一个特征（截距
<span class="math inline">\(\beta_0\)</span> 增加了一列全为 1
的值）。</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of the
actual response values.实际响应值的向量。</li>
</ul>
<p>Statistical software and programming libraries (like Scikit-learn in
Python) use this equation (or more computationally stable versions of
it) to find the best coefficients for you instantly.</p>
<h3 id="an-alternative-method-gradient-descent-梯度下降">## An
Alternative Method: Gradient Descent 梯度下降</h3>
<p>While the Normal Equation gives a direct answer, it can be very slow
if you have a massive number of features (e.g., hundreds of thousands).
An alternative, iterative method used across machine learning is
<strong>Gradient Descent</strong>.</p>
<p><strong>The Intuition:</strong> Imagine the SSR cost function is a
big valley. Your initial (random) <span
class="math inline">\(\beta\)</span> coefficients place you somewhere on
the slope of this valley.</p>
<ol type="1">
<li><strong>Check the slope</strong> (the gradient) at your current
position. <strong>检查您当前位置的斜率</strong>（梯度）。</li>
<li><strong>Take a small step</strong> in the steepest <em>downhill</em>
direction. <strong>朝最陡的<em>下坡</em>方向</strong>迈出一小步**。</li>
<li><strong>Repeat.</strong> You keep taking steps downhill until you
reach the bottom of the valley. The bottom of the valley represents the
minimum SSR, and your coordinates at that point are the optimal <span
class="math inline">\(\beta\)</span> coefficients.
<strong>重复</strong>。您继续向下走，直到到达山谷底部。谷底代表最小SSR，该点的坐标即为最优<span
class="math inline">\(\beta\)</span>系数。</li>
</ol>
<p>The size of each “step” you take is controlled by a parameter called
the <strong>learning rate</strong>. Gradient Descent is the foundational
optimization algorithm for many complex models, including neural
networks.
每次“步进”的大小由一个称为<strong>学习率</strong>的参数控制。梯度下降是许多复杂模型（包括神经网络）的基础优化算法。</p>
<h3 id="summary-ols-vs.-gradient-descent">## Summary: OLS vs. Gradient
Descent</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Ordinary Least Squares (OLS)</th>
<th style="text-align: left;">Gradient Descent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>How it works</strong></td>
<td style="text-align: left;">Direct calculation using the Normal
Equation.</td>
<td style="text-align: left;">Iterative; takes steps to minimize
error.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Pros</strong></td>
<td style="text-align: left;">Provides an exact, optimal solution. No
parameters to tune.</td>
<td style="text-align: left;">More efficient for very large datasets.
Very versatile.</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Cons</strong></td>
<td style="text-align: left;">Can be computationally expensive with many
features.</td>
<td style="text-align: left;">Requires choosing a learning rate. May not
find the exact minimum.</td>
</tr>
</tbody>
</table>
<h1 id="simple-linear-regression">2. Simple Linear Regression</h1>
<p><img src="/imgs/5054C3/Simple_Linear_Regression.png" alt="Simple_Linear_Regression"></p>
<h2 id="simple-linear-regression-1">2.1 Simple Linear Regression</h2>
<ul>
<li><strong>内容</strong>: <strong>Simple Linear Regression:</strong> a
special case of the general model you showed earlier where you only have
<strong>one</strong> predictor variable (<span
class="math inline">\(p=1\)</span>).</li>
</ul>
<h3 id="the-model-and-the-goal-模型和目标">## The Model and the Goal
模型和目标</h3>
<p>Sets up the simplified equation for a line: <span
class="math display">\[y_i = \beta_0 + \beta_1x_i + \epsilon_i\]</span>
* <span class="math inline">\(y_i\)</span> is the outcome you want to
predict.要预测的结果。 * <span class="math inline">\(x_i\)</span> is
your single input feature or covariate.单个输入特征或协变量。 * <span
class="math inline">\(\beta_1\)</span> is the <strong>slope</strong> of
the line. It tells you how much <span class="math inline">\(y\)</span>
is expected to increase for a one-unit increase in <span
class="math inline">\(x\)</span>.表示 <span
class="math inline">\(x\)</span> 每增加一个单位，<span
class="math inline">\(y\)</span> 预计会增加多少。 * <span
class="math inline">\(\beta_0\)</span> is the
<strong>intercept</strong>. It’s the predicted value of <span
class="math inline">\(y\)</span> when <span
class="math inline">\(x\)</span> is zero.当 <span
class="math inline">\(x\)</span> 为零时 <span
class="math inline">\(y\)</span> 的预测值。 * <span
class="math inline">\(\epsilon_i\)</span> is the random error
term.是随机误差项。</p>
<p>The goal, stated as “Minimize the sum of squares of err,” is exactly
the <strong>Ordinary Least Squares (OLS)</strong> method we just
discussed. It’s written here as: <span class="math display">\[\min_{a,b}
\sum_{i=1}^{n} (y_i - a - bx_i)^2\]</span> This is just a different way
of writing the same thing, where they use <code>a</code> for the
intercept (<span class="math inline">\(\beta_0\)</span>) and
<code>b</code> for the slope (<span
class="math inline">\(\beta_1\)</span>). You’re trying to find the
specific values of the slope and intercept that make the sum of all the
squared errors as small as possible.
目标，即“最小化误差平方和”，正是<strong>普通最小二乘法
(OLS)</strong>。： <span class="math display">\[\min_{a,b}
\sum_{i=1}^{n} (y_i - a - bx_i)^2\]</span> 这是另一种写法，其中用
<code>a</code> 表示截距 (<span
class="math inline">\(\beta_0\)</span>)，<code>b</code> 表示斜率 (<span
class="math inline">\(\beta_1\)</span>)。尝试找到斜率和截距的具体值，使得所有平方误差之和尽可能小。</p>
<h3 id="the-solution-the-estimator-formulas-解决方案估计公式">## The
Solution: The Estimator Formulas 解决方案：估计公式</h3>
<p>The most important part of this slide is the
<strong>solution</strong>. For the simple case with only one variable,
you don’t need complex matrix algebra (the Normal Equation). Instead,
the minimization problem can be solved with these two straightforward
formulas:
对于只有一个变量的简单情况，不需要复杂的矩阵代数（正态方程）。相反，最小化问题可以用以下两个简单的公式来解决：</p>
<h4 id="the-slope-hatbeta_1">1. The Slope: <span
class="math inline">\(\hat{\beta}_1\)</span></h4>
<p><span class="math display">\[\hat{\beta}_1 = \frac{\sum_{i=1}^{n}
(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i -
\bar{x})^2}\]</span> * <strong>Intuition:</strong> This formula might
look complex, but it’s actually very intuitive. * The numerator, <span
class="math inline">\(\sum(x_i - \bar{x})(y_i - \bar{y})\)</span>, is
closely related to the <strong>covariance</strong> between X and Y. It
measures whether X and Y tend to move in the same direction (positive
slope) or in opposite directions (negative slope). 与 X 和 Y
之间的<strong>协方差</strong>密切相关。它衡量 X 和 Y
是倾向于朝相同方向（正斜率）还是朝相反方向（负斜率）移动。 * The
denominator, <span class="math inline">\(\sum(x_i - \bar{x})^2\)</span>,
is related to the <strong>variance</strong> of X. It measures how much X
varies on its own. 它衡量 X 自身的变化量。 * <strong>In short, the slope
is a measure of how X and Y vary together, scaled by how much X varies
by itself.</strong> 斜率衡量的是 X 和 Y 共同变化的程度，并以 X
自身的变化量为标度。</p>
<h4 id="the-intercept-hatbeta_0-截距">2. The Intercept: <span
class="math inline">\(\hat{\beta}_0\)</span> 截距</h4>
<p><span class="math display">\[\hat{\beta}_0 = \bar{y} -
\hat{\beta}_1\bar{x}\]</span> * <strong>Intuition:</strong> This formula
is even simpler and has a wonderful geometric meaning. It ensures that
the <strong>line of best fit always passes through the “center of mass”
of the data</strong>, which is the point of averages <span
class="math inline">\((\bar{x}, \bar{y})\)</span>.
它确保<strong>最佳拟合线始终穿过数据的“质心”</strong>，即平均值 <span
class="math inline">\((\bar{x}, \bar{y})\)</span> 的点。计算出最佳斜率
(<span class="math inline">\(\hat{\beta}_1\)</span>)
后，就可以将其代入此公式。然后，可以调整截距 (<span
class="math inline">\(\hat{\beta}_0\)</span>)，使直线完美地围绕数据云的中心点旋转。
* Once you’ve calculated the best slope (<span
class="math inline">\(\hat{\beta}_1\)</span>), you can plug it into this
formula. You then adjust the intercept (<span
class="math inline">\(\hat{\beta}_0\)</span>) so that the line pivots
perfectly around the central point of your data cloud.</p>
<p>In summary, this slide provides the precise, closed-form formulas to
calculate the slope and intercept for the line of best fit in a simple
linear regression model.</p>
<h1 id="statistical-inference">3. Statistical Inference</h1>
<p><img src="/imgs/5054C3/Statistical_Inference1.png" alt="Statistical_Inference1">
<img src="/imgs/5054C3/Statistical_Inference2.png" alt="Statistical_Inference2">
## 3.1 Statistical Inference - <strong>内容</strong>:
<strong>Statistical Inference:</strong> These two slides are deeply
connected and explain how we go from just <em>calculating</em> the
coefficients to understanding how <em>accurate</em> and
<em>reliable</em> they are.
解释了我们如何从仅仅<em>计算</em>系数到理解它们的<em>准确性</em>和<em>可靠性</em>。</p>
<h3 id="the-core-problem-quantifying-uncertainty-量化不确定性">## The
Core Problem: Quantifying Uncertainty 量化不确定性</h3>
<p>The second slide poses the fundamental questions: * “How accurate are
<span class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span>?”准确性如何？ * “What are
the distributions of <span class="math inline">\(\hat{\beta}_0\)</span>
and <span class="math inline">\(\hat{\beta}_1\)</span>?”分布是什么？</p>
<p>The reason we ask this is that our estimated coefficients (<span
class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>) were
calculated from a <strong>specific sample of data</strong>. If we
collected a different random sample from the same population, we would
get slightly different estimates.估计的系数 (<span
class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span>)
是根据<strong>特定的数据样本</strong>计算出来的。如果我们从同一总体中随机抽取不同的样本，我们得到的估计值会略有不同。</p>
<p>The goal of statistical inference is to use the estimates from our
single sample to make conclusions about the <strong>true, unknown
population parameters</strong> (<span class="math inline">\(\beta_0,
\beta_1\)</span>) and to quantify our uncertainty about
them.统计推断的目标是利用单个样本的估计值得出关于<strong>真实、未知的总体参数</strong>（<span
class="math inline">\(\beta_0,
\beta_1\)</span>）的结论，并量化对这些参数的不确定性。</p>
<h3
id="the-key-assumption-that-makes-it-possible-实现这一目标的关键假设">##
The Key Assumption That Makes It Possible 实现这一目标的关键假设</h3>
<p>To figure out the distribution of our estimates, we must make an
assumption about the distribution of the errors. This is the most
important assumption in linear regression for inference:
为了确定估计值的分布，必须对误差的分布做出假设。这是线性回归推断中最重要的假设：
<strong>Assumption:</strong> <span class="math inline">\(\epsilon_i
\stackrel{\text{i.i.d.}}{\sim} N(0, \sigma^2)\)</span></p>
<p>This means we assume the random error terms are: * <strong>Normally
distributed</strong> (<span class="math inline">\(N\)</span>).*
<strong>正态分布</strong>（<span class="math inline">\(N\)</span>）。 *
Have a mean of <strong>zero</strong> (our model is correct on average).*
均值为<strong>零</strong>（模型平均而言是正确的）。 * Have a constant
variance <strong><span class="math inline">\(\sigma^2\)</span></strong>
(homoscedasticity).* 方差为常数<strong><span
class="math inline">\(\sigma^2\)</span></strong>（方差齐性）。 * Are
<strong>independent and identically distributed</strong> (i.i.d.),
meaning each error is independent of the others.*
是<strong>独立同分布</strong>（i.i.d.）的，这意味着每个误差都独立于其他误差。</p>
<p><strong>Why is this important?</strong> Because our coefficients
<span class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span> are calculated as weighted
sums of the <span class="math inline">\(y_i\)</span> values, and the
<span class="math inline">\(y_i\)</span> values depend on the errors
<span class="math inline">\(\epsilon_i\)</span>. This assumption about
the errors allows us to prove that our estimated coefficients themselves
are also normally distributed. 系数 <span
class="math inline">\(\hat{\beta}_0\)</span> 和 <span
class="math inline">\(\hat{\beta}_1\)</span> 是通过 <span
class="math inline">\(y_i\)</span> 值的加权和计算的，而 <span
class="math inline">\(y_i\)</span> 值取决于误差 <span
class="math inline">\(\epsilon_i\)</span>。这个关于误差的假设使能够证明估计的系数本身也服从正态分布。</p>
<h3
id="the-solution-the-theorem-and-the-t-distribution-定理和-t-分布">##
The Solution: The Theorem and the t-distribution 定理和 t 分布</h3>
<p>The first slide provides the central theorem that allows us to
perform inference. It tells us exactly how to standardize our estimated
coefficients so they follow a known distribution.
第一张幻灯片提供了进行推断的核心定理。它准确地告诉我们如何对估计的系数进行标准化，使其服从已知的分布。</p>
<h4 id="the-standard-error-s.e.-标准误差-s.e.">1. The Standard Error
(s.e.) 标准误差 (s.e.)</h4>
<p>First, look at the denominators in the red dotted boxes. These are
the <strong>standard errors</strong> of the coefficients,
<code>s.e.($\hat&#123;\beta&#125;_1$)</code> and
<code>s.e.($\hat&#123;\beta&#125;_0$)</code>.
第一张幻灯片提供了进行推断的核心定理。它准确地告诉我们如何对估计的系数进行标准化，使其服从已知的分布。</p>
<ul>
<li><strong>What it is:</strong> The standard error is the estimated
<strong>standard deviation of the coefficient’s sampling
distribution</strong>. In simpler terms, it’s a measure of the average
amount by which our estimate <span
class="math inline">\(\hat{\beta}_1\)</span> would differ from the true
<span class="math inline">\(\beta_1\)</span> if we were to repeat the
experiment many times.
标准误差是系数抽样分布的<strong>标准差</strong>估计值。简单来说，它衡量的是如果我们重复实验多次，我们估计的
<span class="math inline">\(\hat{\beta}_1\)</span> 与真实的 <span
class="math inline">\(\beta_1\)</span> 之间的平均差异。</li>
<li><strong>A smaller standard error means a more precise and reliable
estimate.</strong>
<strong>标准误差越小，估计值越精确可靠。</strong></li>
</ul>
<h4 id="the-t-statistic-t-统计量">2. The t-statistic t 统计量</h4>
<p>The theorem shows two fractions that form a
<strong>t-statistic</strong>. The general structure for this is:
该定理展示了两个构成<strong>t 统计量</strong>的分数。其一般结构如下：
<span class="math display">\[t = \frac{\text{ (Sample Estimate - True
Value) }}{\text{ Standard Error of the Estimate }}\]</span></p>
<p>For <span class="math inline">\(\beta_1\)</span>, this is: <span
class="math inline">\(\frac{\hat{\beta}_1 -
\beta_1}{\text{s.e.}(\hat{\beta}_1)}\)</span>.</p>
<p>The key insight is that this specific quantity follows a
<strong>Student’s t-distribution</strong> with <strong><span
class="math inline">\(n-2\)</span> degrees of freedom</strong>.
关键在于，这个特定量服从<strong>学生 t
分布</strong>，其自由度为<strong><span
class="math inline">\(n-2\)</span>。 * </strong>Student’s
t-distribution:** This is a probability distribution that looks very
similar to the normal distribution but has slightly “heavier” tails. We
use it instead of the normal distribution because we had to
<em>estimate</em> the standard deviation of the errors (<code>s</code>
in the formula), which adds extra uncertainty.
这是一种概率分布，与正态分布非常相似，但尾部略重。使用它来代替正态分布，是因为必须<em>估计</em>误差的标准差（公式中的
<code>s</code>），这会增加额外的不确定性。 * <strong>Degrees of Freedom
(n-2):</strong> We start with <code>n</code> data points, but we lose
two degrees of freedom because we used the data to estimate two
parameters: <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>. 从 <code>n</code>
个数据点开始，但由于用这些数据估计了两个参数：<span
class="math inline">\(\beta_0\)</span> 和 <span
class="math inline">\(\beta_1\)</span>，因此损失了两个自由度。 #### 3.
Estimating the Error Variance (<span
class="math inline">\(s^2\)</span>)估计误差方差 (<span
class="math inline">\(s^2\)</span>) To calculate the standard errors, we
need a value for <code>s</code>, which is our estimate of the true error
standard deviation <span class="math inline">\(\sigma\)</span>. This is
calculated from the <strong>Residual Sum of Squares (RSS)</strong>.
为了计算标准误差，我们需要一个 <code>s</code> 的值，它是对真实误差标准差
<span class="math inline">\(\sigma\)</span>
的估计值。该值由<strong>残差平方和 (RSS)</strong> 计算得出。 *
<strong>RSS:</strong> First, we calculate the RSS = <span
class="math inline">\(\sum(y_i - \hat{y}_i)^2\)</span>, which is the sum
of all the squared errors.* <strong>RSS</strong>：首先，计算 RSS = <span
class="math inline">\(\sum(y_i -
\hat{y}_i)^2\)</span>，即所有平方误差之和。 * <strong><span
class="math inline">\(s^2\)</span>:</strong> Then, we find the estimate
of the error variance: <span class="math inline">\(s^2 = \text{RSS} /
(n-2)\)</span>. We divide by <span class="math inline">\(n-2\)</span> to
get an unbiased estimate. * <strong><span
class="math inline">\(s^2\)</span></strong>：然后，计算误差方差的估计值：<span
class="math inline">\(s^2 = \text{RSS} / (n-2)\)</span>。我们将其除以
<span class="math inline">\(n-2\)</span> 即可得到无偏估计值。 *
<code>s</code> is simply the square root of <span
class="math inline">\(s^2\)</span>. This <code>s</code> is the value
used in the standard error formulas.* <code>s</code> 就是 <span
class="math inline">\(s^2\)</span> 的平方根。这个 <code>s</code>
是标准误差公式中使用的值。</p>
<h3 id="what-this-allows-us-to-do-the-practical-use">## What This Allows
Us To Do (The Practical Use)</h3>
<p>Because we know the exact distribution of our t-statistic, we can now
achieve our goal of quantifying uncertainty: 因为知道 t
统计量的精确分布，所以现在可以实现量化不确定性的目标：</p>
<ol type="1">
<li><strong>Hypothesis Testing:</strong> We can test if a predictor is
actually useful. The most common test is for the null hypothesis <span
class="math inline">\(H_0: \beta_1 = 0\)</span>. If we can prove the
observed <span class="math inline">\(\hat{\beta}_1\)</span> is very
unlikely to occur if the true <span
class="math inline">\(\beta_1\)</span> were zero, we can conclude there
is a statistically significant relationship between <span
class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span>.
可以检验一个预测变量是否真的有用。最常见的检验是零假设 <span
class="math inline">\(H_0: \beta_1 = 0\)</span>。如果能证明，当真实的
<span class="math inline">\(\beta_1\)</span> 为零时，观测到的 <span
class="math inline">\(\hat{\beta}_1\)</span>
不太可能发生，那么就可以得出结论，<span class="math inline">\(x\)</span>
和 <span class="math inline">\(y\)</span>
之间存在统计学上的显著关系。</li>
<li><strong>Confidence Intervals:</strong> We can construct a range of
plausible values for the true coefficient. For example, we can calculate
a 95% confidence interval for <span
class="math inline">\(\beta_1\)</span>. This gives us a range where we
are 95% confident the true value of <span
class="math inline">\(\beta_1\)</span> lies.
可以为真实系数构建一系列合理的值。</li>
</ol>
<h1 id="multiple-linear-regression">4. Multiple Linear Regression</h1>
<p><img src="/imgs/5054C3/Multiple_Linear Regression1.png" alt="Multiple_Linear Regression1">
<img src="/imgs/5054C3/Multiple_Linear Regression2.png" alt="Multiple_Linear Regression2">
## 4.1 Multiple Linear Regression - <strong>内容</strong>:
<strong>Multiple Linear Regression:</strong></p>
<p>Here’s a detailed breakdown that connects both slides.</p>
<h3
id="the-model-from-one-to-many-predictors-从单预测变量到多预测变量">##
The Model: From One to Many Predictors 从单预测变量到多预测变量</h3>
<p>The first slide introduces the <strong>Multiple Linear Regression
model</strong>. This is a direct extension of the simple model, but
instead of using just one predictor variable, we use multiple (<span
class="math inline">\(p\)</span>) predictors to explain our response
variable.
多元线性回归模型是简单模型的直接扩展，但不是只使用一个预测变量，而是使用多个（<span
class="math inline">\(p\)</span>）预测变量来解释响应变量。</p>
<p>The general formula is: <span class="math display">\[y_i = \beta_0 +
\beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_px_{ip} +
\epsilon_i\]</span></p>
<h4 id="key-change-in-interpretation">Key Change in Interpretation</h4>
<p>This is the most important new concept. In simple regression, <span
class="math inline">\(\beta_1\)</span> was just the slope. In multiple
regression, each coefficient has a more nuanced meaning:
在简单回归中，<span class="math inline">\(\beta_1\)</span>
只是斜率。在多元回归中，每个系数都有更微妙的含义：</p>
<p><strong><span class="math inline">\(\beta_j\)</span> is the average
change in <span class="math inline">\(y\)</span> for a one-unit increase
in <span class="math inline">\(x_j\)</span>, while holding all other
predictors constant.</strong></p>
<p>This is incredibly powerful. Using the advertising example from your
slide: * <span class="math inline">\(y_i = \beta_0 +
\beta_1(\text{TV}_i) + \beta_2(\text{Radio}_i) +
\beta_3(\text{Newspaper}_i) + \epsilon_i\)</span> * <span
class="math inline">\(\beta_1\)</span> represents the effect of TV
advertising on sales, <strong>after controlling for</strong> the amount
spent on Radio and Newspaper ads. This allows you to isolate the unique
contribution of each advertising
channel.表示在<strong>控制</strong>广播和报纸广告支出后，电视广告对销售额的影响。这可以让您区分每个广告渠道的独特贡献。</p>
<h3 id="the-solution-deriving-the-normal-equation-推导正态方程">## The
Solution: Deriving the Normal Equation 推导正态方程</h3>
<p>The second slide shows the mathematical process for finding the best
coefficients (<span class="math inline">\(\beta_0, \beta_1, \dots,
\beta_p\)</span>) using the <strong>Ordinary Least Squares
(OLS)</strong> method. It’s essentially a condensed derivation of the
<strong>Normal Equation</strong>. 使用<strong>普通最小二乘法
(OLS)</strong> 寻找最佳系数 (<span class="math inline">\(\beta_0,
\beta_1, \dots, \beta_p\)</span>)
的数学过程。它本质上是<strong>正态方程</strong>的简化推导。</p>
<h4 id="the-goal-minimizing-the-sum-of-squares-最小化平方和">1. The
Goal: Minimizing the Sum of Squares 最小化平方和</h4>
<p>Just like before, our goal is to minimize the sum of the squared
errors (or residuals): 目标是最小化平方误差（或残差）之和。</p>
<ul>
<li><strong>Scalar Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_{i1} -
\beta_2x_{i2} - \beta_3x_{i3})^2\)</span>
<ul>
<li>This is easy to read but gets very long with more variables.
代码易于阅读，但变量越多，代码越长。</li>
</ul></li>
<li><strong>Vector Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \boldsymbol{\beta}^T
\mathbf{x}_i)^2\)</span>
<ul>
<li>This is a more compact and powerful way to write the same thing
using linear algebra, where <span
class="math inline">\(\boldsymbol{\beta}^T \mathbf{x}_i\)</span> is the
dot product that calculates the entire predicted value <span
class="math inline">\(\hat{y}_i\)</span>.
这是一种更简洁、更强大的线性代数表示方法，其中 <span
class="math inline">\(\boldsymbol{\beta}^T \mathbf{x}_i\)</span>
是计算整个预测值 <span class="math inline">\(\hat{y}_i\)</span>
的点积。</li>
</ul></li>
</ul>
<h4
id="the-method-using-calculus-to-find-the-minimum-使用微积分求最小值">2.
The Method: Using Calculus to Find the Minimum 使用微积分求最小值</h4>
<p>To find the set of <span class="math inline">\(\beta\)</span> values
that results in the lowest possible error, we use calculus.</p>
<ul>
<li><p><strong>The Derivative (Gradient):</strong> Since our error
function depends on multiple <span class="math inline">\(\beta\)</span>
coefficients, we can’t take a simple derivative. Instead, we take the
<strong>gradient</strong>, which is a vector of partial derivatives (one
for each coefficient). This tells us the “slope” of the error function
in every direction. 导数（梯度） 误差函数依赖于多个 <span
class="math inline">\(\beta\)</span>
系数，因此我们不能简单地求导数。相反，采用<strong>梯度</strong>，它是一个由偏导数组成的向量（每个系数对应一个偏导数）。这告诉误差函数在各个方向上的“斜率”。</p></li>
<li><p><strong>Setting the Gradient to Zero:</strong> The minimum of a
function occurs where its slope is zero (the very bottom of the error
“valley”). The slide shows the result of taking this gradient and
setting it to
zero.函数的最小值出现在其斜率为零的地方（即误差“谷底”的最低点）。幻灯片展示了取此梯度并将其设为零的结果。</p></li>
</ul>
<p>The equation shown on the slide: <span class="math display">\[2
\sum_{i=1}^{n} (\boldsymbol{\beta}^T \mathbf{x}_i - y_i)\mathbf{x}_i^T =
0\]</span> …is the result of this calculus step. The goal is now to
algebraically rearrange this equation to solve for <span
class="math inline">\(\boldsymbol{\beta}\)</span>.
是这一微积分步骤的结果。现在的目标是用代数方法重新排列这个方程，以求解
<span class="math inline">\(\boldsymbol{\beta}\)</span>。</p>
<h4 id="the-result-the-normal-equation-正则方程">3. The Result: The
Normal Equation 正则方程</h4>
<p>After rearranging the equation from the previous step and expressing
the sums in their full matrix form, we arrive at a clean and beautiful
solution. While the slide doesn’t show the final step, the result of
“Setting the gradient zero and solve <span
class="math inline">\(\beta\)</span>” is the <strong>Normal
Equation</strong>:
重新排列上一步中的方程，并将和表示为完整的矩阵形式后，得到了一个简洁美观的解。虽然幻灯片没有展示最后一步，“设置梯度零点并求解
<span class="math inline">\(\beta\)</span>”
的结果就是<strong>正态方程</strong>：</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the
vector of our optimal coefficient estimates.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is the “design
matrix” where each row is an observation and each column is a predictor
variable. <span class="math inline">\(\mathbf{X}\)</span>
是“设计矩阵”，其中每一行代表一个观测值，每一列代表一个预测变量。</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of our
response variable. <span class="math inline">\(\mathbf{y}\)</span>
是我们的响应变量的向量。</li>
</ul>
<p>This single equation is the general solution for finding the OLS
coefficients for <strong>any</strong> linear regression model, no matter
how many predictors you have. This is what statistical software
calculates for you under the hood.
无论有多少个预测变量，这个简单的方程都是<strong>任何</strong>线性回归模型中
OLS 系数的通解。</p>
<h1 id="matrix-notatio">5. matrix notatio</h1>
<p><img src="/imgs/5054C3/matrix_notatio.png"></p>
<ul>
<li><strong>内容</strong>: This slide introduces the <strong>matrix
notation</strong> for multiple linear regression, which is a powerful
way to represent the entire system of equations in a compact form. This
notation isn’t just for tidiness—it’s the foundation for how the
solutions are derived and calculated in software.</li>
</ul>
<p>多元线性回归的<strong>矩阵符号</strong>，这是一种以紧凑形式表示整个方程组的有效方法。这种符号不仅仅是为了简洁，它还是软件中推导和计算解的基础。
Here is a more detailed breakdown.</p>
<h3 id="why-use-matrix-notation">## Why Use Matrix Notation?</h3>
<p>Imagine you have 10,000 observations (<span
class="math inline">\(n=10,000\)</span>) and 5 predictor variables
(<span class="math inline">\(p=5\)</span>). Writing out the model
equation for each observation would be impossible: <span
class="math inline">\(y_1 = \beta_0 + \beta_1x_{11} + \dots +
\beta_5x_{15} + \epsilon_1\)</span> <span class="math inline">\(y_2 =
\beta_0 + \beta_1x_{21} + \dots + \beta_5x_{25} + \epsilon_2\)</span>
…and so on for 10,000 lines.</p>
<p>假设你有 10,000 个观测值（n=10,000）和 5
个预测变量（p=5）。为每个观测值写出模型方程是不可能的： <span
class="math inline">\(y_1 = \beta_0 + \beta_1x_{11} + \dots +
\beta_5x_{15} + \epsilon_1\)</span> <span class="math inline">\(y_2 =
\beta_0 + \beta_1x_{21} + \dots + \beta_5x_{25} + \epsilon_2\)</span>
……以此类推，直到 10,000 行。 Matrix notation allows us to consolidate
this entire system into a single, elegant
equation:矩阵符号使我们能够将整个系统合并成一个简洁的方程： <span
class="math display">\[\mathbf{y} = \mathbf{X}\boldsymbol{\beta} +
\boldsymbol{\epsilon}\]</span> Let’s break down each component shown on
your slide.</p>
<h3 id="the-components-explained">## The Components Explained</h3>
<h4 id="the-design-matrix-mathbfx-设计矩阵">1. The Design Matrix: <span
class="math inline">\(\mathbf{X}\)</span> 设计矩阵</h4>
<p><span class="math display">\[\mathbf{X} = \begin{pmatrix} 1 &amp;
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\ 1 &amp; x_{21} &amp;
x_{22} &amp; \cdots &amp; x_{2p} \\ \vdots &amp; \vdots &amp; \vdots
&amp; \ddots &amp; \vdots \\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots
&amp; x_{np} \end{pmatrix}\]</span> This is the most important matrix.
It contains all of your predictor variable
data.这是最重要的矩阵。它包含所有预测变量数据。 * <strong>Rows:</strong>
Each row represents a single observation (e.g., a person, a company, a
day). There are <strong>n</strong>
rows.每一行代表一个观察值（例如，一个人、一家公司、一天）。共有
<strong>n</strong> 行。 * <strong>Columns:</strong> Each column
represents a predictor variable. There are <strong>p</strong> predictor
columns, plus one special column.每列代表一个预测变量。共有
<strong>p</strong> 个预测列，外加一个特殊列。 * <strong>The Column of
Ones:</strong> This is a crucial detail. This first column of all ones
is a placeholder for the <strong>intercept term (<span
class="math inline">\(\beta_0\)</span>)</strong>. When you perform
matrix multiplication, this <code>1</code> gets multiplied by <span
class="math inline">\(\beta_0\)</span>, ensuring the intercept is
included in the model for every single observation.
这是一个至关重要的细节。第一列（全 1）是<strong>截距项 (<span
class="math inline">\(\beta_0\)</span>)</strong>
的占位符。执行矩阵乘法时，这个 <code>1</code> 会乘以 <span
class="math inline">\(\beta_0\)</span>，以确保截距包含在模型中，适用于每个观测值。</p>
<h4 id="the-coefficient-vector-boldsymbolbeta-系数向量">2. The
Coefficient Vector: <span
class="math inline">\(\boldsymbol{\beta}\)</span> 系数向量</h4>
<p><span class="math display">\[\boldsymbol{\beta} = \begin{pmatrix}
\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{pmatrix}\]</span> This is a
column vector that contains all the model parameters—the unknown values
we want to estimate. The goal of linear regression is to find the
numerical values for this vector.</p>
<h4 id="the-response-vector-mathbfy-响应向量">3. The Response Vector:
<span class="math inline">\(\mathbf{y}\)</span> 响应向量</h4>
<p><span class="math display">\[\mathbf{y} = \begin{pmatrix} y_1 \\
\vdots \\ y_n \end{pmatrix}\]</span> This is a column vector containing
all the observed outcomes you are trying to predict (e.g., sales, test
scores, stock prices).</p>
<h4 id="the-error-vector-boldsymbolepsilon-误差向量">4. The Error
Vector: <span class="math inline">\(\boldsymbol{\epsilon}\)</span>
误差向量</h4>
<p><span class="math display">\[\boldsymbol{\epsilon} = \begin{pmatrix}
\epsilon_1 \\ \vdots \\ \epsilon_n \end{pmatrix}\]</span> This column
vector bundles together all the individual, unobserved random errors. It
represents the portion of <strong>y</strong> that our model cannot
explain with <strong>X</strong>.</p>
<h3 id="putting-it-all-together">## Putting It All Together</h3>
<p>When you write the equation <span class="math inline">\(\mathbf{y} =
\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span>, you are
actually representing the entire system of individual equations.</p>
<p>Let’s look at the multiplication <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>: <span
class="math display">\[\begin{pmatrix} 1 &amp; x_{11} &amp; \dots &amp;
x_{1p} \\ 1 &amp; x_{21} &amp; \dots &amp; x_{2p} \\ \vdots &amp; \vdots
&amp; \ddots &amp; \vdots \\ 1 &amp; x_{n1} &amp; \dots &amp; x_{np}
\end{pmatrix} \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p
\end{pmatrix} = \begin{pmatrix} 1\cdot\beta_0 + x_{11}\cdot\beta_1 +
\dots + x_{1p}\cdot\beta_p \\ 1\cdot\beta_0 + x_{21}\cdot\beta_1 + \dots
+ x_{2p}\cdot\beta_p \\ \vdots \\ 1\cdot\beta_0 + x_{n1}\cdot\beta_1 +
\dots + x_{np}\cdot\beta_p \end{pmatrix}\]</span> As you can see, the
result of this multiplication is a single column vector where each row
is the “predictor” part of the regression equation for that observation.
此乘法的结果是一个单列向量，其中每一行都是该观测值的回归方程的“预测变量”部分。</p>
<p>By setting this equal to <span class="math inline">\(\mathbf{y} -
\boldsymbol{\epsilon}\)</span>, you perfectly recreate the entire set of
<code>n</code> equations in one clean statement. This compact form is
what allows us to easily derive and compute the <strong>Normal
Equation</strong> solution: <span
class="math inline">\(\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>.这种紧凑形式使我们能够轻松推导和计算<strong>正态方程</strong>的解</p>
<h1
id="the-core-mathematical-conclusion-of-ordinary-least-squares-ols">6.
the core mathematical conclusion of Ordinary Least Squares (OLS)</h1>
<p><img src="/imgs/5054C3/OLS1.png">
<img src="/imgs/5054C3/OLS2.png"></p>
<ul>
<li><strong>内容</strong>: Of course. These slides present the core
mathematical conclusion of Ordinary Least Squares (OLS) and a key
geometric property that explains <em>why</em> this solution works.
展示了普通最小二乘法 (OLS)
的核心数学结论，以及一个关键的几何性质，解释了该解决方案<em>为何</em>有效。
Let’s break down the concepts and the calculation processes in
detail.</li>
</ul>
<hr />
<h3 id="part-1-the-objective-and-the-solution-slide-1-最小化几何距离">##
Part 1: The Objective and the Solution (Slide 1) 最小化几何距离</h3>
<p>This slide summarizes the entire OLS problem and its solution in the
language of matrix algebra.</p>
<h4 id="the-concept-minimizing-geometric-distance"><strong>The Concept:
Minimizing Geometric Distance</strong></h4>
<p>“最小二乘准则”是我们模型的目标。 The “least squares criterion” is the
objective of our model. The slide shows it in two equivalent forms:</p>
<ol type="1">
<li><strong>Summation Form:</strong> <span
class="math inline">\(\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_{i1} -
\dots - \beta_px_{ip})^2\)</span> This is the sum of the squared
differences between the actual values (<span
class="math inline">\(y_i\)</span>) and the predicted values. 这是实际值
(<span class="math inline">\(y_i\)</span>) 与预测值之差的平方和。</li>
<li><strong>Matrix Form:</strong> <span
class="math inline">\(||\mathbf{y} - \mathbf{X}\boldsymbol{\beta}||^2 =
(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} -
\mathbf{X}\boldsymbol{\beta})\)</span> This is the more powerful way to
view the problem. Think of <span
class="math inline">\(\mathbf{y}\)</span> (the vector of all actual
outcomes) and <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> (the vector
of all predicted outcomes) as two points in an n-dimensional space. The
expression <span class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{\beta}||^2\)</span> represents the <strong>squared
Euclidean distance</strong> between these two points. 将 <span
class="math inline">\(\mathbf{y}\)</span>（所有实际结果的向量）和 <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>（所有预测结果的向量）视为
n 维空间中的两个点。表达式 <span class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{\beta}||^2\)</span>
表示这两点之间的<strong>平方欧氏距离</strong>。 Therefore, the OLS
problem is a geometric one: <strong>Find the coefficient vector <span
class="math inline">\(\boldsymbol{\beta}\)</span> that makes the
predicted values vector <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span> as close as
possible to the actual values vector <span
class="math inline">\(\mathbf{y}\)</span>.</strong> 因此，OLS
问题是一个几何问题：<strong>找到一个系数向量 <span
class="math inline">\(\boldsymbol{\beta}\)</span>，使预测值向量 <span
class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>
尽可能接近实际值向量 <span
class="math inline">\(\mathbf{y}\)</span>。</strong></li>
</ol>
<h4
id="the-solution-the-least-squares-estimator-lse最小二乘估计器-lse"><strong>The
Solution: The Least Squares Estimator (LSE)</strong>最小二乘估计器
(LSE)</h4>
<p>The slide provides the direct solution to this minimization problem,
which is the <strong>Normal
Equation</strong>:此最小化问题的直接解，即<strong>正态方程</strong>：</p>
<p><span class="math display">\[\hat{\boldsymbol{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span></p>
<p>This formula gives you the exact vector of coefficients <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that minimizes
the squared distance. We get this formula by taking the gradient (the
multidimensional version of a derivative) of the distance function with
respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>,
setting it to zero, and solving, as hinted at in your previous slides.
给出了使平方距离最小化的精确系数向量 通过取距离函数关于 <span
class="math inline">\(\boldsymbol{\beta}\)</span>
的梯度（导数的多维版本），将其设为零，然后求解，即可得到此公式。
Finally, the slide defines: * <strong>Fitted values:</strong> <span
class="math inline">\(\hat{\mathbf{y}} =
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span> (The vector of predictions
using our optimal coefficients). 拟合值 * <strong>Residuals:</strong>
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span> (The vector of errors, representing the
difference between actuals and
predictions).误差向量，表示实际值与预测值之间的差异</p>
<h3
id="part-2-the-geometric-property-and-proofs-slide-2几何性质及证明">##
Part 2: The Geometric Property and Proofs (Slide 2)几何性质及证明</h3>
<p>This slide explains a beautiful and fundamental property of the least
squares solution:
<strong>orthogonality</strong>.解释了最小二乘解的一个美妙而基本的性质：<strong>正交性</strong>。</p>
<h4 id="the-concept-orthogonality-of-residuals残差的正交性"><strong>The
Concept: Orthogonality of Residuals</strong>残差的正交性</h4>
<p>The main idea is that the residual vector <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> is
<strong>orthogonal</strong> (perpendicular) to every predictor variable
in your model. 主要思想是残差向量 <span
class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>
与模型中的每个预测变量<strong>正交</strong>（垂直）。</p>
<ul>
<li><p><strong>Geometric Intuition:</strong> Think of the columns of
your matrix <span class="math inline">\(\mathbf{X}\)</span> (i.e., your
predictors and the intercept) as defining a flat surface, or a
“hyperplane,” in a high-dimensional space. Your actual data vector <span
class="math inline">\(\mathbf{y}\)</span> exists somewhere in this
space, likely not on the hyperplane. The OLS process finds the point on
that hyperplane, <span class="math inline">\(\hat{\mathbf{y}}\)</span>,
that is closest to <span class="math inline">\(\mathbf{y}\)</span>. The
shortest line from a point to a plane is always one that is
<strong>perpendicular</strong> to the plane. The residual vector, <span
class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span>, <em>is</em> that line. 将矩阵 <span
class="math inline">\(\mathbf{X}\)</span>
的列（即预测变量和截距）想象成在高维空间中定义一个平面或“超平面”。实际数据向量
<span class="math inline">\(\mathbf{y}\)</span>
存在于该空间的某个位置，可能不在超平面上。OLS 过程会在该超平面 <span
class="math inline">\(\hat{\mathbf{y}}\)</span> 上找到与 <span
class="math inline">\(\mathbf{y}\)</span>
最接近的点。从一个点到一个平面的最短线始终是与该平面<strong>垂直</strong>的线。残差向量
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\hat{\mathbf{y}}\)</span> 就是这条直线。</p></li>
<li><p><strong>Mathematical Statement:</strong> This geometric property
is stated as <span class="math inline">\(\mathbf{X}^T
\hat{\boldsymbol{\epsilon}} = \mathbf{0}\)</span>. This equation means
that the dot product of the residual vector with every column of <span
class="math inline">\(\mathbf{X}\)</span> is zero, which is the
mathematical definition of orthogonality. 该等式意味着残差向量与 <span
class="math inline">\(\mathbf{X}\)</span>
每一列的点积都为零，这正是正交性的数学定义。</p></li>
</ul>
<h4 id="the-calculation-process-the-proofs"><strong>The Calculation
Process (The Proofs)</strong></h4>
<p><strong>1. Proof of Orthogonality:</strong> The slide shows a
step-by-step calculation to prove that <span
class="math inline">\(\mathbf{X}^T \hat{\boldsymbol{\epsilon}}\)</span>
is indeed zero. * <strong>Step 1:</strong> Start with the expression to
be proven: <span class="math inline">\(\mathbf{X}^T
\hat{\boldsymbol{\epsilon}}\)</span> 从待证明的表达式开始： *
<strong>Step 2:</strong> Substitute the definition of the residual,
<span class="math inline">\(\hat{\boldsymbol{\epsilon}} = \mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\mathbf{X}^T (\mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}})\]</span> 代入残差的定义 *
<strong>Step 3:</strong> Distribute the <span
class="math inline">\(\mathbf{X}^T\)</span>: <span
class="math display">\[\mathbf{X}^T \mathbf{y} -
\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}}\]</span><em>分配 </em>
<strong>Step 4:</strong> Substitute the Normal Equation for <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>: <span
class="math display">\[\mathbf{X}^T \mathbf{y} - \mathbf{X}^T\mathbf{X}
[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}]\]</span> *
<strong>Step 5:</strong> The key step is the cancellation. A matrix
<span class="math inline">\((\mathbf{X}^T\mathbf{X})\)</span> multiplied
by its inverse <span
class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span> equals the
identity matrix <span class="math inline">\(\mathbf{I}\)</span>, which
acts like the number 1 in multiplication. <span
class="math display">\[\mathbf{X}^T \mathbf{y} - \mathbf{I}
\mathbf{X}^T\mathbf{y} = \mathbf{X}^T \mathbf{y} -
\mathbf{X}^T\mathbf{y} = \mathbf{0}\]</span> 关键步骤是消去。 This
completes the proof, showing that the orthogonality property is a direct
consequence of the Normal Equation solution.</p>
<p><strong>2. Proof of LSE:</strong> This is a more abstract proof
showing that our <span
class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> truly gives the
minimum possible error. It uses the orthogonality property and the
Pythagorean theorem for vectors. It essentially shows that for any other
possible coefficient vector <span
class="math inline">\(\boldsymbol{v}\)</span>, the error <span
class="math inline">\(||\mathbf{y} -
\mathbf{X}\boldsymbol{v}||^2\)</span> will always be greater than or
equal to the error from our LSE, <span
class="math inline">\(||\mathbf{y} -
\mathbf{X}\hat{\boldsymbol{\beta}}||^2\)</span>.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/09/15/Pandoc_Deployment/" rel="prev" title="Pandoc Deployment">
      <i class="fa fa-chevron-left"></i> Pandoc Deployment
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#general-linear-regression-model."><span class="nav-number">1.</span> <span class="nav-text">1. General linear regression
model.</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#how-we-actually-find-the-best-values-for-the-%CE%B2-coefficients-parameter-estimation"><span class="nav-number">1.1.</span> <span class="nav-text">1.2
How we actually find the best values for the \(β\) coefficients (parameter
estimation)?:</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-main-method-ordinary-least-squares-ols-%E6%99%AE%E9%80%9A%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95-ols"><span class="nav-number">1.1.1.</span> <span class="nav-text">##
The Main Method: Ordinary Least Squares (OLS) 普通最小二乘法 (OLS)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#define-the-error-residuals-%E8%AF%AF%E5%B7%AE"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">1. Define the Error (Residuals)
误差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-cost-function-sum-of-squared-residuals-%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0%E6%AE%8B%E5%B7%AE%E5%B9%B3%E6%96%B9%E5%92%8C"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">2.
The Cost Function: Sum of Squared Residuals 成本函数：残差平方和</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#solving-for-the-coefficients-the-normal-equation-%E6%B1%82%E8%A7%A3%E7%B3%BB%E6%95%B0%E6%AD%A3%E6%80%81%E6%96%B9%E7%A8%8B"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">3.
Solving for the Coefficients: The Normal Equation
求解系数：正态方程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#an-alternative-method-gradient-descent-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">1.1.2.</span> <span class="nav-text">## An
Alternative Method: Gradient Descent 梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary-ols-vs.-gradient-descent"><span class="nav-number">1.1.3.</span> <span class="nav-text">## Summary: OLS vs. Gradient
Descent</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#simple-linear-regression"><span class="nav-number">2.</span> <span class="nav-text">2. Simple Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#simple-linear-regression-1"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 Simple Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-model-and-the-goal-%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%9B%AE%E6%A0%87"><span class="nav-number">2.1.1.</span> <span class="nav-text">## The Model and the Goal
模型和目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-solution-the-estimator-formulas-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E4%BC%B0%E8%AE%A1%E5%85%AC%E5%BC%8F"><span class="nav-number">2.1.2.</span> <span class="nav-text">## The
Solution: The Estimator Formulas 解决方案：估计公式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#the-slope-hatbeta_1"><span class="nav-number">2.1.2.1.</span> <span class="nav-text">1. The Slope: \(\hat{\beta}_1\)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-intercept-hatbeta_0-%E6%88%AA%E8%B7%9D"><span class="nav-number">2.1.2.2.</span> <span class="nav-text">2. The Intercept: \(\hat{\beta}_0\) 截距</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#statistical-inference"><span class="nav-number">3.</span> <span class="nav-text">3. Statistical Inference</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-core-problem-quantifying-uncertainty-%E9%87%8F%E5%8C%96%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7"><span class="nav-number">3.0.1.</span> <span class="nav-text">## The
Core Problem: Quantifying Uncertainty 量化不确定性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-key-assumption-that-makes-it-possible-%E5%AE%9E%E7%8E%B0%E8%BF%99%E4%B8%80%E7%9B%AE%E6%A0%87%E7%9A%84%E5%85%B3%E9%94%AE%E5%81%87%E8%AE%BE"><span class="nav-number">3.0.2.</span> <span class="nav-text">##
The Key Assumption That Makes It Possible 实现这一目标的关键假设</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-solution-the-theorem-and-the-t-distribution-%E5%AE%9A%E7%90%86%E5%92%8C-t-%E5%88%86%E5%B8%83"><span class="nav-number">3.0.3.</span> <span class="nav-text">##
The Solution: The Theorem and the t-distribution 定理和 t 分布</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#the-standard-error-s.e.-%E6%A0%87%E5%87%86%E8%AF%AF%E5%B7%AE-s.e."><span class="nav-number">3.0.3.1.</span> <span class="nav-text">1. The Standard Error
(s.e.) 标准误差 (s.e.)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-t-statistic-t-%E7%BB%9F%E8%AE%A1%E9%87%8F"><span class="nav-number">3.0.3.2.</span> <span class="nav-text">2. The t-statistic t 统计量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#what-this-allows-us-to-do-the-practical-use"><span class="nav-number">3.0.4.</span> <span class="nav-text">## What This Allows
Us To Do (The Practical Use)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#multiple-linear-regression"><span class="nav-number">4.</span> <span class="nav-text">4. Multiple Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#the-model-from-one-to-many-predictors-%E4%BB%8E%E5%8D%95%E9%A2%84%E6%B5%8B%E5%8F%98%E9%87%8F%E5%88%B0%E5%A4%9A%E9%A2%84%E6%B5%8B%E5%8F%98%E9%87%8F"><span class="nav-number">4.0.1.</span> <span class="nav-text">##
The Model: From One to Many Predictors 从单预测变量到多预测变量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#key-change-in-interpretation"><span class="nav-number">4.0.1.1.</span> <span class="nav-text">Key Change in Interpretation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-solution-deriving-the-normal-equation-%E6%8E%A8%E5%AF%BC%E6%AD%A3%E6%80%81%E6%96%B9%E7%A8%8B"><span class="nav-number">4.0.2.</span> <span class="nav-text">## The
Solution: Deriving the Normal Equation 推导正态方程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#the-goal-minimizing-the-sum-of-squares-%E6%9C%80%E5%B0%8F%E5%8C%96%E5%B9%B3%E6%96%B9%E5%92%8C"><span class="nav-number">4.0.2.1.</span> <span class="nav-text">1. The
Goal: Minimizing the Sum of Squares 最小化平方和</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-method-using-calculus-to-find-the-minimum-%E4%BD%BF%E7%94%A8%E5%BE%AE%E7%A7%AF%E5%88%86%E6%B1%82%E6%9C%80%E5%B0%8F%E5%80%BC"><span class="nav-number">4.0.2.2.</span> <span class="nav-text">2.
The Method: Using Calculus to Find the Minimum 使用微积分求最小值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-result-the-normal-equation-%E6%AD%A3%E5%88%99%E6%96%B9%E7%A8%8B"><span class="nav-number">4.0.2.3.</span> <span class="nav-text">3. The Result: The
Normal Equation 正则方程</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#matrix-notatio"><span class="nav-number">5.</span> <span class="nav-text">5. matrix notatio</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#why-use-matrix-notation"><span class="nav-number">5.0.1.</span> <span class="nav-text">## Why Use Matrix Notation?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-components-explained"><span class="nav-number">5.0.2.</span> <span class="nav-text">## The Components Explained</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#the-design-matrix-mathbfx-%E8%AE%BE%E8%AE%A1%E7%9F%A9%E9%98%B5"><span class="nav-number">5.0.2.1.</span> <span class="nav-text">1. The Design Matrix: \(\mathbf{X}\) 设计矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-coefficient-vector-boldsymbolbeta-%E7%B3%BB%E6%95%B0%E5%90%91%E9%87%8F"><span class="nav-number">5.0.2.2.</span> <span class="nav-text">2. The
Coefficient Vector: \(\boldsymbol{\beta}\) 系数向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-response-vector-mathbfy-%E5%93%8D%E5%BA%94%E5%90%91%E9%87%8F"><span class="nav-number">5.0.2.3.</span> <span class="nav-text">3. The Response Vector:
\(\mathbf{y}\) 响应向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-error-vector-boldsymbolepsilon-%E8%AF%AF%E5%B7%AE%E5%90%91%E9%87%8F"><span class="nav-number">5.0.2.4.</span> <span class="nav-text">4. The Error
Vector: \(\boldsymbol{\epsilon}\)
误差向量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#putting-it-all-together"><span class="nav-number">5.0.3.</span> <span class="nav-text">## Putting It All Together</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#the-core-mathematical-conclusion-of-ordinary-least-squares-ols"><span class="nav-number">6.</span> <span class="nav-text">6.
the core mathematical conclusion of Ordinary Least Squares (OLS)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#part-1-the-objective-and-the-solution-slide-1-%E6%9C%80%E5%B0%8F%E5%8C%96%E5%87%A0%E4%BD%95%E8%B7%9D%E7%A6%BB"><span class="nav-number">6.0.1.</span> <span class="nav-text">##
Part 1: The Objective and the Solution (Slide 1) 最小化几何距离</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#the-concept-minimizing-geometric-distance"><span class="nav-number">6.0.1.1.</span> <span class="nav-text">The Concept:
Minimizing Geometric Distance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-solution-the-least-squares-estimator-lse%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E4%BC%B0%E8%AE%A1%E5%99%A8-lse"><span class="nav-number">6.0.1.2.</span> <span class="nav-text">The
Solution: The Least Squares Estimator (LSE)最小二乘估计器
(LSE)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#part-2-the-geometric-property-and-proofs-slide-2%E5%87%A0%E4%BD%95%E6%80%A7%E8%B4%A8%E5%8F%8A%E8%AF%81%E6%98%8E"><span class="nav-number">6.0.2.</span> <span class="nav-text">##
Part 2: The Geometric Property and Proofs (Slide 2)几何性质及证明</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#the-concept-orthogonality-of-residuals%E6%AE%8B%E5%B7%AE%E7%9A%84%E6%AD%A3%E4%BA%A4%E6%80%A7"><span class="nav-number">6.0.2.1.</span> <span class="nav-text">The
Concept: Orthogonality of Residuals残差的正交性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#the-calculation-process-the-proofs"><span class="nav-number">6.0.2.2.</span> <span class="nav-text">The Calculation
Process (The Proofs)</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
